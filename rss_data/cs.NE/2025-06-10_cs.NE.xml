<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>EvoGrad: Metaheuristics in a Differentiable Wonderland</title>
      <link>https://arxiv.org/abs/2506.06320</link>
      <description>arXiv:2506.06320v1 Announce Type: new 
Abstract: Differentiable programming has revolutionised optimisation by enabling efficient gradient-based training of complex models, such as Deep Neural Networks (NNs) with billions and trillions of parameters. However, traditional Evolutionary Computation (EC) and Swarm Intelligence (SI) algorithms, widely successful in discrete or complex search spaces, typically do not leverage local gradient information, limiting their optimisation efficiency. In this paper, we introduce EvoGrad, a unified differentiable framework that integrates EC and SI with gradient-based optimisation through backpropagation. EvoGrad converts conventional evolutionary and swarm operators (e.g., selection, mutation, crossover, and particle updates) into differentiable operators, facilitating end-to-end gradient optimisation. Extensive experiments on benchmark optimisation functions and training of small NN regressors reveal that our differentiable versions of EC and SI metaheuristics consistently outperform traditional, gradient-agnostic algorithms in most scenarios. Our results show the substantial benefits of fully differentiable evolutionary and swarm optimisation, setting a new standard for hybrid optimisation frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06320v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice F. R. Citterio, Andrea Tangherloni</dc:creator>
    </item>
    <item>
      <title>Neural networks with image recognition by pairs</title>
      <link>https://arxiv.org/abs/2506.06322</link>
      <description>arXiv:2506.06322v1 Announce Type: new 
Abstract: Neural networks based on metric recognition methods have a strictly determined architecture. Number of neurons, connections, as well as weights and thresholds values are calculated analytically, based on the initial conditions of tasks: number of recognizable classes, number of samples, metric expressions used. This paper discusses the possibility of transforming these networks in order to apply classical learning algorithms to them without using analytical expressions that calculate weight values. In the received network, training is carried out by recognizing images in pairs. This approach simplifies the learning process and easily allows to expand the neural network by adding new images to the recognition task. The advantages of these networks, including such as: 1) network architecture simplicity and transparency; 2) training simplicity and reliability; 3) the possibility of using a large number of images in the recognition problem using a neural network; 4) a consistent increase in the number of recognizable classes without changing the previous values of weights and thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06322v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Optical Memory and Neural Networks, Vol. 27, pp. 113-119, 2018</arxiv:journal_reference>
      <dc:creator>Polad Geidarov</dc:creator>
    </item>
    <item>
      <title>Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies</title>
      <link>https://arxiv.org/abs/2506.06325</link>
      <description>arXiv:2506.06325v1 Announce Type: new 
Abstract: This paper proposes a decentralized model of energy cooperation between microgrids, in which decisions are made locally, at the level of the microgrid community. Each microgrid is modeled as an autonomous agent that adopts a Hawk or Dove strategy, depending on the level of energy stored in the battery and its role in the energy trading process. The interactions between selling and buying microgrids are modeled through an evolutionary algorithm. An individual in the algorithm population is represented as an energy trading matrix that encodes the amounts of energy traded between the selling and buying microgrids. The population evolution is achieved by recombination and mutation operators. Recombination uses a specialized operator for matrix structures, and mutation is applied to the matrix elements according to a Gaussian distribution. The evaluation of an individual is made with a multi-criteria fitness function that considers the seller profit, the degree of energy stability at the community level, penalties for energy imbalance at the community level and for the degradation of microgrids batteries. The method was tested on a simulated scenario with 100 microgrids, each with its own selling and buying thresholds, to reflect a realistic environment with variable storage characteristics of microgrids batteries. By applying the algorithm on this scenario, 95 out of the 100 microgrids reached a stable energy state. This result confirms the effectiveness of the proposed model in achieving energy balance both at the individual level, for each microgrid, and at the level of the entire community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06325v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Viorica Rozina Chifu, Tudor Cioara, Cristina Bianca Pop, Ionut Anghel</dc:creator>
    </item>
    <item>
      <title>Introduction to Predictive Coding Networks for Machine Learning</title>
      <link>https://arxiv.org/abs/2506.06332</link>
      <description>arXiv:2506.06332v1 Announce Type: new 
Abstract: Predictive coding networks (PCNs) constitute a biologically inspired framework for understanding hierarchical computation in the brain, and offer an alternative to traditional feedforward neural networks in ML. This note serves as a quick, onboarding introduction to PCNs for machine learning practitioners. We cover the foundational network architecture, inference and learning update rules, and algorithmic implementation. A concrete image-classification task (CIFAR-10) is provided as a benchmark-smashing application, together with an accompanying Python notebook containing the PyTorch implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06332v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikko Stenlund</dc:creator>
    </item>
    <item>
      <title>CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms</title>
      <link>https://arxiv.org/abs/2506.06362</link>
      <description>arXiv:2506.06362v1 Announce Type: new 
Abstract: Bilevel optimization poses a significant computational challenge due to its nested structure, where each upper-level candidate solution requires solving a corresponding lower-level problem. While evolutionary algorithms (EAs) are effective at navigating such complex landscapes, their high resource demands remain a key bottleneck -- particularly the redundant evaluation of numerous unpromising lower-level tasks. Despite recent advances in multitasking and transfer learning, resource waste persists. To address this issue, we propose a novel resource allocation framework for bilevel EAs that selectively identifies and focuses on promising lower-level tasks. Central to our approach is a contrastive ranking network that learns relational patterns between paired upper- and lower-level solutions online. This knowledge guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on estimated population quality. Comprehensive experiments across five state-of-the-art bilevel algorithms show that our framework significantly reduces computational cost while preserving -- or even enhancing -- solution accuracy. This work offers a generalizable strategy to improve the efficiency of bilevel EAs, paving the way for more scalable bilevel optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06362v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dejun Xu, Jijia Chen, Gary G. Yen, Min Jiang</dc:creator>
    </item>
    <item>
      <title>Structured State Space Model Dynamics and Parametrization for Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2506.06374</link>
      <description>arXiv:2506.06374v1 Announce Type: new 
Abstract: Multi-state spiking neurons such as the adaptive leaky integrate-and-fire (AdLIF) neuron offer compelling alternatives to conventional deep learning models thanks to their sparse binary activations, second-order nonlinear recurrent dynamics, and efficient hardware realizations. However, such internal dynamics can cause instabilities during inference and training, often limiting performance and scalability. Meanwhile, state space models (SSMs) excel in long sequence processing using linear state-intrinsic recurrence resembling spiking neurons' subthreshold regime. Here, we establish a mathematical bridge between SSMs and second-order spiking neuron models. Based on structure and parametrization strategies of diagonal SSMs, we propose two novel spiking neuron models. The first extends the AdLIF neuron through timestep training and logarithmic reparametrization to facilitate training and improve final performance. The second additionally brings initialization and structure from complex-state SSMs, broadening the dynamical regime to oscillatory dynamics. Together, our two models achieve beyond or near state-of-the-art (SOTA) performances for reset-based spiking neuron models across both event-based and raw audio speech recognition datasets. We achieve this with a favorable number of parameters and required dynamic memory while maintaining high activity sparsity. Our models demonstrate enhanced scalability in network size and strike a favorable balance between performance and efficiency with respect to SSM models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06374v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxime Fabre, Lyubov Dudchenko, Emre Neftci</dc:creator>
    </item>
    <item>
      <title>Employing Discrete Fourier Transform in Representational Learning</title>
      <link>https://arxiv.org/abs/2506.06765</link>
      <description>arXiv:2506.06765v1 Announce Type: new 
Abstract: Image Representation learning via input reconstruction is a common technique in machine learning for generating representations that can be effectively utilized by arbitrary downstream tasks. A well-established approach is using autoencoders to extract latent representations at the network's compression point. These representations are valuable because they retain essential information necessary for reconstructing the original input from the compressed latent space. In this paper, we propose an alternative learning objective. Instead of using the raw input as the reconstruction target, we employ the Discrete Fourier Transform (DFT) of the input. The DFT provides meaningful global information at each frequency level, making individual frequency components useful as separate learning targets. When dealing with multidimensional input data, the DFT offers remarkable flexibility by enabling selective transformation across specific dimensions while preserving others in the computation. Moreover, certain types of input exhibit distinct patterns in their frequency distributions, where specific frequency components consistently contain most of the magnitude, allowing us to focus on a subset of frequencies rather than the entire spectrum. These characteristics position the DFT as a viable learning objective for representation learning and we validate our approach by achieving 52.8% top-1 accuracy on CIFAR-10 with ResNet-50 and outperforming the traditional autoencoder by 12.8 points under identical architectural configurations. Additionally, we demonstrate that training on only the lower-frequency components - those with the highest magnitudes yields results comparable to using the full frequency spectrum, with only minimal reductions in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06765v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raoof HojatJalali, Edmondo Trentin</dc:creator>
    </item>
    <item>
      <title>Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example</title>
      <link>https://arxiv.org/abs/2506.06904</link>
      <description>arXiv:2506.06904v1 Announce Type: new 
Abstract: Understanding how the brain learns may be informed by studying biologically plausible learning rules. These rules, often approximating gradient descent learning to respect biological constraints such as locality, must meet two critical criteria to be considered an appropriate brain model: (1) good neuroscience task performance and (2) alignment with neural recordings. While extensive research has assessed the first criterion, the second remains underexamined. Employing methods such as Procrustes analysis on well-known neuroscience datasets, this study demonstrates the existence of a biologically plausible learning rule -- namely e-prop, which is based on gradient truncation and has demonstrated versatility across a wide range of tasks -- that can achieve neural data similarity comparable to Backpropagation Through Time (BPTT) when matched for task accuracy. Our findings also reveal that model architecture and initial conditions can play a more significant role in determining neural similarity than the specific learning rule. Furthermore, we observe that BPTT-trained models and their biologically plausible counterparts exhibit similar dynamical properties at comparable accuracies. These results underscore the substantial progress made in developing biologically plausible learning rules, highlighting their potential to achieve both competitive task performance and neural data similarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06904v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhan Helena Liu, Guangyu Robert Yang, Christopher J. Cueva</dc:creator>
    </item>
    <item>
      <title>Research on Aerodynamic Performance Prediction of Airfoils Based on a Fusion Algorithm of Transformer and GAN</title>
      <link>https://arxiv.org/abs/2506.06979</link>
      <description>arXiv:2506.06979v1 Announce Type: new 
Abstract: Predicting of airfoil aerodynamic performance is a key part of aircraft design optimization, but the traditional methods (such as wind tunnel test and CFD simulation) have the problems of high cost and low efficiency, and the existing data-driven models face the challenges of insufficient accuracy and strong data dependence in multi-objective prediction. Therefore, this study proposes a deep learning model, Deeptrans, based on the fusion of improved Transformer and generative Adversarial network (GAN), which aims to predict the multi-parameter aerodynamic performance of airfoil efficiently. By constructing a large-scale data set and designing a model structure that integrates a Transformer coding-decoding framework and confrontation training, synchronous and high-precision prediction of aerodynamic parameters is realized. Experiments show that the MSE loss of Deeptrans on the verification set is reduced to 5.6*10-6, and the single-sample prediction time is only 0.0056 seconds, which is nearly 700 times more efficient than the traditional CFD method. Horizontal comparison shows that the prediction accuracy is significantly better than the original Transformer, GAN, and VAE models. This study provides an efficient data-driven solution for airfoil aerodynamic performance prediction and a new idea for deep learning modeling complex flow problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06979v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> MaolinYang, Yaohui Wang, Pingyu Jiang</dc:creator>
    </item>
    <item>
      <title>Transient Dynamics in Lattices of Differentiating Ring Oscillators</title>
      <link>https://arxiv.org/abs/2506.07253</link>
      <description>arXiv:2506.07253v1 Announce Type: new 
Abstract: Recurrent neural networks (RNNs) are machine learning models widely used for learning temporal relationships. Current state-of-the-art RNNs use integrating or spiking neurons -- two classes of computing units whose outputs depend directly on their internal states -- and accordingly there is a wealth of literature characterizing the behavior of large networks built from these neurons. On the other hand, past research on differentiating neurons, whose outputs are computed from the derivatives of their internal states, remains limited to small hand-designed networks with fewer than one-hundred neurons. Here we show via numerical simulation that large lattices of differentiating neuron rings exhibit local neural synchronization behavior found in the Kuramoto model of interacting oscillators. We begin by characterizing the periodic orbits of uncoupled rings, herein called ring oscillators. We then show the emergence of local correlations between oscillators that grow over time when these rings are coupled together into lattices. As the correlation length grows, transient dynamics arise in which large regions of the lattice settle to the same periodic orbit, and thin domain boundaries separate adjacent, out-of-phase regions. The steady-state scale of these correlated regions depends on how the neurons are shared between adjacent rings, which suggests that lattices of differentiating ring oscillator might be tuned to be used as reservoir computers. Coupled with their simple circuit design and potential for low-power consumption, differentiating neural nets therefore represent a promising substrate for neuromorphic computing that will enable low-power AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07253v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter DelMastro, Arjun Karuvally, Hananel Hazan, Hava Siegelmann, Edward Rietman</dc:creator>
    </item>
    <item>
      <title>Rapid training of Hamiltonian graph networks without gradient descent</title>
      <link>https://arxiv.org/abs/2506.06558</link>
      <description>arXiv:2506.06558v1 Announce Type: cross 
Abstract: Learning dynamical systems that respect physical symmetries and constraints remains a fundamental challenge in data-driven modeling. Integrating physical laws with graph neural networks facilitates principled modeling of complex N-body dynamics and yields accurate and permutation-invariant models. However, training graph neural networks with iterative, gradient-based optimization algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training, especially for large, complex systems. In comparison to 15 different optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained up to 600x faster--but with comparable accuracy--by replacing iterative optimization with random feature-based parameter construction. We show robust performance in diverse simulations, including N-body mass-spring systems in up to 3 dimensions with different geometries, while retaining essential physical invariances with respect to permutation, rotation, and translation. We reveal that even when trained on minimal 8-node systems, the model can generalize in a zero-shot manner to systems as large as 4096 nodes without retraining. Our work challenges the dominance of iterative gradient-descent-based optimization algorithms for training neural network models for physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06558v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atamert Rahma, Chinmay Datar, Ana Cukarska, Felix Dietrich</dc:creator>
    </item>
    <item>
      <title>Adam assisted Fully informed Particle Swarm Optimzation ( Adam-FIPSO ) based Parameter Prediction for the Quantum Approximate Optimization Algorithm (QAOA)</title>
      <link>https://arxiv.org/abs/2506.06790</link>
      <description>arXiv:2506.06790v1 Announce Type: cross 
Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent variational algorithm used for solving combinatorial optimization problems such as the Max-Cut problem. A key challenge in QAOA lies in efficiently identifying suitable parameters (gamma, beta) that lead to high-quality solutions. In this paper, we propose a framework that combines Fully Informed Particle Swarm Optimization (FIPSO) with adaptive gradient correction using the Adam Optimizer to navigate the QAOA parameter space. This approach aims to avoid issues such as barren plateaus and convergence to local minima. The proposed algorithm is evaluated against two classes of graph instances, Erdos Renyi and Watts-Strogatz. Experimental results across multiple QAOA depths consistently demonstrate superior performance compared to random initialization, underscoring the effectiveness and robustness of the proposed optimization framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06790v1</guid>
      <category>quant-ph</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Sanjay Bhat, Peiyong Wang, Udaya Parampalli</dc:creator>
    </item>
    <item>
      <title>ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors</title>
      <link>https://arxiv.org/abs/2506.06817</link>
      <description>arXiv:2506.06817v1 Announce Type: cross 
Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design parameters. However, standard BO does not support constraints involving categorical parameters such as types of branch predictors and division circuits. In addition, optimization time of BO grows with processor complexity, which becomes increasingly significant especially for FPGA-based soft processors. This paper introduces ASPO, an approach that leverages disjunctive form to enable BO to handle constraints involving categorical parameters. Unlike existing methods that directly apply standard BO, the proposed ASPO method, for the first time, customizes the mathematical mechanism of BO to address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO supports categorical parameters using a novel customized BO covariance kernel. It also accelerates the design evaluation procedure by penalizing the BO acquisition function with potential evaluation time and by reusing FPGA synthesis checkpoints from previously evaluated configurations. ASPO targets three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce execution time for the ``multiply'' benchmark on the BOOM processor by up to 35\% compared to the default configuration. Furthermore, it reduces design time for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art hardware-oriented BO approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06817v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.PF</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proc. Int. Conf. Field-Programmable Logic and Applications (FPL), 2025</arxiv:journal_reference>
      <dc:creator>Haoran Wu, Ce Guo, Wayne Luk, Robert Mullins</dc:creator>
    </item>
    <item>
      <title>Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures</title>
      <link>https://arxiv.org/abs/2506.06832</link>
      <description>arXiv:2506.06832v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06832v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement Hongler, Andrew Emil</dc:creator>
    </item>
    <item>
      <title>Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings</title>
      <link>https://arxiv.org/abs/2506.07109</link>
      <description>arXiv:2506.07109v1 Announce Type: cross 
Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a longstanding goal. However, unlike domains such as language or vision, where scaling structured data has driven generalization, progress in offline BBO remains hindered by the lack of unified representations for heterogeneous numerical spaces. Thus, existing offline BBO approaches are constrained to single-task and fixed-dimensional settings, failing to achieve cross-domain universal optimization. Recent advances in language models (LMs) offer a promising path forward: their embeddings capture latent relationships in a unifying way, enabling universal optimization across different data types possible. In this paper, we discuss multiple potential approaches, including an end-to-end learning framework in the form of next-token prediction, as well as prioritizing the learning of latent spaces with strong representational capabilities. To validate the effectiveness of these methods, we collect offline BBO tasks and data from open-source academic works for training. Experiments demonstrate the universality and effectiveness of our proposed methods. Our findings suggest that unifying language model priors and learning string embedding space can overcome traditional barriers in universal BBO, paving the way for general-purpose BBO algorithms. The code is provided at https://github.com/lamda-bbo/universal-offline-bbo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07109v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong-Xi Tan, Ming Chen, Ke Xue, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian</dc:creator>
    </item>
    <item>
      <title>Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models</title>
      <link>https://arxiv.org/abs/2506.07121</link>
      <description>arXiv:2506.07121v1 Announce Type: cross 
Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this framework, the diversity of adversarial prompts is essential for comprehensive safety assessments. We find that previous approaches to red-teaming may suffer from two key limitations. First, they often pursue diversity through simplistic metrics like word frequency or sentence embedding similarity, which may not capture meaningful variation in attack strategies. Second, the common practice of training a single attacker model restricts coverage across potential attack styles and risk categories. This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to address these limitations. QDRT achieves goal-driven diversity through behavior-conditioned training and implements a behavioral replay buffer in an open-ended manner. Additionally, it trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories. Our empirical evaluation demonstrates that QDRT generates attacks that are both more diverse and more effective against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the field of LLM safety by providing a systematic and effective approach to automated red-teaming, ultimately supporting the responsible deployment of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07121v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ren-Jian Wang, Ke Xue, Zeyu Qin, Ziniu Li, Sheng Tang, Hao-Tian Li, Shengcai Liu, Chao Qian</dc:creator>
    </item>
    <item>
      <title>Generative Voice Bursts during Phone Call</title>
      <link>https://arxiv.org/abs/2506.07526</link>
      <description>arXiv:2506.07526v1 Announce Type: cross 
Abstract: In critical situations, conventional mobile telephony fails to convey emergency voice messages to a callee already engaged in another call. The standard call waiting alert does not provide the urgency or content of the waiting call. This paper proposes a novel method for transmitting Generative Voice Bursts short, context aware audio messages during ongoing calls, from either preauthorized or dynamically prioritized callers. By leveraging generative AI techniques, the system automatically generates spoken messages from contextual inputs example like location, health data, images, background noise when the caller is unable to speak due to incapacitation or environmental constraints. The solution incorporates voice, text, and priority inference mechanisms, allowing high priority emergency messages to bypass conventional call waiting barriers. The approach employs models such as GPT Neo for generative text, which is synthesized into audio and delivered in configurable intervals G seconds and counts N times, ensuring minimal disruption while preserving urgency. This method holds potential for significant impact across telecom, mobile device manufacturing, and emergency communication platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07526v1</guid>
      <category>cs.SD</category>
      <category>cs.NE</category>
      <category>eess.AS</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paritosh Ranjan, Surajit Majumder, Prodip Roy</dc:creator>
    </item>
    <item>
      <title>REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models</title>
      <link>https://arxiv.org/abs/2506.07759</link>
      <description>arXiv:2506.07759v1 Announce Type: cross 
Abstract: Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07759v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diego Forni\'es-Tabuenca, Alejandro Uribe, Urtzi Otamendi, Arkaitz Artetxe, Juan Carlos Rivera, Oier Lopez de Lacalle</dc:creator>
    </item>
    <item>
      <title>Runtime Analysis of the SMS-EMOA for Many-Objective Optimization</title>
      <link>https://arxiv.org/abs/2312.10290</link>
      <description>arXiv:2312.10290v4 Announce Type: replace 
Abstract: This paper conducts the first rigorous runtime analysis of the SMS-EMOA for many-objective optimization. To this aim, we first propose a many-objective counterpart of the bi-objective OJZJ benchmark. We prove that SMS-EMOA computes the full Pareto front of this benchmark in an expected number of $O(\mu M n^k)$ iterations, where $n$ denotes the problem size (length of the bit-string representation), $k$ the gap size (a difficulty parameter of the problem), $M=(2n/m-2k+3)^{m/2}$ the size of the Pareto front, and $\mu$ the population size (at least the same size as the largest incomparable set). This result together with the existing negative result for the original NSGA-II shows that, in principle, the general approach of the NSGA-II is suitable for many-objective optimization, but the crowding distance as tie-breaker has deficiencies.
  We obtain three additional insights on the SMS-EMOA. Different from a recent result for the bi-objective \ojzj benchmark, a recently proposed stochastic population update often does not help for its many-objective counterpart. It at most results in a speed-up by a factor of order $2^{k} / \mu$, which is $\Theta(1)$ for large $m$, such as $m&gt;k$. On the positive side, we prove that heavy-tailed mutation irrespective of the number $m$ of objectives results in a speed-up of order $k^{0.5+k-\beta}/e^k$. Finally, we conduct the first runtime analyses of the SMS-EMOA on the classic OMM and LOTZ and show that the SMS-EMOA has a performance comparable to the GSEMO and the NSGA-II.
  Our main technical insight, a general condition ensuring that the SMS-EMOA does not lose Pareto-optimal objective values, promises to be useful also in other runtime analyses of this algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10290v4</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1609/AAAI.V38I18.30077</arxiv:DOI>
      <arxiv:journal_reference>Conference on Artificial Intelligence, AAAI 2024, pages 20874-20882</arxiv:journal_reference>
      <dc:creator>Weijie Zheng, Benjamin Doerr</dc:creator>
    </item>
    <item>
      <title>Learnable Activation Functions in Physics-Informed Neural Networks for Solving Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2411.15111</link>
      <description>arXiv:2411.15111v3 Announce Type: replace 
Abstract: We investigate learnable activation functions in Physics-Informed Neural Networks (PINNs) for solving Partial Differential Equations (PDEs): comparing traditional Multilayer Perceptrons (MLPs) with fixed and trainable activations against Kolmogorov-Arnold Networks (KANs) that employ learnable basis functions. While PINNs effectively incorporate physical laws into the learning process, they suffer from convergence and spectral bias problems, which limit their applicability to problems with rapid oscillations or sharp transitions. In this work, we study various activation and basis functions across diverse PDEs, including oscillatory, nonlinear wave, mixed-physics, and fluid dynamics problems. Using empirical Neural Tangent Kernel (NTK) analysis and Hessian eigenvalue decomposition, we assess convergence behavior, stability, and high-frequency approximation capacity. While KANs offer improved expressivity for capturing complex, high-frequency PDE solutions, they introduce new optimization challenges, especially in deeper networks. Our findings show that KANs face a curse of functional dimensionality, creating intractable optimization landscapes in deeper networks. Low spectral bias alone does not guarantee good performance; adaptive spectral bias approaches such as B-splines achieve optimal results by balancing global stability with local high-frequency resolution. Different PDE types require tailored strategies: smooth global activation functions excel for wave phenomena, while local adaptive activation functions suit problems with sharp transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15111v3</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afrah Farea, Mustafa Serdar Celebi</dc:creator>
    </item>
    <item>
      <title>Proven Approximation Guarantees in Multi-Objective Optimization: SPEA2 Beats NSGA-II</title>
      <link>https://arxiv.org/abs/2505.01323</link>
      <description>arXiv:2505.01323v2 Announce Type: replace 
Abstract: Together with the NSGA-II and SMS-EMOA, the strength Pareto evolutionary algorithm 2 (SPEA2) is one of the most prominent dominance-based multi-objective evolutionary algorithms (MOEAs). Different from the NSGA-II, it does not employ the crowding distance (essentially the distance to neighboring solutions) to compare pairwise non-dominating solutions but a complex system of $\sigma$-distances that builds on the distances to all other solutions. In this work, we give a first mathematical proof showing that this more complex system of distances can be superior. More specifically, we prove that a simple steady-state SPEA2 can compute optimal approximations of the Pareto front of the OneMinMax benchmark in polynomial time. The best proven guarantee for a comparable variant of the NSGA-II only assures approximation ratios of roughly a factor of two, and both mathematical analyses and experiments indicate that optimal approximations are not found efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01323v2</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasser Alghouass, Benjamin Doerr, Martin S. Krejca, Mohammed Lagmah</dc:creator>
    </item>
    <item>
      <title>Preprocessing Methods for Memristive Reservoir Computing for Image Recognition</title>
      <link>https://arxiv.org/abs/2506.05588</link>
      <description>arXiv:2506.05588v2 Announce Type: replace 
Abstract: Reservoir computing (RC) has attracted attention as an efficient recurrent neural network architecture due to its simplified training, requiring only its last perceptron readout layer to be trained. When implemented with memristors, RC systems benefit from their dynamic properties, which make them ideal for reservoir construction. However, achieving high performance in memristor-based RC remains challenging, as it critically depends on the input preprocessing method and reservoir size. Despite growing interest, a comprehensive evaluation that quantifies the impact of these factors is still lacking. This paper systematically compares various preprocessing methods for memristive RC systems, assessing their effects on accuracy and energy consumption. We also propose a parity-based preprocessing method that improves accuracy by 2-6% while requiring only a modest increase in device count compared to other methods. Our findings highlight the importance of informed preprocessing strategies to improve the efficiency and scalability of memristive RC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05588v2</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rishona Daniels, Duna Wattad, Ronny Ronen, David Saad, Shahar Kvatinsky</dc:creator>
    </item>
    <item>
      <title>R-FORCE: Robust Learning for Random Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2003.11660</link>
      <description>arXiv:2003.11660v2 Announce Type: replace-cross 
Abstract: Random Recurrent Neural Networks (RRNN) are the simplest recurrent networks to model and extract features from sequential data. The simplicity however comes with a price; RRNN are known to be susceptible to diminishing/exploding gradient problem when trained with gradient-descent based optimization. To enhance robustness of RRNN, alternative training approaches have been proposed. Specifically, FORCE learning approach proposed a recursive least squares alternative to train RRNN and was shown to be applicable even for the challenging task of target-learning, where the network is tasked with generating dynamic patterns with no guiding input. While FORCE training indicates that solving target-learning is possible, it appears to be effective only in a specific regime of network dynamics (edge-of-chaos). We thereby investigate whether initialization of RRNN connectivity according to a tailored distribution can guarantee robust FORCE learning. We are able to generate such distribution by inference of four generating principles constraining the spectrum of the network Jacobian to remain in stability region. This initialization along with FORCE learning provides a robust training method, i.e., Robust-FORCE (R-FORCE). We validate R-FORCE performance on various target functions for a wide range of network configurations and compare with alternative methods. Our experiments indicate that R-FORCE facilitates significantly more stable and accurate target-learning for a wide class of RRNN. Such stability becomes critical in modeling multi-dimensional sequences as we demonstrate on modeling time-series of human body joints during physical movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.11660v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Zheng, Eli Shlizerman</dc:creator>
    </item>
    <item>
      <title>Effective Regularization Through Loss-Function Metalearning</title>
      <link>https://arxiv.org/abs/2010.00788</link>
      <description>arXiv:2010.00788v4 Announce Type: replace-cross 
Abstract: Evolutionary computation can be used to optimize several different aspects of neural network architectures. For instance, the TaylorGLO method discovers novel, customized loss functions, resulting in improved performance, faster training, and improved data utilization. A likely reason is that such functions discourage overfitting, leading to effective regularization. This paper demonstrates theoretically that this is indeed the case for TaylorGLO. Learning rule decomposition reveals that evolved loss functions balance two factors: the pull toward zero error, and a push away from it to avoid overfitting. This is a general principle that may be used to understand other regularization techniques as well (as demonstrated in this paper for label smoothing). The theoretical analysis leads to a constraint that can be utilized to find more effective loss functions in practice; the mechanism also results in networks that are more robust (as demonstrated in this paper with adversarial inputs). The analysis in this paper thus constitutes a first step towards understanding regularization, and demonstrates the power of evolutionary neural architecture search in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.00788v4</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Congress on Evolutionary Computation (CEC), 2025</arxiv:journal_reference>
      <dc:creator>Santiago Gonzalez, Xin Qiu, Risto Miikkulainen</dc:creator>
    </item>
    <item>
      <title>Diversity from Human Feedback</title>
      <link>https://arxiv.org/abs/2310.06648</link>
      <description>arXiv:2310.06648v3 Announce Type: replace-cross 
Abstract: Diversity plays a significant role in many problems, such as ensemble learning, reinforcement learning, and combinatorial optimization. How to define the diversity measure is a longstanding problem. Many methods rely on expert experience to define a proper behavior space and then obtain the diversity measure, which is, however, challenging in many scenarios. In this paper, we propose the problem of learning a behavior space from human feedback and present a general method called Diversity from Human Feedback (DivHF) to solve it. DivHF learns a behavior descriptor consistent with human preference by querying human feedback. The learned behavior descriptor can be combined with any distance measure to define a diversity measure. We demonstrate the effectiveness of DivHF by integrating it with the Quality-Diversity optimization algorithm MAP-Elites and conducting experiments on the QDax suite. The results show that the behavior learned by DivHF is much more consistent with human requirements than the one learned by direct data-driven approaches without human feedback, and makes the final solutions more diverse under human preference. Our contributions include formulating the problem, proposing the DivHF method, and demonstrating its effectiveness through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06648v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ren-Jian Wang, Ke Xue, Yutong Wang, Peng Yang, Haobo Fu, Qiang Fu, Chao Qian</dc:creator>
    </item>
    <item>
      <title>Nonparametric Modern Hopfield Models</title>
      <link>https://arxiv.org/abs/2404.03900</link>
      <description>arXiv:2404.03900v2 Announce Type: replace-cross 
Abstract: We present a nonparametric interpretation for deep learning compatible modern Hopfield models and utilize this new perspective to debut efficient variants. Our key contribution stems from interpreting the memory storage and retrieval processes in modern Hopfield models as a nonparametric regression problem subject to a set of query-memory pairs. Interestingly, our framework not only recovers the known results from the original dense modern Hopfield model but also fills the void in the literature regarding efficient modern Hopfield models, by introducing \textit{sparse-structured} modern Hopfield models with sub-quadratic complexity. We establish that this sparse model inherits the appealing theoretical properties of its dense analogue -- connection with transformer attention, fixed point convergence and exponential memory capacity. Additionally, we showcase the versatility of our framework by constructing a family of modern Hopfield models as extensions, including linear, random masked, top-$K$ and positive random feature modern Hopfield models. Empirically, we validate our framework in both synthetic and realistic settings for memory retrieval and learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03900v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, Han Liu</dc:creator>
    </item>
    <item>
      <title>Counter-Inferential Behavior in Natural and Artificial Cognitive Systems</title>
      <link>https://arxiv.org/abs/2505.13551</link>
      <description>arXiv:2505.13551v2 Announce Type: replace-cross 
Abstract: This study explores the emergence of counter-inferential behavior in natural and artificial cognitive systems, that is, patterns in which agents misattribute empirical success or suppress adaptation, leading to epistemic rigidity or maladaptive stability. We analyze archetypal scenarios in which such behavior arises: reinforcement of stability through reward imbalance, meta-cognitive attribution of success to internal superiority, and protective reframing under perceived model fragility. Rather than arising from noise or flawed design, these behaviors emerge through structured interactions between internal information models, empirical feedback, and higher-order evaluation mechanisms. Drawing on evidence from artificial systems, biological cognition, human psychology, and social dynamics, we identify counter-inferential behavior as a general cognitive vulnerability that can manifest even in otherwise well-adapted systems. The findings highlight the importance of preserving minimal adaptive activation under stable conditions and suggest design principles for cognitive architectures that can resist rigidity under informational stress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13551v2</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>cs.SI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Serge Dolgikh</dc:creator>
    </item>
  </channel>
</rss>
