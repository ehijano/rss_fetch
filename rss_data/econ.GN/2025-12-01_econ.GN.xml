<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Dec 2025 05:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The dynamic of a tax on land value : concepts, models and impact scenario</title>
      <link>https://arxiv.org/abs/2511.21766</link>
      <description>arXiv:2511.21766v1 Announce Type: new 
Abstract: This paper develops a spatial-dynamic framework to analyze the theoretical and quantitative effects of a Land Value Tax (LVT) on urban land markets, capital accumulation, and spatial redistribution. Building upon the Georgist distinction between produced value and unearned rent, the model departs from the static equilibrium tradition by introducing an explicit diffusion process for land values and a local investment dynamic governed by profitability thresholds. Land value $V (x, y, t)$ and built capital $K(x, y, t)$evolve over a two-dimensional urban domain according to coupled nonlinear partial differential equations, incorporating local productivity $A(x, y)$, centrality effects $\mu(x, y)$, depreciation $\delta$, and fiscal pressure $\tau$ . Analytical characterization of the steady states reveals a transcritical bifurcation in the parameter $\tau$ , separating inactive (low-investment) and active (self-sustaining) spatial regimes. The equilibrium pair $(V ^*, K^*)$ is shown to exist only when the effective decay rate $\alpha = r + \tau - \mu(x, y)$ exceeds a profitability threshold $\theta = \kappa + \delta / I_0$, and becomes locally unstable beyond this boundary. The introduction of diffusion, $D_V \Delta V$, stabilizes spatial dynamics and generates continuous gradients of land value and capital intensity, mitigating speculative clustering while preserving productive incentives. Numerical simulations confirm these analytical properties and display the emergence of spatially heterogeneous steady states driven by urban centrality and local productivity. The model also quantifies key aggregate outcomes, including dynamic tax revenues, adjusted capital-to-land ratios, and net present values under spatial heterogeneity and temporal discounting. Sensitivity analyses demonstrate that the main qualitative mechanisms-critical activation, spatial recomposition, and bifurcation structure-remain robust under alternative spatial profiles $(A, \mu)$, discretization schemes, and moderate differentiation of the tax rate $\tau (x, y)$. From an economic perspective, the results clarify the dual nature of the LVT: while it erodes unproductive rents and speculative land holding, its dynamic incidence on built capital depends on local profitability and financing constraints. The taxation parameter $\tau$ thus acts as a control variable in a nonlinear spatial system, shaping transitions between rent-driven and production-driven equilibria. Within a critical range around $\tau_c$, the LVT functions as an efficient spatial reallocation operator-reducing inequality in land values and investment density without impairing aggregate productivity. Beyond this range, excessive taxation induces systemic contraction and investment stagnation. Overall, this research bridges static urban tax theory with dynamic spatial economics by formalizing how a land-based fiscal instrument can reshape the geography of value creation through endogenous diffusion and nonlinear feedback. The framework provides a foundation for future extensions involving stochastic shocks, adaptive policy feedbacks, or endogenous public investment, offering a unified quantitative perspective on the dynamic efficiency and spatial equity of land value taxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21766v1</guid>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Spring-Ragain (HEIP)</dc:creator>
    </item>
    <item>
      <title>A Unified Metric Architecture for AI Infrastructure: A Cross-Layer Taxonomy Integrating Performance, Efficiency, and Cost</title>
      <link>https://arxiv.org/abs/2511.21772</link>
      <description>arXiv:2511.21772v1 Announce Type: new 
Abstract: The growth of large-scale AI systems is increasingly constrained by infrastructure limits: power availability, thermal and water constraints, interconnect scaling, memory pressure, data-pipeline throughput, and rapidly escalating lifecycle cost. Across hyperscale clusters, these constraints interact, yet the main metrics remain fragmented. Existing metrics, ranging from facility measures (PUE) and rack power density to network metrics (all-reduce latency), data-pipeline measures, and financial metrics (TCO series), each capture only their own domain and provide no integrated view of how physical, computational, and economic constraints interact. This fragmentation obscures the structural relationships among energy, computation, and cost, preventing a coherent optimization across sector and how bottlenecks emerge, propagate, and jointly determine the efficiency frontier of AI infrastructure.
  This paper develops an integrated framework that unifies these disparate metrics through a three-domain semantic classification and a six-layer architectural decomposition, producing a 6x3 taxonomy that maps how various sectors propagate across the AI infrastructure stack. The taxonomy is grounded in a systematic review and meta-analysis of all metrics with economic and financial relevance, identifying the most widely used measures, their research intensity, and their cross-domain interdependencies. Building on this evidence base, the Metric Propagation Graph (MPG) formalizes cross-layer dependencies, enabling systemwide interpretation, composite-metric construction, and multi-objective optimization of energy, carbon, and cost.
  The framework offers a coherent foundation for benchmarking, cluster design, capacity planning, and lifecycle economic analysis by linking physical operations, computational efficiency, and cost outcomes within a unified analytic structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21772v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi He</dc:creator>
    </item>
    <item>
      <title>Invited to Develop: Institutional Belonging and the Counterfactual Architecture of Development</title>
      <link>https://arxiv.org/abs/2511.21865</link>
      <description>arXiv:2511.21865v1 Announce Type: new 
Abstract: This paper examines how institutional belonging shapes long-term development by comparing Spain and Uruguay, two small democracies with similar historical endowments whose trajectories diverged sharply after the 1960s. While Spain integrated into dense European institutional architectures, Uruguay remained embedded within the Latin American governance regime, characterized by weaker coordination and lower institutional coherence. To assess how alternative institutional embeddings could have altered these paths, the study develops a generative counterfactual framework grounded in economic complexity, institutional path dependence, and a Wasserstein GAN trained on data from 1960-2020. The resulting Expected Developmental Shift (EDS) quantifies structural gains or losses from hypothetical re-embedding in different institutional ecosystems. Counterfactual simulations indicate that Spain would have experienced significant developmental decline under a Latin American configuration, while Uruguay would have achieved higher complexity and resilience within a European regime. These findings suggest that development is not solely determined by domestic reforms but emerges from a country's structural position within transnational institutional networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21865v1</guid>
      <category>econ.GN</category>
      <category>cs.LG</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Vallarino</dc:creator>
    </item>
    <item>
      <title>Tradeable Import Certificates: A Promising Instrument to Support Domestic Production in Strategic Sectors?</title>
      <link>https://arxiv.org/abs/2511.22159</link>
      <description>arXiv:2511.22159v1 Announce Type: new 
Abstract: Recent crises have increased concerns about supply security in sectors that are considered strategically important. The goal of sufficient domestic production capacities has motivated various forms of subsidies, tariffs and other instruments. This paper revisits Warren Buffett's (2003) proposal of tradeable import certificates (TIC) in this context. TIC differ from classical import quotas mainly by linking the import volume to export performance. The certificate price functions like a mix of flexible tariffs and export subsidies whose levels depend on net imports in the strategic sector. We analyse benefits and drawbacks in a simple two-country model. In competitive markets, TIC constitute a transparent and efficient instrument that effectively reduces incentives for other countries to deviate from agreements via hidden subsidies or non-tariff trade barriers. However, TIC can have adverse effects if there are domestic producers with market power in the certificate market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22159v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Kranz</dc:creator>
    </item>
    <item>
      <title>Killed in and after Action: The Long-lasting Effects of Combat Exposure on Mortality</title>
      <link>https://arxiv.org/abs/2511.22776</link>
      <description>arXiv:2511.22776v1 Announce Type: new 
Abstract: This study examines long-term mortality effects of combat exposure using the Vietnam War draft lottery as a quasi-experiment. We validate the lottery by analyzing combat fatalities, revealing that 1951-1952 cohorts had notably fewer lottery-induced deployments than 1950, limiting detectable long-term mortality impacts at the cohort level. Using deceased-only datasets, we invert standard identification by modeling draft eligibility as the outcome. We find significant excess mortality among Black men in the 1950 cohort (1.09\%, approximately 2,700 additional deaths), and null effects in later cohorts. Findings suggest that pooling cohorts with limited combat exposure may prevent detection of true treatment effects at cohort levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22776v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helmut Farbmacher, Rebecca Groh</dc:creator>
    </item>
    <item>
      <title>Causes of Failure of the Phillips Curve: Does Tranquillity of Economic Environment Matter?</title>
      <link>https://arxiv.org/abs/2511.22785</link>
      <description>arXiv:2511.22785v1 Announce Type: new 
Abstract: Although empirical literature regarding the Phillips curve is sizeable enough, there is still no wide consensus on its validity and stability. The literature shows that the Phillips relationship is fragile and varies across countries and time periods; a statistical relationship that appears strong during one decade (country) may be weak the next (other). This variability might have some grounds for idiosyncrasy of a country and its economic environment. To address it, this paper scrutinizes the Phillips relationship over 41 countries over the period 1980-2016, paying attention to how inflation dynamics behave during tranquil and recessionary periods. As a result, the paper confirms the variability of the Phillips relationship across countries, as well as time periods. It documents that the relationship holds in the majority of developed countries, while it fails to hold in emerging and frontier economies during tranquil periods. On the other hand, the relationship totally collapses during recessionary periods, even in developed markets. This shows that tranquillity of economic environment is significantly important for the Phillip trade-off to work smoothly. Moreover, both backward- and forward-looking fractions of inflation remarkably increase during recessionary periods as a result of the Phillips coefficient loses its significance within the model. This indicates that markets become more inflation-sensitive during these periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22785v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5937/EJAE16-21569</arxiv:DOI>
      <arxiv:journal_reference>The European Journal of Applied Economics, 2019, 16(2), pp. 139-154</arxiv:journal_reference>
      <dc:creator>Yhlas Sovbetov, Muhittin Kaplan</dc:creator>
    </item>
    <item>
      <title>Empirical examination of the stability of Expectations-Augmented Phillips Curve for developing and developed countries</title>
      <link>https://arxiv.org/abs/2511.22786</link>
      <description>arXiv:2511.22786v1 Announce Type: new 
Abstract: The empirical literature provides mixed results on the relationship between inflation and unemployment, therefore, there is no consensus on validity and stability of the Phillips Curve. It also seems to be closely related with country-specific factors and the examination time periods. Considering the importance of this trade-off for policy-makers, this study aims to examine validity and stability of expectations-augmented Phillips Curve across 41 countries focusing on three different time periods between 1980 and 2016. The study documents several findings both in country-specific and in panel estimation analysis. First, we find that forward-looking characteristic of inflation picks up weight after 1990's which indicates that inflation became more sensitive to the expected prices. Second, we observe that inflation in developed markets is more forward-looking comparing to emerging and frontier markets. This indicates that developed markets dear forward-looking price expectations more than other markets. Third, we find that that both forward- and backward-looking Phillips Curve fails to work in Brazil, Greece, Indonesia, Mexico, South Africa, Romania, and Turkey. We address it to their long history of high and volatile inflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22786v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Theoretical and Applied Economics, 2019, 26 (2/619), pp. 63-78</arxiv:journal_reference>
      <dc:creator>Yhlas Sovbetov, Muhittin Kaplan</dc:creator>
    </item>
    <item>
      <title>Complexity Beyond Incentives: The Critical Role of Reporting Language</title>
      <link>https://arxiv.org/abs/2511.22834</link>
      <description>arXiv:2511.22834v1 Announce Type: new 
Abstract: Many assignment systems require applicants to rank multi-attribute bundles (e.g., programs combining institution, major, and tuition). We study whether this reporting task is inherently difficult and how reporting interfaces affect accuracy and welfare. In laboratory experiments, we induce preferences over programs via utility over attributes, generating lexicographic, separable, or complementary preferences. We compare three reporting interfaces for the direct serial dictatorship mechanism: (i) a full ranking over programs; (ii) a lexicographic-nesting interface; and (iii) a weighted-attributes interface, the latter two eliciting rankings over attributes rather than programs. We also study the sequential serial dictatorship mechanism that is obviously strategy-proof and simplifies reporting by asking for a single choice at each step. Finally, we run a baseline that elicits a full ranking over programs but rewards pure accuracy rather than allocation outcomes. Four main findings emerge. First, substantial misreporting occurs even in the pure-accuracy baseline and increases with preference complexity. Second, serial dictatorship induces additional mistakes consistent with misperceived incentives. Third, simplified interfaces for the direct serial dictatorship fail to improve (and sometimes reduce) accuracy, even when they match the preference structure. Fourth, sequential choice achieves the highest accuracy while improving efficiency and reducing justified envy. These findings caution against restricted reporting languages and favor sequential choice when ranking burdens are salient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22834v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rustamdjan Hakimov, Manshu Khanna</dc:creator>
    </item>
    <item>
      <title>Does cross-modal discounting generalize to non-WEIRD cultures? A comparison of the USA and Japan</title>
      <link>https://arxiv.org/abs/2511.23126</link>
      <description>arXiv:2511.23126v1 Announce Type: new 
Abstract: This paper examines how outcome modality in intertemporal choice influences time preferences and whether the process differs across cultures, specifically Japan and the United States. Uni-modal choices are those when the outcomes being compared over time are very similar, and cross-modal choices are those when the outcomes are very different. The cross-modal effect, previously shown in the U.S., is that there is greater patience in cross-modal decisions. In Experiment 1, we employed a between-participants design, in which participants either made uni-modal or cross-modal decisions. In Experiment 2, we employed a within-participants design in which everyone made both types of decision. In both Experiments we replicated the cross-modal effect. Moreover, the magnitude of the effect did not vary with factors known to relate to time preference, such as cognitive ability and social status, and it did not differ across cultures, even though Japanese participants were much more patient than American ones. The effect was stronger in the between- than within-participants experiment. These results strengthen the conclusion that the cross-modal effect is universal and strengthens the argument that it is due to the fundamental process of attentional dilution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23126v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shohei Yamamoto, Rebecca McDonald, Daniel Read</dc:creator>
    </item>
    <item>
      <title>Too Big to Monitor? Network Scale and the Breakdown of Decentralized Monitoring</title>
      <link>https://arxiv.org/abs/2511.23320</link>
      <description>arXiv:2511.23320v1 Announce Type: new 
Abstract: Many public services are produced in networked systems where quality depends on local effort and on how higher-level authorities monitor providers. We develop a simple model in which monitoring is a public good on a network with strategic complementarities. A regulator chooses between decentralized monitoring (cheaper, local oversight) and centralized monitoring (more costly, but internalizing spillovers). The model delivers an endogenous centralization threshold: for a given spillover strength, there exists a network size $n^\ast(\lambda)$ above which centralized monitoring strictly dominates; equivalently, for a given network size $n$, there is a critical complementarity $\lambda^\ast(n)$ beyond which decentralized oversight becomes fragile. A stochastic extension suggests that, above this region, idiosyncratic shocks are amplified, producing stronger peer correlations, higher variance, and more frequent deterioration in quality. We test these predictions in the U.S. nursing home sector, where facilities belong to overlapping organizational (chain) and geographic (county) networks. Using CMS facility data, We document strong within-chain and within-county peer effects and estimate network-size thresholds for severe regulatory failure (Special Focus Facility designations). We find sharp breakpoints at roughly 7 homes per county and 34 homes per chain, above which spillovers intensify and deficiency outcomes become more dispersed and prone to deterioration, especially in large counties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23320v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Tchuente</dc:creator>
    </item>
    <item>
      <title>Suppression and Deterrence: Revisiting the Welfare Consequence of HIV Stigma</title>
      <link>https://arxiv.org/abs/2511.23328</link>
      <description>arXiv:2511.23328v1 Announce Type: new 
Abstract: The government's effort to alleviate HIV stigma has been justified by the suppression effect of stigma on the HIV testing rate. Nevertheless, the deterrence effect of stigma on undesirable sexual behaviours has long been overlooked. This study adapts the existing framework on HIV stigma with an additional stage that formally models people's choices on whether to take preventive measures in sex. The model shows that, when sex is explicitly modelled, the suppression and deterrence effects coexist, which makes the net societal impact of HIV stigma ambiguous. A utilitarian welfare analysis concludes that the welfare-maximizing stigma level can be higher than its natural level, implying that the government's effort to reduce stigma is not always welfare-improving. Instead, the study provides a rationale for maintaining a certain level of HIV stigma to maximize social welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23328v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pengyu Li</dc:creator>
    </item>
    <item>
      <title>La ley del descenso tendencial de la tasa de ganancia: Evidencia emp\'irica para la econom\'ia espa\~nola</title>
      <link>https://arxiv.org/abs/2511.23427</link>
      <description>arXiv:2511.23427v1 Announce Type: new 
Abstract: This article examines the law of the tendency of the rate of profit to fall in the Spanish economy between 1960 and 2024, considering the organic composition of capital and the rate of surplus value as central variables. Its aim is to determine whether this law, formulated by Marx in Capital (Vol. III), continues to operate in the contemporary context. The methodology consists of transforming orthodox macroeconomic categories derived from the Spanish National Accounts (CNE), available in BDMACRO, into Marxist variables: constant capital ($c$), variable capital ($v$), and surplus value ($pv$). Based on these, historical series of the organic composition of capital ($q$), the rate of surplus value ($pv'$), and the rate of profit ($g'$) are constructed, adjusted to constant prices to ensure temporal coherence and comparability. The results show a sustained increase in $q$ and a slight decrease in $pv'$, generating a tendential decline in $g'$ with cyclical fluctuations associated with specific crises. The conclusions empirically confirm the validity of the law in Spain, highlighting the historical limits of capitalism and providing quantitative evidence on the structural dynamics of profitability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23427v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iv\'an L\'opez-Espejo</dc:creator>
    </item>
    <item>
      <title>Tacit Bidder-Side Collusion: Artificial Intelligence in Dynamic Auctions</title>
      <link>https://arxiv.org/abs/2511.21802</link>
      <description>arXiv:2511.21802v1 Announce Type: cross 
Abstract: We study whether large language models acting as autonomous bidders can tacitly collude by coordinating when to accept platform posted payouts in repeated Dutch auctions, without any communication. We present a minimal repeated auction model that yields a simple incentive compatibility condition and a closed form threshold for sustainable collusion for subgame-perfect Nash equilibria. In controlled simulations with multiple language models, we observe systematic supra-competitive prices in small auction settings and a return to competitive behavior as the number of bidders in the market increases, consistent with the theoretical model. We also find LLMs use various mechanisms to facilitate tacit coordination, such as focal point acceptance timing versus patient strategies that track the theoretical incentives. The results provide, to our knowledge, the first evidence of bidder side tacit collusion by LLMs and show that market structure levers can be more effective than capability limits for mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21802v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sriram Tolety</dc:creator>
    </item>
    <item>
      <title>DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks</title>
      <link>https://arxiv.org/abs/2511.22314</link>
      <description>arXiv:2511.22314v1 Announce Type: cross 
Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22314v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Wu, Kejiang Qian, Alexis Lui, Christopher Jack, Yue Wu, Peter McBurney, Fengxiang He, Bryan Zhang</dc:creator>
    </item>
    <item>
      <title>Can industrial overcapacity enable seasonal flexibility in electricity use? A case study of aluminum smelting in China</title>
      <link>https://arxiv.org/abs/2511.22839</link>
      <description>arXiv:2511.22839v1 Announce Type: cross 
Abstract: In many countries, declining demand in energy-intensive industries (EIIs) such as cement, steel, and aluminum is leading to industrial overcapacity. Although overcapacity is traditionally seen as problematic, it could unlock EIIs' flexibility in electricity use. Using China's aluminum smelting sector as a case, we evaluate the system-level cost-benefit of retaining EII overcapacity for flexible electricity use in decarbonized systems. We find that overcapacity enables smelters to adopt a seasonal operation paradigm, ceasing production during winter load peaks driven by heating electrification and renewable seasonality. In a 2050-net-zero scenario, this paradigm reduces China's electricity-system investment and operating costs by 15-72 billion CNY per year (8-34% of the industry's product value), enough to offset the costs of maintaining overcapacity and product storage. Seasonal operation also cuts workforce fluctuations across aluminum smelting and thermal-power sectors by up to 62%, potentially mitigating socio-economic disruptions from industrial restructuring and the energy transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22839v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruike Lyu, Anna Li, Jianxiao Wang, Hongxi Luo, Yan Shen, Hongye Guo, Ershun Du, Chongqing Kang, Jesse Jenkins</dc:creator>
    </item>
    <item>
      <title>Standard Occupation Classifier -- A Natural Language Processing Approach</title>
      <link>https://arxiv.org/abs/2511.23057</link>
      <description>arXiv:2511.23057v1 Announce Type: cross 
Abstract: Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23057v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sidharth Rony, Jack Patman</dc:creator>
    </item>
    <item>
      <title>Do Unions Shape Political Ideologies at Work?</title>
      <link>https://arxiv.org/abs/2209.02637</link>
      <description>arXiv:2209.02637v5 Announce Type: replace 
Abstract: Labor unions influence economic outcomes not only through bargaining with employers over work contracts but also via political activities that can profoundly shape political systems. In unionized workplaces, they may mobilize and change the ideological positions of both unionizing workers and their non-unionizing management. In this paper, we analyze the workplace-level impact of unionization on workers' and managers' political campaign contributions. We link establishment-level union election data with transaction-level campaign contributions to federal and local candidates in the United States. Using a difference-in-differences design, validated through regression discontinuity tests and a novel instrumental variable approach, we find that unionization leads to a leftward shift of campaign contributions. Unionization increases support for Democrats relative to Republicans not only among workers but especially among managers, suggesting that managers converge toward workers' political preferences. The effects are stronger in settings with more cooperative union-employer interactions, such as when union elections are not contested by an unfair labor practice charge and result in a collective bargaining agreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.02637v5</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Matzat, Aiko Schmei{\ss}er</dc:creator>
    </item>
    <item>
      <title>Public sentiments on the fourth industrial revolution: An unsolicited public opinion poll from Twitter</title>
      <link>https://arxiv.org/abs/2411.14230</link>
      <description>arXiv:2411.14230v2 Announce Type: replace 
Abstract: This paper establishes an empirical baseline of public sentiment toward Fourth Industrial Revolution (4IR) technologies across six European countries during the period 2006--2019, prior to the widespread adoption of generative AI systems. Employing transformer-based natural language processing models on a corpus of approximately 90,000 tweets and news articles, I document a European public sphere increasingly divided in its assessment of technological change: neutral sentiment declined markedly over the study period as citizens sorted into camps of enthusiasm and concern, a pattern that manifests distinctively across national contexts and technology domains. Approximately 6\% of users inhabit echo chambers characterized by sentiment-aligned networks, with privacy discourse exhibiting the highest susceptibility to such dynamics. These findings provide a methodologically rigorous reference point for evaluating how the introduction of ChatGPT and subsequent generative AI systems has transformed public discourse on automation, employment, and technological change. The results carry implications for policymakers seeking to align technological governance with societal values in an era of rapid AI advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14230v2</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diletta Abbonato</dc:creator>
    </item>
    <item>
      <title>Training for Obsolescence? The AI-Driven Education Trap</title>
      <link>https://arxiv.org/abs/2508.19625</link>
      <description>arXiv:2508.19625v2 Announce Type: replace 
Abstract: Artificial intelligence is simultaneously transforming the production function of human capital in schools and the return to skills in the labor market. We develop a theoretical model to analyze the potential for misallocation when these two forces are considered in isolation. We study an educational planner who observes AI's immediate productivity benefits in teaching specific skills but fails to fully internalize the technology's future wage-suppressing effects on those same skills. Motivated by a pre-registered pilot study suggesting a positive correlation between a skill's "teachability" by AI and its vulnerability to automation, we show that this information friction leads to a systematic skill mismatch. The planner over-invests in skills destined for obsolescence, a distortion that increases monotonically with AI prevalence. Extensions demonstrate that this mismatch is exacerbated by the neglect of unpriced non-cognitive skills and by the endogenous over-adoption of educational technology. Our findings caution that policies promoting AI in education, if not paired with forward-looking labor market signals, may paradoxically undermine students' long-term human capital, such as by crowding out skills like persistence that are forged through intellectual struggle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19625v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew J. Peterson</dc:creator>
    </item>
    <item>
      <title>Big Wins, Small Net Gains: Direct and Spillover Effects of First Industry Entries in Puerto Rico</title>
      <link>https://arxiv.org/abs/2511.19469</link>
      <description>arXiv:2511.19469v2 Announce Type: replace 
Abstract: I study how first sizable industry entries reshape local and neighboring labor markets in Puerto Rico. Using over a decade of quarterly municipality--industry data (2014Q1--2025Q1), I identify ``first sizable entries'' as large, persistent jumps in establishments, covered employment, and wage bill, and treat these as shocks to local industry presence at the municipio--industry level. Methodologically, I combine staggered-adoption difference-in-differences estimators that are robust to heterogeneous treatment timing with an imputation-based event-study approach, and I use a doubly robust difference-in-differences framework that explicitly allows for interference through pre-specified exposure mappings on a contiguity graph. The estimates show large and persistent direct gains in covered employment and wage bill in the treated municipality--industry cells over 0--16 quarters. Same-industry neighbors experience sizable short-run gains that reverse over the medium run, while within-municipality cross-industry and neighbor all-industries spillovers are small and imprecisely estimated. Once these spillovers are taken into account and spatially robust inference and sensitivity checks are applied, the net regional 0--16 quarter effect on covered employment is positive but modest in magnitude and estimated with considerable uncertainty. The results imply that first sizable entries generate substantial local gains where they occur, but much smaller and less precisely measured net employment gains for the broader regional economy, highlighting the importance of accounting for spatial spillovers when evaluating place-based policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19469v2</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge A. Arroyo</dc:creator>
    </item>
    <item>
      <title>Randomisation with moral hazard: a path to existence of optimal contracts</title>
      <link>https://arxiv.org/abs/2311.13278</link>
      <description>arXiv:2311.13278v2 Announce Type: replace-cross 
Abstract: We study a generic principal-agent problem in continuous time on a finite time horizon. We introduce a framework in which the agent is allowed to employ measure-valued controls and characterise the continuation utility as a solution to a specific form of a backward stochastic differential equation driven by a martingale measure. We leverage this characterisation to prove that, under appropriate conditions, an optimal solution to the principal's problem exists, even when constraints on the contract are imposed. In doing so, we employ compactification techniques and, as a result, circumvent the typical challenge of showing well-posedness for a degenerate partial differential equation with potential boundary conditions, where regularity problems often arise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13278v2</guid>
      <category>math.PR</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Kr\v{s}ek, Dylan Possama\"i</dc:creator>
    </item>
  </channel>
</rss>
