<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>It's Hard to HAC with Average Linkage!</title>
      <link>https://arxiv.org/abs/2404.14730</link>
      <description>arXiv:2404.14730v1 Announce Type: new 
Abstract: Average linkage Hierarchical Agglomerative Clustering (HAC) is an extensively studied and applied method for hierarchical clustering. Recent applications to massive datasets have driven significant interest in near-linear-time and efficient parallel algorithms for average linkage HAC.
  We provide hardness results that rule out such algorithms. On the sequential side, we establish a runtime lower bound of $n^{3/2-\epsilon}$ on $n$ node graphs for sequential combinatorial algorithms under standard fine-grained complexity assumptions. This essentially matches the best-known running time for average linkage HAC. On the parallel side, we prove that average linkage HAC likely cannot be parallelized even on simple graphs by showing that it is CC-hard on trees of diameter $4$. On the possibility side, we demonstrate that average linkage HAC can be efficiently parallelized (i.e., it is in NC) on paths and can be solved in near-linear time when the height of the output cluster hierarchy is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14730v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MohammadHossein Bateni, Laxman Dhulipala, Kishen N Gowda, D Ellis Hershkowitz, Rajesh Jayaram, Jakub {\L}\k{a}cki</dc:creator>
    </item>
    <item>
      <title>On the Number of Steps of CyclePopping in Weakly Inconsistent U(1)-Connection Graphs</title>
      <link>https://arxiv.org/abs/2404.14803</link>
      <description>arXiv:2404.14803v1 Announce Type: new 
Abstract: A U(1)-connection graph $G$ is a graph in which each oriented edge is endowed with a unit complex number, the latter being conjugated under orientation flip. We consider cycle-rooted spanning forests (CRSFs), a particular kind of spanning subgraphs of $G$ that have recently found computational applications as randomized spectral sparsifiers. In this context, CRSFs are drawn from a determinantal measure. Under a condition on the connection, Kassel and Kenyon gave an elegant algorithm, named CyclePopping, to sample from this distribution. The algorithm is an extension of the celebrated algorithm of Wilson that uses a loop-erased random walk to sample uniform spanning trees. In this paper, we give an alternative, elementary proof of correctness of CyclePopping for CRSF sampling; we fill the gaps of a proof sketch by Kassel, who was himself inspired by Marchal's proof of the correctness of Wilson's original algorithm. One benefit of the full proof \`a la Marchal is that we obtain a concise expression for the law of the number of steps to complete the sampling procedure, shedding light on practical situations where the algorithm is expected to run fast. Furthermore, we show how to extend the proof to more general distributions over CRSFs, which are not determinantal. The correctness of CyclePopping is known even in the non-determinantal case from the work of Kassel and Kenyon, so our merit is only to provide an alternate proof. One interest of this alternate proof is again to provide the distribution of the time complexity of the algorithm, in terms of a Poisson point process on the graph loops, or equivalently as a Poisson process on pyramids of cycles, a combinatorial notion introduced by Viennot. Finally, we strive to make the connections to loop measures and combinatorial structures as explicit as possible, to provide a reference for future extensions of the algorithm and its analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14803v1</guid>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha\"el Fanuel, R\'emi Bardenet</dc:creator>
    </item>
    <item>
      <title>Parameterized Maximum Node-Disjoint Paths</title>
      <link>https://arxiv.org/abs/2404.14849</link>
      <description>arXiv:2404.14849v1 Announce Type: new 
Abstract: We revisit the Maximum Node-Disjoint Paths problem, the natural optimization version of Node-Disjoint Paths, where we are given a graph $G$, $k$ pairs of vertices $(s_i, t_i)$ and an integer $\ell$, and are asked whether there exist at least $\ell$ vertex-disjoint paths in $G$ whose endpoints are given pairs. We present several results, with an emphasis towards FPT approximation.
  Our main positive contribution is to show that the problem's intractability can be overcome using approximation and that for several of the structural parameters for which the problem is hard, most notably tree-depth, it admits an efficient FPT approximation scheme, returning a $(1-\varepsilon)$-approximate solution in time $f(td,\varepsilon)n^{O(1)}$. We manage to obtain these results by comprehensively mapping out the structural parameters for which the problem is FPT if $\ell$ is also a parameter, hence showing that understanding $\ell$ as a parameter is key to the problem's approximability. This, in turn, is a problem we are able to solve via a surprisingly simple color-coding algorithm, which relies on identifying an insightful problem-specific variant of the natural parameter, namely the number of vertices used in the solution.
  A natural question is whether the FPT approximation algorithm we devised for tree-depth can be extended to pathwidth. We resolve this negatively, showing that under the Parameterized Inapproximability Hypothesis no FPT approximation scheme for this parameter is possible, even in time $f(pw,\varepsilon)n^{g(\varepsilon)}$, thus precisely determining the parameter border where the problem transitions from ``hard but approximable'' to ``inapproximable''.
  Lastly, we strengthen existing lower bounds by replacing W[1]-hardness by XNLP-completeness for parameter pathwidth, and improving the $n^{o(\sqrt{td})}$ ETH-based lower bound for tree-depth to $n^{o(td)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14849v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis, Manolis Vasilakis</dc:creator>
    </item>
    <item>
      <title>An inexact augmented Lagrangian algorithm for unsymmetric saddle-point systems</title>
      <link>https://arxiv.org/abs/2404.14636</link>
      <description>arXiv:2404.14636v1 Announce Type: cross 
Abstract: Augmented Lagrangian (AL) methods are a well known class of algorithms for solving constrained optimization problems. They have been extended to the solution of saddle-point systems of linear equations. We study an AL (SPAL) algorithm for unsymmetric saddle-point systems and derive convergence and semi-convergence properties, even when the system is singular. At each step, our SPAL requires the exact solution of a linear system of the same size but with an SPD (2,2) block. To improve efficiency, we introduce an inexact SPAL algorithm. We establish its convergence properties under reasonable assumptions. Specifically, we use a gradient method, known as the Barzilai-Borwein (BB) method, to solve the linear system at each iteration. We call the result the augmented Lagrangian BB (SPALBB) algorithm and study its convergence. Numerical experiments on test problems from Navier-Stokes equations and coupled Stokes-Darcy flow show that SPALBB is more robust and efficient than BICGSTAB and GMRES. SPALBB often requires the least CPU time, especially on large systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14636v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.17308.09602</arxiv:DOI>
      <dc:creator>N. Huang, Y. -H. Dai, D. Orban, M. A. Saunders</dc:creator>
    </item>
    <item>
      <title>On the sizes of BDDs and ZDDs representing matroids</title>
      <link>https://arxiv.org/abs/2404.14670</link>
      <description>arXiv:2404.14670v1 Announce Type: cross 
Abstract: Matroids are often represented as oracles since there are no unified and compact representations for general matroids. This paper initiates the study of binary decision diagrams (BDDs) and zero-suppressed binary decision diagrams (ZDDs) as relatively compact data structures for representing matroids in a computer. This study particularly focuses on the sizes of BDDs and ZDDs representing matroids. First, we compare the sizes of different variations of BDDs and ZDDs for a matroid. These comparisons involve concise transformations between specific decision diagrams. Second, we provide upper bounds on the size of BDDs and ZDDs for several classes of matroids. These bounds are closely related to the number of minors of the matroid and depend only on the connectivity function or pathwidth of the matroid, which deeply relates to the classes of matroids called strongly pigeonhole classes. In essence, these results indicate upper bounds on the number of minors for specific classes of matroids and new strongly pigeonhole classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14670v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiromi Emoto, Yuni Iwamasa, Shin-ichi Minato</dc:creator>
    </item>
    <item>
      <title>$\alpha_i$-Metric Graphs: Hyperbolicity</title>
      <link>https://arxiv.org/abs/2404.14792</link>
      <description>arXiv:2404.14792v1 Announce Type: cross 
Abstract: A graph is called $\alpha_i$-metric ($i \in {\cal N}$) if it satisfies the following $\alpha_i$-metric property for every vertices $u, w, v$ and $x$: if a shortest path between $u$ and $w$ and a shortest path between $x$ and $v$ share a terminal edge $vw$, then $d(u,x) \ge d(u,v) + d(v,x) - i$. The latter is a discrete relaxation of the property that in Euclidean spaces the union of two geodesics sharing a terminal segment must be also a geodesic. Recently in (Dragan &amp; Ducoffe, WG'23) we initiated the study of the algorithmic applications of $\alpha_i$-metric graphs. Our results in this prior work were very similar to those established in (Chepoi et al., SoCG'08) and (Chepoi et al., COCOA'18) for graphs with bounded hyperbolicity. The latter is a heavily studied metric tree-likeness parameter first introduced by Gromov. In this paper, we clarify the relationship between hyperbolicity and the $\alpha_i$-metric property, proving that $\alpha_i$-metric graphs are $f(i)$-hyperbolic for some function $f$ linear in $i$. We give different proofs of this result, using various equivalent definitions to graph hyperbolicity. By contrast, we give simple constructions of $1$-hyperbolic graphs that are not $\alpha_i$-metric for any constant $i$. Finally, in the special case of $i=1$, we prove that $\alpha_1$-metric graphs are $1$-hyperbolic, and the bound is sharp. By doing so, we can answer some questions left open in (Dragan &amp; Ducoffe, WG'23).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14792v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feodor F. Dragan, Guillaume Ducoffe</dc:creator>
    </item>
    <item>
      <title>Near-Universally-Optimal Differentially Private Minimum Spanning Trees</title>
      <link>https://arxiv.org/abs/2404.15035</link>
      <description>arXiv:2404.15035v1 Announce Type: cross 
Abstract: Devising mechanisms with good beyond-worst-case input-dependent performance has been an important focus of differential privacy, with techniques such as smooth sensitivity, propose-test-release, or inverse sensitivity mechanism being developed to achieve this goal. This makes it very natural to use the notion of universal optimality in differential privacy. Universal optimality is a strong instance-specific optimality guarantee for problems on weighted graphs, which roughly states that for any fixed underlying (unweighted) graph, the algorithm is optimal in the worst-case sense, with respect to the possible setting of the edge weights.
  In this paper, we give the first such result in differential privacy. Namely, we prove that a simple differentially private mechanism for approximately releasing the minimum spanning tree is near-optimal in the sense of universal optimality for the $\ell_1$ neighbor relation. Previously, it was only known that this mechanism is nearly optimal in the worst case. We then focus on the $\ell_\infty$ neighbor relation, for which the described mechanism is not optimal. We show that one may implement the exponential mechanism for MST in polynomial time, and that this results in universal near-optimality for both the $\ell_1$ and the $\ell_\infty$ neighbor relations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15035v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Hlad\'ik, Jakub T\v{e}tek</dc:creator>
    </item>
    <item>
      <title>Showcasing straight-line programs with memory via matrix Bruhat decomposition</title>
      <link>https://arxiv.org/abs/1305.5617</link>
      <description>arXiv:1305.5617v2 Announce Type: replace 
Abstract: We suggest that straight-line programs designed for algebraic computations should be accompanied by a comprehensive complexity analysis that takes into account both the number of fundamental algebraic operations needed, as well as memory requirements arising during evaluation. We introduce an approach for formalising this idea and, as illustration, construct and analyse straight-line programs for the Bruhat decomposition of $d\times d$ matrices with determinant $1$ over a finite field of order $q$ that have length $O(d^2\log(q))$ and require storing only $O(\log(q))$ matrices during evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:1305.5617v2</guid>
      <category>cs.DS</category>
      <category>math.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice C. Niemeyer, Tomasz Popiel, Cheryl E. Praeger, Daniel Rademacher</dc:creator>
    </item>
    <item>
      <title>Computing Tree Decompositions with Small Independence Number</title>
      <link>https://arxiv.org/abs/2207.09993</link>
      <description>arXiv:2207.09993v2 Announce Type: replace 
Abstract: The independence number of a tree decomposition is the maximum of the independence numbers of the subgraphs induced by its bags. The tree-independence number of a graph is the minimum independence number of a tree decomposition of it. Several NP-hard graph problems, like maximum weight independent set, can be solved in time n^{O(k)} if the input n-vertex graph is given together with a tree decomposition of independence number k. Yolov in [SODA 2018] gave an algorithm that given an n-vertex graph G and an integer k, in time n^{O(k^3)} either constructs a tree decomposition of G whose independence number is O(k^3) or correctly reports that the tree-independence number of G is larger than k.
  In this paper, we first give an algorithm for computing the tree-independence number with a better approximation ratio and running time and then prove that our algorithm is, in some sense, the best one can hope for. More precisely, our algorithm runs in time 2^{O(k^2)} n^{O(k)} and either outputs a tree decomposition of G with independence number at most 8k, or determines that the tree-independence number of G is larger than k. This implies 2^{O(k^2)} n^{O(k)}-time algorithms for various problems, like maximum weight independent set, parameterized by the tree-independence number k without needing the decomposition as an input. Assuming Gap-ETH, an n^{\Omega(k)} factor in the running time is unavoidable for any approximation algorithm for the tree-independence number.
  Our second result is that the exact computation of the tree-independence number is para-NP-hard: We show that for every constant k \ge 4 it is NP-hard to decide if a given graph has the tree-independence number at most k.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.09993v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement Dallard, Fedor V. Fomin, Petr A. Golovach, Tuukka Korhonen, Martin Milani\v{c}</dc:creator>
    </item>
    <item>
      <title>On the cut-query complexity of approximating max-cut</title>
      <link>https://arxiv.org/abs/2211.04506</link>
      <description>arXiv:2211.04506v3 Announce Type: replace 
Abstract: We consider the problem of query-efficient global max-cut on a weighted undirected graph in the value oracle model examined by [RSW18]. Graph algorithms in this cut query model and other query models have recently been studied for various other problems such as min-cut, connectivity, bipartiteness, and triangle detection. Max-cut in the cut query model can also be viewed as a natural special case of submodular function maximization: on query $S \subseteq V$, the oracle returns the total weight of the cut between $S$ and $V \backslash S$.
  Our first main technical result is a lower bound stating that a deterministic algorithm achieving a $c$-approximation for any $c &gt; 1/2$ requires $\Omega(n)$ queries. This uses an extension of the cut dimension to rule out approximation (prior work of [GPRW20] introducing the cut dimension only rules out exact solutions). Secondly, we provide a randomized algorithm with $\tilde{O}(n)$ queries that finds a $c$-approximation for any $c &lt; 1$. We achieve this using a query-efficient sparsifier for undirected weighted graphs (prior work of [RSW18] holds only for unweighted graphs).
  To complement these results, for most constants $c \in (0,1]$, we nail down the query complexity of achieving a $c$-approximation, for both deterministic and randomized algorithms (up to logarithmic factors). Analogously to general submodular function maximization in the same model, we observe a phase transition at $c = 1/2$: we design a deterministic algorithm for global $c$-approximate max-cut in $O(\log n)$ queries for any $c &lt; 1/2$, and show that any randomized algorithm requires $\Omega(n/\log n)$ queries to find a $c$-approximate max-cut for any $c &gt; 1/2$. Additionally, we show that any deterministic algorithm requires $\Omega(n^2)$ queries to find an exact max-cut (enough to learn the entire graph).</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04506v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orestis Plevrakis, Seyoon Ragavan, S. Matthew Weinberg</dc:creator>
    </item>
    <item>
      <title>Evaluating Regular Path Queries on Compressed Adjacency Matrices</title>
      <link>https://arxiv.org/abs/2307.14930</link>
      <description>arXiv:2307.14930v2 Announce Type: replace 
Abstract: Regular Path Queries (RPQs), which are essentially regular expressions to be matched against the labels of paths in labeled graphs, are at the core of graph database query languages like SPARQL. A way to solve RPQs is to translate them into a sequence of operations on the adjacency matrices of each label. We design and implement a Boolean algebra on sparse matrix representations and, as an application, use them to handle RPQs. Our baseline representation uses the same space as the previously most compact index for RPQs and outperforms it on the hardest types of queries -- those where both RPQ endpoints are unspecified. Our more succinct structure, based on $k^2$-trees, is 4 times smaller than any existing representation that handles RPQs, and still solves complex RPQs in a few seconds. Our new sparse-matrix-based representations dominate a good portion of the space/time tradeoff map, being outperformed only by representations that use much more space. They are also of independent interest beyond solving RPQs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14930v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Arroyuelo, Adri\'an G\'omez-Brand\'on, Gonzalo Navarro</dc:creator>
    </item>
    <item>
      <title>A Piecewise Approach for the Analysis of Exact Algorithms</title>
      <link>https://arxiv.org/abs/2402.10015</link>
      <description>arXiv:2402.10015v2 Announce Type: replace 
Abstract: To analyze the worst-case running time of branching algorithms, the majority of work in exponential time algorithms focuses on designing complicated branching rules over developing better analysis methods for simple algorithms. In the mid-$2000$s, Fomin et al. [2005] introduced measure &amp; conquer, an advanced general analysis method, sparking widespread adoption for obtaining tighter worst-case running time upper bounds for many fundamental NP-complete problems. Yet, much potential in this direction remains untapped, as most subsequent work applied it without further advancement. Motivated by this, we present piecewise analysis, a new general method that analyzes the running time of branching algorithms. Our approach is to define a similarity ratio that divides instances into groups and then analyze the running time within each group separately. The similarity ratio is a scale between two parameters of an instance I. Instead of relying on a single measure and a single analysis for the whole instance space, our method allows to take advantage of different intrinsic properties of instances with different similarity ratios. To showcase its potential, we reanalyze two $17$-year-old algorithms from Fomin et al. [2007] that solve $4$-Coloring and #$3$-Coloring respectively. The original analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$ respectively for these algorithms, our analysis improves these running times to $O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running time $O(1.7215^n)$ is the first improvement in the best known running time for the 4-Coloring problem since 2007.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10015v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katie Clinch, Serge Gaspers, Abdallah Saffidine, Tiankuang Zhang</dc:creator>
    </item>
    <item>
      <title>Algorithms for Galois Words: Detection, Factorization, and Rotation</title>
      <link>https://arxiv.org/abs/2403.02636</link>
      <description>arXiv:2403.02636v2 Announce Type: replace 
Abstract: Lyndon words are extensively studied in combinatorics on words -- they play a crucial role on upper bounding the number of runs a word can have [Bannai+, SIAM J. Comput.'17]. We can determine Lyndon words, factorize a word into Lyndon words in lexicographically non-increasing order, and find the Lyndon rotation of a word, all in linear time within constant additional working space. A recent research interest emerged from the question of what happens when we change the lexicographic order, which is at the heart of the definition of Lyndon words. In particular, the alternating order, where the order of all odd positions becomes reversed, has been recently proposed. While a Lyndon word is, among all its cyclic rotations, the smallest one with respect to the lexicographic order, a Galois word exhibits the same property by exchanging the lexicographic order with the alternating order. Unfortunately, this exchange has a large impact on the properties Galois words exhibit, which makes it a nontrivial task to translate results from Lyndon words to Galois words. Up until now, it has only been conjectured that linear-time algorithms with constant additional working space in the spirit of Duval's algorithm are possible for computing the Galois factorization or the Galois rotation.
  Here, we affirm this conjecture as follows. Given a word $T$ of length $n$, we can determine whether $T$ is a Galois word, in $O(n)$ time with constant additional working space. Within the same complexities, we can also determine the Galois rotation of $T$, and compute the Galois factorization of $T$ online. The last result settles Open Problem~1 in [Dolce et al., TCS 2019] for Galois words.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02636v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.CPM.2024.1</arxiv:DOI>
      <dc:creator>Diptarama Hendrian, Dominik K\"oppl, Ryo Yoshinaka, Ayumi Shinohara</dc:creator>
    </item>
    <item>
      <title>BAT-LZ Out of Hell</title>
      <link>https://arxiv.org/abs/2403.09893</link>
      <description>arXiv:2403.09893v2 Announce Type: replace 
Abstract: Despite consistently yielding the best compression on repetitive text collections, the Lempel-Ziv parsing has resisted all attempts at offering relevant guarantees on the cost to access an arbitrary symbol. This makes it less attractive for use on compressed self-indexes and other compressed data structures. In this paper we introduce a variant we call BAT-LZ (for Bounded Access Time Lempel-Ziv) where the access cost is bounded by a parameter given at compression time. We design and implement a linear-space algorithm that, in time $O(n\log^3 n)$, obtains a BAT-LZ parse of a text of length $n$ by greedily maximizing each next phrase length. The algorithm builds on a new linear-space data structure that solves 5-sided orthogonal range queries in rank space, allowing updates to the coordinate where the one-sided queries are supported, in $O(\log^3 n)$ time for both queries and updates. This time can be reduced to $O(\log^2 n)$ if $O(n\log n)$ space is used.
  We design a second algorithm that chooses the sources for the phrases in a clever way, using an enhanced suffix tree, albeit no longer guaranteeing longest possible phrases. This algorithm is much slower in theory, but in practice it is comparable to the greedy parser, while achieving significantly superior compression. We then combine the two algorithms, resulting in a parser that always chooses the longest possible phrases, and the best sources for those. Our experimentation shows that, on most repetitive texts, our algorithms reach an access cost close to $\log_2 n$ on texts of length $n$, while incurring almost no loss in the compression ratio when compared with classical LZ-compression. Several open challenges are discussed at the end of the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09893v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zsuzsanna Lipt\'ak, Francesco Masillo, Gonzalo Navarro</dc:creator>
    </item>
    <item>
      <title>Sinking an Algorithmic Isthmus: (1 + {\epsilon})-Approximate Min-Sum Subset Convolution</title>
      <link>https://arxiv.org/abs/2404.11364</link>
      <description>arXiv:2404.11364v2 Announce Type: replace 
Abstract: Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \subseteq [n]$ as \[
  (f \star g)(S) = \min_{T \subseteq S}\:\big(f(T) + g(S \setminus T)\big), \] lies at the heart of several NP-hard optimization problems, such as minimum-cost $k$-coloring, the prize-collecting Steiner tree, and many others in computational biology. Despite its importance, its na\"ive $O(3^n)$-time evaluation remains the fastest known, the other alternative being an $\tilde O(2^n M)$-time algorithm for instances where the input functions have a bounded integer range $\{-M, \ldots, M\}$.
  We study for the first time the $(1 + \varepsilon)$-approximate min-sum subset convolution and present both a weakly- and strongly-polynomial approximation algorithm, running in time $\tilde O(2^n \log M / \varepsilon)$ and $\tilde O(2^\frac{3n}{2} / \sqrt{\varepsilon})$, respectively. To demonstrate the applicability of our work, we present the first exponential-time $(1 + \varepsilon)$-approximation schemes for the above optimization problems.
  Our algorithms lie at the intersection of two lines of research that have been so far considered separately: $\textit{sequence}$ and $\textit{subset}$ convolutions in semi-rings. We also extend the recent framework of Bringmann, K\"unnemann, and W\k{e}grzycki [STOC 2019] to the context of subset convolutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11364v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihail Stoian</dc:creator>
    </item>
    <item>
      <title>Exploiting New Properties of String Net Frequency for Efficient Computation</title>
      <link>https://arxiv.org/abs/2404.12701</link>
      <description>arXiv:2404.12701v2 Announce Type: replace 
Abstract: Knowing which strings in a massive text are significant -- that is, which strings are common and distinct from other strings -- is valuable for several applications, including text compression and tokenization. Frequency in itself is not helpful for significance, because the commonest strings are the shortest strings. A compelling alternative is net frequency, which has the property that strings with positive net frequency are of maximal length. However, net frequency remains relatively unexplored, and there is no prior art showing how to compute it efficiently. We first introduce a characteristic of net frequency that simplifies the original definition. With this, we study strings with positive net frequency in Fibonacci words. We then use our characteristic and solve two key problems related to net frequency. First, \textsc{single-nf}, how to compute the net frequency of a given string of length $m$, in an input text of length $n$ over an alphabet size $\sigma$. Second, \textsc{all-nf}, given length-$n$ input text, how to report every string of positive net frequency. Our methods leverage suffix arrays, components of the Burrows-Wheeler transform, and solution to the coloured range listing problem. We show that, for both problems, our data structure has $O(n)$ construction cost: with this structure, we solve \textsc{single-nf} in $O(m + \sigma)$ time and \textsc{all-nf} in $O(n)$ time. Experimentally, we find our method to be around 100 times faster than reasonable baselines for \textsc{single-nf}. For \textsc{all-nf}, our results show that, even with prior knowledge of the set of strings with positive net frequency, simply confirming that their net frequency is positive takes longer than with our purpose-designed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12701v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peaker Guo, Patrick Eades, Anthony Wirth, Justin Zobel</dc:creator>
    </item>
    <item>
      <title>Variational Quantum Algorithms for Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2112.08859</link>
      <description>arXiv:2112.08859v2 Announce Type: replace-cross 
Abstract: A semidefinite program (SDP) is a particular kind of convex optimization problem with applications in operations research, combinatorial optimization, quantum information science, and beyond. In this work, we propose variational quantum algorithms for approximately solving SDPs. For one class of SDPs, we provide a rigorous analysis of their convergence to approximate locally optimal solutions, under the assumption that they are weakly constrained (i.e., $N\gg M$, where $N$ is the dimension of the input matrices and $M$ is the number of constraints). We also provide algorithms for a more general class of SDPs that requires fewer assumptions. Finally, we numerically simulate our quantum algorithms for applications such as MaxCut, and the results of these simulations provide evidence that convergence still occurs in noisy settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.08859v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhrumil Patel, Patrick J. Coles, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Streaming word problems</title>
      <link>https://arxiv.org/abs/2202.04060</link>
      <description>arXiv:2202.04060v3 Announce Type: replace-cross 
Abstract: We study deterministic and randomized streaming algorithms for word problems of finitely generated groups. For finitely generated linear groups, metabelian groups and free solvable groups we show the existence of randomized streaming algorithms with logarithmic space complexity for their word problems. We also show that the class of finitely generated groups with a logspace randomized streaming algorithm for the word problem is closed under several group theoretical constructions: finite extensions, graph products and wreath products by free abelian groups. We contrast these results with several lower bound. An example of a finitely presented group, where the word problem has only a linear space randomized streaming algorithm, is Thompson's group $F$. Finally, randomized streaming algorithms for subgroup membership problems in free groups and direct products of free groups are studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.04060v3</guid>
      <category>math.GR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Lohrey, Lukas L\"uck, Julio Xochitemol</dc:creator>
    </item>
    <item>
      <title>Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples</title>
      <link>https://arxiv.org/abs/2309.03847</link>
      <description>arXiv:2309.03847v3 Announce Type: replace-cross 
Abstract: We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\text{poly}(k,d,1/\alpha,1/\varepsilon,\log(1/\delta))$ samples are sufficient to estimate a mixture of $k$ Gaussians in $\mathbb{R}^d$ up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.
  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover (Bun et al., 2021) with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover (Aden-Ali et al., 2021b).</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03847v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Afzali, Hassan Ashtiani, Christopher Liaw</dc:creator>
    </item>
    <item>
      <title>An Algorithm to Recover Shredded Random Matrices</title>
      <link>https://arxiv.org/abs/2310.16715</link>
      <description>arXiv:2310.16715v2 Announce Type: replace-cross 
Abstract: Given some binary matrix $M$, suppose we are presented with the collection of its rows and columns in independent arbitrary orderings. From this information, are we able to recover the unique original orderings and matrix? We present an algorithm that identifies whether there is a unique ordering associated with a set of rows and columns, and outputs either the unique correct orderings for the rows and columns or the full collection of all valid orderings and valid matrices. We show that there is a constant $c &gt; 0$ such that the algorithm terminates in $O(n^2)$ time with high probability and in expectation for random $n \times n$ binary matrices with i.i.d.\ Bernoulli $(p)$ entries $(m_{ij})_{ij=1}^n$ such that $\frac{c\log^2(n)}{n(\log\log(n))^2} \leq p \leq \frac{1}{2}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16715v2</guid>
      <category>math.PR</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caelan Atamanchuk, Luc Devroye, Massimo Vicenzo</dc:creator>
    </item>
    <item>
      <title>A near-optimal zero-free disk for the Ising model</title>
      <link>https://arxiv.org/abs/2311.05574</link>
      <description>arXiv:2311.05574v2 Announce Type: replace-cross 
Abstract: The partition function of the Ising model of a graph $G=(V,E)$ is defined as $Z_{\text{Ising}}(G;b)=\sum_{\sigma:V\to \{0,1\}} b^{m(\sigma)}$, where $m(\sigma)$ denotes the number of edges $e=\{u,v\}$ such that $\sigma(u)=\sigma(v)$. We show that for any positive integer $\Delta$ and any graph $G$ of maximum degree at most $\Delta$, $Z_{\text{Ising}}(G;b)\neq 0$ for all $b\in \mathbb{C}$ satisfying $|\frac{b-1}{b+1}| \leq \frac{1-o_\Delta(1)}{\Delta-1}$ (where $o_\Delta(1) \to 0$ as $\Delta\to \infty$). This is optimal in the sense that $\tfrac{1-o_\Delta(1)}{\Delta-1}$ cannot be replaced by $\tfrac{c}{\Delta-1}$ for any constant $c &gt; 1$ subject to a complexity theoretic assumption.
  To prove our result we use a standard reformulation of the partition function of the Ising model as the generating function of even sets. We establish a zero-free disk for this generating function inspired by techniques from statistical physics on partition functions of a polymer models. Our approach is quite general and we discuss extensions of it to a certain types of polymer models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05574v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viresh Patel, Guus Regts, Ayla Stam</dc:creator>
    </item>
    <item>
      <title>Two-sided Assortment Optimization: Adaptivity Gaps and Approximation Algorithms</title>
      <link>https://arxiv.org/abs/2403.08929</link>
      <description>arXiv:2403.08929v3 Announce Type: replace-cross 
Abstract: To address the challenge of choice congestion in matching markets, in this work, we introduce a two-sided assortment optimization framework under general choice preferences. The goal in this problem is to maximize the expected number of matches by deciding which assortments are displayed to the agents and the order in which they are shown. In this context, we identify several classes of policies that platforms can use in their design. Our goals are: (1) to measure the value that one class of policies has over another one, and (2) to approximately solve the optimization problem itself for a given class. For (1), we define the adaptivity gap as the worst-case ratio between the optimal values of two different policy classes. First, we show that the gap between the class of policies that statically show assortments to one-side first and the class of policies that adaptively show assortments to one-side first is exactly $1-1/e$. Second, we show that the gap between the latter class of policies and the fully adaptive class of policies that show assortments to agents one by one is exactly $1/2$. We also note that the worst policies are those who simultaneously show assortments to all the agents, in fact, we show that their adaptivity gap even with respect to one-sided static policies can be arbitrarily small. For (2), we first show that there exists a polynomial time policy that achieves a $1/4$ approximation factor within the class of policies that adaptively show assortments to agents one by one. Finally, when agents' preferences are governed by multinomial-logit models, we show that a 0.082 approximation factor can be obtained within the class of policies that show assortments to all agents at once.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08929v3</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Omar El Housni, Alfredo Torrico, Ulysse Hennebelle</dc:creator>
    </item>
    <item>
      <title>The ESPRIT algorithm under high noise: Optimal error scaling and noisy super-resolution</title>
      <link>https://arxiv.org/abs/2404.03885</link>
      <description>arXiv:2404.03885v2 Announce Type: replace-cross 
Abstract: Subspace-based signal processing techniques, such as the Estimation of Signal Parameters via Rotational Invariant Techniques (ESPRIT) algorithm, are popular methods for spectral estimation. These algorithms can achieve the so-called super-resolution scaling under low noise conditions, surpassing the well-known Nyquist limit. However, the performance of these algorithms under high-noise conditions is not as well understood. Existing state-of-the-art analysis indicates that ESPRIT and related algorithms can be resilient even for signals where each observation is corrupted by statistically independent, mean-zero noise of size $\mathcal{O}(1)$, but these analyses only show that the error $\epsilon$ decays at a slow rate $\epsilon=\mathcal{\tilde{O}}(n^{-1/2})$ with respect to the cutoff frequency $n$. In this work, we prove that under certain assumptions of bias and high noise, the ESPRIT algorithm can attain a significantly improved error scaling $\epsilon = \mathcal{\tilde{O}}(n^{-3/2})$, exhibiting noisy super-resolution scaling beyond the Nyquist limit. We further establish a theoretical lower bound and show that this scaling is optimal. Our analysis introduces novel matrix perturbation results, which could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03885v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyan Ding, Ethan N. Epperly, Lin Lin, Ruizhe Zhang</dc:creator>
    </item>
  </channel>
</rss>
