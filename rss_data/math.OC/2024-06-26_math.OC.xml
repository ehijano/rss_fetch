<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Input-to-state stability meets small-gain theory</title>
      <link>https://arxiv.org/abs/2406.17909</link>
      <description>arXiv:2406.17909v1 Announce Type: new 
Abstract: Input-to-state stability (ISS) unifies global asymptotic stability with respect to variations of initial conditions with robustness with respect to external disturbances. First, we present Lyapunov characterizations for input-to-state stability as well as ISS superpositions theorems showing relations of ISS to other robust stability properties. Next, we present one of the characteristic applications of the ISS framework - the design of event-based control schemes for the stabilization of nonlinear systems. In the second half of the paper, we focus on small-gain theorems for stability analysis of finite and infinite networks with input-to-state stable components. First, we present a classical small-gain theorem in terms of trajectories for the feedback interconnection of 2 nonlinear systems. Finally, a recent Lyapunov-based small-gain result for a network with infinitely many ISS components is shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17909v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Mironchenko</dc:creator>
    </item>
    <item>
      <title>Frank-Wolfe meets Shapley-Folkman: a systematic approach for solving nonconvex separable problems with linear constraints</title>
      <link>https://arxiv.org/abs/2406.18282</link>
      <description>arXiv:2406.18282v1 Announce Type: new 
Abstract: We consider separable nonconvex optimization problems under affine constraints. For these problems, the Shapley-Folkman theorem provides an upper bound on the duality gap as a function of the nonconvexity of the objective functions, but does not provide a systematic way to construct primal solutions satisfying that bound. In this work, we develop a two-stage approach to do so. The first stage approximates the optimal dual value with a large set of primal feasible solutions. In the second stage, this set is trimmed down to a primal solution by computing (approximate) Caratheodory representations. The main computational requirement of our method is tractability of the Fenchel conjugates of the component functions and their (sub)gradients. When the function domains are convex, the method recovers the classical duality gap bounds obtained via Shapley-Folkman. When the function domains are nonconvex, the method also recovers classical duality gap bounds from the literature, based on a more general notion of nonconvexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18282v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Dubois-Taine, Alexandre d'Aspremont</dc:creator>
    </item>
    <item>
      <title>Learning-rate-free Momentum SGD with Reshuffling Converges in Nonsmooth Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2406.18287</link>
      <description>arXiv:2406.18287v1 Announce Type: new 
Abstract: In this paper, we propose a generalized framework for developing learning-rate-free momentum stochastic gradient descent (SGD) methods in the minimization of nonsmooth nonconvex functions, especially in training nonsmooth neural networks. Our framework adaptively generates learning rates based on the historical data of stochastic subgradients and iterates. Under mild conditions, we prove that our proposed framework enjoys global convergence to the stationary points of the objective function in the sense of the conservative field, hence providing convergence guarantees for training nonsmooth neural networks. Based on our proposed framework, we propose a novel learning-rate-free momentum SGD method (LFM). Preliminary numerical experiments reveal that LFM performs comparably to the state-of-the-art learning-rate-free methods (which have not been shown theoretically to be convergence) across well-known neural network training benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18287v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyin Hu, Nachuan Xiao, Xin Liu, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>A Nesterov-style Accelerated Gradient Descent Algorithm for the Symmetric Eigenvalue Problem</title>
      <link>https://arxiv.org/abs/2406.18433</link>
      <description>arXiv:2406.18433v1 Announce Type: new 
Abstract: We develop an accelerated gradient descent algorithm on the Grassmann manifold to compute the subspace spanned by a number of leading eigenvectors of a symmetric positive semi-definite matrix. This has a constant cost per iteration and a provable iteration complexity of $\tilde{\mathcal{O}}(1/\sqrt{\delta})$, where $\delta$ is the spectral gap and $\tilde{\mathcal{O}}$ hides logarithmic factors. This improves over the $\tilde{\mathcal{O}}(1/\delta)$ complexity achieved by subspace iteration and standard gradient descent, in cases that the spectral gap is tiny. It also matches the iteration complexity of the Lanczos method that has however a growing cost per iteration. On the theoretical part, we rely on the formulation of Riemannian accelerated gradient descent by [26] and new characterizations of the geodesic convexity of the symmetric eigenvalue problem by [8]. On the empirical part, we test our algorithm in synthetic and real matrices and compare with other popular methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18433v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Alimisis, Simon Vary, Bart Vandereycken</dc:creator>
    </item>
    <item>
      <title>Federated Dynamical Low-Rank Training with Global Loss Convergence Guarantees</title>
      <link>https://arxiv.org/abs/2406.17887</link>
      <description>arXiv:2406.17887v1 Announce Type: cross 
Abstract: In this work, we propose a federated dynamical low-rank training (FeDLRT) scheme to reduce client compute and communication costs - two significant performance bottlenecks in horizontal federated learning. Our method builds upon dynamical low-rank splitting schemes for manifold-constrained optimization to create a global low-rank basis of network weights, which enables client training on a small coefficient matrix. A consistent global low-rank basis allows us to incorporate a variance correction scheme and prove global loss descent and convergence to a stationary point. Dynamic augmentation and truncation of the low-rank bases automatically optimizes computing and communication resource utilization. We demonstrate the efficiency of FeDLRT in an array of computer vision benchmarks and show a reduction of client compute and communication costs by up to an order of magnitude with minimal impacts on global accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17887v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Steffen Schotth\"ofer, M. Paul Laiu</dc:creator>
    </item>
    <item>
      <title>Why Line Search when you can Plane Search? SO-Friendly Neural Networks allow Per-Iteration Optimization of Learning and Momentum Rates for Every Layer</title>
      <link>https://arxiv.org/abs/2406.17954</link>
      <description>arXiv:2406.17954v1 Announce Type: cross 
Abstract: We introduce the class of SO-friendly neural networks, which include several models used in practice including networks with 2 layers of hidden weights where the number of inputs is larger than the number of outputs. SO-friendly networks have the property that performing a precise line search to set the step size on each iteration has the same asymptotic cost during full-batch training as using a fixed learning. Further, for the same cost a planesearch can be used to set both the learning and momentum rate on each step. Even further, SO-friendly networks also allow us to use subspace optimization to set a learning rate and momentum rate for each layer on each iteration. We explore augmenting gradient descent as well as quasi-Newton methods and Adam with line optimization and subspace optimization, and our experiments indicate that this gives fast and reliable ways to train these networks that are insensitive to hyper-parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17954v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Betty Shea, Mark Schmidt</dc:creator>
    </item>
    <item>
      <title>Alexandrov's Soap Bubble Theorem for Polygons</title>
      <link>https://arxiv.org/abs/2406.18163</link>
      <description>arXiv:2406.18163v1 Announce Type: cross 
Abstract: Regular polygons are characterized as area-constrained critical points of the perimeter functional with respect to particular families of perturbations in the class of polygons with a fixed number of sides. We also review recent results in the literature involving other shape functionals as well as further open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18163v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bonacini, Riccardo Cristoferi, Ihsan Topaloglu</dc:creator>
    </item>
    <item>
      <title>Bayesian inverse Navier-Stokes problems: joint flow field reconstruction and parameter learning</title>
      <link>https://arxiv.org/abs/2406.18464</link>
      <description>arXiv:2406.18464v1 Announce Type: cross 
Abstract: We formulate and solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates velocimetry data in order to jointly reconstruct a 3D flow field and learn the unknown N-S parameters, including the boundary position. By hardwiring a generalised N-S problem, and regularising its unknown parameters using Gaussian prior distributions, we learn the most likely parameters in a collapsed search space. The most likely flow field reconstruction is then the N-S solution that corresponds to the learned parameters. We develop the method in the variational setting and use a stabilised Nitsche weak form of the N-S problem that permits the control of all N-S parameters. To regularise the inferred the geometry, we use a viscous signed distance field (vSDF) as an auxiliary variable, which is given as the solution of a viscous Eikonal boundary value problem. We devise an algorithm that solves this inverse problem, and numerically implement it using an adjoint-consistent stabilised cut-cell finite element method. We then use this method to reconstruct magnetic resonance velocimetry (flow-MRI) data of a 3D steady laminar flow through a physical model of an aortic arch for two different Reynolds numbers and signal-to-noise ratio (SNR) levels (low/high). We find that the method can accurately i) reconstruct the low SNR data by filtering out the noise/artefacts and recovering flow features that are obscured by noise, and ii) reproduce the high SNR data without overfitting. Although the framework that we develop applies to 3D steady laminar flows in complex geometries, it readily extends to time-dependent laminar and Reynolds-averaged turbulent flows, as well as non-Newtonian (e.g. viscoelastic) fluids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18464v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandros Kontogiannis, Scott V. Elgersma, Andrew J. Sederman, Matthew P. Juniper</dc:creator>
    </item>
    <item>
      <title>Algebraic Connectivity Control and Maintenance in Multi-Agent Networks under Attack</title>
      <link>https://arxiv.org/abs/2406.18467</link>
      <description>arXiv:2406.18467v1 Announce Type: cross 
Abstract: This paper studies the problem of increasing the connectivity of an ad-hoc peer-to-peer network subject to cyber-attacks targeting the agents in the network. The adopted strategy involves the design of local interaction rules for the agents to locally modify the graph topology by adding and removing links with neighbors. Two distributed protocols are presented to boost the algebraic connectivity of the network graph beyond $k-2\sqrt{k-1}$ where $k\in \mathbb{N}$ is a free design parameter; these two protocols are achieved through the distributed construction of random (approximate) regular graphs. One protocol leverages coordinated actions between pairs of neighboring agents and is mathematically proven to converge to the desired graph topology. The other protocol relies solely on the uncoordinated actions of individual agents and it is validated by a spectral analysis through Monte-Carlo simulations. Numerical simulations offer a comparative analysis with other state-of-the-art algorithms, showing the ability of both proposed protocols to maintain high levels of connectivity despite attacks carried out with full knowledge of the network structure, and highlighting their superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18467v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenjie Zhao, Diego Deplano, Zhiwu Li, Alessandro Giua, Mauro Franceschelli</dc:creator>
    </item>
    <item>
      <title>Variance Decay Property for Filter Stability</title>
      <link>https://arxiv.org/abs/2305.12850</link>
      <description>arXiv:2305.12850v3 Announce Type: replace 
Abstract: This paper is concerned with the problem of nonlinear (stochastic) filter stability of a hidden Markov model (HMM) with white noise observations. A contribution is the variance decay property which is used to conclude filter stability. For this purpose, a new notion of the Poincar\'e inequality (PI) is introduced for the nonlinear filter. PI is related to both the ergodicity of the Markov process as well as the observability of the HMM. The proofs are based upon a recently discovered minimum variance duality which is used to transform the nonlinear filtering problem into a stochastic optimal control problem for a backward stochastic differential equation (BSDE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12850v3</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2024.3413573</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Automatic Control, 2024</arxiv:journal_reference>
      <dc:creator>Jin Won Kim, Prashant G. Mehta</dc:creator>
    </item>
    <item>
      <title>On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems</title>
      <link>https://arxiv.org/abs/2306.13928</link>
      <description>arXiv:2306.13928v2 Announce Type: replace 
Abstract: This paper is concerned with a finite-horizon inverse control problem, which has the goal of reconstructing, from observations, the possibly non-convex and non-stationary cost driving the actions of an agent. In this context, we present a result enabling cost reconstruction by solving an optimization problem that is convex even when the agent cost is not and when the underlying dynamics is nonlinear, non-stationary and stochastic. To obtain this result, we also study a finite-horizon forward control problem that has randomized policies as decision variables. We turn our findings into algorithmic procedures and show the effectiveness of our approach via in-silico and hardware validations. All experiments confirm the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13928v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>math.DS</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emiland Garrabe, Hozefa Jesawada, Carmen Del Vecchio, Giovanni Russo</dc:creator>
    </item>
    <item>
      <title>Convergence towards a local minimum by direct search methods with a covering step</title>
      <link>https://arxiv.org/abs/2401.07097</link>
      <description>arXiv:2401.07097v2 Announce Type: replace 
Abstract: This paper introduces a new step to the Direct Search Method (DSM) to strengthen its convergence analysis. By design, this so-called covering step may ensure that for all refined points of the sequence of incumbent solutions generated by the resulting cDSM (covering DSM), the set of all evaluated trial points is dense in a neighborhood of that refined point. We prove that this additional property guarantees that all refined points are local solutions to the optimization problem. This new result holds true even for discontinuous objective function, under a mild assumption that we discuss in details. We also provide a practical construction scheme for the covering step that works at low additional cost per iteration. Finally, we show that the covering step may be adapted to classes of algorithms differing from the DSM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07097v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Audet, Pierre-Yves Bouchet. Lo\"ic Bourdin</dc:creator>
    </item>
    <item>
      <title>Good and Fast Row-Sparse ah-Symmetric Reflexive Generalized Inverses</title>
      <link>https://arxiv.org/abs/2401.17540</link>
      <description>arXiv:2401.17540v2 Announce Type: replace 
Abstract: We present several algorithms aimed at constructing sparse and structured sparse (row-sparse) generalized inverses, with application to the efficient computation of least-squares solutions, for inconsistent systems of linear equations, in the setting of multiple right-hand sides and a rank-deficient constraint matrix. Leveraging our earlier formulations to minimize the 1- and 2,1- norms of generalized inverses that satisfy important properties of the Moore-Penrose pseudoinverse, we develop efficient and scalable ADMM algorithms to address these norm-minimization problems and to limit the number of nonzero rows in the solution. We establish a 2,1-norm approximation result for a local-search procedure that was originally designed for 1-norm minimization, and we compare the ADMM algorithms with the local-search procedure and with general-purpose optimization solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17540v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Ponte, Marcia Fampa, Jon Lee, Luze Xu</dc:creator>
    </item>
    <item>
      <title>Characterization of optimization problems that are solvable iteratively with linear convergence</title>
      <link>https://arxiv.org/abs/2402.12090</link>
      <description>arXiv:2402.12090v3 Announce Type: replace 
Abstract: In this work, we state a general conjecture on the solvability of optimization problems via algorithms with linear convergence guarantees. We make a first step towards examining its correctness by fully characterizing the problems that are solvable via Riemannian gradient descent with linear convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12090v3</guid>
      <category>math.OC</category>
      <category>math.DG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Alimisis</dc:creator>
    </item>
    <item>
      <title>Quantitative asymptotic regularity of the VAM iteration with error terms for m-accretive operators in Banach spaces</title>
      <link>https://arxiv.org/abs/2402.17947</link>
      <description>arXiv:2402.17947v4 Announce Type: replace 
Abstract: In this paper we obtain, by using proof mining methods, quantitative results on the asymptotic regularity of the viscosity approximation method (VAM) with error terms for m-accretive operators in Banach spaces. For concrete instances of the parameter sequences, linear rates are computed by applying a lemma due to Sabach and Shtern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17947v4</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulo Firmino, Laurentiu Leustean</dc:creator>
    </item>
    <item>
      <title>Efficient Low-rank Identification via Accelerated Iteratively Reweighted Nuclear Norm Minimization</title>
      <link>https://arxiv.org/abs/2406.15713</link>
      <description>arXiv:2406.15713v2 Announce Type: replace 
Abstract: This paper considers the problem of minimizing the sum of a smooth function and the Schatten-$p$ norm of the matrix. Our contribution involves proposing accelerated iteratively reweighted nuclear norm methods designed for solving the nonconvex low-rank minimization problem. Two major novelties characterize our approach. Firstly, the proposed method possesses a rank identification property, enabling the provable identification of the "correct" rank of the stationary point within a finite number of iterations. Secondly, we introduce an adaptive updating strategy for smoothing parameters. This strategy automatically fixes parameters associated with zero singular values as constants upon detecting the "correct" rank while quickly driving the rest of the parameters to zero. This adaptive behavior transforms the algorithm into one that effectively solves smooth problems after a few iterations, setting our work apart from existing iteratively reweighted methods for low-rank optimization. We prove the global convergence of the proposed algorithm, guaranteeing that every limit point of the iterates is a critical point. Furthermore, a local convergence rate analysis is provided under the Kurdyka-{\L}ojasiewicz property. We conduct numerical experiments using both synthetic and real data to showcase our algorithm's efficiency and superiority over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15713v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Ye Wang, Xiangyu Yang</dc:creator>
    </item>
    <item>
      <title>A four-operator splitting algorithm for nonconvex and nonsmooth optimization</title>
      <link>https://arxiv.org/abs/2406.16025</link>
      <description>arXiv:2406.16025v2 Announce Type: replace 
Abstract: In this work, we address a class of nonconvex nonsmooth optimization problems where the objective function is the sum of two smooth functions (one of which is proximable) and two nonsmooth functions (one weakly convex and proximable and the other continuous and weakly concave). We introduce a new splitting algorithm that extends the Davis-Yin splitting (DYS) algorithm to handle such four-term nonconvex nonsmooth problems. We prove that with appropriately chosen step sizes, our algorithm exhibits global subsequential convergence to stationary points with a stationarity measure converging at a rate of $1/k$. When specialized to the setting of the DYS algorithm, our results allow for larger stepsizes compared to existing bounds in the literature. Experimental results demonstrate the practical applicability and effectiveness of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16025v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Harold Alcantara, Ching-pei Lee, Akiko Takeda</dc:creator>
    </item>
    <item>
      <title>Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach</title>
      <link>https://arxiv.org/abs/2406.17054</link>
      <description>arXiv:2406.17054v2 Announce Type: replace 
Abstract: Mean-field Langevin dynamics (MLFD) is a class of interacting particle methods that tackle convex optimization over probability measures on a manifold, which are scalable, versatile, and enjoy computational guarantees. However, some important problems -- such as risk minimization for infinite width two-layer neural networks, or sparse deconvolution -- are originally defined over the set of signed, rather than probability, measures. In this paper, we investigate how to extend the MFLD framework to convex optimization problems over signed measures. Among two known reductions from signed to probability measures -- the lifting and the bilevel approaches -- we show that the bilevel reduction leads to stronger guarantees and faster rates (at the price of a higher per-iteration complexity). In particular, we investigate the convergence rate of MFLD applied to the bilevel reduction in the low-noise regime and obtain two results. First, this dynamics is amenable to an annealing schedule, adapted from Suzuki et al. (2023), that results in improved convergence rates to a fixed multiplicative accuracy. Second, we investigate the problem of learning a single neuron with the bilevel approach and obtain local exponential convergence rates that depend polynomially on the dimension and noise level (to compare with the exponential dependence that would result from prior analyses).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17054v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Wang, Alireza Mousavi-Hosseini, L\'ena\"ic Chizat</dc:creator>
    </item>
    <item>
      <title>Lipschitz sub-actions for locally maximal hyperbolic sets of a $C^1$ flow</title>
      <link>https://arxiv.org/abs/2205.10135</link>
      <description>arXiv:2205.10135v2 Announce Type: replace-cross 
Abstract: Liv\v{s}ic theorem for flows asserts that a Lipschitz observable that has zero mean average along every periodic orbit is necessarily a coboundary, that is the Lie derivative of a Lipschitz function smooth along the flow direction. The positive Liv\v{s}ic theorem bounds from below the observable by such a coboundary as soon as the mean average along every periodic orbit is non negative. Previous proofs give a H\"older coboundary. Assuming that the dynamics is given by a locally maximal hyperbolic flow, we show that the coboundary can be Lipschitz. We introduce a new tool: the Lax-Oleinik semigroup, inspired by Fathi's weak KAM theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10135v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xifeng Su, Philippe Thieullen</dc:creator>
    </item>
    <item>
      <title>A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation</title>
      <link>https://arxiv.org/abs/2403.02476</link>
      <description>arXiv:2403.02476v2 Announce Type: replace-cross 
Abstract: We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: \textit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $\alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(\alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads to an overall approach that considerably simplifies existing proofs. We conjecture that our inductive proof technique will find applications in the analyses of more complex stochastic approximation algorithms, and conclude by providing some examples of such applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02476v2</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aritra Mitra</dc:creator>
    </item>
    <item>
      <title>Neural Optimization with Adaptive Heuristics for Intelligent Marketing System</title>
      <link>https://arxiv.org/abs/2405.10490</link>
      <description>arXiv:2405.10490v3 Announce Type: replace-cross 
Abstract: Computational marketing has become increasingly important in today's digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn's email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10490v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671591</arxiv:DOI>
      <dc:creator>Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida</dc:creator>
    </item>
  </channel>
</rss>
