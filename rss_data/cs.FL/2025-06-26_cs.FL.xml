<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 04:03:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simulating Hard Attention Using Soft Attention</title>
      <link>https://arxiv.org/abs/2412.09925</link>
      <description>arXiv:2412.09925v2 Announce Type: replace-cross 
Abstract: We study conditions under which transformers using soft attention can simulate hard attention, that is, effectively focus all attention on a subset of positions. First, we examine several subclasses of languages recognized by hard-attention transformers, which can be defined in variants of linear temporal logic. We demonstrate how soft-attention transformers can compute formulas of these logics using unbounded positional embeddings or temperature scaling. Second, we demonstrate how temperature scaling allows softmax transformers to simulate general hard-attention transformers, using a temperature that depends on the minimum gap between the maximum attention scores and other attention scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09925v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy Yang, Lena Strobl, David Chiang, Dana Angluin</dc:creator>
    </item>
    <item>
      <title>Formalizing zeta and L-functions in Lean</title>
      <link>https://arxiv.org/abs/2503.00959</link>
      <description>arXiv:2503.00959v4 Announce Type: replace-cross 
Abstract: The Riemann zeta function, and more generally the L-functions of Dirichlet characters, are among the central objects of study in number theory. We report on a project to formalize the theory of these objects in Lean's "Mathlib" library, including a proof of Dirichlet's theorem on primes in arithmetic progressions and a formal statement of the Riemann hypothesis</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00959v4</guid>
      <category>math.NT</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Loeffler, Michael Stoll</dc:creator>
    </item>
  </channel>
</rss>
