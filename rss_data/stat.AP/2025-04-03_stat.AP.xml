<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Bayesian Age-Period-Cohort approach for modeling fertility in Puerto Rico</title>
      <link>https://arxiv.org/abs/2504.01148</link>
      <description>arXiv:2504.01148v1 Announce Type: new 
Abstract: Puerto Rico has one of the lowest total fertility rates (TFR) in the world. Combined with a negative net migration and a high proportion of older adults, its unique situation motivates the need for further demographic analysis. Determining whether low fertility rates are mostly due to period or cohort effects is crucial for developing effective public policies that adapt to changes in fertility and population structures. The main objective of this work is to develop an Age-Period-Cohort model, in order to describe fertility data in Puerto Rico, from 1948-1952 and determine the contribution of period and cohort effects to fertility decline. The APC model was developed following a Bayesian framework, with a Poisson likelihood, RW(2) autorregressive priors for the APC parameters, and Scaled Beta2 priors for the precision parameters.Both frequentist and Bayesian methodologies attribute more importance to cohort effects when explaining fertility changes in Puerto Rico. Birth cohorts born in 1963-1967 onward have notably low fertility rates. There is no evidence of postponement of births in Puerto Rico, contrary to other countries with lowest-low fertility. Both frequentist and Bayesian methodologies attribute more importance to cohort effects when explaining fertility changes in Puerto Rico. Birth cohorts born in 1963-1967 onward have notably low fertility rates. There is no evidence of postponement of births in Puerto Rico, contrary to other countries with lowest-low fertility. This is the first application of APC analysis to fertility data in Puerto Rico, which describes fertility changes in a unique scenario in terms of demographic indicators, and the first APC analysis that shows the predominance of cohort effects when explaining fertility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01148v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jomarie Jim\'enez Gonz\'alez, Ang\'elica M. Rosario Santos, Luis R. Pericchi Guerra, Hernando Mattei</dc:creator>
    </item>
    <item>
      <title>Addressing an extreme positivity violation to distinguish the causal effects of surgery and anesthesia via separable effects</title>
      <link>https://arxiv.org/abs/2504.01171</link>
      <description>arXiv:2504.01171v1 Announce Type: new 
Abstract: The U.S. Food and Drug Administration has cautioned that prenatal exposure to anesthetic drugs during the third trimester may have neurotoxic effects; however, there is limited clinical evidence available to substantiate this recommendation. One major scientific question of interest is whether such neurotoxic effects might be due to surgery, anesthesia, or both. Isolating the effects of these two exposures is challenging because they are observationally equivalent, thereby inducing an extreme positivity violation. To address this, we adopt the separable effects framework of Robins and Richardson (2010) to identify the effect of anesthesia (alone) by blocking effects through variables that are assumed to completely mediate the causal pathway from surgery to the outcome. We apply this approach to data from the nationwide Medicaid Analytic eXtract (MAX) from 1999 through 2013, which linked 16,778,281 deliveries to mothers enrolled in Medicaid during pregnancy. Furthermore, we assess the sensitivity of our results to violations of our key identification assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01171v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amy J. Pitts, Ling Guo, Caleb Ing, Caleb H. Miles</dc:creator>
    </item>
    <item>
      <title>SELIC: Semantic-Enhanced Learned Image Compression via High-Level Textual Guidance</title>
      <link>https://arxiv.org/abs/2504.01279</link>
      <description>arXiv:2504.01279v1 Announce Type: new 
Abstract: Learned image compression (LIC) techniques have achieved remarkable progress; however, effectively integrating high-level semantic information remains challenging. In this work, we present a \underline{S}emantic-\underline{E}nhanced \underline{L}earned \underline{I}mage \underline{C}ompression framework, termed \textbf{SELIC}, which leverages high-level textual guidance to improve rate-distortion performance. Specifically, \textbf{SELIC} employs a text encoder to extract rich semantic descriptions from the input image. These textual features are transformed into fixed-dimension tensors and seamlessly fused with the image-derived latent representation. By embedding the \textbf{SELIC} tensor directly into the compression pipeline, our approach enriches the bitstream without requiring additional inputs at the decoder, thereby maintaining fast and efficient decoding. Extensive experiments on benchmark datasets (e.g., Kodak) demonstrate that integrating semantic information substantially enhances compression quality. Our \textbf{SELIC}-guided method outperforms a baseline LIC model without semantic integration by approximately 0.1-0.15 dB across a wide range of bit rates in PSNR and achieves a 4.9\% BD-rate improvement over VVC. Moreover, this improvement comes with minimal computational overhead, making the proposed \textbf{SELIC} framework a practical solution for advanced image compression applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01279v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haisheng Fu, Jie Liang, Zhenman Fang, Jingning Han</dc:creator>
    </item>
    <item>
      <title>Evaluating probabilities without model risk</title>
      <link>https://arxiv.org/abs/2504.01390</link>
      <description>arXiv:2504.01390v1 Announce Type: new 
Abstract: This article presents methods for estimating extreme probabilities, beyond the range of the observations. These methods are model-free and applicable to almost any sample size. They are grounded in order statistics theory and have a wide range of applications, as they simply require the assumption of a finite expectation. Even in cases when a particular risk model exists, the new methods provide clarity, security and simplicity. The methodology is applicable to the behavior of financial markets, and the results may be compared to those provided by extreme value theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01390v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joan del Castillo, Pedro Puig</dc:creator>
    </item>
    <item>
      <title>Predicting passenger injury distributions under uncertainty variables using Gaussian process modeling with GHBMC</title>
      <link>https://arxiv.org/abs/2504.01530</link>
      <description>arXiv:2504.01530v1 Announce Type: new 
Abstract: This work presents a Gaussian Process (GP) modeling method to predict statistical characteristics of injury kinematics responses using Human Body Models (HBM) more accurately and efficiently. We validate the GHBMC model against a 50\%tile male Post-Mortem Human Surrogate (PMHS) test. Using this validated model, we create various postured models and generate injury prediction data across different postures and personalized D-ring heights through parametric crash simulations. We then train the GP using this simulation data, implementing a novel adaptive sampling approach to improve accuracy. The trained GP model demonstrates robustness by achieving target prediction accuracy at points with high uncertainty. The proposed method performs continuous injury prediction for various crash scenarios using just 27 computationally expensive simulation runs. This method can be effectively applied to designing highly reliable occupant restraint systems across diverse crash conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01530v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changmin Baek, Junik Cho, Dongjin Lee</dc:creator>
    </item>
    <item>
      <title>Segmentation variability and radiomics stability for predicting Triple-Negative Breast Cancer subtype using Magnetic Resonance Imaging</title>
      <link>https://arxiv.org/abs/2504.01692</link>
      <description>arXiv:2504.01692v1 Announce Type: new 
Abstract: Most papers caution against using predictive models for disease stratification based on unselected radiomic features, as these features are affected by contouring variability. Instead, they advocate for the use of the Intraclass Correlation Coefficient (ICC) as a measure of stability for feature selection. However, the direct effect of segmentation variability on the predictive models is rarely studied. This study investigates the impact of segmentation variability on feature stability and predictive performance in radiomics-based prediction of Triple-Negative Breast Cancer (TNBC) subtype using Magnetic Resonance Imaging. A total of 244 images from the Duke dataset were used, with segmentation variability introduced through modifications of manual segmentations. For each mask, explainable radiomic features were selected using the Shapley Additive exPlanations method and used to train logistic regression models. Feature stability across segmentations was assessed via ICC, Pearson's correlation, and reliability scores quantifying the relationship between feature stability and segmentation variability. Results indicate that segmentation accuracy does not significantly impact predictive performance. While incorporating peritumoral information may reduce feature reproducibility, it does not diminish feature predictive capability. Moreover, feature selection in predictive models is not inherently tied to feature stability with respect to segmentation, suggesting that an overreliance on ICC or reliability scores for feature selection might exclude valuable predictive features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01692v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isabella Cama, Alejandro Guzm\'an, Cristina Campi, Michele Piana, Karim Lekadir, Sara Garbarino, Oliver D\'iaz</dc:creator>
    </item>
    <item>
      <title>Stock Return Prediction based on a Functional Capital Asset Pricing Model</title>
      <link>https://arxiv.org/abs/2504.01239</link>
      <description>arXiv:2504.01239v1 Announce Type: cross 
Abstract: The capital asset pricing model (CAPM) is readily used to capture a linear relationship between the daily returns of an asset and a market index. We extend this model to an intraday high-frequency setting by proposing a functional CAPM estimation approach. The functional CAPM is a stylized example of a function-on-function linear regression with a bivariate functional regression coefficient. The two-dimensional regression coefficient measures the cross-covariance between cumulative intraday asset returns and market returns. We apply it to the Standard and Poor's 500 index and its constituent stocks to demonstrate its practicality. We investigate the functional CAPM's in-sample goodness-of-fit and out-of-sample prediction for an asset's cumulative intraday return. The findings suggest that the proposed functional CAPM methods have superior model goodness-of-fit and forecast accuracy compared to the traditional CAPM empirical estimation. In particular, the functional methods produce better model goodness-of-fit and prediction accuracy for stocks traditionally considered less price-efficient or more information-opaque.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01239v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Kaiying Ji, Han Lin Shang, Eliza Wu</dc:creator>
    </item>
    <item>
      <title>Multi-Marker Similarity enables reduced-reference and interpretable image quality assessment in optical microscopy</title>
      <link>https://arxiv.org/abs/2504.01537</link>
      <description>arXiv:2504.01537v1 Announce Type: cross 
Abstract: Optical microscopy contributes to the ever-increasing progress in biological and biomedical studies, as it allows the implementation of minimally invasive experimental pipelines to translate the data of measured samples into valuable knowledge. Within these pipelines, reliable quality assessment must be ensured to validate the generated results. Image quality assessment is often applied with full-reference methods to estimate the similarity between the ground truth and the output images. However, current methods often show poor agreement with visual perception and lead to the generation of various full-reference metrics tailored to specific applications. Additionally, they rely on pixel-wise comparisons, emphasizing local intensity similarity while often overlooking comprehensive and interpretable image quality assessment. To address these issues, we have developed a multi-marker similarity method that compares standard quality markers, such as resolution, signal-to-noise ratio, contrast, and high frequency components. The method computes a similarity score between the image and the ground truth for each marker, then combines these scores into an overall similarity estimate. This provides a full-reference estimate of image quality while extracting global quality features and detecting experimental artifacts. Multi-marker similarity provides a reliable and interpretable method for image quality assessment and the generation of quality rankings. By focusing on the comparison of quality markers rather than direct image distances, the method enables reduced reference implementations, where a single field of view is used as a benchmark for multiple measurements. This opens the way for reliable automatic evaluation of big datasets, typical of large biomedical studies, when manual assessment of single images and defining the ground truth for each field of view is not feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01537v1</guid>
      <category>q-bio.QM</category>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Corbetta, Thomas Bocklitz</dc:creator>
    </item>
    <item>
      <title>Hidden Markov Model Filtering with Equal Exit Probabilities</title>
      <link>https://arxiv.org/abs/2504.01759</link>
      <description>arXiv:2504.01759v1 Announce Type: cross 
Abstract: Hidden Markov Models (HMMs) provide a rigorous framework for inference in dynamic environments. In this work, we study the alpha-HMM algorithm motivated by the optimal online filtering formulation in settings where the true state evolves as a Markov chain with equal exit probabilities. We quantify the dynamics of the algorithm in stationary environments, revealing a trade-off between inference and adaptation, showing how key parameters and the quality of observations affect performance. Comprehensive theoretical analysis on the nonlinear dynamical system that governs the evolution of the log-belief ratio over time and numerical experiments demonstrate that the proposed approach effectively balances adaptation and inference performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01759v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongyan Sui, Haotian Pu, Siyang Leng, Stefan Vlaski</dc:creator>
    </item>
    <item>
      <title>Comparison of Bayesian methods for extrapolation of treatment effects: a large scale simulation study</title>
      <link>https://arxiv.org/abs/2504.01949</link>
      <description>arXiv:2504.01949v1 Announce Type: cross 
Abstract: Extrapolating treatment effects from related studies is a promising strategy for designing and analyzing clinical trials in situations where achieving an adequate sample size is challenging. Bayesian methods are well-suited for this purpose, as they enable the synthesis of prior information through the use of prior distributions. While the operating characteristics of Bayesian approaches for borrowing data from control arms have been extensively studied, methods that borrow treatment effects -- quantities derived from the comparison between two arms -- remain less well understood. In this paper, we present the findings of an extensive simulation study designed to address this gap. We evaluate the frequentist operating characteristics of these methods, including the probability of success, mean squared error, bias, precision, and credible interval coverage. Our results provide insights into the strengths and limitations of existing methods in the context of confirmatory trials. In particular, we show that the Conditional Power Prior and the Robust Mixture Prior perform better overall, while the test-then-pool variants and the p-value-based power prior display suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01949v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Fauvel, Julien Tanniou, Pascal Godbillot, Billy Amzal</dc:creator>
    </item>
    <item>
      <title>Adoption and implication of the Biased-Annotator Competence Estimation (BACE) model into COVID-19 vaccine Twitter data: Human annotation for latent message features</title>
      <link>https://arxiv.org/abs/2302.09482</link>
      <description>arXiv:2302.09482v4 Announce Type: replace 
Abstract: Traditional quantitative content analysis approach (human coding method) has weaknesses, such as assuming all human coders are equally accurate once the intercoder reliability for training reaches a threshold score. We applied the Biased-Annotator Competence Estimation (BACE) model (Tyler, 2021), which draws on Bayesian modeling to improve human coding. An important contribution of this model is it takes each coder's potential biases and reliability into consideration and treats the "true" label of each message as a latent parameter, with quantifiable estimation uncertainties. In contrast, in conventional human coding, each message will receive a fixed label without estimates for measurement uncertainties. In this extended abstract, we first summarize the weaknesses of conventional human coding; and then apply the BACE model to COVID-19 vaccine Twitter data and compare BACE with other statistical models; finally, we discuss how the BACE model can be applied to improve human coding of latent message features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09482v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luhang Sun, Yun-Shiuan Chuang, Yibing Sun, Sijia Yang</dc:creator>
    </item>
    <item>
      <title>Granger causal inference for climate change attribution</title>
      <link>https://arxiv.org/abs/2408.16004</link>
      <description>arXiv:2408.16004v3 Announce Type: replace 
Abstract: Climate change detection and attribution (D&amp;A) is concerned with determining the extent to which anthropogenic activities have influenced specific aspects of the global climate system. D&amp;A fits within the broader field of causal inference, the collection of statistical methods that identify cause and effect relationships. There are a wide variety of methods for making attribution statements, each of which require different types of input data and each of which are conditional to varying extents. Some methods are based on Pearl causality (experimental interference) while others leverage Granger (predictive) causality, and the causal framing provides important context for how the resulting attribution conclusion should be interpreted. However, while Granger-causal attribution analyses have become more common, there is no clear statement of their strengths and weaknesses and no clear consensus on where and when Granger-causal perspectives are appropriate. In this prospective paper, we provide a formal definition for Granger-based approaches to trend and event attribution and a clear comparison with more traditional methods for assessing the human influence on extreme weather and climate events. Broadly speaking, Granger-causal attribution statements can be constructed quickly from observations and do not require computationally-intesive dynamical experiments. These analyses also enable rapid attribution, which is useful in the aftermath of a severe weather event, and provide multiple lines of evidence for anthropogenic climate change when paired with Pearl-causal attribution. Confidence in attribution statements is increased when different methodologies arrive at similar conclusions. Moving forward, we encourage the D&amp;A community to embrace hybrid approaches to climate change attribution that leverage the strengths of both Granger and Pearl causality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16004v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Mohammed Ombadi, Michael F. Wehner</dc:creator>
    </item>
    <item>
      <title>Dependence-based fuzzy clustering of functional time series</title>
      <link>https://arxiv.org/abs/2405.04904</link>
      <description>arXiv:2405.04904v2 Announce Type: replace-cross 
Abstract: Time series clustering is essential in scientific applications, yet methods for functional time series, collections of infinite-dimensional curves treated as random elements in a Hilbert space, remain underdeveloped. This work presents clustering approaches for functional time series that combine the fuzzy $C$-medoids and fuzzy $C$-means procedures with a novel dissimilarity measure tailored for functional data. This dissimilarity is based on an extension of the quantile autocorrelation to the functional context. Our methods effectively groups time series with similar dependence structures, achieving high accuracy and computational efficiency in simulations. The practical utility of the approach is demonstrated through case studies on high-frequency financial stock data and multi-country age-specific mortality improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04904v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Angel Lopez-Oriona, Ying Sun, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Phase vs coin vs position disorder as a probe for the resilience and revival of single particle entanglement in cyclic quantum walks</title>
      <link>https://arxiv.org/abs/2410.12710</link>
      <description>arXiv:2410.12710v2 Announce Type: replace-cross 
Abstract: Quantum states exhibiting single-particle entanglement (SPE) can encode and process quantum information more robustly than their multi-particle analogs. Understanding the vulnerability and resilience of SPE to disorder is therefore crucial. This letter investigates phase, coin, and position disorder via discrete-time quantum walks on odd and even cyclic graphs to study their effect on SPE. The reduction in SPE is insignificant for low levels of phase or coin disorder, showing the resilience of SPE to minor perturbations. However, SPE is seen to be more vulnerable to position disorder. We analytically prove that maximally entangled single-particle states (MESPS) at time step $t=1$ are impervious to phase disorder regardless of the choice of the initial state. Further, MESPS at timestep $t=1$ is also wholly immune to coin disorder for phase-symmetric initial states. Position disorder breaks odd-even parity and distorts the physical time cone of the quantum walker, unlike phase or coin disorder. SPE saturates towards a fixed value for position disorder, irrespective of the disorder strength at large timestep $t$. Furthermore, SPE can be enhanced with moderate to significant phase or coin disorder strengths at specific time steps. Interestingly, disorder can revive single-particle entanglement from absolute zero in some instances, too. These results are crucial in understanding single-particle entanglement evolution and dynamics in a lab setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12710v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Phys. Rev. E (Letters) (2025)</arxiv:journal_reference>
      <dc:creator>Dinesh Kumar Panda, Colin Benjamin</dc:creator>
    </item>
    <item>
      <title>Leveraging statistical models to improve pre-season forecasting and in-season management of a recreational fishery</title>
      <link>https://arxiv.org/abs/2503.17293</link>
      <description>arXiv:2503.17293v4 Announce Type: replace-cross 
Abstract: Effective management of recreational fisheries requires accurate forecasting of future harvests and real-time monitoring of ongoing harvests. Traditional methods that rely on historical catch data to predict short-term harvests can be unreliable, particularly if changes in management regulations alter angler behavior. In contrast, statistical modeling approaches can provide faster, more flexible, and potentially more accurate predictions, enhancing management outcomes. In this study, we developed and tested models to improve predictions of Gulf of Mexico gag harvests for both pre-season planning and in-season monitoring. Our best-fitting model outperformed traditional methods (i.e., estimates derived from historical average harvest) for both cumulative pre-season projections and in-season monitoring. Notably, our modeling framework appeared to be more accurate in more recent, shorter seasons due to its ability to account for effort compression. A key advantage of our framework is its ability to explicitly quantify the probability of exceeding harvest quotas for any given season duration. This feature enables managers to evaluate trade-offs between season duration and conservation goals. This is especially critical for vulnerable, highly targeted stocks. Our findings also underscore the value of statistical models to complement and advance traditional fisheries management approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17293v4</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Challen Hyman, Chloe Ramsay, Tiffanie A. Cross, Beverly Sauls, Thomas K. Frazer</dc:creator>
    </item>
  </channel>
</rss>
