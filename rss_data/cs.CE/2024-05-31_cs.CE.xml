<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 May 2024 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Calibration and Validation of a Phase-Field Model of Brittle Fracture within the Damage Mechanics Challenge</title>
      <link>https://arxiv.org/abs/2405.19491</link>
      <description>arXiv:2405.19491v1 Announce Type: new 
Abstract: In the context of the Damage Mechanics Challenge, we adopt a phase-field model of brittle fracture to blindly predict the behavior up to failure of a notched three-point-bending specimen loaded under mixed-mode conditions. The beam is additively manufactured using a geo-architected gypsum based on the combination of bassanite and a water-based binder. The calibration of the material parameters involved in the model is based on a set of available independent experimental tests and on a two-stage procedure. In the first stage an estimate of most of the elastic parameters is obtained, whereas the remaining parameters are optimized in the second stage so as to minimize the discrepancy between the numerical predictions and a set of experimental results on notched three-point-bending beams. The good agreement between numerical predictions and experimental results in terms of load-displacement curves and crack paths demonstrates the predictive ability of the model and the reliability of the calibration procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19491v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Heinzmann, Pietro Carrara, Chenyi Luo, Manav Manav, Akanksha Mishra, Sindhu Nagaraja, Hamza Oudich, Francesco Vicentini, Laura De Lorenzis</dc:creator>
    </item>
    <item>
      <title>The Accuracy of Domain Specific and Descriptive Analysis Generated by Large Language Models</title>
      <link>https://arxiv.org/abs/2405.19578</link>
      <description>arXiv:2405.19578v1 Announce Type: new 
Abstract: Large language models (LLMs) have attracted considerable attention as they are capable of showcasing impressive capabilities generating comparable high-quality responses to human inputs. LLMs, can not only compose textual scripts such as emails and essays but also executable programming code. Contrary, the automated reasoning capability of these LLMs in performing statistically-driven descriptive analysis, particularly on user-specific data and as personal assistants to users with limited background knowledge in an application domain who would like to carry out basic, as well as advanced statistical and domain-specific analysis is not yet fully explored. More importantly, the performance of these LLMs has not been compared and discussed in detail when domain-specific data analysis tasks are needed. This study, consequently, explores whether LLMs can be used as generative AI-based personal assistants to users with minimal background knowledge in an application domain infer key data insights. To demonstrate the performance of the LLMs, the study reports a case study through which descriptive statistical analysis, as well as Natural Language Processing (NLP) based investigations, are performed on a number of phishing emails with the objective of comparing the accuracy of the results generated by LLMs to the ones produced by analysts. The experimental results show that LangChain and the Generative Pre-trained Transformer (GPT-4) excel in numerical reasoning tasks i.e., temporal statistical analysis, achieve competitive correlation with human judgments on feature engineering tasks while struggle to some extent on domain specific knowledge reasoning, where domain-specific knowledge is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19578v1</guid>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denish Omondi Otieno, Faranak Abri, Sima Siami-Namini, Akbar Siami Namin</dc:creator>
    </item>
    <item>
      <title>Deep Learning Model for Detecting Abnormal Corn Kernels</title>
      <link>https://arxiv.org/abs/2405.19628</link>
      <description>arXiv:2405.19628v1 Announce Type: new 
Abstract: This research aims to detect the physical characteristics of corn kernels and analyze images using a deep learning model. The data analysis based on the CRISP-DM framework which consists of six steps, business understanding, data understanding, data preparation, modelling, evaluation, and deployment. The business goal reduces the cost of the separation of abnormal corn kernels. The dataset comprises 1,800 images of corn kernels and divided equally between normal and abnormal corn kernels. The dataset was divided into three subsets: 1,000 images for training the deep learning model, 600 images for validation and 200 images for testing. The tools for analysis in this research are Jupyter Lab, Python, TensorFlow Keras, and Convolutional Neural Networks. The results revealed that the deep learning model achieved the accuracy rate of 99% in differentiating between normal and abnormal corn kernel images that is a highly effective model in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19628v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suwannee Adsavakulchai, Mawin Prommasaeng</dc:creator>
    </item>
    <item>
      <title>A Deep Reinforcement Learning Approach for Trading Optimization in the Forex Market with Multi-Agent Asynchronous Distribution</title>
      <link>https://arxiv.org/abs/2405.19982</link>
      <description>arXiv:2405.19982v1 Announce Type: new 
Abstract: In today's forex market traders increasingly turn to algorithmic trading, leveraging computers to seek more profits. Deep learning techniques as cutting-edge advancements in machine learning, capable of identifying patterns in financial data. Traders utilize these patterns to execute more effective trades, adhering to algorithmic trading rules. Deep reinforcement learning methods (DRL), by directly executing trades based on identified patterns and assessing their profitability, offer advantages over traditional DL approaches. This research pioneers the application of a multi-agent (MA) RL framework with the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) algorithm. The proposed method employs parallel learning across multiple asynchronous workers, each specialized in trading across multiple currency pairs to explore the potential for nuanced strategies tailored to different market conditions and currency pairs. Two different A3C with lock and without lock MA model was proposed and trained on single currency and multi-currency. The results indicate that both model outperform on Proximal Policy Optimization model. A3C with lock outperforms other in single currency training scenario and A3C without Lock outperforms other in multi-currency scenario. The findings demonstrate that this approach facilitates broader and faster exploration of different currency pairs, significantly enhancing trading returns. Additionally, the agent can learn a more profitable trading strategy in a shorter time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19982v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Davoud Sarani, Dr. Parviz Rashidi-Khazaee</dc:creator>
    </item>
    <item>
      <title>OpenTM: An Open-source, Single-GPU, Large-scale Thermal Microstructure Design Framework</title>
      <link>https://arxiv.org/abs/2405.19991</link>
      <description>arXiv:2405.19991v1 Announce Type: new 
Abstract: Thermal microstructures are artificially engineered materials designed to manipulate and control heat flow in unconventional ways. This paper presents an educational framework, called \emph{OpenTM}, to use a single GPU for designing periodic 3D high-resolution thermal microstructures to match the predefined thermal conductivity matrices with volume fraction constraints. Specifically, we use adaptive volume fraction to make the Optimality Criteria (OC) method run stably to obtain the thermal microstructures without a large memory overhead.Practical examples with a high resolution $128 \times 128 \times 128$ run under 90 seconds per structure on an NVIDIA GeForce GTX 4070Ti GPU with a peak GPU memory of 355 MB. Our open-source, high-performance implementation is publicly accessible at \url{https://github.com/quanyuchen2000/OPENTM}, and it is easy to install using Anaconda. Moreover, we provide a Python interface to make OpenTM well-suited for novices in C/C++.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19991v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Quan, Xiaoya Zhai, Xiao-Ming Fu</dc:creator>
    </item>
    <item>
      <title>Bridging eResearch Infrastructure and Experimental Materials Science Process in the Quantum Data Hub</title>
      <link>https://arxiv.org/abs/2405.19706</link>
      <description>arXiv:2405.19706v1 Announce Type: cross 
Abstract: Experimental materials science is experiencing significant growth due to automated experimentation and AI techniques. Integrated autonomous platforms are emerging, combining generative models, robotics, simulations, and automated systems for material synthesis. However, two major challenges remain: democratizing access to these technologies and creating accessible infrastructure for under-resourced scientists. This paper introduces the Quantum Data Hub (QDH), a community-accessible research infrastructure aimed at researchers working with quantum materials. QDH integrates with the National Data Platform, adhering to FAIR principles while proposing additional UNIT principles for usability, navigability, interpretability, and timeliness. The QDH facilitates collaboration and extensibility, allowing seamless integration of new researchers, instruments, and data into the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19706v1</guid>
      <category>cs.SE</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amarnath Gupta, Shweta Purawat, Subhasis Dasgupta, Pratyush Karmakar, Elaine Chi, Ilkay Altintas</dc:creator>
    </item>
    <item>
      <title>A segregated reduced order model of a pressure-based solver for turbulent compressible flows</title>
      <link>https://arxiv.org/abs/2205.09396</link>
      <description>arXiv:2205.09396v2 Announce Type: replace-cross 
Abstract: This article provides a reduced-order modelling framework for turbulent compressible flows discretized by the use of finite volume approaches. The basic idea behind this work is the construction of a reduced-order model capable of providing closely accurate solutions with respect to the high fidelity flow fields. Full-order solutions are often obtained through the use of segregated solvers (solution variables are solved one after another), employing slightly modified conservation laws so that they can be decoupled and then solved one at a time. Classical reduction architectures, on the contrary, rely on the Galerkin projection of a complete Navier-Stokes system to be projected all at once, causing a mild discrepancy with the high order solutions. This article relies on segregated reduced-order algorithms for the resolution of turbulent and compressible flows in the context of physical and geometrical parameters. At the full-order level turbulence is modeled using an eddy viscosity approach. Since there is a variety of different turbulence models for the approximation of this supplementary viscosity, one of the aims of this work is to provide a reduced-order model which is independent on this selection. This goal is reached by the application of hybrid methods where Navier-Stokes equations are projected in a standard way while the viscosity field is approximated by the use of data-driven interpolation methods or by the evaluation of a properly trained neural network. By exploiting the aforementioned expedients it is possible to predict accurate solutions with respect to the full-order problems characterized by high Reynolds numbers and elevated Mach numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.09396v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Zancanaro, Valentin Nkana Ngan, Giovanni Stabile, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>ORLM: Training Large Language Models for Optimization Modeling</title>
      <link>https://arxiv.org/abs/2405.17743</link>
      <description>arXiv:2405.17743v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for tackling complex Operations Research (OR) problem by providing the capacity in automating optimization modeling. However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications. To tackle this issue, we propose training open-source LLMs for optimization modeling. We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements. We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems. We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling. Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data are available at \url{https://github.com/Cardinal-Operations/ORLM}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17743v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang</dc:creator>
    </item>
  </channel>
</rss>
