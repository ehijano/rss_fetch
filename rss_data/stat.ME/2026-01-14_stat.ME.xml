<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Jan 2026 02:33:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Symmetric Random Scan Collapsed Gibbs Sampler for Fully Bayesian Variable Selection with Spike-and-Slab Priors</title>
      <link>https://arxiv.org/abs/2601.07864</link>
      <description>arXiv:2601.07864v1 Announce Type: new 
Abstract: We introduce a symmetric random scan Gibbs sampler for scalable Bayesian variable selection that eliminates storage of the full cross-product matrix by computing required quantities on-the-fly. Data-informed proposal weights, constructed from marginal correlations, concentrate sampling effort on promising candidates while a uniform mixing component ensures theoretical validity. We provide explicit guidance for selecting tuning parameters based on the ratio of signal to null correlations, ensuring adequate posterior exploration. The posterior-mean-size selection rule provides an adaptive alternative to the median probability model that automatically calibrates to the effective signal density without requiring an arbitrary threshold. In simulations with one hundred thousand predictors, the method achieves sensitivity of 1.000 and precision above 0.76. Application to a genomic dataset studying riboflavin production in Bacillus subtilis identifies six genes, all validated by previous studies using alternative methods. The underlying model combines a Dirac spike-and-slab prior with Laplace-type shrinkage: the Dirac spike enforces exact sparsity by setting inactive coefficients to precisely zero, while the Laplace-type slab provides adaptive regularization for active coefficients through a local-global scale mixture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07864v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengta Chung</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Two Stochastic Processes, with Application to Learning Hospitalization Dynamics from Wastewater Viral Concentrations</title>
      <link>https://arxiv.org/abs/2601.07977</link>
      <description>arXiv:2601.07977v1 Announce Type: new 
Abstract: In the post-pandemic era of COVID-19, hospitalization remains a primary public health concern and wastewater surveillance has become an important tool for monitoring its dynamics at the level of community. However, there is usually no sufficient information to know the infection process that results in both wastewater viral signals and hospital admissions. That key challenge has motived a statistical framework proposed in this paper. We formulate the connection of overtime wastewater viral signals and hospitalization counts through a latent process of infection at the level of individual subject. We provide a strategy for accommodating aggregated data, a typical form of surveillance data. Moreover, we ease the conventional procedure of the statistical learning with the joint modeling using available information on the infection process, which can be under-reporting. A simulation study demonstrates that the proposed approach yields stable inference under different degrees of under-ascertainment. The COVID-19 surveillance data from Ottawa, Canada shows that the framework recovers coherent temporal patterns in infection prevalence and variant-specific hospitalization risk under several reporting assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07977v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Ken Peng, Charmaine B. Dean, Robert Delatolla, X. Joan Hu, Elizabeth Renouf</dc:creator>
    </item>
    <item>
      <title>Spatial Covariance Constraints for Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2601.07979</link>
      <description>arXiv:2601.07979v1 Announce Type: new 
Abstract: Although extensive research exists in spatial modeling, few studies have addressed finite mixture model-based clustering methods for spatial data. Finite mixture models, especially Gaussian mixture models, particularly suffer from high dimensionality due to the number of free covariance parameters. This study introduces a spatial covariance constraint for Gaussian mixture models that requires only four free parameters for each component, independent of dimensionality. Using a coordinate system, the spatially constrained Gaussian mixture model enables clustering of multi-way spatial data and inference of spatial patterns. The parameter estimation is conducted by combining the expectation-maximization (EM) algorithm with the generalized least squares (GLS) estimator. Simulation studies and applications to Raman spectroscopy data are provided to demonstrate the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07979v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzhang Lu, Keiran Malott, Venkat Suprabath Bitra, Kirsty Milligan, Sanjeena Subedi, Edana Cassol, Vinita Chauhan, Connor McNairn, Bryan Muir, Prarthana Pasricha, Sangeeta Murugkar, Rowan Thomson, Andrew Jirasek, Jeffrey L. Andrews</dc:creator>
    </item>
    <item>
      <title>Modeling Event Dynamics by Self-Exciting Processes with Random Memory</title>
      <link>https://arxiv.org/abs/2601.07980</link>
      <description>arXiv:2601.07980v1 Announce Type: new 
Abstract: Event history data from sports competitions have recently drawn increasing attention in sports analytics to generate data-driven strategies. Such data often exhibit self-excitation in the event occurrence and dependence within event clusters. The conventional event models based on gap times may struggle to capture those features. In particular, while consecutive events may occur within a short timeframe, the self-excitation effect caused by previous events is often transient and continues for a period of uncertain time. This paper introduces an extended Hawkes process model with random self-excitation duration to formulate the dynamics of event occurrence. We present examples of the proposed model and procedures for estimating the associated model parameters. We employ the collection of the corner kicks in the games of the 2019 regular season of the Chinese Super League to motivate and illustrate the modeling and its usefulness. We also design algorithms for simulating the event process under proposed models. The proposed approach can be adapted with little modification in many other research fields such as Criminology and Infectious Disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Ken Peng, X. Joan Hu, Tim B. Swartz</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric models for zero-inflated count-compositional data using ensembles of regression trees</title>
      <link>https://arxiv.org/abs/2601.08067</link>
      <description>arXiv:2601.08067v1 Announce Type: new 
Abstract: Count-compositional data arise in many different fields, including high-throughput microbiome sequencing and palynology experiments, where a common, important goal is to understand how covariates relate to the observed compositions. Existing methods often fail to simultaneously address key challenges inherent in such data, namely: overdispersion, an excess of zeros, cross-sample heterogeneity, and nonlinear covariate effects. To address these concerns, we propose novel Bayesian models based on ensembles of regression trees. Specifically, we leverage the recently introduced zero-and-$N$-inflated multinomial distribution and assign independent nonparametric Bayesian additive regression tree (BART) priors to both the compositional and structural zero probability components of our model, to flexibly capture covariate effects. We further extend this by adding latent random effects to capture overdispersion and more general dependence structures among the categories. We develop an efficient inferential algorithm combining recent data augmentation schemes with established BART sampling routines. We evaluate our proposed models in simulation studies and illustrate their applicability with two case studies in microbiome and palaeoclimate modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08067v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e F. B. Menezes, Andrew C. Parnell, Keefe Murphy</dc:creator>
    </item>
    <item>
      <title>REAMP: A Stochastic Resonance Approach for Multi-Change Point Detection in High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2601.08084</link>
      <description>arXiv:2601.08084v1 Announce Type: new 
Abstract: Detecting multiple structural breaks in high-dimensional data remains a challenge, particularly when changes occur in higher-order moments or within complex manifold structures. In this paper, we propose REAMP (Resonance-Enhanced Analysis of Multi-change Points), a novel framework that integrates optimal transport theory with the physical principles of stochastic resonance. By utilizing a two-stage dimension reduction via the Earth Movers Distance (EMD) and Shortest Hamiltonian Paths (SHP), we map high-dimensional observations onto a graph-based count statistic. To overcome the locality constraints of traditional search algorithms, we implement a stochastic resonance system that utilizes randomized Beta-density priors to vibrate the objective function. This process allows multiple change points to resonate as global minima across iterative simulations, generating a candidate point cloud. A double-sharpening procedure is then applied to these candidates to pinpoint precise change point locations. We establish the asymptotic consistency of the resonance estimator and demonstrate through simulations that REAMP outperforms state-of-the-art methods, especially in scenarios involving simultaneous mean and variance shifts. The practical utility of the method is further validated through an application to time-lapse embryo monitoring, where REAMP provides both accurate detection and intuitive visualization of cell division stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08084v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoping Shi, Baisuo Jin, Xianhui Liu, Qiong Li</dc:creator>
    </item>
    <item>
      <title>Sparsifying transform priors in Gaussian graphical models</title>
      <link>https://arxiv.org/abs/2601.08596</link>
      <description>arXiv:2601.08596v1 Announce Type: new 
Abstract: Bayesian methods constitute a popular approach for estimating the conditional independence structure in Gaussian graphical models, since they can quantify the uncertainty through the posterior distribution. Inference in this framework is typically carried out with Markov chain Monte Carlo (MCMC). However, the most widely used choice of prior distribution for the precision matrix, the so called G-Wishart distribution, suffers from an intractable normalizing constant, which gives rise to the problem of double intractability in the updating steps of the MCMC algorithm. In this article, we propose a new class of prior distributions for the precision matrix, termed ST priors, that allow for the construction of MCMC algorithms that do not suffer from double intractability issues. A realization from an ST prior distribution is obtained by applying a sparsifying transform on a matrix from a distribution with support in the set of all positive definite matrices. We carefully present the theory behind the construction of our proposed class of priors and also perform some numerical experiments, where we apply our methods on a human gene expression dataset. The results suggest that our proposed MCMC algorithm is able to converge and achieve acceptable mixing when applied on the real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08596v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcus Gehrmann, H{\aa}kon Tjelmeland</dc:creator>
    </item>
    <item>
      <title>Flexible modeling of nonnegative continuous data: Box-Cox symmetric regression and its zero-adjusted extension</title>
      <link>https://arxiv.org/abs/2601.08600</link>
      <description>arXiv:2601.08600v1 Announce Type: new 
Abstract: The Box-Cox symmetric distributions constitute a broad class of probability models for positive continuous data, offering flexibility in modeling skewness and tail behavior. Their parameterization allows a straightforward quantile-based interpretation, which is particularly useful in regression modeling. Despite their potential, only a few specific distributions within this class have been explored in regression contexts, and zero-adjusted extensions have not yet been formally addressed in the literature. This paper formalizes the class of Box-Cox symmetric regression models and introduces a new zero-adjusted extension suitable for modeling data with a non-negligible proportion of observations equal to zero. We discuss maximum likelihood estimation, assess finite-sample performance through simulations, and develop diagnostic tools including residual analysis, local influence measures, and goodness-of-fit statistics. An empirical application on basic education expenditure illustrates the models' ability to capture complex patterns in zero-inflated and highly skewed nonnegative data. To support practical use, we developed the new BCSreg R package, which implements all proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08600v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo M. R. de Medeiros, Francisco F. Queiroz</dc:creator>
    </item>
    <item>
      <title>Permutation Inference under Multi-way Clustering and Missing Data</title>
      <link>https://arxiv.org/abs/2601.08610</link>
      <description>arXiv:2601.08610v1 Announce Type: new 
Abstract: Econometric applications with multi-way clustering often feature a small number of effective clusters or heavy-tailed data, making standard cluster-robust and bootstrap inference unreliable in finite samples. In this paper, we develop a framework for finite-sample valid permutation inference in linear regression with multi-way clustering under an assumption of conditional exchangeability of the errors. Our assumption is closely related to the notion of separate exchangeability studied in earlier work, but can be more realistic in many economic settings as it imposes minimal restrictions on the covariate distribution. We construct permutation tests of significance that are valid in finite samples and establish theoretical power guarantees, in contrast to existing methods that are justified only asymptotically. We also extend our methodology to settings with missing data and derive power results that reveal phase transitions in detectability. Through simulation studies, we demonstrate that the proposed tests maintain correct size and competitive power, while standard cluster-robust and bootstrap procedures can exhibit substantial size distortions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08610v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenxuan Guo, Panos Toulis, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Data Integration Using the Dual-Frame Sampling Framework</title>
      <link>https://arxiv.org/abs/2601.08707</link>
      <description>arXiv:2601.08707v1 Announce Type: new 
Abstract: Integrating probability and non-probability samples is increasingly important, yet unknown sampling mechanisms in non-probability sources complicate identification and efficient estimation. We develop semiparametric theory for dual-frame data integration and propose two complementary estimators. The first models the non-probability inclusion probability parametrically and attains the semiparametric efficiency bound. We introduce an identifiability condition based on strong monotonicity that identifies sampling-model parameters without instrumental variables, even under informative (non-ignorable) selection, using auxiliary information from the probability sample; it remains valid without record linkage between samples. The second estimator, motivated by a two-stage sampling approximation, avoids explicit modeling of the non-probability mechanism; though not fully efficient, it is efficient within a restricted augmentation class and is robust to misspecification. Simulations and an application to the Culture and Community in a Time of Crisis public simulation dataset show efficiency gains under correct specification and stable performance under misspecification and weak identification. Methods are implemented in the R package \texttt{dfSEDI}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08707v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Morikawa, Jae Kwang Kim</dc:creator>
    </item>
    <item>
      <title>Note on High Dimensional Spatial-Sign Test for One Sample Problem</title>
      <link>https://arxiv.org/abs/2601.08736</link>
      <description>arXiv:2601.08736v1 Announce Type: new 
Abstract: We revisit the null distribution of the high-dimensional spatial-sign test of Wang et al. (2015) under mild structural assumptions on the scatter matrix. We show that the standardized test statistic converges to a non-Gaussian limit, characterized as a mixture of a normal component and a weighted chi-square component. To facilitate practical implementation, we propose a wild bootstrap procedure for computing critical values and establish its asymptotic validity. Numerical experiments demonstrate that the proposed bootstrap test delivers accurate size control across a wide range of dependence settings and dimension-sample-size regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08736v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ping Zhao, Long Feng</dc:creator>
    </item>
    <item>
      <title>Truncated Multidimensional Trigonometric Moment Problem: A Choice of Bases and the Unique Solution</title>
      <link>https://arxiv.org/abs/2601.08551</link>
      <description>arXiv:2601.08551v1 Announce Type: cross 
Abstract: In this prelinimary version of paper, we propose to give a complete solution to the Truncated Multidimensional Trigonometric Moment Problem (TMTMP) from a system and signal processing perspective. In mathematical TMTMPs, people care about whether a solution exists for a given sequence of multidimensional trigonometric moments. The solution can have the form of an atomic measure. However, for the TMTMPs in system and signal processing, a solution as an analytic rational function, of which the numerator and the denominator are positive polynomials, is desired for the ARMA modelling of a stochastic process, which is the so-called Multidimensional Rational Covariance Extension problem (RCEP) . In the literature, the feasible domain of the TMTMPs, where the spectral density is positive, is difficult to obtain given a specific choice of basis functions, which causes severe problems in the Multidimensional RCEP. In this paper, we propose a choice of basis functions, and a corresponding estimation scheme by convex optimization, for the TMTMPs, with which the trigonometric moments of the spectral estimate are exactly the sample moments. We propose an explicit condition for the convex optimization problem for guaranteeing the positiveness of the spectral estimation. The map from the parameters of the estimate to the trigonometric moments is proved to be a diffeomorphism, which ensures the existence and uniqueness of solution. The statistical properties of the proposed spectral density estimation scheme are comprehensively proved, including the consistency, (asymptotical) unbiasedness, convergence rate and efficiency under a mild assumption. This well-posed treatment is then applied to a system identification task, and the simulation results validate our proposed treatment for the TMTMP in system and signal processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08551v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guangyu Wu, Anders Lindquist</dc:creator>
    </item>
    <item>
      <title>Systemic Risk Surveillance</title>
      <link>https://arxiv.org/abs/2601.08598</link>
      <description>arXiv:2601.08598v1 Announce Type: cross 
Abstract: Following several episodes of financial market turmoil in recent decades, changes in systemic risk have drawn growing attention. Therefore, we propose surveillance schemes for systemic risk, which allow to detect misspecified systemic risk forecasts in an "online" fashion. This enables daily monitoring of the forecasts while controlling for the accumulation of false test rejections. Such online schemes are vital in taking timely countermeasures to avoid financial distress. Our monitoring procedures allow multiple series at once to be monitored, thus increasing the likelihood and the speed at which early signs of trouble may be picked up. The tests hold size by construction, such that the null of correct systemic risk assessments is only rejected during the monitoring period with (at most) a pre-specified probability. Monte Carlo simulations illustrate the good finite-sample properties of our procedures. An empirical application to US banks during multiple crises demonstrates the usefulness of our surveillance schemes for both regulators and financial institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08598v1</guid>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Dimitriadis, Yannick Hoga</dc:creator>
    </item>
    <item>
      <title>Robust low-rank estimation with multiple binary responses using pairwise AUC loss</title>
      <link>https://arxiv.org/abs/2601.08618</link>
      <description>arXiv:2601.08618v1 Announce Type: cross 
Abstract: Multiple binary responses arise in many modern data-analytic problems. Although fitting separate logistic regressions for each response is computationally attractive, it ignores shared structure and can be statistically inefficient, especially in high-dimensional and class-imbalanced regimes. Low-rank models offer a natural way to encode latent dependence across tasks, but existing methods for binary data are largely likelihood-based and focus on pointwise classification rather than ranking performance. In this work, we propose a unified framework for learning with multiple binary responses that directly targets discrimination by minimizing a surrogate loss for the area under the ROC curve (AUC). The method aggregates pairwise AUC surrogate losses across responses while imposing a low-rank constraint on the coefficient matrix to exploit shared structure. We develop a scalable projected gradient descent algorithm based on truncated singular value decomposition. Exploiting the fact that the pairwise loss depends only on differences of linear predictors, we simplify computation and analysis. We establish non-asymptotic convergence guarantees, showing that under suitable regularity conditions, leading to linear convergence up to the minimax-optimal statistical precision. Extensive simulation studies demonstrate that the proposed method is robust in challenging settings such as label switching and data contamination and consistently outperforms likelihood-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08618v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Stable Filtering for Efficient Dimensionality Reduction of Streaming Manifold Data</title>
      <link>https://arxiv.org/abs/2601.08685</link>
      <description>arXiv:2601.08685v1 Announce Type: cross 
Abstract: Many areas in science and engineering now have access to technologies that enable the rapid collection of overwhelming data volumes. While these datasets are vital for understanding phenomena from physical to biological and social systems, the sheer magnitude of the data makes even simple storage, transmission, and basic processing highly challenging. To enable efficient and accurate execution of these data processing tasks, we require new dimensionality reduction tools that 1) do not need expensive, time-consuming training, and 2) preserve the underlying geometry of the data that has the information required to understand the measured system. Specifically, the geometry to be preserved is that induced by the fact that in many applications, streaming high-dimensional data evolves on a low-dimensional attractor manifold. Importantly, we may not know the exact structure of this manifold a priori. To solve these challenges, we present randomized filtering (RF), which leverages a specific instantiation of randomized dimensionality reduction to provably preserve non-linear manifold structure in the embedded space while remaining data-independent and computationally efficient. In this work we build on the rich theoretical promise of randomized dimensionality reduction to develop RF as a real, practical approach. We introduce novel methods, analysis, and experimental verification to illuminate the practicality of RF in diverse scientific applications, including several simulated and real-data examples that showcase the tangible benefits of RF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08685v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas P. Bertrand, Eva Yezerets, Han Lun Yap, Adam S. Charles, Christopher J. Rozell</dc:creator>
    </item>
    <item>
      <title>Multiple Testing of Local Extrema for Detection of Structural Breaks in Piecewise Linear Models</title>
      <link>https://arxiv.org/abs/2308.04368</link>
      <description>arXiv:2308.04368v5 Announce Type: replace 
Abstract: In this paper, we propose a new generic method for detecting the number and locations of structural breaks or change points in piecewise linear models under stationary Gaussian noise. Our method transforms the change point detection problem into identifying local extrema (local maxima and local minima) through kernel smoothing and differentiation of the data sequence. By computing p-values for all local extrema based on peak height distributions of smooth Gaussian processes, we utilize the Benjamini-Hochberg procedure to identify significant local extrema as the detected change points. Our method can distinguish between two types of change points: continuous breaks (Type I) and jumps (Type II). We study three scenarios of piecewise linear signals, namely pure Type I, pure Type II and a mixture of Type I and Type II change points. The results demonstrate that our proposed method ensures asymptotic control of the False Discover Rate (FDR) and power consistency, as sequence length, slope changes, and jump size increase. Furthermore, compared to traditional change point detection methods based on recursive segmentation, our approach only requires a single test for all candidate local extrema, thereby achieving the smallest computational complexity proportionate to the data sequence length. Additionally, numerical studies illustrate that our method maintains FDR control and power consistency, even in non-asymptotic cases when the size of slope changes or jumps is not large. We have implemented our method in the R package "dSTEM" (available from https://cran.r-project.org/web/packages/dSTEM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04368v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibing He, Dan Cheng, Yunpeng Zhao</dc:creator>
    </item>
    <item>
      <title>A two-step approach to production frontier estimation and the Matsuoka's distribution</title>
      <link>https://arxiv.org/abs/2311.06086</link>
      <description>arXiv:2311.06086v3 Announce Type: replace 
Abstract: In this work, we introduce a deterministic frontier model in which efficiency is governed by the Matsuoka distribution, a parsimonious one-parameter specification on $(0,1)$ designed to reflect patterns typically observed in efficiency data. Based on this formulation, we develop a two-step semiparametric estimation procedure: a nonparametric smoothing for the regression component, followed by a feasible method of moments estimation for the efficiency parameter with plug-in reconstruction of the frontier. Theoretical results establish convergence rates, asymptotic normality, and an oracle property for the parametric estimator of the efficiency parameter. A Monte Carlo study demonstrates that the procedure performs consistently with the theoretical results and improves upon a fully nonparametric alternative. Applying the method to Brazilian temporary crops with land and agrochemicals as inputs, we find that both regions exhibit isoquants close to the constant elasticity substitution form, but differ in the relative productivity of inputs. Most notably, statistical tests provide evidence that the South is relatively more efficient than the Center-West, highlighting the empirical relevance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06086v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danilo Hiroshi Matsuoka, Guilherme Pumi, Hudson da Silva Torrent, Marcio valk</dc:creator>
    </item>
    <item>
      <title>Benchmarking multi-step methods for the dynamic prediction of survival with numerous longitudinal predictors</title>
      <link>https://arxiv.org/abs/2403.14336</link>
      <description>arXiv:2403.14336v3 Announce Type: replace 
Abstract: In recent years, the growing availability of biomedical datasets featuring numerous longitudinal covariates has motivated the development of several multi-step methods for the dynamic prediction of survival outcomes. These methods employ either mixed-effects models or multivariate functional principal component analysis to model and summarize the longitudinal covariates' evolution over time. Then, they use Cox models or random survival forests to predict survival probabilities, using as covariates both baseline variables and the summaries of the longitudinal variables obtained in the previous modelling step. Because these multi-step methods are still quite new, to date little is known about their applicability, limitations, and predictive performance when applied to real-world data. To gain a better understanding of these aspects, we performed a benchmarking of these multi-step methods (and two simpler prediction approaches) using three datasets that differ in sample size, number of longitudinal covariates and length of follow-up. We discuss the different modelling choices made by these methods, and some adjustments that one may need to do in order to be able to apply them to real-world data. Furthermore, we compare their predictive performance using multiple performance measures and landmark times, assess their computing time, and discuss their strengths and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14336v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1515/ijb-2025-0049</arxiv:DOI>
      <arxiv:journal_reference>The International Journal of Biostatistics (2025)</arxiv:journal_reference>
      <dc:creator>Mirko Signorelli, Sophie Retif</dc:creator>
    </item>
    <item>
      <title>tidychangepoint: A Unified Framework for Analyzing Changepoint Detection in Univariate Time Series</title>
      <link>https://arxiv.org/abs/2407.14369</link>
      <description>arXiv:2407.14369v3 Announce Type: replace 
Abstract: We present tidychangepoint, a new R package for changepoint detection analysis. Most R packages for segmenting univariate time series focus on providing one or two algorithms for changepoint detection that work with a small set of models and penalized objective functions, and all of them return a custom, nonstandard object type. This makes comparing results across various algorithms, models, and penalized objective functions unnecessarily difficult. tidychangepoint solves this problem by wrapping functions from a variety of existing packages and storing the results in a common S3 class called tidycpt. The package then provides functionality for easily extracting comparable numeric or graphical information from a tidycpt object, all in a tidyverse-compliant framework. tidychangepoint is versatile: it supports both deterministic algorithms like PELT (from changepoint), and also flexible, randomized, genetic algorithms (via GA) that -- via new functionality built into tidychangepoint -- can be used with any compliant model-fitting function and any penalized objective function. By bringing all of these disparate tools together in a cohesive fashion, tidychangepoint facilitates comparative analysis of changepoint detection algorithms and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14369v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin S. Baumer, Biviana Marcela Suarez Sierra</dc:creator>
    </item>
    <item>
      <title>The Conflict Graph Design: Estimating Causal Effects under Arbitrary Neighborhood Interference</title>
      <link>https://arxiv.org/abs/2411.10908</link>
      <description>arXiv:2411.10908v3 Announce Type: replace 
Abstract: A fundamental problem in network experiments is selecting an appropriate experimental design in order to precisely estimate a given causal effect of interest. In this work, we propose the Conflict Graph Design, a general approach for constructing experiment designs under network interference with the goal of precisely estimating a pre-specified causal effect. A central aspect of our approach is the notion of a conflict graph, which captures the fundamental unobservability associated with the causal effect and the underlying network. In order to estimate effects, we propose a modified Horvitz--Thompson estimator. We show that its variance under the Conflict Graph Design is bounded as $O(\lambda(H) / n )$, where $\lambda(H)$ is the largest eigenvalue of the adjacency matrix of the conflict graph. These rates depend on both the underlying network and the particular causal effect under investigation. Not only does this yield the best known rates of estimation for several well-studied causal effects (e.g. the global and direct effects) but it also provides new methods for effects which have received less attention from the perspective of experiment design (e.g. spill-over effects). Finally, we construct conservative variance estimators which facilitate asymptotically valid confidence intervals for the causal effect of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10908v3</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vardis Kandiros, Charilaos Pipis, Constantinos Daskalakis, Christopher Harshaw</dc:creator>
    </item>
    <item>
      <title>Simulating transgenerational hologenomes under selection with RITHMS</title>
      <link>https://arxiv.org/abs/2502.07366</link>
      <description>arXiv:2502.07366v5 Announce Type: replace 
Abstract: A holobiont is made up of a host organism together with its microbiota. In the context of animal breeding, the holobiont can be viewed as the single unit upon which selection operates. Therefore, integrating microbiota data into genomic prediction models may be a promising approach to improve predictions of phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic transgenerational data to address this hypothesis, and thus to fill this gap, we propose a new simulation framework. Our approach, an R Implementation of a Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source package. It builds upon simulated transgenerational genotypes from the Modular Breeding Program Simulator (MoBPS) package and incorporates distinctive characteristics of the microbiota, notably vertical and horizontal transmission as well as modulation due to the environment and host genetics. In addition, RITHMS can account for a variety of selection strategies and is adaptable to different genetic architectures. We simulated transgenerational hologenomic data using RITHMS under a wide variety of scenarios, varying heritability, microbiability, and microbiota transmissibility. We found that simulated data accurately preserved key characteristics across generations, notably microbial diversity metrics, exhibited the expected behavior in terms of correlation between taxa and of modulation of vertical and horizontal transmission, response to environmental effects and the evolution of phenotypic values depending on selection strategy. Our results support the relevance of our simulation framework and illustrate its possible use for building a selection index balancing genetic gain and microbial diversity and for evaluating the impact of partially observed microbiota data. RITHMS is an advanced, flexible tool for generating transgenerational hologenomes under selection that incorporate the complex interplay between genetics, microbiota and environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07366v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sol\`ene Pety (MaIAGE, GABI, INRAE), Ingrid David (GenPhySE, INRAE), Andrea Rau (GABI, INRAE), Mahendra Mariadassou (MaIAGE, INRAE)</dc:creator>
    </item>
    <item>
      <title>A Hybrid Framework Combining Autoregression and Common Factors for Matrix Time Series</title>
      <link>https://arxiv.org/abs/2503.05340</link>
      <description>arXiv:2503.05340v3 Announce Type: replace 
Abstract: Matrix-valued time series are ubiquitous in modern economics and finance, yet modeling them requires navigating a trade-off between flexibility and parsimony. We propose the Matrix Autoregressive model with Common Factors (MARCF), a unified framework for high-dimensional matrix time series that bridges the structural gap between the Matrix Autoregression (MAR) and Matrix Factor Model (MFM). While MAR typically assumes distinct predictor and response subspaces and MFM enforces identical ones, MARCF explicitly characterizes the intersection of these subspaces. By decomposing the coefficient matrices into common, predictor-specific, and response-specific components, the framework accommodates distinct input and output structures while exploiting their overlap for dimension reduction. We develop a regularized gradient descent estimator that is scalable for high-dimensional data and can efficiently handle the non-convex parameter space. Theoretical analysis establishes local linear convergence of the algorithm and statistical consistency of the estimator under high-dimensional scaling. The estimation efficiency and interpretability of the proposed methods are demonstrated through simulations and an application to global macroeconomic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05340v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyun Fan, Xiaoyu Zhang, Di Wang</dc:creator>
    </item>
    <item>
      <title>Nonparametric inference for ratios of densities via uniformly valid and powerful permutation tests</title>
      <link>https://arxiv.org/abs/2505.24529</link>
      <description>arXiv:2505.24529v2 Announce Type: replace 
Abstract: We propose the density ratio permutation test, a hypothesis test that assesses whether the ratio between two densities is proportional to a known function based on independent samples from each distribution. The test uses an efficient Markov Chain Monte Carlo scheme to draw weighted permutations of the pooled data, yielding exchangeable samples and finite sample validity. For power, if the statistic is an integral probability metric, our procedure is consistent under mild assumptions on the defining function class; specializing to a reproducing kernel Hilbert space, we introduce the shifted maximum mean discrepancy and prove minimax optimality of our test when a normalized difference between the densities lies in a Sobolev ball. We extend to the case of an unknown density ratio by estimating it on an independent training sample and derive type~I error bounds in terms of the estimation error as well as power results. This allows adapting our method to conditional two sample testing, making it a versatile tool for assessing covariate-shift and related assumptions, which frequently arise in transfer learning and causal inference. Finally, we validate our theoretical findings through experiments on both simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24529v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Bordino, Thomas B. Berrett</dc:creator>
    </item>
    <item>
      <title>Bayesian Smoothed Quantile Regression</title>
      <link>https://arxiv.org/abs/2508.01738</link>
      <description>arXiv:2508.01738v4 Announce Type: replace 
Abstract: The standard asymmetric Laplace framework for Bayesian quantile regression (BQR) suffers from a fundamental decision-theoretic misalignment, yielding biased finite-sample estimates, and precludes gradient-based computation due to non-smoothness. We propose Bayesian smoothed quantile regression (BSQR), a principled framework built on a kernel-smoothed, fully differentiable likelihood. Methodologically, the symmetrizing property of our objective reduces inferential bias and aligns the posterior mean with the true conditional quantile. Theoretically, we establish posterior consistency and a Bernstein--von Mises theorem under misspecification, delivering asymptotic normality and valid frequentist coverage via a generalized Wilks phenomenon, while guaranteeing global posterior existence unlike empirical likelihood approaches. Computationally, BSQR enables Hamiltonian Monte Carlo for BQR, alleviating high-dimensional mixing bottlenecks. In simulations, BSQR reduces out-of-sample prediction error by up to 50% and improves sampling efficiency by up to 80% relative to asymmetric Laplace benchmarks, with uniform and triangular kernels performing particularly well. In a financial application to asymmetric systemic risk, BSQR uncovers distinct regime shifts around the COVID-19 period and yields sharper yet well-calibrated predictive quantiles, underscoring its practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01738v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingqi Liu, Kangqiang Li, Tianxiao Pang</dc:creator>
    </item>
    <item>
      <title>Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design</title>
      <link>https://arxiv.org/abs/2511.19677</link>
      <description>arXiv:2511.19677v2 Announce Type: replace 
Abstract: Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19677v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Stockton, Michele Santacatterina, Soutrik Mandal, Charles M. Cleland, Erinn M. Hade, Nicholas Illenberger, Sharon Meropol, Andrea B. Troxel, Eva Petkova, Chang Yu, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem</title>
      <link>https://arxiv.org/abs/2512.16012</link>
      <description>arXiv:2512.16012v2 Announce Type: replace 
Abstract: Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a "reliability omission" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16012v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>JoonHo Lee</dc:creator>
    </item>
    <item>
      <title>Bayesian Additive Regression Tree Copula Processes for Scalable Distributional Prediction</title>
      <link>https://arxiv.org/abs/2601.04913</link>
      <description>arXiv:2601.04913v2 Announce Type: replace 
Abstract: We show how to construct the implied copula process of response values from a Bayesian additive regression tree (BART) model with prior on the leaf node variances. This copula process, defined on the covariate space, can be paired with any marginal distribution for the dependent variable to construct a flexible distributional BART model. Bayesian inference is performed via Markov chain Monte Carlo on an augmented posterior, where we show that key sampling steps can be realized as those of Chipman et al. (2010), preserving scalability and computational efficiency even though the copula process is high dimensional. The posterior predictive distribution from the copula process model is derived in closed form as the push-forward of the posterior predictive distribution of the underlying BART model with an optimal transport map. Under suitable conditions, we establish posterior consistency for the regression function and posterior means and prove convergence in distribution of the predictive process and conditional expectation. Simulation studies demonstrate improved accuracy of distributional predictions compared to the original BART model and leading benchmarks. Applications to five real datasets with 506 to 515,345 observations and 8 to 90 covariates further highlight the efficacy and scalability of our proposed BART copula process model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04913v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Martin Wenkel, Michael Stanley Smith, Nadja Klein</dc:creator>
    </item>
    <item>
      <title>Principal component-guided sparse reduced-rank regression</title>
      <link>https://arxiv.org/abs/2601.07202</link>
      <description>arXiv:2601.07202v2 Announce Type: replace 
Abstract: Reduced-rank regression estimates regression coefficients by imposing a low-rank constraint on the matrix of regression coefficients, thereby accounting for correlations among response variables. To further improve predictive accuracy and model interpretability, several regularized reduced-rank regression methods have been proposed. However, these existing methods cannot bias the regression coefficients toward the leading principal component directions while accounting for the correlation structure among explanatory variables. In addition, when the explanatory variables exhibit a group structure, the correlation structure within each group cannot be adequately incorporated. To overcome these limitations, we propose a new method that introduces pcLasso into the reduced-rank regression framework. The proposed method improves predictive accuracy by accounting for the correlation among response variables while strongly biasing the matrix of regression coefficients toward principal component directions with large variance. Furthermore, even in settings where the explanatory variables possess a group structure, the proposed method is capable of explicitly incorporating this structure into the estimation process. Finally, we illustrate the effectiveness of the proposed method through numerical simulations and real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07202v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanji Goto, Shintaro Yuki, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>CausAdv: A Causal-based Framework for Detecting Adversarial Examples</title>
      <link>https://arxiv.org/abs/2411.00839</link>
      <description>arXiv:2411.00839v2 Announce Type: replace-cross 
Abstract: Deep learning has led to tremendous success in computer vision, largely due to Convolutional Neural Networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations. This vulnerability of adversarial examples has has motivated research into improving model robustness through adversarial detection and defense methods. In this paper, we address the adversarial robustness of CNNs through causal reasoning. We propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns both causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. We then perform a statistical analysis of the filters' CI across clean and adversarial samples, to demonstrate that adversarial examples exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarial detection without the need to train a separate detector. Moreover, we illustrate the efficiency of causal explanations as a helpful detection tool by visualizing the extracted causal features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00839v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hichem Debbi</dc:creator>
    </item>
    <item>
      <title>Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization</title>
      <link>https://arxiv.org/abs/2504.09629</link>
      <description>arXiv:2504.09629v3 Announce Type: replace-cross 
Abstract: Layer-wise PTQ is a promising technique for compressing large language models (LLMs), due to its simplicity and effectiveness without requiring retraining. However, recent progress in this area is saturating, underscoring the need to revisit its core limitations and explore further improvements. We address this challenge by identifying a key limitation of existing layer-wise PTQ methods: the growth of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this fundamental issue, we propose Quantization Error Propagation (QEP), a general, lightweight, and scalable framework that enhances layer-wise PTQ by explicitly propagating quantization errors and compensating for accumulated errors. QEP also offers a tunable propagation mechanism that prevents overfitting and controls computational overhead, enabling the framework to adapt to various architectures and resource budgets. Extensive experiments on several LLMs demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher accuracy than existing methods. Notably, the gains are most pronounced in the extremely low-bit quantization regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09629v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yamato Arai, Yuma Ichikawa</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v5 Announce Type: replace-cross 
Abstract: We introduce a novel measure of dependence that captures the extent to which a random variable $Y$ is determined by a random vector $X$. The measure equals zero precisely when $Y$ and $X$ are independent, and it attains one exactly when $Y$ is almost surely a measurable function of $X$. We further extend this framework to define a measure of conditional dependence between $Y$ and $X$ given $Z$. We propose a simple and interpretable estimator with computational complexity comparable to classical correlation coefficients, including those of Pearson, Spearman, and Chatterjee. Leveraging this dependence measure, we develop a tuning-free, model-agnostic variable selection procedure and establish its consistency under appropriate sparsity conditions. Extensive experiments on synthetic and real datasets highlight the strong empirical performance of our methodology and demonstrate substantial gains over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v5</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>$\phi$-test: Global Feature Selection and Inference for Shapley Additive Explanations</title>
      <link>https://arxiv.org/abs/2512.07578</link>
      <description>arXiv:2512.07578v2 Announce Type: replace-cross 
Abstract: We propose $\phi$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $\phi$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $\phi$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $\phi$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07578v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh</dc:creator>
    </item>
    <item>
      <title>The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection</title>
      <link>https://arxiv.org/abs/2601.05371</link>
      <description>arXiv:2601.05371v2 Announce Type: replace-cross 
Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05371v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Shafiqul Islam, Shakti Prasad Padhy, Douglas Allaire, Raymundo Arr\'oyave</dc:creator>
    </item>
  </channel>
</rss>
