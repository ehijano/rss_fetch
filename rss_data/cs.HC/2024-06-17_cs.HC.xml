<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Let's Get to the Point: LLM-Supported Planning, Drafting, and Revising of Research-Paper Blog Posts</title>
      <link>https://arxiv.org/abs/2406.10370</link>
      <description>arXiv:2406.10370v1 Announce Type: new 
Abstract: Research-paper blog posts help scientists disseminate their work to a larger audience, but translating papers into this format requires substantial additional effort. Blog post creation is not simply transforming a long-form article into a short output, as studied in most prior work on human-AI summarization. In contrast, blog posts are typically full-length articles that require a combination of strategic planning grounded in the source document, well-organized drafting, and thoughtful revisions. Can tools powered by large language models (LLMs) assist scientists in writing research-paper blog posts? To investigate this question, we conducted a formative study (N=6) to understand the main challenges of writing such blog posts with an LLM: high interaction costs for 1) reviewing and utilizing the paper content and 2) recurrent sub-tasks of generating and modifying the long-form output. To address these challenges, we developed Papers-to-Posts, an LLM-powered tool that implements a new Plan-Draft-Revise workflow, which 1) leverages an LLM to generate bullet points from the full paper to help users find and select content to include (Plan) and 2) provides default yet customizable LLM instructions for generating and modifying text (Draft, Revise). Through a within-subjects lab study (N=20) and between-subjects deployment study (N=37 blog posts, 26 participants) in which participants wrote blog posts about their papers, we compared Papers-to-Posts to a strong baseline tool that provides an LLM-generated draft and access to free-form LLM prompting. Results show that Papers-to-Posts helped researchers to 1) write significantly more satisfying blog posts and make significantly more changes to their blog posts in a fixed amount of time without a significant change in cognitive load (lab) and 2) make more changes to their blog posts for a fixed number of writing actions (deployment).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10370v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marissa Radensky, Daniel S. Weld, Joseph Chee Chang, Pao Siangliulue, Jonathan Bragg</dc:creator>
    </item>
    <item>
      <title>Exploring Parent-Child Perceptions on Safety in Generative AI: Concerns, Mitigation Strategies, and Design Implications</title>
      <link>https://arxiv.org/abs/2406.10461</link>
      <description>arXiv:2406.10461v1 Announce Type: new 
Abstract: The widespread use of Generative Artificial Intelligence (GAI) among teenagers has led to significant misuse and safety concerns. To identify risks and understand parental controls challenges, we conducted a content analysis on Reddit and interviewed 20 participants (seven teenagers and 13 parents). Our study reveals a significant gap in parental awareness of the extensive ways children use GAI, such as interacting with character-based chatbots for emotional support or engaging in virtual relationships. Parents and children report differing perceptions of risks associated with GAI. Parents primarily express concerns about data collection, misinformation, and exposure to inappropriate content. In contrast, teenagers are more concerned about becoming addicted to virtual relationships with GAI, the potential misuse of GAI to spread harmful content in social groups, and the invasion of privacy due to unauthorized use of their personal data in GAI applications. The absence of parental control features on GAI platforms forces parents to rely on system-built controls, manually check histories, share accounts, and engage in active mediation. Despite these efforts, parents struggle to grasp the full spectrum of GAI-related risks and to perform effective real-time monitoring, mediation, and education. We provide design recommendations to improve parent-child communication and enhance the safety of GAI use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10461v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaman Yu, Tanusree Sharma, Melinda Hu, Justin Wang, Yang Wang</dc:creator>
    </item>
    <item>
      <title>LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot</title>
      <link>https://arxiv.org/abs/2406.10590</link>
      <description>arXiv:2406.10590v1 Announce Type: new 
Abstract: Developing domain-specific conversational agents (CAs) has been challenged by the need for extensive domain-focused data. Recent advancements in Large Language Models (LLMs) make them a viable option as a knowledge backbone. LLMs behaviour can be enhanced through prompting, instructing them to perform downstream tasks in a zero-shot fashion (i.e. without training). To this end, we incorporated structural knowledge into prompts and used prompted LLMs to build domain-specific voice-based CAs. We demonstrate this approach for the specific domain of textile circularity in form of the design, development, and evaluation of TextileBot. We present the design and development of the voice agent TextileBot and also the insights from an in-person user study (N=30) evaluating three variations of TextileBots. We analyse the human-agent interactions, combining quantitative and qualitative methods. Our results suggest that participants engaged in multi-turn conversations, and their perceptions of the three variation agents and respective interactions varied demonstrating the effectiveness of our prompt-based LLM approach. We discuss the dynamics of these interactions and their implications for designing future voice-based CAs. The results show that our method's potential for building domain-specific CAs. Furthermore, most participants engaged in multi-turn conversations, and their perceptions of the three voice agents and respective interactions varied demonstrating the effectiveness of our prompt-based LLM approach. We discuss the dynamics of these interactions and their implications for designing future voice-based CAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10590v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu Zhong, Elia Gatti, James Hardwick, Miriam Ribul, Youngjun Cho, Marianna Obrist</dc:creator>
    </item>
    <item>
      <title>From Computational to Conversational Notebooks</title>
      <link>https://arxiv.org/abs/2406.10636</link>
      <description>arXiv:2406.10636v1 Announce Type: new 
Abstract: Today, we see a drastic increase in LLM-based user interfaces to support users in various tasks. Also, in programming, we witness a productivity boost with features like LLM-supported code completion and conversational agents to generate code. In this work, we look at the future of computational notebooks by enriching them with LLM support. We propose a spectrum of support, from simple inline code completion to executable code that was the output of a conversation. We showcase five concrete examples for potential user interface designs and discuss their benefits and drawbacks. With this, we hope to inspire the future development of LLM-supported computational notebooks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10636v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Weber, Sven Mayer</dc:creator>
    </item>
    <item>
      <title>Exploring the Impact of AI-generated Image Tools on Professional and Non-professional Users in the Art and Design Fields</title>
      <link>https://arxiv.org/abs/2406.10640</link>
      <description>arXiv:2406.10640v1 Announce Type: new 
Abstract: The rapid proliferation of AI-generated image tools is transforming the art and design fields, challenging traditional notions of creativity and impacting both professional and non-professional users. For the purposes of this paper, we define 'professional users' as individuals who self-identified in our survey as 'artists,' 'designers,' 'filmmakers,' or 'art and design students,' and 'non-professional users' as individuals who self-identified as 'others.' This study explores how AI-generated image tools influence these different user groups. Through an online survey (N=380) comprising 173 professional users and 207 non-professional users, we examine differences in the utilization of AI tools, user satisfaction and challenges, applications in creative processes, perceptions and impacts, and acceptance levels. Our findings indicate persistent concerns about image quality, cost, and copyright issues. Additionally, the usage patterns of non-professional users suggest that AI tools have the potential to democratize creative processes, making art and design tasks more accessible to individuals without traditional expertise. This study provides insights into the needs of different user groups and offers recommendations for developing more user-centered AI tools, contributing to the broader discussion on the future of AI in the art and design fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10640v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuying Tang, Ningning Zhang, Mariana Ciancia, Zhigang Wang</dc:creator>
    </item>
    <item>
      <title>Character Animation in AR: Character Animation in AR: a mobile application development study</title>
      <link>https://arxiv.org/abs/2406.10732</link>
      <description>arXiv:2406.10732v1 Announce Type: new 
Abstract: Digital preservation of the cultural heritages is one of the major applications of various computer graphics and vision algorithms. The advancement in the AR/VR technologies is giving the cultural heritage preservation an interesting spin due to its immense visualization ability. The use of these technologies to digitally recreate heritage sites and art is becoming a popular trend. A project, called Indian Digital Heritage (IDH), for recreating the heritage site of Hampi, Karnataka during Vijaynagara empire ($1336$ - $1646$ CE) has been initiated by the Department of Science and Technology (DST) few years back. Immense work on surveying the site, collecting geographical and historic information about the life in Hampi, creating 3D models for buildings and people of Hampi and many other related tasks has been undertaken by various participants of this project. A major part of this project is to make tourists visiting Hampi visualize the life of people in ancient Hampi through any handy device. With such a requirement, the mobile AR based platform becomes a natural choice for developing any application for this purpose. We contributed to the project by developing an AR based mobile application to recreate a scene from Virupaksha Bazaar of Hampi with two components - author scene with augmented virtual contents at any scale, and visualize the same scene by reaching the physical location of augmentation. We develop an interactive application for the purpose of digitally recreating ancient Hampi. Though the focus of this work is not creating any static or dynamic content from scratch, it shows an interesting application of the content created in a real world scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10732v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sukanya Bhattacharjee, Parag Chaudhuri</dc:creator>
    </item>
    <item>
      <title>EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos</title>
      <link>https://arxiv.org/abs/2406.10750</link>
      <description>arXiv:2406.10750v1 Announce Type: new 
Abstract: Self-recording eating behaviors is a step towards a healthy lifestyle recommended by many health professionals. However, the current practice of manually recording eating activities using paper records or smartphone apps is often unsustainable and inaccurate. Smart glasses have emerged as a promising wearable form factor for tracking eating behaviors, but existing systems primarily identify when eating occurs without capturing details of the eating activities (E.g., what is being eaten). In this paper, we present EchoGuide, an application and system pipeline that leverages low-power active acoustic sensing to guide head-mounted cameras to capture egocentric videos, enabling efficient and detailed analysis of eating activities. By combining active acoustic sensing for eating detection with video captioning models and large-scale language models for retrieval augmentation, EchoGuide intelligently clips and analyzes videos to create concise, relevant activity records on eating. We evaluated EchoGuide with 9 participants in naturalistic settings involving eating activities, demonstrating high-quality summarization and significant reductions in video data needed, paving the way for practical, scalable eating activity tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10750v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vineet Parikh, Saif Mahmud, Devansh Agarwal, Ke Li, Fran\c{c}ois Guimbreti\`ere, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>A Bayesian dynamic stopping method for evoked response brain-computer interfacing</title>
      <link>https://arxiv.org/abs/2406.11081</link>
      <description>arXiv:2406.11081v1 Announce Type: new 
Abstract: As brain-computer interfacing (BCI) systems transition from assistive technology to more diverse applications, their speed, reliability, and user experience become increasingly important. Dynamic stopping methods enhance BCI system speed by deciding at any moment whether to output a result or wait for more information. Such approach leverages trial variance, allowing good trials to be detected earlier, thereby speeding up the process without significantly compromising accuracy. Existing dynamic stopping algorithms typically optimize measures such as symbols per minute (SPM) and information transfer rate (ITR). However, these metrics may not accurately reflect system performance for specific applications or user types. Moreover, many methods depend on arbitrary thresholds or parameters that require extensive training data. We propose a model-based approach that takes advantage of the analytical knowledge that we have about the underlying classification model. By using a risk minimisation approach, our model allows precise control over the types of errors and the balance between precision and speed. This adaptability makes it ideal for customizing BCI systems to meet the diverse needs of various applications. We validate our proposed method on a publicly available dataset, comparing it with established static and dynamic stopping methods. Our results demonstrate that our approach offers a broad range of accuracy-speed trade-offs and achieves higher precision than baseline stopping methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11081v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Ahmadi, Peter Desain, Jordy Thielen</dc:creator>
    </item>
    <item>
      <title>Conversational Agents as Catalysts for Critical Thinking: Challenging Design Fixation in Group Design</title>
      <link>https://arxiv.org/abs/2406.11125</link>
      <description>arXiv:2406.11125v1 Announce Type: new 
Abstract: This paper investigates the potential of LLM-based conversational agents (CAs) to enhance critical reflection and mitigate design fixation in group design work. By challenging AI-generated recommendations and prevailing group opinions, these agents address issues such as groupthink and promote a more dynamic and inclusive design process. Key design considerations include optimizing intervention timing, ensuring clarity in counterarguments, and balancing critical thinking with designers' satisfaction. CAs can also adapt to various roles, supporting individual and collective reflection. Our work aligns with the "Death of the Design Researcher?" workshop's goals, emphasizing the transformative potential of generative AI in reshaping design practices and promoting ethical considerations. By exploring innovative uses of generative AI in group design contexts, we aim to stimulate discussion and open new pathways for future research and development, ultimately contributing to practical tools and resources for design researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11125v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Soohwan Lee, Seoyeong Hwang, Kyungho Lee</dc:creator>
    </item>
    <item>
      <title>Eliciting New Perspectives in RtD Studies through Annotated Portfolios: A Case Study of Robotic Artefacts</title>
      <link>https://arxiv.org/abs/2406.11133</link>
      <description>arXiv:2406.11133v1 Announce Type: new 
Abstract: In this paper, we investigate how to elicit new perspectives in research-through-design (RtD) studies through annotated portfolios. Situating the usage in human-robot interaction (HRI), we used two robotic artefacts as a case study: we first created our own annotated portfolio and subsequently ran online workshops during which we asked HRI experts to annotate our robotic artefacts. We report on the different aspects revealed about the value, use, and further improvements of the robotic artefacts through using the annotated portfolio technique ourselves versus using it with experts. We suggest that annotated portfolios - when performed by external experts - allow design researchers to obtain a form of creative and generative peer critique. Our paper offers methodological considerations for conducting expert annotation sessions. Further, we discuss the use of annotated portfolios to unveil designerly HRI knowledge in RtD studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11133v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3461778.3462134</arxiv:DOI>
      <dc:creator>Marius Hoggenmuller, Wen-Ying Lee, Luke Hespanhol, Malte Jung, Martin Tomitsch</dc:creator>
    </item>
    <item>
      <title>Towards Understanding Emotions for Engaged Mental Health Conversations</title>
      <link>https://arxiv.org/abs/2406.11135</link>
      <description>arXiv:2406.11135v1 Announce Type: new 
Abstract: Providing timely support and intervention is crucial in mental health settings. As the need to engage youth comfortable with texting increases, mental health providers are exploring and adopting text-based media such as chatbots, community-based forums, online therapies with licensed professionals, and helplines operated by trained responders. To support these text-based media for mental health--particularly for crisis care--we are developing a system to perform passive emotion-sensing using a combination of keystroke dynamics and sentiment analysis. Our early studies of this system posit that the analysis of short text messages and keyboard typing patterns can provide emotion information that may be used to support both clients and responders. We use our preliminary findings to discuss the way forward for applying AI to support mental health providers in providing better care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11135v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3656156.3663694</arxiv:DOI>
      <dc:creator>Kellie Yu Hui Sim, Kohleen Tijing Fortuno, Kenny Tsu Wei Choo</dc:creator>
    </item>
    <item>
      <title>Designing Interactions with Autonomous Physical Systems</title>
      <link>https://arxiv.org/abs/2406.11146</link>
      <description>arXiv:2406.11146v1 Announce Type: new 
Abstract: In this position paper, we present a collection of four different prototyping approaches which we have developed and applied to prototype and evaluate interfaces for and interactions around autonomous physical systems. Further, we provide a classification of our approaches aiming to support other researchers and designers in choosing appropriate prototyping platforms and representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11146v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marius Hoggenmueller, Tram Thi Minh Tran, Luke Hespanhol, Martin Tomitsch</dc:creator>
    </item>
    <item>
      <title>Optimum signal duration for Human Activity Recognition based on Deep Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2406.11164</link>
      <description>arXiv:2406.11164v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) stands as a pivotal technique within pattern recognition, dedicated to deciphering human movements and actions utilizing one or multiple sensory inputs. Its significance extends across diverse applications, encompassing monitoring, security protocols, and the development of human-in-the-loop technologies. However, prevailing studies in HAR often overlook the integration of human-centered devices, wherein distinct parameters and criteria hold varying degrees of importance compared to other applications. Notably, within this realm, curtailing the sensor observation period assumes paramount importance to safeguard the efficiency of exoskeletons and prostheses. This study embarks on the optimization of this observation period specifically tailored for HAR using Inertial Measurement Unit (IMU) sensors. Employing a Deep Convolutional Neural Network (DCNN), the aim is to identify activities based on segments of IMU signals spanning durations from 0.1 to 4 seconds. Intriguingly, the outcomes spotlight an optimal observation duration of 0.5 seconds, showcasing an impressive classification accuracy of 99.95%. This revelation holds immense significance, elucidating the criticality of precise temporal analysis within HAR, particularly concerning human-centric devices. Such findings not only enhance our understanding of the optimal observation period but also lay the groundwork for refining the performance and efficacy of devices crucially relied upon for aiding human mobility and functionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11164v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Farhad Nazari, Arian Shajari, Darius Nahavandi, Navid Mohajer</dc:creator>
    </item>
    <item>
      <title>Expanding the Design Space of Computer Vision-based Interactive Systems for Group Dance Practice</title>
      <link>https://arxiv.org/abs/2406.11236</link>
      <description>arXiv:2406.11236v1 Announce Type: new 
Abstract: Group dance, a sub-genre characterized by intricate motions made by a cohort of performers in tight synchronization, has a longstanding and culturally significant history and, in modern forms such as cheerleading, a broad base of current adherents. However, despite its popularity, learning group dance routines remains challenging. Based on the prior success of interactive systems to support individual dance learning, this paper argues that group dance settings are fertile ground for augmentation by interactive aids. To better understand these design opportunities, this paper presents a sequence of user-centered studies of and with amateur cheerleading troupes, spanning from the formative (interviews, observations) through the generative (an ideation workshop) to concept validation (technology probes and speed dating). The outcomes are a nuanced understanding of the lived practice of group dance learning, a set of interactive concepts to support those practices, and design directions derived from validating the proposed concepts. Through this empirical work, we expand the design space of interactive dance practice systems from the established context of single-user practice (primarily focused on gesture recognition) to a multi-user, group-based scenario focused on feedback and communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11236v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643834.3661568</arxiv:DOI>
      <arxiv:journal_reference>ACM Designing Interactive Systems Conference, 2024, (DIS '24)</arxiv:journal_reference>
      <dc:creator>Soohwan Lee, Seoyeong Hwang, Ian Oakley, Kyungho Lee</dc:creator>
    </item>
    <item>
      <title>PyGWalker: On-the-fly Assistant for Exploratory Visual Data Analysis</title>
      <link>https://arxiv.org/abs/2406.11637</link>
      <description>arXiv:2406.11637v1 Announce Type: new 
Abstract: Exploratory visual data analysis tools empower data analysts to efficiently and intuitively explore data insights throughout the entire analysis cycle. However, the gap between common programmatic analysis (e.g., within computational notebooks) and exploratory visual analysis leads to a disjointed and inefficient data analysis experience. To bridge this gap, we developed PyGWalker, a Python library that offers on-the-fly assistance for exploratory visual data analysis. It features a lightweight and intuitive GUI with a shelf builder modality. Its loosely coupled architecture supports multiple computational environments to accommodate varying data sizes. Since its release in February 2023, PyGWalker has gained much attention, with 612k downloads on PyPI and over 10.5k stars on GitHub as of June 2024. This demonstrates its value to the data science and visualization community, with researchers and developers integrating it into their own applications and studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11637v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yue Yu, Leixian Shen, Fei Long, Huamin Qu, Hao Chen</dc:creator>
    </item>
    <item>
      <title>SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking</title>
      <link>https://arxiv.org/abs/2406.11645</link>
      <description>arXiv:2406.11645v1 Announce Type: new 
Abstract: Seams are areas of overlapping fabric formed by stitching two or more pieces of fabric together in the cut-and-sew apparel manufacturing process. In SeamPose, we repurposed seams as capacitive sensors in a shirt for continuous upper-body pose estimation. Compared to previous all-textile motion-capturing garments that place the electrodes on the surface of clothing, our solution leverages existing seams inside of a shirt by machine-sewing insulated conductive threads over the seams. The unique invisibilities and placements of the seams afford the sensing shirt to look and wear the same as a conventional shirt while providing exciting pose-tracking capabilities. To validate this approach, we implemented a proof-of-concept untethered shirt. With eight capacitive sensing seams, our customized deep-learning pipeline accurately estimates the upper-body 3D joint positions relative to the pelvis. With a 12-participant user study, we demonstrated promising cross-user and cross-session tracking performance. SeamPose represents a step towards unobtrusive integration of smart clothing for everyday pose estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11645v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianhong Catherine Yu (Mary),  Manru (Mary),  Zhang, Peter He, Chi-Jung Lee, Cassidy Cheesman, Saif Mahmud, Ruidong Zhang, Fran\c{c}ois Guimbreti\`ere, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>Towards commands recommender system in BIM authoring tool using transformers</title>
      <link>https://arxiv.org/abs/2406.10237</link>
      <description>arXiv:2406.10237v1 Announce Type: cross 
Abstract: The complexity of BIM software presents significant barriers to the widespread adoption of BIM and model-based design within the Architecture, Engineering, and Construction (AEC) sector. End-users frequently express concerns regarding the additional effort required to create a sufficiently detailed BIM model when compared with conventional 2D drafting. This study explores the potential of sequential recommendation systems to accelerate the BIM modeling process. By treating BIM software commands as recommendable items, we introduce a novel end-to-end approach that predicts the next-best command based on user historical interactions. Our framework extensively preprocesses real-world, large-scale BIM log data, utilizes the transformer architectures from the latest large language models as the backbone network, and ultimately results in a prototype that provides real-time command suggestions within the BIM authoring tool Vectorworks. Subsequent experiments validated that our proposed model outperforms the previous study, demonstrating the immense potential of the recommendation system in enhancing design efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10237v1</guid>
      <category>cs.IR</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Changyu Du, Zihan Deng, Stavros Nousias, Andr\'e Borrmann</dc:creator>
    </item>
    <item>
      <title>Autograding Mathematical Induction Proofs with Natural Language Processing</title>
      <link>https://arxiv.org/abs/2406.10268</link>
      <description>arXiv:2406.10268v1 Announce Type: cross 
Abstract: In mathematical proof education, there remains a need for interventions that help students learn to write mathematical proofs. Research has shown that timely feedback can be very helpful to students learning new skills. While for many years natural language processing models have struggled to perform well on tasks related to mathematical texts, recent developments in natural language processing have created the opportunity to complete the task of giving students instant feedback on their mathematical proofs. In this paper, we present a set of training methods and models capable of autograding freeform mathematical proofs by leveraging existing large language models and other machine learning techniques. The models are trained using proof data collected from four different proof by induction problems. We use four different robust large language models to compare their performances, and all achieve satisfactory performances to various degrees. Additionally, we recruit human graders to grade the same proofs as the training data, and find that the best grading model is also more accurate than most human graders. With the development of these grading models, we create and deploy an autograder for proof by induction problems and perform a user study with students. Results from the study shows that students are able to make significant improvements to their proofs using the feedback from the autograder, but students still do not trust the AI autograders as much as they trust human graders. Future work can improve on the autograder feedback and figure out ways to help students trust AI autograders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10268v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyan Zhao, Mariana Silva, Seth Poulsen</dc:creator>
    </item>
    <item>
      <title>Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis</title>
      <link>https://arxiv.org/abs/2406.10273</link>
      <description>arXiv:2406.10273v1 Announce Type: cross 
Abstract: Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks. Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in Risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis. Method. We manually curated \totalscenarios unique scenarios leading to \totalsamples representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other three human experts to review the models and the former human expert's analysis. The reviewers analyzed 5,000 scenario analyses. Results and Conclusions. HEs demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs for an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10273v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi</dc:creator>
    </item>
    <item>
      <title>Advancing Robot-Assisted Autism Therapy: A Novel Algorithm for Enhancing Joint Attention Interventions</title>
      <link>https://arxiv.org/abs/2406.10392</link>
      <description>arXiv:2406.10392v1 Announce Type: cross 
Abstract: Recent studies have revealed that using social robots can accelerate the learning process of several skills in areas where autistic children typically show deficits. However, most early research studies conducted interactions via free play. More recent research has demonstrated that robot-mediated autism therapies focusing on core impairments of autism spectrum disorder (e.g., joint attention) yield better results than unstructured interactions. This paper aims to systematically review the most relevant findings concerning the application of social robotics to joint attention tasks, a cardinal feature of autism spectrum disorder that significantly influences the neurodevelopmental trajectory of autistic children. Initially, we define autism spectrum disorder and explore its societal implications. Following this, we examine the need for technological aid and the potentialities of robot-assisted autism therapy. We then define joint attention and highlight its crucial role in children's social and cognitive development. Subsequently, we analyze the importance of structured interactions and the role of selecting the optimal robot for specific tasks. This is followed by a comparative analysis of the works reviewed earlier, presenting an in-depth examination of two distinct formal models employed to design the prompts and reward system that enables the robot to adapt to children's responses. These models are critically compared to highlight their strengths and limitations. Next, we introduce a novel algorithm to address the identified limitations, integrating interactive environmental factors and a more sophisticated prompting and reward system. Finally, we propose further research directions, discuss the most relevant open questions, and draw conclusions regarding the effectiveness of social robotics in the medical treatment of autism spectrum disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10392v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Giannetti</dc:creator>
    </item>
    <item>
      <title>Interpreting Multi-objective Evolutionary Algorithms via Sokoban Level Generation</title>
      <link>https://arxiv.org/abs/2406.10663</link>
      <description>arXiv:2406.10663v1 Announce Type: cross 
Abstract: This paper presents an interactive platform to interpret multi-objective evolutionary algorithms. Sokoban level generation is selected as a showcase for its widespread use in procedural content generation. By balancing the emptiness and spatial diversity of Sokoban levels, we illustrate the improved two-archive algorithm, Two_Arch2, a well-known multi-objective evolutionary algorithm. Our web-based platform integrates Two_Arch2 into an interface that visually and interactively demonstrates the evolutionary process in real-time. Designed to bridge theoretical optimisation strategies with practical game generation applications, the interface is also accessible to both researchers and beginners to multi-objective evolutionary algorithms or procedural content generation on a website. Through dynamic visualisations and interactive gameplay demonstrations, this web-based platform also has potential as an educational tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10663v1</guid>
      <category>cs.NE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingquan Zhang, Yuchen Li, Yuhang Lin, Handing Wang, Jialin Liu</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Automatic Milestone Detection in Group Discussions</title>
      <link>https://arxiv.org/abs/2406.10842</link>
      <description>arXiv:2406.10842v1 Announce Type: cross 
Abstract: Large language models like GPT have proven widely successful on natural language understanding tasks based on written text documents. In this paper, we investigate an LLM's performance on recordings of a group oral communication task in which utterances are often truncated or not well-formed. We propose a new group task experiment involving a puzzle with several milestones that can be achieved in any order. We investigate methods for processing transcripts to detect if, when, and by whom a milestone has been completed. We demonstrate that iteratively prompting GPT with transcription chunks outperforms semantic similarity search methods using text embeddings, and further discuss the quality and randomness of GPT responses under different context window sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10842v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhuoxu Duan, Zhengye Yang, Samuel Westby, Christoph Riedl, Brooke Foucault Welles, Richard J. Radke</dc:creator>
    </item>
    <item>
      <title>A transparency-based action model implemented in a robotic physical trainer for improved HRI</title>
      <link>https://arxiv.org/abs/2406.10848</link>
      <description>arXiv:2406.10848v1 Announce Type: cross 
Abstract: Transparency is an important aspect of human-robot interaction (HRI), as it can improve system trust and usability leading to improved communication and performance. However, most transparency models focus only on the amount of information given to users. In this paper, we propose a bidirectional transparency model, termed a transparency-based action (TBA) model, which in addition to providing transparency information (robot-to-human), allows the robot to take actions based on transparency information received from the human (robot-of-human and human-to-robot). We implemented a three-level (High, Medium and Low) TBA model on a robotic system trainer in two pilot studies (with students as participants) to examine its impact on acceptance and HRI. Based on the pilot studies results, the Medium TBA level was not included in the main experiment, which was conducted with older adults (aged 75-85). In that experiment, two TBA levels were compared: Low (basic information including only robot-to-human transparency) and High (including additional information relating to predicted outcomes with robot-of-human and human-to-robot transparency). The results revealed a significant difference between the two TBA levels of the model in terms of perceived usefulness, ease of use, and attitude. The High TBA level resulted in improved user acceptance and was preferred by the users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10848v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aharony Naama, Krakovski Maya, Edan Yael</dc:creator>
    </item>
    <item>
      <title>Beyond Answers: Large Language Model-Powered Tutoring System in Physics Education for Deep Learning and Precise Understanding</title>
      <link>https://arxiv.org/abs/2406.10934</link>
      <description>arXiv:2406.10934v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) in education has shown significant promise, yet the effective personalization of learning, particularly in physics education, remains a challenge. This paper proposes Physics-STAR, a framework for large language model (LLM)- powered tutoring system designed to address this gap by providing personalized and adaptive learning experiences for high school students. Our study evaluates Physics-STAR against traditional teacher-led lectures and generic LLM tutoring through a controlled experiment with 12 high school sophomores. Results showed that Physics-STAR increased students' average scores and efficiency on conceptual, computational, and on informational questions. In particular, students' average scores on complex information problems increased by 100% and their efficiency increased by 5.95%. By facilitating step-by-step guidance and reflective learning, Physics-STAR helps students develop critical thinking skills and a robust comprehension of abstract concepts. The findings underscore the potential of AI-driven personalized tutoring systems to transform physics education. As LLM continues to advance, the future of student-centered AI in education looks promising, with the potential to significantly improve learning outcomes and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10934v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhoumingju Jiang, Mengjun Jiang</dc:creator>
    </item>
    <item>
      <title>Implementing dynamic high-performance computing supported workflows on Scanning Transmission Electron Microscope</title>
      <link>https://arxiv.org/abs/2406.11018</link>
      <description>arXiv:2406.11018v1 Announce Type: cross 
Abstract: Scanning Transmission Electron Microscopy (STEM) coupled with Electron Energy Loss Spectroscopy (EELS) presents a powerful platform for detailed material characterization via rich imaging and spectroscopic data. Modern electron microscopes can access multiple length scales and sampling rates far beyond human perception and reaction time. Recent advancements in machine learning (ML) offer a promising avenue to enhance these capabilities by integrating ML algorithms into the STEM-EELS framework, fostering an environment of active learning. This work enables the seamless integration of STEM with High-Performance Computing (HPC) systems. We present several implemented workflows that exemplify this integration. These workflows include sophisticated techniques such as object finding and Deep Kernel Learning (DKL). Through these developments, we demonstrate how the fusion of STEM-EELS with ML and HPC enhances the efficiency and scope of material characterization for 70% STEM available globally. The codes are available at GitHub link.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11018v1</guid>
      <category>physics.ins-det</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Utkarsh Pratiush, Austin Houston, Sergei V Kalinin, Gerd Duscher</dc:creator>
    </item>
    <item>
      <title>Robots in Family Routines: Development of and Initial Insights from the Family-Robot Routines Inventory</title>
      <link>https://arxiv.org/abs/2406.11136</link>
      <description>arXiv:2406.11136v1 Announce Type: cross 
Abstract: Despite advances in areas such as the personalization of robots, sustaining adoption of robots for long-term use in families remains a challenge. Recent studies have identified integrating robots into families' routines and rituals as a promising approach to support long-term adoption. However, few studies explored the integration of robots into family routines and there is a gap in systematic measures to capture family preferences for robot integration. Building upon existing routine inventories, we developed Family-Robot Routines Inventory (FRRI), with 24 family routines and 24 child routine items, to capture parents' attitudes toward and expectations from the integration of robotic technology into their family routines. Using this inventory, we collected data from 150 parents through an online survey. Our analysis indicates that parents had varying perceptions for the utility of integrating robots into their routines. For example, parents found robot integration to be more helpful in children's individual routines, than to the collective routines of their families. We discuss the design implications of these preliminary findings, and how they may serve as a first step toward understanding the diverse challenges and demands of designing and integrating household robots for families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11136v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael F. Xu, Bengisu Cagiltay, Joseph Michaelis, Sarah Sebo, Bilge Mutlu</dc:creator>
    </item>
    <item>
      <title>An Initial Study Review of Designing a Technology Solution for Women in Technologically Deprived Areas or Low Resource Constraint Communities</title>
      <link>https://arxiv.org/abs/2406.11186</link>
      <description>arXiv:2406.11186v1 Announce Type: cross 
Abstract: In the West African country of Ghana, depression is a significant issue affecting a large number of women. Despite its importance, the issue received insufficient attention during the COVID-19 pandemic. In developed countries, mobile phones serve as a convenient medium for accessing health information and providers. However, in Ghana, women's access to mobile phones is limited by cultural, social, and financial constraints, hindering their ability to seek mental health information and support. While some women in deprived areas can afford feature phones, such as the Nokia 3310, the lack of advanced smartphone features further restricts their access to necessary health information. This paper reviews the potential of Unstructured Supplementary Service Data (USSD) technology to address these challenges. Unlike Short Messaging Service (SMS), USSD can facilitate data collection, complex transactions, and provide information access without the need for internet connectivity. This research proposes studying the use of USSD to improve access to mental health resources for resource-deprived women in Ghana.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11186v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jones Yeboah, Sophia Bampoh, Annu Sible Prabhakar</dc:creator>
    </item>
    <item>
      <title>GUICourse: From General Vision Language Models to Versatile GUI Agents</title>
      <link>https://arxiv.org/abs/2406.11317</link>
      <description>arXiv:2406.11317v1 Announce Type: cross 
Abstract: Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11317v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, Maosong Sun</dc:creator>
    </item>
    <item>
      <title>ESI-GAL: EEG Source Imaging-based Kinemaics Parameter Estimation for Grasp and Lift Task</title>
      <link>https://arxiv.org/abs/2406.11500</link>
      <description>arXiv:2406.11500v1 Announce Type: cross 
Abstract: Objective: Electroencephalogram (EEG) signals-based motor kinematics prediction (MKP) has been an active area of research to develop brain-computer interface (BCI) systems such as exosuits, prostheses, and rehabilitation devices. However, EEG source imaging (ESI) based kinematics prediction is sparsely explored in the literature. Approach: In this study, pre-movement EEG features are utilized to predict three-dimensional (3D) hand kinematics for the grasp-and-lift motor task. A public dataset, WAY-EEG-GAL, is utilized for MKP analysis. In particular, sensor-domain (EEG data) and source-domain (ESI data) based features from the frontoparietal region are explored for MKP. Deep learning-based models are explored to achieve efficient kinematics decoding. Various time-lagged and window sizes are analyzed for hand kinematics prediction. Subsequently, intra-subject and inter-subject MKP analysis is performed to investigate the subject-specific and subject-independent motor-learning capabilities of the neural decoders. The Pearson correlation coefficient (PCC) is used as the performance metric for kinematics trajectory decoding. Main results: The rEEGNet neural decoder achieved the best performance with sensor-domain and source-domain features with the time lag and window size of 100 ms and 450 ms, respectively. The highest mean PCC values of 0.790, 0.795, and 0.637 are achieved using sensor-domain features, while 0.769, 0.777, and 0.647 are achieved using source-domain features in x, y, and z-directions, respectively. Significance: This study explores the feasibility of trajectory prediction using EEG sensor-domain and source-domain EEG features for the grasp-and-lift task. Furthermore, inter-subject trajectory estimation is performed using the proposed deep learning decoder with EEG source domain features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11500v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anant Jain, Lalan Kumar</dc:creator>
    </item>
    <item>
      <title>STAR: SocioTechnical Approach to Red Teaming Language Models</title>
      <link>https://arxiv.org/abs/2406.11757</link>
      <description>arXiv:2406.11757v1 Announce Type: cross 
Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11757v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</dc:creator>
    </item>
    <item>
      <title>Folk-ontological stances toward robots and psychological human likeness</title>
      <link>https://arxiv.org/abs/2406.11759</link>
      <description>arXiv:2406.11759v1 Announce Type: cross 
Abstract: It has often been argued that people can attribute mental states to robots without making any ontological commitments to the reality of those states. But what does it mean to 'attribute' a mental state to a robot, and what is an 'ontological commitment'? It will be argued that, on a plausible interpretation of these two notions, it is not clear how mental state attribution can occur without any ontological commitment. Taking inspiration from the philosophical debate on scientific realism, a provisional taxonomy of folk-ontological stances towards robots will also be identified, corresponding to different ways of understanding robotic minds. They include realism, non-realism, eliminativism, reductionism, fictionalism and agnosticism. Instrumentalism will also be discussed and presented as a folk-epistemological stance. In the last part of the article it will be argued that people's folk-ontological stances towards robots and humans can influence their perception of the human-likeness of robots. The analysis carried out here can be seen as encouraging a 'folk-ontological turn' in human-robot interaction research, aimed at explicitly determining what beliefs people have about the reality of robot minds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11759v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Edoardo Datteri</dc:creator>
    </item>
    <item>
      <title>Unleashing GPT on the Metaverse: Savior or Destroyer?</title>
      <link>https://arxiv.org/abs/2303.13856</link>
      <description>arXiv:2303.13856v3 Announce Type: replace 
Abstract: Incorporating artificial intelligence (AI) technology, particularly large language models (LLMs), is becoming increasingly vital for developing immersive and interactive metaverse experiences. GPT, a representative LLM developed by OpenAI, is leading LLM development and gaining attention for its potential in building the metaverse. The article delves into the pros and cons of utilizing GPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of GPT, according to its unique technological advantages, on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13856v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Realizing Immersive Communications in Human Digital Twin by Edge Computing Empowered Tactile Internet: Visions and Case Study</title>
      <link>https://arxiv.org/abs/2304.07454</link>
      <description>arXiv:2304.07454v3 Announce Type: replace 
Abstract: Human digital twin (HDT) is expected to revolutionize the future human lifestyle and prompts the development of advanced human-centric applications (e.g., Metaverse) by bridging physical and virtual spaces. However, the fulfillment of HDT poses stringent demands on the pervasive connectivity, real-time feedback, multi-modal data transmission and ultra-high reliability, which urge the need of enabling immersive communications. In this article, we shed light on the design of an immersive communication framework for HDT by edge computing empowered tactile Internet (namely IC-HDT-ECoTI). Aiming at offering strong interactions and extremely immersive quality of experience, we introduce the system architecture of IC-HDT-ECoTI, and analyze its major design requirements and challenges. Moreover, we present core guidelines and detailed steps for system implementations. In addition, we conduct an experimental study based on our recently built testbed, which shows a particular use case of IC-HDT-ECoTI in physical therapy, and the obtained results indicate that the proposed framework can significantly improve the effectiveness of the system. Finally, we conclude this article with a brief discussion of open issues and future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07454v3</guid>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Xiang (Sherman), Changyan Yi (Sherman), Kun Wu (Sherman), Jiayuan Chen (Sherman), Jun Cai (Sherman), Dusit Niyato (Sherman),  Xuemin (Sherman),  Shen</dc:creator>
    </item>
    <item>
      <title>Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities</title>
      <link>https://arxiv.org/abs/2404.02718</link>
      <description>arXiv:2404.02718v3 Announce Type: replace 
Abstract: Human-like Agents with diverse and dynamic personalities could serve as an essential design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive applications. In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior. The Personality system includes Cognition, Emotion, and Character Growth modules. The Behavior system comprises two modules: Planning and Action. We also build a simulation platform that enables agents to interact with the environment and other agents. Evolving Agents can simulate the human personality evolution process. Compared to its initial state, agents' personality and behavior patterns undergo believable development after several days of simulation. Agents reflect on their behavior to reason and develop new personality traits. These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution. Our experiment utilized a simulation platform with ten agents for evaluation. During the assessment, these agents experienced believable and inspirational personality evolution. Through ablation and control experiments, we demonstrated the effectiveness of agent personality evolution, and all of our agent architecture modules contribute to creating believable human-like agents with diverse and dynamic personalities. We also demonstrated through workshops how Evolving Agents could inspire designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02718v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiale Li, Jiayang Li, Jiahao Chen, Yifan Li, Shijie Wang, Hugo Zhou, Minjun Ye, Yunsheng Su</dc:creator>
    </item>
    <item>
      <title>Majority Voting of Doctors Improves Appropriateness of AI Reliance in Pathology</title>
      <link>https://arxiv.org/abs/2404.04485</link>
      <description>arXiv:2404.04485v3 Announce Type: replace 
Abstract: As Artificial Intelligence (AI) making advancements in medical decision-making, there is a growing need to ensure doctors develop appropriate reliance on AI to avoid adverse outcomes. However, existing methods in enabling appropriate AI reliance might encounter challenges while being applied in the medical domain. With this regard, this work employs and provides the validation of an alternative approach -- majority voting -- to facilitate appropriate reliance on AI in medical decision-making. This is achieved by a multi-institutional user study involving 32 medical professionals with various backgrounds, focusing on the pathology task of visually detecting a pattern, mitoses, in tumor images. Here, the majority voting process was conducted by synthesizing decisions under AI assistance from a group of pathology doctors (pathologists). Two metrics were used to evaluate the appropriateness of AI reliance: Relative AI Reliance (RAIR) and Relative Self-Reliance (RSR). Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR -- by approximately 9% and 31%, respectively -- compared to decisions made by one pathologist collaborating with AI. This increased appropriateness resulted in better precision and recall in the detection of mitoses. While our study is centered on pathology, we believe these insights can be extended to general high-stakes decision-making processes involving similar visual tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04485v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyan Gu, Chunxu Yang, Shino Magaki, Neda Zarrin-Khameh, Nelli S. Lakis, Inma Cobos, Negar Khanlou, Xinhai R. Zhang, Jasmeet Assi, Joshua T. Byers, Ameer Hamza, Karam Han, Anders Meyer, Hilda Mirbaha, Carrie A. Mohila, Todd M. Stevens, Sara L. Stone, Wenzhong Yan, Mohammad Haeri, Xiang 'Anthony' Chen</dc:creator>
    </item>
    <item>
      <title>AI.vs.Clinician: Unveiling Intricate Interactions Between AI and Clinicians through an Open-Access Database</title>
      <link>https://arxiv.org/abs/2406.07362</link>
      <description>arXiv:2406.07362v2 Announce Type: replace 
Abstract: Artificial Intelligence (AI) plays a crucial role in medical field and has the potential to revolutionize healthcare practices. However, the success of AI models and their impacts hinge on the synergy between AI and medical specialists, with clinicians assuming a dominant role. Unfortunately, the intricate dynamics and interactions between AI and clinicians remain undiscovered and thus hinder AI from being translated into medical practice. To address this gap, we have curated a groundbreaking database called AI.vs.Clinician. This database is the first of its kind for studying the interactions between AI and clinicians. It derives from 7,500 collaborative diagnosis records on a life-threatening medical emergency -- Sepsis -- from 14 medical centers across China. For the patient cohorts well-chosen from MIMIC databases, the AI-related information comprises the model property, feature input, diagnosis decision, and inferred probabilities of sepsis onset presently and within next three hours. The clinician-related information includes the viewed examination data and sequence, viewed time, preliminary and final diagnosis decisions with or without AI assistance, and recommended treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07362v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wanling Gao, Yuan Liu, Zhuoming Yu, Dandan Cui, Wenjing Liu, Xiaoshuang Liang, Jiahui Zhao, Jiyue Xie, Hao Li, Li Ma, Ning Ye, Yumiao Kang, Dingfeng Luo, Peng Pan, Wei Huang, Zhongmou Liu, Jizhong Hu, Fan Huang, Gangyuan Zhao, Chongrong Jiang, Tianyi Wei, Zhifei Zhang, Yunyou Huang, Jianfeng Zhan</dc:creator>
    </item>
    <item>
      <title>Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions</title>
      <link>https://arxiv.org/abs/2406.09264</link>
      <description>arXiv:2406.09264v2 Announce Type: replace 
Abstract: Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as alignment. However, the lack of clarified definitions and scopes of human-AI alignment poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment. In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (i.e., aiming to ensure that AI systems' objectives match humans) rather than an ongoing, mutual alignment problem [429]. This perspective largely neglects the long-term interaction and dynamic changes of alignment. To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML), and others. We characterize, define and scope human-AI alignment. From this, we present a conceptual framework of "Bidirectional Human-AI Alignment" to organize the literature from a human-centered perspective. This framework encompasses both 1) conventional studies of aligning AI to humans that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of aligning humans to AI, which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally. Additionally, we articulate the key findings derived from literature analysis, including discussions about human values, interaction techniques, and evaluations. To pave the way for future studies, we envision three key challenges for future directions and propose examples of potential future solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09264v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith Ringel Morris, Paul Resnick, David Jurgens</dc:creator>
    </item>
    <item>
      <title>Lifelong and Continual Learning Dialogue Systems</title>
      <link>https://arxiv.org/abs/2211.06553</link>
      <description>arXiv:2211.06553v2 Announce Type: replace-cross 
Abstract: Dialogue systems, commonly known as chatbots, have gained escalating popularity in recent times due to their wide-spread applications in carrying out chit-chat conversations with users and task-oriented dialogues to accomplish various user tasks. Existing chatbots are usually trained from pre-collected and manually-labeled data and/or written with handcrafted rules. Many also use manually-compiled knowledge bases (KBs). Their ability to understand natural language is still limited, and they tend to produce many errors resulting in poor user satisfaction. Typically, they need to be constantly improved by engineers with more labeled data and more manually compiled knowledge. This book introduces the new paradigm of lifelong learning dialogue systems to endow chatbots the ability to learn continually by themselves through their own self-initiated interactions with their users and working environments to improve themselves. As the systems chat more and more with users or learn more and more from external sources, they become more and more knowledgeable and better and better at conversing. The book presents the latest developments and techniques for building such continual learning dialogue systems that continuously learn new language expressions and lexical and factual knowledge during conversation from users and off conversation from external sources, acquire new training examples during conversation, and learn conversational skills. Apart from these general topics, existing works on continual learning of some specific aspects of dialogue systems are also surveyed. The book concludes with a discussion of open challenges for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06553v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Springer Nature 2024</arxiv:journal_reference>
      <dc:creator>Sahisnu Mazumder, Bing Liu</dc:creator>
    </item>
    <item>
      <title>ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations</title>
      <link>https://arxiv.org/abs/2306.08141</link>
      <description>arXiv:2306.08141v4 Announce Type: replace-cross 
Abstract: As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate similar images. Interestingly, prompt diversity does not decrease as users find better prompts. We further propose a new metric to quantify the steerability of AI using our dataset. We define steerability as the expected number of interactions required to adequately complete a task. We estimate this value by fitting a Markov chain for each target task and calculating the expected time to reach an adequate score in the Markov chain. We quantify and compare AI steerability across different types of target images and two different models, finding that images of cities and natural world images are more steerable than artistic and fantasy images. These findings provide insights into human-AI interaction behavior, present a concrete method of assessing AI steerability, and demonstrate the general utility of the ArtWhisperer dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08141v4</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kailas Vodrahalli, James Zou</dc:creator>
    </item>
    <item>
      <title>PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling</title>
      <link>https://arxiv.org/abs/2402.08702</link>
      <description>arXiv:2402.08702v3 Announce Type: replace-cross 
Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6\%-29.3\% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are available at https://github.com/yongchao98/PROMST. Project Page is available at https://yongchao98.github.io/MIT-REALM-PROMST/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08702v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan</dc:creator>
    </item>
    <item>
      <title>Multimodal Infusion Tuning for Large Models</title>
      <link>https://arxiv.org/abs/2403.05060</link>
      <description>arXiv:2403.05060v2 Announce Type: replace-cross 
Abstract: Recent advancements in large-scale models have showcased remarkable generalization capabilities in various tasks. However, integrating multimodal processing into these models presents a significant challenge, as it often comes with a high computational burden. To address this challenge, we introduce a new parameter-efficient multimodal tuning strategy for large models in this paper, referred to as Multimodal Infusion Tuning (MiT). MiT leverages decoupled self-attention mechanisms within large language models to effectively integrate information from diverse modalities such as images and acoustics. In MiT, we also design a novel adaptive rescaling strategy at the attention head level, which optimizes the representation of infused multimodal features. Notably, all foundation models are kept frozen during the tuning process to reduce the computational burden and only 2.5\% parameters are tunable. We conduct experiments across a range of multimodal tasks, including image-related tasks like referring segmentation and non-image tasks such as sentiment analysis. Our results showcase that MiT achieves state-of-the-art performance in multimodal understanding while significantly reducing computational overhead(10\% of previous methods). Moreover, our tuned model exhibits robust reasoning abilities even in complex scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05060v2</guid>
      <category>cs.MM</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Sun, Yu Song, Jihong Hu, Xinyao Yu, Jiaqing Liu, Yen-Wei Chen, Lanfen Lin</dc:creator>
    </item>
    <item>
      <title>Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach</title>
      <link>https://arxiv.org/abs/2403.14597</link>
      <description>arXiv:2403.14597v2 Announce Type: replace-cross 
Abstract: The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole. Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies. The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14597v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yehor Karpichev, Todd Charter, Jayden Hong, Amir M. Soufi Enayati, Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran</dc:creator>
    </item>
    <item>
      <title>Sequential Decision-Making for Inline Text Autocomplete</title>
      <link>https://arxiv.org/abs/2403.15502</link>
      <description>arXiv:2403.15502v2 Announce Type: replace-cross 
Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretical and experimental evidence that, under certain objectives, the sequential decision-making formulation of the autocomplete problem provides a better suggestion policy than myopic single-step reasoning. However, aligning these objectives with real users requires further exploration. In particular, we hypothesize that the objectives under which sequential decision-making can improve autocomplete systems are not tailored solely to text entry speed, but more broadly to metrics such as user satisfaction and convenience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15502v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Chitnis, Shentao Yang, Alborz Geramifard</dc:creator>
    </item>
    <item>
      <title>Designing a Dashboard for Transparency and Control of Conversational AI</title>
      <link>https://arxiv.org/abs/2406.07882</link>
      <description>arXiv:2406.07882v2 Announce Type: replace-cross 
Abstract: Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status. Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07882v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Vi\'egas</dc:creator>
    </item>
  </channel>
</rss>
