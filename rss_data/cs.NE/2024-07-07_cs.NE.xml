<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CLPB: Chaotic Learner Performance Based Behaviour</title>
      <link>https://arxiv.org/abs/2407.03324</link>
      <description>arXiv:2407.03324v1 Announce Type: new 
Abstract: This paper presents an enhanced version of the Learner Performance-based Behavior (LPB), a novel metaheuristic algorithm inspired by the process of accepting high-school students into various departments at the university. The performance of the LPB is not according to the required level. This paper aims to improve the performance of a single objective LPB by embedding ten chaotic maps within LPB to propose Chaotic LPB (CLPB). The proposed algorithm helps in reducing the Processing Time (PT), getting closer to the global optima, and bypassing the local optima with the best convergence speed. Another improvement that has been made in CLPB is that the best individuals of a sub-population are forced into the interior crossover to improve the quality of solutions. CLPB is evaluated against multiple well-known test functions such as classical (TF1_TF19) and (CEC_C06 2019). Additionally, the results have been compared to the standard LPB and several well-known metaheuristic algorithms such as Dragon Fly Algorithm (DA), Genetic Algorithm (GA), and Particle Swarm Optimization (PSO). Finally, the numerical results show that CLPB has been improved with chaotic maps. Furthermore, it is verified that CLPB has a great ability to deal with large optimization problems compared to LPB, GA, DA, and PSO. Overall, Gauss and Tent maps both have a great impact on improving CLPB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03324v1</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dona A. Franci, Tarik A. Rashid</dc:creator>
    </item>
    <item>
      <title>Prototype Analysis in Hopfield Networks with Hebbian Learning</title>
      <link>https://arxiv.org/abs/2407.03342</link>
      <description>arXiv:2407.03342v1 Announce Type: new 
Abstract: We discuss prototype formation in the Hopfield network. Typically, Hebbian learning with highly correlated states leads to degraded memory performance. We show this type of learning can lead to prototype formation, where unlearned states emerge as representatives of large correlated subsets of states, alleviating capacity woes. This process has similarities to prototype learning in human cognition. We provide a substantial literature review of prototype learning in associative memories, covering contributions from psychology, statistical physics, and computer science. We analyze prototype formation from a theoretical perspective and derive a stability condition for these states based on the number of examples of the prototype presented for learning, the noise in those examples, and the number of non-example states presented. The stability condition is used to construct a probability of stability for a prototype state as the factors of stability change. We also note similarities to traditional network analysis, allowing us to find a prototype capacity. We corroborate these expectations of prototype formation with experiments using a simple Hopfield network with standard Hebbian learning. We extend our experiments to a Hopfield network trained on data with multiple prototypes and find the network is capable of stabilizing multiple prototypes concurrently. We measure the basins of attraction of the multiple prototype states, finding attractor strength grows with the number of examples and the agreement of examples. We link the stability and dominance of prototype states to the energy profile of these states, particularly when comparing the profile shape to target states or other spurious states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03342v1</guid>
      <category>cs.NE</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayden McAlister, Anthony Robins, Lech Szymanski</dc:creator>
    </item>
    <item>
      <title>FOXANN: A Method for Boosting Neural Network Performance</title>
      <link>https://arxiv.org/abs/2407.03369</link>
      <description>arXiv:2407.03369v1 Announce Type: new 
Abstract: Artificial neural networks play a crucial role in machine learning and there is a need to improve their performance. This paper presents FOXANN, a novel classification model that combines the recently developed Fox optimizer with ANN to solve ML problems. Fox optimizer replaces the backpropagation algorithm in ANN; optimizes synaptic weights; and achieves high classification accuracy with a minimum loss, improved model generalization, and interpretability. The performance of FOXANN is evaluated on three standard datasets: Iris Flower, Breast Cancer Wisconsin, and Wine. The results presented in this paper are derived from 100 epochs using 10-fold cross-validation, ensuring that all dataset samples are involved in both the training and validation stages. Moreover, the results show that FOXANN outperforms traditional ANN and logistic regression methods as well as other models proposed in the literature such as ABC-ANN, ABC-MNN, CROANN, and PSO-DNN, achieving a higher accuracy of 0.9969 and a lower validation loss of 0.0028. These results demonstrate that FOXANN is more effective than traditional methods and other proposed models across standard datasets. Thus, FOXANN effectively addresses the challenges in ML algorithms and improves classification performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03369v1</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahmood A. Jumaah, Yossra H. Ali, Tarik A. Rashid, S. Vimal</dc:creator>
    </item>
    <item>
      <title>Decomposition of Difficulties in Complex Optimization Problems Using a Bilevel Approach</title>
      <link>https://arxiv.org/abs/2407.03454</link>
      <description>arXiv:2407.03454v1 Announce Type: new 
Abstract: Practical optimization problems may contain different kinds of difficulties that are often not tractable if one relies on a particular optimization method. Different optimization approaches offer different strengths that are good at tackling one or more difficulty in an optimization problem. For instance, evolutionary algorithms have a niche in handling complexities like discontinuity, non-differentiability, discreteness and non-convexity. However, evolutionary algorithms may get computationally expensive for mathematically well behaved problems with large number of variables for which classical mathematical programming approaches are better suited. In this paper, we demonstrate a decomposition strategy that allows us to synergistically apply two complementary approaches at the same time on a complex optimization problem. Evolutionary algorithms are useful in this context as their flexibility makes pairing with other solution approaches easy. The decomposition idea is a special case of bilevel optimization that separates the difficulties into two levels and assigns different approaches at each level that is better equipped at handling them. We demonstrate the benefits of the proposed decomposition idea on a wide range of test problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03454v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankur Sinha, Dhaval Pujara, Hemant Kumar Singh</dc:creator>
    </item>
    <item>
      <title>Natively neuromorphic LMU architecture for encoding-free SNN-based HAR on commercial edge devices</title>
      <link>https://arxiv.org/abs/2407.04076</link>
      <description>arXiv:2407.04076v1 Announce Type: new 
Abstract: Neuromorphic models take inspiration from the human brain by adopting bio-plausible neuron models to build alternatives to traditional Machine Learning (ML) and Deep Learning (DL) solutions. The scarce availability of dedicated hardware able to actualize the emulation of brain-inspired computation, which is otherwise only simulated, yet still hinders the wide adoption of neuromorphic computing for edge devices and embedded systems. With this premise, we adopt the perspective of neuromorphic computing for conventional hardware and we present the L2MU, a natively neuromorphic Legendre Memory Unit (LMU) which entirely relies on Leaky Integrate-and-Fire (LIF) neurons. Specifically, the original recurrent architecture of LMU has been redesigned by modelling every constituent element with neural populations made of LIF or Current-Based (CuBa) LIF neurons. To couple neuromorphic computing and off-the-shelf edge devices, we equipped the L2MU with an input module for the conversion of real values into spikes, which makes it an encoding-free implementation of a Recurrent Spiking Neural Network (RSNN) able to directly work with raw sensor signals on non-dedicated hardware. As a use case to validate our network, we selected the task of Human Activity Recognition (HAR). We benchmarked our L2MU on smartwatch signals from hand-oriented activities, deploying it on three different commercial edge devices in compressed versions too. The reported results remark the possibility of considering neuromorphic models not only in an exclusive relationship with dedicated hardware but also as a suitable choice to work with common sensors and devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04076v1</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vittorio Fra, Benedetto Leto, Andrea Pignata, Enrico Macii, Gianvito Urgese</dc:creator>
    </item>
    <item>
      <title>Advanced Artificial Intelligence Strategy for Optimizing Urban Rail Network Design using Nature-Inspired Algorithms</title>
      <link>https://arxiv.org/abs/2407.04087</link>
      <description>arXiv:2407.04087v1 Announce Type: new 
Abstract: This study introduces an innovative methodology for the planning of metro network routes within the urban environment of Chennai, Tamil Nadu, India. A comparative analysis of the modified Ant Colony Optimization (ACO) method (previously developed) with recent breakthroughs in nature-inspired algorithms demonstrates the modified ACO's superiority over modern techniques. By utilizing the modified ACO algorithm, the most efficient routes connecting the origin and destination of the metro route are generated. Additionally, the model is applied to the existing metro network to highlight variations between the model's results and the current network. The Google Maps platform, integrated with Python, handles real-time data, including land utilization, Geographical Information Systems (GIS) data, census information, and points of interest. This processing enables the identification of stops within the city and along the chosen routes. The resulting metro network showcases substantial benefits compared to conventional route planning methods, with noteworthy enhancements in workforce productivity, decreased planning time, and cost-efficiency. This study significantly enhances the efficiency of urban transport systems, specifically in rapidly changing metropolitan settings such as chennai.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04087v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hariram Sampath Kumar, Archana Singh, Manish Kumar Ojha</dc:creator>
    </item>
    <item>
      <title>Iris and Palmprint Multimodal Biometric Recognition using Novel Preactivated Inverted ResNet and Hybrid Metaheuristic Optimized DenseNet</title>
      <link>https://arxiv.org/abs/2407.03498</link>
      <description>arXiv:2407.03498v1 Announce Type: cross 
Abstract: Biometric recognition technology has witnessed widespread integration into daily life due to the growing emphasis on information security. In this domain, multimodal biometrics, which combines multiple biometric traits, has overcome limitations found in unimodal systems like susceptibility to spoof attacks or failure to adapt to changes over time. This paper proposes a novel multimodal biometric recognition system that utilizes deep learning algorithms using iris and palmprint modalities. A pioneering approach is introduced, beginning with the implementation of the novel Modified Firefly Algorithm with L\'evy Flights (MFALF) to optimize the Contrast Limited Adaptive Histogram Equalization (CLAHE) algorithm, thereby effectively enhancing image contrast. Subsequently, feature selection is carried out through a unique hybrid of ReliefF and Moth Flame Optimization (MFOR) to extract informative features. For classification, we employ a parallel approach, first introducing a novel Preactivated Inverted ResNet (PIR) architecture, and secondly, harnessing metaheuristics with hybrid of innovative Johnson Flower Pollination Algorithm and Rainfall Optimization Algorithm for fine tuning of the learning rate and dropout parameters of Transfer Learning based DenseNet architecture (JFPA-ROA). Finally, a score-level fusion strategy is implemented to combine the outputs of the two classifiers, providing a robust and accurate multimodal biometric recognition system. The system's performance is assessed based on accuracy, Detection Error Tradeoff (DET) Curve, Equal Error Rate (EER), and Total Training time. The proposed multimodal recognition architecture, tested across CASIA Palmprint, MMU, BMPD, and IIT datasets, achieves 100% recognition accuracy, outperforming unimodal iris and palmprint identification approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03498v1</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Indu Singh, Gunbir Singh Baveja, Shruti Khatri, Sunaina Luthra, Tanvi Singh</dc:creator>
    </item>
    <item>
      <title>Dancing to the State of the Art? How Candidate Lists Influence LKH for Solving the Traveling Salesperson Problem</title>
      <link>https://arxiv.org/abs/2407.03927</link>
      <description>arXiv:2407.03927v1 Announce Type: cross 
Abstract: Solving the Traveling Salesperson Problem (TSP) remains a persistent challenge, despite its fundamental role in numerous generalized applications in modern contexts. Heuristic solvers address the demand for finding high-quality solutions efficiently. Among these solvers, the Lin-Kernighan-Helsgaun (LKH) heuristic stands out, as it complements the performance of genetic algorithms across a diverse range of problem instances. However, frequent timeouts on challenging instances hinder the practical applicability of the solver.
  Within this work, we investigate a previously overlooked factor contributing to many timeouts: The use of a fixed candidate set based on a tree structure. Our investigations reveal that candidate sets based on Hamiltonian circuits contain more optimal edges. We thus propose to integrate this promising initialization strategy, in the form of POPMUSIC, within an efficient restart version of LKH. As confirmed by our experimental studies, this refined TSP heuristic is much more efficient - causing fewer timeouts and improving the performance (in terms of penalized average runtime) by an order of magnitude - and thereby challenges the state of the art in TSP solving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03927v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Heins, Lennart Sch\"apermeier, Pascal Kerschke, Darrell Whitley</dc:creator>
    </item>
    <item>
      <title>Predictive Coding Networks and Inference Learning: Tutorial and Survey</title>
      <link>https://arxiv.org/abs/2407.04117</link>
      <description>arXiv:2407.04117v1 Announce Type: cross 
Abstract: Recent years have witnessed a growing call for renewed emphasis on neuroscience-inspired approaches in artificial intelligence research, under the banner of $\textit{NeuroAI}$. This is exemplified by recent attention gained by predictive coding networks (PCNs) within machine learning (ML). PCNs are based on the neuroscientific framework of predictive coding (PC), which views the brain as a hierarchical Bayesian inference model that minimizes prediction errors from feedback connections. PCNs trained with inference learning (IL) have potential advantages to traditional feedforward neural networks (FNNs) trained with backpropagation. While historically more computationally intensive, recent improvements in IL have shown that it can be more efficient than backpropagation with sufficient parallelization, making PCNs promising alternatives for large-scale applications and neuromorphic hardware. Moreover, PCNs can be mathematically considered as a superset of traditional FNNs, which substantially extends the range of possible architectures for both supervised and unsupervised learning. In this work, we provide a comprehensive review as well as a formal specification of PCNs, in particular placing them in the context of modern ML methods, and positioning PC as a versatile and promising framework worthy of further study by the ML community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04117v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bj\"orn van Zwol, Ro Jefferson, Egon L. van den Broek</dc:creator>
    </item>
    <item>
      <title>Dance of the ADS: Orchestrating Failures through Historically-Informed Scenario Fuzzing</title>
      <link>https://arxiv.org/abs/2407.04359</link>
      <description>arXiv:2407.04359v1 Announce Type: cross 
Abstract: As autonomous driving systems (ADS) advance towards higher levels of autonomy, orchestrating their safety verification becomes increasingly intricate. This paper unveils ScenarioFuzz, a pioneering scenario-based fuzz testing methodology. Designed like a choreographer who understands the past performances, it uncovers vulnerabilities in ADS without the crutch of predefined scenarios. Leveraging map road networks, such as OPENDRIVE, we extract essential data to form a foundational scenario seed corpus. This corpus, enriched with pertinent information, provides the necessary boundaries for fuzz testing in the absence of starting scenarios. Our approach integrates specialized mutators and mutation techniques, combined with a graph neural network model, to predict and filter out high-risk scenario seeds, optimizing the fuzzing process using historical test data. Compared to other methods, our approach reduces the time cost by an average of 60.3%, while the number of error scenarios discovered per unit of time increases by 103%. Furthermore, we propose a self-supervised collision trajectory clustering method, which aids in identifying and summarizing 54 high-risk scenario categories prone to inducing ADS faults. Our experiments have successfully uncovered 58 bugs across six tested systems, emphasizing the critical safety concerns of ADS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04359v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Wang, Taotao Gu, Huan Deng, Hu Li, Xiaohui Kuang, Gang Zhao</dc:creator>
    </item>
    <item>
      <title>Wavelet-based Temporal Attention Improves Traffic Forecasting</title>
      <link>https://arxiv.org/abs/2407.04440</link>
      <description>arXiv:2407.04440v1 Announce Type: cross 
Abstract: Spatio-temporal forecasting of traffic flow data represents a typical problem in the field of machine learning, impacting urban traffic management systems. Traditional statistical and machine learning methods cannot adequately handle both the temporal and spatial dependencies in these complex traffic flow datasets. A prevalent approach in the field is to combine graph convolutional networks and multi-head attention mechanisms for spatio-temporal processing. This paper proposes a wavelet-based temporal attention model, namely a wavelet-based dynamic spatio-temporal aware graph neural network (W-DSTAGNN), for tackling the traffic forecasting problem. Benchmark experiments using several statistical metrics confirm that our proposal efficiently captures spatio-temporal correlations and outperforms ten state-of-the-art models on three different real-world traffic datasets. Our proposed ensemble data-driven method can handle dynamic temporal and spatial dependencies and make long-term forecasts in an efficient manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04440v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yash Jakhmola, Nitish Kumar Mishra, Kripabandhu Ghosh, Tanujit Chakraborty</dc:creator>
    </item>
    <item>
      <title>Resource-Efficient Speech Quality Prediction through Quantization Aware Training and Binary Activation Maps</title>
      <link>https://arxiv.org/abs/2407.04578</link>
      <description>arXiv:2407.04578v1 Announce Type: cross 
Abstract: As speech processing systems in mobile and edge devices become more commonplace, the demand for unintrusive speech quality monitoring increases. Deep learning methods provide high-quality estimates of objective and subjective speech quality metrics. However, their significant computational requirements are often prohibitive on resource-constrained devices. To address this issue, we investigated binary activation maps (BAMs) for speech quality prediction on a convolutional architecture based on DNSMOS. We show that the binary activation model with quantization aware training matches the predictive performance of the baseline model. It further allows using other compression techniques. Combined with 8-bit weight quantization, our approach results in a 25-fold memory reduction during inference, while replacing almost all dot products with summations. Our findings show a path toward substantial resource savings by supporting mixed-precision binary multiplication in hard- and software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04578v1</guid>
      <category>cs.SD</category>
      <category>cs.NE</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattias Nilsson, Riccardo Miccini, Cl\'ement Laroche, Tobias Piechowiak, Friedemann Zenke</dc:creator>
    </item>
    <item>
      <title>PyPop7: A Pure-Python Library for Population-Based Black-Box Optimization</title>
      <link>https://arxiv.org/abs/2212.05652</link>
      <description>arXiv:2212.05652v4 Announce Type: replace 
Abstract: In this paper, we present an open-source pure-Python library called PyPop7 for black-box optimization (BBO). As population-based methods (e.g., evolutionary algorithms, swarm intelligence, and pattern search) become increasingly popular for BBO, the design goal of PyPop7 is to provide a unified API and elegant implementations for them, particularly in challenging high-dimensional scenarios. Since these population-based methods easily suffer from the notorious curse of dimensionality owing to random sampling as one of core operations for most of them, recently various improvements and enhancements have been proposed to alleviate this issue more or less mainly via exploiting possible problem structures: such as, decomposition of search distribution or space, low-memory approximation, low-rank metric learning, variance reduction, ensemble of random subspaces, model self-adaptation, and fitness smoothing. These novel sampling strategies could better exploit different problem structures in high-dimensional search space and therefore they often result in faster rates of convergence and/or better qualities of solution for large-scale BBO. Now PyPop7 has covered many of these important advances on a set of well-established BBO algorithm families and also provided an open-access interface to adding the latest or missed black-box optimizers for further functionality extensions. Its well-designed source code (under GPL-3.0 license) and full-fledged online documents (under CC-BY 4.0 license) have been freely available at \url{https://github.com/Evolutionary-Intelligence/pypop} and \url{https://pypop.readthedocs.io}, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05652v4</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiqi Duan, Guochen Zhou, Chang Shao, Zhuowei Wang, Mingyang Feng, Yuwei Huang, Yajing Tan, Yijun Yang, Qi Zhao, Yuhui Shi</dc:creator>
    </item>
    <item>
      <title>Meta-Learning an Evolvable Developmental Encoding</title>
      <link>https://arxiv.org/abs/2406.09020</link>
      <description>arXiv:2406.09020v2 Announce Type: replace 
Abstract: Representations for black-box optimisation methods (such as evolutionary algorithms) are traditionally constructed using a delicate manual process. This is in contrast to the representation that maps DNAs to phenotypes in biological organisms, which is at the hear of biological complexity and evolvability. Additionally, the core of this process is fundamentally the same across nearly all forms of life, reflecting their shared evolutionary origin. Generative models have shown promise in being learnable representations for black-box optimisation but they are not per se designed to be easily searchable. Here we present a system that can meta-learn such representation by directly optimising for a representation's ability to generate quality-diversity. In more detail, we show our meta-learning approach can find one Neural Cellular Automata, in which cells can attend to different parts of a "DNA" string genome during development, enabling it to grow different solvable 2D maze structures. We show that the evolved genotype-to-phenotype mappings become more and more evolvable, not only resulting in a faster search but also increasing the quality and diversity of grown artefacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09020v2</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milton L. Montero, Erwan Plantec, Eleni Nisioti, Joachim W. Pedersen, Sebastian Risi</dc:creator>
    </item>
    <item>
      <title>Compressed Latent Replays for Lightweight Continual Learning on Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2407.03111</link>
      <description>arXiv:2407.03111v2 Announce Type: replace 
Abstract: Rehearsal-based Continual Learning (CL) has been intensely investigated in Deep Neural Networks (DNNs). However, its application in Spiking Neural Networks (SNNs) has not been explored in depth. In this paper we introduce the first memory-efficient implementation of Latent Replay (LR)-based CL for SNNs, designed to seamlessly integrate with resource-constrained devices. LRs combine new samples with latent representations of previously learned data, to mitigate forgetting. Experiments on the Heidelberg SHD dataset with Sample and Class-Incremental tasks reach a Top-1 accuracy of 92.5% and 92%, respectively, without forgetting the previously learned information. Furthermore, we minimize the LRs' requirements by applying a time-domain compression, reducing by two orders of magnitude their memory requirement, with respect to a naive rehearsal setup, with a maximum accuracy drop of 4%. On a Multi-Class-Incremental task, our SNN learns 10 new classes from an initial set of 10, reaching a Top-1 accuracy of 78.4% on the full test set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03111v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alberto Dequino, Alessio Carpegna, Davide Nadalini, Alessandro Savino, Luca Benini, Stefano Di Carlo, Francesco Conti</dc:creator>
    </item>
    <item>
      <title>Socialz: Multi-Feature Social Fuzz Testing</title>
      <link>https://arxiv.org/abs/2302.08664</link>
      <description>arXiv:2302.08664v4 Announce Type: replace-cross 
Abstract: Online social networks have become an integral aspect of our daily lives and play a crucial role in shaping our relationships with others. However, bugs and glitches, even minor ones, can cause anything from frustrating problems to serious data leaks that can have farreaching impacts on millions of users. To mitigate these risks, fuzz testing, a method of testing with randomised inputs, can provide increased confidence in the correct functioning of a social network. However, implementing traditional fuzz testing methods can be prohibitively difficult or impractical for programmers outside of the social network's development team. To tackle this challenge, we present Socialz, a novel approach to social fuzz testing that (1) characterises real users of a social network, (2) diversifies their interaction using evolutionary computation across multiple, non-trivial features, and (3) collects performance data as these interactions are executed. With Socialz, we aim to put social testing tools in everybody's hands, thereby improving the reliability and security of social networks used worldwide. In our study, we came across (1) one known limitation of the current GitLab CE and (2) 6,907 errors, of which 40.16% are beyond our debugging skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08664v4</guid>
      <category>cs.SE</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3638529.3654033</arxiv:DOI>
      <dc:creator>Francisco Zanartu, Christoph Treude, Markus Wagner</dc:creator>
    </item>
    <item>
      <title>What's the Magic Word? A Control Theory of LLM Prompting</title>
      <link>https://arxiv.org/abs/2310.04444</link>
      <description>arXiv:2310.04444v4 Announce Type: replace-cross 
Abstract: Prompt engineering is crucial for deploying LLMs but is poorly understood mathematically. We formalize LLM systems as a class of discrete stochastic dynamical systems to explore prompt engineering through the lens of control theory. We offer a mathematical analysis of the limitations on the controllability of self-attention as a function of the singular values of the parameter matrices. We present complementary empirical results on the controllability of a panel of LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Given initial state $\mathbf x_0$ from Wikitext and prompts of length $k \leq 10$ tokens, we find that the "correct" next token is reachable at least 97% of the time, and that the top 75 most likely next tokens are reachable at least 85% of the time. Intriguingly, short prompt sequences can dramatically alter the likelihood of specific outputs, even making the least likely tokens become the most likely ones. This control-theoretic analysis of LLMs demonstrates the significant and poorly understood role of input sequences in steering output probabilities, offering a foundational perspective for enhancing language model system capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04444v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, Matt Thomson</dc:creator>
    </item>
    <item>
      <title>Path Analysis for Effective Fault Localization in Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2310.18987</link>
      <description>arXiv:2310.18987v3 Announce Type: replace-cross 
Abstract: Despite deep learning's transformative impact on various domains, the reliability of Deep Neural Networks (DNNs) is still a pressing concern due to their complexity and data dependency. Traditional software fault localization techniques, such as Spectrum-based Fault Localization (SBFL), have been adapted to DNNs with limited success. Existing methods like DeepFault utilize SBFL measures but fail to account for fault propagation across neural pathways, leading to suboptimal fault detection. Addressing this gap, we propose the NP-SBFL method, leveraging Layer-wise Relevance Propagation (LRP) to identify and verify critical neural pathways. Our innovative multi-stage gradient ascent (MGA) technique, an extension of gradient ascent (GA), activates neurons sequentially, enhancing fault detection efficacy. We evaluated the effectiveness of our method, i.e. NP-SBFL-MGA, on two commonly used datasets, MNIST and CIFAR-10, two baselines DeepFault and NP- SBFL-GA, and three suspicious neuron measures, Tarantula, Ochiai, and Barinel. The empirical results showed that NP-SBFL-MGA is statistically more effective than the baselines at identifying suspicious paths and synthesizing adversarial inputs. Particularly, Tarantula on NP-SBFL-MGA had the highest fault detection rate at 96.75%, surpassing DeepFault on Ochiai (89.90%) and NP-SBFL-GA on Ochiai (60.61%). Our approach also yielded results comparable to those of the baselines in synthesizing naturalness inputs, and we found a positive correlation between the coverage of critical paths and the number of failed tests in DNN fault localization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18987v3</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soroush Hashemifar, Saeed Parsa, Akram Kalaee</dc:creator>
    </item>
    <item>
      <title>Reinforced In-Context Black-Box Optimization</title>
      <link>https://arxiv.org/abs/2402.17423</link>
      <description>arXiv:2402.17423v2 Announce Type: replace-cross 
Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with \textit{regret-to-go} tokens, which are designed to represent the performance of an algorithm based on cumulative regret over the future part of the histories. The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBO benchmark functions, hyper-parameter optimization and robot control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17423v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian</dc:creator>
    </item>
    <item>
      <title>Semantically Rich Local Dataset Generation for Explainable AI in Genomics</title>
      <link>https://arxiv.org/abs/2407.02984</link>
      <description>arXiv:2407.02984v2 Announce Type: replace-cross 
Abstract: Black box deep learning models trained on genomic sequences excel at predicting the outcomes of different gene regulatory mechanisms. Therefore, interpreting these models may provide novel insights into the underlying biology, supporting downstream biomedical applications. Due to their complexity, interpretable surrogate models can only be built for local explanations (e.g., a single instance). However, accomplishing this requires generating a dataset in the neighborhood of the input, which must maintain syntactic similarity to the original data while introducing semantic variability in the model's predictions. This task is challenging due to the complex sequence-to-function relationship of DNA.
  We propose using Genetic Programming to generate datasets by evolving perturbations in sequences that contribute to their semantic diversity. Our custom, domain-guided individual representation effectively constrains syntactic similarity, and we provide two alternative fitness functions that promote diversity with no computational effort. Applied to the RNA splicing domain, our approach quickly achieves good diversity and significantly outperforms a random baseline in exploring the search space, as shown by our proof-of-concept, short RNA sequence. Furthermore, we assess its generalizability and demonstrate scalability to larger sequences, resulting in a ~30% improvement over the baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02984v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.GN</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro Barbosa, Rosina Savisaar, Alcides Fonseca</dc:creator>
    </item>
  </channel>
</rss>
