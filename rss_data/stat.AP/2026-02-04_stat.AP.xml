<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 05:01:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>De-Linearizing Agent Traces: Bayesian Inference of Latent Partial Orders for Efficient Execution</title>
      <link>https://arxiv.org/abs/2602.02806</link>
      <description>arXiv:2602.02806v1 Announce Type: new 
Abstract: I agents increasingly execute procedural workflows as sequential action traces, which obscures latent concurrency and induces repeated step-by-step reasoning. We introduce BPOP, a Bayesianframework that infers a latent dependency partial order from noisy linearized traces. BPOP models traces as stochastic linear extensions of an underlying graph and performs efficient MCMC inference via a tractable frontier-softmax likelihood that avoids #P-hard marginalization over linear extensions. We evaluate on our open-sourced Cloud-IaC-6, a suite of cloud provisioning tasks with heterogeneous LLM-generated traces, and WFCommons scientific workflows. BPOP recover dependency structure more accurately than trace-only and process-mining baselines, and the inferred graphs support a compiled executor that prunes irrelevant context, yielding substantial reductions in token usage and execution time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02806v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongqing Li, Zheqiao Cheng, Geoff K. Nicholls, Quyu Kong</dc:creator>
    </item>
    <item>
      <title>Downscaling land surface temperature data using edge detection and block-diagonal Gaussian process regression</title>
      <link>https://arxiv.org/abs/2602.02813</link>
      <description>arXiv:2602.02813v1 Announce Type: new 
Abstract: Accurate and high-resolution estimation of land surface temperature (LST) is crucial in estimating evapotranspiration, a measure of plant water use and a central quantity in agricultural applications. In this work, we develop a novel statistical method for downscaling LST data obtained from NASA's ECOSTRESS mission, using high-resolution data from the Landsat 8 mission as a proxy for modeling agricultural field structure. Using the Landsat data, we identify the boundaries of agricultural fields through edge detection techniques, allowing us to capture the inherent block structure present in the spatial domain. We propose a block-diagonal Gaussian process (BDGP) model that captures the spatial structure of the agricultural fields, leverages independence of LST across fields for computational tractability, and accounts for the change of support present in ECOSTRESS observations. We use the resulting BDGP model to perform Gaussian process regression and obtain high-resolution estimates of LST from ECOSTRESS data, along with uncertainty quantification. Our results demonstrate the practicality of the proposed method in producing reliable high-resolution LST estimates, with potential applications in agriculture, urban planning, and climate studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02813v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Margaret Johnson, Madeleine Pascolini-Campbell, Glynn Hulley, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>On the consistent and scalable detection of spatial patterns</title>
      <link>https://arxiv.org/abs/2602.02825</link>
      <description>arXiv:2602.02825v1 Announce Type: new 
Abstract: Detecting spatial patterns is fundamental to scientific discovery, yet current methods lack statistical consensus and face computational barriers when applied to large-scale spatial omics datasets. We unify major approaches through a single quadratic form and derive general consistency conditions. We reveal that several widely used methods, including Moran's I, are inconsistent, and propose scalable corrections. The resulting test enables robust pattern detection across millions of spatial locations and single-cell lineage-tracing datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02825v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayu Su, Jun Hou Fung, Haoyu Wang, Dian Yang, David A. Knowles, Raul Rabadan</dc:creator>
    </item>
    <item>
      <title>Scalable non-separable spatio-temporal Gaussian process models for large-scale short-term weather prediction</title>
      <link>https://arxiv.org/abs/2602.03609</link>
      <description>arXiv:2602.03609v1 Announce Type: new 
Abstract: Monitoring daily weather fields is critical for climate science, agriculture, and environmental planning, yet fully probabilistic spatio-temporal models become computationally prohibitive at continental scale. We present a case study on short-term forecasting of daily maximum temperature and precipitation across the conterminous United States using novel scalable spatio-temporal Gaussian process methodology. Building on three approximation families - inducing-point methods (FITC), Vecchia approximations, and a hybrid Vecchia-inducing-point full-scale approach (VIF) - we introduce three extensions that address key bottlenecks in large space-time settings: (i) a scalable correlation-based neighbor selection strategy for Vecchia approximations with point-referenced data, enabling accurate conditioning under complex dependence structures, (ii) a space-time kMeans++ inducing-point selection algorithm, and (iii) GPU-accelerated implementations of computationally expensive operations, including matrix operations and neighbor searches. Using both synthetic experiments and a large NOAA station dataset containing approximately 1.7 million space-time observations, we analyze the models with respect to predictive performance, parameter estimation, and computational efficiency. Our results demonstrate that scalable Gaussian process models can yield accurate continental-scale forecasts while remaining computationally feasible, offering practical tools for weather applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03609v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Gyger, Reinhard Furrer, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting</title>
      <link>https://arxiv.org/abs/2602.02583</link>
      <description>arXiv:2602.02583v1 Announce Type: cross 
Abstract: The rapid growth of renewable energy penetration has intensified the need for reliable probabilistic forecasts to support grid operations at aggregated (fleet or system) levels. In practice, however, system operators often lack access to fleet-level probabilistic models and instead rely on site-level forecasts produced by heterogeneous third-party providers. Constructing coherent and calibrated fleet-level probabilistic forecasts from such inputs remains challenging due to complex cross-site dependencies and aggregation-induced miscalibration. This paper proposes a calibrated probabilistic aggregation framework that directly converts site-level probabilistic forecasts into reliable fleet-level forecasts in settings where system-level models cannot be trained or maintained. The framework integrates copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level. This combination enables dependence-aware aggregation while providing valid coverage and maintaining sharp prediction intervals. Experiments on large-scale solar generation datasets from MISO, ERCOT, and SPP demonstrate that the proposed Copula+CACP approach consistently achieves near-nominal coverage with significantly sharper intervals than uncalibrated aggregation baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02583v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alireza Moradi, Mathieu Tanneau, Reza Zandehshahvar, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Ionospheric Observations from the ISS: Overcoming Noise Challenges in Signal Extraction</title>
      <link>https://arxiv.org/abs/2602.02706</link>
      <description>arXiv:2602.02706v1 Announce Type: cross 
Abstract: The Electric Propulsion Electrostatic Analyzer Experiment (\`EP\`EE) is a compact ion energy bandpass filter deployed on the International Space Station (ISS) in March 2023 and providing continuous measurements through April 2024. This period coincides with the Solar Cycle 25 maximum, capturing unique observations of solar activity extremes in the mid- to low-latitude regions of the topside ionosphere. From these in situ spectra we derive plasma parameters that inform space-weather impacts on satellite navigation and radio communication. We present a statistical processing pipeline for \`EP\`EE that (i) estimates the instrument noise floor, (ii) accounts for irregular temporal sampling, and (iii) extracts ionospheric signals. Rather than discarding noisy data, the method learns a baseline noise model and fits the measurement surface using a scaled Vecchia Gaussian process approximation, recovering values typically rejected by thresholding. The resulting products increase data coverage and enable noise-assisted monitoring of ionospheric variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02706v1</guid>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Ulrich, Kelly R. Moran, Ky Potter, Lauren A. Castro, Gabriel R. Wilson, Brian Weaver, Carlos Maldonado</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Shrinkage of Functional Effects, with Application to Analysis of Dynamic eQTLs</title>
      <link>https://arxiv.org/abs/2602.03077</link>
      <description>arXiv:2602.03077v1 Announce Type: cross 
Abstract: We introduce functional adaptive shrinkage (FASH), an empirical Bayes method for joint analysis of observation units in which each unit estimates an effect function at several values of a continuous condition variable. The ideas in this paper are motivated by dynamic expression quantitative trait locus (eQTL) studies, which aim to characterize how genetic effects on gene expression vary with time or another continuous condition. FASH integrates a broad family of Gaussian processes defined through linear differential operators into an empirical Bayes shrinkage framework, enabling adaptive smoothing and borrowing of information across units. This provides improved estimation of effect functions and principled hypothesis testing, allowing straightforward computation of significance measures such as local false discovery and false sign rates. To encourage conservative inferences, we propose a simple prior- adjustment method that has theoretical guarantees and can be more broadly used with other empirical Bayes methods. We illustrate the benefits of FASH by reanalyzing dynamic eQTL data on cardiomyocyte differentiation from induced pluripotent stem cells. FASH identified novel dynamic eQTLs, revealed diverse temporal effect patterns, and provided improved power compared with the original analysis. More broadly, FASH offers a flexible statistical framework for joint analysis of functional data, with applications extending beyond genomics. To facilitate use of FASH in dynamic eQTL studies and other settings, we provide an accompanying R package at https: //github.com/stephenslab/fashr.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03077v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziang Zhang, Peter Carbonetto, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>Blinded sample size re-estimation accounting for uncertainty in mid-trial estimation</title>
      <link>https://arxiv.org/abs/2602.03218</link>
      <description>arXiv:2602.03218v1 Announce Type: cross 
Abstract: For randomized controlled trials to be conclusive, it is important to set the target sample size accurately at the design stage. Comparing two normal populations, the sample size calculation requires specification of the variance other than the treatment effect and misspecification can lead to underpowered studies. Blinded sample size re-estimation is an approach to minimize the risk of inconclusive studies. Existing methods proposed to use the total (one-sample) variance that is estimable from blinded data without knowledge of the treatment allocation. We demonstrate that, since the expectation of this estimator is greater than or equal to the true variance, the one-sample variance approach can be regarded as providing an upper bound of the variance in blind reviews. This worst-case evaluation can likely reduce a risk of underpowered studies. However, blinded reviews of small sample size may still lead to underpowered studies. We propose a refined method accounting for estimation error in blind reviews using an upper confidence limit of the variance. A similar idea had been proposed in the setting of external pilot studies. Furthermore, we developed a method to select an appropriate confidence level so that the re-estimated sample size attains the target power. Numerical studies showed that our method works well and outperforms existing methods. The proposed procedure is motivated and illustrated by recent randomized clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03218v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirotada Maeda, Satoshi Hattori, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Score-based diffusion models for diffuse optical tomography with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2602.03449</link>
      <description>arXiv:2602.03449v1 Announce Type: cross 
Abstract: Score-based diffusion models are a recently developed framework for posterior sampling in Bayesian inverse problems with a state-of-the-art performance for severely ill-posed problems by leveraging a powerful prior distribution learned from empirical data. Despite generating significant interest especially in the machine-learning community, a thorough study of realistic inverse problems in the presence of modelling error and utilization of physical measurement data is still outstanding. In this work, the framework of unconditional representation for the conditional score function (UCoS) is evaluated for linearized difference imaging in diffuse optical tomography (DOT). DOT uses boundary measurements of near-infrared light to estimate the spatial distribution of absorption and scattering parameters in biological tissues. The problem is highly ill-posed and thus sensitive to noise and modelling errors. We introduce a novel regularization approach that prevents overfitting of the score function by constructing a mixed score composed of a learned and a model-based component. Validation of this approach is done using both simulated and experimental measurement data. The experiments demonstrate that a data-driven prior distribution results in posterior samples with low variance, compared to classical model-based estimation, and centred around the ground truth, even in the context of a highly ill-posed problem and in the presence of modelling errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03449v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Schneider, Meghdoot Mozumder, Konstantin Tamarov, Leila Taghizadeh, Tanja Tarvainen, Tapio Helin, Duc-Lam Duong</dc:creator>
    </item>
    <item>
      <title>Bayesian spatiotemporal modelling of political violence and conflict events using discrete-time Hawkes processes</title>
      <link>https://arxiv.org/abs/2408.14940</link>
      <description>arXiv:2408.14940v2 Announce Type: replace 
Abstract: The monitoring of conflict risk in the humanitarian sector is largely based on simple historic averages. The overarching goal of this work is to assess the potential for using a more statistically rigorous approach to monitor the risk of political violence and conflict events in practice, and thereby improve our understanding of their temporal and spatial patterns, to inform preventative measures.
  In particular, a Bayesian, spatiotemporal variant of the Hawkes process is fitted to data gathered by the Armed Conflict Location and Event Data (ACLED) project to obtain sub-national estimates of conflict risk in South Asia over time and space. Our model can effectively estimate the risk level of these events within a statistically sound framework, with a more precise understanding of uncertainty than was previously possible. The model also provides insights into differences in behaviours between countries and conflict types. We also show how our model can be used to monitor short and long term trends, and that it is more stable and robust to outliers compared to current practices that rely on historical averages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14940v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raiha Browning, Hamish Patten, Judith Rousseau, Kerrie Mengersen</dc:creator>
    </item>
    <item>
      <title>Attenuation Bias with Latent Predictors</title>
      <link>https://arxiv.org/abs/2507.22218</link>
      <description>arXiv:2507.22218v3 Announce Type: replace 
Abstract: Many core concepts in political science are latent and therefore can only be measured with error. Measurement error in a predictor attenuates slope coefficient estimates in regression, biasing them toward zero. We show that widely used strategies for correcting attenuation bias -- including instrumental variables and the method of composition -- are themselves biased when applied to latent regressors, sometimes even more than simple regression ignoring the measurement error altogether. We derive a correlation-based correction using split-sample measurement strategies. Rather than assuming a particular estimation strategy for the latent trait, our approach is modular and can be easily deployed with a wide variety of latent trait measurement strategies, including additive score, factor, or machine learning models, requiring no joint estimation while yielding consistent slopes under standard assumptions. Simulations and applications show stronger relationships after our correction, sometimes by as much as 50%. Open-source software implements the procedure. Results underscore that latent predictors demand tailored error correction; otherwise, conventional practice can exacerbate bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22218v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Stephen A. Jessee</dc:creator>
    </item>
    <item>
      <title>Green Finance and Carbon Emissions: A Nonlinear and Interaction Analysis Using Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2510.19785</link>
      <description>arXiv:2510.19785v2 Announce Type: replace 
Abstract: As a core policy tool for China in addressing climate risks, green finance plays a strategically important role in shaping carbon mitigation outcomes. This study investigates the nonlinear and interaction effects of green finance on carbon emission intensity (CEI) using Chinese provincial panel data from 2000 to 2022. The Climate Physical Risk Index (CPRI) is incorporated into the analytical framework to assess its potential role in shaping carbon outcomes. We employ Bayesian Additive Regression Trees (BART) to capture complex nonlinear relationships and interaction pathways, and use SHapley Additive exPlanations values to enhance model interpretability. Results show that the Green Finance Index (GFI) has a statistically significant inverted U-shaped effect on CEI, with notable regional heterogeneity. Contrary to expectations, CPRI does not show a significant impact on carbon emissions. Further analysis reveals that in high energy consumption scenarios, stronger green finance development contributes to lower CEI. These findings highlight the potential of green finance as an effective instrument for carbon intensity reduction, especially in energy-intensive contexts, and underscore the importance of accounting for nonlinear effects and regional disparities when designing and implementing green financial policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19785v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengxiang Zhu, Riccardo Rastelli</dc:creator>
    </item>
    <item>
      <title>Matrix-Response Generalized Linear Mixed Model with Applications to Longitudinal Brain Images</title>
      <link>https://arxiv.org/abs/2601.16340</link>
      <description>arXiv:2601.16340v2 Announce Type: replace 
Abstract: Longitudinal brain imaging data facilitate the monitoring of structural and functional alterations in individual brains across time, offering essential understanding of dynamic neurobiological mechanisms. Such data improve sensitivity for detecting early biomarkers of disease progression and enhance the evaluation of intervention effects. While recent matrix-response regression models can relate static brain networks to external predictors, there remain few statistical methods for longitudinal brain networks, especially those derived from high-dimensional imaging data. We introduce a matrix-response generalized linear mixed model that accommodates longitudinal brain networks and identifies edges whose connectivity is influenced by external predictors. An efficient Monte Carlo Expectation-Maximization algorithm is developed for parameter estimation. Extensive simulations demonstrate effective identification of covariate-related network components and accurate parameter estimation. We further demonstrate the usage of the proposed method through applications to diffusion tensor imaging (DTI) and functional MRI (fMRI) datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16340v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhentao Yu, Jiaqi Ding, Guorong Wu, Quefeng Li</dc:creator>
    </item>
    <item>
      <title>Functional-SVD for Heterogeneous Trajectories: Case Studies in Health</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v5 Announce Type: replace-cross 
Abstract: Trajectory data, including time series and longitudinal measurements, are increasingly common in health-related domains such as biomedical research and epidemiology. Real-world trajectory data frequently exhibit heterogeneity across subjects such as patients, sites, and subpopulations, yet many traditional methods are not designed to accommodate such heterogeneity in data analysis. To address this, we propose a unified framework, termed Functional Singular Value Decomposition (FSVD), for statistical learning with heterogeneous trajectories. We establish the theoretical foundations of FSVD and develop a corresponding estimation algorithm that accommodates noisy and irregular observations. We further adapt FSVD to a wide range of trajectory-learning tasks, including dimension reduction, factor modeling, regression, clustering, and data completion, while preserving its ability to account for heterogeneity, leverage inherent smoothness, and handle irregular sampling. Through extensive simulations, we demonstrate that FSVD-based methods consistently outperform existing approaches across these tasks. Finally, we apply FSVD to a COVID-19 case-count dataset and electronic health record datasets, showcasing its effective performance in global and subgroup pattern discovery and factor analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Adventures in Demand Analysis Using AI</title>
      <link>https://arxiv.org/abs/2501.00382</link>
      <description>arXiv:2501.00382v2 Announce Type: replace-cross 
Abstract: This paper advances empirical demand analysis by integrating multimodal product representations derived from artificial intelligence (AI). Using a detailed dataset of toy cars on textit{Amazon.com}, we combine text descriptions, images, and tabular covariates to represent each product using transformer-based embedding models. These embeddings capture nuanced attributes, such as quality, branding, and visual characteristics, that traditional methods often struggle to summarize. Moreover, we fine-tune these embeddings for causal inference tasks. We show that the resulting embeddings substantially improve the predictive accuracy of sales ranks and prices and that they lead to more credible causal estimates of price elasticity. Notably, we uncover strong heterogeneity in price elasticity driven by these product-specific features. Our findings illustrate that AI-driven representations can enrich and modernize empirical demand analysis. The insights generated may also prove valuable for applied causal inference more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00382v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philipp Bach, Victor Chernozhukov, Sven Klaassen, Martin Spindler, Jan Teichert-Kluge, Suhas Vijaykumar</dc:creator>
    </item>
    <item>
      <title>Bayesian Estimation of Causal Effects Using Proxies of a Latent Interference Network</title>
      <link>https://arxiv.org/abs/2505.08395</link>
      <description>arXiv:2505.08395v2 Announce Type: replace-cross 
Abstract: Network interference occurs when treatments assigned to some units affect the outcomes of others. Traditional approaches often assume that the observed network correctly specifies the interference structure. However, in practice, researchers frequently only have access to proxy measurements of the interference network due to limitations in data collection or potential mismatches between measured networks and actual interference pathways. In this paper, we introduce a framework for estimating causal effects when only proxy networks are available. Our approach leverages a structural causal model that accommodates diverse proxy types, including noisy measurements, multiple data sources, and multilayer networks, and defines causal effects as interventions on population-level treatments. The latent nature of the true interference network poses significant challenges. To overcome them, we develop a Bayesian inference framework. We propose a Block Gibbs sampler with Locally Informed Proposals to update the latent network, thereby efficiently exploring the high-dimensional posterior space composed of both discrete and continuous parameters. The latent network updates are driven by information from the proxy networks, treatments, and outcomes. We illustrate the performance of our method through numerical experiments, demonstrating its accuracy in recovering causal effects even when only proxies of the interference network are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08395v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bar Weinstein, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
      <link>https://arxiv.org/abs/2505.17961</link>
      <description>arXiv:2505.17961v3 Announce Type: replace-cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data.
  We propose a novel method to estimate propensity scores via a federated weighted average of local scores using Membership Weights (MW), defined as probabilities of site membership conditional on covariates. MW can be flexibly estimated with parametric or non-parametric classification models using standard FL algorithms. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. In contrast to meta-analysis methods, which fail when any site violates positivity, our approach exploits heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Theoretical analysis and experiments on simulated and real-world data demonstrate clear advantages over meta-analysis and related approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17961v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R\'emi Khellaf, Aur\'elien Bellet, Julie Josse</dc:creator>
    </item>
    <item>
      <title>Efficient and Robust Block Designs for Order-of-Addition Experiments</title>
      <link>https://arxiv.org/abs/2506.07096</link>
      <description>arXiv:2506.07096v2 Announce Type: replace-cross 
Abstract: Designs for Order-of-Addition (OofA) experiments have received growing attention due to their impact on responses based on the sequence of component addition. In certain cases, these experiments involve heterogeneous groups of units, which necessitates the use of blocking to manage variation effects. Despite this, the exploration of block OofA designs remains limited in the literature. As experiments become increasingly complex, addressing this gap is essential to ensure that the designs accurately reflect the effects of the addition sequence and effectively handle the associated variability. Motivated by this, this paper seeks to address the gap by expanding the indicator function framework for block OofA designs. We propose the use of the word length pattern as a criterion for selecting robust block OofA designs. To improve search efficiency and reduce computational demands, we develop algorithms that employ orthogonal Latin squares for design construction and selection, minimizing the need for exhaustive searches. Our analysis, supported by correlation plots, reveals that the algorithms effectively manage confounding and aliasing between effects. Additionally, simulation studies indicate that designs based on our proposed criterion and algorithms achieve power and type I error rates comparable to those of full block OofA designs. This approach offers a practical and efficient method for constructing block OofA designs and may provide valuable insights for future research and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07096v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang-Yun Lin</dc:creator>
    </item>
  </channel>
</rss>
