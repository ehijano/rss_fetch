<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 02:33:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Psychophysics assessment of anomalous contrast</title>
      <link>https://arxiv.org/abs/2501.04139</link>
      <description>arXiv:2501.04139v1 Announce Type: new 
Abstract: Study of image encoding mechanisms of the retina is made possible by the high precision control of timing and intensity of LED displays. One can provide stimulus flicker that reveals differential activation of ON and OFF retinal channels at frequencies above the flicker-fusion threshold. The light energy provided to ON and OFF channels can be balanced, such that a flickering letter will vanish into the background. Yet if the luminance balance has been produced using ultra-brief flashes, an "anomalous contrast" is produced that provides the letter with visibility. The present work contributes additional details about the conditions that will produce anomalous contrast, and discusses how the retinal circuitry can provide this visibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04139v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ernest Greene, Jack Morrison</dc:creator>
    </item>
    <item>
      <title>Role of connectivity anisotropies in the dynamics of cultured neuronal networks</title>
      <link>https://arxiv.org/abs/2501.04427</link>
      <description>arXiv:2501.04427v1 Announce Type: new 
Abstract: Laboratory-grown, engineered living neuronal networks in vitro have emerged in the last years as an experimental technique to understand the collective behavior of neuronal assemblies in relation to their underlying connectivity. An inherent obstacle in the design of such engineered systems is the difficulty to predict the dynamic repertoire of the emerging network and its dependence on experimental variables. To fill this gap, and inspired on recent experimental studies, here we present a numerical model that aims at, first, replicating the anisotropies in connectivity imprinted through engineering, to next realize the collective behavior of the neuronal network and make predictions. We use experimentally measured, biologically-realistic data combined with the Izhikevich model to quantify the dynamics of the neuronal network in relation to tunable structural and dynamical parameters. These parameters include the synaptic noise, strength of the imprinted anisotropies, and average axon lengths. The latter are involved in the simulation of the development of neurons in vitro. We show that the model captures the behavior of engineered neuronal cultures, in which a rich repertoire of activity patterns emerge but whose details are strongly dependent on connectivity details and noise. Results also show that the presence of connectivity anisotropies substantially improves the capacity of reconstructing structural connectivity from activity data, an aspect that is important in the quest for understanding the structure-to-function relationship in neuronal networks. Our work provides the in silico basis to assist experimentalists in the design of laboratory in vitro networks and anticipate their outcome, an aspect that is particularly important in the effort to conceive reliable brain-on-a-chip circuits and explore key aspects such as input-output relationships or information coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04427v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akke Mats Houben, Jordi Garcia-Ojalvo, Jordi Soriano</dc:creator>
    </item>
    <item>
      <title>Fluctuation induced intermittent transitions between distinct rhythms in balanced excitatory-inhibitory spiking networks</title>
      <link>https://arxiv.org/abs/2501.04037</link>
      <description>arXiv:2501.04037v1 Announce Type: cross 
Abstract: Intermittent transitions, associated with critical dynamics and characterized by power-law distributions, are commonly observed during sleep. These critical behaviors are evident at the microscopic level through neuronal avalanches and at the macroscopic level through transitions between sleep stages. To clarify these empirical observations, models grounded in statistical physics have been proposed. At the mesoscopic level of cortical activity, critical behavior is indicated by the intermittent transitions between various cortical rhythms. For instance, empirical investigations utilizing EEG data from rats have identified intermittent transitions between $\delta$ and $\theta$ rhythms, with the duration of $\theta$ rhythm exhibiting a power-law distribution. However, a dynamic model to account for this phenomenon is currently absent. In this study, we introduce a network of sparsely coupled excitatory and inhibitory populations of quadratic integrate-and-fire (QIF) neurons to demonstrate that intermittent transitions can emerge from the intrinsic fluctuations of a finite-sized system, particularly when the system is positioned near a Hopf bifurcation point, which is a critical point. The resulting power-law distributions and exponents are consistent with empirical observations. Additionally, we illustrate how modifications in network connectivity can affect the power-law exponent by influencing the attractivity and oscillation frequency of the stable limit cycle. Our findings, interpreted through the fundamental dynamics of neuronal networks, provide a plausible mechanism for the generation of intermittent transitions between cortical rhythms, in alignment with the power-law distributions documented in empirical researches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04037v1</guid>
      <category>physics.soc-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiyun Zhang, Bojun Wang, Hongjie Bi</dc:creator>
    </item>
    <item>
      <title>Structure of activity in multiregion recurrent neural networks</title>
      <link>https://arxiv.org/abs/2402.12188</link>
      <description>arXiv:2402.12188v3 Announce Type: replace 
Abstract: Neural circuits comprise multiple interconnected regions, each with complex dynamics. The interplay between local and global activity is thought to underlie computational flexibility, yet the structure of multiregion neural activity and its origins in synaptic connectivity remain poorly understood. We investigate recurrent neural networks with multiple regions, each containing neurons with random and structured connections. Inspired by experimental evidence of communication subspaces, we use low-rank connectivity between regions to enable selective activity routing. These networks exhibit high-dimensional fluctuations within regions and low-dimensional signal transmission between them. Using dynamical mean-field theory, with cross-region currents as order parameters, we show that regions act as both generators and transmitters of activity -- roles that are often in tension. Taming within-region activity can be crucial for effective signal routing. Unlike previous models that suppressed neural activity to control signal flow, our model achieves routing by exciting different high-dimensional activity patterns through connectivity structure and nonlinear dynamics. Our analysis offers insights into multiregion neural data and trained neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12188v3</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David G. Clark, Manuel Beiran</dc:creator>
    </item>
    <item>
      <title>Adaptive behavior with stable synapses</title>
      <link>https://arxiv.org/abs/2404.07150</link>
      <description>arXiv:2404.07150v4 Announce Type: replace 
Abstract: Behavioral changes in animals and humans, as a consequence of an error or a verbal instruction, can be extremely rapid. Improvement in behavioral performances are usually associated in machine learning and reinforcement learning to synaptic plasticity, and, in general, to changes and optimization of network parameters. However, such rapid changes are not coherent with the timescales of synaptic plasticity, suggesting that the mechanism responsible for that could be a dynamical network reconfiguration. In the last few years, similar capabilities have been observed in transformers, foundational architecture in the field of machine learning that are widely used in applications such as natural language and image processing. Transformers are capable of in-context learning, the ability to adapt and acquire new information dynamically within the context of the task or environment they are currently engaged in, without the need for significant changes to their underlying parameters. Building upon the notion of something unique within transformers enabling the emergence of this property, we claim that it could also be supported by input segregation and dendritic amplification, features extensively observed in biological networks. We propose an architecture composed of gain-modulated recurrent networks that excels at in-context learning, showing abilities inaccessible to standard networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07150v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristiano Capone, Luca Falorsi</dc:creator>
    </item>
  </channel>
</rss>
