<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2025 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A multi-arm multi-stage design for trials with all pairwise testing</title>
      <link>https://arxiv.org/abs/2502.07013</link>
      <description>arXiv:2502.07013v1 Announce Type: new 
Abstract: Multi-arm multi-stage (MAMS) trials have gained popularity to enhance the efficiency of clinical trials, potentially reducing both duration and costs. This paper focuses on designing MAMS trials where no control treatment exists. This can arise when multiple standard treatments are already established or no treatment is available for a severe disease, making it unethical to withhold a potentially helpful option. The proposed design incorporates interim analyses to allow early termination of notably worst treatments and stops the trial entirely if all remaining treatments are performing similarly. The proposed design controls the familywise error rate (FWER) for all pairwise comparisons and provides the conditions guaranteeing FWER control in the strong sense. The FWER and power are used to calculate both the stopping boundaries and the sample size required. Analytic solutions to compute the expected sample size are also derived. A trial motivated by a study conducted in sepsis, where there was no control treatment, is shown. The multi-arm multi-stage all pairwise (MAMSAP) design proposed here is compared to multiple different approaches. For the trial studied, the proposed method yields the lowest required maximum and expected sample size when controlling the FWER and power at the desired levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07013v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Greenstreet, Thomas Jaki, Alun Bedding, Pavel Mozgunov</dc:creator>
    </item>
    <item>
      <title>Compatible Imputation for Hierarchical Linear Models with Incomplete Data: Interaction Effects of Continuous and Categorical Covariates MAR</title>
      <link>https://arxiv.org/abs/2502.07033</link>
      <description>arXiv:2502.07033v1 Announce Type: new 
Abstract: This article focuses on Bayesian estimation of a hierarchical linear model (HLM) from incomplete data assumed missing at random where continuous covariates C and discrete categorical covariates $D$ have interaction effects on a continuous response $R$. Given small sample sizes, maximum likelihood estimation is suboptimal, and existing Gibbs samplers are based on a Bayesian joint distribution compatible with the HLM, but impute missing values of $C$ and the underlying latent continuous variables $D^*$ of $D$ by a Metropolis algorithm via proposal normal densities having constant variances while the target conditional distributions of $C$ and $D$ have nonconstant variances. Therefore, the samplers are neither guaranteed to be compatible with the joint distribution nor ensured to always produce unbiased estimation of the HLM. We assume a Bayesian joint distribution of parameters and partially observed variables, including correlated categorical $D$, and introduce a compatible Gibbs sampler that draws parameters and missing values directly from the exact posterior distributions. We apply our sampler to incompletely observed longitudinal data from the small number of patient-physician encounters during office visits, and compare our estimators with those of existing methods by simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07033v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongho Shin, Yongyun Shin</dc:creator>
    </item>
    <item>
      <title>CLaRe: Compact near-lossless Latent Representations of High-Dimensional Object Data</title>
      <link>https://arxiv.org/abs/2502.07084</link>
      <description>arXiv:2502.07084v1 Announce Type: new 
Abstract: Latent feature representation methods play an important role in the dimension reduction and statistical modeling of high-dimensional complex data objects. However, existing approaches to assess the quality of these methods often rely on aggregated statistics that reflect the central tendency of the distribution of information losses, such as average or total loss, which can mask variation across individual observations. We argue that controlling average performance is insufficient to guarantee that statistical analysis in the latent space reflects the data-generating process and instead advocate for controlling the worst-case generalization error, or a tail quantile of the generalization error distribution. Our framework, CLaRe (Compact near-lossless Latent Representations), introduces a systematic way to balance compactness of the representation with preservation of information when assessing and selecting among latent feature representation methods. To facilitate the application of the CLaRe framework, we have developed GLaRe (Graphical Analysis of Latent Representations), an open-source R package that implements the framework and provides graphical summaries of the full generalization error distribution. We demonstrate the utility of CLaRe through three case studies on high-dimensional datasets from diverse fields of application. We apply the CLaRe framework to select among principal components, wavelets and autoencoder representations for each dataset. The case studies reveal that the optimal latent feature representation varies depending on dataset characteristics, emphasizing the importance of a flexible evaluation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07084v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Zohner, Edward Gunning, Giles Hooker, Jeffrey Morris</dc:creator>
    </item>
    <item>
      <title>Power and sample size calculation for multivariate longitudinal trials using the longitudinal rank sum test</title>
      <link>https://arxiv.org/abs/2502.07152</link>
      <description>arXiv:2502.07152v1 Announce Type: new 
Abstract: Neurodegenerative diseases such as Alzheimer's and Parkinson's often exhibit complex, multivariate longitudinal outcomes that require advanced statistical methods to comprehensively evaluate treatment efficacy. The Longitudinal Rank Sum Test (LRST) offers a nonparametric framework to assess global treatment effects across multiple longitudinal endpoints without requiring multiplicity corrections. This study develops a robust methodology for power and sample size estimation specific to the LRST, integrating theoretical derivations, asymptotic properties, and practical estimation techniques. Validation through numerical simulations demonstrates the accuracy of the proposed methods, while real-world applications to clinical trials in Alzheimer's and Parkinson's disease highlight their practical significance. This framework facilitates the design of efficient, well-powered trials, advancing the evaluation of treatments for complex diseases with multivariate longitudinal outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07152v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Xiaoming Xu, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>Distilling heterogeneous treatment effects: Stable subgroup estimation in causal inference</title>
      <link>https://arxiv.org/abs/2502.07275</link>
      <description>arXiv:2502.07275v1 Announce Type: new 
Abstract: Recent methodological developments have introduced new black-box approaches to better estimate heterogeneous treatment effects; however, these methods fall short of providing interpretable characterizations of the underlying individuals who may be most at risk or benefit most from receiving the treatment, thereby limiting their practical utility. In this work, we introduce a novel method, causal distillation trees (CDT), to estimate interpretable subgroups. CDT allows researchers to fit any machine learning model of their choice to estimate the individual-level treatment effect, and then leverages a simple, second-stage tree-based model to "distill" the estimated treatment effect into meaningful subgroups. As a result, CDT inherits the improvements in predictive performance from black-box machine learning models while preserving the interpretability of a simple decision tree. We derive theoretical guarantees for the consistency of the estimated subgroups using CDT, and introduce stability-driven diagnostics for researchers to evaluate the quality of the estimated subgroups. We illustrate our proposed method on a randomized controlled trial of antiretroviral treatment for HIV from the AIDS Clinical Trials Group Study 175 and show that CDT out-performs state-of-the-art approaches in constructing stable, clinically relevant subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07275v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Huang, Tiffany M. Tang, Ana M. Kenney</dc:creator>
    </item>
    <item>
      <title>RITHMS : An advanced stochastic framework for the simulation of transgenerational hologenomic data</title>
      <link>https://arxiv.org/abs/2502.07366</link>
      <description>arXiv:2502.07366v1 Announce Type: new 
Abstract: A holobiont is made up of a host organism together with its microbiota. In the context of animal breeding, as the holobiont can be viewed as the single unit upon which selection operates, integrating microbiota data into genomic prediction models may be a promising approach to improve predictions of phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic transgenerational data to address this hypothesis, and thus to fill this gap, we propose a new simulation framework. Our approach, an R Implementation of a Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source package, builds upon the MoBPS package and incorporates distinctive characteristics of the microbiota, notably vertical and horizontal transmission as well as modulation due to the environment and host genetics. In addition, RITHMS can account for a variety of selection strategies and is adaptable to different genetic architectures. We simulated transgenerational hologenomic data using RITHMS under a wide variety of scenarios, varying heritability, microbiability, and microbiota heritability. We found that simulated data accurately reflected expected characteristics, notably based on microbial diversity metrics, correlation between taxa, modulation of vertical and horizontal transmission, response to environmental effects and the evolution of phenotypic values depending on selection strategy. Our results support the relevance of our simulation framework and illustrate its possible use for building a selection index balancing genetic gain and microbial diversity. RITHMS is an advanced, flexible tool for generating  transgenerational hologenomic data that incorporate the complex interplay between genetics, microbiota and environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07366v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sol\`ene Pety (MaIAGE, GABI), Mahendra Mariadassou (INRAE, MaIAGE), Ingrid David (GenPhySE), Andrea Rau (INRAE, GABI)</dc:creator>
    </item>
    <item>
      <title>The Variable Multiple Bandpass Periodic Block Bootstrap for Time Series with Multiple Periodic Correlations</title>
      <link>https://arxiv.org/abs/2502.07462</link>
      <description>arXiv:2502.07462v1 Announce Type: new 
Abstract: This work introduces a novel block bootstrap method for time series with multiple periodically correlated (MPC) components called the Variable Multiple Bandpass Periodic Block Bootstrap (VMBPBB). While past methodological advancements permitted bootstrapping time series to preserve certain correlations, and then periodically correlated (PC) structures, there does not appear to be adequate or efficient methods to bootstrap estimate the sampling distribution of estimators for MPC time series. Current methods that preserve the PC correlation structure resample the original time series, selecting block size to preserve one PC component frequency while simultaneously and unnecessarily resampling all frequencies. This destroys PC components at other frequencies. VMBPBB uses bandpass filters to separate each PC component, creating a set of PC component time series each composed principally of one component. VMBPBB then resamples each PC component time series, not the original MPC time series, with the respective block size preserving the correlation structure of each PC component. Finally, VMBPBB aggregates the PC component bootstraps to form a bootstrap of the MPC time series, successfully preserving all correlations. A simulation study across a wide range of different MPC component frequencies and signal-to-noise ratios is presented and reveals that VMBPBB almost universally outperforms existing methods that fail to bandpass filter the MPC time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07462v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Kolmogorov-Smirnov Estimation of Self-Similarity in Long-Range Dependent Fractional Processes</title>
      <link>https://arxiv.org/abs/2502.07545</link>
      <description>arXiv:2502.07545v1 Announce Type: new 
Abstract: This paper investigates the estimation of the self-similarity parameter in fractional processes. We re-examine the Kolmogorov-Smirnov (KS) test as a distribution-based method for assessing self-similarity, emphasizing its robustness and independence from specific probability distributions. Despite these advantages, the KS test encounters significant challenges when applied to fractional processes, primarily due to intrinsic data dependencies that induce both intradependent and interdependent effects. To address these limitations, we propose a novel method based on random permutation theory, which effectively removes autocorrelations while preserving the self-similarity structure of the process. Simulation results validate the robustness of the proposed approach, demonstrating its effectiveness in providing reliable estimation in the presence of strong dependencies. These findings establish a statistically rigorous framework for self-similarity analysis in fractional processes, with potential applications across various scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07545v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniele Angelini, Sergio Bianchi</dc:creator>
    </item>
    <item>
      <title>Distributional Instrumental Variable Method</title>
      <link>https://arxiv.org/abs/2502.07641</link>
      <description>arXiv:2502.07641v1 Announce Type: new 
Abstract: The instrumental variable (IV) approach is commonly used to infer causal effects in the presence of unmeasured confounding. Conventional IV models commonly make the additive noise assumption, which is hard to ensure in practice, but also typically lack flexibility if the causal effects are complex. Further, the vast majority of the existing methods aims to estimate the mean causal effects only, a few other methods focus on the quantile effects. This work aims for estimation of the entire interventional distribution. We propose a novel method called distributional instrumental variables (DIV), which leverages generative modelling in a nonlinear instrumental variable setting. We establish identifiability of the interventional distribution under general assumptions and demonstrate an `under-identified' case where DIV can identify the causal effects while two-step least squares fails to. Our empirical results show that the DIV method performs well for a broad range of simulated data, exhibiting advantages over existing IV approaches in terms of the identifiability and estimation error of the mean or quantile treatment effects. Furthermore, we apply DIV to an economic data set to examine the causal relation between institutional quality and economic development and our results that closely align with the original study. We also apply DIV to a single-cell data set, where we study the generalizability and stability in predicting gene expression under unseen interventions. The software implementations of DIV are available in R and Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07641v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasiia Holovchak, Sorawit Saengkyongam, Nicolai Meinshausen, Xinwei Shen</dc:creator>
    </item>
    <item>
      <title>A Short Note of Comparison between Convex and Non-convex Penalized Likelihood</title>
      <link>https://arxiv.org/abs/2502.07655</link>
      <description>arXiv:2502.07655v1 Announce Type: new 
Abstract: This paper compares convex and non-convex penalized likelihood methods in high-dimensional statistical modeling, focusing on their strengths and limitations. Convex penalties, like LASSO, offer computational efficiency and strong theoretical guarantees but often introduce bias in parameter estimation. Non-convex penalties, such as SCAD and MCP, reduce bias and achieve oracle properties but pose optimization challenges due to non-convexity. The paper highlights key differences in bias-variance trade-offs, computational complexity, and robustness, offering practical guidance for method selection. It concludes that the choice depends on the problem context, balancing accuracy</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07655v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kasy Du</dc:creator>
    </item>
    <item>
      <title>A Nonparametric and Functional Wombling Methodology</title>
      <link>https://arxiv.org/abs/2502.07740</link>
      <description>arXiv:2502.07740v1 Announce Type: new 
Abstract: Wombling methods, first introduced in 1951, have been widely applied to detect boundaries and variations across spatial domains, particularly in biological, public health and meteorological studies. Traditional applications focus on finite-dimensional observations, where significant changes in measurable traits indicate structural boundaries. In this work, wombling methodologies are extended to functional data, enabling the identification of spatial variation in infinite-dimensional settings. Proposed is a nonparametric approach that accommodates functional observations without imposing strict distributional assumptions. This methodology successfully captures complex spatial structures and discontinuities, demonstrating superior sensitivity and robustness compared to existing finite-dimensional techniques. This methodology is then applied to analyse regional epidemiological disparities between London and the rest of the UK, identifying key spatial boundaries in the shape of the first trajectory of Covid-19 incidence in 2020. Through extensive simulations and empirical validation, demonstrated is the method's effectiveness in uncovering meaningful spatial variations, with potential applications in a wide variety of fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07740v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke A. Barratt (Statistical Laboratory, DPMMS, University of Cambridge, UK), John A. D. Aston (Statistical Laboratory, DPMMS, University of Cambridge, UK)</dc:creator>
    </item>
    <item>
      <title>General-Purpose $f$-DP Estimation and Auditing in a Black-Box Setting</title>
      <link>https://arxiv.org/abs/2502.07066</link>
      <description>arXiv:2502.07066v1 Announce Type: cross 
Abstract: In this paper we propose new methods to statistically assess $f$-Differential Privacy ($f$-DP), a recent refinement of differential privacy (DP) that remedies certain weaknesses of standard DP (including tightness under algorithmic composition). A challenge when deploying differentially private mechanisms is that DP is hard to validate, especially in the black-box setting. This has led to numerous empirical methods for auditing standard DP, while $f$-DP remains less explored. We introduce new black-box methods for $f$-DP that, unlike existing approaches for this privacy notion, do not require prior knowledge of the investigated algorithm. Our procedure yields a complete estimate of the $f$-DP trade-off curve, with theoretical guarantees of convergence. Additionally, we propose an efficient auditing method that empirically detects $f$-DP violations with statistical certainty, merging techniques from non-parametric estimation and optimal classification theory. Through experiments on a range of DP mechanisms, we demonstrate the effectiveness of our estimation and auditing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07066v1</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Onder Askin (Ruhr-University Bochum), Holger Dette (Ruhr-University Bochum), Martin Dunsche (Ruhr-University Bochum), Tim Kutta (Aarhus University), Yun Lu (University of Victoria), Yu Wei (Georgia Institute of Technology), Vassilis Zikas (Georgia Institute of Technology)</dc:creator>
    </item>
    <item>
      <title>Likelihood-Free Estimation for Spatiotemporal Hawkes processes with missing data and application to predictive policing</title>
      <link>https://arxiv.org/abs/2502.07111</link>
      <description>arXiv:2502.07111v1 Announce Type: cross 
Abstract: With the growing use of AI technology, many police departments use forecasting software to predict probable crime hotspots and allocate patrolling resources effectively for crime prevention. The clustered nature of crime data makes self-exciting Hawkes processes a popular modeling choice. However, one significant challenge in fitting such models is the inherent missingness in crime data due to non-reporting, which can bias the estimated parameters of the predictive model, leading to inaccurate downstream hotspot forecasts, often resulting in over or under-policing in various communities, especially the vulnerable ones. Our work introduces a Wasserstein Generative Adversarial Networks (WGAN) driven likelihood-free approach to account for unreported crimes in Spatiotemporal Hawkes models. We demonstrate through empirical analysis how this methodology improves the accuracy of parametric estimation in the presence of data missingness, leading to more reliable and efficient policing strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07111v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pramit Das, Moulinath Banerjee, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>Causal Additive Models with Unobserved Causal Paths and Backdoor Paths</title>
      <link>https://arxiv.org/abs/2502.07646</link>
      <description>arXiv:2502.07646v1 Announce Type: cross 
Abstract: Causal additive models have been employed as tractable yet expressive frameworks for causal discovery involving hidden variables. State-of-the-art methodologies suggest that determining the causal relationship between a pair of variables is infeasible in the presence of an unobserved backdoor or an unobserved causal path. Contrary to this assumption, we theoretically show that resolving the causal direction is feasible in certain scenarios by incorporating two novel components into the theory. The first component introduces a novel characterization of regression sets within independence between regression residuals. The second component leverages conditional independence among the observed variables. We also provide a search algorithm that integrates these innovations and demonstrate its competitive performance against existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07646v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thong Pham, Takashi Nicholas Maeda, Shohei Shimizu</dc:creator>
    </item>
    <item>
      <title>Cheap Permutation Testing</title>
      <link>https://arxiv.org/abs/2502.07672</link>
      <description>arXiv:2502.07672v1 Announce Type: cross 
Abstract: Permutation tests are a popular choice for distinguishing distributions and testing independence, due to their exact, finite-sample control of false positives and their minimax optimality when paired with U-statistics. However, standard permutation tests are also expensive, requiring a test statistic to be computed hundreds or thousands of times to detect a separation between distributions. In this work, we offer a simple approach to accelerate testing: group your datapoints into bins and permute only those bins. For U and V-statistics, we prove that these cheap permutation tests have two remarkable properties. First, by storing appropriate sufficient statistics, a cheap test can be run in time comparable to evaluating a single test statistic. Second, cheap permutation power closely approximates standard permutation power. As a result, cheap tests inherit the exact false positive control and minimax optimality of standard permutation tests while running in a fraction of the time. We complement these findings with improved power guarantees for standard permutation testing and experiments demonstrating the benefits of cheap permutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt independence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney, cross-MMD, and cross-HSIC tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07672v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Domingo-Enrich, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Evaluating the impact of racial disproportionality in Stop &amp; Search on expressive crimes in London</title>
      <link>https://arxiv.org/abs/2502.07695</link>
      <description>arXiv:2502.07695v1 Announce Type: cross 
Abstract: Racial disproportionality in Stop &amp; Search practices elicits substantial concerns about its societal and behavioral impacts. This paper aims to investigate the effect of disproportionality, particularly on the black community, on expressive crimes in London using data from January 2019 to December 2023. We focus on a semi-parametric partially linear structural regression method and introduce a Bayesian empirical likelihood procedure combined with double machine learning techniques to control for high-dimensional confounding and to accommodate the strong prior assumption. In addition, we show that the proposed procedure generates a valid posterior in terms of coverage. Applying this approach to the Stop &amp; Search dataset, we find that racial disproportionality aimed at the Black community may be alleviated by taking into account the proportion of the Black population when focusing on expressive crimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07695v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Yijing Li</dc:creator>
    </item>
    <item>
      <title>Non-Data-Splitting Estimator Selection for Regression in Exponential Families</title>
      <link>https://arxiv.org/abs/2212.12954</link>
      <description>arXiv:2212.12954v2 Announce Type: replace 
Abstract: We observe $n$ independent pairs of random variables $(W_{i}, Y_{i})$, where the conditional distribution of $Y_{i}$ given $W_{i}=w_{i}$ follows a one-parameter exponential family with parameter $\bsg^{*}(w_{i})\in\R$. Our goal is to estimate the regression function $\bsg^{*}$. We start with an arbitrary collection of piecewise constant candidate estimators based on our observations and, using the same data, select an estimator from this collection. Our approach is agnostic to the dependencies of the candidate estimators on the data, differing from methods like data splitting, cross-validation, and hold-out. To demonstrate its theoretical performance, we provide a non-asymptotic risk bound for the selected estimator. We then explain how to apply the procedure to changepoint detection in exponential families. The practical performance of the proposed approach is illustrated through a comparative simulation study under different scenarios and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12954v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juntong Chen</dc:creator>
    </item>
    <item>
      <title>Missing Data in Discrete Time State-Space Modeling of Ecological Momentary Assessment Data: A Monte-Carlo Study of Imputation Methods</title>
      <link>https://arxiv.org/abs/2301.13144</link>
      <description>arXiv:2301.13144v3 Announce Type: replace 
Abstract: When using ecological momentary assessment data (EMA), missing data is pervasive as participant attrition is a common issue. Thus, any EMA study must have a missing data plan. In this paper, we discuss missingness in time series analysis and the appropriate way to handle missing data when the data is modeled as an idiographic discrete time continuous measure state-space model. We found that Missing Completely At Random, Missing At Random, and Time-dependent Missing At Random data have less bias and variability than Autoregressive Time-dependent Missing At Random and Missing Not At Random. The Kalman filter excelled at handling missing data under most conditions. Contrary to the literature, we found that, using a variety of methods, multiple imputation struggled to recover the parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13144v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lindley R. Slipetz, Ami Falk, Teague R. Henry</dc:creator>
    </item>
    <item>
      <title>BayesCPclust: A Bayesian Approach for Clustering Constant-Wise Change-Point Data</title>
      <link>https://arxiv.org/abs/2305.17631</link>
      <description>arXiv:2305.17631v4 Announce Type: replace 
Abstract: Change-point models deal with ordered data sequences. Their primary goal is to infer the locations where an aspect of the data sequence changes. In this paper, we propose and implement a nonparametric Bayesian model for clustering observations based on their constant-wise change-point profiles via Gibbs sampler. Our model incorporates a Dirichlet Process on the constant-wise change-point structures to cluster observations while simultaneously performing multiple change-point estimation. Additionally, our approach controls the number of clusters in the model, not requiring the specification of the number of clusters a priori. Satisfactory clustering and estimation results were obtained when evaluating our method under various simulated scenarios and on a real dataset from single-cell genomic sequencing. Our proposed methodology is implemented as an R package called BayesCPclust and is available from the Comprehensive R Archive Network at https://CRAN.R-project.org/package=BayesCPclust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17631v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Carolina da Cruz, Camila P. E. de Souza</dc:creator>
    </item>
    <item>
      <title>Harmonized Estimation of Subgroup-Specific Treatment Effects in Randomized Trials: The Use of External Control Data</title>
      <link>https://arxiv.org/abs/2308.05073</link>
      <description>arXiv:2308.05073v3 Announce Type: replace 
Abstract: Subgroup analyses of randomized controlled trials (RCTs) constitute an important component of the drug development process in precision medicine. In particular, subgroup analyses of early-stage trials often influence the design and eligibility criteria of subsequent confirmatory trials and ultimately influence which subpopulations will receive the treatment after regulatory approval. However, subgroup analyses are often complicated by small sample sizes, which leads to substantial uncertainty about subgroup-specific treatment effects. We explore the use of external control (EC) data to augment RCT subgroup analyses. We define and discuss harmonized estimators of subpopulation-specific treatment effects that leverage EC data. Our approach can be used to modify any subgroup-specific treatment effect estimates that are obtained by combining RCT and EC data, such as linear regression. We alter these subgroup-specific estimates to make them coherent with a robust estimate of the average effect in the randomized population based only on RCT data. The weighted average of the resulting subgroup-specific harmonized estimates matches the RCT-only estimate of the overall effect in the randomized population. We discuss the proposed harmonized estimators through analytic results and simulations, and investigate standard performance metrics. The method is illustrated with a case study in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05073v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Schwartz, Riddhiman Saha, Steffen Ventz, Lorenzo Trippa</dc:creator>
    </item>
    <item>
      <title>A Criterion for Aggregation Error for Multivariate Spatial Data</title>
      <link>https://arxiv.org/abs/2312.12287</link>
      <description>arXiv:2312.12287v3 Announce Type: replace 
Abstract: The criterion for aggregation error (CAGE) is an important metric that aims to measure errors that arise in multiscale (or multi-resolution) spatial data, referred to as the modifiable areal unit problem and the ecological fallacy. Specifically, CAGE is a measure of between scale variance of eigenvectors in a Karhunen-Lo\'{e}ve expansion (KLE), motivated by a theoretical result, referred to as the ``null-MAUP-theorem,'' that states that the MAUP/ecological fallacy are not present when this variance is zero. CAGE was originally developed for univariate spatial data, but its use has been applied to multivariate spatial data without the development of a null-MAUP-theorem in the multivariate spatial setting. To fill this gap, we provide theoretical justification for a multivariate CAGE (MVCAGE), which includes multiscale multivariate extensions of the KLE, Mercer's theorem, and the-null-MAUP theorem. Additionally, we provide technical results that demonstrate that the MVCAGE is preferable to spatial-only CAGE, and extend commonly used basis functions used to compute CAGE to the multivariate spatial setting. Empirical results are provided to demonstrate the use of MVCAGE for uncertainty quantification and regionalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12287v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ranadeep Daw, Jonathan R. Bradley, Christopher K. Wikle, Scott H. Holan</dc:creator>
    </item>
    <item>
      <title>Zero-inflated stochastic volatility model for disaggregated inflation data with exact zeros</title>
      <link>https://arxiv.org/abs/2403.10945</link>
      <description>arXiv:2403.10945v2 Announce Type: replace 
Abstract: The disaggregated time-series data for Consumer Price Index (CPI) often exhibits frequent instances of exact zero price changes, stemming from measurement errors inherent in the data collection process. However, the currently prominent stochastic volatility model of trend inflation is designed for aggregate measures of price inflation, where exact zero price changes rarely occur. We formulate a zero-inflated stochastic volatility model applicable to such non-stationary real-valued multivariate time-series data with exact zeros. The Bayesian dynamic generalized linear model jointly specifies the dynamic zero-generating process. We construct an efficient custom Gibbs sampler, leveraging the P\'{o}lya-Gamma augmentation. Applying the model to disaggregated CPI data in four advanced economies -- US, UK, Germany, and Japan -- we find that the zero-inflated model provides more sensible and informative estimates of time-varying trend and volatility. Through an out-of-sample forecasting exercise, we find that the zero-inflated model delivers improved in point forecasts and better-calibrated interval forecasts, particularly when zero-inflation is prevalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10945v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Kaoru Irie</dc:creator>
    </item>
    <item>
      <title>Analysis of Two-Stage Rollout Designs with Clustering for Causal Inference under Network Interference</title>
      <link>https://arxiv.org/abs/2405.05119</link>
      <description>arXiv:2405.05119v3 Announce Type: replace 
Abstract: Estimating causal effects under interference is pertinent to many real-world settings. Recent work with low-order potential outcomes models uses a rollout design to obtain unbiased estimators that require no interference network information. However, the required extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experiment that selects a sub-population in the first stage and restricts treatment rollout to this sub-population in the second stage. We explore the role of clustering in the first stage by analyzing the bias and variance of a polynomial interpolation-style estimator under this experimental design. Bias increases with the number of edges cut in the clustering of the interference network, but variance depends on qualities of the clustering that relate to homophily and covariate balance. There is a tension between clustering objectives that minimize the number of cut edges versus those that maximize covariate balance across clusters. Through simulations, we explore a bias-variance trade-off and compare the performance of the estimator under different clustering strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05119v3</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayleen Cortez-Rodriguez, Matthew Eichhorn, Christina Lee Yu</dc:creator>
    </item>
    <item>
      <title>Augmented two-stage estimation for treatment crossover in oncology trials: Leveraging external data for improved precision</title>
      <link>https://arxiv.org/abs/2412.10563</link>
      <description>arXiv:2412.10563v2 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) in oncology often allow control group participants to crossover to experimental treatments, a practice that, while often ethically necessary, complicates the accurate estimation of long-term treatment effects. When crossover rates are high or sample sizes are limited, commonly used methods for crossover adjustment (such as the rank-preserving structural failure time model, inverse probability of censoring weights, and two-stage estimation (TSE)) may produce imprecise estimates. Real-world data (RWD) can be used to develop an external control arm for the RCT, although this approach ignores evidence from trial subjects who did not crossover and ignores evidence from the data obtained prior to crossover for those subjects who did. This paper introduces ''augmented two-stage estimation'' (ATSE), a method that combines data from non-switching participants in a RCT with an external dataset, forming a ''hybrid non-switching arm''. With a simulation study, we evaluate the ATSE method's performance compared to TSE crossover adjustment and an external control arm approach. Results indicate that performance is dependent on scenario characteristics, but when unconfounded external data are available, ATSE may result in less bias and improved precision compared to TSE and external control arm approaches. When external data are affected by unmeasured confounding, ATSE becomes prone to bias, but to a lesser extent compared to an external control arm approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10563v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harlan Campbell, Nicholas Latimer, Jeroen P Jansen, Shannon Cope</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Sparse Regression</title>
      <link>https://arxiv.org/abs/2501.18360</link>
      <description>arXiv:2501.18360v3 Announce Type: replace 
Abstract: In this paper, we introduce ``UniLasso'' -- a novel statistical method for regression. This two-stage approach preserves the signs of the univariate coefficients and leverages their magnitude. Both of these properties are attractive for stability and interpretation of the model. Through comprehensive simulations and applications to real-world datasets, we demonstrate that UniLasso outperforms Lasso in various settings, particularly in terms of sparsity and model interpretability. We prove asymptotic support recovery and mean-squared error consistency under a set of conditions different from the well-known irrepresentability conditions for the Lasso. Extensions to generalized linear models (GLMs) and Cox regression are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18360v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Trevor Hastie, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>The Case for Time in Causal DAGs</title>
      <link>https://arxiv.org/abs/2501.19311</link>
      <description>arXiv:2501.19311v2 Announce Type: replace 
Abstract: We make the case for incorporating time explicitly into the definition of variables in causal directed acyclic graphs (DAGs). Causality requires that causes precede effects in time, meaning that the causal relationships between variables in one time order may not be the same in another. Therefore, any causal model requires temporal qualification; this applies even if the model does not describe a time series of repeated measurements. We formalize a notion of time for causal variables and argue that it resolves existing ambiguity in causal DAGs and is essential to assessing the validity of the acyclicity assumption. If variables are separated in time, their causal relationship is necessarily acyclic. Otherwise, acyclicity depends on the absence of any causal cycles permitted by the time order. We introduce a formal distinction between these two conditions and lay out their respective implications. We outline connections of our contribution with different strands of the broader causality literature and discuss the ramifications of considering time for the interpretation and applicability of DAGs as causal models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19311v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander G. Reisach, Alberto Su\'arez, Sebastian Weichwald, Antoine Chambaz</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v2 Announce Type: replace 
Abstract: The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>Cover It Up! Bipartite Graphs Uncover Identifiability in Sparse Factor Analysis</title>
      <link>https://arxiv.org/abs/2211.00671</link>
      <description>arXiv:2211.00671v4 Announce Type: replace-cross 
Abstract: Despite the popularity of factor models with sparse loading matrices, little attention has been given to formally address identifiability of these models beyond standard rotation-based identification such as the positive lower triangular constraint. To fill this gap, we present a counting rule on the number of nonzero factor loadings that is sufficient for achieving generic uniqueness of the variance decomposition in the factor representation. This is formalized in the framework of sparse matrix spaces and some classical elements from graph and network theory. Furthermore, we provide a computationally efficient tool for verifying the counting rule. Our methodology is illustrated for real data in the context of post-processing posterior draws in Bayesian sparse factor analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.00671v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darjus Hosszejni, Sylvia Fr\"uhwirth-Schnatter</dc:creator>
    </item>
    <item>
      <title>Isotonic Mechanism for Exponential Family Estimation in Machine Learning Peer Review</title>
      <link>https://arxiv.org/abs/2304.11160</link>
      <description>arXiv:2304.11160v4 Announce Type: replace-cross 
Abstract: In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism to exponential family distributions. This mechanism generates adjusted scores that closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, implementing this mechanism does not require knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question involves only pairwise comparisons between her submissions, thus indicating the optimality of ranking in truthful information elicitation. Moreover, we show that the adjusted scores improve dramatically the estimation accuracy compared to the original scores and achieve nearly minimax optimality when the ground-truth scores have bounded total variation. We conclude with a numerical analysis of the ICML 2023 ranking data, showing substantial estimation gains in approximating a proxy ground-truth quality of the papers using the Isotonic Mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.11160v4</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuling Yan, Weijie J. Su, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Shapley-PC: Constraint-based Causal Structure Learning with a Shapley Inspired Framework</title>
      <link>https://arxiv.org/abs/2312.11582</link>
      <description>arXiv:2312.11582v3 Announce Type: replace-cross 
Abstract: Causal Structure Learning (CSL), also referred to as causal discovery, amounts to extracting causal relations among variables in data. CSL enables the estimation of causal effects from observational data alone, avoiding the need to perform real life experiments. Constraint-based CSL leverages conditional independence tests to perform causal discovery. We propose Shapley-PC, a novel method to improve constraint-based CSL algorithms by using Shapley values over the possible conditioning sets, to decide which variables are responsible for the observed conditional (in)dependences. We prove soundness, completeness and asymptotic consistency of Shapley-PC and run a simulation study showing that our proposed algorithm is superior to existing versions of PC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11582v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabrizio Russo, Francesca Toni</dc:creator>
    </item>
    <item>
      <title>SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation On Diverse Modalities</title>
      <link>https://arxiv.org/abs/2407.11676</link>
      <description>arXiv:2407.11676v3 Announce Type: replace-cross 
Abstract: Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-bench, we propose a framework to evaluate DA methods on diverse modalities, beyond computer vision task that have been largely explored in the literature. We present a complete and fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-bench is available on Github at https://github.com/scikit-adaptation/skada-bench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11676v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanis Lalou, Th\'eo Gnassounou, Antoine Collas, Antoine de Mathelin, Oleksii Kachaiev, Ambroise Odonnat, Alexandre Gramfort, Thomas Moreau, R\'emi Flamary</dc:creator>
    </item>
    <item>
      <title>Generative Conformal Prediction with Vectorized Non-Conformity Scores</title>
      <link>https://arxiv.org/abs/2410.13735</link>
      <description>arXiv:2410.13735v2 Announce Type: replace-cross 
Abstract: Conformal prediction (CP) provides model-agnostic uncertainty quantification with guaranteed coverage, but conventional methods often produce overly conservative uncertainty sets, especially in multi-dimensional settings. This limitation arises from simplistic non-conformity scores that rely solely on prediction error, failing to capture the prediction error distribution's complexity. To address this, we propose a generative conformal prediction framework with vectorized non-conformity scores, leveraging a generative model to sample multiple predictions from the fitted data distribution. By computing non-conformity scores across these samples and estimating empirical quantiles at different density levels, we construct adaptive uncertainty sets using density-ranked uncertainty balls. This approach enables more precise uncertainty allocation -- yielding larger prediction sets in high-confidence regions and smaller or excluded sets in low-confidence regions -- enhancing both flexibility and efficiency. We establish theoretical guarantees for statistical validity and demonstrate through extensive numerical experiments that our method outperforms state-of-the-art techniques on synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13735v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxing Zheng, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>On the Unknowable Limits to Prediction</title>
      <link>https://arxiv.org/abs/2411.19223</link>
      <description>arXiv:2411.19223v5 Announce Type: replace-cross 
Abstract: We propose a rigorous decomposition of predictive error, highlighting that not all 'irreducible' error is genuinely immutable. Many domains stand to benefit from iterative enhancements in measurement, construct validity, and modeling. Our approach demonstrates how apparently 'unpredictable' outcomes can become more tractable with improved data (across both target and features) and refined algorithms. By distinguishing aleatoric from epistemic error, we delineate how accuracy may asymptotically improve--though inherent stochasticity may remain--and offer a robust framework for advancing computational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19223v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiani Yan, Charles Rahal</dc:creator>
    </item>
  </channel>
</rss>
