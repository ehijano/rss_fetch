<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:01:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Disentangling the Drivers of LLM Social Conformity: An Uncertainty-Moderated Dual-Process Mechanism</title>
      <link>https://arxiv.org/abs/2508.14918</link>
      <description>arXiv:2508.14918v1 Announce Type: new 
Abstract: As large language models (LLMs) integrate into collaborative teams, their social conformity -- the tendency to align with majority opinions -- has emerged as a key concern. In humans, conformity arises from informational influence (rational use of group cues for accuracy) or normative influence (social pressure for approval), with uncertainty moderating this balance by shifting from purely analytical to heuristic processing. It remains unclear whether these human psychological mechanisms apply to LLMs. This study adapts the information cascade paradigm from behavioral economics to quantitatively disentangle the two drivers to investigate the moderate effect. We evaluated nine leading LLMs across three decision-making scenarios (medical, legal, investment), manipulating information uncertainty (q = 0.667, 0.55, and 0.70, respectively). Our results indicate that informational influence underpins the models' behavior across all contexts, with accuracy and confidence consistently rising with stronger evidence. However, this foundational mechanism is dramatically modulated by uncertainty. In low-to-medium uncertainty scenarios, this informational process is expressed as a conservative strategy, where LLMs systematically underweight all evidence sources. In contrast, high uncertainty triggers a critical shift: while still processing information, the models additionally exhibit a normative-like amplification, causing them to overweight public signals (beta &gt; 1.55 vs. private beta = 0.81).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14918v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huixin Zhong, Yanan Liu, Qi Cao, Shijin Wang, Zijing Ye, Zimu Wang, Shiyao Zhang</dc:creator>
    </item>
    <item>
      <title>Designing an Interdisciplinary Artificial Intelligence Curriculum for Engineering: Evaluation and Insights from Experts</title>
      <link>https://arxiv.org/abs/2508.14921</link>
      <description>arXiv:2508.14921v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) increasingly impacts professional practice, there is a growing need to AI-related competencies into higher education curricula. However, research on the implementation of AI education within study programs remains limited and requires new forms of collaboration across disciplines. This study addresses this gap and explores perspectives on interdisciplinary curriculum development through the lens of different stakeholders. In particular, we examine the case of curriculum development for a novel undergraduate program in AI in engineering. The research uses a mixed methods approach, combining quantitative curriculum mapping with qualitative focus group interviews. In addition to assessing the alignment of the curriculum with the targeted competencies, the study also examines the perceived quality, consistency, practicality and effectiveness from both academic and industry perspectives, as well as differences in perceptions between educators who were involved in the development and those who were not. The findings provide a practical understanding of the outcomes of interdisciplinary AI curriculum development and contribute to a broader understanding of how educator participation in curriculum development influences perceptions of quality aspects. It also advances the field of AI education by providing a reference point and insights for further interdisciplinary curriculum developments in response to evolving industry needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14921v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Johannes Schleiss, Anke Manukjan, Michelle Ines Bieber, Sebastian Lang, Sebastian Stober</dc:creator>
    </item>
    <item>
      <title>Bridging Research Gaps Between Academic Research and Legal Investigations of Algorithmic Discrimination</title>
      <link>https://arxiv.org/abs/2508.14954</link>
      <description>arXiv:2508.14954v1 Announce Type: new 
Abstract: As algorithms increasingly take on critical roles in high-stakes areas such as credit scoring, housing, and employment, civil enforcement actions have emerged as a powerful tool for countering potential discrimination. These legal actions increasingly draw on algorithmic fairness research to inform questions such as how to define and detect algorithmic discrimination. However, current algorithmic fairness research, while theoretically rigorous, often fails to address the practical needs of legal investigations. We identify and analyze 15 civil enforcement actions in the United States including regulatory enforcement, class action litigation, and individual lawsuits to identify practical challenges in algorithmic discrimination cases that machine learning research can help address. Our analysis reveals five key research gaps within existing algorithmic bias research, presenting practical opportunities for more aligned research: 1) finding an equally accurate and less discriminatory algorithm, 2) cascading algorithmic bias, 3) quantifying disparate impact, 4) navigating information barriers, and 5) handling missing protected group information. We provide specific recommendations for developing tools and methodologies that can strengthen legal action against unfair algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14954v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colleen V. Chien, Anna Zink, Irene Y. Chen</dc:creator>
    </item>
    <item>
      <title>Systematic Review Of Collaborative Learning Activities For Promoting AI Literacy</title>
      <link>https://arxiv.org/abs/2508.15111</link>
      <description>arXiv:2508.15111v1 Announce Type: new 
Abstract: Improving artificial intelligence (AI) literacy has become an important consideration for academia and industry with the widespread adoption of AI technologies. Collaborative learning (CL) approaches have proven effective for information literacy, and in this study, we investigate the effectiveness of CL in improving AI knowledge and skills. We systematically collected data to create a corpus of nine studies from 2015-2023. We used the Interactive-Constructive-Active-Passive (ICAP) framework to theoretically analyze the CL outcomes for AI literacy reported in each. Findings suggest that CL effectively increases AI literacy across a range of activities, settings, and groups of learners. While most studies occurred in classroom settings, some aimed to broaden participation by involving educators and families or using AI agents to support teamwork. Additionally, we found that instructional activities included all the ICAP modes. We draw implications for future research and teaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15111v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ashish Hingle, Aditya Johri</dc:creator>
    </item>
    <item>
      <title>Mapping Students' AI Literacy Framing and Learning through Reflective Journals</title>
      <link>https://arxiv.org/abs/2508.15112</link>
      <description>arXiv:2508.15112v1 Announce Type: new 
Abstract: This research paper presents a study of undergraduate technology students' self-reflective learning about artificial intelligence (AI). Research on AI literacy proposes that learners must develop five competencies associated with AI: awareness, knowledge, application, evaluation, and development. It is important to understand what, how, and why students learn about AI so formal instruction can better support their learning. We conducted a reflective journal study where students described their interactions with AI each week. Data was collected over six weeks and analyzed using an emergent interpretive process. We found that the participants were aware of AI, expressed opinions on their future use of AI skills, and conveyed conflicted feelings about developing deep AI expertise. They also described ethical concerns with AI use and saw themselves as intermediaries of knowledge for friends and family. We present the implications of this study and propose ideas for future work in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15112v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ashish Hingle, Aditya Johri</dc:creator>
    </item>
    <item>
      <title>Multilateralism in the Global Governance of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2508.15397</link>
      <description>arXiv:2508.15397v1 Announce Type: new 
Abstract: This chapter inquires how international multilateralism addresses the emergence of the general-purpose technology of Artificial Intelligence. In more detail, it analyses two key features of AI multilateralism: its generalized principles and the coordination of state relations in the realm of AI. Firstly, it distinguishes the generalized principles of AI multilateralism of epochal change, determinism, and dialectical understanding. In the second place, the adaptation of multilateralism to AI led to the integration of AI issues into the agendas of existing cooperation frameworks and the creation of new ad hoc frameworks focusing exclusively on AI issues. In both cases, AI multilateralism develops in the shadow of the state hierarchy in relations with other AI stakeholders. While AI multilateralism is multi-stakeholder, and the hierarchy between state and non-state actors may seem blurred, states preserve the competence as decisive decision-makers in agenda-setting, negotiation, and implementation of soft law international commitments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15397v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michal Natorski</dc:creator>
    </item>
    <item>
      <title>The Digital Life of Parisian Parks: Multifunctionality and Urban Context Uncovered by Mobile Application Traffic</title>
      <link>https://arxiv.org/abs/2508.15516</link>
      <description>arXiv:2508.15516v1 Announce Type: new 
Abstract: Landscape architecture typically considers urban parks through the lens of form and function. While past research on equitable access has focused mainly on form, studies of functions have been constrained by limited scale and coarse measurement. Existing efforts have partially quantified functions through small-scale surveys and movement data (e.g., GPS) or general usage records (e.g., CDR), but have not captured the activities and motivations underlying park visits. As a result, our understanding of the functional roles urban parks play remains incomplete. To address this gap, we introduce a method that refines mobile base station coverage using antenna azimuths, enabling clearer distinction of mobile traffic within parks versus surrounding areas. Using Paris as a case study, we analyze a large-scale set of passively collected per-app mobile network traffic - 492 million hourly records for 45 parks. We test two hypotheses: the central-city hypothesis, which posits multifunctional parks emerge in dense, high-rent areas due to land scarcity; and the socio-spatial hypothesis, which views parks as reflections of neighborhood routines and preferences. Our analysis shows that parks have distinctive mobile traffic signatures, differing from both their surroundings and from each other. By clustering parks on temporal and app usage patterns, we identify three functional types - lunchbreak, cultural, and recreational - with different visitation motivations. Centrally located parks (cultural and lunchbreak) display more diverse app use and temporal variation, while suburban (recreational) parks reflect digital behaviors of nearby communities, with app preferences aligned to neighborhood income. These findings demonstrate the value of mobile traffic as a proxy for studying urban green space functions, with implications for park planning, public health, and well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15516v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Felipe Zanella, Linus W. Dietz, Sanja \v{S}\'cepanovi\'c, Ke Zhou, Zbigniew Smoreda, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Fairness for the People, by the People: Minority Collective Action</title>
      <link>https://arxiv.org/abs/2508.15374</link>
      <description>arXiv:2508.15374v1 Announce Type: cross 
Abstract: Machine learning models often preserve biases present in training data, leading to unfair treatment of certain minority groups. Despite an array of existing firm-side bias mitigation techniques, they typically incur utility costs and require organizational buy-in. Recognizing that many models rely on user-contributed data, end-users can induce fairness through the framework of Algorithmic Collective Action, where a coordinated minority group strategically relabels its own data to enhance fairness, without altering the firm's training process. We propose three practical, model-agnostic methods to approximate ideal relabeling and validate them on real-world datasets. Our findings show that a subgroup of the minority can substantially reduce unfairness with a small impact on the overall prediction error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15374v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Omri Ben-Dov, Samira Samadi, Amartya Sanyal, Alexandru \c{T}ifrea</dc:creator>
    </item>
    <item>
      <title>SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version</title>
      <link>https://arxiv.org/abs/2508.15478</link>
      <description>arXiv:2508.15478v1 Announce Type: cross 
Abstract: Small Language Models (SLMs) offer computational efficiency and accessibility, yet a systematic evaluation of their performance and environmental impact remains lacking. We introduce SLM-Bench, the first benchmark specifically designed to assess SLMs across multiple dimensions, including accuracy, computational efficiency, and sustainability metrics. SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14 domains. The evaluation is conducted on 4 hardware configurations, providing a rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench quantifies 11 metrics across correctness, computation, and consumption, enabling a holistic assessment of efficiency trade-offs. Our evaluation considers controlled hardware conditions, ensuring fair comparisons across models. We develop an open-source benchmarking pipeline with standardized evaluation protocols to facilitate reproducibility and further research. Our findings highlight the diverse trade-offs among SLMs, where some models excel in accuracy while others achieve superior energy efficiency. SLM-Bench sets a new standard for SLM evaluation, bridging the gap between resource efficiency and real-world applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15478v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.PF</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nghiem Thanh Pham, Tung Kieu, Duc-Manh Nguyen, Son Ha Xuan, Nghia Duong-Trung, Danh Le-Phuoc</dc:creator>
    </item>
    <item>
      <title>Beyond Traditional Surveillance: Harnessing Expert Knowledge for Public Health Forecasting</title>
      <link>https://arxiv.org/abs/2508.15623</link>
      <description>arXiv:2508.15623v1 Announce Type: cross 
Abstract: Downsizing the US public health workforce throughout 2025 amplifies potential risks during public health crises. Expert judgment from public health officials represents a vital information source, distinct from traditional surveillance infrastructure, that should be valued -- not discarded. Understanding how expert knowledge functions under constraints is essential for understanding the potential impact of reduced capacity. To explore expert forecasting capabilities, 114 public health officials at the 2024 CSTE workshop generated 103 predictions plus 102 rationales of peak hospitalizations and 114 predictions of influenza H3 versus H1 dominance in Pennsylvania for the 2024/25 season. We compared expert predictions to computational models and used rationales to analyze reasoning patterns using Latent Dirichlet Allocation. Experts better predicted H3 dominance and assigned lower probability to implausible scenarios than models. Expert rationales drew on historical patterns, pathogen interactions, vaccine data, and cumulative experience. Expert public health knowledge constitutes a critical data source that should be valued equally with traditional datasets. We recommend developing a national toolkit to systematically collect and analyze expert predictions and rationales, treating human judgment as quantifiable data alongside surveillance systems to enhance crisis response capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15623v1</guid>
      <category>q-bio.PE</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garrik Hoyt, Eleanor Bergren, Gabrielle String, Thomas McAndrew</dc:creator>
    </item>
    <item>
      <title>Using a cognitive architecture to consider antiBlackness in design and development of AI systems</title>
      <link>https://arxiv.org/abs/2207.00644</link>
      <description>arXiv:2207.00644v3 Announce Type: replace 
Abstract: How might we use cognitive modeling to consider the ways in which antiblackness, and racism more broadly, impact the design and development of AI systems? We provide a discussion and an example towards an answer to this question. We use the ACT-R/{\Phi} cognitive architecture and an existing knowledge graph system, ConceptNet, to consider this question not only from a cognitive and sociocultural perspective, but also from a physiological perspective. In addition to using a cognitive modeling as a means to explore how antiblackness may manifest in the design and development of AI systems (particularly from a software engineering perspective), we also introduce connections between antiblackness, the Human, and computational cognitive modeling. We argue that the typical eschewing of sociocultural processes and knowledge structures in cognitive architectures and cognitive modeling implicitly furthers a colorblind approach to cognitive modeling and hides sociocultural context that is always present in human behavior and affects cognitive processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.00644v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christopher L. Dancy</dc:creator>
    </item>
    <item>
      <title>Democratic AI is Possible. The Democracy Levels Framework Shows How It Might Work</title>
      <link>https://arxiv.org/abs/2411.09222</link>
      <description>arXiv:2411.09222v4 Announce Type: replace 
Abstract: This position paper argues that effectively "democratizing AI" requires democratic governance and alignment of AI, and that this is particularly valuable for decisions with systemic societal impacts. Initial steps -- such as Meta's Community Forums and Anthropic's Collective Constitutional AI -- have illustrated a promising direction, where democratic processes could be used to meaningfully improve public involvement and trust in critical decisions. To more concretely explore what increasingly democratic AI might look like, we provide a "Democracy Levels" framework and associated tools that: (i) define milestones toward meaningfully democratic AI, which is also crucial for substantively pluralistic, human-centered, participatory, and public-interest AI, (ii) can help guide organizations seeking to increase the legitimacy of their decisions on difficult AI governance and alignment questions, and (iii) support the evaluation of such efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09222v4</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aviv Ovadya, Kyle Redman, Luke Thorburn, Quan Ze Chen, Oliver Smith, Flynn Devine, Andrew Konya, Smitha Milli, Manon Revel, K. J. Kevin Feng, Amy X. Zhang, Bilva Chandra, Michiel A. Bakker, Atoosa Kasirzadeh</dc:creator>
    </item>
    <item>
      <title>Modeling Discrimination with Causal Abstraction</title>
      <link>https://arxiv.org/abs/2501.08429</link>
      <description>arXiv:2501.08429v2 Announce Type: replace 
Abstract: A person is directly racially discriminated against only if her race caused her worse treatment. This implies that race is an attribute sufficiently separable from other attributes to isolate its causal role. But race is embedded in a nexus of social factors that resist isolated treatment. If race is socially constructed, in what sense can it cause worse treatment? Some propose that the perception of race, rather than race itself, causes worse treatment. Others suggest that since causal models require \textit{modularity}, i.e. the ability to isolate causal effects, attempts to causally model discrimination are misguided.
  This paper addresses the problem differently. We introduce a framework for reasoning about discrimination, in which race is a high-level \textit{abstraction} of lower-level features. In this framework, race can be modeled as itself causing worse treatment. Modularity is ensured by allowing assumptions about social construction to be precisely and explicitly stated, via an alignment between race and its constituents. Such assumptions can then be subjected to normative and empirical challenges, which lead to different views of when discrimination occurs. By distinguishing constitutive and causal relations, the abstraction framework pinpoints disagreements in the current literature on modeling discrimination, while preserving a precise causal account of discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08429v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milan Moss\'e, Kara Schechtman, Frederick Eberhardt, Thomas Icard</dc:creator>
    </item>
    <item>
      <title>A Case for Specialisation in Non-Human Entities</title>
      <link>https://arxiv.org/abs/2503.04742</link>
      <description>arXiv:2503.04742v2 Announce Type: replace 
Abstract: With the rise of large multi-modal AI models, fuelled by recent interest in large language models (LLMs), the notion of artificial general intelligence (AGI) went from being restricted to a fringe community, to dominate mainstream large AI development programs. In contrast, in this paper, we make a case for specialisation, by reviewing the pitfalls of generality and stressing the industrial value of specialised systems.
  Our contribution is threefold. First, we review the most widely accepted arguments against specialisation, and discuss how their relevance in the context of human labour is actually an argument for specialisation in the case of non human agents, be they algorithms or human organisations. Second, we propose four arguments in favor of specialisation, ranging from machine learning robustness, to computer security, social sciences and cultural evolution. Third, we finally make a case for specification, discuss how the machine learning approach to AI has so far failed to catch up with good practices from safety-engineering and formal verification of software, and discuss how some emerging good practices in machine learning help reduce this gap. In particular, we justify the need for specified governance for hard-to-specify systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04742v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>El-Mahdi El-Mhamdi, L\^e-Nguy\^en Hoang, Mariame Tighanimine</dc:creator>
    </item>
    <item>
      <title>A Moral Agency Framework for Legitimate Integration of AI in Bureaucracies</title>
      <link>https://arxiv.org/abs/2508.08231</link>
      <description>arXiv:2508.08231v3 Announce Type: replace 
Abstract: Public-sector bureaucracies seek to reap the benefits of artificial intelligence (AI), but face important concerns about accountability and transparency when using AI systems. In particular, perception or actuality of AI agency might create ethics sinks - constructs that facilitate dissipation of responsibility when AI systems of disputed moral status interface with bureaucratic structures. Here, we reject the notion that ethics sinks are a necessary consequence of introducing AI systems into bureaucracies. Rather, where they appear, they are the product of structural design decisions across both the technology and the institution deploying it. We support this claim via a systematic application of conceptions of moral agency in AI ethics to Weberian bureaucracy. We establish that it is both desirable and feasible to render AI systems as tools for the generation of organizational transparency and legibility, which continue the processes of Weberian rationalization initiated by previous waves of digitalization. We present a three-point Moral Agency Framework for legitimate integration of AI in bureaucratic structures: (a) maintain clear and just human lines of accountability, (b) ensure humans whose work is augmented by AI systems can verify the systems are functioning correctly, and (c) introduce AI only where it doesn't inhibit the capacity of bureaucracies towards either of their twin aims of legitimacy and stewardship. We suggest that AI introduced within this framework can not only improve efficiency and productivity while avoiding ethics sinks, but also improve the transparency and even the legitimacy of a bureaucracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08231v3</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Schmitz, Joanna Bryson</dc:creator>
    </item>
    <item>
      <title>Documenting Deployment with Fabric: A Repository of Real-World AI Governance</title>
      <link>https://arxiv.org/abs/2508.14119</link>
      <description>arXiv:2508.14119v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) is increasingly integrated into society, from financial services and traffic management to creative writing. Academic literature on the deployment of AI has mostly focused on the risks and harms that result from the use of AI. We introduce Fabric, a publicly available repository of deployed AI use cases to outline their governance mechanisms. Through semi-structured interviews with practitioners, we collect an initial set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow with the practitioners. We discuss the oversight mechanisms and guardrails used in practice to safeguard AI use. The Fabric repository includes visual diagrams of AI use cases and descriptions of the deployed systems. Using the repository, we surface gaps in governance and find common patterns in human oversight of deployed AI systems. We intend for Fabric to serve as an extendable, evolving tool for researchers to study the effectiveness of AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14119v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mackenzie Jorgensen, Kendall Brogle, Katherine M. Collins, Lujain Ibrahim, Arina Shah, Petra Ivanovic, Noah Broestl, Gabriel Piles, Paul Dongha, Hatim Abdulhussein, Adrian Weller, Jillian Powers, Umang Bhatt</dc:creator>
    </item>
    <item>
      <title>Quantum computing inspired paintings: reinterpreting classical masterpieces</title>
      <link>https://arxiv.org/abs/2411.09549</link>
      <description>arXiv:2411.09549v5 Announce Type: replace-cross 
Abstract: We aim to apply a quantum computing technique to compose artworks. The main idea is to revisit three paintings of different styles and historical periods: ''Narciso'', painted circa 1597-1599 by Michelangelo Merisi (Caravaggio), ''Les fils de l'homme'', painted in 1964 by Rene Magritte and ''192 Farben'', painted in 1966 by Gerard Richter. We utilize the output of a quantum computation to change the composition in the paintings, leading to a paintings series titled ''Quantum Transformation I, II, III''. In particular, the figures are discretized into square lattices and the order of the pieces is changed according to the result of the quantum simulation. We consider an Ising Hamiltonian as the observable in the quantum computation and its time evolution as the final outcome. From a classical subject to abstract forms, we seek to combine classical and quantum aesthetics through these three art pieces. Besides experimenting with hardware runs and circuit noise, our goal is to reproduce these works as physical oil paintings on wooden panels. With this process, we complete a full circle between classical and quantum techniques and contribute to rethinking Art practice in the era of quantum computing technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09549v5</guid>
      <category>quant-ph</category>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arianna Crippa, Yahui Chai, Omar Costa Hamido, Paulo Itaborai, Karl Jansen</dc:creator>
    </item>
    <item>
      <title>Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques</title>
      <link>https://arxiv.org/abs/2506.21584</link>
      <description>arXiv:2506.21584v2 Announce Type: replace-cross 
Abstract: Current literature suggests that alignment faking (deceptive alignment) is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based ethics are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for alignment evaluations across model sizes and deployment settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21584v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. Koorndijk</dc:creator>
    </item>
    <item>
      <title>CEO-DC: Driving Decarbonization in HPC Data Centers with Actionable Insights</title>
      <link>https://arxiv.org/abs/2507.08923</link>
      <description>arXiv:2507.08923v2 Announce Type: replace-cross 
Abstract: The rapid growth of data centers is increasing energy demand and widening the carbon gap in the ICT sector, as fossil fuels still dominate global energy production. Addressing this challenge requires collaboration across research, policy, and industry to rethink how computing infrastructures are designed and scaled sustainably. This work addresses central trade-offs in procurement decisions that affect carbon emissions, economic costs, and scaling of compute resources. We present these factors in a holistic decision-making framework for Carbon and Economy Optimization in Data Centers (CEO-DC). CEO-DC introduces new carbon and price metrics that enable DC managers, platform designers, and policymakers to make informed decisions. Applying CEO-DC to current trends in AI and HPC reveals that, in 72% of the cases, platform improvements lag behind demand growth. Moreover, prioritizing energy efficiency over latency can reduce the economic appeal of sustainable designs. Our analysis shows that in many countries with electricity with medium to high carbon intensity, replacing platforms older than four years could reduce their projected emissions by at least 75%. However, current carbon incentives worldwide remain insufficient to steer data center procurement strategies toward sustainable goals. In summary, our findings underscore the need for a shift in hardware design and faster grid decarbonization to ensure sustainability and technological viability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08923v2</guid>
      <category>cs.AR</category>
      <category>cs.CY</category>
      <category>cs.PF</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rub\'en Rodr\'iguez \'Alvarez, Denisa-Andreea Constantinescu, Miguel Pe\'on-Quir\'os, David Atienza</dc:creator>
    </item>
    <item>
      <title>Inequality in the Age of Pseudonymity</title>
      <link>https://arxiv.org/abs/2508.04668</link>
      <description>arXiv:2508.04668v4 Announce Type: replace-cross 
Abstract: Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings that are common in the digital age. One key challenge of such environments is the ability of actors to create fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently hamper inequality measurements. As we prove, it is impossible for measures satisfying the literature's canonical set of desired properties to assess the inequality of an economy that may harbor Sybils. We characterize the class of all Sybil-proof measures, and prove that they must satisfy relaxed version of the aforementioned properties. Furthermore, we show that the structure imposed restricts the ability to assess inequality at a fine-grained level. By applying our results, we prove that large classes of popular measures are not Sybil-proof, with the famous Gini coefficient being but one example out of many. Finally, we examine the dynamics leading to the creation of Sybils in digital and traditional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04668v4</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>econ.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aviv Yaish, Nir Chemaya, Lin William Cong, Dahlia Malkhi</dc:creator>
    </item>
  </channel>
</rss>
