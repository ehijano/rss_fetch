<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2024 01:41:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting the Distribution of Treatment Effects: A Covariate-Adjustment Approach</title>
      <link>https://arxiv.org/abs/2407.14635</link>
      <description>arXiv:2407.14635v1 Announce Type: new 
Abstract: Important questions for impact evaluation require knowledge not only of average effects, but of the distribution of treatment effects. What proportion of people are harmed? Does a policy help many by a little? Or a few by a lot? The inability to observe individual counterfactuals makes these empirical questions challenging. I propose an approach to inference on points of the distribution of treatment effects by incorporating predicted counterfactuals through covariate adjustment. I show that finite-sample inference is valid under weak assumptions, for example when data come from a Randomized Controlled Trial (RCT), and that large-sample inference is asymptotically exact under suitable conditions. Finally, I revisit five RCTs in microcredit where average effects are not statistically significant and find evidence of both positive and negative treatment effects in household income. On average across studies, at least 13.6% of households benefited and 12.5% were negatively affected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14635v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno Fava</dc:creator>
    </item>
    <item>
      <title>Leveraging Uniformization and Sparsity for Computation of Continuous Time Dynamic Discrete Choice Games</title>
      <link>https://arxiv.org/abs/2407.14914</link>
      <description>arXiv:2407.14914v1 Announce Type: new 
Abstract: Continuous-time formulations of dynamic discrete choice games offer notable computational advantages, particularly in modeling strategic interactions in oligopolistic markets. This paper extends these benefits by addressing computational challenges in order to improve model solution and estimation. We first establish new results on the rates of convergence of the value iteration, policy evaluation, and relative value iteration operators in the model, holding fixed player beliefs. Next, we introduce a new representation of the value function in the model based on uniformization -- a technique used in the analysis of continuous time Markov chains -- which allows us to draw a direct analogy to discrete time models. Furthermore, we show that uniformization also leads to a stable method to compute the matrix exponential, an operator appearing in the model's log likelihood function when only discrete time "snapshot" data are available. We also develop a new algorithm that concurrently computes the matrix exponential and its derivatives with respect to model parameters, enhancing computational efficiency. By leveraging the inherent sparsity of the model's intensity matrix, combined with sparse matrix techniques and precomputed addresses, we show how to significantly speed up computations. These strategies allow researchers to estimate more sophisticated and realistic models of strategic interactions and policy impacts in empirical industrial organization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14914v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason R. Blevins</dc:creator>
    </item>
    <item>
      <title>Big Data Analytics-Enabled Dynamic Capabilities and Market Performance: Examining the Roles of Marketing Ambidexterity and Competitor Pressure</title>
      <link>https://arxiv.org/abs/2407.15522</link>
      <description>arXiv:2407.15522v1 Announce Type: new 
Abstract: This study, rooted in dynamic capability theory and the developing era of Big Data Analytics, explores the transformative effect of BDA EDCs on marketing. Ambidexterity and firms market performance in the textile sector of Pakistans cities. Specifically, focusing on the firms who directly deal with customers, investigates the nuanced role of BDA EDCs in textile retail firms potential to navigate market dynamics. Emphasizing the exploitation component of marketing ambidexterity, the study investigated the mediating function of marketing ambidexterity and the moderating influence of competitive pressure. Using a survey questionnaire, the study targets key choice makers in textile firms of Faisalabad, Chiniot and Lahore, Pakistan. The PLS-SEM model was employed as an analytical technique, allows for a full examination of the complicated relations between BDA EDCs, marketing ambidexterity, rival pressure, and market performance. The study Predicting a positive impact of Big Data on marketing ambidexterity, with a specific emphasis on exploitation. The study expects this exploitation-orientated marketing ambidexterity to significantly enhance the firms market performance. This research contributes to the existing literature on dynamic capabilities-based frameworks from the perspective of the retail segment of textile industry. The study emphasizes the role of BDA-EDCs in the retail sector, imparting insights into the direct and indirect results of BDA EDCs on market performance inside the retail area. The study s novelty lies in its contextualization of BDA-EDCs in the textile zone of Faisalabad, Lahore and Chiniot, providing a unique perspective on the effect of BDA on marketing ambidexterity and market performance in firms. Methodologically, the study uses numerous samples of retail sectors to make sure broader universality, contributing realistic insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15522v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gulfam Haider, Laiba Zubair, Aman Saleem</dc:creator>
    </item>
    <item>
      <title>Weak-instrument-robust subvector inference in instrumental variables regression: A subvector Lagrange multiplier test and properties of subvector Anderson-Rubin confidence sets</title>
      <link>https://arxiv.org/abs/2407.15256</link>
      <description>arXiv:2407.15256v1 Announce Type: cross 
Abstract: We propose a weak-instrument-robust subvector Lagrange multiplier test for instrumental variables regression. We show that it is asymptotically size-correct under a technical condition. This is the first weak-instrument-robust subvector test for instrumental variables regression to recover the degrees of freedom of the commonly used Wald test, which is not robust to weak instruments. Additionally, we provide a closed-form solution for subvector confidence sets obtained by inverting the subvector Anderson-Rubin test. We show that they are centered around a k-class estimator. Also, we show that the subvector confidence sets for single coefficients of the causal parameter are jointly bounded if and only if Anderson's likelihood-ratio test rejects the hypothesis that the first-stage regression parameter is of reduced rank, that is, that the causal parameter is not identified. Finally, we show that if a confidence set obtained by inverting the Anderson-Rubin test is bounded and nonempty, it is equal to a Wald-based confidence set with a data-dependent confidence level. We explicitly compute this Wald-based confidence test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15256v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte Londschien, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Nonlinear Binscatter Methods</title>
      <link>https://arxiv.org/abs/2407.15276</link>
      <description>arXiv:2407.15276v1 Announce Type: cross 
Abstract: Binned scatter plots are a powerful statistical tool for empirical work in the social, behavioral, and biomedical sciences. Available methods rely on a quantile-based partitioning estimator of the conditional mean regression function to primarily construct flexible yet interpretable visualization methods, but they can also be used to estimate treatment effects, assess uncertainty, and test substantive domain-specific hypotheses. This paper introduces novel binscatter methods based on nonlinear, possibly nonsmooth M-estimation methods, covering generalized linear, robust, and quantile regression models. We provide a host of theoretical results and practical tools for local constant estimation along with piecewise polynomial and spline approximations, including (i) optimal tuning parameter (number of bins) selection, (ii) confidence bands, and (iii) formal statistical tests regarding functional form or shape restrictions. Our main results rely on novel strong approximations for general partitioning-based estimators covering random, data-driven partitions, which may be of independent interest. We demonstrate our methods with an empirical application studying the relation between the percentage of individuals without health insurance and per capita income at the zip-code level. We provide general-purpose software packages implementing our methods in Python, R, and Stata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15276v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng</dc:creator>
    </item>
    <item>
      <title>A unified diagnostic test for regression discontinuity designs</title>
      <link>https://arxiv.org/abs/2205.04345</link>
      <description>arXiv:2205.04345v4 Announce Type: replace 
Abstract: Diagnostic tests for regression discontinuity design face a size-control problem. We document a massive over-rejection of the identifying restriction among empirical studies in the top five economics journals. At least one diagnostic test was rejected for 21 out of 60 studies, whereas less than 5% of the collected 799 tests rejected the null hypotheses. In other words, more than one-third of the studies rejected at least one of their diagnostic tests, whereas their underlying identifying restrictions appear valid. Multiple testing causes this problem because the median number of tests per study was as high as 12. Therefore, we offer unified tests to overcome the size-control problem. Our procedure is based on the new joint asymptotic normality of local polynomial mean and density estimates. In simulation studies, our unified tests outperformed the Bonferroni correction. We implement the procedure as an R package rdtest with two empirical examples in its vignettes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.04345v4</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Fusejima, Takuya Ishihara, Masayuki Sawada</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Stratified Experiments</title>
      <link>https://arxiv.org/abs/2302.03687</link>
      <description>arXiv:2302.03687v4 Announce Type: replace 
Abstract: This paper studies covariate adjusted estimation of the average treatment effect in stratified experiments. We work in a general framework that includes matched tuples designs, coarse stratification, and complete randomization as special cases. Regression adjustment with treatment-covariate interactions is known to weakly improve efficiency for completely randomized designs. By contrast, we show that for stratified designs such regression estimators are generically inefficient, potentially even increasing estimator variance relative to the unadjusted benchmark. Motivated by this result, we derive the asymptotically optimal linear covariate adjustment for a given stratification. We construct several feasible estimators that implement this efficient adjustment in large samples. In the special case of matched pairs, for example, the regression including treatment, covariates, and pair fixed effects is asymptotically optimal. We also provide novel asymptotically exact inference methods that allow researchers to report smaller confidence intervals, fully reflecting the efficiency gains from both stratification and adjustment. Simulations and an empirical application demonstrate the value of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03687v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Cytrynbaum</dc:creator>
    </item>
    <item>
      <title>Localized Neural Network Modelling of Time Series: A Case Study on US Monetary Policy</title>
      <link>https://arxiv.org/abs/2306.05593</link>
      <description>arXiv:2306.05593v2 Announce Type: replace 
Abstract: In this paper, we investigate a semiparametric regression model under the context of treatment effects via a localized neural network (LNN) approach. Due to a vast number of parameters involved, we reduce the number of effective parameters by (i) exploring the use of identification restrictions; and (ii) adopting a variable selection method based on the group-LASSO technique. Subsequently, we derive the corresponding estimation theory and propose a dependent wild bootstrap procedure to construct valid inferences accounting for the dependence of data. Finally, we validate our theoretical findings through extensive numerical studies. In an empirical study, we revisit the impacts of a tightening monetary policy action on a variety of economic variables, including short-/long-term interest rate, inflation, unemployment rate, industrial price and equity return via the newly proposed framework using a monthly dataset of the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05593v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jiti Gao, Fei Liu, Bin Peng, Yanrong Yang</dc:creator>
    </item>
    <item>
      <title>Merger Analysis with Latent Price</title>
      <link>https://arxiv.org/abs/2404.07684</link>
      <description>arXiv:2404.07684v2 Announce Type: replace 
Abstract: Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer and producer surplus, and compensating marginal cost reductions. I also describe assumptions on demand that facilitate the estimation of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Albertsons/Safeway merger (2015) and the Staples/Office Depot merger (2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07684v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul S. Koh</dc:creator>
    </item>
    <item>
      <title>Instrumented Difference-in-Differences with Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.12083</link>
      <description>arXiv:2405.12083v4 Announce Type: replace 
Abstract: Many studies exploit variation in the timing of policy adoption across units as an instrument for treatment. This paper formalizes the underlying identification strategy as an instrumented difference-in-differences (DID-IV). In this design, a Wald-DID estimand, which scales the DID estimand of the outcome by the DID estimand of the treatment, captures the local average treatment effect on the treated (LATET). In contrast to Fuzzy DID design considered in de Chaisemartin and D'Haultfoeuille (2018), our DID-IV design does not ex ante require strong restrictions on treatment adoption behavior across units. Additionally, our target parameter, the LATET, is policy-relevant if the instrument is based on the policy change of interest to the researcher. We extend the canonical DID-IV design to multiple period settings with the staggered adoption of the instrument across units, calling it a staggered DID-IV design, and propose an estimation method that is robust to treatment effect heterogeneity. We illustrate our findings in the setting of Oreopoulos (2006), estimating returns to schooling in the United Kingdom. In this application, the two-way fixed effects instrumental variable regression, which is the conventional approach to implement a staggered DID-IV design, yields a negative estimate, whereas our estimation method indicates a substantial gain from schooling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12083v4</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sho Miyaji</dc:creator>
    </item>
    <item>
      <title>Information-Enriched Selection of Stationary and Non-Stationary Autoregressions using the Adaptive Lasso</title>
      <link>https://arxiv.org/abs/2402.16580</link>
      <description>arXiv:2402.16580v2 Announce Type: replace-cross 
Abstract: We propose a novel approach to elicit the weight of a potentially non-stationary regressor in the consistent and oracle-efficient estimation of autoregressive models using the adaptive Lasso. The enhanced weight builds on a statistic that exploits distinct orders in probability of the OLS estimator in time series regressions when the degree of integration differs. We provide theoretical results on the benefit of our approach for detecting stationarity when a tuning criterion selects the $\ell_1$ penalty parameter. Monte Carlo evidence shows that our proposal is superior to using OLS-based weights, as suggested by Kock [Econom. Theory, 32, 2016, 243-259]. We apply the modified estimator to model selection for German inflation rates after the introduction of the Euro. The results indicate that energy commodity price inflation and headline inflation are best described by stationary autoregressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16580v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thilo Reinschl\"ussel, Martin C. Arnold</dc:creator>
    </item>
  </channel>
</rss>
