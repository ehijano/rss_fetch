<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 01:35:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning</title>
      <link>https://arxiv.org/abs/2507.09094</link>
      <description>arXiv:2507.09094v1 Announce Type: new 
Abstract: In this paper, a novel Three dimensional (3D) positioning framework of fluid antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In the proposed framework, a set of controlled UAVs cooperatively estimate the real-time 3D position of a target UAV. Here, the active UAV transmits a measurement signal to the passive UAVs via the reflection from the target UAV. Each passive UAV estimates the distance of the active-target-passive UAV link and selects an antenna port to share the distance information with the base station (BS) that calculates the real-time position of the target UAV. As the target UAV is moving due to its task operation, the controlled UAVs must optimize their trajectories and select optimal antenna port, aiming to estimate the real-time position of the target UAV. We formulate this problem as an optimization problem to minimize the target UAV positioning error via optimizing the trajectories of all controlled UAVs and antenna port selection of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use the local Q function to determine its trajectory and antenna port while optimizing the target UAV positioning performance without knowing the trajectories and antenna port selections of other controlled UAVs. Different from current MARL methods, the proposed method uses a recurrent neural network (RNN) that incorporates historical state-action pairs of each controlled UAV, and an attention mechanism to analyze the importance of these historical state-action pairs, thus improving the global Q function approximation accuracy and the target UAV positioning accuracy. Simulation results show that the proposed AR-MARL scheme can reduce the average positioning error by up to 17.5% and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09094v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoren Xu, Hao Xu, Dongyu Wei, Walid Saad, Mehdi Bennis, Mingzhe Chen</dc:creator>
    </item>
    <item>
      <title>Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks</title>
      <link>https://arxiv.org/abs/2507.09124</link>
      <description>arXiv:2507.09124v1 Announce Type: new 
Abstract: The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims to unlock a unified 6G platform capable of seamlessly supporting AI and RAN workloads over shared infrastructure. However, the architectural framework and intelligent resource orchestration strategies necessary to realize this vision remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN Architectural (CAORA) framework based on O-RAN specifications, enabling the dynamic coexistence of real-time RAN and computationally intensive AI workloads. We design custom xApps within the Near-Real-Time RAN Intelligent Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The orchestrator incorporates workload forecasting and anomaly detection modules, augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that proactively manages resource allocation, including Multi-Instance GPU (MIG) partitioning. Using real-world 5G traffic traces from Barcelona, our trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment of RAN demands, supports dynamic AI workloads, and maximizes infrastructure utilization even under highly dynamic conditions. Our results reveal that predictive orchestration significantly improves system adaptability, resource efficiency, and service continuity, offering a viable blueprint for future AI-and-RAN converged 6G systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09124v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syed Danial Ali Shah, Maryam Hafeez, Abdelaziz Salama, Syed Ali Raza Zaidi</dc:creator>
    </item>
    <item>
      <title>On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response</title>
      <link>https://arxiv.org/abs/2507.09153</link>
      <description>arXiv:2507.09153v1 Announce Type: new 
Abstract: Natural disasters often disrupt communication networks and severely hamper emergency response and disaster management. Existing solutions, such as portable communication units and cloud-based network architectures, have improved disaster resilience but fall short if both the Radio Access Network (RAN) and backhaul infrastructure become inoperable. To address these challenges, we propose a demand-driven communication system supported by High Altitude Platform Stations (HAPS) to restore communication in an affected area and enable effective disaster relief. The proposed emergency response network is a promising solution as it provides a rapidly deployable, resilient communications infrastructure. The proposed HAPS-based communication can play a crucial role not only in ensuring connectivity for mobile users but also in restoring backhaul connections when terrestrial networks fail. As a bridge between the disaster management center and the affected areas, it can facilitate the exchange of information in real time, collect data from the affected regions, and relay crucial updates to emergency responders. Enhancing situational awareness, coordination between relief agencies, and ensuring efficient resource allocation can significantly strengthen disaster response capabilities. In this paper, simulations show that HAPS with hybrid optical/THz links boosts backhaul capacity and resilience, even in harsh conditions. HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first responders and disaster-affected populations. This paper also explores the integration of HAPS into emergency communication frameworks and standards, as it has the potential to improve network resilience and support effective disaster management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09153v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilal Karaman, Ilhan Ba\c{s}t\"urk, Ferdi Kara, Engin Zeydan, Esra Aycan Beyaz{\i}t, Sezai Ta\c{s}k{\i}n, Emil Bj\"ornson, Halim Yanikomeroglu</dc:creator>
    </item>
    <item>
      <title>Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications</title>
      <link>https://arxiv.org/abs/2507.09270</link>
      <description>arXiv:2507.09270v1 Announce Type: new 
Abstract: In this paper, we consider a semantic-aware reconfigurable intelligent surface (RIS)-assisted wireless network, where multiple semantic users (SUs) simultaneously transmit semantic information to an access point (AP) by using the non-orthogonal multiple access (NOMA) method. The SUs can reshape their traffic demands by modifying the semantic extraction factor, while the RIS can reconfigure the channel conditions via the passive beamforming. This provides the AP with greater flexibility to decode the superimposed signals from the SUs. We aim to minimize the system's overall energy consumption, while ensuring that each SU's traffic demand is satisfied. Hence, we formulate a joint optimization problem of the SUs' decoding order and semantic control, as well as the RIS's passive beamforming strategy. This problem is intractable due to the complicated coupling in constraints. To solve this, we decompose the original problem into two subproblems and solve them by using a series of approximate methods. Numerical results show that the joint traffic reshaping and channel reconfiguration scheme significantly improves the energy saving performance of the NOMA transmissions compared to the benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09270v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songhan Zhao, Yusi Long, Lanhua Li, Bo Gu, Shimin Gong, Zehui Xiong</dc:creator>
    </item>
    <item>
      <title>Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks</title>
      <link>https://arxiv.org/abs/2507.09341</link>
      <description>arXiv:2507.09341v1 Announce Type: new 
Abstract: Vehicular Mobile Edge Computing (VEC) drives the future by enabling low-latency, high-efficiency data processing at the very edge of vehicular networks. This drives innovation in key areas such as autonomous driving, intelligent transportation systems, and real-time analytics. Despite its potential, VEC faces significant challenges, particularly in adhering to strict task offloading deadlines, as vehicles remain within the coverage area of Roadside Units (RSUs) for only brief periods. To tackle this challenge, this paper evaluates the performance boundaries of task processing by initially establishing a theoretical limit using Particle Swarm Optimization (PSO) in a static environment. To address more dynamic and practical scenarios, PSO, Deep Q-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented in an online setting. The objective is to minimize dropped tasks and reduce end-to-end (E2E) latency, covering both communication and computation delays. Experimental results demonstrate that the DQN model considerably surpasses the dynamic PSO approach, achieving a 99.2% reduction in execution time. Furthermore, It leads to a reduction in dropped tasks by 2.5% relative to dynamic PSO and achieves 18.6\% lower E2E latency, highlighting the effectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and efficient task management for VEC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09341v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahsa Paknejad, Parisa Fard Moshiri, Murat Simsek, Burak Kantarci, Hussein T. Mouftah</dc:creator>
    </item>
    <item>
      <title>Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks</title>
      <link>https://arxiv.org/abs/2507.09346</link>
      <description>arXiv:2507.09346v1 Announce Type: new 
Abstract: Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for meeting the low-latency demands of modern IoT and dynamic task scheduling scenarios. MEC reduces the processing burden on resource-constrained devices by enabling task execution at nearby edge servers. However, efficient task scheduling remains a challenge in dynamic, time-sensitive environments. Conventional methods -- such as heuristic algorithms and mixed-integer programming -- suffer from high computational overhead, limiting their real-time applicability. Existing deep learning (DL) approaches offer faster inference but often lack scalability and adaptability to dynamic workloads. To address these issues, we propose a Pointer Network-based architecture for task scheduling in dynamic edge computing scenarios. Our model is trained on a generated synthetic dataset using genetic algorithms to determine the optimal task ordering. Experimental results show that our model achieves lower drop ratios and waiting times than baseline methods, and a soft sequence accuracy of up to 89.2%. Our model consistently achieves inference times under 2 seconds across all evaluated task counts, whereas the integer and binary programming approaches require approximately up to 18 seconds and 90 seconds, respectively. It also shows strong generalization across varying scenarios, and adaptability to real-time changes, offering a scalable and efficient solution for edge-based task management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09346v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arild Yonkeu, Mohammadreza Amini, Burak Kantarci</dc:creator>
    </item>
    <item>
      <title>Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling</title>
      <link>https://arxiv.org/abs/2507.09352</link>
      <description>arXiv:2507.09352v1 Announce Type: new 
Abstract: Mobile Edge Computing (MEC) enables low-latency applications by bringing computation closer to the user, but dynamic task arrivals and communication threats like jamming complicate reliable task offloading and resource allocation. In this paper, we formulate a dynamic MEC framework considering the transmission diversity that jointly addresses task scheduling and resource block (RB) assignment in the presence of jamming. First, we define and evaluate key network metrics-including dropped task ratio and bandwidth utilization-while maintaining service continuity by accounting for the existing commitments of the edge server to previously offloaded tasks. Then, we propose a jamming-aware offloading and RB allocation framework that leverages transmission diversity and optimal scheduling across distributed gNBs. The proposed solution is compared to a similar scenario without transmission diversity and two baseline strategies of first-come-first-served (FCFS) and shortest task first (STF). The proposed algorithm effectively mitigates the impact of jamming while enhancing resource utilization and minimizing task drop rates, making it highly suitable for mission-critical MEC applications. At signal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves a $0.26$ task drop rate, outperforming the scenario without transmission diversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52 and 0.63 task drop rates, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09352v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ghazal Asemian, Mohammadreza Amini, Burak Kantarci</dc:creator>
    </item>
    <item>
      <title>MobiWorld: World Models for Mobile Wireless Network</title>
      <link>https://arxiv.org/abs/2507.09462</link>
      <description>arXiv:2507.09462v1 Announce Type: new 
Abstract: Accurate modeling and simulation of mobile networks are essential for enabling intelligent and cost-effective network optimization. In this paper, we propose MobiWorld, a generative world model designed to support high-fidelity and flexible environment simulation for mobile network planning and optimization. Unlike traditional predictive models constrained by limited generalization capabilities, MobiWorld exhibits strong universality by integrating heterogeneous data sources, including sensors, mobile devices, and base stations, as well as multimodal data types such as sequences and images. It is capable of generating both network element-level observations (e.g., traffic load, user distribution) and system-level performance indicators (e.g., throughput, energy consumption) to support a wide range of planning and optimization tasks. Built upon advanced diffusion models, MobiWorld offers powerful controllable generation capabilities by modeling the joint distribution between mobile network data and diverse conditional factors including spatio temporal contexts, user behaviors, and optimization policies. This enables accurate simulation of dynamic network states under varying policy configurations, providing optimization agents with precise environmental feedback and facilitating effective decision-making without relying on costly real-network interactions. We demonstrate the effectiveness of MobiWorld in a collaborative energy-saving scenario, where an agent uses observations and rewards generated by MobiWorld to optimize base station sleep and user offloading policies. Experimental results show that MobiWorld exhibits strong controllable generation performance and outperforms traditional methods in energy optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09462v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoye Chai, Yuan Yuan, Yong Li</dc:creator>
    </item>
    <item>
      <title>Wi-Fi: Twenty-Five Years and Counting</title>
      <link>https://arxiv.org/abs/2507.09613</link>
      <description>arXiv:2507.09613v1 Announce Type: new 
Abstract: Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding name, today's Wi-Fi boasts entirely new capabilities that were not even on the roadmap 25 years ago. This article aims to provide a holistic and comprehensive technical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi 1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial article to span these eight generations. Rather than a generation-by-generation exposition, we describe the key mechanisms that have advanced Wi-Fi. We begin by discussing spectrum allocation and coexistence, and detailing the IEEE 802.11 standardization cycle. Second, we provide an overview of the physical layer and describe key elements that have enabled data rates to increase by over 1,000x. Third, we describe how Wi-Fi Medium Access Control has been enhanced from the original Distributed Coordination Function to now include capabilities spanning from frame aggregation to wideband spectrum access. Fourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and introduced multi-user access. Fifth, given the increasing use of mobile, battery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the generations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate spectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput, reliability, and latency. Finally, we describe how Wi-Fi enables nearby Access Points to coordinate in order to improve performance and efficiency. In the Appendix, we further discuss Wi-Fi developments beyond 802.11bn, including integrated mmWave operations, sensing, security and privacy extensions, and the adoption of AI/ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09613v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Geraci, Francesca Meneghello, Francesc Wilhelmi, David Lopez-Perez, I\~naki Val, Lorenzo Galati Giordano, Carlos Cordeiro, Monisha Ghosh, Edward Knightly, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>Towards Robust RTC in Sparse LEO Constellations</title>
      <link>https://arxiv.org/abs/2507.09798</link>
      <description>arXiv:2507.09798v1 Announce Type: new 
Abstract: Google's congestion control (GCC) has become a cornerstone for real-time video and audio communication, yet its performance remains fragile in emerging Low Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer longer duration links and reduced handover frequency compared to dense deployments, presenting a unique opportunity for high-quality real-time communication (RTC) in environments with limited terrestrial network infrastructure. In this paper, we study the behavior of videoconferencing systems in sparse LEO constellations. We observe that video quality degrades due to inherent delays and network instability introduced by the high altitude and rapid movement of LEO satellites, with these effects exacerbated by WebRTC's conventional ``one-size-fits-all'' sender-side pacing queue management. To boost RTC performance, we introduce a data-driven queue management mechanism that adapts the maximum pacing queue capacity based on predicted handover activity. Specifically, our approach employs shorter queue limits during stable, no-handover phases to prioritize low latency communication, and preemptively increases pacing queue capacity when entering periods of increased handover activity to absorb disruptions. Our method yields up to $3$x improvements in video bitrate and reduces freeze rate by $62\%$ compared to default WebRTC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09798v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aashish Gottipati, Lili Qiu</dc:creator>
    </item>
    <item>
      <title>UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks</title>
      <link>https://arxiv.org/abs/2507.09852</link>
      <description>arXiv:2507.09852v1 Announce Type: new 
Abstract: In unmanned aerial vehicle (UAV) networks, communication protocols and algorithms are essential for cooperation and collaboration between UAVs. Simulation provides a cost-effective solution for prototyping, debugging, and analyzing protocols and algorithms, avoiding the prohibitive expenses of field experiments. In this paper, we present ``UavNetSim-v1'', an open-source Python-based simulation platform designed for rapid development, testing, and evaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1'' provides most of the functionalities developers may need, including routing/medium access control (MAC) protocols, topology control algorithms and mobility/energy models, while maintaining ease of use. Furthermore, the platform supports comprehensive performance evaluation and features an interactive visualization interface for in-depth algorithm analysis. In short, ``UavNetSim-v1'' lends itself to both rapid prototyping and educational purposes, and can serve as a lightweight yet powerful alternative to mature network simulators for UAV communication research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09852v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Zhou, Zipeng Dai, Linyi Huang, Cui Yang, Youjun Xiang, Jie Tang, Kai-kit Wong</dc:creator>
    </item>
    <item>
      <title>Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference</title>
      <link>https://arxiv.org/abs/2507.09942</link>
      <description>arXiv:2507.09942v1 Announce Type: new 
Abstract: This letter investigates the optimal allocation of large language model (LLM) inference workloads across heterogeneous edge data centers (DCs) over time. Each DC features on-site renewable generation and faces dynamic electricity prices and spatiotemporal variability in renewable availability. The central question is: how can inference workloads be optimally distributed to the DCs to minimize energy consumption, carbon emissions, and water usage while enhancing user experience? This letter proposes a novel optimization model for LLM service providers to reduce operational costs and environmental impacts. Numerical results validate the efficacy of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09942v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Cheng, Duong Tung Nguyen</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit</title>
      <link>https://arxiv.org/abs/2507.10210</link>
      <description>arXiv:2507.10210v1 Announce Type: new 
Abstract: Proper coordination is needed to guarantee the performance of wireless networks in dense deployments. Contention-based systems suffer badly in terms of latency when multiple devices compete for the same resources. Coordinated Orthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi 8 to remedy this, as it enables multiple Access Points (APs) to share spectrum more efficiently. However, fine-grained resource allocation, namely within 20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling overhead and complexity in terms of physical layer signaling. A wired backhaul mitigates the need for over-the-air scheduling and synchronization, and it allows for coordination even if APs are not in each others' range. Furthermore, it forms the basis for more advanced multi-AP coordination schemes like coordinated beamforming and joint transmission. In this work we demonstrate the realization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul, enabled by the open-source platforms openwifi and White Rabbit. We show that the performance in terms of carrier frequency offset pre-compensation and time synchronization between two APs exceeds related wireless standard requirements. Furthermore, the quality of the received constellation of the Co-OFDMA frame as reported by a wireless connectivity tester is better than individual frames sent by the APs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10210v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thijs Havinga, Xianjun Jiao, Wei Liu, Baiheng Chen, Robbe Gaeremynck, Ingrid Moerman</dc:creator>
    </item>
    <item>
      <title>Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI</title>
      <link>https://arxiv.org/abs/2507.10510</link>
      <description>arXiv:2507.10510v1 Announce Type: new 
Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from "humans watching video" to "AI understanding video". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10510v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangkai Wu, Zhiyuan Ren, Liming Liu, Xinggong Zhang</dc:creator>
    </item>
    <item>
      <title>Hybrid Quantum Security for IPsec</title>
      <link>https://arxiv.org/abs/2507.09288</link>
      <description>arXiv:2507.09288v1 Announce Type: cross 
Abstract: Quantum Key Distribution (QKD) offers information-theoretic security against quantum computing threats, but integrating QKD into existing security protocols remains an unsolved challenge due to fundamental mismatches between pre-distributed quantum keys and computational key exchange paradigms. This paper presents the first systematic comparison of sequential versus parallel hybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental protocol design principles that extend beyond specific implementations. We introduce two novel approaches for incorporating QKD into Internet Key Exchange version 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS QKD 014 stateless API specifications: (1) a pure QKD approach that replaces computational key derivation with identifier-based quantum key coordination, and (2) a unified QKD-KEM abstraction that enables parallel composition of quantum and post-quantum cryptographic methods within existing protocol frameworks. Our key insight is that parallel hybrid approaches eliminate the multiplicative latency penalties inherent in sequential methods mandated by RFC 9370, achieving significant performance improvements under realistic network conditions. Performance evaluation using a Docker-based testing framework with IDQuantique QKD hardware demonstrates that the parallel hybrid approach significantly outperforms sequential methods under network latency conditions, while pure QKD achieves minimal bandwidth overhead through identifier-based key coordination. Our implementations provide practical quantum-enhanced IPsec solutions suitable for critical infrastructure deployments requiring defense-in-depth security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09288v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Blanco-Romero, Pedro Otero Garc\'ia, Daniel Sobral-Blanco, Florina Almenares Mendoza, Ana Fern\'andez Vilas, Manuel Fern\'andez-Veiga</dc:creator>
    </item>
    <item>
      <title>Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS</title>
      <link>https://arxiv.org/abs/2507.09301</link>
      <description>arXiv:2507.09301v1 Announce Type: cross 
Abstract: The emergence of quantum computers poses a significant threat to current secure service, application and/or protocol implementations that rely on RSA and ECDSA algorithms, for instance DNSSEC, because public-key cryptography based on number factorization or discrete logarithm is vulnerable to quantum attacks. This paper presents the integration of post-quantum cryptographic (PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality. We have developed a plugin that extends CoreDNS with support for five PQC signature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our implementation maintains compatibility with existing DNS resolution flows while providing on-the-fly signing using quantum-resistant signatures. A benchmark has been performed and performance evaluation results reveal significant trade-offs between security and efficiency. The results indicate that while PQC algorithms introduce operational overhead, several candidates offer viable compromises for transitioning DNSSEC to quantum-resistant cryptography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09301v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julio Gento Suela, Javier Blanco-Romero, Florina Almenares Mendoza, Daniel D\'iaz-S\'anchez</dc:creator>
    </item>
    <item>
      <title>Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices</title>
      <link>https://arxiv.org/abs/2507.09627</link>
      <description>arXiv:2507.09627v1 Announce Type: cross 
Abstract: Next-generation wireless technologies such as 6G aim to meet demanding requirements such as ultra-high data rates, low latency, and enhanced connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and energy efficiency through numerous antennas, and RIS offering dynamic control over the wireless environment via passive reflective elements. However, realizing their full potential depends on accurate Channel State Information (CSI). Recent advances in deep learning have facilitated efficient cascaded channel estimation. However, the scalability and practical deployment of existing estimation models in XL-MIMO systems remain limited. The growing number of antennas and RIS elements introduces a significant barrier to real-time and efficient channel estimation, drastically increasing data volume, escalating computational complexity, requiring advanced hardware, and resulting in substantial energy consumption. To address these challenges, we propose a lightweight deep learning framework for efficient cascaded channel estimation in XL-MIMO systems, designed to minimize computational complexity and make it suitable for deployment on resource-constrained edge devices. Using spatial correlations in the channel, we introduce a patch-based training mechanism that reduces the dimensionality of input to patch-level representations while preserving essential information, allowing scalable training for large-scale systems. Simulation results under diverse conditions demonstrate that our framework significantly improves estimation accuracy and reduces computational complexity, regardless of the increasing number of antennas and RIS elements in XL-MIMO systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09627v1</guid>
      <category>cs.IT</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Kamran Saeed, Ashfaq Khokhar, Shakil Ahmed</dc:creator>
    </item>
    <item>
      <title>Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations</title>
      <link>https://arxiv.org/abs/2507.09719</link>
      <description>arXiv:2507.09719v2 Announce Type: cross 
Abstract: We analyze the power consumption of quantum key distribution (QKD) networks under various protocol and detector configurations. Using realistic network topologies, we evaluate discrete-variable vs continuous-variable QKD and optimize device placement, quantifying power trade-offs of SNSPD vs APD detectors and the benefits of optical bypass.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09719v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaheng Xiong, Qiaolun Zhang, Yoann Pi\'etri, Raja Yehia, Raouf Boutaba, Francesco Musumeci, Massimo Tornatore</dc:creator>
    </item>
    <item>
      <title>Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems</title>
      <link>https://arxiv.org/abs/2507.09859</link>
      <description>arXiv:2507.09859v1 Announce Type: cross 
Abstract: Self-Sovereign Identity (SSI) offers significant potential for managing identities in the Internet of Things (IoT), enabling decentralized authentication and credential management without reliance on centralized entities. However, existing SSI frameworks often limit credential issuance and revocation to trusted entities, such as IoT manufacturers, which restricts flexibility in dynamic IoT ecosystems. In this paper, we propose a blockchain-based SSI framework that allows any individual with a verifiable trust linkage to act as a credential issuer, ensuring decentralized and scalable identity management. Our framework incorporates a layered architecture, where trust is dynamically established through endorsement-based calculations and maintained via a hierarchical chain-of-trust mechanism. Blockchain serves as the Verifiable Data Registry, ensuring transparency and immutability of identity operations, while smart contracts automate critical processes such as credential issuance, verification, and revocation. A proof-of-concept implementation demonstrates that the proposed framework is feasible and incurs minimal overheads compared to the baseline, making it well-suited for dynamic and resource-constrained IoT environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09859v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guntur Dharma Putra, Bagus Rakadyanto Oktavianto Putra</dc:creator>
    </item>
    <item>
      <title>Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2507.10259</link>
      <description>arXiv:2507.10259v1 Announce Type: cross 
Abstract: The rapid growth of large language model (LLM) services imposes increasing demands on distributed GPU inference infrastructure. Most existing scheduling systems rely on the current system state to make decisions, without considering how task demand and resource availability evolve over time. This lack of temporal awareness leads to inefficient GPU utilization, high task migration overhead, and poor system responsiveness under dynamic workloads. In this work, we identify the fundamental limitations of these instantaneous-state-only scheduling approaches and propose Temporal Optimal Resource scheduling via Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling framework that captures both long-term workload patterns and short-term execution constraints. It adopts a two-layer design: a macro-level scheduler leverages reinforcement learning and optimal transport to coordinate inter-region task distribution, while a micro-level allocator refines task-to-server assignments within each region to reduce latency and switching costs. Experimental results across multiple network topologies show that TORTA reduces average inference response time by up to 15\%, improves load balance by approximately 4-5\%, and cuts total operational cost by 10-20\% compared to state-of-the-art baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10259v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengze Du, Zhiwei Yu, Heng Xu, Haojie Wang, Bo liu, Jialong Li</dc:creator>
    </item>
    <item>
      <title>DNS Tunneling: Threat Landscape and Improved Detection Solutions</title>
      <link>https://arxiv.org/abs/2507.10267</link>
      <description>arXiv:2507.10267v1 Announce Type: cross 
Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in security due to its capacity to hide harmful actions within DNS traffic that appears to be normal and legitimate. Traditional detection methods are based on rule-based approaches or signature matching methods that are often insufficient to accurately identify such covert communication channels. This research is about effectively detecting DNS tunneling. We propose a novel approach to detect DNS tunneling with machine learning algorithms. We combine machine learning algorithms to analyze the traffic by using features extracted from DNS traffic. Analyses results show that the proposed approach is a good candidate to detect DNS tunneling accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10267v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Novruz Amirov, Baran Isik, Bilal Ihsan Tuncer, Serif Bahtiyar</dc:creator>
    </item>
    <item>
      <title>Cost and Power-Consumption Analysis for Power Profile Monitoring with Multiple Monitors per Link in Optical Networks</title>
      <link>https://arxiv.org/abs/2407.04977</link>
      <description>arXiv:2407.04977v4 Announce Type: replace 
Abstract: As deploying large amounts of monitoring equipment results in elevated cost and power consumption, novel low-cost monitoring methods are being continuously investigated. A new technique called Power Profile Monitoring (PPM) has recently gained traction thanks to its ability to monitor an entire lightpath using a single post-processing unit at the lightpath receiver. PPM does not require to deploy an individual monitor for each span, as in the traditional monitoring technique using Optical Time-Domain Reflectometer (OTDR). In this work, we aim to quantify the cost and power consumption of PPM (using OTDR as a baseline reference), as this analysis can provide guidelines for the implementation and deployment of PPM. First, we discuss how PPM and OTDR monitors are deployed, and we formally state a new Optimized Monitoring Placement (OMP) problem for PPM. Solving the OMP problem allows to identify the minimum number of PPM monitors that guarantees that all links in the networks are monitored by at least $n$ PPM monitors (note that using $n&gt;1$ allows for increased monitoring accuracy). We prove the NP-hardness of the OMP problem and formulate it using an Integer Linear Programming (ILP) model. Finally, we also devise a heuristic algorithm for the OMP problem to scale to larger topologies. Our numerical results, obtained on realistic topologies, suggest that the cost (and power) of one PPM module should be lower than 2.6 times that of one OTDR for nation-wide and 10.2 times for continental-wide topology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04977v4</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiaolun Zhang, Patricia Layec, Alix May, Annalisa Morea, Aryanaz Attarpour, Massimo Tornatore</dc:creator>
    </item>
    <item>
      <title>UAV Communications: Impact of Obstacles on Channel Characteristics</title>
      <link>https://arxiv.org/abs/2412.17934</link>
      <description>arXiv:2412.17934v4 Announce Type: replace 
Abstract: In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as effective platforms for carrying Wi-Fi Access Points (APs) and cellular Base Stations (BSs), enabling low-cost, agile, and flexible wireless networks with high Quality of Service (QoS). The next generation of wireless communications will rely on increasingly higher frequencies, which are easily obstructed by obstacles. One of the most critical concepts yet to be fully addressed is positioning the UAV at optimal coordinates while accounting for obstacles. To ensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS, and establish reliable wireless links with maximum coverage, obstacles must be integrated into the proposed placement algorithms. This paper introduces a simulation-based measurement approach for characterizing an air-to-ground (AG) channel in a simple scenario. By considering obstacles, we present a novel perspective on channel characterization. The results, in terms of throughput, packet delivery, packet loss, and delay, are compared using the proposed positioning approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17934v4</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamal Shayegan</dc:creator>
    </item>
    <item>
      <title>Enhancements to P4TG: Protocols, Performance, and Automation</title>
      <link>https://arxiv.org/abs/2501.17127</link>
      <description>arXiv:2501.17127v4 Announce Type: replace 
Abstract: P4TG is a hardware-based traffic generator (TG) running on the Intel Tofino 1 ASIC and was programmed using the programming language P4. In its initial version, P4TG could generate up to 10x100 Gb/s of traffic and directly measure rates, packet loss, and other metrics in the data plane. Many researchers and industrial partners requested new features to be incorporated into P4TG since its publication in 2023. With the recently added features, P4TG supports the generation of packets encapsulated with a customizable VLAN, QinQ, VxLAN, MPLS, and SRv6 header. Further, generation of IPv6 traffic is added and P4TG is ported to the Intel Tofino 2 platform enabling a generation capability of up to 10x400 Gb/s. The improvement in user experience focuses on ease of operation. Features like automated ARP replies, improved visualization, report generation, and automated testing based on the IMIX distribution and RFC 2544 are added. Future work on P4TG includes NDP to facilitate IPv6 traffic, and a NETCONF integration to further ease the configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17127v4</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.15496/publikation-105106</arxiv:DOI>
      <dc:creator>Fabian Ihle, Etienne Zink, Steffen Lindner, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Rust Barefoot Runtime (RBFRT): Fast Runtime Control for the Intel Tofino</title>
      <link>https://arxiv.org/abs/2501.17271</link>
      <description>arXiv:2501.17271v4 Announce Type: replace 
Abstract: Data plane programming enables the programmability of network devices with domain-specific programming languages, like P4. One commonly used P4-programmable hardware target is the Intel Tofino switching ASIC. The runtime behavior of an implemented P4 program on Tofino can be configured with shell scripts or a Python library from Barefoot provided with the Tofino. Both are limited in their capabilities and usability. This paper introduces the Rust Barefoot Runtime (RBFRT), a Rust-based control plane library. The RBFRT provides a fast and memory-safe interface to configure the Intel Tofino. We showed that the RBFRT achieves a higher insertion rate for MAT entries and has a shorter response time compared to the Python library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17271v4</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.15496/publikation-105108</arxiv:DOI>
      <dc:creator>Etienne Zink, Moritz Fl\"uchter, Steffen Lindner, Fabian Ihle, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Multi-objective Low-altitude IRS-assisted ISAC Optimization via Generative AI-enhanced Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.10687</link>
      <description>arXiv:2502.10687v2 Announce Type: replace 
Abstract: Integrated sensing and communication (ISAC) has garnered substantial research interest owing to its pivotal role in advancing the development of next-generation (6G) wireless networks. However, achieving a performance balance between communication and sensing in the dual-function radar communication (DFRC)-based ISAC system remains a significant challenge. In this paper, a low-altitude intelligent reflecting surface (IRS)-assisted ISAC system is explored, where a base station (BS) supports dual-functional operations, enabling both data transmission for multiple users and sensing for a blocked target, with the channel quality enhanced by an IRS mounted on the unmanned aerial vehicle (UAV). Moreover, we formulate an integrated communication, sensing, and energy efficiency multi-objective optimization problem (CSEMOP), which aims to maximize the communication rate of the users and the sensing rate of the target, while minimizing UAV propulsion energy consumption by jointly optimizing the BS beamforming matrix, IRS phase shifts, the flight velocity and angle of the UAV. Considering the non-convexity, trade-off, and dynamic nature of the formulated CSEMOP, we propose a generative diffusion model-based deep deterministic policy gradient (GDMDDPG) algorithm to solve the problem. Specifically, the diffusion model is incorporated into the actor network of DDPG to improve the action quality, with noise perturbation mechanism for better exploration and recent prioritized experience replay (RPER) sampling mechanism for enhanced training efficiency. Simulation results indicate that the GDMDDPG algorithm delivers superior performance compared to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10687v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwen Xie, Geng Sun, Jiacheng Wang, Hongyang Du, Jiawen Kang, Dusit Niyato, Kaibin Huang, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>Direct-to-Cell: A First Look into Starlink's Direct Satellite-to-Device Radio Access Network through Crowdsourced Measurements</title>
      <link>https://arxiv.org/abs/2506.00283</link>
      <description>arXiv:2506.00283v5 Announce Type: replace 
Abstract: Low Earth Orbit (LEO) satellite mega-constellations have recently emerged as a viable access solution for broadband services in underserved areas. In 2024, Direct Satellite-to-Device (DS2D) communications, which enable unmodified smartphones to connect directly to spaceborne base stations, entered large-scale beta testing, with Starlink globally leading deployments. This paper presents the first measurement study of commercial DS2D services. Using crowdsourced mobile network data collected in the U.S. between October 2024 and April 2025, our research derives evidence-based insights into the capabilities, limitations, and prospective evolution of DS2D technologies providing Supplemental Coverage from Space (SCS) services to expand existing mobile network connectivity. We observe a strong correlation between the number of satellites deployed and the expanding extension of observed measurements, concentrated in accessible but poorly covered areas by terrestrial networks, such as national parks and large low-density counties. The data reveal stable physical-layer value measurement throughout the observation period, with a lower median RSRP (24-dB difference) and a higher RSRQ (3 dB difference) compared to terrestrial networks, reflecting the SMS-only usage of the DS2D network during this period. Based on SINR measurements, we estimate the expected performance of the announced DS2D mobile data service to be around 4 Mbps per beam in outdoor conditions. We also discuss strategies to expand this capacity up to 12 Mbps in the future, depending on key regulatory decisions regarding satellite licenses, spectrum availability, and allowable radiated power levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00283v5</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge Garcia-Cabeza, Javier Albert-Smet, Zoraida Frias, Luis Mendo, Santiago Andr\'es Azcoitia, Eduardo Yraola</dc:creator>
    </item>
    <item>
      <title>An Integrated Blockchain and IPFS Solution for Secure and Efficient Source Code Repository Hosting using Middleman Approach</title>
      <link>https://arxiv.org/abs/2409.14530</link>
      <description>arXiv:2409.14530v2 Announce Type: replace-cross 
Abstract: Centralized version control systems (VCS) are vital for software development but pose risks of data loss and ownership disputes. While blockchain offers a decentralized alternative, existing solutions are often hindered by high latency, compromising the real-time collaboration essential for modern workflows. This study introduces a novel hybrid architecture combining the security of the Ethereum blockchain and the InterPlanetary File System (IPFS) with two key contributions: 1) Shamir's Secret Sharing (SSS) to create a trust-minimized model for key distribution, and 2) an authoritative-first, optimistic-fallback retrieval protocol utilizing a temporary middleware to decouple the user experience from blockchain confirmation delays. We implemented a full prototype and conducted a comprehensive performance evaluation on the public Sepolia testnet. Our results demonstrate that this architecture not only provides a secure, auditable, and resilient platform for source code hosting but also achieves highly competitive user-perceived performance. Our user-perceived push time reduces submission latency by up to 49% compared to a standard git push for common repository sizes, proving that a well-designed decentralized VCS can balance the core tenets of security and decentralization with the practical need for speed and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14530v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Rafid Haque, Sakibul Islam Munna, Sabbir Ahmed, Md. Tahmid Islam, Md Mehedi Hassan Onik, A. B. M. Ashikur Rahman</dc:creator>
    </item>
    <item>
      <title>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</title>
      <link>https://arxiv.org/abs/2506.09397</link>
      <description>arXiv:2506.09397v4 Announce Type: replace-cross 
Abstract: The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09397v4</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos</dc:creator>
    </item>
  </channel>
</rss>
