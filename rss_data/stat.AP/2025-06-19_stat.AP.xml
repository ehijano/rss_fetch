<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>GLASD: A Loss-Function-Agnostic Global Optimizer for Robust Correlation Estimation under Data Contamination and Heavy Tails</title>
      <link>https://arxiv.org/abs/2506.14801</link>
      <description>arXiv:2506.14801v1 Announce Type: new 
Abstract: Robust correlation estimation is essential in high-dimensional settings, particularly when data are contaminated by outliers or exhibit heavy-tailed behavior. Many robust loss functions of practical interest-such as those involving truncation or redescending M-estimators-lead to objective functions that are inherently non-convex and non-differentiable. Traditional methods typically focus on a single loss function tailored to a specific contamination model and develop custom algorithms tightly coupled with that loss, limiting generality and adaptability. We introduce GLASD (Global Adaptive Stochastic Descent), a general-purpose black-box optimization algorithm designed to operate over the manifold of positive definite correlation matrices. Unlike conventional solvers, GLASD requires no gradient information and imposes no assumptions of convexity or smoothness, making it ideally suited for optimizing a wide class of loss functions-including non-convex, non-differentiable, or discontinuous objectives. This flexibility allows GLASD to serve as a unified framework for robust estimation under arbitrary user-defined criteria. We demonstrate its effectiveness through extensive simulations involving contaminated and heavy-tailed distributions, as well as a real-data application to breast cancer proteomic network inference, where GLASD successfully identifies biologically plausible interactions despite the presence of outliers. The proposed method is scalable, constraint-aware, and available as open-source software at GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14801v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das</dc:creator>
    </item>
    <item>
      <title>How many federal employees are not satisfied? Using response times to estimate population proportions under the survey variable cause model</title>
      <link>https://arxiv.org/abs/2506.14915</link>
      <description>arXiv:2506.14915v1 Announce Type: new 
Abstract: We propose a statistical model to estimate population proportions under the survey variable cause model (Groves 2006), the setting in which the characteristic measured by the survey has a direct causal effect on survey participation. For example, we estimate employee satisfaction from a survey in which the decision of an employee to participate depends on their satisfaction. We model the time at which a respondent 'arrives' to take the survey, leveraging results from the counting processes literature that has been developed to analyze similar problems with survival data. Our approach is particularly useful for nonresponse bias analysis because it relies on different assumptions than traditional adjustments such as poststratification, which assumes the common cause model, the setting in which external factors explain the characteristic measured by the survey and participation. Our motivation is the Federal Employee Viewpoint Survey, which asks federal employees whether they are satisfied with their work organization. Our model suggests that the sample proportion overestimates the proportion of federal employees that are not satisfied with their work organization even after adjustment by poststratification. Employees that are not satisfied likely select into the survey, and this selection cannot be explained by personal characteristics like race, gender, and occupation or work-place characteristics like agency, unit, and location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14915v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>Digital twin for virtual sensing of ferry quays via a Gaussian Process Latent Force Model</title>
      <link>https://arxiv.org/abs/2506.14925</link>
      <description>arXiv:2506.14925v1 Announce Type: new 
Abstract: Ferry quays experience rapid deterioration due to their exposure to harsh maritime environments and ferry impacts. Vibration-based structural health monitoring offers a valuable approach to assessing structural integrity and understanding the structural implications of these impacts. However, practical limitations often restrict sensor placement at critical locations. Consequently, virtual sensing techniques become essential for establishing a Digital Twin and estimating the structural response. This study investigates the application of the Gaussian Process Latent Force Model (GPLFM) for virtual sensing on the Magerholm ferry quay, combining in-operation experimental data collected during a ferry impact with a detailed physics-based model. The proposed Physics-Encoded Machine Learning model integrates a reduced-order structural model with a data-driven GPLFM representing the unknown impact forces via their modal contributions. Significant challenges are addressed for the development of the Digital Twin of the ferry quay, including unknown impact characteristics (location, direction, intensity), time-varying boundary conditions, and sparse sensor configurations. Results show that the GPLFM provides accurate acceleration response estimates at most locations, even under simplifying modeling assumptions such as linear time-invariant behavior during the impact phase. Lower accuracy was observed at locations in the impact zone. A numerical study was conducted to explore an optimal real-world sensor placement strategy using a Backward Sequential Sensor Placement approach. Sensitivity analyses were conducted to examine the influence of sensor types, sampling frequencies, and incorrectly assumed damping ratios. The results suggest that the GP latent forces can help accommodate modeling and measurement uncertainties, maintaining acceptable estimation accuracy across scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14925v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luigi Sibille, Torodd Skjerve Nord, Alice Cicirello</dc:creator>
    </item>
    <item>
      <title>Interpretable Machine Learning Model for Early Prediction of 30-Day Mortality in ICU Patients With Coexisting Hypertension and Atrial Fibrillation: A Retrospective Cohort Study</title>
      <link>https://arxiv.org/abs/2506.15036</link>
      <description>arXiv:2506.15036v1 Announce Type: new 
Abstract: Hypertension and atrial fibrillation (AF) often coexist in critically ill patients, significantly increasing mortality rates in the ICU. Early identification of high-risk individuals is crucial for targeted interventions. However, limited research has focused on short-term mortality prediction for this subgroup.
  This study analyzed 1,301 adult ICU patients with hypertension and AF from the MIMIC-IV database. Data including chart events, laboratory results, procedures, medications, and demographic information from the first 24 hours of ICU admission were extracted. After quality control, missing data imputation, and feature selection, 17 clinically relevant variables were retained. The cohort was split into training (70%) and test (30%) sets, with outcome-weighted training applied to address class imbalance. The CatBoost model, along with five baseline models (LightGBM, XGBoost, logistic regression, Naive Bayes, and neural networks), was evaluated using five-fold cross-validation, with AUROC as the primary performance metric. Model interpretability was assessed using SHAP, ALE, and DREAM analyses.
  The CatBoost model showed strong performance with an AUROC of 0.889 (95% CI: 0.840-0.924), accuracy of 0.831, and sensitivity of 0.837. Key predictors identified by SHAP and other methods included the Richmond-RAS Scale, pO2, CefePIME, and Invasive Ventilation, demonstrating the model's robustness and clinical applicability.
  This model shows strong performance and interpretability in early mortality prediction, enabling early intervention and personalized care decisions. Future work will involve multi-center validation and extending the approach to other diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15036v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuheng Chen, Yong Si, Junyi Fan, Li Sun, Greg Placencia, Elham Pishgar, Kamiar Alaei, Maryam Pishgar</dc:creator>
    </item>
    <item>
      <title>Predicting Short-Term Mortality in Elderly ICU Patients with Diabetes and Heart Failure: A Distributional Inference Framework</title>
      <link>https://arxiv.org/abs/2506.15058</link>
      <description>arXiv:2506.15058v1 Announce Type: new 
Abstract: Elderly ICU patients with coexisting diabetes mellitus and heart failure experience markedly elevated short-term mortality, yet few predictive models are tailored to this high-risk group. Diabetes mellitus affects nearly 30% of U.S. adults over 65, and significantly increases the risk of heart failure. When combined, these conditions worsen frailty, renal dysfunction, and hospitalization risk, leading to one-year mortality rates of up to 40%. Despite their clinical burden and complexity, no established models address individualized mortality prediction in elderly ICU patients with both diabetes mellitus and heart failure.
  We developed and validated a probabilistic mortality prediction framework using the MIMIC-IV database, targeting 65-90-year-old patients with both diabetes mellitus and heart failure. Using a two-stage feature selection pipeline and a cohort of 1,478 patients, we identified 19 clinically significant variables that reflect physiology, comorbidities, and intensity of treatment. Among six ML models benchmarked, CatBoost achieved the highest test AUROC (0.863), balancing performance and interpretability.
  To enhance clinical relevance, we employed the DREAM algorithm to generate posterior mortality risk distributions rather than point estimates, enabling assessment of both risk magnitude and uncertainty. This distribution-aware approach facilitates individualized triage in complex ICU settings.
  Interpretability was further supported via ablation and ALE analysis, highlighting key predictors such as APS III, oxygen flow, GCS eye, and Braden Mobility. Our model enables transparent, personalized, and uncertainty-informed decision support for high-risk ICU populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15058v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junyi Fan, Shuheng Chen, Li Sun, Yong Si, Elham Pishgar, Kamiar Alaei, Greg Placencia, Maryam Pishgar</dc:creator>
    </item>
    <item>
      <title>Determination of Optimum Warranty Region for Two Dimensional Dependent Data</title>
      <link>https://arxiv.org/abs/2506.15152</link>
      <description>arXiv:2506.15152v1 Announce Type: new 
Abstract: This paper presents a method for determining the optimal two-dimensional warranty region for age and mileage scales across all possible combinations of free replacement warranty (FRW), prorata warranty (PRW), and FRW-PRW combined policies. The operational time or lifetime and usage of the products are modeled using a bivariate Gumbel copula with Weibull as the marginal distribution. The optimal warranty region is derived by maximizing an expected utility function, which incorporates two cost components: the economic benefit function and the warranty cost function, specifically constructed for the two-dimensional warranty scenario. To obtain the optimal warranty region, a real-world dataset of traction motors, including age and mileage information, is analyzed. The results show that considering a two-dimensional warranty cost function yields the highest utility compared to all other scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15152v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rathin Das, Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>A sequential ensemble approach to epidemic modeling: Combining Hawkes and SEIR models using SMC$^2$</title>
      <link>https://arxiv.org/abs/2506.15511</link>
      <description>arXiv:2506.15511v1 Announce Type: new 
Abstract: This paper proposes a sequential ensemble methodology for epidemic modeling that integrates discrete-time Hawkes processes (DTHP) and Susceptible-Exposed-Infectious-Removed (SEIR) models. Motivated by the need for accurate and reliable epidemic forecasts to inform timely public health interventions, we develop a flexible model averaging (MA) framework using Sequential Monte Carlo Squared. While generating estimates from each model individually, our approach dynamically assigns them weights based on their incrementally estimated marginal likelihoods, accounting for both model and parameter uncertainty to produce a single ensemble estimate. We assess the methodology through simulation studies mimicking abrupt changes in epidemic dynamics, followed by an application to the Irish influenza and COVID-19 outbreaks. Our results show that combining the two models can improve both estimates of the infection trajectory and reproduction number compared to using either model alone. Moreover, the MA consistently produces more stable and informative estimates of the time-varying reproduction number, with credible intervals that maintain appropriate coverage. These features are particularly valuable in high-uncertainty contexts where reliable situational awareness is essential. This research contributes to pandemic preparedness by enhancing forecast reliability and supporting more informed public health responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15511v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhorasso Temfack, Jason Wyse</dc:creator>
    </item>
    <item>
      <title>Image Processing Techniques to Identify and Quantify Spatiotemporal Carbon Cycle Extremes</title>
      <link>https://arxiv.org/abs/2506.15555</link>
      <description>arXiv:2506.15555v1 Announce Type: new 
Abstract: Rising atmospheric carbon dioxide due to human activities through fossil fuel emissions and land use changes have increased climate extremes such as heat waves and droughts that have led to and are expected to increase the occurrence of carbon cycle extremes. Carbon cycle extremes represent large anomalies in the carbon cycle that are associated with gains or losses in carbon uptake. Carbon cycle extremes could be continuous in space and time and cross political boundaries. Here, we present a methodology to identify large spatiotemporal extremes (STEs) in the terrestrial carbon cycle using image processing tools for feature detection. We characterized the STE events based on neighborhood structures that are three-dimensional adjacency matrices for the detection of spatiotemporal manifolds of carbon cycle extremes. We found that the area affected and carbon loss during negative carbon cycle extremes were consistent with continuous neighborhood structures. In the gross primary production data we used, 100 carbon cycle STEs accounted for more than 75\% of all the negative carbon cycle extremes. This paper presents a comparative analysis of the magnitude of carbon cycle STEs and attribution of those STEs to climate drivers as a function of neighborhood structures for two observational datasets and an Earth system model simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15555v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bharat Sharma, Jitendra Kumar, Auroop R. Ganguly, Forrest M. Hoffman</dc:creator>
    </item>
    <item>
      <title>Statistical post-processing of operational dual-resolution wind-speed ensemble forecasts</title>
      <link>https://arxiv.org/abs/2506.15578</link>
      <description>arXiv:2506.15578v1 Announce Type: new 
Abstract: Weather forecasting presents several challenges, including the chaotic nature of the atmosphere and the high computational demands of numerical weather prediction models. To achieve the most accurate predictions, the ideal scenario involves the lowest possible horizontal resolution and the largest ensemble size. This study provides a detailed comparative analysis of the forecast skill of the raw and post-processed medium- and extended-range wind-speed ensemble forecasts of the European Centre for Medium-Range Weather Forecasts issued at 9 km and 36 km horizontal resolutions, respectively, and their various mixtures. We utilized the ensemble model output statistic approach for forecast calibration with three different spatial training data selection techniques. First, we investigate the performance of the 50-member medium-range and 100-member extended-range predictions - referred to as high and low resolution, respectively - and their 150-member dual-resolution combination. Further, we examine whether the performance of raw and post-processed low-resolution forecasts can be improved by incorporating high-resolution ensemble members. Our results confirm that all post-processed forecasts outperform the raw ensemble predictions in terms of probabilistic calibration and point forecast accuracy and that post-processing considerably reduces the differences between the various configurations. We also show that spatial resolution is superior to the ensemble size; augmenting a sufficiently large ensemble of high-resolution forecasts with low-resolution predictions does not necessarily result in a gain in forecast skill. However, our study also highlights the clear benefit of the other direction, namely, incorporating high-resolution members into low-resolution ensemble forecasts, where the most significant gains are observed in configurations with the highest number of high-resolution members.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15578v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor Baran, M\'aria Lakatos</dc:creator>
    </item>
    <item>
      <title>Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning</title>
      <link>https://arxiv.org/abs/2506.14817</link>
      <description>arXiv:2506.14817v1 Announce Type: cross 
Abstract: Forecasting violent conflict at high spatial and temporal resolution remains a central challenge for both researchers and policymakers. This study presents a novel neural network architecture for forecasting three distinct types of violence -- state-based, non-state, and one-sided -- at the subnational (priogrid-month) level, up to 36 months in advance. The model jointly performs classification and regression tasks, producing both probabilistic estimates and expected magnitudes of future events. It achieves state-of-the-art performance across all tasks and generates approximate predictive posterior distributions to quantify forecast uncertainty.
  The architecture is built on a Monte Carlo Dropout Long Short-Term Memory (LSTM) U-Net, integrating convolutional layers to capture spatial dependencies with recurrent structures to model temporal dynamics. Unlike many existing approaches, it requires no manual feature engineering and relies solely on historical conflict data. This design enables the model to autonomously learn complex spatiotemporal patterns underlying violent conflict.
  Beyond achieving state-of-the-art predictive performance, the model is also highly extensible: it can readily integrate additional data sources and jointly forecast auxiliary variables. These capabilities make it a promising tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14817v1</guid>
      <category>stat.OT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon P. von der Maase</dc:creator>
    </item>
    <item>
      <title>CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration</title>
      <link>https://arxiv.org/abs/2506.14843</link>
      <description>arXiv:2506.14843v1 Announce Type: cross 
Abstract: Machine Learning (ML) is used to tackle various tasks, such as disease classification and prediction. The effectiveness of ML models relies heavily on having large amounts of complete data. However, healthcare data is often limited or incomplete, which can hinder model performance. Additionally, issues like the trustworthiness of solutions vary with the datasets used. The lack of transparency in some ML models further complicates their understanding and use. In healthcare, particularly in the case of Age-related Macular Degeneration (AMD), which affects millions of older adults, early diagnosis is crucial due to the absence of effective treatments for reversing progression. Diagnosing AMD involves assessing retinal images along with patients' symptom reports. There is a need for classification approaches that consider genetic, dietary, clinical, and demographic factors. Recently, we introduced the -Comprehensive Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed at improving AMD stage classification. CACTUS offers explainability and flexibility, outperforming standard ML models. It enhances decision-making by identifying key factors and providing confidence in its results. The important features identified by CACTUS allow us to compare with existing medical knowledge. By eliminating less relevant or biased data, we created a clinical scenario for clinicians to offer feedback and address biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14843v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Gherardini, Imre Lengyel, Tunde Peto, Caroline C. W. Klaverd, Magda A. Meester-Smoord, Johanna Maria Colijnd, EYE-RISK Consortium, E3 Consortium, Jose Sousa</dc:creator>
    </item>
    <item>
      <title>Enhancing the prediction of publications' long-term impact using early citations, readerships, and non-scientific factors</title>
      <link>https://arxiv.org/abs/2506.15040</link>
      <description>arXiv:2506.15040v1 Announce Type: cross 
Abstract: This study aims to improve the accuracy of long-term citation impact prediction by integrating early citation counts, Mendeley readership, and various non-scientific factors, such as journal impact factor, authorship and reference list characteristics, funding and open-access status. Traditional citation-based models often fall short by relying solely on early citations, which may not capture broader indicators of a publication's potential influence. By incorporating non-scientific predictors, this model provides a more nuanced and comprehensive framework that outperforms existing models in predicting long-term impact. Using a dataset of Italian-authored publications from the Web of Science, regression models were developed to evaluate the impact of these predictors over time. Results indicate that early citations and Mendeley readership are significant predictors of long-term impact, with additional contributions from factors like authorship diversity and journal impact factor. The study finds that open-access status and funding have diminishing predictive power over time, suggesting their influence is primarily short-term. This model benefits various stakeholders, including funders and policymakers, by offering timely and more accurate assessments of emerging research. Future research could extend this model by incorporating broader altmetrics and expanding its application to other disciplines and regions. The study concludes that integrating non-citation-based factors with early citations captures a more complex view of scholarly impact, aligning better with real-world research influence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15040v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Abramo, Tindaro Cicero, Ciriaco Andrea D'Angelo</dc:creator>
    </item>
    <item>
      <title>Candidate Dark Galaxy-2: Validation and Analysis of an Almost Dark Galaxy in the Perseus Cluster</title>
      <link>https://arxiv.org/abs/2506.15644</link>
      <description>arXiv:2506.15644v1 Announce Type: cross 
Abstract: Candidate Dark Galaxy-2 (CDG-2) is a potential dark galaxy consisting of four globular clusters (GCs) in the Perseus cluster, first identified in Li et al. (2025) through a sophisticated statistical method. The method searched for over-densities of GCs from a \textit{Hubble Space Telescope} (\textit{HST}) survey targeting Perseus. Using the same \textit{HST} images and the new imaging data from the \textit{Euclid} survey, we report the detection of extremely faint but significant diffuse emission around the four GCs of CDG-2. We thus have exceptionally strong evidence that CDG-2 is a galaxy. This is the first galaxy detected purely through its GC population. Under the conservative assumption that the four GCs make up the entire GC population, preliminary analysis shows that CDG-2 has a total luminosity of $L_{V, \mathrm{gal}}= 6.2\pm{3.0} \times 10^6 L_{\odot}$ and a minimum GC luminosity of $L_{V, \mathrm{GC}}= 1.03\pm{0.2}\times 10^6 L_{\odot}$. Our results indicate that CDG-2 is one of the faintest galaxies having associated GCs, while at least $\sim 16.6\%$ of its light is contained in its GC population. This ratio is likely to be much higher ($\sim 33\%$) if CDG-2 has a canonical GC luminosity function (GCLF). In addition, if the previously observed GC-to-halo mass relations apply to CDG-2, it would have a minimum dark matter halo mass fraction of $99.94\%$ to $99.98\%$. If it has a canonical GCLF, then the dark matter halo mass fraction is $\gtrsim 99.99\%$. Therefore, CDG-2 may be the most GC dominated galaxy and potentially one of the most dark matter dominated galaxies ever discovered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15644v1</guid>
      <category>astro-ph.GA</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3847/2041-8213/adddab</arxiv:DOI>
      <arxiv:journal_reference>The Astrophysical Journal Letters, 986 (2), L18 (2025)</arxiv:journal_reference>
      <dc:creator>Dayi Li, Qing Liu, Gwendolyn Eadie, Roberto Abraham, Francine Marleau, William Harris, Pieter van Dokkum, Aaron Romanowsky, Shany Danieli, Patrick Brown, Alex Stringer</dc:creator>
    </item>
    <item>
      <title>Machine learning based prediction of dynamical clustering in granular gases</title>
      <link>https://arxiv.org/abs/2506.15657</link>
      <description>arXiv:2506.15657v1 Announce Type: cross 
Abstract: When dense granular gases are continuously excited under microgravity conditions, spatial inhomogeneities of the particle number density can emerge. A significant share of particles may collect in strongly overpopulated regions, called clusters. This dynamical clustering, or gas-cluster transition, is caused by a complex interplay and balance between the energy influx and dissipation in particle collisions. Particle number density, container geometry, and excitation strength influence this transition. We perform Discrete Element Method (DEM) simulations for ensembles of frictional spheres in a cuboid container and apply the Kolmogorov Smirnov test and a caging criterion to the local packing fraction profiles to detect clusters. Machine learning can be used to study the gas-cluster transition, and can be a promising alternative to identify the state of the system for a given set of system parameters without time-consuming complex DEM simulations. We test various machine learning models and identify the best models to predict dynamical clustering of frictional spheres in a specific experimental geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15657v1</guid>
      <category>cond-mat.soft</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sai Preetham Sata, Ralf Stannarius, Benjamin Noack, Dmitry Puzyrev</dc:creator>
    </item>
    <item>
      <title>The Impact of Social Isolation on Subjective Cognitive Decline in Older Adults: A Study Based on Network Analysis and Longitudinal Model</title>
      <link>https://arxiv.org/abs/2506.13914</link>
      <description>arXiv:2506.13914v2 Announce Type: replace 
Abstract: Social isolation (SI) in older adults has emerged as a critical mental health concern, with established links to cognitive decline. While depression is hypothesized to mediate this relationship, the longitudinal mechanisms remain unclear. This study employed network analysis and cross-lagged modeling to examine these relationships during pandemic-related social restrictions. We collected data from 1,230 older adults (Mage=64.49, SD=3.84) across three timepoints (during lockdown, immediately post-lockdown, and 6 months later). Network analysis identified depressive symptoms (particularly PHQ-9 item 9) as central nodes bridging SI and subjective cognitive decline (SCD). Longitudinal analyses revealed: 1) T1 SI predicted T2 depression; 2) T2 depression predicted T3 SCD. These patterns held for both online and offline SI, with excellent model fit. Our findings demonstrate depression's mediating role in the "SI-depression-SCD" pathway, highlighting how reduced social connections may gradually impair cognitive self-assessment through depressive symptoms. The results underscore the universal mental health impact of SI, regardless of interaction modality (virtual/in-person). These insights suggest that combined interventions targeting both social connection and depression management may be particularly effective for mitigating age-related cognitive decline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13914v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingchen Liu, Haixin Jiang</dc:creator>
    </item>
    <item>
      <title>Quantile Treatment Effects in High Dimensional Panel Data</title>
      <link>https://arxiv.org/abs/2504.00785</link>
      <description>arXiv:2504.00785v2 Announce Type: replace-cross 
Abstract: We introduce novel estimators for quantile causal effects with high dimensional panel data (large $N$ and $T$), where only one or a few units are affected by the intervention or policy. Our method extends the generalized synthetic control method \citep{xu_2017} from average treatment effects on the treated to quantile treatment effects on the treated, allowing the underlying factor structure to change across the quantile of the interested outcome distribution. Our method involves estimating the quantile-dependent factors using the control group, followed by a quantile regression to estimate the quantile treatment effect using the treated units. We establish the asymptotic properties of our estimators and propose a bootstrap procedure for statistical inference, supported by simulation studies. An empirical application of the 2008 China Stimulus Program is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00785v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Li Zheng</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis of Priors in the Bayesian Dirichlet Auto-Regressive Moving Average Model</title>
      <link>https://arxiv.org/abs/2506.13973</link>
      <description>arXiv:2506.13973v2 Announce Type: replace-cross 
Abstract: Prior choice can strongly influence Bayesian Dirichlet ARMA (B-DARMA) inference for compositional time-series. Using simulations with (i) correct lag order, (ii) overfitting, and (iii) underfitting, we assess five priors: weakly-informative, horseshoe, Laplace, mixture-of-normals, and hierarchical. With the true lag order, all priors achieve comparable RMSE, though horseshoe and hierarchical slightly reduce bias. Under overfitting, aggressive shrinkage-especially the horseshoe-suppresses noise and improves forecasts, yet no prior rescues a model that omits essential VAR or VMA terms.
  We then fit B-DARMA to daily SP 500 sector weights using an intentionally large lag structure. Shrinkage priors curb spurious dynamics, whereas weakly-informative priors magnify errors in volatile sectors. Two lessons emerge: (1) match shrinkage strength to the degree of overparameterization, and (2) prioritize correct lag selection, because no prior repairs structural misspecification. These insights guide prior selection and model complexity management in high-dimensional compositional time-series applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13973v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Liz Medina, Robert E. Weiss</dc:creator>
    </item>
  </channel>
</rss>
