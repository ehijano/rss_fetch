<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 May 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Critical feature learning in deep neural networks</title>
      <link>https://arxiv.org/abs/2405.10761</link>
      <description>arXiv:2405.10761v1 Announce Type: new 
Abstract: A key property of neural networks driving their success is their ability to learn features from data. Understanding feature learning from a theoretical viewpoint is an emerging field with many open questions. In this work we capture finite-width effects with a systematic theory of network kernels in deep non-linear neural networks. We show that the Bayesian prior of the network can be written in closed form as a superposition of Gaussian processes, whose kernels are distributed with a variance that depends inversely on the network width N . A large deviation approach, which is exact in the proportional limit for the number of data points $P = \alpha N \rightarrow \infty$, yields a pair of forward-backward equations for the maximum a posteriori kernels in all layers at once. We study their solutions perturbatively to demonstrate how the backward propagation across layers aligns kernels with the target. An alternative field-theoretic formulation shows that kernel adaptation of the Bayesian posterior at finite-width results from fluctuations in the prior: larger fluctuations correspond to a more flexible network prior and thus enable stronger adaptation to data. We thus find a bridge between the classical edge-of-chaos NNGP theory and feature learning, exposing an intricate interplay between criticality, response functions, and feature scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10761v1</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kirsten Fischer, Javed Lindner, David Dahmen, Zohar Ringel, Michael Kr\"amer, Moritz Helias</dc:creator>
    </item>
    <item>
      <title>Integer Traffic Assignment Problem: Algorithms and Insights on Random Graphs</title>
      <link>https://arxiv.org/abs/2405.10763</link>
      <description>arXiv:2405.10763v1 Announce Type: new 
Abstract: Path optimization is a fundamental concern across various real-world scenarios, ranging from traffic congestion issues to efficient data routing over the internet. The Traffic Assignment Problem (TAP) is a classic continuous optimization problem in this field. This study considers the Integer Traffic Assignment Problem (ITAP), a discrete variant of TAP. ITAP involves determining optimal routes for commuters in a city represented by a graph, aiming to minimize congestion while adhering to integer flow constraints on paths. This restriction makes ITAP an NP-hard problem. While conventional TAP prioritizes repulsive interactions to minimize congestion, this work also explores the case of attractive interactions, related to minimizing the number of occupied edges. We present and evaluate multiple algorithms to address ITAP, including a message passing algorithm, a greedy approach, simulated annealing, and relaxation of ITAP to TAP. Inspired by studies of random ensembles in the large-size limit in statistical physics, comparisons between these algorithms are conducted on large sparse random regular graphs with a random set of origin-destination pairs. Our results indicate that while the simplest greedy algorithm performs competitively in the repulsive scenario, in the attractive case the message-passing-based algorithm and simulated annealing demonstrate superiority. We then investigate the relationship between TAP and ITAP in the repulsive case. We find that, as the number of paths increases, the solution of TAP converges toward that of ITAP, and we investigate the speed of this convergence. Depending on the number of paths, our analysis leads us to identify two scaling regimes: in one the average flow per edge is of order one, and in another the number of paths scales quadratically with the size of the graph, in which case the continuous relaxation solves the integer problem closely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10763v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rayan Harfouche, Giovanni Piccioli, Lenka Zdeborov\'a</dc:creator>
    </item>
    <item>
      <title>Generative modeling through internal high-dimensional chaotic activity</title>
      <link>https://arxiv.org/abs/2405.10822</link>
      <description>arXiv:2405.10822v1 Announce Type: cross 
Abstract: Generative modeling aims at producing new datapoints whose statistical properties resemble the ones in a training dataset. In recent years, there has been a burst of machine learning techniques and settings that can achieve this goal with remarkable performances. In most of these settings, one uses the training dataset in conjunction with noise, which is added as a source of statistical variability and is essential for the generative task. Here, we explore the idea of using internal chaotic dynamics in high-dimensional chaotic systems as a way to generate new datapoints from a training dataset. We show that simple learning rules can achieve this goal within a set of vanilla architectures and characterize the quality of the generated datapoints through standard accuracy measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10822v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samantha J. Fournier, Pierfrancesco Urbani</dc:creator>
    </item>
    <item>
      <title>Optimal quantum key distribution networks: capacitance versus security</title>
      <link>https://arxiv.org/abs/2312.04221</link>
      <description>arXiv:2312.04221v3 Announce Type: replace-cross 
Abstract: The rate and security of quantum communications between users placed at arbitrary points of a quantum communication network depend on the structure of the network, on its extension and on the nature of the communication channels. In this work we propose a strategy for the optimization of trusted-relays based networks that intertwines classical network approaches and quantum information theory. Specifically, by suitably defining a quantum communication efficiency functional, we identify the optimal quantum communication connections through the network by balancing security and the quantum communication rate. The optimized network is then constructed as the network of the maximal quantum communication efficiency connections and its performance is evaluated by studying the scaling of average properties as functions of the number of nodes and of the network spatial extension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04221v3</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41534-024-00828-7</arxiv:DOI>
      <arxiv:journal_reference>npj Quantum Inf 10, 44 (2024)</arxiv:journal_reference>
      <dc:creator>Lorenzo Cirigliano, Valentina Brosco, Claudio Castellano, Claudio Conti, Laura Pilozzi</dc:creator>
    </item>
  </channel>
</rss>
