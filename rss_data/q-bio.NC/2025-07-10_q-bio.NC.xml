<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Jul 2025 01:20:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers</title>
      <link>https://arxiv.org/abs/2507.06645</link>
      <description>arXiv:2507.06645v1 Announce Type: new 
Abstract: Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as e.g. accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy -- thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06645v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Klein, Sascha Meyen, Wieland Brendel, Felix A. Wichmann, Kristof Meding</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease</title>
      <link>https://arxiv.org/abs/2507.06326</link>
      <description>arXiv:2507.06326v1 Announce Type: cross 
Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's disease (PD), but conventional open-loop systems lack adaptability, are energy-inefficient due to continuous stimulation, and provide limited personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a closed-loop alternative, using biomarkers such as beta-band oscillations to dynamically modulate stimulation. While reinforcement learning (RL) holds promise for personalized aDBS control, existing methods suffer from high sample complexity, unstable exploration in binary action spaces, and limited deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a predictive reward model to reduce reliance on real-time feedback and employs Gumbel Softmax-based exploration for stable, differentiable policy updates in binary action spaces. Together, these components improve sample efficiency, exploration robustness, and compatibility with resource-constrained neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic simulation of Parkinsonian basal ganglia activity, demonstrating faster convergence, stronger suppression of pathological beta-band power, and resilience to post-training FP16 quantization. Our results show that SEA-DBS offers a practical and effective RL-based aDBS framework for real-time, resource-constrained neuromodulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06326v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harsh Ravivarapu, Gaurav Bagwe, Xiaoyong Yuan, Chunxiu Yu, Lan Zhang</dc:creator>
    </item>
    <item>
      <title>KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks</title>
      <link>https://arxiv.org/abs/2507.06381</link>
      <description>arXiv:2507.06381v1 Announce Type: cross 
Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling efficient training of recurrent dynamical systems such as Recurrent Neural Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics that are formed in these models exhibit features such as neural collapse and emergence of latent representations that may support the remarkable generalization properties of networks. In neuroscience, qualitative features of these representations are used to compare learning in biological and artificial systems. Despite recent progress, there remains a need for theoretical tools to rigorously understand the mechanisms shaping learned representations, especially in finite, non-linear models. Here, we show that the gradient flow, which describes how the model's dynamics evolve over GD, can be decomposed into a product that involves two operators: a Parameter Operator, K, and a Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in feed-forward neural networks, while P appears in Lyapunov stability and optimal control theory. We demonstrate two applications of our decomposition. First, we show how their interplay gives rise to low-dimensional latent dynamics under GD, and, specifically, how the collapse is a result of the network structure, over and above the nature of the underlying task. Second, for multi-task training, we show that the operators can be used to measure how objectives relevant to individual sub-tasks align. We experimentally and theoretically validate these findings, providing an efficient Pytorch package, \emph{KPFlow}, implementing robust analysis tools for general recurrent architectures. Taken together, our work moves towards building a next stage of understanding of GD learning in non-linear recurrent models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06381v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Hazelden, Laura Driscoll, Eli Shlizerman, Eric Shea-Brown</dc:creator>
    </item>
    <item>
      <title>Equivalence of stationary dynamical solutions in a directed chain and a Delay Differential Equation of neuroscientific relevance</title>
      <link>https://arxiv.org/abs/2506.11654</link>
      <description>arXiv:2506.11654v2 Announce Type: replace-cross 
Abstract: While synchronized states, and the dynamical pathways through which they emerge, are often regarded as the paradigm to understand the dynamics of information spreading on undirected networks of nonlinear dynamical systems, when we consider directed network architectures, dynamical stationary states can arise. To study this phenomenon we consider the simplest directed network, a single cycle, and excitable FitzHugh-Nagumo (FHN) neurons. We show numerically that a stationary dynamical state emerges in the form of a self-sustained traveling wave, through a saddle-point bifurcation of limit cycles that does not destabilize the global fixed point of the system. We then formulate an effective model for the dynamical steady state of the cycle in terms of a single-neuron Delay Differential Equation (DDE) featuring an explicitly delayed feedback, demonstrating numerically the possibility of mapping stationary solutions between the two models. The DDE based model is shown to reproduce the entire bifurcation, which also in this case does not destabilize the global fixed point, even though global properties differ in general between the systems. The discrete nature of the cycle graph is revealed as the origin of these coordinated states by the parametric analysis of solutions, and the DDE effective model is shown to preserve this feature accurately. Finally, the scaling of the inter-site propagation times hints to a solitonic nature of the wave state in the limit of large chain size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11654v2</guid>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Colombini, Nicola Guglielmi, Armando Bazzani</dc:creator>
    </item>
  </channel>
</rss>
