<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Visual Perception, Quantity of Information Function and the Concept of the Quantity of Information Continuous Splines</title>
      <link>https://arxiv.org/abs/2502.15730</link>
      <description>arXiv:2502.15730v1 Announce Type: new 
Abstract: The geometric shapes of the outside world objects hide an undisclosed emotional, psychological, artistic, aesthetic and shape-generating potential; they may attract or cause fear as well as a variety of other emotions. This suggests that living beings with vision perceive geometric objects within an information-handling process. However, not many studies have been performed for a better understanding of visual perception from the view of information theory and mathematical modelling, but the evidence first found by Attneave (1954) suggests that the concepts and techniques of information theory may shed light on a better and deeper understanding of visual perception. The quantity of information function can theoretically explain the concentration of information on the visual contours, and, based on this, we first propose the concept of the quantity of information continuous splines for visualization of shapes from a given set of discrete data without adding any in-between points with curvature extreme. Additionally, we first discover planar curve with a constant quantity of information function and demonstrate one of the conditions when a monotonic curvature curve has a constant quantity of information function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15730v1</guid>
      <category>q-bio.NC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Scientific Visualization 8(1): 168-178, 2016</arxiv:journal_reference>
      <dc:creator>Rushan Ziatdinov</dc:creator>
    </item>
    <item>
      <title>MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding</title>
      <link>https://arxiv.org/abs/2502.15786</link>
      <description>arXiv:2502.15786v1 Announce Type: new 
Abstract: Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 16.4%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15786v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weikang Qiu, Zheng Huang, Haoyu Hu, Aosong Feng, Yujun Yan, Rex Ying</dc:creator>
    </item>
    <item>
      <title>Electrophysiological Investigation of Insect Pain Threshold</title>
      <link>https://arxiv.org/abs/2502.16088</link>
      <description>arXiv:2502.16088v1 Announce Type: new 
Abstract: The question of whether insects experience pain has long been debated in neuroscience and animal behavior research. Increasing evidence suggests that insects possess the ability to detect and respond to noxious stimuli, exhibiting behaviors indicative of pain perception. This study investigates the relationship between pain stimuli and physiological responses in crickets (Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We applied a range of mechanical, chemical, thermal, and electrical stimuli to crickets, recording ECG and EEG data while employing a deep learning-based model to classify pain levels. Our findings revealed significant heart rate changes and EEG fluctuations in response to various stimuli, with the highest intensity stimuli inducing marked physiological stress. The AI-based analysis, utilizing AlexNet for EEG signal classification, achieved 90% accuracy in distinguishing between resting, low-pain, and high-pain states. While no social sharing of pain was observed through ECG measurements, these results contribute to the growing body of evidence supporting insect nociception and offer new insights into their physiological responses to external stressors. This research advances the understanding of insect pain mechanisms and demonstrates the potential for AI-driven analysis in entomological studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16088v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Josep Montagut Marques, Pan Minghao, Ryuichi Okada, Midori Sakura, Kayo Hirose, Shinjiro Umezu</dc:creator>
    </item>
    <item>
      <title>Epilepsy and its driving forces: understanding the forces behind epileptical pathogenisis</title>
      <link>https://arxiv.org/abs/2502.16144</link>
      <description>arXiv:2502.16144v1 Announce Type: new 
Abstract: Epilepsy is a neurological disorder characterized by seizures and epileptic events intertwined with religious and personal beliefs since prehistoric times. This review paper explores the historical context and challenges in defining epilepsy. The formal definition was established twenty years ago, and the multifaceted causes of this neurological disorder. It aims to pave the way for personalised therapeutic strategies, research advancements, and informed public health planning to enhance the lives of those affected by this complex neurological condition. In addition, this review paper focuses on the mechanisms and etiologies of epileptogenesis, categorizing them by mechanisms and the underlying causes of the disorder. The review paper provides a brief overview of the current state of the art in the diagnosis, diagnosis, treatment, and treatment of epileptiform seizures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16144v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shreya Shah, Manan Shah, Bhavin Parekh</dc:creator>
    </item>
    <item>
      <title>Brain-Model Evaluations Need the NeuroAI Turing Test</title>
      <link>https://arxiv.org/abs/2502.16238</link>
      <description>arXiv:2502.16238v1 Announce Type: new 
Abstract: What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16238v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Category-Selective Neurons in Deep Networks: Comparing Purely Visual and Visual-Language Models</title>
      <link>https://arxiv.org/abs/2502.16456</link>
      <description>arXiv:2502.16456v1 Announce Type: new 
Abstract: Category-selective regions in the human brain, such as the fusiform face area (FFA), extrastriate body area (EBA), parahippocampal place area (PPA), and visual word form area (VWFA), play a crucial role in high-level visual processing. Here, we investigate whether artificial neural networks (ANNs) exhibit similar category-selective neurons and how these neurons vary across model layers and between purely visual and vision-language models. Inspired by fMRI functional localizer experiments, we presented images from different categories (faces, bodies, scenes, words, scrambled scenes, and scrambled words) to deep networks and identified category-selective neurons using statistical criteria. Comparing ResNet and the structurally controlled ResNet-based CLIP model, we found that both models contain category-selective neurons, with their proportion increasing across layers, mirroring category selectivity in higher-level visual brain regions. However, CLIP exhibited a higher proportion but lower specificity of category-selective neurons compared to ResNet. Additionally, CLIP's category-selective neurons were more evenly distributed across feature maps and demonstrated greater representational consistency across layers. These findings suggest that language learning increases the number of category-selective neurons while reducing their selectivity strength, reshaping visual representations in deep networks. Our study provides insights into how ANNs mirror biological vision and how multimodal learning influences category-selective representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16456v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zitong Lu, Yuxin Wang</dc:creator>
    </item>
    <item>
      <title>Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling</title>
      <link>https://arxiv.org/abs/2502.16525</link>
      <description>arXiv:2502.16525v1 Announce Type: new 
Abstract: Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet their high computational complexity poses challenges for scalability and real-time deployment. Traditional pruning techniques, predominantly based on weight magnitudes, often overlook the intrinsic structural properties of these networks. We introduce a novel framework that models RNNs as partially ordered sets (posets) and constructs corresponding dependency lattices. By identifying meet irreducible neurons, our lattice-based pruning algorithm selectively retains critical connections while eliminating redundant ones. The method is implemented using both binary and continuous-valued adjacency matrices to capture different aspects of network connectivity. Evaluated on the MNIST dataset, our approach exhibits a clear trade-off between sparsity and classification accuracy. Moderate pruning maintains accuracy above 98%, while aggressive pruning achieves higher sparsity with only a modest performance decline. Unlike conventional magnitude-based pruning, our method leverages the structural organization of RNNs, resulting in more effective preservation of functional connectivity and improved efficiency in multilayer networks with top-down feedback. The proposed lattice-based pruning framework offers a rigorous and scalable approach for reducing RNN complexity while sustaining robust performance, paving the way for more efficient hierarchical models in both machine learning and computational neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16525v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rakesh Sengupta</dc:creator>
    </item>
    <item>
      <title>Predictability of temporal network dynamics in normal ageing and brain pathology</title>
      <link>https://arxiv.org/abs/2502.16557</link>
      <description>arXiv:2502.16557v1 Announce Type: new 
Abstract: Spontaneous brain activity generically displays transient spatiotemporal coherent structures, which can selectively be affected in various neurological and psychiatric pathologies. Here we model the full brain's electroencephalographic activity as a high-dimensional functional network performing a trajectory in a latent graph phase space. This approach allows us to investigate the orbital stability of brain's activity and in particular its short-term predictability. We do this by constructing a non-parametric statistic quantifying the expansion of initially close functional network trajectories. We apply the method to cohorts of healthy ageing individuals, and patients previously diagnosed with Parkinson's or Alzheimer's disease. Results not only characterise brain dynamics from a new angle, but further show that functional network predictability varies in a marked scale-dependent way across healthy controls and patient groups. The path towards both pathologies is markedly different. Furthermore, healthy ageing's predictability appears to strongly differ from that of Parkinson's disease, but much less from that of patients with Alzheimer's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16557v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Annalisa Caligiuri, David Papo, G\"orsev Yener, Bahar G\"untekin, Tobias Galla, Lucas Lacasa, Massimiliano Zanin</dc:creator>
    </item>
    <item>
      <title>Reasoning within and between collective action problems</title>
      <link>https://arxiv.org/abs/2502.16677</link>
      <description>arXiv:2502.16677v1 Announce Type: new 
Abstract: Understanding cooperation in social systems is challenging because the ever-changing rules that govern societies interact with individual actions, resulting in intricate collective outcomes. In virtual-world experiments, we allowed people to make changes in the systems that they are making decisions within and investigated how they weigh the influence of different rules in decision-making. When choosing between worlds differing in more than one rule, a naive heuristics model predicted participants decisions as well, and in some cases better, than game earnings (utility) or by the subjective quality of single rules. In contrast, when a subset of engaged participants made instantaneous (within-world) decisions, their behavior aligned very closely with objective utility and not with the heuristics model. Findings suggest that, whereas choices between rules may deviate from rational benchmarks, the frequency of real time cooperation decisions to provide feedback can be a reliable indicator of the objective utility of these rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16677v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ofer Tchernichovski, Seth Frey, Dalton C. Conley, Nori Jacoby</dc:creator>
    </item>
    <item>
      <title>How constraints on editing affects cultural evolution</title>
      <link>https://arxiv.org/abs/2502.16694</link>
      <description>arXiv:2502.16694v1 Announce Type: new 
Abstract: When is it beneficial to constrain creativity? Creativity thrives with freedom, but when people collaborate to create artifacts, there is tension between giving individuals freedom to revise, and protecting prior achievements. To test how imposing constraints may affect collective creativity, we performed cultural evolution experiments where participants collaborated to create melodies and images in chains. With melodies, we found that limiting step size (number of musical notes that can be changed) improved pleasantness ratings for created tunes. Similar results were observed in cohorts of musicians, and with different selection regimes. In contrast, limiting step size in creating images consistently reduced pleasantness. These conflicting findings suggest that in domains such as music, where artifacts can be easily damaged, and where evolutionary outcomes are hard to foresee, collective creativity may benefit from imposing small step sizes. We discuss parallels with search algorithms and the evolution of conservative birdsong cultures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16694v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ofer Tchernichovski, Eitan Globerson, Peter Harrison, Nori Jacoby</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Powered Electrical Brain Signals Analysis: Advancing Neurological Diagnostics</title>
      <link>https://arxiv.org/abs/2502.17213</link>
      <description>arXiv:2502.17213v1 Announce Type: new 
Abstract: Neurological disorders represent significant global health challenges, driving the advancement of brain signal analysis methods. Scalp electroencephalography (EEG) and intracranial electroencephalography (iEEG) are widely used to diagnose and monitor neurological conditions. However, dataset heterogeneity and task variations pose challenges in developing robust deep learning solutions. This review systematically examines recent advances in deep learning approaches for EEG/iEEG-based neurological diagnostics, focusing on applications across 7 neurological conditions using 46 datasets. We explore trends in data utilization, model design, and task-specific adaptations, highlighting the importance of pre-trained multi-task models for scalable, generalizable solutions. To advance research, we propose a standardized benchmark for evaluating models across diverse datasets to enhance reproducibility. This survey emphasizes how recent innovations can transform neurological diagnostics and enable the development of intelligent, adaptable healthcare solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17213v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahe Li, Xin Chen, Fanqi Shen, Junru Chen, Yuxin Liu, Daoze Zhang, Zhizhang Yuan, Fang Zhao, Meng Li, Yang Yang</dc:creator>
    </item>
    <item>
      <title>Automated generation of epilepsy surgery resection masks; The RAMPS pipeline</title>
      <link>https://arxiv.org/abs/2502.17287</link>
      <description>arXiv:2502.17287v1 Announce Type: new 
Abstract: MRI-based delineation of brain tissue removed by epilepsy surgery can be challenging due to post-operative brain shift. In consequence, most studies use manual approaches which are prohibitively time-consuming for large sample sizes, require expertise, and can be prone to errors.
  We propose RAMPS (Resections And Masks in Preoperative Space), an automated pipeline to generate a 3D resection mask of pre-operative tissue. Our pipeline leverages existing software including FreeSurfer, SynthStrip, Sythnseg and ANTS to generate a mask in the same space as the patient's pre-operative T1 weighted MRI. We compare our automated masks against manually drawn masks and two other existing pipelines (Epic-CHOP and ResectVol).
  Comparing to manual masks (N=87), RAMPS achieved a median(IQR) dice similarity of 0.86(0.078) in temporal lobe resections, and 0.72(0.32) in extratemporal resections. In comparison to other pipelines, RAMPS had higher dice similarities (N=62) (RAMPS:0.86, Epic-CHOP: 0.72, ResectVol: 0.72).
  We release a user-friendly, easy to use pipeline, RAMPS, open source for accurate delineation of resected tissue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17287v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Callum Simpson, Gerard Hall, John S. Duncan, Yujiang Wang, Peter N. Taylor</dc:creator>
    </item>
    <item>
      <title>Unraveling the geometry of visual relational reasoning</title>
      <link>https://arxiv.org/abs/2502.17382</link>
      <description>arXiv:2502.17382v1 Announce Type: new 
Abstract: Humans and other animals readily generalize abstract relations, such as recognizing constant in shape or color, whereas neural networks struggle. To investigate how neural networks generalize abstract relations, we introduce SimplifiedRPM, a novel benchmark for systematic evaluation. In parallel, we conduct human experiments to benchmark relational difficulty, enabling direct model-human comparisons. Testing four architectures--ResNet-50, Vision Transformer, Wild Relation Network, and Scattering Compositional Learner (SCL)--we find that SCL best aligns with human behavior and generalizes best. Building on a geometric theory of neural representations, we show representational geometries that predict generalization. Layer-wise analysis reveals distinct relational reasoning strategies across models and suggests a trade-off where unseen rule representations compress into training-shaped subspaces. Guided by our geometric perspective, we propose and evaluate SNRloss, a novel objective balancing representation geometry. Our findings offer geometric insights into how neural networks generalize abstract relations, paving the way for more human-like visual reasoning in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17382v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Shang, Gabriel Kreiman, Haim Sompolinsky</dc:creator>
    </item>
    <item>
      <title>Dynamical phases of short-term memory mechanisms in RNNs</title>
      <link>https://arxiv.org/abs/2502.17433</link>
      <description>arXiv:2502.17433v1 Announce Type: new 
Abstract: Short-term memory is essential for cognitive processing, yet our understanding of its neural mechanisms remains unclear. A key focus in neuroscience has been the study of sequential activity patterns, where neurons fire one after another within large networks, to explain how information is maintained. While recurrent connections were shown to drive sequential dynamics, a mechanistic understanding of this process still remains unknown. In this work, we first introduce two unique mechanisms that can subserve short-term memory: slow-point manifolds generating direct sequences or limit cycles providing temporally localized approximations. Then, through analytical models, we identify fundamental properties that govern the selection of these mechanisms, \textit{i.e.}, we derive theoretical scaling laws for critical learning rates as a function of the delay period length, beyond which no learning is possible. We empirically verify these observations by training and evaluating more than 35,000 recurrent neural networks (RNNs) that we will publicly release upon publication. Overall, our work provides new insights into short-term memory mechanisms and proposes experimentally testable predictions for systems neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17433v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bariscan Kurtkaya, Fatih Dinc, Mert Yuksekgonul, Marta Blanco-Pozo, Ege Cirakman, Mark Schnitzer, Yucel Yemez, Hidenori Tanaka, Peng Yuan, Nina Miolane</dc:creator>
    </item>
    <item>
      <title>Perceptogram: Reconstructing Visual Percepts from EEG</title>
      <link>https://arxiv.org/abs/2404.01250</link>
      <description>arXiv:2404.01250v2 Announce Type: replace 
Abstract: Visual neural decoding from EEG has improved significantly due to diffusion models that can reconstruct high-quality images from decoded latents. While recent works have focused on relatively complex architectures to achieve good reconstruction performance from EEG, less attention has been paid to the source of this information. In this work, we attempt to discover EEG features that represent perceptual and semantic visual categories, using a simple pipeline. Notably, the high temporal resolution of EEG allows us to go beyond static semantic maps as obtained from fMRI. We show (a) Training a simple linear decoder from EEG to CLIP latent space, followed by a frozen pre-trained diffusion model, is sufficient to decode images with state-of-the-art reconstruction performance. (b) Mapping the decoded latents back to EEG using a linear encoder isolates CLIP-relevant EEG spatiotemporal features. (c) By using other latent spaces representing lower-level image features, we obtain similar time-courses of texture/hue-related information. We thus use our framework, Perceptogram, to probe EEG signals at various levels of the visual information hierarchy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01250v2</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Teng Fei, Abhinav Uppal, Ian Jackson, Srinivas Ravishankar, David Wang, Virginia R. de Sa</dc:creator>
    </item>
    <item>
      <title>Towards a Quantitative Theory of Digraph-Based Complexes and its Applications in Brain Network Analysis</title>
      <link>https://arxiv.org/abs/2409.09862</link>
      <description>arXiv:2409.09862v2 Announce Type: replace 
Abstract: In this work, we developed new mathematical methods for analyzing network topology and applied these methods to the analysis of brain networks. More specifically, we rigorously developed quantitative methods based on complexes constructed from digraphs (digraph-based complexes), such as path complexes and directed clique complexes (alternatively, we refer to these complexes as "higher-order structures," or "higher-order topologies," or "simplicial structures"), and, in the case of directed clique complexes, also methods based on the interrelations between the directed cliques, what we called "directed higher-order connectivities." This new quantitative theory for digraph-based complexes can be seen as a step towards the formalization of a "quantitative simplicial theory." Subsequently, we used these new methods, such as characterization measures and similarity measures for digraph-based complexes, to analyze the topology of digraphs derived from brain connectivity estimators, specifically the estimator known as information partial directed coherence (iPDC), which is a multivariate estimator that can be considered a representation of Granger causality in the frequency-domain, particularly estimated from electroencephalography (EEG) data from patients diagnosed with left temporal lobe epilepsy, in the delta, theta and alpha frequency bands, to try to find new biomarkers based on the higher-order structures and connectivities of these digraphs. In particular, we attempted to answer the following questions: How does the higher-order topology of the brain network change from the pre-ictal to the ictal phase, from the ictal to the post-ictal phase, at each frequency band and in each cerebral hemisphere? Does the analysis of higher-order structures provide new and better biomarkers for seizure dynamics and also for the laterality of the seizure focus than the usual graph theoretical analyses?</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09862v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.11606/T.95.2024.tde-04072024-124243</arxiv:DOI>
      <dc:creator>Heitor Baldo</dc:creator>
    </item>
    <item>
      <title>Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder Auxiliary Diagnosis Based on Hyperbolic Space</title>
      <link>https://arxiv.org/abs/2502.05493</link>
      <description>arXiv:2502.05493v2 Announce Type: replace 
Abstract: In the medical field, most resting-state fMRI (rs-fMRI) data are collected from multiple hospital sites. Multi-site rs-fMRI data can increase the volume of training data, enabling auxiliary diagnostic algorithms for brain diseases to learn more accurate and stable models. However, due to the significant heterogeneity and domain shift in rs-fMRI data across different sites, the accuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has been limited exploration of multi-source domain adaptation algorithms, and the interpretability of models is often poor. To address these challenges, we proposed a domain-adaptive algorithm based on hyperbolic space embedding. Hyperbolic space is naturally suited for representing the topology of complex networks such as brain functional networks. Therefore, we embedded the brain functional network into hyperbolic space and constructed the corresponding hyperbolic space community network to effectively extract brain network representations. To address the heterogeneity of data across different sites and the issue of domain shift, we introduce a constraint loss function, HMMD (Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in the hyperbolic space. Additionally, we employ class prototype alignment to align the conditional distributions. This significantly improves the quality of brain representations and enhances diagnostic classification accuracy for Autism Spectrum Disorder (ASD). Experimental results demonstrated that the proposed algorithm is robust to multi-site heterogeneity and shows promising potential for brain network mechanism analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05493v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqian Luo, Qiurong Chen, Yangsong Zhang, Fali Li, Peng Xu, Zhengguo Chen</dc:creator>
    </item>
    <item>
      <title>BG-GAN: Generative AI Enable Representing Brain Structure-Function Connections for Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2309.08916</link>
      <description>arXiv:2309.08916v4 Announce Type: replace-cross 
Abstract: The relationship between brain structure and function is critical for revealing the pathogenesis of brain disorders, including Alzheimer's disease (AD). However, mapping brain structure to function connections is a very challenging task. In this work, a bidirectional graph generative adversarial network (BG-GAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BG-GAN can employ features of direct and indirect brain regions to learn the mapping function between the structural domain and the functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BG-GAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental results using the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset show that both generated structure and function connections can improve the identification accuracy of AD. The experimental findings suggest that the relationship between brain structure and function is not a complete one-to-one correspondence. They also suggest that brain structure is the basis of brain function, and the strong structural connections are majorly accompanied by strong functional connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08916v4</guid>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Zhou, Chen Ding, Changhong Jing, Feng Liu, Kevin Hung, Hieu Pham, Mufti Mahmud, Zhihan Lyu, Sibo Qiao, Shuqiang Wang, Kim-Fung Tsang</dc:creator>
    </item>
    <item>
      <title>Generative AI Enables EEG Super-Resolution via Spatio-Temporal Adaptive Diffusion Learning</title>
      <link>https://arxiv.org/abs/2407.03089</link>
      <description>arXiv:2407.03089v5 Announce Type: replace-cross 
Abstract: Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG) devices, is widely used in fields such as neuroscience. HD EEG devices improve the spatial resolution of EEG by placing more electrodes on the scalp, which meet the requirements of clinical diagnostic applications such as epilepsy focus localization. However, this technique faces challenges, such as high acquisition costs and limited usage scenarios. In this paper, spatio-temporal adaptive diffusion models (STAD) are proposed to pioneer the use of diffusion models for achieving spatial SR reconstruction from low-resolution (LR, 64 channels or fewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically, a spatio-temporal condition module is designed to extract the spatio-temporal features of LR EEG, which are then used as conditional inputs to direct the reverse denoising process. Additionally, a multi-scale Transformer denoising module is constructed to leverage multi-scale convolution blocks and cross-attention-based diffusion Transformer blocks for conditional guidance to generate subject-adaptive SR EEG. Experimental results demonstrate that the STAD significantly enhances the spatial resolution of LR EEG and quantitatively outperforms existing methods. Furthermore, STAD demonstrate their value by applying synthetic SR EEG to classification and source localization tasks, indicating their potential to substantially boost the spatial resolution of EEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03089v5</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuqiang Wang, Tong Zhou, Yanyan Shen, Ye Li, Guoheng Huang, Yong Hu</dc:creator>
    </item>
    <item>
      <title>CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding</title>
      <link>https://arxiv.org/abs/2412.07236</link>
      <description>arXiv:2412.07236v3 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) is a non-invasive technique to measure and record brain electrical activity, widely used in various BCI and healthcare applications. Early EEG decoding methods rely on supervised learning, limited by specific tasks and datasets, hindering model performance and generalizability. With the success of large language models, there is a growing body of studies focusing on EEG foundation models. However, these studies still leave challenges: Firstly, most of existing EEG foundation models employ full EEG modeling strategy. It models the spatial and temporal dependencies between all EEG patches together, but ignores that the spatial and temporal dependencies are heterogeneous due to the unique structural characteristics of EEG signals. Secondly, existing EEG foundation models have limited generalizability on a wide range of downstream BCI tasks due to varying formats of EEG data, making it challenging to adapt to. To address these challenges, we propose a novel foundation model called CBraMod. Specifically, we devise a criss-cross transformer as the backbone to thoroughly leverage the structural characteristics of EEG signals, which can model spatial and temporal dependencies separately through two parallel attention mechanisms. And we utilize an asymmetric conditional positional encoding scheme which can encode positional information of EEG patches and be easily adapted to the EEG with diverse formats. CBraMod is pre-trained on a very large corpus of EEG through patch-based masked EEG reconstruction. We evaluate CBraMod on up to 10 downstream BCI tasks (12 public datasets). CBraMod achieves the state-of-the-art performance across the wide range of tasks, proving its strong capability and generalizability. The source code is publicly available at https://github.com/wjq-learning/CBraMod.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07236v3</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan</dc:creator>
    </item>
  </channel>
</rss>
