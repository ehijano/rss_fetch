<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Theta oscillons in behaving rats</title>
      <link>https://arxiv.org/abs/2404.13851</link>
      <description>arXiv:2404.13851v1 Announce Type: new 
Abstract: Recently discovered constituents of the brain waves -- the oscillons -- provide high-resolution representation of the extracellular field dynamics. Here we study the most robust, highest-amplitude oscillons that manifest in actively behaving rats and generally correspond to the traditional theta-waves. We show that the resemblances between theta-oscillons and the conventional theta-waves apply to the ballpark characteristics -- mean frequencies, amplitudes, and bandwidths. In addition, both hippocampal and cortical oscillons exhibit a number of intricate, behavior-attuned, transient properties that suggest a new vantage point for understanding the theta-rhythms' structure, origins and functions. We demonstrate that oscillons are frequency-modulated waves, with speed-controlled parameters, embedded into a noise background. We also use a basic model of neuronal synchronization to contextualize and to interpret the observed phenomena. In particular, we argue that the synchronicity level in physiological networks is fairly weak and modulated by the animal's locomotion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13851v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>M. S. Zobaer, N. Lotfi, C. M. Domenico, C. Hoffman, L. Perotti, D. Ji, Y. Dabaghian</dc:creator>
    </item>
    <item>
      <title>Quantitative Analysis of Roles of Direct and Indirect Pathways for Action Selection in The Basal Ganglia</title>
      <link>https://arxiv.org/abs/2404.13888</link>
      <description>arXiv:2404.13888v1 Announce Type: new 
Abstract: The basal ganglia (BG) show diverse functions for motor and cognition. Here, we are concerned about action selection performed by the BG. Particularly, we make quantitative analysis of roles of direct pathway (DP) and indirect pathway (IP) for action selection in a spiking neural network with 3 competing channels. For such quantitative work, in each channel, we get the competition degree ${\cal C}_d$, given by the ratio of strength of DP (${\cal S}_{DP}$) to strength of IP (${\cal S}_{IP}$) (i.e., ${\cal C}_d = {\cal S}_{DP} / {\cal S}_{IP}$). Then, desired action is selected in the channel with the largest ${\cal C}_d$. Desired action selection is made mainly due to strong focused inhibitory projection to the output nucleus, SNr (substantia nigra pars reticulata) via the "Go" DP in the corresponding channel. Unlike the case of DP, there are two types of IPs; intra-channel IP and inter-channel IP, due to widespread diffusive excitation from the STN (subthalamic nucleus). The intra-channel "No-Go" IP plays a role of brake to suppress the desired action selection. On the other hand, the inter-channel IP to the SNr in the neighboring channels suppresses competing actions, leading to spotlight the desired action selection. In this way, role of the inter-channel IP is opposite to that of the intra-channel IP. But, to the best of our knowledge, no quantitative analysis for such roles of the DP and the two IPs was made. Here, by direct calculations of the DP and the intra- and the inter-channel IP presynaptic currents into the SNr in each channel, we get the competition degree of each channel to determine a desired action, and then roles of the DP and the intra- and inter-channel IPs are quantitatively made clear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13888v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sang-Yoon Kim, Woochang Lim</dc:creator>
    </item>
    <item>
      <title>Cooperativity, information gain, and energy cost during early LTP in dendritic spines</title>
      <link>https://arxiv.org/abs/2404.14123</link>
      <description>arXiv:2404.14123v1 Announce Type: new 
Abstract: We investigate a mutual relationship between information and energy during early phase of LTP induction and maintenance in a large-scale system of mutually coupled dendritic spines, with discrete internal states and probabilistic dynamics, within the framework of nonequilibrium stochastic thermodynamics. In order to analyze this computationally intractable stochastic multidimensional system, we introduce a pair approximation, which allows us to reduce the spine dynamics into a lower dimensional manageable system of closed equations. It is found that the rates of information gain and energy attain their maximal values during an initial period of LTP (i.e. during stimulation), and after that they recover to their baseline low values, as opposed to a memory trace that lasts much longer. This suggests that learning phase is much more energy demanding than the memory phase. We show that positive correlations between neighboring spines increase both a duration of memory trace and energy cost during LTP, but the memory time per invested energy increases dramatically for very strong positive synaptic cooperativity, suggesting a beneficial role of synaptic clustering on memory duration. In contrast, information gain after LTP is the largest for negative correlations, and energy efficiency of that information generally declines with increasing synaptic cooperativity. We also find that dendritic spines can use sparse representations for encoding of long-term information, as both energetic and structural efficiencies of retained information and its lifetime exhibit maxima for low fractions of stimulated synapses during LTP. In general, our stochastic thermodynamics approach provides a unifying framework for studying, from first principles, information encoding and its energy cost during learning and memory in stochastic systems of interacting synapses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14123v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1162/neco_a_01632</arxiv:DOI>
      <arxiv:journal_reference>Neural Computation 36: 271-311 (2024)</arxiv:journal_reference>
      <dc:creator>Jan Karbowski, Paulina Urban</dc:creator>
    </item>
    <item>
      <title>Fermi-Bose Machine</title>
      <link>https://arxiv.org/abs/2404.13631</link>
      <description>arXiv:2404.13631v1 Announce Type: cross 
Abstract: Distinct from human cognitive processing, deep neural networks trained by backpropagation can be easily fooled by adversarial examples. To design a semantically meaningful representation learning, we discard backpropagation, and instead, propose a local contrastive learning, where the representation for the inputs bearing the same label shrink (akin to boson) in hidden layers, while those of different labels repel (akin to fermion). This layer-wise learning is local in nature, being biological plausible. A statistical mechanics analysis shows that the target fermion-pair-distance is a key parameter. Moreover, the application of this local contrastive learning to MNIST benchmark dataset demonstrates that the adversarial vulnerability of standard perceptron can be greatly mitigated by tuning the target distance, i.e., controlling the geometric separation of prototype manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13631v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingshan Xie, Yuchen Wang, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Exploring neural oscillations during speech perception via surrogate gradient spiking neural networks</title>
      <link>https://arxiv.org/abs/2404.14024</link>
      <description>arXiv:2404.14024v1 Announce Type: cross 
Abstract: Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales. We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network. Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs. Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance. Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14024v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexandre Bittar, Philip N. Garner</dc:creator>
    </item>
    <item>
      <title>Adapting to time: why nature evolved a diverse set of neurons</title>
      <link>https://arxiv.org/abs/2404.14325</link>
      <description>arXiv:2404.14325v1 Announce Type: cross 
Abstract: Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information. In addition, it is known empirically that spike timing is a significant factor in neural computations. However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases. In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns. In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure. Indeed, when adaptation was restricted to weights, networks were unable to solve most problems. We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14325v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karim G. Habashy, Benjamin D. Evans, Dan F. M. Goodman, Jeffrey S. Bowers</dc:creator>
    </item>
    <item>
      <title>Neuro-evolutionary evidence for a universal fractal primate brain shape</title>
      <link>https://arxiv.org/abs/2209.08066</link>
      <description>arXiv:2209.08066v5 Announce Type: replace 
Abstract: The cerebral cortex displays a bewildering diversity of shapes and sizes across and within species. Despite this diversity, we present a universal multi-scale description of primate cortices. We show that all cortical shapes can be described as a set of nested folds of different sizes. As neighbouring folds are gradually merged, the cortices of 11 primate species follow a common scale-free morphometric trajectory, that also overlaps with over 70 other mammalian species. Our results indicate that all cerebral cortices are approximations of the same archetypal fractal shape with a fractal dimension of d_f=2.5. Importantly, this new understanding enables a more precise quantification of brain morphology as a function of scale. To demonstrate the importance of this new understanding, we show a scale-dependent effect of ageing on brain morphology. We observe a more than four-fold increase in effect size (from 2 standard deviations to 8 standard deviations) at a spatial scale of approximately 2~mm compared to standard morphological analyses. Our new understanding may therefore generate superior biomarkers for a range of conditions in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08066v5</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujiang Wang, Karoline Leiberg, Nathan Kindred, Christopher R. Madan, Colline Poirier, Christopher I. Petkov, Peter N. Taylor, Bruno C. C. Mota</dc:creator>
    </item>
    <item>
      <title>Upper bounds for integrated information</title>
      <link>https://arxiv.org/abs/2305.09826</link>
      <description>arXiv:2305.09826v2 Announce Type: replace 
Abstract: Originally developed as a theory of consciousness, integrated information theory provides a mathematical framework to quantify the causal irreducibility of systems and subsets of units in the system. Specifically, mechanism integrated information quantifies how much of the causal powers of a subset of units in a state, also referred to as a mechanism, cannot be accounted for by its parts. If the causal powers of the mechanism can be fully explained by its parts, it is reducible and its integrated information is zero. Here, we study the upper bound of this measure and how it is achieved. We study mechanisms in isolation, groups of mechanisms, and groups of causal relations among mechanisms. We put forward new theoretical results that show mechanisms that share parts with each other cannot all achieve their maximum. We also introduce techniques to design systems that can maximize the integrated information of a subset of their mechanisms or relations. Our results can potentially be used to exploit the symmetries and constraints to reduce the computations significantly and to compare different connectivity profiles in terms of their maximal achievable integrated information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09826v2</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Zaeemzadeh, Giulio Tononi</dc:creator>
    </item>
    <item>
      <title>Toward stochastic neural computing</title>
      <link>https://arxiv.org/abs/2305.13982</link>
      <description>arXiv:2305.13982v2 Announce Type: replace-cross 
Abstract: The highly irregular spiking activity of cortical neurons and behavioral variability suggest that the brain could operate in a fundamentally probabilistic way. Mimicking how the brain implements and learns probabilistic computation could be a key to developing machine intelligence that can think more like humans. In this work, we propose a theory of stochastic neural computing (SNC) in which streams of noisy inputs are transformed and processed through populations of nonlinearly coupled spiking neurons. To account for the propagation of correlated neural variability, we derive from first principles a moment embedding for spiking neural network (SNN). This leads to a new class of deep learning model called the moment neural network (MNN) which naturally generalizes rate-based neural networks to second order. As the MNN faithfully captures the stationary statistics of spiking neural activity, it can serve as a powerful proxy for training SNN with zero free parameters. Through joint manipulation of mean firing rate and noise correlations in a task-driven way, the model is able to learn inference tasks while simultaneously minimizing prediction uncertainty, resulting in enhanced inference speed. We further demonstrate the application of our method to Intel's Loihi neuromorphic hardware. The proposed theory of SNC may open up new opportunities for developing machine intelligence capable of computing uncertainty and for designing unconventional computing architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13982v2</guid>
      <category>cs.NE</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Qi, Zhichao Zhu, Yiming Wei, Lu Cao, Zhigang Wang, Jie Zhang, Wenlian Lu, Jianfeng Feng</dc:creator>
    </item>
    <item>
      <title>A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology</title>
      <link>https://arxiv.org/abs/2403.07945</link>
      <description>arXiv:2403.07945v2 Announce Type: replace-cross 
Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue, but applied efforts have been relatively limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of the algorithmic problems faced by attackers attempting to violate privacy and autonomy, and defenders attempting to obstruct such attempts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07945v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bryce Allen Bagley</dc:creator>
    </item>
  </channel>
</rss>
