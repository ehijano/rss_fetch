<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Aug 2024 01:35:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM Approach</title>
      <link>https://arxiv.org/abs/2408.06634</link>
      <description>arXiv:2408.06634v1 Announce Type: new 
Abstract: Accurate stock market predictions following earnings reports are crucial for investors. Traditional methods, particularly classical machine learning models, struggle with these predictions because they cannot effectively process and interpret extensive textual data contained in earnings reports and often overlook nuances that influence market movements. This paper introduces an advanced approach by employing Large Language Models (LLMs) instruction fine-tuned with a novel combination of instruction-based techniques and quantized low-rank adaptation (QLoRA) compression. Our methodology integrates 'base factors', such as financial metric growth and earnings transcripts, with 'external factors', including recent market indices performances and analyst grades, to create a rich, supervised dataset. This comprehensive dataset enables our models to achieve superior predictive performance in terms of accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially evident in the comparison with benchmarks such as GPT-4. We specifically highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases significant improvements over baseline models. The paper also discusses the potential of expanding the output capabilities to include a 'Hold' option and extending the prediction horizon, aiming to accommodate various investment styles and time frames. This study not only demonstrates the power of integrating cutting-edge AI with fine-tuned financial data but also paves the way for future research in enhancing AI-driven financial analysis tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06634v1</guid>
      <category>q-fin.CP</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haowei Ni, Shuchen Meng, Xupeng Chen, Ziqing Zhao, Andi Chen, Panfeng Li, Shiyao Zhang, Qifu Yin, Yuanqing Wang, Yuxi Chan</dc:creator>
    </item>
    <item>
      <title>Adaptive Multilevel Stochastic Approximation of the Value-at-Risk</title>
      <link>https://arxiv.org/abs/2408.06531</link>
      <description>arXiv:2408.06531v1 Announce Type: cross 
Abstract: Cr\'epey, Frikha, and Louzi (2023) introduced a multilevel stochastic approximation scheme to compute the value-at-risk of a financial loss that is only simulatable by Monte Carlo. The optimal complexity of the scheme is in $O({\varepsilon}^{-5/2})$, ${\varepsilon} &gt; 0$ being a prescribed accuracy, which is suboptimal when compared to the canonical multilevel Monte Carlo performance. This suboptimality stems from the discontinuity of the Heaviside function involved in the biased stochastic gradient that is recursively evaluated to derive the value-at-risk. To mitigate this issue, this paper proposes and analyzes a multilevel stochastic approximation algorithm that adaptively selects the number of inner samples at each level, and proves that its optimal complexity is in $O({\varepsilon}^{-2}|\ln {\varepsilon}|^{5/2})$. Our theoretical analysis is exemplified through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06531v1</guid>
      <category>q-fin.RM</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>St\'ephane Cr\'epey, Noufel Frikha, Azar Louzi, Jonathan Spence</dc:creator>
    </item>
    <item>
      <title>Neural networks can detect model-free static arbitrage strategies</title>
      <link>https://arxiv.org/abs/2306.16422</link>
      <description>arXiv:2306.16422v2 Announce Type: replace 
Abstract: In this paper we demonstrate both theoretically as well as numerically that neural networks can detect model-free static arbitrage opportunities whenever the market admits some. Due to the use of neural networks, our method can be applied to financial markets with a high number of traded securities and ensures almost immediate execution of the corresponding trading strategies. To demonstrate its tractability, effectiveness, and robustness we provide examples using real financial data. From a technical point of view, we prove that a single neural network can approximately solve a class of convex semi-infinite programs, which is the key result in order to derive our theoretical results that neural networks can detect model-free static arbitrage strategies whenever the financial market admits such opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16422v2</guid>
      <category>q-fin.CP</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Julian Sester</dc:creator>
    </item>
  </channel>
</rss>
