<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Forecasting in small open emerging economies Evidence from Thailand</title>
      <link>https://arxiv.org/abs/2509.14805</link>
      <description>arXiv:2509.14805v1 Announce Type: new 
Abstract: Forecasting inflation in small open economies is difficult because limited time series and strong external exposures create an imbalance between few observations and many potential predictors. We study this challenge using Thailand as a representative case, combining more than 450 domestic and international indicators. We evaluate modern Bayesian shrinkage and factor models, including Horseshoe regressions, factor-augmented autoregressions, factor-augmented VARs, dynamic factor models, and Bayesian additive regression trees.
  Our results show that factor models dominate at short horizons, when global shocks and exchange rate movements drive inflation, while shrinkage-based regressions perform best at longer horizons. These models not only improve point and density forecasts but also enhance tail-risk performance at the one-year horizon.
  Shrinkage diagnostics, on the other hand, additionally reveal that Google Trends variables, especially those related to food essential goods and housing costs, progressively rotate into predictive importance as the horizon lengthens. This underscores their role as forward-looking indicators of household inflation expectations in small open economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14805v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paponpat Taveeapiradeecharoen, Nattapol Aunsri</dc:creator>
    </item>
    <item>
      <title>Comprehensive indicators and fine granularity refine density scaling laws in rural-urban systems</title>
      <link>https://arxiv.org/abs/2509.14258</link>
      <description>arXiv:2509.14258v1 Announce Type: cross 
Abstract: Density scaling laws complement traditional population scaling laws by enabling the analysis of the full range of human settlements and revealing rural-to-urban transitions with breakpoints at consistent population densities. However, previous studies have been constrained by the granularity of rural and urban units, as well as limitations in the quantity and diversity of indicators. This study addresses these gaps by examining Middle Layer Super Output Areas (MSOAs) in England and Wales, incorporating an extensive set of 117 indicators for the year 2021, spanning age, ethnicity, educational attainment, religion, disability, economic activity, mortality, crime, property transactions, and road accidents. Results indicate that the relationship between indicator density and population density is best described by a segmented power-law model with a consistent breakpoint (33 +- 5 persons per hectare) for 92 of the 117 indicators. Additionally, increasing granularity reveals further rural-to-urban transitions not observed at coarser spatial resolutions. Our findings also highlight the influence of population characteristics on scaling exponents, where stratifying dementia and ischaemic heart disease by older age groups (aged 70 and above) significantly affects these exponents, illustrating a protective urban effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14258v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jack Sutton, Quentin S. Hanley, Gerri Mortimore, Ovidiu Bagdasar, Haroldo V. Ribeiro, Thomas Peron, Golnaz Shahtahmassebi, Peter Scriven</dc:creator>
    </item>
    <item>
      <title>Randomization inference for stepped-wedge designs with noncompliance with application to a palliative care pragmatic trial</title>
      <link>https://arxiv.org/abs/2509.14598</link>
      <description>arXiv:2509.14598v1 Announce Type: cross 
Abstract: While palliative care is increasingly commonly delivered to hospitalized patients with serious illnesses, few studies have estimated its causal effects. Courtright et al. (2016) adopted a cluster-randomized stepped-wedge design to assess the effect of palliative care on a patient-centered outcome. The randomized intervention was a nudge to administer palliative care but did not guarantee receipt of palliative care, resulting in noncompliance (compliance rate ~30%). A subsequent analysis using methods suited for standard trial designs produced statistically anomalous results, as an intention-to-treat analysis found no effect while an instrumental variable analysis did (Courtright et al., 2024). This highlights the need for a more principled approach to address noncompliance in stepped-wedge designs. We provide a formal causal inference framework for the stepped-wedge design with noncompliance by introducing a relevant causal estimand and corresponding estimators and inferential procedures. Through simulation, we compare an array of estimators across a range of stepped-wedge designs and provide practical guidance in choosing an analysis method. Finally, we apply our recommended methods to reanalyze the trial of Courtright et al. (2016), producing point estimates suggesting a larger effect than the original analysis of (Courtright et al., 2024), but intervals that did not reach statistical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14598v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Zhang, Zhe Chen, Katherine R. Courtright, Scott D. Halpern, Michael O. Harhay, Dylan S. Small, Fan Li</dc:creator>
    </item>
    <item>
      <title>Explaining deep learning for ECG using time-localized clusters</title>
      <link>https://arxiv.org/abs/2509.15198</link>
      <description>arXiv:2509.15198v1 Announce Type: cross 
Abstract: Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15198v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahc\`ene Boubekki, Konstantinos Patlatzoglou, Joseph Barker, Fu Siong Ng, Ant\^onio H. Ribeiro</dc:creator>
    </item>
    <item>
      <title>Media Bias and Polarization through the Lens of a Markov Switching Latent Space Network Model</title>
      <link>https://arxiv.org/abs/2306.07939</link>
      <description>arXiv:2306.07939v3 Announce Type: replace 
Abstract: News outlets are now more than ever incentivized to provide their audience with slanted news, while the intrinsic homophilic nature of online social media may exacerbate polarized opinions. Here, we propose a new dynamic latent space model for time-varying online audience-duplication networks, which exploits social media content to conduct inference on media bias and polarization of news outlets. We contribute to the literature in several directions: 1) Our model provides a novel measure of media bias that combines information from both network data and text-based indicators; 2) we endow our model with Markov-Switching dynamics to capture polarization regimes while maintaining a parsimonious specification; 3) we contribute to the literature on the statistical properties of latent space network models. The proposed model is applied to a set of data on the online activity of national and local news outlets from four European countries in the years 2015 and 2016. We find evidence of a strong positive correlation between our media slant measure and a well-grounded external source of media bias. In addition, we provide insight into the polarization regimes across the four countries considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07939v3</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Casarin, Antonio Peruzzi, Mark F. J. Steel</dc:creator>
    </item>
    <item>
      <title>Application of Multivariate Selective Bandwidth Kernel Density Estimation for Data Correction</title>
      <link>https://arxiv.org/abs/2306.16043</link>
      <description>arXiv:2306.16043v2 Announce Type: replace 
Abstract: This paper presents an intuitive application of multivariate kernel density estimation (KDE) for data correction. The method utilizes the expected value of the conditional probability density function (PDF) and a credible interval to quantify correction uncertainty. A selective KDE factor is proposed to adjust both kernel size and shape, determined through least-squares cross-validation (LSCV) or mean conditional squared error (MCSE) criteria. The selective bandwidth method can be used in combination with the adaptive method to potentially improve accuracy. Two examples, involving a hypothetical dataset and a realistic dataset, demonstrate the efficacy of the method. The selective bandwidth methods consistently outperform non-selective methods, while the adaptive bandwidth methods improve results for the hypothetical dataset but not for the realistic dataset. The MCSE criterion minimizes root mean square error but may yield under-smoothed distributions, whereas the LSCV criterion strikes a balance between PDF fitness and low RMSE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16043v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai Bui, Mostafa Bakhoday-Paskyabi</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causality in Time Series Networks: With Application to Motor Imagery vs Execution</title>
      <link>https://arxiv.org/abs/2409.10374</link>
      <description>arXiv:2409.10374v3 Announce Type: replace 
Abstract: Causal interactions in time series networks can be dynamic and nonlinear, making it difficult to identify them using conventional linear causality estimations. We propose a novel approach, called Threshold Autoregressive Modeling for Causality (TAR4C), a causality detection approach built on threshold autoregressive (TAR) models, where a potential driver (cause variable) acts both as a predictor and as a trigger (switching threshold) that governs which autoregressive process the target (effect variable) follows. Threshold nonlinearity is conceptualized here to determine causality. The flow of the target is forced to transition between regimes with distinct dynamics when the driver exceeds a data-driven threshold in the past. We propose a two-stage inference procedure: Stage 1 tests for threshold connectivity (TC); Stage 2, conditional on a detected threshold effect, estimates threshold Granger causality (TGC). TAR4C is applied to a multichannel EEG dataset collected from a motor imagery and execution experiment. Delay-dependent directional interactions are observed among channels across different sites of the EEG map. The real-world application demonstrates the usefulness of the proposed approach for determining nonlinear causal connectivity in complex time-series networks, such as brain circuitry. The proposed model-based methodology extends to other complex networks of time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10374v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>A Simple and Explainable Model for Park-and-Ride Car Park Occupancy Prediction</title>
      <link>https://arxiv.org/abs/2503.12044</link>
      <description>arXiv:2503.12044v2 Announce Type: replace 
Abstract: In a scenario of growing usage of park-and-ride facilities, understanding and predicting car park occupancy is becoming increasingly important. This study presents a model that effectively captures the occupancy patterns of park-and-ride car parks for commuters using truncated normal distributions for vehicle arrival and departure times. The objective is to develop a predictive model with minimal parameters corresponding to commuter behaviour, enabling the estimation of parking saturation and unfulfilled demand. The proposed model successfully identifies the regular, periodic nature of commuter parking behaviour, where vehicles arrive in the morning and depart in the afternoon. It operates using aggregate data, eliminating the need for individual tracking of arrivals and departures. The model's predictive and now-casting capabilities are demonstrated through real-world data from car parks in the Barcelona Metropolitan Area. A simple model extension furthermore enables the prediction of when a car park will reach its occupancy limit and estimates the additional spaces required to accommodate such excess demand. Thus, beyond forecasting, the model serves as a valuable tool for evaluating interventions, such as expanding parking capacity, to optimize park-and-ride facilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12044v2</guid>
      <category>stat.AP</category>
      <category>cs.ET</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1140/epjds/s13688-025-00588-0</arxiv:DOI>
      <dc:creator>Andreas Kaltenbrunner, Josep Ferrer, David Moreno, Vicen\c{c} G\'omez</dc:creator>
    </item>
    <item>
      <title>New Insights into Global Warming: End-to-End Visual Analysis and Prediction of Temperature Variations</title>
      <link>https://arxiv.org/abs/2409.16311</link>
      <description>arXiv:2409.16311v2 Announce Type: replace-cross 
Abstract: Global warming presents an unprecedented challenge to our planet however comprehensive understanding remains hindered by geographical biases temporal limitations and lack of standardization in existing research. An end to end visual analysis of global warming using three distinct temperature datasets is presented. A baseline adjusted from the Paris Agreements one point five degrees Celsius benchmark based on data analysis is employed. A closed loop design from visualization to prediction and clustering is created using classic models tailored to the characteristics of the data. This approach reduces complexity and eliminates the need for advanced feature engineering. A lightweight convolutional neural network and long short term memory model specifically designed for global temperature change is proposed achieving exceptional accuracy in long term forecasting with a mean squared error of three times ten to the power of negative six and an R squared value of zero point nine nine nine nine. Dynamic time warping and KMeans clustering elucidate national level temperature anomalies and carbon emission patterns. This comprehensive method reveals intricate spatiotemporal characteristics of global temperature variations and provides warming trend attribution. The findings offer new insights into climate change dynamics demonstrating that simplicity and precision can coexist in environmental analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16311v2</guid>
      <category>physics.ao-ph</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meihua Zhou, Nan Wan, Tianlong Zheng, Hanwen Xu, Li Yang, Tingting Wang</dc:creator>
    </item>
    <item>
      <title>Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data</title>
      <link>https://arxiv.org/abs/2411.05869</link>
      <description>arXiv:2411.05869v3 Announce Type: replace-cross 
Abstract: The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05869v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Marcus M. Noack, Hengrui Luo, Ronald Pandolfi</dc:creator>
    </item>
    <item>
      <title>Generalized Correlation Regression for Disentangling Dependence in Clustered Data</title>
      <link>https://arxiv.org/abs/2509.01774</link>
      <description>arXiv:2509.01774v2 Announce Type: replace-cross 
Abstract: Clustered and longitudinal data are pervasive in scientific studies, from prenatal health programs to clinical trials and public health surveillance. Such data often involve non-Gaussian responses--including binary, categorical, and count outcomes--that exhibit complex correlation structures driven by multilevel clustering, covariates, over-dispersion, or zero inflation. Conventional approaches such as mixed-effects models and generalized estimating equations (GEEs) can capture some of these dependencies, but they are often too rigid or impose restrictive assumptions that limit interpretability and predictive performance.
  We investigate \emph{generalized correlation regression} (GCR), a unified framework that models correlations directly as functions of interpretable covariates while simultaneously estimating marginal means. By applying a generalized $z$-transformation, GCR guarantees valid correlation matrices, accommodates unbalanced cluster sizes, and flexibly incorporates covariates such as time, space, or group membership into the dependence structure. Through applications to modern prenatal care, a longitudinal toenail infection trial, and clustered health count data, we show that GCR not only achieves superior predictive performance over standard methods, but also reveals family-, community-, and individual-level drivers of dependence that are obscured under conventional modeling. These results demonstrate the broad applied value of GCR for analyzing binary, count, and categorical data in clustered and longitudinal settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01774v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Wang, Chenlei Leng, Cheng Yong Tang</dc:creator>
    </item>
  </channel>
</rss>
