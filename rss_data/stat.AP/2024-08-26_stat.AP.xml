<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Aug 2024 02:28:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Pinpointing Important Genetic Variants via A Feature-level Filter and Group Knockoffs</title>
      <link>https://arxiv.org/abs/2408.12618</link>
      <description>arXiv:2408.12618v1 Announce Type: new 
Abstract: Identifying variants that carry substantial information on the trait of interest remains a core topic in genetic studies. In analyzing the EADB-UKBB dataset to identify genetic variants associated with Alzheimer's disease (AD), however, we recognize that both existing marginal association tests and conditional independence tests using knockoffs suffer either power loss or lack of informativeness, especially when strong correlations exist among variants. To address the limitations of existing knockoff filters, we propose a new feature-versus-group (FVG) filter that is more powerful and precise in identifying important features from a set of strongly correlated features using group knockoffs. In extensive simulation studies, the FVG filter controls the expected proportion of false discoveries and identifies important features with enhanced power and greater precision. Applying the proposed method to the EADB-UKBB dataset, we discover important variants from 84 loci (same as the most powerful group knockoff filter) with catching sets of substantially smaller size and higher purity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12618v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Gu, Zhaomeng Chen, Zihuai He</dc:creator>
    </item>
    <item>
      <title>Does Spatial Information Improve Influenza Forecasting?</title>
      <link>https://arxiv.org/abs/2408.12722</link>
      <description>arXiv:2408.12722v1 Announce Type: new 
Abstract: Seasonal influenza forecasting is critical for public health and individual decision making. We investigate whether the inclusion of data about influenza activity in neighboring states can improve point predictions and distribution forecasting of influenza-like illness (ILI) in each US state using statistical regression models. Using CDC FluView ILI data from 2010-2019, we forecast weekly ILI in each US state with quantile, linear, and Poisson autoregressive models fit using different combinations of ILI data from the target state, neighboring states, and US weighted average. Scoring with root mean squared error and weighted interval score indicated that the variants including neighbors and/or the US average showed slightly higher accuracy than models fit only using lagged ILI in the target state, on average. Additionally, the improvement in performance when including neighbors was similar to the improvement when including the US average instead, suggesting the proximity of the neighboring states is not the driver of the slight increase in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12722v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabrielle Thivierge, Aaron Rumack, F. William Townes</dc:creator>
    </item>
    <item>
      <title>Broad versus narrow research questions in evidence synthesis: a parallel to (and plea for) estimands</title>
      <link>https://arxiv.org/abs/2408.12932</link>
      <description>arXiv:2408.12932v1 Announce Type: new 
Abstract: There has been a transition from broad to more specific research questions in the practice of network meta-analysis (NMA). Such convergence is also taking place in the context of individual registrational trials, following the recent introduction of the estimand framework, which is impacting the design, data collection strategy, analysis and interpretation of clinical trials. The language of estimands has much to offer to NMA, particularly given the "narrow" perspective of treatments and target populations taken in health technology assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12932v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/jrsm.1741</arxiv:DOI>
      <dc:creator>Antonio Remiro-Az\'ocar, Anders Gorst-Rasmussen</dc:creator>
    </item>
    <item>
      <title>When is truncated stop loss optimal?</title>
      <link>https://arxiv.org/abs/2408.12933</link>
      <description>arXiv:2408.12933v1 Announce Type: new 
Abstract: The paper examines how reinsurance can be used to strike a balance between expected profit and VaR/CVaR risk. Conditions making truncated stop loss contracts optimal are derived, and it is argued that those are usually satisfied in practice. One of the prerequisites is that reinsurance is not too cheap, and an argument resembling arbitrage suggests that it is not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12933v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik B{\o}lviken, Yinzhi Wang</dc:creator>
    </item>
    <item>
      <title>Estimation of ratios of normalizing constants using stochastic approximation : the SARIS algorithm</title>
      <link>https://arxiv.org/abs/2408.13022</link>
      <description>arXiv:2408.13022v1 Announce Type: new 
Abstract: Computing ratios of normalizing constants plays an important role in statistical modeling. Two important examples are hypothesis testing in latent variables models, and model comparison in Bayesian statistics. In both examples, the likelihood ratio and the Bayes factor are defined as the ratio of the normalizing constants of posterior distributions. We propose in this article a novel methodology that estimates this ratio using stochastic approximation principle. Our estimator is consistent and asymptotically Gaussian. Its asymptotic variance is smaller than the one of the popular optimal bridge sampling estimator. Furthermore, it is much more robust to little overlap between the two unnormalized distributions considered. Thanks to its online definition, our procedure can be integrated in an estimation process in latent variables model, and therefore reduce the computational effort. The performances of the estimator are illustrated through a simulation study and compared to two other estimators : the ratio importance sampling and the optimal bridge sampling estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13022v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Gu\'edon, Charlotte Baey, Estelle Kuhn</dc:creator>
    </item>
    <item>
      <title>Bayesian Network Modeling of Causal Influence within Cognitive Domains and Clinical Dementia Severity Ratings for Western and Indian Cohorts</title>
      <link>https://arxiv.org/abs/2408.12669</link>
      <description>arXiv:2408.12669v1 Announce Type: cross 
Abstract: This study investigates the causal relationships between Clinical Dementia Ratings (CDR) and its six domain scores across two distinct aging datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Longitudinal Aging Study of India (LASI). Using Directed Acyclic Graphs (DAGs) derived from Bayesian network models, we analyze the dependencies among domain scores and their influence on the global CDR. Our approach leverages the PC algorithm to estimate the DAG structures for both datasets, revealing notable differences in causal relationships and edge strengths between the Western and Indian populations. The analysis highlights a stronger dependency of CDR scores on memory functions in both datasets, but with significant variations in edge strengths and node degrees. By contrasting these findings, we aim to elucidate population-specific differences and similarities in dementia progression, providing insights that could inform targeted interventions and improve understanding of dementia across diverse demographic contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12669v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wupadrasta Santosh Kumar, Sayali Rajendra Bhutare, Neelam Sinha, Thomas Gregor Issac</dc:creator>
    </item>
    <item>
      <title>Causal Hierarchy in the Financial Market Network -- Uncovered by the Helmholtz-Hodge-Kodaira Decomposition</title>
      <link>https://arxiv.org/abs/2408.12839</link>
      <description>arXiv:2408.12839v1 Announce Type: cross 
Abstract: Granger causality can uncover the cause and effect relationships in financial networks. However, such networks can be convoluted and difficult to interpret, but the Helmholtz-Hodge-Kodaira decomposition can split them into a rotational and gradient component which reveals the hierarchy of Granger causality flow. Using Kenneth French's business sector return time series, it is revealed that during the Covid crisis, precious metals and pharmaceutical products are causal drivers of the financial network. Moreover, the estimated Granger causality network shows a high connectivity during crisis which means that the research presented here can be especially useful to better understand crises in the market by revealing the dominant drivers of the crisis dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12839v1</guid>
      <category>q-fin.ST</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Wand, Oliver Kamps, Hiroshi Iyetomi</dc:creator>
    </item>
    <item>
      <title>Machine Learning and the Yield Curve: Tree-Based Macroeconomic Regime Switching</title>
      <link>https://arxiv.org/abs/2408.12863</link>
      <description>arXiv:2408.12863v1 Announce Type: cross 
Abstract: We explore tree-based macroeconomic regime-switching in the context of the dynamic Nelson-Siegel (DNS) yield-curve model. In particular, we customize the tree-growing algorithm to partition macroeconomic variables based on the DNS model's marginal likelihood, thereby identifying regime-shifting patterns in the yield curve. Compared to traditional Markov-switching models, our model offers clear economic interpretation via macroeconomic linkages and ensures computational simplicity. In an empirical application to U.S. Treasury bond yields, we find (1) important yield curve regime switching, and (2) evidence that macroeconomic variables have predictive power for the yield curve when the short rate is high, but not in other regimes, thereby refining the notion of yield curve ``macro-spanning".</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12863v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyu Bie, Francis X. Diebold, Jingyu He, Junye Li</dc:creator>
    </item>
    <item>
      <title>A Web-Based Solution for Federated Learning with LLM-Based Automation</title>
      <link>https://arxiv.org/abs/2408.13010</link>
      <description>arXiv:2408.13010v1 Announce Type: cross 
Abstract: Federated Learning (FL) offers a promising approach for collaborative machine learning across distributed devices. However, its adoption is hindered by the complexity of building reliable communication architectures and the need for expertise in both machine learning and network programming. This paper presents a comprehensive solution that simplifies the orchestration of FL tasks while integrating intent-based automation. We develop a user-friendly web application supporting the federated averaging (FedAvg) algorithm, enabling users to configure parameters through an intuitive interface. The backend solution efficiently manages communication between the parameter server and edge nodes. We also implement model compression and scheduling algorithms to optimize FL performance. Furthermore, we explore intent-based automation in FL using a fine-tuned Language Model (LLM) trained on a tailored dataset, allowing users to conduct FL tasks using high-level prompts. We observe that the LLM-based automated solution achieves comparable test accuracy to the standard web-based solution while reducing transferred bytes by up to 64% and CPU time by up to 46% for FL tasks. Also, we leverage the neural architecture search (NAS) and hyperparameter optimization (HPO) using LLM to improve the performance. We observe that by using this approach test accuracy can be improved by 10-20% for the carried out FL tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13010v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chamith Mawela, Chaouki Ben Issaid, Mehdi Bennis</dc:creator>
    </item>
    <item>
      <title>Trimmed Mean for Partially Observed Functional Data</title>
      <link>https://arxiv.org/abs/2408.13062</link>
      <description>arXiv:2408.13062v1 Announce Type: cross 
Abstract: In practice, as opposed to a large set of finite-dimensional vectors approximated from discrete data, we often prefer to utilize functional data. In recent years, partially observable function data have frequently appeared in practical applications and are the objectofan increasing interest by the literature. In this thesis, we learn the concept of data integration depth of partially observable functions proposed by Elias et al. 2023, which can be used to measure the degree of data centralization. At the same time, we also studied the trimmed-mean estimator method and consistency proof proposed by Fraiman and Muniz 2001 for completely observable functions. This method refers to the process of removing some of the smallest and largest values before calculating the mean to enhance the robustness of the estimate. In this thesis, we introduce the concept of trimmed-mean estimator for partially observable functions. We discuss several theoretical and practical issues, including the proof that the proposed trimmed-mean estimator converges almost surely and provide a simulation study. The results show that our estimator performs better in terms of efficiency and robustness compared to the ordinary mean under partially observable functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13062v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Wang</dc:creator>
    </item>
    <item>
      <title>A latent space model for multivariate count data time series analysis</title>
      <link>https://arxiv.org/abs/2408.13162</link>
      <description>arXiv:2408.13162v1 Announce Type: cross 
Abstract: Motivated by a dataset of burglaries in Chicago, USA, we introduce a novel framework to analyze time series of count data combining common multivariate time series models with latent position network models. This novel methodology allows us to gain a new latent variable perspective on the crime dataset that we consider, allowing us to disentangle and explain the complex patterns exhibited by the data, while providing a natural time series framework that can be used to make future predictions. Our model is underpinned by two well known statistical approaches: a log-linear vector autoregressive model, which is prominent in the literature on multivariate count time series, and a latent projection model, which is a popular latent variable model for networks. The role of the projection model is to characterize the interaction parameters of the vector autoregressive model, thus uncovering the underlying network that is associated with the pairwise relationships between the time series. Estimation and inferential procedures are performed using an optimization algorithm and a Hamiltonian Monte Carlo procedure for efficient Bayesian inference. We also include a simulation study to illustrate the merits of our methodology in recovering consistent parameter estimates, and in making accurate future predictions for the time series. As we demonstrate in our application to the crime dataset, this new methodology can provide very meaningful model-based interpretations of the data, and it can be generalized to other time series contexts and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13162v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hardeep Kaur, Riccardo Rastelli</dc:creator>
    </item>
    <item>
      <title>A New Perspective to Fish Trajectory Imputation: A Methodology for Spatiotemporal Modeling of Acoustically Tagged Fish Data</title>
      <link>https://arxiv.org/abs/2408.13220</link>
      <description>arXiv:2408.13220v1 Announce Type: cross 
Abstract: The focus of this paper is a key component of a methodology for understanding, interpolating, and predicting fish movement patterns based on spatiotemporal data recorded by spatially static acoustic receivers. For periods of time, fish may be far from the receivers, resulting in the absence of observations. The lack of information on the fish's location for extended time periods poses challenges to the understanding of fish movement patterns, and hence, the identification of proper statistical inference frameworks for modeling the trajectories. As the initial step in our methodology, in this paper, we implement an imputation strategy that relies on both Markov chain and Brownian motion principles to enhance our dataset over time. This methodology will be generalizable and applicable to all fish species with similar migration patterns or data with similar structures due to the use of static acoustic receivers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13220v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahshid Ahmadian, Edward L. Boone, Grace S. Chiu</dc:creator>
    </item>
    <item>
      <title>Variational inference of effective range parameters for ${}^3$He-${}^4$He scattering</title>
      <link>https://arxiv.org/abs/2408.13250</link>
      <description>arXiv:2408.13250v1 Announce Type: cross 
Abstract: We use two different methods, Monte Carlo sampling and variational inference (VI), to perform a Bayesian calibration of the effective-range parameters in ${}^3$He-${}^4$He elastic scattering. The parameters are calibrated to data from a recent set of $^{3}$He-${}^4$He elastic scattering differential cross section measurements. Analysis of these data for $E_{\rm lab} \leq 4.3$ MeV yields a unimodal posterior for which both methods obtain the same structure. However, the effective-range expansion amplitude does not account for the $7/2^-$ state of ${}^7$Be so, even after calibration, the description of data at the upper end of this energy range is poor. The data up to $E_{\rm lab}=2.6$ MeV can be well described, but calibration to this lower-energy subset of the data yields a bimodal posterior. After adapting VI to treat such a multi-modal posterior we find good agreement between the VI results and those obtained with parallel-tempered Monte Carlo sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13250v1</guid>
      <category>nucl-th</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrius Burnelis, Vojta Kejzlar, Daniel R. Phillips</dc:creator>
    </item>
    <item>
      <title>An open-source framework for data-driven trajectory extraction from AIS data -- the $\alpha$-method</title>
      <link>https://arxiv.org/abs/2407.04402</link>
      <description>arXiv:2407.04402v3 Announce Type: replace 
Abstract: Ship trajectories from Automatic Identification System (AIS) messages are important in maritime safety, domain awareness, and algorithmic testing. Although the specifications for transmitting and receiving AIS messages are fixed, it is well known that technical inaccuracies and lacking seafarer compliance lead to severe data quality impairment. This paper proposes an adaptable, data-driven, maneuverability-dependent, $\alpha$-quantile-based framework for decoding, constructing, splitting, and assessing trajectories from raw AIS records to improve transparency in AIS data mining. Results indicate the proposed filtering algorithm robustly extracts clean, long, and uninterrupted trajectories for further processing. An open-source Python implementation of the framework is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04402v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Niklas Paulig, Ostap Okhrin</dc:creator>
    </item>
    <item>
      <title>Disclosure risk assessment with Bayesian non-parametric hierarchical modelling</title>
      <link>https://arxiv.org/abs/2408.12521</link>
      <description>arXiv:2408.12521v2 Announce Type: replace 
Abstract: Micro and survey datasets often contain private information about individuals, like their health status, income or political preferences. Previous studies have shown that, even after data anonymization, a malicious intruder could still be able to identify individuals in the dataset by matching their variables to external information. Disclosure risk measures are statistical measures meant to quantify how big such a risk is for a specific dataset. One of the most common measures is the number of sample unique values that are also population-unique. \cite{Man12} have shown how mixed membership models can provide very accurate estimates of this measure. A limitation of that approach is that the number of extreme profiles has to be chosen by the modeller. In this article, we propose a non-parametric version of the model, based on the Hierarchical Dirichlet Process (HDP). The proposed approach does not require any tuning parameter or model selection step and provides accurate estimates of the disclosure risk measure, even with samples as small as 1$\%$ of the population size. Moreover, a data augmentation scheme to address the presence of structural zeros is presented. The proposed methodology is tested on a real dataset from the New York census.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12521v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Battiston, Lorenzo Rimella</dc:creator>
    </item>
    <item>
      <title>Combining observational and experimental data for causal inference considering data privacy</title>
      <link>https://arxiv.org/abs/2308.02974</link>
      <description>arXiv:2308.02974v2 Announce Type: replace-cross 
Abstract: Combining observational and experimental data for causal inference can improve treatment effect estimation. However, many observational data sets cannot be released due to data privacy considerations, so one researcher may not have access to both experimental and observational data. Nonetheless, a small amount of risk of disclosing sensitive information might be tolerable to organizations that house confidential data. In these cases, organizations can employ data privacy techniques, which decrease disclosure risk, potentially at the expense of data utility. In this paper, we explore disclosure limiting transformations of observational data, which can be combined with experimental data to estimate the sample and population average treatment effects. We consider leveraging observational data to improve generalizability of treatment effect estimates when a randomized experiment (RCT) is not representative of the population of interest, and to increase precision of treatment effect estimates. Through simulation studies, we illustrate the trade-off between privacy and utility when employing different disclosure limiting transformations. We find that leveraging transformed observational data in treatment effect estimation can still improve estimation over only using data from an RCT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02974v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlotte Z. Mann, Adam C. Sales, Johann A. Gagnon-Bartsch</dc:creator>
    </item>
    <item>
      <title>On the Impact of Dynamic Beamforming on EMF Exposure and Network Coverage: A Stochastic Geometry Perspective</title>
      <link>https://arxiv.org/abs/2405.01190</link>
      <description>arXiv:2405.01190v3 Announce Type: replace-cross 
Abstract: This paper introduces a new mathematical framework for dynamic beamforming-based cellular networks, grounded in stochastic geometry. The framework is used to study the electromagnetic field exposure (EMFE) of active and idle users as a function of the distance between them. A novel multi-cosine antenna pattern is introduced, offering more accurate modeling by incorporating both main and side lobes. Results show that the cumulative distribution functions of EMFE and coverage obtained with the multi-cosine pattern align closely with theoretical models, reducing error to less than 2\%, compared to a minimum of 8\% for other models. The marginal distribution of EMFE for each user type is mathematically derived. A unique contribution is the introduction of the SCAIU (\underline{S}patial \underline{C}DF for \underline{A}ctive and \underline{I}dle \underline{U}sers), a metric that ensures coverage for active users while limiting EMFE for idle users. Network performance is analyzed using these metrics across varying distances and antenna elements. The analysis reveals that, for the chosen network parameters, with 64 antenna elements, the impact on idle user EMFE becomes negligible beyond 60~m. However, to maintain active user SINR above 10 dB and idle user EMFE below -50~dBm at 2~m, more than 256 elements are required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01190v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Gontier, Charles Wiame, Joe Wiart, Fran\c{c}ois Horlin, Christo Tsigros, Claude Oestges, Philippe De Doncker</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v3 Announce Type: replace-cross 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators use machine learning to fit the relationship between a binary win/loss outcome variable and certain game-state variables. To illustrate just how difficult it is to accurately fit such a model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don't achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage. Our findings are not unique to the particular application of estimating win probability; they are broadly applicable across sports analytics, as myriad other sports datasets are clustered into groups of observations that share the same outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Optical ISAC: Fundamental Performance Limits and Transceiver Design</title>
      <link>https://arxiv.org/abs/2408.11792</link>
      <description>arXiv:2408.11792v3 Announce Type: replace-cross 
Abstract: This paper characterizes the optimal capacity-distortion (C-D) tradeoff in an optical point-to-point (P2P) system with single-input single-output for communication and single-input multiple-output for sensing (SISO-COM and SIMO-SEN) within an integrated sensing and communication (ISAC) framework. We consider the optimal rate-distortion (R-D) region and explore several inner (IB) and outer (OB) bounds. We introduce practical, asymptotically optimal maximum a posteriori (MAP) and maximum likelihood estimators (MLE) for target distance, addressing nonlinear measurement-to-state relationships and non-conjugate priors. As the number of sensing antennas increases, these estimators converge to the Bayesian Cram\'er-Rao bound (BCRB). We also establish that the achievable rate-CRB (AR-CRB) serves as an OB for the optimal C-D region, valid for both unbiased estimators and asymptotically large numbers of receive antennas. To clarify that the input distribution determines the tradeoff across the Pareto boundary of the C-D region, we propose two algorithms: \textit{i}) an iterative Blahut-Arimoto algorithm (BAA)-type method, and \textit{ii}) a memory-efficient closed-form (CF) approach. The CF approach includes a CF optimal distribution for high optical signal-to-noise ratio (O-SNR) conditions. Additionally, we adapt and refine the Deterministic-Random Tradeoff (DRT) to this optical ISAC context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11792v3</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Ghazavi Khorasgani, Mahtab Mirmohseni, Ahmed Elzanaty</dc:creator>
    </item>
  </channel>
</rss>
