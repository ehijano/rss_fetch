<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:30:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Approximating Graphic Multi-Path TSP and Graphic Ordered TSP</title>
      <link>https://arxiv.org/abs/2509.00448</link>
      <description>arXiv:2509.00448v1 Announce Type: new 
Abstract: The path version of the Traveling Salesman Problem is one of the most well-studied variants of the ubiquitous TSP. Its generalization, the Multi-Path TSP, has recently been used in the best known algorithm for path TSP by Traub and Vygen [Cambridge University Press, 2024]. The best known approximation factor for this problem is $2.214$ by B\"{o}hm, Friggstad, M\"{o}mke and Spoerhase [SODA 2025]. In this paper we show that for the case of graphic metrics, a significantly better approximation guarantee of $2$ can be attained. Our algorithm is based on sampling paths from a decomposition of the flow corresponding to the optimal solution to the LP for the problem, and connecting the left-out vertices with doubled edges. The cost of the latter is twice the optimum in the worst case; we show how the cost of the sampled paths can be absorbed into it without increasing the approximation factor. Furthermore, we prove that any below-$2$ approximation algorithm for the special case of the problem where each source is the same as the corresponding sink yields a below-$2$ approximation algorithm for Graphic Multi-Path TSP.
  We also show that our ideas can be utilized to give a factor $1.791$-approximation algorithm for Ordered TSP in graphic metrics, for which the aforementioned paper [SODA 2025] and Armbruster, Mnich and N\"agele [APPROX 2024] give a $1.868$-approximation algorithm in general metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00448v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morteza Alimi, Niklas Dahlmeier, Tobias M\"omke, Philipp Pabst, Laura Vargas Koch</dc:creator>
    </item>
    <item>
      <title>How to Compute a Moving Sum</title>
      <link>https://arxiv.org/abs/2509.00537</link>
      <description>arXiv:2509.00537v1 Announce Type: new 
Abstract: Windowed recurrences are sliding window calculations where a function is applied iteratively across the window of data, and are ubiquitous throughout the natural, social, and computational sciences. In this monograph we explore the computational aspects of these calculations, including sequential and parallel computation, and develop the theory underlying the algorithms and their applicability. We introduce an efficient new sequential algorithm with low latency, and develop new techniques to derive and analyze the complexity and domain of validity of existing sequential algorithms. For parallel computation we derive new parallel and vector algorithms by relating windowed recurrences to the algebraic construction of semidirect products, and to algorithms for exponentiation in semigroups. In the middle chapters of the monograph we further develop the theory of semi-associativity and the algebraic conditions for representing function composition and function application by data. This systematizes the techniques used by practitioners to parallelize recurrence calculations. We end the monograph with an extensive gallery of examples of interest to specialists in many fields. Throughout the monograph new algorithms are described with pseudo-code transcribed from functioning source code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00537v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David K. Maslen, Daniel N. Rockmore</dc:creator>
    </item>
    <item>
      <title>Triangle Counting in Hypergraph Streams: A Complete and Practical Approach</title>
      <link>https://arxiv.org/abs/2509.00674</link>
      <description>arXiv:2509.00674v1 Announce Type: new 
Abstract: Triangle counting in hypergraph streams, including both hyper-vertex and hyper-edge triangles, is a fundamental problem in hypergraph analytics, with broad applications. However, existing methods face two key limitations: (i) an incomplete classification of hyper-vertex triangle structures, typically considering only inner or outer triangles; and (ii) inflexible sampling schemes that predefine the number of sampled hyperedges, which is impractical under strict memory constraints due to highly variable hyperedge sizes. To address these challenges, we first introduce a complete classification of hyper-vertex triangles, including inner, hybrid, and outer triangles. Based on this, we develop HTCount, a reservoir-based algorithm that dynamically adjusts the sample size based on the available memory M. To further improve memory utilization and reduce estimation error, we develop HTCount-P, a partition-based variant that adaptively partitions unused memory into independent sample subsets. We provide theoretical analysis of the unbiasedness and variance bounds of the proposed algorithms. Case studies demonstrate the expressiveness of our triangle structures in revealing meaningful interaction patterns. Extensive experiments on real-world hypergraphs show that both our algorithms achieve highly accurate triangle count estimates under strict memory constraints, with relative errors that are 1 to 2 orders of magnitude lower than those of existing methods and consistently high throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00674v1</guid>
      <category>cs.DS</category>
      <category>cs.GR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingkai Meng, Long Yuan, Xuemin Lin, Wenjie Zhang, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>Large cliques and large independent sets: can they coexist?</title>
      <link>https://arxiv.org/abs/2509.00721</link>
      <description>arXiv:2509.00721v1 Announce Type: new 
Abstract: For a graph $G$ and a parameter $k$, we call a vertex $k$-enabling if it belongs both to a clique of size $k$ and to an independent set of size $k$, and we call it $k$-excluding otherwise. Motivated by issues that arise in secret sharing schemes, we study the complexity of detecting vertices that are $k$-excluding. We show that for every $\epsilon$, for sufficiently large $n$, if $k &gt; (\frac{1}{4} + \epsilon)n$, then every graph on $n$ vertices must have a $k$-excluding vertex, and moreover, such a vertex can be found in polynomial time. In contrast, if $k &lt; (\frac{1}{4} - \epsilon)n$, a regime in which it might be that all vertices are $k$-enabling, deciding whether a graph has no $k$-excluding vertex is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00721v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Uriel Feige, Ilia Pauzner</dc:creator>
    </item>
    <item>
      <title>New approximate distance oracles and their applications</title>
      <link>https://arxiv.org/abs/2509.00890</link>
      <description>arXiv:2509.00890v1 Announce Type: new 
Abstract: Let $G = (V, E)$ be an undirected graph with $n$ vertices and $m$ edges, and let $\mu = m/n$. A \emph{distance oracle} is a data structure designed to answer approximate distance queries, with the goal of achieving low stretch, efficient space usage, and fast query time. While much of the prior work focused on distance oracles with constant query time, this paper presents a comprehensive study of distance oracles with non-constant query time. We explore the tradeoffs between space, stretch, and query time of distance oracles in various regimes. Specifically, we consider both weighted and unweighted graphs in the regimes of stretch $&lt; 2$ and stretch $\ge 2$. Among our results, we present a new three-way trade-off between stretch, space, and query time, offering a natural extension of the classical Thorup-Zwick distance oracle [STOC'01 and JACM'05] to regimes with larger query time. Specifically, for any $0 &lt; r &lt; 1/2$ and integer $k \ge 1$, we construct a $(2k(1 - 2r) - 1)$-stretch distance oracle with $\tilde{O}(m + n^{1 + 1/k})$ space and $\tilde{O}(\mu n^r)$ query time. In addition, we demonstrate several applications of our new distance oracles to the $n$-Pairs Shortest Paths ($n$-PSP) problem and the All Nodes Shortest Cycles ($ANSC$) problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00890v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avi Kadria, Liam Roditty</dc:creator>
    </item>
    <item>
      <title>Almost Tight Approximation Hardness and Online Algorithms for Resource Scheduling</title>
      <link>https://arxiv.org/abs/2509.01086</link>
      <description>arXiv:2509.01086v1 Announce Type: new 
Abstract: We study the precedence-constrained resource scheduling problem [SICOMP'75]. There are $n$ jobs where each job takes a certain time to finish and has a resource requirement throughout the execution time. There are precedence among the jobs. The problem asks that given a resource budget, schedule the jobs obeying the precedence constraints to minimize makespan (maximum completion time of a job) such that at any point in time, the total resource being used by all the jobs is at most the given resource budget. In the offline setting, an important open question is whether a polynomial-time $O(1)$-factor approximation algorithm can be found. We prove almost tight hardness of approximation: For some constant $\alpha &gt; 0$, there is no $o((\log t_{\max})^{\alpha})$-factor ( or $o( ( \log n )^\alpha )$-factor ) approximation algorithm with $n$ jobs of maximum job length $t_{\max}$, unless P = NP ( or NP $\subset$ DTIME$(O( 2^{\text{polylog}(n)}))$ ).
  We further show a connection between this scheduling problem and a seemingly unrelated problem called the shortest common super-sequence (SCS) problem, which has wide application in Biology and Genomics. We prove that an $o(\log t_{\max})$-factor approximation of the scheduling problem would imply the existence of an $o(|\Sigma|)$-approximation algorithm for SCS with alphabet $\Sigma$.
  We then consider the online setting. We present $\Omega(\log n)$ and $\Omega(\log t_{\max})$ lower bounds of the competitive ratio of any randomized online algorithm. Moreover, we present a matching $O(\min\{\log n, \log t_{\max}\})$-competitive deterministic online algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01086v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rathish Das, Hao Sun</dc:creator>
    </item>
    <item>
      <title>Column-generation for a two-dimensional multi-criteria bin-packing problem</title>
      <link>https://arxiv.org/abs/2509.01218</link>
      <description>arXiv:2509.01218v1 Announce Type: new 
Abstract: In this study, we examine a two-dimensional bin-packing problem in printed circuit board manufacturing. Among other objectives, the number of bins, but also the number of different bin layouts, is to be minimized. As the running times of an earlier MIP presentation are only acceptable for small problem instances, we will now discuss a branch-and-price approach by using an adapted Ryan-Foster-branching. The pricing problem computes the layouts, separating the time-consuming constraints from the master problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01218v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christof Groschke, Steffen Goebbels, Jochen Rethmann</dc:creator>
    </item>
    <item>
      <title>Hitting Geodesic Intervals in Structurally Restricted Graphs</title>
      <link>https://arxiv.org/abs/2509.01413</link>
      <description>arXiv:2509.01413v1 Announce Type: new 
Abstract: Given a graph $G = (V,E)$, a set $T$ of vertex pairs, and an integer $k$, Hitting Geodesic Intervals asks whether there is a set $S \subseteq V$ of size at most $k$ such that for each terminal pair $\{u,v\} \in T$, the set $S$ intersects at least one shortest $u$-$v$ path. Aravind and Saxena [WALCOM 2024] introduced this problem and showed several parameterized complexity results. In this paper, we extend the known results in both negative and positive directions and present sharp complexity contrasts with respect to structural graph parameters.
  We first show that the problem is NP-complete even on graphs obtained by adding a single vertex to a disjoint union of 5-vertex paths. By modifying the proof of this result, we also show the NP-completeness on graphs obtained from a path by adding one vertex and on graphs obtained from a disjoint union of triangles by adding one universal vertex. Furthermore, we show the NP-completeness on graphs of bandwidth 4 and maximum degree 5 by replacing the universal vertex in the last case with a long path. Under standard complexity assumptions, these negative results rule out fixed-parameter algorithms for most of the structural parameters studied in the literature (if the solution size $k$ is not part of the parameter).
  We next present fixed-parameter algorithms parameterized by $k$ plus modular-width and by $k$ plus vertex integrity. The algorithm for the latter case does indeed solve a more general setting that includes the parameterization by the minimum vertex multiway-cut size of the terminal vertices. We show that this is tight in the sense that the problem parameterized by the minimum vertex multicut size of the terminal pairs is W[2]-complete. We then modify the proof of this intractability result and show that the problem is W[2]-complete parameterized by $k$ even in the setting where $T = \binom{Q}{2}$ for some $Q \subseteq V$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01413v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatsuya Gima, Yasuaki Kobayashi, Yuto Okada, Yota Otachi, Hayato Takaike</dc:creator>
    </item>
    <item>
      <title>Fixed-Parameter Tractable Submodular Maximization over a Matroid</title>
      <link>https://arxiv.org/abs/2509.01591</link>
      <description>arXiv:2509.01591v1 Announce Type: new 
Abstract: In this paper, we design fixed-parameter tractable (FPT) algorithms for (non-monotone) submodular maximization subject to a matroid constraint, where the matroid rank $r$ is treated as a fixed parameter that is independent of the total number of elements $n$. We provide two FPT algorithms: one for the offline setting and another for the random-order streaming setting. Our streaming algorithm achieves a $\frac{1}{2}-\varepsilon$ approximation using $\widetilde{O}\left(\frac{r}{\textrm{poly}(\varepsilon)}\right)$ memory, while our offline algorithm obtains a $1-\frac{1}{e}-\varepsilon$ approximation with $n\cdot 2^{\widetilde{O}\left(\frac{r}{\textrm{poly}(\varepsilon)}\right)}$ runtime and $\widetilde{O}\left(\frac{r}{\textrm{poly}(\varepsilon)}\right)$ memory. Both approximation factors are near-optimal in their respective settings, given existing hardness results. In particular, our offline algorithm demonstrates that--unlike in the polynomial-time regime--there is essentially no separation between monotone and non-monotone submodular maximization under a matroid constraint in the FPT framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01591v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamisa Nematollahi, Adrian Vladu, Junyao Zhao</dc:creator>
    </item>
    <item>
      <title>Fast Computation of $k$-Runs, Parameterized Squares, and Other Generalised Squares</title>
      <link>https://arxiv.org/abs/2509.02179</link>
      <description>arXiv:2509.02179v1 Announce Type: new 
Abstract: A $k$-mismatch square is a string of the form $XY$ where $X$ and $Y$ are two equal-length strings that have at most $k$ mismatches. Kolpakov and Kucherov [Theor. Comput. Sci., 2003] defined two notions of $k$-mismatch repeats, called $k$-repetitions and $k$-runs, each representing a sequence of consecutive $k$-mismatch squares of equal length. They proposed algorithms for computing $k$-repetitions and $k$-runs working in $O(nk \log k + output)$ time for a string of length $n$ over an integer alphabet, where $output$ is the number of the reported repeats. We show that $output=O(nk \log k)$, both in case of $k$-repetitions and $k$-runs, which implies that the complexity of their algorithms is actually $O(nk \log k)$. We apply this result to computing parameterized squares.
  A parameterized square is a string of the form $XY$ such that $X$ and $Y$ parameterized-match, i.e., there exists a bijection $f$ on the alphabet such that $f(X) = Y$. Two parameterized squares $XY$ and $X'Y'$ are equivalent if they parameterized match. Recently Hamai et al. [SPIRE 2024] showed that a string of length $n$ over an alphabet of size $\sigma$ contains less than $n\sigma$ non-equivalent parameterized squares, improving an earlier bound by Kociumaka et al. [Theor. Comput. Sci., 2016]. We apply our bound for $k$-mismatch repeats to propose an algorithm that reports all non-equivalent parameterized squares in $O(n\sigma \log \sigma)$ time. We also show that the number of non-equivalent parameterized squares can be computed in $O(n \log n)$ time. This last algorithm applies to squares under any substring compatible equivalence relation and also to counting squares that are distinct as strings. In particular, this improves upon the $O(n\sigma)$-time algorithm of Gawrychowski et al. [CPM 2023] for counting order-preserving squares that are distinct as strings if $\sigma = \omega(\log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02179v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuto Nakashima, Jakub Radoszewski, Tomasz Wale\'n</dc:creator>
    </item>
    <item>
      <title>A Simple and Fast Reduction from Gomory-Hu Trees to Polylog Maxflows</title>
      <link>https://arxiv.org/abs/2509.02520</link>
      <description>arXiv:2509.02520v1 Announce Type: new 
Abstract: Given an undirected graph $G=(V,E,w)$, a Gomory-Hu tree $T$ (Gomory and Hu, 1961) is a tree on $V$ that preserves all-pairs mincuts of $G$ exactly.
  We present a simple, efficient reduction from Gomory-Hu trees to polylog maxflow computations. On unweighted graphs, our reduction reduces to maxflow computations on graphs of total instance size $\tilde{O}(m)$ and the algorithm requires only $\tilde{O}(m)$ additional time. Our reduction is the first that is tight up to polylog factors. The reduction also seamlessly extends to weighted graphs, however, instance sizes and runtime increase to $\tilde{O}(n^2)$.
  Finally, we show how to extend our reduction to reduce Gomory-Hu trees for unweighted hypergraphs to maxflow in hypergraphs. Again, our reduction is the first that is tight up to polylog factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02520v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Probst Gutenberg, Rasmus Kyng, Weixuan Yuan, Wuwei Yuan</dc:creator>
    </item>
    <item>
      <title>Reusing Samples in Variance Reduction</title>
      <link>https://arxiv.org/abs/2509.02526</link>
      <description>arXiv:2509.02526v1 Announce Type: new 
Abstract: We provide a general framework to improve trade-offs between the number of full batch and sample queries used to solve structured optimization problems. Our results apply to a broad class of randomized optimization algorithms that iteratively solve sub-problems to high accuracy. We show that such algorithms can be modified to reuse the randomness used to query the input across sub-problems. Consequently, we improve the trade-off between the number of gradient (full batch) and individual function (sample) queries for finite sum minimization, the number of matrix-vector multiplies (full batch) and random row (sample) queries for top-eigenvector computation, and the number of matrix-vector multiplies with the transition matrix (full batch) and generative model (sample) queries for optimizing Markov Decision Processes. To facilitate our analysis we introduce the notion of pseudo-independent algorithms, a generalization of pseudo-deterministic algorithms [Gat and Goldwasser 2011], that quantifies how independent the output of a randomized algorithm is from a randomness source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02526v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Jin, Ishani Karmarkar, Aaron Sidford, Jiayi Wang</dc:creator>
    </item>
    <item>
      <title>LHS in LHS: A new expansion strategy for Latin hypercube sampling in simulation design</title>
      <link>https://arxiv.org/abs/2509.00159</link>
      <description>arXiv:2509.00159v1 Announce Type: cross 
Abstract: Latin Hypercube Sampling (LHS) is a prominent tool in simulation design, with a variety of applications in high-dimensional and computationally expensive problems. LHS allows for various optimization strategies, most notably to ensure space-filling properties. However, LHS is a single-stage algorithm that requires a priori knowledge of the targeted sample size. In this work, we present LHS in LHS, a new expansion algorithm for LHS that enables the addition of new samples to an existing LHS-distributed set while (approximately) preserving its properties. In summary, the algorithm identifies regions of the parameter space that are far from the initial set, draws a new LHS within those regions, and then merges it with the original samples. As a by-product, we introduce a new metric, the LHS degree, which quantifies the deviation of a given design from an LHS distribution. Our public implementation is distributed via the Python package expandLHS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00159v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.HE</category>
      <category>cs.DS</category>
      <category>gr-qc</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.softx.2025.102294</arxiv:DOI>
      <arxiv:journal_reference>SoftwareX 31, 102294 (2025)</arxiv:journal_reference>
      <dc:creator>Matteo Boschini, Davide Gerosa, Alessandro Crespi, Matteo Falcone</dc:creator>
    </item>
    <item>
      <title>Analysis of Algorithms for Moser's Problems on Sums of Consecutive Primes</title>
      <link>https://arxiv.org/abs/2509.00236</link>
      <description>arXiv:2509.00236v1 Announce Type: cross 
Abstract: In his 1963 paper on the sum of consecutive primes, Moser posed four open questions related to $f(n)$, the number of ways an integer $n$ can be written as a sum of consecutive primes. (See also problem C2 from Richard K.~Guy's \textit{Unsolved Problems in Number Theory}.) In this paper, we present and analyze two algorithms that, when given a bound $x$, construct a histogram of values of $f(n)$ for all $n\le x$. These two algorithms were described, but not analyzed, by Jean Charles Meyrignac (2000) and Michael S. Branicky (2022). We show the first algorithm takes $O(x\log x)$ time using $x^{2/3}$ space, and the second has two versions, one of which takes $O(x\log x)$ time but only $x^{3/5}$ space, and the other which takes $O(x(\log x)^2)$ time but only $O( \sqrt{x\log x})$ space. However, Meyrinac's algorithm is easier to parallelize. We then present data generated by these algorithms that address all four open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00236v1</guid>
      <category>math.NT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan P. Sorenson, Eleanor Waiss</dc:creator>
    </item>
    <item>
      <title>Statistics-Friendly Confidentiality Protection for Establishment Data, with Applications to the QCEW</title>
      <link>https://arxiv.org/abs/2509.01597</link>
      <description>arXiv:2509.01597v1 Announce Type: cross 
Abstract: Confidentiality for business data is an understudied area of disclosure avoidance, where legacy methods struggle to provide acceptable results. Modern formal privacy techniques designed for person-level data do not provide suitable confidentiality/utility trade-offs due to the highly skewed nature of business data and because extreme outlier records are often important contributors to query answers. In this paper, inspired by Gaussian Differential Privacy, we propose a novel confidentiality framework for business data with a focus on interpretability for policy makers. We propose two query-answering mechanisms and analyze new challenges that arise when noisy query answers are converted into confidentiality-preserving microdata. We evaluate our mechanisms on confidential Quarterly Census of Employment and Wages (QCEW) microdata and a public substitute dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01597v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn Webb, Prottay Protivash, John Durrell, Daniell Toth, Aleksandra Slavkovi\'c, Daniel Kifer</dc:creator>
    </item>
    <item>
      <title>Derivation and Verification of Array Sorting by Merging, and its Certification in Dafny</title>
      <link>https://arxiv.org/abs/2509.01758</link>
      <description>arXiv:2509.01758v1 Announce Type: cross 
Abstract: We provide full certifications of two versions of merge sort of arrays in the verification-aware programming language Dafny. We start by considering schemas for applying the divide-and-conquer or partition method of solution to specifications given by pre- and post-conditions involving linear arrays. We then derive the merge sort and merging algorithms as instances of these schemas, thereby arriving at a fully recursive formulation. Further, the analysis of the tree of subproblems arising from the partition facilitates the design of loop invariants that allow to derive a fully iterative version (sometimes called bottom-up merge sort) that does not employ a stack. We show how the use of the provided schemas conveniently conducts the formalization and actual verification in Dafny. The whole method is also applicable to deriving variants of quicksort, which we sketch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01758v1</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juan Pablo Carbonell, Jos\'e E. Solsona, Nora Szasz, \'Alvaro Tasistro</dc:creator>
    </item>
    <item>
      <title>Continuous Petri Nets for Fast Yield Computation: Polynomial-Time and MILP Approaches</title>
      <link>https://arxiv.org/abs/2509.02371</link>
      <description>arXiv:2509.02371v1 Announce Type: cross 
Abstract: Petri nets provide accurate analogues to chemical reaction networks, with places representing individual molecules (the resources of the system) and transitions representing chemical reactions which convert educt molecules into product molecules. Their natural affinity for modeling chemical reaction networks is, however, impeded by their computational complexity, which is at least PSpace-hard for most interesting questions, including reachability. Continuous Petri nets offer the same structure and discrete time as discrete Petri nets, but use continuous state-space, which allows them to answer the reachability question in polynomial time. We exploit this property to introduce a polynomial time algorithm for computing the maximal yield of a molecule in a chemical system. Additionally, we provide an alternative algorithm based on mixed-integer linear programming with worse theoretical complexity, but better runtime in practice, as demonstrated on both synthetic and chemical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02371v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Addie Jordon, Juri Kol\v{c}\'ak, Daniel Merkle</dc:creator>
    </item>
    <item>
      <title>Tree algorithms for set reconciliation</title>
      <link>https://arxiv.org/abs/2509.02373</link>
      <description>arXiv:2509.02373v1 Announce Type: cross 
Abstract: In this work, a set reconciliation setting is considered in which two parties have similar sets that they would like to reconcile. In particular, we focus on a divide-and-conquer strategy known as partitioned set reconciliation (PSR), in which the sets to be reconciled are successively partitioned until they contain a number of differences below some predetermined value. Borrowing techniques from tree-algorithms for random-access protocols, we present and analyze a novel set reconciliation scheme that we term enhanced partitioned set reconciliation (EPSR). This approach improves the efficiency in terms of overhead, i.e., it yields a lower communication cost, while keeping the same time and communication round complexity as PSR. Additionally, we simulate the performance of the proposed algorithm in an event-driven simulator. Our findings indicate that this novel protocol nearly halves the communication cost of PSR while maintaining the same time complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02373v1</guid>
      <category>cs.NI</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco L\'azaro, \v{C}edomir Stefanovi\'c</dc:creator>
    </item>
    <item>
      <title>Cache Management for Mixture-of-Experts LLMs -- extended version</title>
      <link>https://arxiv.org/abs/2509.02408</link>
      <description>arXiv:2509.02408v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks. One of the main challenges towards the successful deployment of LLMs is memory management, since they typically involve billions of parameters. To this end, architectures based on Mixture-of-Experts have been proposed, which aim to reduce the size of the parameters that are activated when producing a token. This raises the equally critical issue of efficiently managing the limited cache of the system, in that frequently used experts should be stored in the fast cache rather than in the slower secondary memory.
  In this work, we introduce and study a new paging problem that models expert management optimization. Our formulation captures both the layered architecture of LLMs and the requirement that experts are cached efficiently. We first present lower bounds on the competitive ratio of both deterministic and randomized algorithms, which show that under mild assumptions, LRU-like policies have good theoretical competitive performance. We then propose a layer-based extension of LRU that is tailored to the problem at hand.
  Extensive simulations on both synthetic datasets and actual traces of MoE usage show that our algorithm outperforms policies for the classic paging problem, such as the standard LRU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02408v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-99872-0_2</arxiv:DOI>
      <dc:creator>Spyros Angelopoulos, Loris Marchal, Adrien Obrecht, Bertrand Simon</dc:creator>
    </item>
    <item>
      <title>Safe Memory Reclamation Techniques</title>
      <link>https://arxiv.org/abs/2509.02457</link>
      <description>arXiv:2509.02457v1 Announce Type: cross 
Abstract: Safe memory reclamation is crucial to memory safety for optimistic and lock-free concurrent data structures in non garbage collected programming languages. However, several challenges arise in designing an ideal safe memory reclamation algorithm, including achieving high speed and scalability, easy of use for programmers, applicability to wide class of data structures, managing the large memory footprint caused by delayed freeing of memory for safety and performance, and avoiding asymmetric overhead on data structure operations. Several approaches to designing safe memory reclamation algorithms are studied by blending ideas and tools from across the hardware-software stack. These solutions cross traditional boundaries and exploit features exposed at different layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02457v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajay Singh</dc:creator>
    </item>
    <item>
      <title>Sublinear Random Access Generators for Preferential Attachment Graphs</title>
      <link>https://arxiv.org/abs/1602.06159</link>
      <description>arXiv:1602.06159v4 Announce Type: replace 
Abstract: We consider the problem of sampling from a distribution on graphs, specifically when the distribution is defined by an evolving graph model, and consider the time, space and randomness complexities of such samplers.
  In the standard approach, the whole graph is chosen randomly according to the randomized evolving process, stored in full, and then queries on the sampled graph are answered by simply accessing the stored graph. This may require prohibitive amounts of time, space and random bits, especially when only a small number of queries are actually issued. Instead, we propose to generate the graph on-the-fly, in response to queries, and therefore to require amounts of time, space, and random bits which are a function of the actual number of queries.
  We focus on two random graph models: the Barab{\'{a}}si-Albert Preferential Attachment model (BA-graphs) and the random recursive tree model. We give on-the-fly generation algorithms for both models. With probability $1-1/\mbox{poly}(n)$, each and every query is answered in $\mbox{polylog}(n)$ time, and the increase in space and the number of random bits consumed by any single query are both $\mbox{polylog}(n)$, where $n$ denotes the number of vertices in the graph.
  Our results show that, although the BA random graph model is defined by a sequential process, efficient random access to the graph's nodes is possible. In addition to the conceptual contribution, efficient on-the-fly generation of random graphs can serve as a tool for the efficient simulation of sublinear algorithms over large BA-graphs, and the efficient estimation of their performance on such graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:1602.06159v4</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ACM Trans. Algorithms 17(4): 28:1-28:26 (2021)</arxiv:journal_reference>
      <dc:creator>Guy Even, Reut Levi, Moti Medina, Adi Rosen</dc:creator>
    </item>
    <item>
      <title>Data Structures for Range Sorted Consecutive Occurrence Queries</title>
      <link>https://arxiv.org/abs/2411.12099</link>
      <description>arXiv:2411.12099v3 Announce Type: replace 
Abstract: The string indexing problem is a fundamental computational problem with numerous applications, including information retrieval and bioinformatics. It aims to efficiently solve the pattern matching problem: given a text T of length n for preprocessing and a pattern P of length m as a query, the goal is to report all occurrences of P as substrings of T. Navarro and Thankachan [CPM 2015, Theor. Comput. Sci. 2016] introduced a variant of this problem called the gap-bounded consecutive occurrence query, which reports pairs of consecutive occurrences of P in T such that their gaps (i.e., the distances between them) lie within a query-specified range [g_1, g_2]. Recently, Bille et al. [FSTTCS 2020, Theor. Comput. Sci. 2022] proposed the top-k close consecutive occurrence query, which reports the k closest consecutive occurrences of P in T, sorted in non-decreasing order of distance. Both problems are optimally solved in query time with O(n \log n)-space data structures. In this paper, we generalize these problems to the range query model, which focuses only on occurrences of P in a specified substring T[a.. b] of T. Our contributions are as follows: (1) We propose an O(n \log^2 n)-space data structure that answers the range top-k consecutive occurrence query in O(m + \log\log n + k) time and can be built in O(n \log^3 n) time; and (2) We propose an O(n \log^{2+\epsilon} n)-space data structure that answers the range gap-bounded consecutive occurrence query in O(m + \log\log n + output) time and can be constructed in O(n \log^{5/2}n) time, where \epsilon is a positive constant and output denotes the number of outputs. As by-products, we present algorithms for geometric problems involving weighted horizontal segments in a 2D plane, which are of independent interest. Furthermore, we observe that consecutive occurrences are related to closed substrings of a string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12099v3</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Waseem Akram, Takuya Mieno</dc:creator>
    </item>
    <item>
      <title>Operational research approaches and mathematical models for kidney exchange: A literature survey and empirical evaluation</title>
      <link>https://arxiv.org/abs/2504.04091</link>
      <description>arXiv:2504.04091v2 Announce Type: replace 
Abstract: Kidney exchange is a transplant modality that has provided new opportunities for living kidney donation in many countries around the world since 1991. It has been extensively studied from an Operational Research (OR) perspective since 2004. This article provides a comprehensive literature survey on OR approaches to fundamental computational problems associated with kidney exchange over the last two decades. We also summarise the key integer linear programming (ILP) models for kidney exchange, showing how to model optimisation problems involving only cycles and chains separately. This allows new combined ILP models, not previously presented, to be obtained by amalgamating cycle and chain models. We present a comprehensive empirical evaluation involving all combined models from this paper in addition to bespoke software packages from the literature involving advanced techniques. This focuses primarily on computation times for 49 methods applied to 4,320 problem instances of varying sizes that reflect the characteristics of real kidney exchange datasets, corresponding to over 200,000 algorithm executions. We have made our implementations of all cycle and chain models described in this paper, together with all instances used for the experiments, and a web application to visualise our experimental results, publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04091v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathijs Barkel, Rachael Colley, Maxence Delorme, David Manlove, William Pettersson</dc:creator>
    </item>
    <item>
      <title>Smaller and More Flexible Cuckoo Filters</title>
      <link>https://arxiv.org/abs/2505.05847</link>
      <description>arXiv:2505.05847v2 Announce Type: replace 
Abstract: Cuckoo filters are space-efficient approximate set membership data structures with a controllable false positive rate (FPR) and zero false negatives, similar to Bloom filters. In contrast to Bloom filters, Cuckoo filters store multi-bit fingerprints of keys in a hash table using variants of Cuckoo hashing, allowing each fingerprint to be stored at a small number of possible locations. Existing Cuckoo filters use fingerprints of $(k+3)$ bits per key and an additional space overhead factor of at least $1.05$ to achieve an FPR of $2^{-k}$. For $k=10$, this amounts to $1.365\, kn$ bits to store $n$ keys, which is better than $1.443\, kn$ bits for Bloom filters. The $+3$ for the fingerprint size is required to balance out the multiplied FPR caused by looking for the fingerprint at several locations. In the original Cuckoo filter, the number of hash table buckets is restricted to a power of 2, which may lead to much larger space overheads, up to $2.1\, (1+3/k)\, kn$ bits.
  We present two improvements of Cuckoo filters. First, we remove the restriction that the number of buckets must be a power of 2 by using a different placement strategy. Second, we reduce the space overhead factor of Cuckoo filters to $1.06 \, (1+2/k)$ by using overlapping windows instead of disjoint buckets to maintain the load threshold of the hash table, while reducing the number of alternative slots where any fingerprint may be found.
  A detailed evaluation demonstrates that the alternative memory layout based on overlapping windows decreases the size of Cuckoo filters not only in theory, but also in practice. A comparison with other state-of-the art filter types, Prefix filters and Vector Quotient filters (VQFs), shows that the reduced space overhead makes windowed Cuckoo filters the smallest filters supporting online insertions, with similarly fast queries, but longer insertion times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05847v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna Elena Schmitz, Jens Zentgraf, Sven Rahmann</dc:creator>
    </item>
    <item>
      <title>Labelling Data with Unknown References</title>
      <link>https://arxiv.org/abs/2506.03083</link>
      <description>arXiv:2506.03083v3 Announce Type: replace 
Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. The two ways to establish trustworthiness are either by testing it, or by assuming the evaluator `knows' somehow the way to label the corpus. However, if labelled references (e.g., a development set) are unavailable, neither of these approaches work: the former requires the data, and the latter is an assumption, not evidence. To address this, we introduce an algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator without any existing references. Our algorithm works by successively posing challenges to said evaluator. We show that this is sufficient to establish trustworthiness w.h.p., in such a way that when the evaluator actually knows the way to label the corpus, the No-Data Algorithm accepts its output; and, conversely, flags untrustworthy evaluators when these are unable to prove it. We present formal proofs of correctness, empirical tests, and applications to LLMs-as-judges on low-resource languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03083v3</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adrian de Wynter</dc:creator>
    </item>
    <item>
      <title>Counting Distinct Square Substrings in Sublinear Time</title>
      <link>https://arxiv.org/abs/2508.03930</link>
      <description>arXiv:2508.03930v2 Announce Type: replace 
Abstract: We show that the number of distinct squares in a packed string of length $n$ over an alphabet of size $\sigma$ can be computed in $O(n/\log_\sigma n)$ time in the word-RAM model. This paper is the first to introduce a sublinear-time algorithm for counting squares in the packed setting. The packed representation of a string of length $n$ over an alphabet of size $\sigma$ is given as a sequence of $O(n/\log_\sigma n)$ machine words in the word-RAM model (a machine word consists of $\omega \ge \log_2 n$ bits). Previously, it was known how to count distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for a string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al., CPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for extracting squares from runs described by Crochemore et al. [TCS 2014]. However, the packed model requires novel approaches.
  We need an $O(n/\log_\sigma n)$-sized representation of all long-period runs (runs with period $\Omega(\log_\sigma n)$) which allows for a sublinear-time counting of the -- potentially linearly-many -- implied squares. The long-period runs with a string period that is periodic itself (called layer runs) are an obstacle, since their number can be $\Omega(n)$. The number of all other long-period runs is $O(n/\log_\sigma n)$ and we can construct an implicit representation of all long-period runs in $O(n/\log_\sigma n)$ time by leveraging the insights of Amir et al. [ESA 2019]. We count squares in layer runs by exploiting combinatorial properties of pyramidally-shaped groups of layer runs. Another difficulty lies in computing the locations of Lyndon roots of runs in packed strings, which is needed for grouping runs that may generate equal squares. To overcome this difficulty, we introduce sparse-Lyndon roots which are based on string synchronizers [Kempa and Kociumaka, STOC 2019].</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03930v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Panagiotis Charalampopoulos, Manal Mohamed, Jakub Radoszewski, Wojciech Rytter, Tomasz Wale\'n, Wiktor Zuba</dc:creator>
    </item>
    <item>
      <title>$\Delta$-Motif: Subgraph Isomorphism at Scale via Data-Centric Parallelism</title>
      <link>https://arxiv.org/abs/2508.21287</link>
      <description>arXiv:2508.21287v2 Announce Type: replace 
Abstract: Subgraph isomorphism is a fundamental problem in graph analysis that seeks to find all instances of a pattern graph within a larger data graph while preserving structural relationships. This NP-complete problem is central to domains such as biological network analysis, social network mining, and quantum circuit optimization. Traditional approaches rely on backtracking algorithms like VF2, which suffer from sequential bottlenecks that limit their ability to exploit modern parallel hardware. In this work, we introduce $\Delta$-Motif, a GPU-accelerated subgraph isomorphism algorithm that reformulates the task through the lens of database operations. Our key insight is to represent both data and pattern graphs in tabular form, turning subgraph isomorphism into database primitives including joins, sorts, merges, and filters. $\Delta$-Motif decomposes graphs into small building blocks called motifs and systematically combines them using scalable relational operations. By leveraging mature, optimized libraries from the NVIDIA RAPIDS ecosystem and Pandas framework, our solution achieves massive parallelism while remaining portable across systems supporting standard relational primitives. Benchmarks show that $\Delta$-Motif outperforms established algorithms like VF2, achieving speedups of up to $595\times$ on GPUs. We further demonstrate its impact by applying it to quantum circuit compilation, addressing a critical bottleneck in quantum computing and enabling scaling to near- and medium-term devices. Our approach democratizes high-performance graph processing by exposing it through familiar database abstractions, eliminating the need for low-level programming while delivering exceptional computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21287v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulun Wang, Esteban Ginez, Jamie Friel, Yuval Baum, Jin-Sung Kim, Alex Shih, Oded Green</dc:creator>
    </item>
    <item>
      <title>Approximating Nash Social Welfare by Matching and Local Search</title>
      <link>https://arxiv.org/abs/2211.03883</link>
      <description>arXiv:2211.03883v4 Announce Type: replace-cross 
Abstract: For any $\eps&gt;0$, we give a simple, deterministic $(4+\eps)$-approximation algorithm for the Nash social welfare (NSW) problem under submodular valuations. We also consider the asymmetric variant of the problem, where the objective is to maximize the weighted geometric mean of agents' valuations, and give an $\ee (\omega + 2 + \eps)$-approximation if the ratio between the largest weight and the average weight is at most $\omega$.
  We also show that the $\nfrac12$-EFX envy-freeness property can be attained simultaneously with a constant-factor approximation. More precisely, we can find an allocation in polynomial time that is both $\nfrac12$-EFX and a $(8+\eps)$-approximation to the symmetric NSW problem under submodular valuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.03883v4</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jugal Garg, Edin Husi\'c, Wenzheng Li, L\'aszl\'o A. V\'egh, Jan Vondr\'ak</dc:creator>
    </item>
    <item>
      <title>Isometric path complexity of graphs</title>
      <link>https://arxiv.org/abs/2301.00278</link>
      <description>arXiv:2301.00278v5 Announce Type: replace-cross 
Abstract: A set $S$ of isometric paths of a graph $G$ is ``$v$-rooted'', where $v$ is a vertex of $G$, if $v$ is one of the endpoints of all the isometric paths in $S$. The isometric path complexity of a graph $G$, denoted by $ipco{G}$, is the minimum integer $k$ such that there exists a vertex $v\in V(G)$ satisfying the following property: the vertices of any single isometric path $P$ of $G$ can be covered by $k$ many $v$-rooted isometric paths.
  First, we provide an $O(n^2 m)$-time algorithm to compute the isometric path complexity of a graph with $n$ vertices and $m$ edges. Then we show that the isometric path complexity remains bounded for graphs in three seemingly unrelated graph classes, namely, hyperbolic graphs, (theta, prism, pyramid)-free graphs, and outerstring graphs.
  There is a direct algorithmic consequence of having small isometric path complexity. Specifically, we show that if the isometric path complexity of a graph $G$ is bounded by a constant, then there exists a polynomial-time constant-factor approximation algorithm for ISOMETRIC PATH COVER, whose objective is to cover all vertices of a graph with a minimum number of isometric paths. This applies to all the above graph classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00278v5</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.disc.2025.114743</arxiv:DOI>
      <arxiv:journal_reference>Discrete Mathematics 349(2):114743, 2026</arxiv:journal_reference>
      <dc:creator>Dibyayan Chakraborty, J\'er\'emie Chalopin, Florent Foucaud, Yann Vax\`es</dc:creator>
    </item>
    <item>
      <title>Adaptive control of dynamic networks</title>
      <link>https://arxiv.org/abs/2302.09743</link>
      <description>arXiv:2302.09743v3 Announce Type: replace-cross 
Abstract: Real-world network systems are inherently dynamic, with network topologies undergoing continuous changes over time. Previous works often focus on static networks or rely on complete prior knowledge of evolving topologies, whereas real-world networks typically undergo stochastic structural changes that are difficult to predict in advance. To address this challenge, we define the adaptive control problem and propose an adaptive control algorithm to reduce the extra control cost caused by driver node switching. We introduce a node-level adaptive control metric to capture both the stability and consistency of each node across historical topologies. By integrating this metric with a partial matching repair strategy, our algorithm adjusts the minimum driver node set in real time at each snapshot, while minimizing unnecessary reconfigurations between consecutive time steps. Extensive experiments on synthetic and real-world dynamic networks demonstrate that the proposed adaptive control algorithm significantly outperforms the existing algorithm, reducing the switching cost by an average of 22% in synthetic networks and 19\% in real-world networks, without requiring foreknowledge of the future evolution of the network. These findings extend the theoretical scope of dynamic network controllability and open new avenues for practical applications in transportation, social, and molecular regulatory systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09743v3</guid>
      <category>eess.SY</category>
      <category>cs.DS</category>
      <category>cs.SY</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunyu Pan, Xizhe Zhang, Haoyu Zheng, Zhao Su, Changsheng Zhang, Weixiong Zhang</dc:creator>
    </item>
    <item>
      <title>Randomly Punctured Reed-Solomon Codes Achieve the List Decoding Capacity over Polynomial-Size Alphabets</title>
      <link>https://arxiv.org/abs/2304.01403</link>
      <description>arXiv:2304.01403v3 Announce Type: replace-cross 
Abstract: This paper shows that, with high probability, randomly punctured Reed-Solomon codes over fields of polynomial size achieve the list decoding capacity. More specifically, we prove that for any $\epsilon&gt;0$ and $R\in (0,1)$, with high probability, randomly punctured Reed-Solomon codes of block length $n$ and rate $R$ are $\left(1-R-\epsilon, O({1}/{\epsilon})\right)$ list decodable over alphabets of size at least $2^{\mathrm{poly}(1/\epsilon)}n^2$. This extends the recent breakthrough of Brakensiek, Gopi, and Makam (STOC 2023) that randomly punctured Reed-Solomon codes over fields of exponential size attain the generalized Singleton bound of Shangguan and Tamo (STOC 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01403v3</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Guo, Zihan Zhang</dc:creator>
    </item>
    <item>
      <title>Succinct Encodings of Binary Trees with Application to AVL Trees</title>
      <link>https://arxiv.org/abs/2311.15511</link>
      <description>arXiv:2311.15511v3 Announce Type: replace-cross 
Abstract: We use a novel decomposition to create succinct data structures -- supporting a wide range of operations on static trees in constant time -- for a variety tree classes, extending results of Munro, Nicholson, Benkner, and Wild. Motivated by the class of AVL trees, we further derive asymptotics for the information-theoretic lower bound on the number of bits needed to store tree classes whose generating functions satisfy certain functional equations. In particular, we prove that AVL trees require approximately $0.938$ bits per node to encode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15511v3</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Chizewer, Stephen Melczer, J. Ian Munro, Ava Pun</dc:creator>
    </item>
    <item>
      <title>Non-Boolean OMv: One More Reason to Believe Lower Bounds for Dynamic Problems</title>
      <link>https://arxiv.org/abs/2409.15970</link>
      <description>arXiv:2409.15970v2 Announce Type: replace-cross 
Abstract: Most of the known tight lower bounds for dynamic problems are based on the Online Boolean Matrix-Vector Multiplication (OMv) Hypothesis, which is not as well studied and understood as some more popular hypotheses in fine-grained complexity. It would be desirable to base hardness of dynamic problems on a more believable hypothesis. We propose analogues of the OMv Hypothesis for variants of matrix multiplication that are known to be harder than Boolean product in the offline setting, namely: equality, dominance, min-witness, min-max, and bounded monotone min-plus products. These hypotheses are a priori weaker assumptions than the standard (Boolean) OMv Hypothesis. Somewhat surprisingly, we show that they are actually equivalent to it. This establishes the first such fine-grained equivalence class for dynamic problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15970v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Hu, Adam Polak</dc:creator>
    </item>
    <item>
      <title>Relative-error monotonicity testing</title>
      <link>https://arxiv.org/abs/2410.09235</link>
      <description>arXiv:2410.09235v2 Announce Type: replace-cross 
Abstract: The standard model of Boolean function property testing is not well suited for testing $\textit{sparse}$ functions which have few satisfying assignments, since every such function is close (in the usual Hamming distance metric) to the constant-0 function. In this work we propose and investigate a new model for property testing of Boolean functions, called $\textit{relative-error testing}$, which provides a natural framework for testing sparse functions.
  This new model defines the distance between two functions $f, g: \{0,1\}^n \to \{0,1\}$ to be $$\textsf{reldist}(f,g) := { \frac{|f^{-1}(1) \triangle g^{-1}(1)|} {|f^{-1}(1)|}}.$$ This is a more demanding distance measure than the usual Hamming distance ${ {|f^{-1}(1) \triangle g^{-1}(1)|}/{2^n}}$ when $|f^{-1}(1)| \ll 2^n$; to compensate for this, algorithms in the new model have access both to a black-box oracle for the function $f$ being tested and to a source of independent uniform satisfying assignments of $f$.
  In this paper we first give a few general results about the relative-error testing model; then, as our main technical contribution, we give a detailed study of algorithms and lower bounds for relative-error testing of $\textit{monotone}$ Boolean functions. We give upper and lower bounds which are parameterized by $N=|f^{-1}(1)|$, the sparsity of the function $f$ being tested. Our results show that there are interesting differences between relative-error monotonicity testing of sparse Boolean functions, and monotonicity testing in the standard model. These results motivate further study of the testability of Boolean function properties in the relative-error model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09235v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Anindya De, Yizhi Huang, Yuhao Li, Shivam Nadimpalli, Rocco A. Servedio, Tianqi Yang</dc:creator>
    </item>
    <item>
      <title>Adaptive and oblivious statistical adversaries are equivalent</title>
      <link>https://arxiv.org/abs/2410.13548</link>
      <description>arXiv:2410.13548v2 Announce Type: replace-cross 
Abstract: We resolve a fundamental question about the ability to perform a statistical task, such as learning, when an adversary corrupts the sample. Such adversaries are specified by the types of corruption they can make and their level of knowledge about the sample. The latter distinguishes between sample-adaptive adversaries which know the contents of the sample when choosing the corruption, and sample-oblivious adversaries, which do not. We prove that for all types of corruptions, sample-adaptive and sample-oblivious adversaries are \emph{equivalent} up to polynomial factors in the sample size. This resolves the main open question introduced by [BLMT22] and further explored in [CHL+23].
  Specifically, consider any algorithm $A$ that solves a statistical task even when a sample-oblivious adversary corrupts its input. We show that there is an algorithm $A'$ that solves the same task when the corresponding sample-adaptive adversary corrupts its input. The construction of $A'$ is simple and maintains the computational efficiency of $A$: It requests a polynomially larger sample than $A$ uses and then runs $A$ on a uniformly random subsample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13548v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Gregory Valiant</dc:creator>
    </item>
    <item>
      <title>Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator</title>
      <link>https://arxiv.org/abs/2506.01107</link>
      <description>arXiv:2506.01107v2 Announce Type: replace-cross 
Abstract: The move-acceptance hyper-heuristic was recently shown to be able to leave local optima with astonishing efficiency (Lissovoi et al., Artificial Intelligence (2023)). In this work, we propose two modifications to this algorithm that demonstrate impressive performances on a large class of benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i) Instead of randomly choosing between the only-improving and any-move acceptance operator, we take this choice via a simple two-state Markov chain. This modification alone reduces the runtime on Jump$_m$ functions with gap parameter $m$ from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves acceptance operator with the operator that only accepts worsenings. Such a, counter-intuitive, operator has not been used before in the literature. However, our proofs show that our only-worsening operator can greatly help in leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3 \log n)$ independent of the gap size. In general, we prove a remarkably good runtime of $O(n^{k+1} \log n)$ for our Markov move-acceptance hyper-heuristic on all members of a new benchmark class SEQOPT$_k$, which contains a large number of functions having $k$ successive local optima, and which contains the commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01107v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abderrahim Bendahi, Benjamin Doerr, Adrien Fradin, Johannes F. Lutzeyer</dc:creator>
    </item>
    <item>
      <title>Designing Parallel Algorithms for Community Detection using Arachne</title>
      <link>https://arxiv.org/abs/2507.06471</link>
      <description>arXiv:2507.06471v2 Announce Type: replace-cross 
Abstract: The rise of graph data in various fields calls for efficient and scalable community detection algorithms. In this paper, we present parallel implementations of two widely used algorithms: Label Propagation and Louvain, specifically designed to leverage the capabilities of Arachne, which is a Python-accessible open-source framework for large-scale graph analysis. Our implementations achieve substantial speedups over existing Python-based tools like NetworkX and igraph, which lack efficient parallelization, and are competitive with parallel frameworks such as NetworKit. Experimental results show that Arachne-based methods outperform these baselines, achieving speedups of up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit. Additionally, we analyze the scalability of our implementation under varying thread counts, demonstrating how different phases contribute to overall performance gains of the parallel Louvain algorithm. Arachne, including our community detection implementation, is open-source and available at https://github.com/Bears-R-Us/arkouda-njit .</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06471v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fuhuan Li, Zhihui Du, David A. Bader</dc:creator>
    </item>
    <item>
      <title>Online Stochastic Packing with General Correlations</title>
      <link>https://arxiv.org/abs/2508.13458</link>
      <description>arXiv:2508.13458v2 Announce Type: replace-cross 
Abstract: There has been a growing interest in studying online stochastic packing under more general correlation structures, motivated by the complex data sets and models driving modern applications. Several past works either assume correlations are weak or have a particular structure, have a complexity scaling with the number of Markovian "states of the world" (which may be exponentially large e.g. in the case of full history dependence), scale poorly with the horizon $T$, or make additional continuity assumptions. Surprisingly, we show that for all $\epsilon$, the online stochastic packing linear programming problem with general correlations (suitably normalized and with sparse columns) has an approximately optimal policy (with optimality gap $\epsilon T$) whose per-decision runtime scales as the time to simulate a single sample path of the underlying stochastic process (assuming access to a Monte Carlo simulator), multiplied by a constant independent of the horizon or number of Markovian states. We derive analogous results for network revenue management, and online bipartite matching and independent set in bounded-degree graphs, by rounding. Our algorithms implement stochastic gradient methods in a novel on-the-fly/recursive manner for the associated massive deterministic-equivalent linear program on the corresponding probability space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13458v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabri Cetin, Yilun Chen, David A. Goldberg</dc:creator>
    </item>
  </channel>
</rss>
