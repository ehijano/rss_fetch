<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jul 2024 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>posteriordb: Testing, Benchmarking and Developing Bayesian Inference Algorithms</title>
      <link>https://arxiv.org/abs/2407.04967</link>
      <description>arXiv:2407.04967v1 Announce Type: new 
Abstract: The generality and robustness of inference algorithms is critical to the success of widely used probabilistic programming languages such as Stan, PyMC, Pyro, and Turing.jl. When designing a new general-purpose inference algorithm, whether it involves Monte Carlo sampling or variational approximation, the fundamental problem arises in evaluating its accuracy and efficiency across a range of representative target models. To solve this problem, we propose posteriordb, a database of models and data sets defining target densities along with reference Monte Carlo draws. We further provide a guide to the best practices in using posteriordb for model evaluation and comparison. To provide a wide range of realistic target densities, posteriordb currently comprises 120 representative models and has been instrumental in developing several general inference algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04967v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M{\aa}ns Magnusson, Jakob Torgander, Paul-Christian B\"urkner, Lu Zhang, Bob Carpenter, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation and Output Analysis for High-Dimensional MCMC</title>
      <link>https://arxiv.org/abs/2407.05492</link>
      <description>arXiv:2407.05492v1 Announce Type: new 
Abstract: The widespread use of Markov Chain Monte Carlo (MCMC) methods for high-dimensional applications has motivated research into the scalability of these algorithms with respect to the dimension of the problem. Despite this, numerous problems concerning output analysis in high-dimensional settings have remained unaddressed. We present novel quantitative Gaussian approximation results for a broad range of MCMC algorithms. Notably, we analyse the dependency of the obtained approximation errors on the dimension of both the target distribution and the feature space. We demonstrate how these Gaussian approximations can be applied in output analysis. This includes determining the simulation effort required to guarantee Markov chain central limit theorems and consistent variance estimation in high-dimensional settings. We give quantitative convergence bounds for termination criteria and show that the termination time of a wide class of MCMC algorithms scales polynomially in dimension while ensuring a desired level of precision. Our results offer guidance to practitioners for obtaining appropriate standard errors and deciding the minimum simulation effort of MCMC algorithms in both multivariate and high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05492v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ardjen Pengel, Jun Yang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Reducing Total Trip Time and Vehicle Emission through Park-and-Ride -- methods and case-study</title>
      <link>https://arxiv.org/abs/2407.05572</link>
      <description>arXiv:2407.05572v1 Announce Type: new 
Abstract: Serious traffic congestion and emission by excessive usage of private cars are crucial issues in our modern society. As one solution for these, a concept of Park-and-Ride (PnR) where people stop their private cars (i.e. single-occupancy vehicles) at stations and ride on public vehicles (i.e. mass transportation) are receiving wide attention recently. In this paper, we propose a comprehensive mathematical model which can evaluate waiting times and traveling times of customers, and the total emission of vehicles for various usage ratio of PnR and operation policies of public transportation. Using a system of queues integrated with an emissions model we perform a case-study of Tsukuba city, in Japan. We indicate an intriguing trade-off between the waiting time of customers for the PnR and the long traveling time due to the traffic congestion (leading to high emissions) caused by private cars depending on the usage ratio through some numerical experiments. Moreover, we study the total cost to society caused by total trip times and pollution, in which the decision variables are the capacities and frequencies of the public transportation for the PnR system. Our numerical results showed a significant reduction in the total social cost under the optimal transit policy for the current high usage rate of single-occupancy vehicles. Furthermore, we show that further reduction in the total social cost can be revealed by considering the reduction on the use of private cars compared to the current state implying the social importance of promoting car-free movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05572v1</guid>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayane Nakamura, Fabiana Ferracina, Naoki Sakata, Takahiro Noguchi, Hiroyasu Ando</dc:creator>
    </item>
    <item>
      <title>Kinetic Interacting Particle Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2407.05790</link>
      <description>arXiv:2407.05790v1 Announce Type: new 
Abstract: This paper introduces and analyses interacting underdamped Langevin algorithms, termed Kinetic Interacting Particle Langevin Monte Carlo (KIPLMC) methods, for statistical inference in latent variable models. We propose a diffusion process that evolves jointly in the space of parameters and latent variables and exploit the fact that the stationary distribution of this diffusion concentrates around the maximum marginal likelihood estimate of the parameters. We then provide two explicit discretisations of this diffusion as practical algorithms to estimate parameters of statistical models. For each algorithm, we obtain nonasymptotic rates of convergence for the case where the joint log-likelihood is strongly concave with respect to latent variables and parameters. In particular, we provide convergence analysis for the diffusion together with the discretisation error, providing convergence rate estimates for the algorithms in Wasserstein-2 distance. To demonstrate the utility of the introduced methodology, we provide numerical experiments that demonstrate the effectiveness of the proposed diffusion for statistical inference and the stability of the numerical integrators utilised for discretisation. Our setting covers a broad number of applications, including unsupervised learning, statistical inference, and inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05790v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Felix Valsecchi Oliva, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>Constructing Level Sets Using Smoothed Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2407.05914</link>
      <description>arXiv:2407.05914v1 Announce Type: cross 
Abstract: This paper presents a novel approach to level set estimation for any function/simulation with an arbitrary number of continuous inputs and arbitrary numbers of continuous responses. We present a method that uses existing data from computer model simulations to fit a Gaussian process surrogate and use a newly proposed Markov Chain Monte Carlo technique, which we refer to as Smoothed Approximate Bayesian Computation to sample sets of parameters that yield a desired response, which improves on ``hard-clipped" versions of ABC. We prove that our method converges to the correct distribution (i.e. the posterior distribution of level sets, or probability contours) and give results of our method on known functions and a dam breach simulation where the relationship between input parameters and responses of interest is unknown. Two versions of S-ABC are offered based on: 1) surrogating an accurately known target model and 2) surrogating an approximate model, which leads to uncertainty in estimating the level sets. In addition, we show how our method can be extended to multiple responses with an accompanying example. As demonstrated, S-ABC is able to estimate a level set accurately without the use of a predefined grid or signed distance function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05914v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Edwards, Julie Bessac, Franck Cappello, Scotland Leman</dc:creator>
    </item>
  </channel>
</rss>
