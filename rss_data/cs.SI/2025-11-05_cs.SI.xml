<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 05:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Unified Model for Human Mobility Generation in Natural Disasters</title>
      <link>https://arxiv.org/abs/2511.01928</link>
      <description>arXiv:2511.01928v1 Announce Type: new 
Abstract: Human mobility generation in disaster scenarios plays a vital role in resource allocation, emergency response, and rescue coordination. During disasters such as wildfires and hurricanes, human mobility patterns often deviate from their normal states, which makes the task more challenging. However, existing works usually rely on limited data from a single city or specific disaster, significantly restricting the model's generalization capability in new scenarios. In fact, disasters are highly sudden and unpredictable, and any city may encounter new types of disasters without prior experience. Therefore, we aim to develop a one-for-all model for mobility generation that can generalize to new disaster scenarios. However, building a universal framework faces two key challenges: 1) the diversity of disaster types and 2) the heterogeneity among different cities. In this work, we propose a unified model for human mobility generation in natural disasters (named UniDisMob). To enable cross-disaster generalization, we design physics-informed prompt and physics-guided alignment that leverage the underlying common patterns in mobility changes after different disasters to guide the generation process. To achieve cross-city generalization, we introduce a meta-learning framework that extracts universal patterns across multiple cities through shared parameters and captures city-specific features via private parameters. Extensive experiments across multiple cities and disaster scenarios demonstrate that our method significantly outperforms state-of-the-art baselines, achieving an average performance improvement exceeding 13%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01928v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyue Long, Huandong Wang, Qi Ryan Wang, Yong Li</dc:creator>
    </item>
    <item>
      <title>Community Notes are Vulnerable to Rater Bias and Manipulation</title>
      <link>https://arxiv.org/abs/2511.02615</link>
      <description>arXiv:2511.02615v1 Announce Type: new 
Abstract: Social media platforms increasingly rely on crowdsourced moderation systems like Community Notes to combat misinformation at scale. However, these systems face challenges from rater bias and potential manipulation, which may undermine their effectiveness. Here we systematically evaluate the Community Notes algorithm using simulated data that models realistic rater and note behaviors, quantifying error rates in publishing helpful versus unhelpful notes. We find that the algorithm suppresses a substantial fraction of genuinely helpful notes and is highly sensitive to rater biases, including polarization and in-group preferences. Moreover, a small minority (5--20\%) of bad raters can strategically suppress targeted helpful notes, effectively censoring reliable information. These findings suggest that while community-driven moderation may offer scalability, its vulnerability to bias and manipulation raises concerns about reliability and trustworthiness, highlighting the need for improved mechanisms to safeguard the integrity of crowdsourced fact-checking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02615v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bao Tran Truong, Siqi Wu, Alessandro Flammini, Filippo Menczer, Alexander J. Stewart</dc:creator>
    </item>
    <item>
      <title>Feedback dynamics in Politics: The interplay between sentiment and engagement</title>
      <link>https://arxiv.org/abs/2511.02663</link>
      <description>arXiv:2511.02663v1 Announce Type: new 
Abstract: We investigate feedback mechanisms in political communication by testing whether politicians adapt the sentiment of their messages in response to public engagement. Using over 1.5 million tweets from Members of Parliament in the United Kingdom, Spain, and Greece during 2021, we identify sentiment dynamics through a simple yet interpretable linear model. The analysis reveals a closed-loop behavior: engagement with positive and negative messages influences the sentiment of subsequent posts. Moreover, the learned coefficients highlight systematic differences across political roles: opposition members are more reactive to negative engagement, whereas government officials respond more to positive signals. These results provide a quantitative, control-oriented view of behavioral adaptation in online politics, showing how feedback principles can explain the self-reinforcing dynamics that emerge in social media discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02663v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Formentin</dc:creator>
    </item>
    <item>
      <title>Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance</title>
      <link>https://arxiv.org/abs/2412.00621</link>
      <description>arXiv:2412.00621v1 Announce Type: cross 
Abstract: Can we trust Large Language Models (LLMs) to accurately predict scam? This paper investigates the vulnerabilities of LLMs when facing adversarial scam messages for the task of scam detection. We addressed this issue by creating a comprehensive dataset with fine-grained labels of scam messages, including both original and adversarial scam messages. The dataset extended traditional binary classes for the scam detection task into more nuanced scam types. Our analysis showed how adversarial examples took advantage of vulnerabilities of a LLM, leading to high misclassification rate. We evaluated the performance of LLMs on these adversarial scam messages and proposed strategies to improve their robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00621v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen-Wei Chang, Shailik Sarkar, Shutonu Mitra, Qi Zhang, Hossein Salemi, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, Chang-Tien Lu</dc:creator>
    </item>
    <item>
      <title>Story and essential meaning dynamics in Bangladesh's July 2024 Student-People's Uprising</title>
      <link>https://arxiv.org/abs/2511.01865</link>
      <description>arXiv:2511.01865v1 Announce Type: cross 
Abstract: News media serves a crucial role in disseminating information and shaping public perception, especially during periods of political unrest. Using over 50,0000 YouTube comments on news coverage from July 16 to August 6, 2024, we investigate the emotional dynamics and evolving discourse of public perception during the July 2024 Student-People's Uprising in Bangladesh. Through integrated analyses of sentiment, emotion, topic, lexical discourse, timeline progression, sentiment shifts, and allotaxonometry, we show how negative sentiment dominated during the movement. We find a negative correlation between comment happiness and number of protest deaths $(r = -0.45,\p = 0.00)$. Using an ousiometer to measure essential meaning, we find public responses reflect a landscape of power, aggression, and danger, alongside persistent expressions of hope, moral conviction, and empowerment through goodnesses. Topic discourse progressed during the movement, with peaks in `Political Conflict', `Media Flow', and `Student Violence' during crisis surges, while topics like `Social Resistance' and `Digital Movement' persisted amid repression. Sentiment shifts reveal that after the second internet blackout, average happiness increased, driven by the more frequent use of positive words such as `victory', `peace' and `freedom' and a decrease in negative terms such as `death' and `lies'. Finally, through allotaxonometric analysis, we observe a clear shift from protest to justice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01865v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tabia Tanzin Prama, Christopher M. Danforth, Peter Sheridan Dodds</dc:creator>
    </item>
    <item>
      <title>Expertise and confidence explain how social influence evolves along intellective tasks</title>
      <link>https://arxiv.org/abs/2011.07168</link>
      <description>arXiv:2011.07168v2 Announce Type: replace 
Abstract: Discovering the antecedents of individuals' influence in collaborative environments is an important, practical, and challenging problem. In this paper, we study interpersonal influence in small groups of individuals who collectively execute a sequence of intellective tasks. We observe that along an issue sequence with feedback, individuals with higher expertise and social confidence are accorded higher interpersonal influence. We also observe that low-performing individuals tend to underestimate their high-performing teammate's expertise. Based on these observations, we introduce three hypotheses and present empirical and theoretical support for their validity. We report empirical evidence on longstanding theories of transactive memory systems, social comparison, and confidence heuristics on the origins of social influence. We propose a cognitive dynamical model inspired by these theories to describe the process by which individuals adjust interpersonal influences over time. We demonstrate the model's accuracy in predicting individuals' influence and provide analytical results on its asymptotic behavior for the case with identically performing individuals. Lastly, we propose a novel approach using deep neural networks on a pre-trained text embedding model for predicting the influence of individuals. Using message contents, message times, and individual correctness collected during tasks, we are able to accurately predict individuals' self-reported influence over time. Extensive experiments verify the accuracy of the proposed models compared to baselines such as structural balance and reflected appraisal model. While the neural networks model is the most accurate, the dynamical model is the most interpretable for influence prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.07168v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omid Askarisichani, Elizabeth Y. Huang, Abed K. Musaffar, Noah E. Friedkin, Francesco Bullo, Ambuj K. Singh</dc:creator>
    </item>
    <item>
      <title>A Behavioural Analysis of Credulous Twitter Users</title>
      <link>https://arxiv.org/abs/2101.10782</link>
      <description>arXiv:2101.10782v2 Announce Type: replace 
Abstract: Thanks to platforms such as Twitter and Facebook, people can know facts and events that otherwise would have been silenced. However, social media significantly contribute also to fast spreading biased and false news while targeting specific segments of the population. We have seen how false information can be spread using automated accounts, known as bots. Using Twitter as a benchmark, we investigate behavioural attitudes of so called `credulous' users, i.e., genuine accounts following many bots. Leveraging our previous work, where supervised learning is successfully applied to single out credulous users, we improve the classification task with a detailed features' analysis and provide evidence that simple and lightweight features are crucial to detect such users. Furthermore, we study the differences in the way credulous and not credulous users interact with bots and discover that credulous users tend to amplify more the content posted by bots and argue that their detection can be instrumental to get useful information on possible dissemination of spam content, propaganda, and, in general, little or no reliable information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.10782v2</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Balestrucci, Rocco De Nicola, Marinella Petrocchi, Catia Trubiani</dc:creator>
    </item>
    <item>
      <title>Language-Agnostic Modeling of Source Reliability on Wikipedia</title>
      <link>https://arxiv.org/abs/2410.18803</link>
      <description>arXiv:2410.18803v3 Announce Type: replace 
Abstract: Over the last few years, verifying the credibility of information sources has become a fundamental need to combat disinformation. Here, we present a language-agnostic model designed to assess the reliability of web domains as sources in references across multiple language editions of Wikipedia. Utilizing editing activity data, the model evaluates domain reliability within different articles of varying controversiality, such as Climate Change, COVID-19, History, Media, and Biology topics. Crafting features that express domain usage across articles, the model effectively predicts domain reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages. For mid-resource languages, we achieve 0.65, while the performance of low-resource languages varies. In all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features. We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance. We believe these findings can assist Wikipedia editors in their ongoing efforts to verify citations and may offer useful insights for other user-generated content communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18803v3</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beir\'o, Pablo Arag\'on</dc:creator>
    </item>
    <item>
      <title>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</title>
      <link>https://arxiv.org/abs/2507.13370</link>
      <description>arXiv:2507.13370v3 Announce Type: replace 
Abstract: The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13370v3</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijun Guo, Haoran Xu, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyi Zhang, Yishan Song</dc:creator>
    </item>
    <item>
      <title>Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions</title>
      <link>https://arxiv.org/abs/2510.27195</link>
      <description>arXiv:2510.27195v2 Announce Type: replace-cross 
Abstract: As AI systems become increasingly integrated into human lives, endowing them with robust social intelligence has emerged as a critical frontier. A key aspect of this intelligence is discerning truth from deception, a ubiquitous element of human interaction that is conveyed through a complex interplay of verbal language and non-verbal visual cues. However, automatic deception detection in dynamic, multi-party conversations remains a significant challenge. The recent rise of powerful Multimodal Large Language Models (MLLMs), with their impressive abilities in visual and textual understanding, makes them natural candidates for this task. Consequently, their capabilities in this crucial domain are mostly unquantified. To address this gap, we introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and present a novel multimodal dataset derived from the social deduction game Werewolf. This dataset provides synchronized video, text, with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating state-of-the-art MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to ground language in visual social cues effectively and may be overly conservative in their alignment, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27195v2</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caixin Kang, Yifei Huang, Liangyang Ouyang, Mingfang Zhang, Yoichi Sato</dc:creator>
    </item>
  </channel>
</rss>
