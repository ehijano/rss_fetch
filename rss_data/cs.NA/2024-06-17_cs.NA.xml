<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An hp-Adaptive Sampling Algorithm for Dispersion Relation Reconstruction of 3D Photonic Crystals</title>
      <link>https://arxiv.org/abs/2406.10523</link>
      <description>arXiv:2406.10523v1 Announce Type: new 
Abstract: In this work we investigate the computation of dispersion relation (i.e., band functions) for three-dimensional photonic crystals, formulated as a parameterized Maxwell eigenvalue problem, using a novel hp-adaptive sampling algorithm. We develop an adaptive sampling algorithm in the parameter domain such that local elements with singular points are refined at each iteration, construct a conforming element-wise polynomial space on the adaptive mesh such that the distribution of the local polynomial spaces reflects the regularity of the band functions, and define an element-wise Lagrange interpolation operator to approximate the band functions. We rigorously prove the convergence of the algorithm. To illustrate the significant potential of the algorithm, we present two numerical tests with band gap optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10523v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Wang, Richard Craster, Guanglian Li</dc:creator>
    </item>
    <item>
      <title>A simple and fast finite difference method for the integral fractional Laplacian of variable order</title>
      <link>https://arxiv.org/abs/2406.10524</link>
      <description>arXiv:2406.10524v1 Announce Type: new 
Abstract: For the fractional Laplacian of variable order, an efficient and accurate numerical evaluation in multi-dimension is a challenge for the nature of a singular integral. We propose a simple and easy-to-implement finite difference scheme for the multi-dimensional variable-order fractional Laplacian defined by a hypersingular integral. We prove that the scheme is of second-order convergence and apply the developed finite difference scheme to solve various equations with the variable-order fractional Laplacian. We present a fast solver with quasi-linear complexity of the scheme for computing variable-order fractional Laplacian and corresponding PDEs. Several numerical examples demonstrate the accuracy and efficiency of our algorithm and verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10524v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaopeng Hao, Siyuan Shi, Zhongqiang Zhang, Rui Du</dc:creator>
    </item>
    <item>
      <title>Strong convergence rates for long-time approximations of SDEs with non-globally Lipschitz continuous coefficients</title>
      <link>https://arxiv.org/abs/2406.10582</link>
      <description>arXiv:2406.10582v1 Announce Type: new 
Abstract: This paper is concerned with long-time strong approximations of SDEs with non-globally Lipschitz coefficients.Under certain non-globally Lipschitz conditions, a long-time version of fundamental strong convergence theorem is established for general one-step time discretization schemes. With the aid of the fundamental strong convergence theorem, we prove the expected strong convergence rate over infinite time for two types of schemes such as the backward Euler method and the projected Euler method in non-globally Lipschitz settings. Numerical examples are finally reported to confirm our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10582v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoming Wu, Xiaojie Wang</dc:creator>
    </item>
    <item>
      <title>Polytopal mesh agglomeration via geometrical deep learning for three-dimensional heterogeneous domains</title>
      <link>https://arxiv.org/abs/2406.10587</link>
      <description>arXiv:2406.10587v1 Announce Type: new 
Abstract: Agglomeration techniques are important to reduce the computational costs of numerical simulations and stand at the basis of multilevel algebraic solvers. To automatically perform the agglomeration of polyhedral grids, we propose a novel Geometrical Deep Learning-based algorithm that can exploit the geometrical and physical information of the underlying computational domain to construct the agglomerated grid and simultaneously guarantee the agglomerated grid's quality. In particular, we propose a bisection model based on Graph Neural Networks (GNNs) to partition a suitable connectivity graph of computational three-dimensional meshes. The new approach has a high online inference speed and can simultaneously process the graph structure of the mesh, the geometrical information of the mesh (e.g. elements' volumes, centers' coordinates), and the physical information of the domain (e.g. physical parameters). Taking advantage of this new approach, our algorithm can agglomerate meshes of a domain composed of heterogeneous media in an automatic way. The proposed GNN techniques are compared with the k-means algorithm and METIS: standard approaches for graph partitioning that are meant to process only the connectivity information on the mesh. We demonstrate that the performance of our algorithms outperforms available approaches in terms of quality metrics and runtimes. Moreover, we demonstrate that our algorithm also shows a good level of generalization when applied to more complex geometries, such as three-dimensional geometries reconstructed from medical images. Finally, the capabilities of the model in performing agglomeration of heterogeneous domains are tested in the framework of problems containing microstructures and on a complex geometry such as the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10587v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paola F. Antonietti, Mattia Corti, Gabriele Martinelli</dc:creator>
    </item>
    <item>
      <title>Finite-difference least square methods for solving Hamilton-Jacobi equations using neural networks</title>
      <link>https://arxiv.org/abs/2406.10758</link>
      <description>arXiv:2406.10758v1 Announce Type: new 
Abstract: We present a simple algorithm to approximate the viscosity solution of Hamilton-Jacobi~(HJ) equations by means of an artificial deep neural network. The algorithm uses a stochastic gradient descent-based algorithm to minimize the least square principle defined by a monotone, consistent numerical scheme. We analyze the least square principle's critical points and derive conditions that guarantee that any critical point approximates the sought viscosity solution. The use of a deep artificial neural network on a finite difference scheme lifts the restriction of conventional finite difference methods that rely on computing functions on a fixed grid. This feature makes it possible to solve HJ equations posed in higher dimensions where conventional methods are infeasible. We demonstrate the efficacy of our algorithm through numerical studies on various canonical HJ equations across different dimensions, showcasing its potential and versatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10758v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Esteve-Yag\"ue, Richard Tsai, Alex Massucco</dc:creator>
    </item>
    <item>
      <title>A note on best n-term approximation for generalized Wiener classes</title>
      <link>https://arxiv.org/abs/2406.10761</link>
      <description>arXiv:2406.10761v1 Announce Type: new 
Abstract: We determine the best n-term approximation of generalized Wiener model classes in a Hilbert space $H $. This theory is then applied to several special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10761v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald DeVore, Guergana Petrova, Przemyslaw Wojtaszczyk</dc:creator>
    </item>
    <item>
      <title>Column reduced digital nets</title>
      <link>https://arxiv.org/abs/2406.10850</link>
      <description>arXiv:2406.10850v1 Announce Type: new 
Abstract: Digital nets provide an efficient way to generate integration nodes of quasi-Monte Carlo (QMC) rules. For certain applications, as e.g. in Uncertainty Quantification, we are interested in obtaining a speed-up in computing products of a matrix with the vectors corresponding to the nodes of a QMC rule. In the recent paper "The fast reduced QMC matrix-vector product" (J. Comput. Appl. Math. 440, 115642, 2024), a speed up was obtained by using so-called reduced lattices and row reduced digital nets. In this work, we propose a different multiplication algorithm where we exploit the repetitive structure of column reduced digital nets instead of row reduced digital nets. This method has advantages over the previous one, as it facilitates the error analysis when using the integration nodes in a QMC rule. We also provide an upper bound for the quality parameter of column reduced digital nets, and numerical tests to illustrate the efficiency of the new algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10850v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishnupriya Anupindi, Peter Kritzer</dc:creator>
    </item>
    <item>
      <title>Two-level overlapping additive Schwarz preconditioner for training scientific machine learning applications</title>
      <link>https://arxiv.org/abs/2406.10997</link>
      <description>arXiv:2406.10997v1 Announce Type: new 
Abstract: We introduce a novel two-level overlapping additive Schwarz preconditioner for accelerating the training of scientific machine learning applications. The design of the proposed preconditioner is motivated by the nonlinear two-level overlapping additive Schwarz preconditioner. The neural network parameters are decomposed into groups (subdomains) with overlapping regions. In addition, the network's feed-forward structure is indirectly imposed through a novel subdomain-wise synchronization strategy and a coarse-level training step. Through a series of numerical experiments, which consider physics-informed neural networks and operator learning approaches, we demonstrate that the proposed two-level preconditioner significantly speeds up the convergence of the standard (LBFGS) optimizer while also yielding more accurate machine learning models. Moreover, the devised preconditioner is designed to take advantage of model-parallel computations, which can further reduce the training time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10997v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Youngkyu Lee, Alena Kopani\v{c}\'akov\'a, George Em Karniadakis</dc:creator>
    </item>
    <item>
      <title>Nested finite element approximation of parabolic SPDEs with Whittle-Mat\'ern noise</title>
      <link>https://arxiv.org/abs/2406.11041</link>
      <description>arXiv:2406.11041v1 Announce Type: new 
Abstract: We propose a new type of fully discrete finite element approximation of a class of semilinear stochastic parabolic equations with additive noise. Our discretization differs from the ones typically considered, in that we employ a nested finite element approximation of the noise. This is well suited for dealing with covariance operators defined in terms of (negative powers of) elliptic operators, like that of Whittle-Mat\'ern random fields. We derive strong and pathwise convergence rates for our proposed discretization, and our results are supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11041v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\O}yvind Stormark Auestad</dc:creator>
    </item>
    <item>
      <title>A MATLAB package computing simultaneous Gaussian quadrature rules for Multiple Orthogonal Polynomials</title>
      <link>https://arxiv.org/abs/2406.11269</link>
      <description>arXiv:2406.11269v1 Announce Type: new 
Abstract: The aim of this paper is to describe a Matlab package for computing the simultaneous Gaussian quadrature rules associated with a variety of multiple orthogonal polynomials. Multiple orthogonal polynomials can be considered as a generalization of classical orthogonal polynomials, satisfying orthogonality constraints with respect to $r$ different measures, with $r \ge 1$. Moreover, they satisfy $(r+2)$--term recurrence relations. In this manuscript, without loss of generality, $r$ is considered equal to $2$. The so-called simultaneous Gaussian quadrature rules associated with multiple orthogonal polynomials can be computed by solving a banded lower Hessenberg eigenvalue problem. Unfortunately, computing the eigendecomposition of such a matrix turns out to be strongly ill-conditioned and the \texttt{Matlab} function \texttt{balance.m} does not improve the condition of the eigenvalue problem. Therefore, most procedures for computing simultaneous Gaussian quadrature rules are implemented with variable precision arithmetic. Here, we propose a \texttt{Matlab} package that allows to reliably compute the simultaneous Gaussian quadrature rules in floating point arithmetic. It makes use of a variant of a new balancing procedure, recently developed by the authors of the present manuscript, that drastically reduces the condition of the Hessenberg eigenvalue problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11269v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Laudadio, Nicola Mastronardi, Walter Van Assche, Paul Van Dooren</dc:creator>
    </item>
    <item>
      <title>A posteriori error estimation for an interior penalty virtual element method of Kirchhoff plates</title>
      <link>https://arxiv.org/abs/2406.11411</link>
      <description>arXiv:2406.11411v1 Announce Type: new 
Abstract: A residual-type a posteriori error estimation is developed for an interior penalty virtual element method (IPVEM) to solve a Kirchhoff plate bending problem. The computable error estimator is incorporated. We derive the reliability and efficiency of the a posteriori error bound by constructing an enriching operator and establishing some related error estimates. As an outcome of the error estimator, an adaptive VEM is introduced by means of the mesh refinement strategy with the one-hanging-node rule. Numerical results on various benchmark tests confirm the robustness of the proposed error estimator and show the efficiency of the resulting adaptive VEM. (This is the initial version; additional content will be included in the final version.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11411v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Feng, Yue Yu</dc:creator>
    </item>
    <item>
      <title>Sparse approximations for contact mechanics</title>
      <link>https://arxiv.org/abs/2406.11461</link>
      <description>arXiv:2406.11461v1 Announce Type: new 
Abstract: Low-rank model order reduction strategies for contact mechanics show limited dimensionality reduction due to linear inseparability of contact pressure field. Therefore, a dictionary based strategy is explored for creating efficient models for frictionless non-adhesive contact. A large dictionary of contact pressure trajectories is generated using a high-fidelity finite element model, while approximating the online query with a small number of dictionary entries. This is achieved by inducing sparsity in the approximation. Accuracy, computational effort and limitations of such methods are demonstrated on few numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11461v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kiran Sagar Kollepara, Jos\'e V. Aguado, Yves Le Guennec, Luisa Silva, Domenico Borzacchiello</dc:creator>
    </item>
    <item>
      <title>Non-negative Einstein tensor factorization for unmixing hyperspectral images</title>
      <link>https://arxiv.org/abs/2406.11471</link>
      <description>arXiv:2406.11471v1 Announce Type: new 
Abstract: In this manuscript, we introduce a tensor-based approach to Non-Negative Tensor Factorization (NTF). The method entails tensor dimension reduction through the utilization of the Einstein product. To maintain the regularity and sparsity of the data, certain constraints are imposed. Additionally, we present an optimization algorithm in the form of a tensor multiplicative updates method, which relies on the Einstein product. To guarantee a minimum number of iterations for the convergence of the proposed algorithm, we employ the Reduced Rank Extrapolation (RRE) and the Topological Extrapolation Transformation Algorithm (TEA). The efficacy of the proposed model is demonstrated through tests conducted on Hyperspectral Images (HI) for denoising, as well as for Hyperspectral Image Linear Unmixing. Numerical experiments are provided to substantiate the effectiveness of the proposed model for both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11471v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Anas El Hachimi, Khalide Jbilou, Ahmed Ratnani</dc:creator>
    </item>
    <item>
      <title>Polygonal Faber-Krahn inequality: Local minimality via validated computing</title>
      <link>https://arxiv.org/abs/2406.11575</link>
      <description>arXiv:2406.11575v1 Announce Type: new 
Abstract: The main result of the paper shows that the regular $n$-gon is a local minimizer for the first Dirichlet-Laplace eigenvalue among $n$-gons having fixed area for $n \in \{5,6\}$. The eigenvalue is seen as a function of the coordinates of the vertices in $\Bbb R^{2n}$. Relying on fine regularity results of the first eigenfunction in a convex polygon, an explicit a priori estimate is given for the eigenvalues of the Hessian matrix associated to the discrete problem, whose coefficients involve the solutions of some Poisson equations with singular right hand sides. The a priori estimates, in conjunction with certified finite element approximations of these singular PDEs imply the local minimality for $n \in \{5,6\}$. All computations, including the finite element computations, are realized using interval arithmetic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11575v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Beniamin Bogosel, Dorin Bucur</dc:creator>
    </item>
    <item>
      <title>Tackling the Curse of Dimensionality in Fractional and Tempered Fractional PDEs with Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2406.11708</link>
      <description>arXiv:2406.11708v1 Announce Type: new 
Abstract: Fractional and tempered fractional partial differential equations (PDEs) are effective models of long-range interactions, anomalous diffusion, and non-local effects. Traditional numerical methods for these problems are mesh-based, thus struggling with the curse of dimensionality (CoD). Physics-informed neural networks (PINNs) offer a promising solution due to their universal approximation, generalization ability, and mesh-free training. In principle, Monte Carlo fractional PINN (MC-fPINN) estimates fractional derivatives using Monte Carlo methods and thus could lift CoD. However, this may cause significant variance and errors, hence affecting convergence; in addition, MC-fPINN is sensitive to hyperparameters. In general, numerical methods and specifically PINNs for tempered fractional PDEs are under-developed. Herein, we extend MC-fPINN to tempered fractional PDEs to address these issues, resulting in the Monte Carlo tempered fractional PINN (MC-tfPINN). To reduce possible high variance and errors from Monte Carlo sampling, we replace the one-dimensional (1D) Monte Carlo with 1D Gaussian quadrature, applicable to both MC-fPINN and MC-tfPINN. We validate our methods on various forward and inverse problems of fractional and tempered fractional PDEs, scaling up to 100,000 dimensions. Our improved MC-fPINN/MC-tfPINN using quadrature consistently outperforms the original versions in accuracy and convergence speed in very high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11708v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheyuan Hu, Kenji Kawaguchi, Zhongqiang Zhang, George Em Karniadakis</dc:creator>
    </item>
    <item>
      <title>A randomized preconditioned Cholesky-QR algorithm</title>
      <link>https://arxiv.org/abs/2406.11751</link>
      <description>arXiv:2406.11751v1 Announce Type: new 
Abstract: We a present and analyze rpCholesky-QR, a randomized preconditioned Cholesky-QR algorithm for computing the thin QR factorization of real mxn matrices with rank n. rpCholesky-QR has a low orthogonalization error, a residual on the order of machine precision, and does not break down for highly singular matrices. We derive rigorous and interpretable two-norm perturbation bounds for rpCholesky-QR that require a minimum of assumptions. Numerical experiments corroborate the accuracy of rpCholesky-QR for preconditioners sampled from as few as 3n rows, and illustrate that the two-norm deviation from orthonormality increases with only the condition number of the preconditioned matrix, rather than its square -- even if the original matrix is numerically singular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11751v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James E. Garrsion, Ilse C. F. Ipsen</dc:creator>
    </item>
    <item>
      <title>Unified analysis of algorithms for equilibrium, non-equilibrium, and hysteresis models of phase transition in permafrost</title>
      <link>https://arxiv.org/abs/2406.11812</link>
      <description>arXiv:2406.11812v1 Announce Type: new 
Abstract: In this paper we consider a nonlinear partial differential equation describing heat flow with ice-water phase transition in permafrost soils. Such models and their numerical approximations have been well explored in the applications literature. In this paper we describe a new direction in which the allow relaxation and hysteresis of the phase transition which introduce additional nonlinear terms and complications for the analysis. We present numerical algorithms as well as analysis of the well-posedness and convergence of the fully implicit iterative schemes. The analysis we propose handles the equilibrium, non-equilibrium, and hysteresis cases in a unified way.
  We also illustrate with numerical examples for a model ODE and PDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11812v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Malgorzata Peszynska, Nicholas Slugg</dc:creator>
    </item>
    <item>
      <title>Suboptimality bounds for trace-bounded SDPs enable a faster and scalable low-rank SDP solver SDPLR+</title>
      <link>https://arxiv.org/abs/2406.10407</link>
      <description>arXiv:2406.10407v1 Announce Type: cross 
Abstract: Semidefinite programs (SDPs) and their solvers are powerful tools with many applications in machine learning and data science. Designing scalable SDP solvers is challenging because by standard the positive semidefinite decision variable is an $n \times n$ dense matrix, even though the input is often an $n \times n$ sparse matrix. However, the information in the solution may not correspond to a full-rank dense matrix as shown by Bavinok and Pataki. Two decades ago, Burer and Monterio developed an SDP solver $\texttt{SDPLR}$ that optimizes over a low-rank factorization instead of the full matrix. This greatly decreases the storage cost and works well for many problems. The original solver $\texttt{SDPLR}$ tracks only the primal infeasibility of the solution, limiting the technique's flexibility to produce moderate accuracy solutions. We use a suboptimality bound for trace-bounded SDP problems that enables us to track the progress better and perform early termination. We then develop $\texttt{SDPLR+}$, which starts the optimization with an extremely low-rank factorization and dynamically updates the rank based on the primal infeasibility and suboptimality. This further speeds up the computation and saves the storage cost. Numerical experiments on Max Cut, Minimum Bisection, Cut Norm, and Lov\'{a}sz Theta problems with many recent memory-efficient scalable SDP solvers demonstrate its scalability up to problems with million-by-million decision variables and it is often the fastest solver to a moderate accuracy of $10^{-2}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10407v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Huang, David F. Gleich</dc:creator>
    </item>
    <item>
      <title>High-Performance Hardware Accelerator with Medium Granularity Dataflow for SpTRSV</title>
      <link>https://arxiv.org/abs/2406.10511</link>
      <description>arXiv:2406.10511v1 Announce Type: cross 
Abstract: Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous studies have been conducted using CPUs, GPUs, and specific hardware accelerators, where dataflow can be categorized into coarse and fine granularity. Coarse dataflow offers good spatial locality but suffers from low parallelism, while fine dataflow provides high parallelism but disrupts the spatial structure, leading to increased nodes and poor data reuse. This paper proposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAG. The accelerator implements a medium granularity dataflow through hardware-software codesign and achieves both excellent spatial locality and high parallelism. Additionally, a partial sum caching mechanism is introduced to reduce the blocking frequency of processing elements (PEs), and a reordering algorithm of intra-node edges computation is developed to enhance data reuse. Experimental results on 264 benchmarks with node counts reaching up to 85,392 demonstrate that this work achieves average performance improvements of 12.2x (up to 874.5x) over CPU and 10.1x (up to 740.4x) over GPU. Compared to the state-of-the-art technique (DPU-v2), this work shows a 2.5x (up to 5.9x) average performance improvement and 1.8x (up to 4.1x) average energy efficiency enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10511v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Chen, Xiaofeng Yang, Shengli Lu</dc:creator>
    </item>
    <item>
      <title>Analysis and approximation of elliptic problems with Uhlenbeck structure in convex polytopes</title>
      <link>https://arxiv.org/abs/2406.10762</link>
      <description>arXiv:2406.10762v1 Announce Type: cross 
Abstract: We prove the well posedness in weighted Sobolev spaces of certain linear and nonlinear elliptic boundary value problems posed on convex domains and under singular forcing. It is assumed that the weights belong to the Muckenhoupt class $A_p$ with $p \in (1,\infty$). We also propose and analyze a convergent finite element discretization for the nonlinear elliptic boundary value problems mentioned above. As an instrumental result, we prove that the discretization of certain linear problems are well posed in weighted spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10762v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tadele Mengesha, Enrique Otarola, Abner J. Salgado</dc:creator>
    </item>
    <item>
      <title>Deep neural networks with ReLU, leaky ReLU, and softplus activation provably overcome the curse of dimensionality for space-time solutions of semilinear partial differential equations</title>
      <link>https://arxiv.org/abs/2406.10876</link>
      <description>arXiv:2406.10876v1 Announce Type: cross 
Abstract: It is a challenging topic in applied mathematics to solve high-dimensional nonlinear partial differential equations (PDEs). Standard approximation methods for nonlinear PDEs suffer under the curse of dimensionality (COD) in the sense that the number of computational operations of the approximation method grows at least exponentially in the PDE dimension and with such methods it is essentially impossible to approximately solve high-dimensional PDEs even when the fastest currently available computers are used. However, in the last years great progress has been made in this area of research through suitable deep learning (DL) based methods for PDEs in which deep neural networks (DNNs) are used to approximate solutions of PDEs. Despite the remarkable success of such DL methods in simulations, it remains a fundamental open problem of research to prove (or disprove) that such methods can overcome the COD in the approximation of PDEs. However, there are nowadays several partial error analysis results for DL methods for high-dimensional nonlinear PDEs in the literature which prove that DNNs can overcome the COD in the sense that the number of parameters of the approximating DNN grows at most polynomially in both the reciprocal of the prescribed approximation accuracy $\varepsilon&gt;0$ and the PDE dimension $d\in\mathbb{N}$. In the main result of this article we prove that for all $T,p\in(0,\infty)$ it holds that solutions $u_d\colon[0,T]\times\mathbb{R}^d\to\mathbb{R}$, $d\in\mathbb{N}$, of semilinear heat equations with Lipschitz continuous nonlinearities can be approximated in the $L^p$-sense on space-time regions without the COD by DNNs with the rectified linear unit (ReLU), the leaky ReLU, or the softplus activation function. In previous articles similar results have been established not for space-time regions but for the solutions $u_d(T,\cdot)$, $d\in\mathbb{N}$, at the terminal time $T$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10876v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Ackermann, Arnulf Jentzen, Benno Kuckuck, Joshua Lee Padgett</dc:creator>
    </item>
    <item>
      <title>Hamilton-Jacobi Based Policy-Iteration via Deep Operator Learning</title>
      <link>https://arxiv.org/abs/2406.10920</link>
      <description>arXiv:2406.10920v1 Announce Type: cross 
Abstract: The framework of deep operator network (DeepONet) has been widely exploited thanks to its capability of solving high dimensional partial differential equations. In this paper, we incorporate DeepONet with a recently developed policy iteration scheme to numerically solve optimal control problems and the corresponding Hamilton--Jacobi--Bellman (HJB) equations. A notable feature of our approach is that once the neural network is trained, the solution to the optimal control problem and HJB equations with different terminal functions can be inferred quickly thanks to the unique feature of operator learning. Furthermore, a quantitative analysis of the accuracy of the algorithm is carried out via comparison principles of viscosity solutions. The effectiveness of the method is verified with various examples, including 10-dimensional linear quadratic regulator problems (LQRs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10920v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jae Yong Lee, Yeoneung Kim</dc:creator>
    </item>
    <item>
      <title>Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving PDEs based on Kolmogorov Arnold Networks</title>
      <link>https://arxiv.org/abs/2406.11045</link>
      <description>arXiv:2406.11045v1 Announce Type: cross 
Abstract: AI for partial differential equations (PDEs) has garnered significant attention, particularly with the emergence of Physics-informed neural networks (PINNs). The recent advent of Kolmogorov-Arnold Network (KAN) indicates that there is potential to revisit and enhance the previously MLP-based PINNs. Compared to MLPs, KANs offer interpretability and require fewer parameters. PDEs can be described in various forms, such as strong form, energy form, and inverse form. While mathematically equivalent, these forms are not computationally equivalent, making the exploration of different PDE formulations significant in computational physics. Thus, we propose different PDE forms based on KAN instead of MLP, termed Kolmogorov-Arnold-Informed Neural Network (KINN). We systematically compare MLP and KAN in various numerical examples of PDEs, including multi-scale, singularity, stress concentration, nonlinear hyperelasticity, heterogeneous, and complex geometry problems. Our results demonstrate that KINN significantly outperforms MLP in terms of accuracy and convergence speed for numerous PDEs in computational solid mechanics, except for the complex geometry problem. This highlights KINN's potential for more efficient and accurate PDE solutions in AI for PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11045v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizheng Wang, Jia Sun, Jinshuai Bai, Cosmin Anitescu, Mohammad Sadegh Eshaghi, Xiaoying Zhuang, Timon Rabczuk, Yinghua Liu</dc:creator>
    </item>
    <item>
      <title>Guaranteed Sampling Flexibility for Low-tubal-rank Tensor Completion</title>
      <link>https://arxiv.org/abs/2406.11092</link>
      <description>arXiv:2406.11092v1 Announce Type: cross 
Abstract: While Bernoulli sampling is extensively studied in tensor completion, t-CUR sampling approximates low-tubal-rank tensors via lateral and horizontal subtensors. However, both methods lack sufficient flexibility for diverse practical applications. To address this, we introduce Tensor Cross-Concentrated Sampling (t-CCS), a novel and straightforward sampling model that advances the matrix cross-concentrated sampling concept within a tensor framework. t-CCS effectively bridges the gap between Bernoulli and t-CUR sampling, offering additional flexibility that can lead to computational savings in various contexts. A key aspect of our work is the comprehensive theoretical analysis provided. We establish a sufficient condition for the successful recovery of a low-rank tensor from its t-CCS samples. In support of this, we also develop a theoretical framework validating the feasibility of t-CUR via uniform random sampling and conduct a detailed theoretical sampling complexity analysis for tensor completion problems utilizing the general Bernoulli sampling model. Moreover, we introduce an efficient non-convex algorithm, the Iterative t-CUR Tensor Completion (ITCURTC) algorithm, specifically designed to tackle the t-CCS-based tensor completion. We have intensively tested and validated the effectiveness of the t-CCS model and the ITCURTC algorithm across both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11092v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Su, Juntao You, HanQin Cai, Longxiu Huang</dc:creator>
    </item>
    <item>
      <title>Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning</title>
      <link>https://arxiv.org/abs/2406.11151</link>
      <description>arXiv:2406.11151v1 Announce Type: cross 
Abstract: Large matrices arise in many machine learning and data analysis applications, including as representations of datasets, graphs, model weights, and first and second-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an area which uses randomness to develop improved algorithms for ubiquitous matrix problems. The area has reached a certain level of maturity; but recent hardware trends, efforts to incorporate RandNLA algorithms into core numerical libraries, and advances in machine learning, statistics, and random matrix theory, have lead to new theoretical and practical challenges. This article provides a self-contained overview of RandNLA, in light of these developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11151v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha{\l} Derezi\'nski, Michael W. Mahoney</dc:creator>
    </item>
    <item>
      <title>A Mixed Tree-Cotree Gauge for the Reduced Basis Approximation of Maxwell's Eigenvalue Problem</title>
      <link>https://arxiv.org/abs/2406.11276</link>
      <description>arXiv:2406.11276v1 Announce Type: cross 
Abstract: Model order reduction methods are a powerful tool to drastically reduce the computational effort of problems which need to be evaluated repeatedly, i.e., when computing the same system for various parameter values. When applying a reduced basis approximation algorithm to the Maxwell eigenvalue problem, we encounter spurious solutions in the reduced system which hence need to be removed during the basis construction. In this paper, we discuss two tree-cotree gauge-based methods for the removal of the spurious eigenmodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11276v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Ziegler, Sebastian Sch\"ops</dc:creator>
    </item>
    <item>
      <title>A modified Cayley transform for SU(3)</title>
      <link>https://arxiv.org/abs/2406.11337</link>
      <description>arXiv:2406.11337v1 Announce Type: cross 
Abstract: We propose a modification to the Cayley transform that defines a suitable local parameterization for the special unitary group $\mathrm{SU(3)}$. The new mapping is used to construct splitting methods for separable Hamiltonian systems whose phase space is the cotangent bundle of $\mathrm{SU(3)}$ or, more general, $\mathrm{SU(3)}^N$, $N \in \mathbb{N}$. Special attention is given to the hybrid Monte Carlo algorithm for gauge field generation in lattice quantum chromodynamics. We show that the use of the modified Cayley transform instead of the matrix exponential neither affects the time-reversibility nor the volume-preservation of the splitting method. Furthermore, the advantages and disadvantages of the Cayley-based algorithms are discussed and illustrated in pure gauge field simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11337v1</guid>
      <category>hep-lat</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin Sch\"afers, Michael Peardon, Michael G\"unther</dc:creator>
    </item>
    <item>
      <title>A parallel in time algorithm based ParaExp for optimal control problems</title>
      <link>https://arxiv.org/abs/2406.11478</link>
      <description>arXiv:2406.11478v1 Announce Type: cross 
Abstract: We propose a new parallel-in-time algorithm for solving optimal control problems constrained bypartial differential equations. Our approach, which is based on a deeper understanding of ParaExp,considers an overlapping time-domain decomposition in which we combine the solution of homogeneous problems using exponential propagation with the local solutions of inhomogeneous problems.The algorithm yields a linear system whose matrix-vector product can be fully performed in parallel.We then propose a preconditioner to speed up the convergence of GMRES in the special cases ofthe heat and wave equations. Numerical experiments are provided to illustrate the efficiency of ourpreconditioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11478v1</guid>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Kwok (ULaval), Djahou N Tognon (SU)</dc:creator>
    </item>
    <item>
      <title>Score-fPINN: Fractional Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck-Levy Equations</title>
      <link>https://arxiv.org/abs/2406.11676</link>
      <description>arXiv:2406.11676v1 Announce Type: cross 
Abstract: We introduce an innovative approach for solving high-dimensional Fokker-Planck-L\'evy (FPL) equations in modeling non-Brownian processes across disciplines such as physics, finance, and ecology. We utilize a fractional score function and Physical-informed neural networks (PINN) to lift the curse of dimensionality (CoD) and alleviate numerical overflow from exponentially decaying solutions with dimensions. The introduction of a fractional score function allows us to transform the FPL equation into a second-order partial differential equation without fractional Laplacian and thus can be readily solved with standard physics-informed neural networks (PINNs). We propose two methods to obtain a fractional score function: fractional score matching (FSM) and score-fPINN for fitting the fractional score function. While FSM is more cost-effective, it relies on known conditional distributions. On the other hand, score-fPINN is independent of specific stochastic differential equations (SDEs) but requires evaluating the PINN model's derivatives, which may be more costly. We conduct our experiments on various SDEs and demonstrate numerical stability and effectiveness of our method in dealing with high-dimensional problems, marking a significant advancement in addressing the CoD in FPL equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11676v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, Kenji Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Computation and Control of Unstable Steady States for Mean Field Multiagent Systems</title>
      <link>https://arxiv.org/abs/2406.11725</link>
      <description>arXiv:2406.11725v1 Announce Type: cross 
Abstract: We study interacting particle systems driven by noise, modeling phenomena such as opinion dynamics. We are interested in systems that exhibit phase transitions i.e. non-uniqueness of stationary states for the corresponding McKean-Vlasov PDE, in the mean field limit. We develop an efficient numerical scheme for identifying all steady states (both stable and unstable) of the mean field McKean-Vlasov PDE, based on a spectral Galerkin approximation combined with a deflated Newton's method to handle the multiplicity of solutions. Having found all possible equilibra, we formulate an optimal control strategy for steering the dynamics towards a chosen unstable steady state. The control is computed using iterated open-loop solvers in a receding horizon fashion. We demonstrate the effectiveness of the proposed steady state computation and stabilization methodology on several examples, including the noisy Hegselmann-Krause model for opinion dynamics and the Haken-Kelso-Bunz model from biophysics. The numerical experiments validate the ability of the approach to capture the rich self-organization landscape of these systems and to stabilize unstable configurations of interest. The proposed computational framework opens up new possibilities for understanding and controlling the collective behavior of noise-driven interacting particle systems, with potential applications in various fields such as social dynamics, biological synchronization, and collective behavior in physical and social systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11725v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Bicego, Dante Kalise, Grigorios A. Pavliotis</dc:creator>
    </item>
    <item>
      <title>Simple matrix expressions for the curvatures of Grassmannian</title>
      <link>https://arxiv.org/abs/2406.11821</link>
      <description>arXiv:2406.11821v1 Announce Type: cross 
Abstract: We show that modeling a Grassmannian as symmetric orthogonal matrices $\operatorname{Gr}(k,\mathbb{R}^n) \cong\{Q \in \mathbb{R}^{n \times n} : Q^{\scriptscriptstyle\mathsf{T}} Q = I, \; Q^{\scriptscriptstyle\mathsf{T}} = Q,\; \operatorname{tr}(Q)=2k - n\}$ yields exceedingly simple matrix formulas for various curvatures and curvature-related quantities, both intrinsic and extrinsic. These include Riemann, Ricci, Jacobi, sectional, scalar, mean, principal, and Gaussian curvatures; Schouten, Weyl, Cotton, Bach, Pleba\'nski, cocurvature, nonmetricity, and torsion tensors; first, second, and third fundamental forms; Gauss and Weingarten maps; and upper and lower delta invariants. We will derive explicit, simple expressions for the aforementioned quantities in terms of standard matrix operations that are stably computable with numerical linear algebra. Many of these aforementioned quantities have never before been presented for the Grassmannian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11821v1</guid>
      <category>math.DG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zehua Lai, Lek-Heng Lim, Ke Ye</dc:creator>
    </item>
    <item>
      <title>Shapley values and machine learning to characterize metamaterials for seismic applications</title>
      <link>https://arxiv.org/abs/2108.00493</link>
      <description>arXiv:2108.00493v2 Announce Type: replace 
Abstract: Given the damages from earthquakes, seismic isolation of critical infrastructure is vital to mitigate losses due to seismic events. A promising approach for seismic isolation systems is metamaterials-based wave barriers. Metamaterials -- engineered composites -- manipulate the propagation and attenuation of seismic waves. Borrowing ideas from phononic and sonic crystals, the central goal of a metamaterials-based wave barrier is to create band gaps that cover the frequencies of seismic waves. The two quantities of interest (QoIs) that characterize band-gaps are the first-frequency cutoff and the band-gap's width. Researchers often use analytical (band-gap analysis), experimental (shake table tests), and statistical (global variance) approaches to tailor the QoIs. However, these approaches are expensive and compute-intensive. So, a pressing need exists for alternative easy-to-use methods to quantify the correlation between input (design) parameters and QoIs. To quantify such a correlation, in this paper, we will use Shapley values, a technique from the cooperative game theory. In addition, we will develop machine learning models that can predict the QoIs for a given set of input (material and geometrical) parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.00493v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. Oniz, Y. L. Mo, K. B. Nakshatrala</dc:creator>
    </item>
    <item>
      <title>Nonlocality and Nonlinearity Implies Universality in Operator Learning</title>
      <link>https://arxiv.org/abs/2304.13221</link>
      <description>arXiv:2304.13221v2 Announce Type: replace 
Abstract: Neural operator architectures approximate operators between infinite-dimensional Banach spaces of functions. They are gaining increased attention in computational science and engineering, due to their potential both to accelerate traditional numerical methods and to enable data-driven discovery. As the field is in its infancy basic questions about minimal requirements for universal approximation remain open. It is clear that any general approximation of operators between spaces of functions must be both nonlocal and nonlinear. In this paper we describe how these two attributes may be combined in a simple way to deduce universal approximation. In so doing we unify the analysis of a wide range of neural operator architectures and open up consideration of new ones.
  A popular variant of neural operators is the Fourier neural operator (FNO). Previous analysis proving universal operator approximation theorems for FNOs resorts to use of an unbounded number of Fourier modes, relying on intuition from traditional analysis of spectral methods. The present work challenges this point of view: (i) the work reduces FNO to its core essence, resulting in a minimal architecture termed the ``averaging neural operator'' (ANO); and (ii) analysis of the ANO shows that even this minimal ANO architecture benefits from universal approximation. This result is obtained based on only a spatial average as its only nonlocal ingredient (corresponding to retaining only a \emph{single} Fourier mode in the special case of the FNO). The analysis paves the way for a more systematic exploration of nonlocality, both through the development of new operator learning architectures and the analysis of existing and new architectures. Numerical results are presented which give insight into complexity issues related to the roles of channel width (embedding dimension) and number of Fourier modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13221v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Lanthaler, Zongyi Li, Andrew M. Stuart</dc:creator>
    </item>
    <item>
      <title>Construction of the Kolmogorov-Arnold representation using the Newton-Kaczmarz method</title>
      <link>https://arxiv.org/abs/2305.08194</link>
      <description>arXiv:2305.08194v2 Announce Type: replace 
Abstract: The Kolmogorov-Arnold representation of a continuous multivariate function is a decomposition of the function into a structure of inner and outer functions of a single variable. It can be a convenient tool for tasks where it is required to obtain a predictive model that maps some vector input of a black box system into a scalar output. However, the construction of such representation based on the recorded input-output data is a challenging task. In the present paper, it is suggested to decompose the underlying functions of the representation into continuous basis functions and parameters. It is then proposed to find the parameters using the Newton-Kaczmarz method for solving systems of non-linear equations. The paper demonstrates that such approach is also an excellent tool for data-driven solution of partial differential equations. Numerical examples show that the proposed approach is efficient and more robust with respect to the section of the initial guess for the parameters than the straightforward application of the Gauss-Newton method for the task of data modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08194v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Poluektov, Andrew Polar</dc:creator>
    </item>
    <item>
      <title>A convergent stochastic scalar auxiliary variable method</title>
      <link>https://arxiv.org/abs/2308.07060</link>
      <description>arXiv:2308.07060v2 Announce Type: replace 
Abstract: We discuss an extension of the scalar auxiliary variable approach, which was originally introduced by Shen et al. ([Shen, Xu, Yang, J. Comput. Phys., 2018]) for the discretization of deterministic gradient flows. By introducing an additional scalar auxiliary variable, this approach allows to derive a linear scheme, while still maintaining unconditional stability. Our extension augments the approximation of the evolution of this scalar auxiliary variable with higher order terms, which enables its application to stochastic partial differential equations. Using the stochastic Allen--Cahn equation as a prototype for nonlinear stochastic partial differential equations with multiplicative noise, we propose an unconditionally energy stable, linear, fully discrete finite element scheme based on our augmented scalar auxiliary variable method. Recovering a discrete version of the energy estimate and establishing Nikolskii estimates with respect to time, we are able to prove convergence of discrete solutions towards pathwise unique martingale solutions by applying Jakubowski's generalization of Skorokhod's theorem. A generalization of the Gy\"ongy--Krylov characterization of convergence in probability to quasi-Polish spaces finally provides convergence of fully discrete solutions towards strong solutions of the stochastic Allen--Cahn equation. Finally, we present numerical simulations underlining the practicality of the scheme and the importance of the introduced augmentation terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07060v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Metzger</dc:creator>
    </item>
    <item>
      <title>Dynamical Low-Rank Approximation for Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2308.11581</link>
      <description>arXiv:2308.11581v3 Announce Type: replace 
Abstract: In this paper, we set the mathematical foundations of the Dynamical Low-Rank Approximation (DLRA) method for stochastic differential equations. DLRA aims at approximating the solution as a linear combination of a small number of basis vectors with random coefficients (low rank format) with the peculiarity that both the basis vectors and the random coefficients vary in time. While the formulation and properties of DLRA are now well understood for random/parametric equations, the same cannot be said for SDEs and this work aims to fill this gap. We start by rigorously formulating a Dynamically Orthogonal (DO) approximation (an instance of DLRA successfully used in applications) for SDEs, which we then generalize to define a parametrization independent DLRA for SDEs. We show local well-posedness of the DO equations and their equivalence with the DLRA formulation. We also characterize the explosion time of the DO solution by a loss of linear independence of the random coefficients defining the solution expansion and give sufficient conditions for global existence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11581v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshihito Kazashi, Fabio Nobile, Fabio Zoccolan</dc:creator>
    </item>
    <item>
      <title>Efficient third order tensor-oriented directional splitting for exponential integrators</title>
      <link>https://arxiv.org/abs/2310.07551</link>
      <description>arXiv:2310.07551v3 Announce Type: replace 
Abstract: Suitable discretizations through tensor product formulas of popular multidimensional operators (diffusion or diffusion--advection, for instance) lead to matrices with $d$-dimensional Kronecker sum structure. For evolutionary Partial Differential Equations containing such operators and integrated in time with exponential integrators, it is then of paramount importance to efficiently approximate the actions of $\varphi$-functions of the arising matrices. In this work, we show how to produce directional split approximations of third order with respect to the time step size. They conveniently employ tensor-matrix products (the so-called $\mu$-mode product and related Tucker operator, realized in practice with high performance level 3 BLAS), and allow for the effective usage of exponential Runge--Kutta integrators up to order three. The technique can also be efficiently implemented on modern computer hardware such as Graphic Processing Units. The approach has been successfully tested against state-of-the-art techniques on two well-known physical models that lead to Turing patterns, namely the 2D Schnakenberg and the 3D FitzHugh--Nagumo systems, on different hardware and software architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07551v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Cassini</dc:creator>
    </item>
    <item>
      <title>A novel and simple spectral method for nonlocal PDEs with the fractional Laplacian</title>
      <link>https://arxiv.org/abs/2311.07814</link>
      <description>arXiv:2311.07814v2 Announce Type: replace 
Abstract: We propose a novel and simple spectral method based on the semi-discrete Fourier transforms to discretize the fractional Laplacian $(-\Delta)^\frac{\alpha}{2}$. Numerical analysis and experiments are provided to study its performance. Our method has the same symbol $|\boldsymbol\xi|^\alpha$ as the fractional Laplacian $(-\Delta)^\frac{\alpha}{2}$ at the discrete level, and thus it can be viewed as the exact discrete analogue of the fractional Laplacian. This {\it unique feature} distinguishes our method from other existing methods for the fractional Laplacian. Note that our method is different from the Fourier pseudospectral methods in the literature which are usually limited to periodic boundary conditions (see Remark \ref{remark0}). Numerical analysis shows that our method can achieve a spectral accuracy. The stability and convergence of our method in solving the fractional Poisson equations were analyzed. Our scheme yields a multilevel Toeplitz stiffness matrix, and thus fast algorithms can be developed for efficient matrix-vector multiplications. The computational complexity is ${\mathcal O}(2N\log(2N))$, and the memory storage is ${\mathcal O}(N)$ with $N$ the total number of points. Extensive numerical experiments verify our analytical results and demonstrate the effectiveness of our method in solving various problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07814v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.camwa.2024.06.001</arxiv:DOI>
      <dc:creator>Shiping Zhou, Yanzhi Zhang</dc:creator>
    </item>
    <item>
      <title>Koopmon trajectories in nonadiabatic quantum-classical dynamics</title>
      <link>https://arxiv.org/abs/2312.13878</link>
      <description>arXiv:2312.13878v2 Announce Type: replace 
Abstract: In order to alleviate the computational costs of fully quantum nonadiabatic dynamics, we present a mixed quantum-classical (MQC) particle method based on the theory of Koopman wavefunctions. Although conventional MQC models often suffer from consistency issues such as the violation of Heisenberg's principle, we overcame these difficulties by blending Koopman's classical mechanics on Hilbert spaces with methods in symplectic geometry. The resulting continuum model enjoys both a variational and a Hamiltonian structure, while its nonlinear character calls for suitable closures. Benefiting from the underlying action principle, here we apply a regularization technique previously developed within our team. This step allows for a singular solution ansatz which introduces the trajectories of computational particles - the koopmons - sampling the Lagrangian classical paths in phase space. In the case of Tully's nonadiabatic problems, the method reproduces the results of fully quantum simulations with levels of accuracy that are not achieved by standard MQC Ehrenfest simulations. In addition, the koopmon method is computationally advantageous over similar fully quantum approaches, which are also considered in our study. As a further step, we probe the limits of the method by considering the Rabi problem in both the ultrastrong and the deep strong coupling regimes, where MQC treatments appear hardly applicable. In this case, the method succeeds in reproducing parts of the fully quantum results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13878v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.chem-ph</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Werner Bauer, Paul Bergold, Fran\c{c}ois Gay-Balmaz, Cesare Tronci</dc:creator>
    </item>
    <item>
      <title>On the best convergence rates of lightning plus polynomial approximations</title>
      <link>https://arxiv.org/abs/2312.16116</link>
      <description>arXiv:2312.16116v3 Announce Type: replace 
Abstract: Building on introducing exponentially clustered poles, Trefethen and his collaborators introduced lightning algorithms for approximating functions of singularities. These schemes may achieve root-exponential convergence rates. In particular, based on a specific choice of the parameter of the tapered exponentially clustered poles, the lightning approximation with either a low-degree polynomial basis may achieve the optimal convergence rate simply as the best rational approximation for prototype $x^\alpha$ on $[0,1]$, which was illustrated through delicate numerical experiments and conjectured in [SIAM J. Numer. Anal., 61:2580-2600, 2023]. By utilizing Poisson's summation formula and results akin to Paley-Wiener Theorem, we rigorously show that all these schemes with a low-degree polynomial basis achieve root-exponential convergence rates with exact orders in approximating $x^\alpha$ for arbitrary clustered parameters theoretically, and provide the best choices of the parameter to achieve the fastest convergence rate for each type of clustered poles, from which the conjecture is confirmed as a special case. Ample numerical evidences demonstrate the optimality and sharpness of the estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16116v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhuang Xiang, Shunfeng Yang, Yanghao Wu</dc:creator>
    </item>
    <item>
      <title>Neumann-Neumann type domain decomposition of elliptic problems on metric graphs</title>
      <link>https://arxiv.org/abs/2402.05707</link>
      <description>arXiv:2402.05707v2 Announce Type: replace 
Abstract: In this paper we develop a Neumann-Neumann type domain decomposition method for elliptic problems on metric graphs. We describe the iteration in the continuous and discrete setting and rewrite the latter as a preconditioner for the Schur complement system. Then we formulate the discrete iteration as an abstract additive Schwarz iteration and prove that it convergences to the finite element solution with a rate that is independent of the finite element mesh size. We show that the condition number of the Schur complement is also independent of the finite element mesh size. We provide an implementation and test it on various examples of interest and compare it to other preconditioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05707v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mih\'aly Kov\'acs, Mih\'aly Andr\'as V\'aghy</dc:creator>
    </item>
    <item>
      <title>Structure-preserving weighted BDF2 methods for Anisotropic Cahn-Hilliard model: uniform/variable-time-steps</title>
      <link>https://arxiv.org/abs/2404.13259</link>
      <description>arXiv:2404.13259v2 Announce Type: replace 
Abstract: In this paper, we innovatively develop uniform/variable-time-step weighted and shifted BDF2 (WSBDF2) methods for the anisotropic Cahn-Hilliard (CH) model, combining the scalar auxiliary variable (SAV) approach with two types of stabilized techniques. Using the concept of $G$-stability, the uniform-time-step WSBDF2 method is theoretically proved to be energy-stable. Due to the inapplicability of the relevant G-stability properties, another technique is adopted in this work to demonstrate the energy stability of the variable-time-step WSBDF2 method. In addition, the two numerical schemes are all mass-conservative.Finally, numerous numerical simulations are presented to demonstrate the stability and accuracy of these schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13259v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Li, Jingjiang Bi, Nan Wang</dc:creator>
    </item>
    <item>
      <title>Combining physics-informed graph neural network and finite difference for solving forward and inverse spatiotemporal PDEs</title>
      <link>https://arxiv.org/abs/2405.20000</link>
      <description>arXiv:2405.20000v2 Announce Type: replace 
Abstract: The great success of Physics-Informed Neural Networks (PINN) in solving partial differential equations (PDEs) has significantly advanced our simulation and understanding of complex physical systems in science and engineering. However, many PINN-like methods are poorly scalable and are limited to in-sample scenarios. To address these challenges, this work proposes a novel discrete approach termed Physics-Informed Graph Neural Network (PIGNN) to solve forward and inverse nonlinear PDEs. In particular, our approach seamlessly integrates the strength of graph neural networks (GNN), physical equations and finite difference to approximate solutions of physical systems. Our approach is compared with the PINN baseline on three well-known nonlinear PDEs (heat, Burgers and FitzHugh-Nagumo). We demonstrate the excellent performance of the proposed method to work with irregular meshes, longer time steps, arbitrary spatial resolutions, varying initial conditions (ICs) and boundary conditions (BCs) by conducting extensive numerical experiments. Numerical results also illustrate the superiority of our approach in terms of accuracy, time extrapolability, generalizability and scalability. The main advantage of our approach is that models trained in small domains with simple settings have excellent fitting capabilities and can be directly applied to more complex situations in large domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20000v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhang, Longxiang Jiang, Xinkun Chu, Yong Wen, Luxiong Li, Yonghao Xiao, Liyuan Wang</dc:creator>
    </item>
    <item>
      <title>PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python</title>
      <link>https://arxiv.org/abs/2201.04040</link>
      <description>arXiv:2201.04040v2 Announce Type: replace-cross 
Abstract: PEPit is a Python package aiming at simplifying the access to worst-case analyses of a large family of first-order optimization methods possibly involving gradient, projection, proximal, or linear optimization oracles, along with their approximate, or Bregman variants. In short, PEPit is a package enabling computer-assisted worst-case analyses of first-order optimization methods. The key underlying idea is to cast the problem of performing a worst-case analysis, often referred to as a performance estimation problem (PEP), as a semidefinite program (SDP) which can be solved numerically. To do that, the package users are only required to write first-order methods nearly as they would have implemented them. The package then takes care of the SDP modeling parts, and the worst-case analysis is performed numerically via a standard solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.04040v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baptiste Goujaud, C\'eline Moucer, Fran\c{c}ois Glineur, Julien Hendrickx, Adrien Taylor, Aymeric Dieuleveut</dc:creator>
    </item>
    <item>
      <title>Rethink Tree Traversal</title>
      <link>https://arxiv.org/abs/2209.04825</link>
      <description>arXiv:2209.04825v5 Announce Type: replace-cross 
Abstract: We will show how to implement binary decision tree traversal in the language of matrix computation. Our main contribution is to propose some equivalent algorithms of binary tree traversal based on a novel matrix representation of the hierarchical structure of the decision tree. Our key idea is to travel the binary decision tree by maximum inner product search. We not only implement decision tree methods without the recursive traverse but also delve into the partitioning nature of tree-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04825v5</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinxiong Zhang</dc:creator>
    </item>
    <item>
      <title>Stable nearly self-similar blowup of the 2D Boussinesq and 3D Euler equations with smooth data II: Rigorous Numerics</title>
      <link>https://arxiv.org/abs/2305.05660</link>
      <description>arXiv:2305.05660v2 Announce Type: replace-cross 
Abstract: This is Part II of our paper in which we prove finite time blowup of the 2D Boussinesq and 3D axisymmetric Euler equations with smooth initial data of finite energy and boundary. In Part I of our paper [ChenHou2023a], we establish an analytic framework to prove stability of an approximate self-similar blowup profile by a combination of a weighted $L^\infty$ norm and a weighted $C^{1/2}$ norm. Under the assumption that the stability constants, which depend on the approximate steady state, satisfy certain inequalities stated in our stability lemma, we prove stable nearly self-similar blowup of the 2D Boussinesq and 3D Euler equations with smooth initial data and boundary. In Part II of our paper, we provide sharp stability estimates of the linearized operator by constructing space-time solutions with rigorous error control. We also obtain sharp estimates of the velocity in the regular case using computer assistance. These results enable us to verify that the stability constants obtained in Part I [ChenHou2023a] indeed satisfy the inequalities in our stability lemma. This completes the analysis of the finite time singularity of the axisymmetric Euler equations with smooth initial data and boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05660v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajie Chen, Thomas Y. Hou</dc:creator>
    </item>
    <item>
      <title>Deep learning probability flows and entropy production rates in active matter</title>
      <link>https://arxiv.org/abs/2309.12991</link>
      <description>arXiv:2309.12991v2 Announce Type: replace-cross 
Abstract: Active matter systems, from self-propelled colloids to motile bacteria, are characterized by the conversion of free energy into useful work at the microscopic scale. They involve physics beyond the reach of equilibrium statistical mechanics, and a persistent challenge has been to understand the nature of their nonequilibrium states. The entropy production rate and the probability current provide quantitative ways to do so by measuring the breakdown of time-reversal symmetry. Yet, their efficient computation has remained elusive, as they depend on the system's unknown and high-dimensional probability density. Here, building upon recent advances in generative modeling, we develop a deep learning framework to estimate the score of this density. We show that the score, together with the microscopic equations of motion, gives access to the entropy production rate, the probability current, and their decomposition into local contributions from individual particles. To represent the score, we introduce a novel, spatially-local transformer network architecture that learns high-order interactions between particles while respecting their underlying permutation symmetry. We demonstrate the broad utility and scalability of the method by applying it to several high-dimensional systems of active particles undergoing motility-induced phase separation (MIPS). We show that a single network trained on a system of 4096 particles at one packing fraction can generalize to other regions of the phase diagram, including systems with as many as 32768 particles. We use this observation to quantify the spatial structure of the departure from equilibrium in MIPS as a function of the number of particles and the packing fraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12991v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.soft</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2318106121</arxiv:DOI>
      <dc:creator>Nicholas M. Boffi, Eric Vanden-Eijnden</dc:creator>
    </item>
    <item>
      <title>Eigenvector Continuation and Projection-Based Emulators</title>
      <link>https://arxiv.org/abs/2310.19419</link>
      <description>arXiv:2310.19419v3 Announce Type: replace-cross 
Abstract: Eigenvector continuation is a computational method for parametric eigenvalue problems that uses subspace projection with a basis derived from eigenvector snapshots from different parameter sets. It is part of a broader class of subspace-projection techniques called reduced-basis methods. In this colloquium article, we present the development, theory, and applications of eigenvector continuation and projection-based emulators. We introduce the basic concepts, discuss the underlying theory and convergence properties, and present recent applications for quantum systems and future prospects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19419v3</guid>
      <category>nucl-th</category>
      <category>cond-mat.quant-gas</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>nucl-ex</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Duguet, Andreas Ekstr\"om, Richard J. Furnstahl, Sebastian K\"onig, Dean Lee</dc:creator>
    </item>
    <item>
      <title>Applications of Moments of Dirichlet Coefficients in Elliptic Curve Families</title>
      <link>https://arxiv.org/abs/2311.17215</link>
      <description>arXiv:2311.17215v2 Announce Type: replace-cross 
Abstract: The moments of the coefficients of elliptic curve L-functions are related to numerous arithmetic problems. Rosen and Silverman proved a conjecture of Nagao relating the first moment of one-parameter families satisfying Tate's conjecture to the rank of the corresponding elliptic surface over Q(T); one can also construct families of moderate rank by finding families with large first moments. Michel proved that if j(T) is not constant, then the second moment of the family is of size p^2 + O(p^(3/2)); these two moments show that for suitably small support the behavior of zeros near the central point agree with that of eigenvalues from random matrix ensembles, with the higher moments impacting the rate of convergence.
  In his thesis, Miller noticed a negative bias in the second moment of every one-parameter family of elliptic curves over the rationals whose second moment had a calculable closed-form expression, specifically the first lower order term which does not average to zero is on average negative. This Bias Conjecture is confirmed for many families; however, these are highly non-generic families whose resulting Legendre sums can be determined. Inspired by the recent successes by Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver, Alexey Pozdnyakov and others in investigations of murmurations of elliptic curve coefficients with machine learning techniques, we pose a similar problem for trying to understand the Bias Conjecture. As a start to this program, we numerically investigate the Bias Conjecture for a family whose bias is positive for half the primes. Since the numerics do not offer conclusive evidence that negative bias for the other half is enough to overwhelm the positive bias, the Bias Conjecture cannot be verified for the family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17215v2</guid>
      <category>math.NT</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zo\"e Batterman, Aditya Jambhale, Steven J. Miller, Akash L. Narayanan, Kishan Sharma, Andrew Yang, Chris Yao</dc:creator>
    </item>
    <item>
      <title>Physics-informed Neural Network Estimation of Material Properties in Soft Tissue Nonlinear Biomechanical Models</title>
      <link>https://arxiv.org/abs/2312.09787</link>
      <description>arXiv:2312.09787v3 Announce Type: replace-cross 
Abstract: The development of biophysical models for clinical applications is rapidly advancing in the research community, thanks to their predictive nature and their ability to assist the interpretation of clinical data. However, high-resolution and accurate multi-physics computational models are computationally expensive and their personalisation involves fine calibration of a large number of parameters, which may be space-dependent, challenging their clinical translation. In this work, we propose a new approach which relies on the combination of physics-informed neural networks (PINNs) with three-dimensional soft tissue nonlinear biomechanical models, capable of reconstructing displacement fields and estimating heterogeneous patient-specific biophysical properties. The proposed learning algorithm encodes information from a limited amount of displacement and, in some cases, strain data, that can be routinely acquired in the clinical setting, and combines it with the physics of the problem, represented by a mathematical model based on partial differential equations, to regularise the problem and improve its convergence properties. Several benchmarks are presented to show the accuracy and robustness of the proposed method and its great potential to enable the robust and effective identification of patient-specific, heterogeneous physical properties, s.a. tissue stiffness properties. In particular, we demonstrate the capability of the PINN to detect the presence, location and severity of scar tissue, which is beneficial to develop personalised simulation models for disease diagnosis, especially for cardiac applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09787v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.bio-ph</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federica Caforio, Francesco Regazzoni, Stefano Pagani, Elias Karabelas, Christoph Augustin, Gundolf Haase, Gernot Plank, Alfio Quarteroni</dc:creator>
    </item>
    <item>
      <title>Sampling and estimation on manifolds using the Langevin diffusion</title>
      <link>https://arxiv.org/abs/2312.14882</link>
      <description>arXiv:2312.14882v2 Announce Type: replace-cross 
Abstract: Error bounds are derived for sampling and estimation using a discretization of an intrinsically defined Langevin diffusion with invariant measure $\text{d}\mu_\phi \propto e^{-\phi} \mathrm{dvol}_g $ on a compact Riemannian manifold. Two estimators of linear functionals of $\mu_\phi $ based on the discretized Markov process are considered: a time-averaging estimator based on a single trajectory and an ensemble-averaging estimator based on multiple independent trajectories. Imposing no restrictions beyond a nominal level of smoothness on $\phi$, first-order error bounds, in discretization step size, on the bias and variance/mean-square error of both estimators are derived. The order of error matches the optimal rate in Euclidean and flat spaces, and leads to a first-order bound on distance between the invariant measure $\mu_\phi$ and a stationary measure of the discretized Markov process. This order is preserved even upon using retractions when exponential maps are unavailable in closed form, thus enhancing practicality of the proposed algorithms. Generality of the proof techniques, which exploit links between two partial differential equations and the semigroup of operators corresponding to the Langevin diffusion, renders them amenable for the study of a more general class of sampling algorithms related to the Langevin diffusion. Conditions for extending analysis to the case of non-compact manifolds are discussed. Numerical illustrations with distributions, log-concave and otherwise, on the manifolds of positive and negative curvature elucidate on the derived bounds and demonstrate practical utility of the sampling algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14882v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Bharath, Alexander Lewis, Akash Sharma, Michael V Tretyakov</dc:creator>
    </item>
    <item>
      <title>PICL: Physics Informed Contrastive Learning for Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2401.16327</link>
      <description>arXiv:2401.16327v3 Announce Type: replace-cross 
Abstract: Neural operators have recently grown in popularity as Partial Differential Equation (PDE) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves accuracy for the Fourier Neural Operator in fixed-future and autoregressive rollout tasks for the 1D and 2D Heat, Burgers', and linear advection equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16327v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cooper Lorsung, Amir Barati Farimani</dc:creator>
    </item>
    <item>
      <title>Benign overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Inductive Bias</title>
      <link>https://arxiv.org/abs/2406.09194</link>
      <description>arXiv:2406.09194v2 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have inspired a surge of research into reconstructing specific quantities of interest from measurements that comply with certain physical laws. These efforts focus on inverse problems that are governed by partial differential equations (PDEs). In this work, we develop an asymptotic Sobolev norm learning curve for kernel ridge(less) regression when addressing (elliptical) linear inverse problems. Our results show that the PDE operators in the inverse problem can stabilize the variance and even behave benign overfitting for fixed-dimensional problems, exhibiting different behaviors from regression problems. Besides, our investigation also demonstrates the impact of various inductive biases introduced by minimizing different Sobolev norms as a form of implicit regularization. For the regularized least squares estimator, we find that all considered inductive biases can achieve the optimal convergence rate, provided the regularization parameter is appropriately chosen. The convergence rate is actually independent to the choice of (smooth enough) inductive bias for both ridge and ridgeless regression. Surprisingly, our smoothness requirement recovered the condition found in Bayesian setting and extend the conclusion to the minimum norm interpolation estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09194v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu</dc:creator>
    </item>
  </channel>
</rss>
