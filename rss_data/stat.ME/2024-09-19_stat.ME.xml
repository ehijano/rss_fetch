<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simultaneous Estimation of Many Sparse Networks via Hierarchical Poisson Log-Normal Model</title>
      <link>https://arxiv.org/abs/2409.12275</link>
      <description>arXiv:2409.12275v1 Announce Type: new 
Abstract: The advancement of single-cell RNA-sequencing (scRNA-seq) technologies allow us to study the individual level cell-type-specific gene expression networks by direct inference of genes' conditional independence structures. scRNA-seq data facilitates the analysis of gene expression data across different conditions or samples, enabling simultaneous estimation of condition- or sample-specific gene networks. Since the scRNA-seq data are count data with many zeros, existing network inference methods based on Gaussian graphs cannot be applied to such single cell data directly. We propose a hierarchical Poisson Log-Normal model to simultaneously estimate many such networks to effectively incorporate the shared network structures. We develop an efficient simultaneous estimation method that uses the variational EM and alternating direction method of multipliers (ADMM) algorithms, optimized for parallel processing. Simulation studies show this method outperforms traditional methods in network structure recovery and parameter estimation across various network models. We apply the method to two single cell RNA-seq datasets, a yeast single-cell gene expression dataset measured under 11 different environmental conditions, and a single-cell gene expression data from 13 inflammatory bowel disease patients. We demonstrate that simultaneous estimation can uncover a wider range of conditional dependence networks among genes, offering deeper insights into gene expression mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12275v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changhao Ge, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Heckman Selection Contaminated Normal Model</title>
      <link>https://arxiv.org/abs/2409.12348</link>
      <description>arXiv:2409.12348v1 Announce Type: new 
Abstract: The Heckman selection model is one of the most well-renounced econometric models in the analysis of data with sample selection. This model is designed to rectify sample selection biases based on the assumption of bivariate normal error terms. However, real data diverge from this assumption in the presence of heavy tails and/or atypical observations. Recently, this assumption has been relaxed via a more flexible Student's t-distribution, which has appealing statistical properties. This paper introduces a novel Heckman selection model using a bivariate contaminated normal distribution for the error terms. We present an efficient ECM algorithm for parameter estimation with closed-form expressions at the E-step based on truncated multinormal distribution formulas. The identifiability of the proposed model is also discussed, and its properties have been examined. Through simulation studies, we compare our proposed model with the normal and Student's t counterparts and investigate the finite-sample properties and the variation in missing rate. Results obtained from two real data analyses showcase the usefulness and effectiveness of our model. The proposed algorithms are implemented in the R package HeckmanEM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12348v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heeju Lim, Jose Alejandro Ordonez, Victor H. Lachos, Antonio Punzo</dc:creator>
    </item>
    <item>
      <title>Neymanian inference in randomized experiments</title>
      <link>https://arxiv.org/abs/2409.12498</link>
      <description>arXiv:2409.12498v1 Announce Type: new 
Abstract: In his seminal work in 1923, Neyman studied the variance estimation problem for the difference-in-means estimator of the average treatment effect in completely randomized experiments. He proposed a variance estimator that is conservative in general and unbiased when treatment effects are homogeneous. While widely used under complete randomization, there is no unique or natural way to extend this estimator to more complex designs. To this end, we show that Neyman's estimator can be alternatively derived in two ways, leading to two novel variance estimation approaches: the imputation approach and the contrast approach. While both approaches recover Neyman's estimator under complete randomization, they yield fundamentally different variance estimators for more general designs. In the imputation approach, the variance is expressed as a function of observed and missing potential outcomes and then estimated by imputing the missing potential outcomes, akin to Fisherian inference. In the contrast approach, the variance is expressed as a function of several unobservable contrasts of potential outcomes and then estimated by exchanging each unobservable contrast with an observable contrast. Unlike the imputation approach, the contrast approach does not require separately estimating the missing potential outcome for each unit. We examine the theoretical properties of both approaches, showing that for a large class of designs, each produces conservative variance estimators that are unbiased in finite samples or asymptotically under homogeneous treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12498v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Guido W. Imbens</dc:creator>
    </item>
    <item>
      <title>Choice of the hypothesis matrix for using the Anova-type-statistic</title>
      <link>https://arxiv.org/abs/2409.12592</link>
      <description>arXiv:2409.12592v1 Announce Type: new 
Abstract: Initially developed in Brunner et al. (1997), the Anova-type-statistic (ATS) is one of the most used quadratic forms for testing multivariate hypotheses for a variety of different parameter vectors $\boldsymbol{\theta}\in\mathbb{R}^d$. Such tests can be based on several versions of ATS and in most settings, they are preferable over those based on other quadratic forms, as for example the Wald-type-statistic (WTS). However, the same null hypothesis $\boldsymbol{H}\boldsymbol{\theta}=\boldsymbol{y}$ can be expressed by a multitude of hypothesis matrices $\boldsymbol{H}\in\mathbb{R}^{m\times d}$ and corresponding vectors $\boldsymbol{y}\in\mathbb{R}^m$, which leads to different values of the test statistic, as it can be seen in simple examples. Since this can entail distinct test decisions, it remains to investigate under which conditions tests using different hypothesis matrices coincide. Here, the dimensions of the different hypothesis matrices can be substantially different, which has exceptional potential to save computation effort.
  In this manuscript, we show that for the Anova-type-statistic and some versions thereof, it is possible for each hypothesis $\boldsymbol{H}\boldsymbol{\theta}=\boldsymbol{y}$ to construct a companion matrix $\boldsymbol{L}$ with a minimal number of rows, which not only tests the same hypothesis but also always yields the same test decisions. This allows a substantial reduction of computation time, which is investigated in several conducted simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12592v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Manuel Rosenbaum</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap Between Design and Analysis: Randomization Inference and Sensitivity Analysis for Matched Observational Studies with Treatment Doses</title>
      <link>https://arxiv.org/abs/2409.12848</link>
      <description>arXiv:2409.12848v1 Announce Type: new 
Abstract: Matching is a commonly used causal inference study design in observational studies. Through matching on measured confounders between different treatment groups, valid randomization inferences can be conducted under the no unmeasured confounding assumption, and sensitivity analysis can be further performed to assess sensitivity of randomization inference results to potential unmeasured confounding. However, for many common matching designs, there is still a lack of valid downstream randomization inference and sensitivity analysis approaches. Specifically, in matched observational studies with treatment doses (e.g., continuous or ordinal treatments), with the exception of some special cases such as pair matching, there is no existing randomization inference or sensitivity analysis approach for studying analogs of the sample average treatment effect (Neyman-type weak nulls), and no existing valid sensitivity analysis approach for testing the sharp null of no effect for any subject (Fisher's sharp null) when the outcome is non-binary. To fill these gaps, we propose new methods for randomization inference and sensitivity analysis that can work for general matching designs with treatment doses, applicable to general types of outcome variables (e.g., binary, ordinal, or continuous), and cover both Fisher's sharp null and Neyman-type weak nulls. We illustrate our approaches via comprehensive simulation studies and a real-data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12848v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Zhang, Siyu Heng</dc:creator>
    </item>
    <item>
      <title>Scaleable Dynamic Forecast Reconciliation</title>
      <link>https://arxiv.org/abs/2409.12856</link>
      <description>arXiv:2409.12856v1 Announce Type: new 
Abstract: We introduce a dynamic approach to probabilistic forecast reconciliation at scale. Our model differs from the existing literature in this area in several important ways. Firstly we explicitly allow the weights allocated to the base forecasts in forming the combined, reconciled forecasts to vary over time. Secondly we drop the assumption, near ubiquitous in the literature, that in-sample base forecasts are appropriate for determining these weights, and use out of sample forecasts instead. Most existing probabilistic reconciliation approaches rely on time consuming sampling based techniques, and therefore do not scale well (or at all) to large data sets. We address this problem in two main ways, firstly by utilising a closed from estimator of covariance structure appropriate to hierarchical forecasting problems, and secondly by decomposing large hierarchies in to components which can be reconciled separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12856v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ross Hollyman, Fotios Petropoulos, Michael E. Tipping</dc:creator>
    </item>
    <item>
      <title>A general condition for bias attenuation by a nondifferentially mismeasured confounder</title>
      <link>https://arxiv.org/abs/2409.12928</link>
      <description>arXiv:2409.12928v1 Announce Type: new 
Abstract: In real-world studies, the collected confounders may suffer from measurement error. Although mismeasurement of confounders is typically unintentional -- originating from sources such as human oversight or imprecise machinery -- deliberate mismeasurement also occurs and is becoming increasingly more common. For example, in the 2020 U.S. Census, noise was added to measurements to assuage privacy concerns. Sensitive variables such as income or age are oftentimes partially censored and are only known up to a range of values. In such settings, obtaining valid estimates of the causal effect of a binary treatment can be impossible, as mismeasurement of confounders constitutes a violation of the no unmeasured confounding assumption. A natural question is whether the common practice of simply adjusting for the mismeasured confounder is justifiable. In this article, we answer this question in the affirmative and demonstrate that in many realistic scenarios not covered by previous literature, adjusting for the mismeasured confounders reduces bias compared to not adjusting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12928v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Zhang, Junu Lee</dc:creator>
    </item>
    <item>
      <title>SplitVAEs: Decentralized scenario generation from siloed data for stochastic optimization problems</title>
      <link>https://arxiv.org/abs/2409.12328</link>
      <description>arXiv:2409.12328v1 Announce Type: cross 
Abstract: Stochastic optimization problems in large-scale multi-stakeholder networked systems (e.g., power grids and supply chains) rely on data-driven scenarios to encapsulate complex spatiotemporal interdependencies. However, centralized aggregation of stakeholder data is challenging due to the existence of data silos resulting from computational and logistical bottlenecks. In this paper, we present SplitVAEs, a decentralized scenario generation framework that leverages variational autoencoders to generate high-quality scenarios without moving stakeholder data. With the help of experiments on distributed memory systems, we demonstrate the broad applicability of SplitVAEs in a variety of domain areas that are dominated by a large number of stakeholders. Our experiments indicate that SplitVAEs can learn spatial and temporal interdependencies in large-scale networks to generate scenarios that match the joint historical distribution of stakeholder data in a decentralized manner. Our experiments show that SplitVAEs deliver robust performance compared to centralized, state-of-the-art benchmark methods while significantly reducing data transmission costs, leading to a scalable, privacy-enhancing alternative to scenario generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12328v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H M Mohaimanul Islam, Huynh Q. N. Vo, Paritosh Ramanan</dc:creator>
    </item>
    <item>
      <title>Parameters on the boundary in predictive regression</title>
      <link>https://arxiv.org/abs/2409.12611</link>
      <description>arXiv:2409.12611v1 Announce Type: cross 
Abstract: We consider bootstrap inference in predictive (or Granger-causality) regressions when the parameter of interest may lie on the boundary of the parameter space, here defined by means of a smooth inequality constraint. For instance, this situation occurs when the definition of the parameter space allows for the cases of either no predictability or sign-restricted predictability. We show that in this context constrained estimation gives rise to bootstrap statistics whose limit distribution is, in general, random, and thus distinct from the limit null distribution of the original statistics of interest. This is due to both (i) the possible location of the true parameter vector on the boundary of the parameter space, and (ii) the possible non-stationarity of the posited predicting (resp. Granger-causing) variable. We discuss a modification of the standard fixed-regressor wild bootstrap scheme where the bootstrap parameter space is shifted by a data-dependent function in order to eliminate the portion of limiting bootstrap randomness attributable to the boundary, and prove validity of the associated bootstrap inference under non-stationarity of the predicting variable as the only remaining source of limiting bootstrap randomness. Our approach, which is initially presented in a simple location model, has bearing on inference in parameter-on-the-boundary situations beyond the predictive regression problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12611v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Iliyan Georgiev, Edoardo Zanelli</dc:creator>
    </item>
    <item>
      <title>Stable and Robust Hyper-Parameter Selection Via Robust Information Sharing Cross-Validation</title>
      <link>https://arxiv.org/abs/2409.12890</link>
      <description>arXiv:2409.12890v1 Announce Type: cross 
Abstract: Robust estimators for linear regression require non-convex objective functions to shield against adverse affects of outliers. This non-convexity brings challenges, particularly when combined with penalization in high-dimensional settings. Selecting hyper-parameters for the penalty based on a finite sample is a critical task. In practice, cross-validation (CV) is the prevalent strategy with good performance for convex estimators. Applied with robust estimators, however, CV often gives sub-par results due to the interplay between multiple local minima and the penalty. The best local minimum attained on the full training data may not be the minimum with the desired statistical properties. Furthermore, there may be a mismatch between this minimum and the minima attained in the CV folds. This paper introduces a novel adaptive CV strategy that tracks multiple minima for each combination of hyper-parameters and subsets of the data. A matching scheme is presented for correctly evaluating minima computed on the full training data using the best-matching minima from the CV folds. It is shown that the proposed strategy reduces the variability of the estimated performance metric, leads to smoother CV curves, and therefore substantially increases the reliability and utility of robust penalized estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12890v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kepplinger, Siqi Wei</dc:creator>
    </item>
    <item>
      <title>Selecting Subpopulations for Causal Inference in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2211.09099</link>
      <description>arXiv:2211.09099v3 Announce Type: replace 
Abstract: The Brazil Bolsa Familia (BF) program is a conditional cash transfer program aimed to reduce short-term poverty by direct cash transfers and to fight long-term poverty by increasing human capital among poor Brazilian people. Eligibility for Bolsa Familia benefits depends on a cutoff rule, which classifies the BF study as a regression discontinuity (RD) design. Extracting causal information from RD studies is challenging. Following Li et al (2015) and Branson and Mealli (2019), we formally describe the BF RD design as a local randomized experiment within the potential outcome approach. Under this framework, causal effects can be identified and estimated on a subpopulation where a local overlap assumption, a local SUTVA and a local ignorability assumption hold. We first discuss the potential advantages of this framework over local regression methods based on continuity assumptions, which concern the definition of the causal estimands, the design and the analysis of the study, and the interpretation and generalizability of the results. A critical issue of this local randomization approach is how to choose subpopulations for which we can draw valid causal inference. We propose a Bayesian model-based finite mixture approach to clustering to classify observations into subpopulations where the RD assumptions hold and do not hold. This approach has important advantages: a) it allows to account for the uncertainty in the subpopulation membership, which is typically neglected; b) it does not impose any constraint on the shape of the subpopulation; c) it is scalable to high-dimensional settings; e) it allows to target alternative causal estimands than the average treatment effect (ATE); and f) it is robust to a certain degree of manipulation/selection of the running variable. We apply our proposed approach to assess causal effects of the Bolsa Familia program on leprosy incidence in 2009.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09099v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Forastiere, Alessandra Mattei, Julia M. Pescarini, Mauricio L. Barreto, Fabrizia Mealli</dc:creator>
    </item>
    <item>
      <title>Targeting relative risk heterogeneity with causal forests</title>
      <link>https://arxiv.org/abs/2309.15793</link>
      <description>arXiv:2309.15793v2 Announce Type: replace 
Abstract: The estimation of heterogeneous treatment effects (HTE) across different subgroups in a population is of significant interest in clinical trial analysis. State-of-the-art HTE estimation methods, including causal forests (Wager and Athey, 2018), generally rely on recursive partitioning for non-parametric identification of relevant covariates and interactions. However, like many other methods in this area, causal forests partition subgroups based on differences in absolute risk. This can dilute statistical power by masking variability in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk, using a novel node-splitting procedure based on exhaustive generalized linear model comparison. We present results that suggest relative risk causal forests can capture otherwise undetected sources of heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15793v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vik Shirvaikar, Xi Lin, Chris Holmes</dc:creator>
    </item>
    <item>
      <title>An accurate percentile method for parametric inference based on asymptotically biased estimators</title>
      <link>https://arxiv.org/abs/2405.05403</link>
      <description>arXiv:2405.05403v2 Announce Type: replace 
Abstract: Inference methods for computing confidence intervals in parametric settings usually rely on consistent estimators of the parameter of interest. However, it may be computationally and/or analytically burdensome to obtain such estimators in various parametric settings, for example when the data exhibit certain features such as censoring, misclassification errors or outliers. To address these challenges, we propose a simulation-based inferential method, called the implicit bootstrap, that remains valid regardless of the potential asymptotic bias of the estimator on which the method is based. We demonstrate that this method allows for the construction of asymptotically valid percentile confidence intervals of the parameter of interest. Additionally, we show that these confidence intervals can also achieve second-order accuracy. We also show that the method is exact in three instances where the standard bootstrap fails. Using simulation studies, we illustrate the coverage accuracy of the method in three examples where standard parametric bootstrap procedures are computationally intensive and less accurate in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05403v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Orso, Mucyo Karemera, Maria-Pia Victoria-Feser, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>On integral priors for multiple comparison in Bayesian model selection</title>
      <link>https://arxiv.org/abs/2406.14184</link>
      <description>arXiv:2406.14184v2 Announce Type: replace 
Abstract: Noninformative priors constructed for estimation purposes are usually not appropriate for model selection and testing. The methodology of integral priors was developed to get prior distributions for Bayesian model selection when comparing two models, modifying initial improper reference priors. We propose a generalization of this methodology to more than two models. Our approach adds an artificial copy of each model under comparison by compactifying the parametric space and creating an ergodic Markov chain across all models that returns the integral priors as marginals of the stationary distribution. Besides the garantee of their existance and the lack of paradoxes attached to estimation reference priors, an additional advantage of this methodology is that the simulation of this Markov chain is straightforward as it only requires simulations of imaginary training samples for all models and from the corresponding posterior distributions. This renders its implementation automatic and generic, both in the nested case and in the nonnested case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14184v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Salmer\'on, Juan Antonio Cano, Christian P. Robert</dc:creator>
    </item>
    <item>
      <title>Interpretability Indices and Soft Constraints for Factor Models</title>
      <link>https://arxiv.org/abs/2409.11525</link>
      <description>arXiv:2409.11525v2 Announce Type: replace 
Abstract: Factor analysis is a way to characterize the relationships between many (observable) variables in terms of a smaller number of unobservable random variables which are called factors. However, the application of factor models and its success can be subjective or difficult to gauge, since infinitely many factor models that produce the same correlation matrix can be fit given sample data. Thus, there is a need to operationalize a criterion that measures how meaningful or "interpretable" a factor model is in order to select the best among many factor models. While there are already techniques that aim to measure and enhance interpretability, new indices, as well as rotation methods via mathematical optimization based on them, are proposed to measure interpretability. The proposed methods directly incorporate semantics with the help of natural language processing and are generalized to incorporate any "prior information". Moreover, the indices allow for complete or partial specification of relationships at a pairwise level. Aside from these, two other main benefits of the proposed methods are that they do not require the estimation of factor scores, which avoids the factor score indeterminacy problem, and that no additional explanatory variables are necessary. The implementation of the proposed methods is written in Python 3 and is made available together with several helper functions through the package interpretablefa on the Python Package Index. The methods' application is demonstrated here using data on the Experiences in Close Relationships Scale, obtained from the Open-Source Psychometrics Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11525v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Philip Tuazon, Gia Mizrane Abubo, Joemari Olea</dc:creator>
    </item>
    <item>
      <title>Optimal Multitask Linear Regression and Contextual Bandits under Sparse Heterogeneity</title>
      <link>https://arxiv.org/abs/2306.06291</link>
      <description>arXiv:2306.06291v2 Announce Type: replace-cross 
Abstract: Large and complex datasets are often collected from several, possibly heterogeneous sources. Multitask learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here, we study multitask linear regression and contextual bandits under sparse heterogeneity, where the source/task-associated parameters are equal to a global parameter plus a sparse task-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing a covariate-wise weighted median of the task-wise linear regression estimates and then shrinking the task-wise estimates towards the weighted median. Compared to task-wise least squares estimates, MOLAR improves the dependence of the estimation error on the data dimension. Extensions of MOLAR to generalized linear models and constructing confidence intervals are discussed in the paper. We then apply MOLAR to develop methods for sparsely heterogeneous multitask contextual bandits, obtaining improved regret guarantees over single-task bandit methods. We further show that our methods are minimax optimal by providing a number of lower bounds. Finally, we support the efficiency of our methods by performing experiments on both synthetic data and the PISA dataset on student educational outcomes from heterogeneous countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06291v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinmeng Huang, Kan Xu, Donghwan Lee, Hamed Hassani, Hamsa Bastani, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Greedy Capon Beamformer</title>
      <link>https://arxiv.org/abs/2404.15329</link>
      <description>arXiv:2404.15329v3 Announce Type: replace-cross 
Abstract: We propose greedy Capon beamformer (GCB) for direction finding of narrow-band sources present in the array's viewing field. After defining the grid covering the location search space, the algorithm greedily builds the interference-plus-noise covariance matrix by identifying a high-power source on the grid using Capon's principle of maximizing the signal to interference plus noise ratio while enforcing unit gain towards the signal of interest. An estimate of the power of the detected source is derived by exploiting the unit power constraint, which subsequently allows to update the noise covariance matrix by simple rank-1 matrix addition composed of outerproduct of the selected steering matrix with itself scaled by the signal power estimate. Our numerical examples demonstrate effectiveness of the proposed GCB in direction finding where it performs favourably compared to the state-of-the-art algorithms under a broad variety of settings. Furthermore, GCB estimates of direction-of-arrivals (DOAs) are very fast to compute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15329v3</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esa Ollila</dc:creator>
    </item>
  </channel>
</rss>
