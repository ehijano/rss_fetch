<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Jul 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Reimagining AI: Exploring Speculative Design Workshops for Supporting BIPOC Youth Critical AI Literacies</title>
      <link>https://arxiv.org/abs/2407.08740</link>
      <description>arXiv:2407.08740v1 Announce Type: new 
Abstract: As Artificial Intelligence ecosystems become increasingly entangled within our everyday lives, designing systems that are ethical, inclusive and socially just is more vital than ever. It is well known that AI can algorithmic biases that reflect, extend and exacerbate our existing systemic injustices. Yet, despite most teenagers interacting with AI daily, only few have the opportunity to learn how it works and its socio-technical complexities. This is a particularly salient issue for marginalized communities. BIPOC teens are often misrepresented throughout AI development and implementation, but they are also less likely to receive STEM education.In response to these unprecedented socio-technical challenges and calls for more critical approaches to child-centered AI design and education, we explore how we can leverage co-speculative design practices to help scaffold BIPOC youth (ages 14-17) critiques of existing AI systems and support the re-imagining of more just AI futures. Drawing on Harway's Situated Knowledges and Speculative Fabulations, these workshops highlight the unique ways marginalized youth perceive AI as having social and ethical implications and how they envision alternative worlds with AI. Our case study describes three 2 hour sessions of a larger 8 week black-led AI STEM program. Analysis includes, data from pre-post surveys, workshop recordings, focus group discussions, learning artifacts, and field notes. We contribute 1) a discussion of how youth perceive AI as having social and ethical implications, 2) a nuanced understanding of how speculative approaches can be leveraged to support youth engagement with complex socio-technical issues and 3) enable youth to open up new AI possibilities in a world absent of techno-capitalist values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08740v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sadhbh Kenny, Alissa N. Antle</dc:creator>
    </item>
    <item>
      <title>A digital twin based approach to smart lighting design</title>
      <link>https://arxiv.org/abs/2407.08741</link>
      <description>arXiv:2407.08741v1 Announce Type: new 
Abstract: Lighting has a critical impact on user mood and behavior, especially in architectural settings. Consequently, smart lighting design is a rapidly growing research area. We describe a digital twin-based approach to smart lighting design that uses an immersive virtual reality digital twin equivalent (virtual environment) of the real world, physical architectural space to explore the visual impact of light configurations. The CLIP neural network is used to obtain a similarity measure between a photo of the physical space with the corresponding rendering in the virtual environment. A case study was used to evaluate the proposed design process. The obtained similarity value of over 87% demonstrates the utility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08741v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elham Mohammadrezaei, Alexander Giovannelli, Logan Lane, Denis Gracanin</dc:creator>
    </item>
    <item>
      <title>UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset</title>
      <link>https://arxiv.org/abs/2407.08850</link>
      <description>arXiv:2407.08850v1 Announce Type: new 
Abstract: Automated UI evaluation can be beneficial for the design process; for example, to compare different UI designs, or conduct automated heuristic evaluation. LLM-based UI evaluation, in particular, holds the promise of generalizability to a wide variety of UI types and evaluation tasks. However, current LLM-based techniques do not yet match the performance of human evaluators. We hypothesize that automatic evaluation can be improved by collecting a targeted UI feedback dataset and then using this dataset to enhance the performance of general-purpose LLMs. We present a targeted dataset of 3,059 design critiques and quality ratings for 983 mobile UIs, collected from seven experienced designers. We carried out an in-depth analysis to characterize the dataset's features. We then applied this dataset to achieve a 55% performance gain in LLM-generated UI feedback via various few-shot and visual prompting techniques. We also discuss future applications of this dataset, including training a reward model for generative UI techniques, and fine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08850v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peitong Duan, Chin-yi Chen, Gang Li, Bjoern Hartmann, Yang Li</dc:creator>
    </item>
    <item>
      <title>GPT-4 is judged more human than humans in displaced and inverted Turing tests</title>
      <link>https://arxiv.org/abs/2407.08853</link>
      <description>arXiv:2407.08853v1 Announce Type: new 
Abstract: Everyday AI detection requires differentiating between people and AI in informal, online conversations. In many cases, people will not interact directly with AI systems but instead read conversations between AI systems and other people. We measured how well people and large language models can discriminate using two modified versions of the Turing test: inverted and displaced. GPT-3.5, GPT-4, and displaced human adjudicators judged whether an agent was human or AI on the basis of a Turing test transcript. We found that both AI and displaced human judges were less accurate than interactive interrogators, with below chance accuracy overall. Moreover, all three judged the best-performing GPT-4 witness to be human more often than human witnesses. This suggests that both humans and current LLMs struggle to distinguish between the two when they are not actively interrogating the person, underscoring an urgent need for more accurate tools to detect AI in conversations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08853v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishika Rathi, Sydney Taylor, Benjamin K. Bergen, Cameron R. Jones</dc:creator>
    </item>
    <item>
      <title>Emerging Practices for Large Multimodal Model (LMM) Assistance for People with Visual Impairments: Implications for Design</title>
      <link>https://arxiv.org/abs/2407.08882</link>
      <description>arXiv:2407.08882v1 Announce Type: new 
Abstract: People with visual impairments perceive their environment non-visually and often use AI-powered assistive tools to obtain textual descriptions of visual information. Recent large vision-language model-based AI-powered tools like Be My AI are more capable of understanding users' inquiries in natural language and describing the scene in audible text; however, the extent to which these tools are useful to visually impaired users is currently understudied. This paper aims to fill this gap. Our study with 14 visually impaired users reveals that they are adapting these tools organically -- not only can these tools facilitate complex interactions in household, spatial, and social contexts, but they also act as an extension of users' cognition, as if the cognition were distributed in the visual information. We also found that although the tools are currently not goal-oriented, users accommodate this limitation and embrace the tools' capabilities for broader use. These findings enable us to envision design implications for creating more goal-oriented, real-time processing, and reliable AI-powered assistive technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08882v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Xie, Rui Yu, He Zhang, Sooyeon Lee, Syed Masum Billah, John M. Carroll</dc:creator>
    </item>
    <item>
      <title>Save A Tree or 6 kg of CO2? Understanding Effective Carbon Footprint Interventions for Eco-Friendly Vehicular Choices</title>
      <link>https://arxiv.org/abs/2407.08897</link>
      <description>arXiv:2407.08897v1 Announce Type: new 
Abstract: From ride-hailing to car rentals, consumers are often presented with eco-friendly options. Beyond highlighting a "green" vehicle and CO2 emissions, CO2 equivalencies have been designed to provide understandable amounts; we ask which equivalencies will lead to eco-friendly decisions. We conducted five ride-hailing scenario surveys where participants picked between regular and eco-friendly options, testing equivalencies, social features, and valence-based interventions. Further, we tested a car-rental embodiment to gauge how an individual (needing a car for several days) might behave versus the immediate ride-hailing context. We find that participants are more likely to choose green rides when presented with additional information about emissions; CO2 by weight was found to be the most effective. Further, we found that information framing - be it individual or collective footprint, positive or negative valence - had an impact on participants' choices. Finally, we discuss how our findings inform the design of effective interventions for reducing car-based carbon-emissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08897v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3544548.3580675</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. (CHI 23), April 23-28, 2023, Hamburg, Germany. ACM, New York, NY, USA</arxiv:journal_reference>
      <dc:creator>Vikram Mohanty, Alexandre Filipowicz, Nayeli Bravo, Scott Carter, David A. Shamma</dc:creator>
    </item>
    <item>
      <title>The Magic XRoom: A Flexible VR Platform for Controlled Emotion Elicitation and Recognition</title>
      <link>https://arxiv.org/abs/2407.09110</link>
      <description>arXiv:2407.09110v1 Announce Type: new 
Abstract: Affective computing has recently gained popularity, especially in the field of human-computer interaction systems, where effectively evoking and detecting emotions is of paramount importance to enhance users experience. However, several issues are hindering progress in the field. In fact, the complexity of emotions makes it difficult to understand their triggers and control their elicitation. Additionally, effective emotion recognition requires analyzing multiple sensor data, such as facial expressions and physiological signals. These factors combined make it hard to collect high-quality datasets that can be used for research purposes (e.g., development of emotion recognition algorithms). Despite these challenges, Virtual Reality (VR) holds promise as a solution. By providing a controlled and immersive environment, VR enables the replication of real-world emotional experiences and facilitates the tracking of signals indicative of emotional states. However, controlling emotion elicitation remains a challenging task also within VR. This research paper introduces the Magic Xroom, a VR platform designed to enhance control over emotion elicitation by leveraging the theory of flow. This theory establishes a mapping between an individuals skill levels, task difficulty, and perceived emotions. In the Magic Xroom, the users skill level is continuously assessed, and task difficulty is adjusted accordingly to evoke specific emotions. Furthermore, user signals are collected using sensors, and virtual panels are utilized to determine the ground truth emotional states, making the Magic Xroom an ideal platform for collecting extensive datasets. The paper provides detailed implementation information, highlights the main properties of the Magic Xroom, and presents examples of virtual scenarios to illustrate its abilities and capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09110v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3565066.3608247</arxiv:DOI>
      <dc:creator>S. M. Hossein Mousavi, Matteo Besenzoni, Davide Andreoletti, Achille Peternier, Silvia Giordano</dc:creator>
    </item>
    <item>
      <title>AI-Powered Immersive Assistance for Interactive Task Execution in Industrial Environments</title>
      <link>https://arxiv.org/abs/2407.09147</link>
      <description>arXiv:2407.09147v1 Announce Type: new 
Abstract: Many industrial sectors rely on well-trained employees that are able to operate complex machinery. In this work, we demonstrate an AI-powered immersive assistance system that supports users in performing complex tasks in industrial environments. Specifically, our system leverages a VR environment that resembles a juice mixer setup. This digital twin of a physical setup simulates complex industrial machinery used to mix preparations or liquids (e.g., similar to the pharmaceutical industry) and includes various containers, sensors, pumps, and flow controllers. This setup demonstrates our system's capabilities in a controlled environment while acting as a proof-of-concept for broader industrial applications. The core components of our multimodal AI assistant are a large language model and a speech-to-text model that process a video and audio recording of an expert performing the task in a VR environment. The video and speech input extracted from the expert's video enables it to provide step-by-step guidance to support users in executing complex tasks. This demonstration showcases the potential of our AI-powered assistant to reduce cognitive load, increase productivity, and enhance safety in industrial environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09147v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomislav Duricic, Peter M\"ullner, Nicole Weidinger, Neven ElSayed, Dominik Kowald, Eduardo Veas</dc:creator>
    </item>
    <item>
      <title>Good Intentions, Risky Inventions: A Method for Assessing the Risks and Benefits of AI in Mobile and Wearable Uses</title>
      <link>https://arxiv.org/abs/2407.09322</link>
      <description>arXiv:2407.09322v1 Announce Type: new 
Abstract: Integrating Artificial Intelligence (AI) into mobile and wearables offers numerous benefits at individual, societal, and environmental levels. Yet, it also spotlights concerns over emerging risks. Traditional assessments of risks and benefits have been sporadic, and often require costly expert analysis. We developed a semi-automatic method that leverages Large Language Models (LLMs) to identify AI uses in mobile and wearables, classify their risks based on the EU AI Act, and determine their benefits that align with globally recognized long-term sustainable development goals; a manual validation of our method by two experts in mobile and wearable technologies, a legal and compliance expert, and a cohort of nine individuals with legal backgrounds who were recruited from Prolific, confirmed its accuracy to be over 85\%. We uncovered that specific applications of mobile computing hold significant potential in improving well-being, safety, and social equality. However, these promising uses are linked to risks involving sensitive data, vulnerable groups, and automated decision-making. To avoid rejecting these risky yet impactful mobile and wearable uses, we propose a risk assessment checklist for the Mobile HCI community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09322v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Constantinides, Edyta Bogucka, Sanja Scepanovic, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>What Do People Think about Sentient AI?</title>
      <link>https://arxiv.org/abs/2407.08867</link>
      <description>arXiv:2407.08867v1 Announce Type: cross 
Abstract: With rapid advances in machine learning, many people in the field have been discussing the rise of digital minds and the possibility of artificial sentience. Future developments in AI capabilities and safety will depend on public opinion and human-AI interaction. To begin to fill this research gap, we present the first nationally representative survey data on the topic of sentient AI: initial results from the Artificial Intelligence, Morality, and Sentience (AIMS) survey, a preregistered and longitudinal study of U.S. public opinion that began in 2021. Across one wave of data collection in 2021 and two in 2023 (total \textit{N} = 3,500), we found mind perception and moral concern for AI well-being in 2021 were higher than predicted and significantly increased in 2023: for example, 71\% agree sentient AI deserve to be treated with respect, and 38\% support legal rights. People have become more threatened by AI, and there is widespread opposition to new technologies: 63\% support a ban on smarter-than-human AI, and 69\% support a ban on sentient AI. Expected timelines are surprisingly short and shortening with a median forecast of sentient AI in only five years and artificial general intelligence in only two years. We argue that, whether or not AIs become sentient, the discussion itself may overhaul human-computer interaction and shape the future trajectory of AI technologies, including existential risks and opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08867v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacy Reese Anthis, Janet V. T. Pauketat, Ali Ladak, Aikaterina Manoli</dc:creator>
    </item>
    <item>
      <title>Analyzing Speech Motor Movement using Surface Electromyography in Minimally Verbal Adults with Autism Spectrum Disorder</title>
      <link>https://arxiv.org/abs/2407.08877</link>
      <description>arXiv:2407.08877v1 Announce Type: cross 
Abstract: Adults who are minimally verbal with autism spectrum disorder (mvASD) have pronounced speech difficulties linked to impaired motor skills. Existing research and clinical assessments primarily use indirect methods such as standardized tests, video-based facial features, and handwriting tasks, which may not directly target speech-related motor skills. In this study, we measure activity from eight facial muscles associated with speech using surface electromyography (sEMG), during carefully designed tasks. The findings reveal a higher power in the sEMG signals and a significantly greater correlation between the sEMG channels in mvASD adults (N=12) compared to age and gender-matched neurotypical controls (N=14). This suggests stronger muscle activation and greater synchrony in the discharge patterns of motor units. Further, eigenvalues derived from correlation matrices indicate lower complexity in muscle coordination in mvASD, implying fewer degrees of freedom in motor control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08877v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wazeer Zulfikar, Nishat Protyasha, Camila Canales, Heli Patel, James Williamson, Laura Sarnie, Lisa Nowinski, Nataliya Kosmyna, Paige Townsend, Sophia Yuditskaya, Tanya Talkar, Utkarsh Oggy Sarawgi, Christopher McDougle, Thomas Quatieri, Pattie Maes, Maria Mody</dc:creator>
    </item>
    <item>
      <title>Prompts First, Finally</title>
      <link>https://arxiv.org/abs/2407.09231</link>
      <description>arXiv:2407.09231v1 Announce Type: cross 
Abstract: Generative AI (GenAI) and large language models in particular, are disrupting Computer Science Education. They are proving increasingly capable at more and more challenges. Some educators argue that they pose a serious threat to computing education, and that we should ban their use in the classroom. While there are serious GenAI issues that remain unsolved, it may be useful in the present moment to step back and examine the overall trajectory of Computer Science writ large. Since the very beginning, our discipline has sought to increase the level of abstraction in each new representation. We have progressed from hardware dip switches, through special purpose languages and visual representations like flow charts, all the way now to ``natural language.'' With the advent of GenAI, students can finally change the abstraction level of a problem to the ``language'' they've been ``problem solving'' with all their lives. In this paper, we argue that our programming abstractions were always headed here -- to natural language. Now is the time to adopt a ``Prompts First'' approach to Computer Science Education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09231v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brent N. Reeves, James Prather, Paul Denny, Juho Leinonen, Stephen MacNeil, Brett A. Becker, Andrew Luxton-Reilly</dc:creator>
    </item>
    <item>
      <title>Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text</title>
      <link>https://arxiv.org/abs/2407.09364</link>
      <description>arXiv:2407.09364v1 Announce Type: cross 
Abstract: The significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09364v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucio La Cava, Davide Costa, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>Tracking Patterns in Toxicity and Antisocial Behavior Over User Lifetimes on Large Social Media Platforms</title>
      <link>https://arxiv.org/abs/2407.09365</link>
      <description>arXiv:2407.09365v1 Announce Type: cross 
Abstract: An increasing amount of attention has been devoted to the problem of "toxic" or antisocial behavior on social media. In this paper we analyze such behavior at very large scales: we analyze toxicity over a 14-year time span on nearly 500 million comments from Reddit and Wikipedia, grounded in two different proxies for toxicity.
  At the individual level, we analyze users' toxicity levels over the course of their time on the site, and find a striking reversal in trends: both Reddit and Wikipedia users tended to become less toxic over their life cycles on the site in the early (pre-2013) history of the site, but more toxic over their life cycles in the later (post-2013) history of the site. We also find that toxicity on Reddit and Wikipedia differ in a key way, with the most toxic behavior on Reddit exhibited in aggregate by the most active users, and the most toxic behavior on Wikipedia exhibited in aggregate by the least active users. Finally, we consider the toxicity of discussion around widely-shared pieces of content, and find that the trends for toxicity in discussion about content bear interesting similarities with the trends for toxicity in discussion by users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09365v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katy Blumer, Jon Kleinberg</dc:creator>
    </item>
    <item>
      <title>Conversational Assistants in Knowledge-Intensive Contexts: An Evaluation of LLM- versus Intent-based Systems</title>
      <link>https://arxiv.org/abs/2402.04955</link>
      <description>arXiv:2402.04955v2 Announce Type: replace 
Abstract: Conversational Assistants (CA) are increasingly supporting human workers in knowledge management. Traditionally, CAs respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing, namely Large Language Models (LLMs), enable CAs to converse in a more flexible, human-like manner, extracting relevant information from texts and capturing information from expert humans but introducing new challenges such as ``hallucinations''. To assess the potential of using LLMs for knowledge management tasks, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and perceived performance than intent-based systems, suggesting that switching NLP techniques can be beneficial in the context of knowledge management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04955v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samuel Kernan Freire, Chaofan Wang, Evangelos Niforatos</dc:creator>
    </item>
    <item>
      <title>BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks</title>
      <link>https://arxiv.org/abs/2404.07387</link>
      <description>arXiv:2404.07387v3 Announce Type: replace 
Abstract: Programmers frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs). However, they encounter difficulties in understanding and working with code produced by LLMs. To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI scaffolds as an intermediate stage between user prompts and code generation. We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code. Through a user study where 10 novices used BISCUIT for machine learning tutorials, we found that BISCUIT offers users representations of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07387v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruijia Cheng, Titus Barik, Alan Leung, Fred Hohman, Jeffrey Nichols</dc:creator>
    </item>
    <item>
      <title>Human-machine social systems</title>
      <link>https://arxiv.org/abs/2402.14410</link>
      <description>arXiv:2402.14410v2 Announce Type: replace-cross 
Abstract: From fake social media accounts and generative-AI chatbots to financial trading algorithms and self-driving vehicles, robots, bots, and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions, and transportation arteries. Networks of multiple interdependent and interacting humans and autonomous machines constitute complex social systems where the collective outcomes cannot be deduced from either human or machine behavior alone. Under this paradigm, we review recent research from across a range of disciplines and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion, and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open-collaboration community, and a discussion forum. To ensure more robust and resilient human-machine communities, researchers should study them using complex-system methods, engineers should explicitly design AI for human-machine and machine-machine interactions, and regulators should govern the ecological diversity and social co-evolution of humans and machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14410v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milena Tsvetkova, Taha Yasseri, Niccolo Pescetelli, Tobias Werner</dc:creator>
    </item>
    <item>
      <title>Establishing a Baseline for Gaze-driven Authentication Performance in VR: A Breadth-First Investigation on a Very Large Dataset</title>
      <link>https://arxiv.org/abs/2404.11798</link>
      <description>arXiv:2404.11798v2 Announce Type: replace-cross 
Abstract: This paper performs the crucial work of establishing a baseline for gaze-driven authentication performance to begin answering fundamental research questions using a very large dataset of gaze recordings from 9202 people with a level of eye tracking (ET) signal quality equivalent to modern consumer-facing virtual reality (VR) platforms. The size of the employed dataset is at least an order-of-magnitude larger than any other dataset from previous related work. Binocular estimates of the optical and visual axes of the eyes and a minimum duration for enrollment and verification are required for our model to achieve a false rejection rate (FRR) of below 3% at a false acceptance rate (FAR) of 1 in 50,000. In terms of identification accuracy which decreases with gallery size, we estimate that our model would fall below chance-level accuracy for gallery sizes of 148,000 or more. Our major findings indicate that gaze authentication can be as accurate as required by the FIDO standard when driven by a state-of-the-art machine learning architecture and a sufficiently large training dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11798v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dillon Lohr, Michael J. Proulx, Oleg Komogortsev</dc:creator>
    </item>
    <item>
      <title>Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks</title>
      <link>https://arxiv.org/abs/2405.10632</link>
      <description>arXiv:2405.10632v5 Announce Type: replace-cross 
Abstract: Model evaluations are central to understanding the safety, risks, and societal impacts of AI systems. While most real-world AI applications involve human-AI interaction, most current evaluations (e.g., common benchmarks) of AI models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations -- "human interaction evaluations" (HIEs) -- which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that HIEs can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused HIE design framework -- containing a human-LLM interaction taxonomy -- with three stages: (1) identifying the risk or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of HIEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10632v5</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujain Ibrahim, Saffron Huang, Lama Ahmad, Markus Anderljung</dc:creator>
    </item>
    <item>
      <title>The Efficacy of Conversational Artificial Intelligence in Rectifying the Theory of Mind and Autonomy Biases: Comparative Analysis</title>
      <link>https://arxiv.org/abs/2406.13813</link>
      <description>arXiv:2406.13813v3 Announce Type: replace-cross 
Abstract: The study evaluates the efficacy of Conversational Artificial Intelligence (CAI) in rectifying cognitive biases and recognizing affect in human-AI interactions, which is crucial for digital mental health interventions. Cognitive biases (systematic deviations from normative thinking) affect mental health, intensifying conditions like depression and anxiety. Therapeutic chatbots can make cognitive-behavioral therapy (CBT) more accessible and affordable, offering scalable and immediate support. The research employs a structured methodology with clinical-based virtual case scenarios simulating typical user-bot interactions. Performance and affect recognition were assessed across two categories of cognitive biases: theory of mind biases (anthropomorphization of AI, overtrust in AI, attribution to AI) and autonomy biases (illusion of control, fundamental attribution error, just-world hypothesis). A qualitative feedback mechanism was used with an ordinal scale to quantify responses based on accuracy, therapeutic quality, and adherence to CBT principles. Therapeutic bots (Wysa, Youper) and general-use LLMs (GTP 3.5, GTP 4, Gemini Pro) were evaluated through scripted interactions, double-reviewed by cognitive scientists and a clinical psychologist. Statistical analysis showed therapeutic bots were consistently outperformed by non-therapeutic bots in bias rectification and in 4 out of 6 biases in affect recognition. The data suggests that non-therapeutic chatbots are more effective in addressing some cognitive biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13813v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcin Rz\k{a}deczka, Anna Sterna, Julia Stoli\'nska, Paulina Kaczy\'nska, Marcin Moskalewicz</dc:creator>
    </item>
  </channel>
</rss>
