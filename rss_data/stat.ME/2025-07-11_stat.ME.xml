<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Jul 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sampled Grid Pairwise Likelihood (SG-PL): An Efficient Approach for Spatial Regression on Large Data</title>
      <link>https://arxiv.org/abs/2507.07113</link>
      <description>arXiv:2507.07113v1 Announce Type: new 
Abstract: Estimating spatial regression models on large, irregularly structured datasets poses significant computational hurdles. While Pairwise Likelihood (PL) methods offer a pathway to simplify these estimations, the efficient selection of informative observation pairs remains a critical challenge, particularly as data volume and complexity grow. This paper introduces the Sampled Grid Pairwise Likelihood (SG-PL) method, a novel approach that employs a grid-based sampling strategy to strategically select observation pairs. Simulation studies demonstrate SG-PL's principal advantage: a dramatic reduction in computational time -- often by orders of magnitude -- when compared to benchmark methods. This substantial acceleration is achieved with a manageable trade-off in statistical efficiency. An empirical application further validates SG-PL's practical utility. Consequently, SG-PL emerges as a highly scalable and effective tool for spatial analysis on very large datasets, offering a compelling balance where substantial gains in computational feasibility are realized for a limited cost in statistical precision, a trade-off that increasingly favors SG-PL with larger N.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07113v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Arbia, Vincenzo Nardelli, Niccolo Salvini</dc:creator>
    </item>
    <item>
      <title>Debiased Semiparametric Efficient Changes-in-Changes Estimation</title>
      <link>https://arxiv.org/abs/2507.07228</link>
      <description>arXiv:2507.07228v1 Announce Type: new 
Abstract: We introduce a novel extension of the influential changes-in-changes (CiC) framework [Athey and Imbens, 2006] to estimate the average treatment effect on the treated (ATT) and distributional causal estimands in panel data settings with unmeasured confounding. While CiC relaxes the parallel trends assumption inherent in difference-in-differences (DiD), existing approaches typically accommodate only a single scalar unobserved confounder and rely on monotonicity assumptions between the confounder and the outcome. Moreover, current formulations lack inference procedures and theoretical guarantees that accommodate continuous covariates. Motivated by the intricate nature of confounding in empirical applications and the need to incorporate continuous covariates in a principled manner, we make two key contributions in this technical report. First, we establish nonparametric identification under a novel set of assumptions that permit high-dimensional unmeasured confounders and non-monotonic relationships between confounders and outcomes. Second, we construct efficient estimators that are Neyman orthogonal to infinite-dimensional nuisance parameters, facilitating valid inference even in the presence of high-dimensional continuous or discrete covariates and flexible machine learning-based nuisance estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07228v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinghao Sun, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Correction of estimator bias in linear regression with categorical covariates with classification error</title>
      <link>https://arxiv.org/abs/2507.07245</link>
      <description>arXiv:2507.07245v1 Announce Type: new 
Abstract: The objective of this work is to propose an asymptotic correction method for the estimators of parameters from regression models with covariates subject to classification errors. A correction was developed based on the least squares estimators from regression with erroneous covariates, the marginal probability of the true covariates, and the conditional probability of the erroneous covariates given the true covariates. In this way, we can correct these estimators without the need to correct the erroneous covariates or observe the true covariates. We performed simulations to quantify the performance of the proposed corrections, identifying, that correcting the intercept is crucial for a significant improvement in estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07245v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Garcia Dias, Mariana Rodrigues Motta, Alexandre Hild Aono</dc:creator>
    </item>
    <item>
      <title>Stratification-based Instrumental Variable Analysis Framework for Nonlinear Effect Analysis</title>
      <link>https://arxiv.org/abs/2507.07349</link>
      <description>arXiv:2507.07349v1 Announce Type: new 
Abstract: Nonlinear causal effects are prevalent in many research scenarios involving continuous exposures, and instrumental variables (IVs) can be employed to investigate such effects, particularly in the presence of unmeasured confounders. However, common IV methods for nonlinear effect analysis, such as IV regression or the control-function method, have inherent limitations, leading to either low statistical power or potentially misleading conclusions. In this work, we propose an alternative IV framework for nonlinear effect analysis, which has recently emerged in genetic epidemiology and addresses many of the drawbacks of existing IV methods. This framework enables study of the effect function while avoiding unnecessary model assumptions. In particular, it facilitates the identification of change points or threshold values in causal effects. Through a wide variety of simulations, we demonstrate that our framework outperforms other representative nonlinear IV methods in predicting the effect shape when the instrument is weak and can accurately estimate the effect function as well as identify the change point and predict its value under various structural model and effect shape scenarios. We further apply our framework to assess the nonlinear effect of alcohol consumption on systolic blood pressure using a genetic instrument (i.e. Mendelian randomization) with UK Biobank data. Our analysis detects a threshold beyond which alcohol intake exhibits a clear causal effect on the outcome. Our results are consistent with published medical guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07349v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haodong Tian, Ashish Patel, Stephen Burgess</dc:creator>
    </item>
    <item>
      <title>Estimation of An Infinite Dimensional Transition Probability Matrix Using a Generalized Hierarchical Stick-Breaking Process</title>
      <link>https://arxiv.org/abs/2507.07433</link>
      <description>arXiv:2507.07433v1 Announce Type: new 
Abstract: Markov chains provide a foundational framework for modeling sequential stochastic processes, with the transition probability matrix characterizing the dynamics of state evolution. While classical estimation methods such as maximum likelihood and empirical Bayes approaches are effective in finite-state settings, they become inadequate in applications involving countably infinite or dynamically expanding state spaces, which frequently arise in domains such as natural language processing, population dynamics, and behavioral modeling. In this work, we introduce a novel Bayesian nonparametric framework for estimating infinite-dimensional transition probability matrices by employing a new class of priors, termed the Generalized Hierarchical Stick-Breaking prior. This prior extends traditional Dirichlet process and stick-breaking constructions, enabling highly flexible modelling of transition probability matrices. The proposed approach offers a principled methodology for inferring transition probabilities in settings characterized by sparsity, high dimensionality, and unobserved state spaces, thereby contributing to the advancement of statistical inference for infinite-dimensional transition probability matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07433v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agamani Saha, Souvik Roy</dc:creator>
    </item>
    <item>
      <title>Learnable Retrieval Enhanced Visual-Text Alignment and Fusion for Radiology Report Generation</title>
      <link>https://arxiv.org/abs/2507.07568</link>
      <description>arXiv:2507.07568v1 Announce Type: new 
Abstract: Automated radiology report generation is essential for improving diagnostic efficiency and reducing the workload of medical professionals. However, existing methods face significant challenges, such as disease class imbalance and insufficient cross-modal fusion. To address these issues, we propose the learnable Retrieval Enhanced Visual-Text Alignment and Fusion (REVTAF) framework, which effectively tackles both class imbalance and visual-text fusion in report generation. REVTAF incorporates two core components: (1) a Learnable Retrieval Enhancer (LRE) that utilizes semantic hierarchies from hyperbolic space and intra-batch context through a ranking-based metric. LRE adaptively retrieves the most relevant reference reports, enhancing image representations, particularly for underrepresented (tail) class inputs; and (2) a fine-grained visual-text alignment and fusion strategy that ensures consistency across multi-source cross-attention maps for precise alignment. This component further employs an optimal transport-based cross-attention mechanism to dynamically integrate task-relevant textual knowledge for improved report generation. By combining adaptive retrieval with multi-source alignment and fusion, REVTAF achieves fine-grained visual-text integration under weak image-report level supervision while effectively mitigating data imbalance issues. The experiments demonstrate that REVTAF outperforms state-of-the-art methods, achieving an average improvement of 7.4% on the MIMIC-CXR dataset and 2.9% on the IU X-Ray dataset. Comparisons with mainstream multimodal LLMs (e.g., GPT-series models), further highlight its superiority in radiology report generation https://github.com/banbooliang/REVTAF-RRG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07568v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qin Zhou, Guoyan Liang, Xindi Li, Jingyuan Chen, Wang Zhe, Chang Yao, Sai Wu</dc:creator>
    </item>
    <item>
      <title>Semantic-guided Masked Mutual Learning for Multi-modal Brain Tumor Segmentation with Arbitrary Missing Modalities</title>
      <link>https://arxiv.org/abs/2507.07592</link>
      <description>arXiv:2507.07592v1 Announce Type: new 
Abstract: Malignant brain tumors have become an aggressive and dangerous disease that leads to death worldwide.Multi-modal MRI data is crucial for accurate brain tumor segmentation, but missing modalities common in clinical practice can severely degrade the segmentation performance. While incomplete multi-modal learning methods attempt to address this, learning robust and discriminative features from arbitrary missing modalities remains challenging. To address this challenge, we propose a novel Semantic-guided Masked Mutual Learning (SMML) approach to distill robust and discriminative knowledge across diverse missing modality scenarios.Specifically, we propose a novel dual-branch masked mutual learning scheme guided by Hierarchical Consistency Constraints (HCC) to ensure multi-level consistency, thereby enhancing mutual learning in incomplete multi-modal scenarios. The HCC framework comprises a pixel-level constraint that selects and exchanges reliable knowledge to guide the mutual learning process. Additionally, it includes a feature-level constraint that uncovers robust inter-sample and inter-class relational knowledge within the latent feature space. To further enhance multi-modal learning from missing modality data, we integrate a refinement network into each student branch. This network leverages semantic priors from the Segment Anything Model (SAM) to provide supplementary information, effectively complementing the masked mutual learning strategy in capturing auxiliary discriminative knowledge. Extensive experiments on three challenging brain tumor segmentation datasets demonstrate that our method significantly improves performance over state-of-the-art methods in diverse missing modality settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07592v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guoyan Liang, Qin Zhou, Jingyuan Chen, Bingcang Huang, Kai Chen, Lin Gu, Zhe Wang, Sai Wu, Chang Yao</dc:creator>
    </item>
    <item>
      <title>Advancing Medical Image Segmentation via Self-supervised Instance-adaptive Prototype Learning</title>
      <link>https://arxiv.org/abs/2507.07602</link>
      <description>arXiv:2507.07602v1 Announce Type: new 
Abstract: Medical Image Segmentation (MIS) plays a crucial role in medical therapy planning and robot navigation. Prototype learning methods in MIS focus on generating segmentation masks through pixel-to-prototype comparison. However, current approaches often overlook sample diversity by using a fixed prototype per semantic class and neglect intra-class variation within each input. In this paper, we propose to generate instance-adaptive prototypes for MIS, which integrates a common prototype proposal (CPP) capturing common visual patterns and an instance-specific prototype proposal (IPP) tailored to each input. To further account for the intra-class variation, we propose to guide the IPP generation by re-weighting the intermediate feature map according to their confidence scores. These confidence scores are hierarchically generated using a transformer decoder. Additionally we introduce a novel self-supervised filtering strategy to prioritize the foreground pixels during the training of the transformer decoder. Extensive experiments demonstrate favorable performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07602v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guoyan Liang, Qin Zhou, Jingyuan Chen, Zhe Wang, Chang Yao</dc:creator>
    </item>
    <item>
      <title>Vecchia approximated Bayesian heteroskedastic Gaussian processes</title>
      <link>https://arxiv.org/abs/2507.07815</link>
      <description>arXiv:2507.07815v1 Announce Type: new 
Abstract: Many computer simulations are stochastic and exhibit input dependent noise. In such situations, heteroskedastic Gaussian processes (hetGPs) make ideal surrogates as they estimate a latent, non-constant variance. However, existing hetGP implementations are unable to deal with large simulation campaigns and use point-estimates for all unknown quantities, including latent variances. This limits applicability to small experiments and undercuts uncertainty. We propose a Bayesian hetGP using elliptical slice sampling (ESS) for posterior variance integration, and the Vecchia approximation to circumvent computational bottlenecks. We show good performance for our upgraded hetGP capability, compared to alternatives, on a benchmark example and a motivating corpus of more than 9-million lake temperature simulations. An open source implementation is provided as bhetGP on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07815v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parul V. Patil, Robert B. Gramacy, Cayelan C. Carey, R. Quinn Thomas</dc:creator>
    </item>
    <item>
      <title>A new coefficient to measure agreement between continuous variables</title>
      <link>https://arxiv.org/abs/2507.07913</link>
      <description>arXiv:2507.07913v1 Announce Type: new 
Abstract: Assessing agreement between two instruments is crucial in clinical studies to evaluate the similarity between two methods measuring the same subjects. This paper introduces a novel coefficient, termed rho1, to measure agreement between continuous variables, focusing on scenarios where two instruments measure experimental units in a study. Unlike existing coefficients, rho1 is based on L1 distances, making it robust to outliers and not relying on nuisance parameters. The coefficient is derived for bivariate normal and elliptically contoured distributions, showcasing its versatility. In the case of normal distributions, rho1 is linked to Lin's coefficient, providing a useful alternative. The paper includes theoretical properties, an inference framework, and numerical experiments to validate the performance of rho1. This novel coefficient presents a valuable tool for researchers assessing agreement between continuous variables in various fields, including clinical studies and spatial analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07913v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ronny Vallejos, Felipe Osorio, Clemente Ferrer</dc:creator>
    </item>
    <item>
      <title>Late Fusion Multi-task Learning for Semiparametric Inference with Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2507.07941</link>
      <description>arXiv:2507.07941v1 Announce Type: new 
Abstract: In the age of large and heterogeneous datasets, the integration of information from diverse sources is essential to improve parameter estimation. Multi-task learning offers a powerful approach by enabling simultaneous learning across related tasks. In this work, we introduce a late fusion framework for multi-task learning with semiparametric models that involve infinite-dimensional nuisance parameters, focusing on applications such as heterogeneous treatment effect estimation across multiple data sources, including electronic health records from different hospitals or clinical trial data. Our framework is two-step: first, initial double machine-learning estimators are obtained through individual task learning; second, these estimators are adaptively aggregated to exploit task similarities while remaining robust to task-specific differences. In particular, the framework avoids individual level data sharing, preserving privacy. Additionally, we propose a novel multi-task learning method for nuisance parameter estimation, which further enhances parameter estimation when nuisance parameters exhibit similarity across tasks. We establish theoretical guarantees for the method, demonstrating faster convergence rates compared to individual task learning when tasks share similar parametric components. Extensive simulations and real data applications complement the theoretical findings of our work while highlight the effectiveness of our framework even in moderate sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07941v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sohom Bhattacharya, Yongzhuo Chen, Muxuan Liang</dc:creator>
    </item>
    <item>
      <title>Class conditional conformal prediction for multiple inputs by p-value aggregation</title>
      <link>https://arxiv.org/abs/2507.07150</link>
      <description>arXiv:2507.07150v1 Announce Type: cross 
Abstract: Conformal prediction methods are statistical tools designed to quantify uncertainty and generate predictive sets with guaranteed coverage probabilities. This work introduces an innovative refinement to these methods for classification tasks, specifically tailored for scenarios where multiple observations (multi-inputs) of a single instance are available at prediction time. Our approach is particularly motivated by applications in citizen science, where multiple images of the same plant or animal are captured by individuals. Our method integrates the information from each observation into conformal prediction, enabling a reduction in the size of the predicted label set while preserving the required class-conditional coverage guarantee. The approach is based on the aggregation of conformal p-values computed from each observation of a multi-input. By exploiting the exact distribution of these p-values, we propose a general aggregation framework using an abstract scoring function, encompassing many classical statistical tools. Knowledge of this distribution also enables refined versions of standard strategies, such as majority voting. We evaluate our method on simulated and real data, with a particular focus on Pl@ntNet, a prominent citizen science platform that facilitates the collection and identification of plant species through user-submitted images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07150v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Baptiste Fermanian (IMAG, IROKO), Mohamed Hebiri (LAMA), Joseph Salmon (IMAG, IROKO)</dc:creator>
    </item>
    <item>
      <title>TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores</title>
      <link>https://arxiv.org/abs/2507.07276</link>
      <description>arXiv:2507.07276v1 Announce Type: cross 
Abstract: Along with accurate prediction, understanding the contribution of each feature to the making of the prediction, i.e., the importance of the feature, is a desirable and arguably necessary component of a machine learning model. For a complex model such as a random forest, such importances are not innate -- as they are, e.g., with linear regression. Efficient methods have been created to provide such capabilities, with one of the most popular among them being permutation feature importance due to its efficiency, model-agnostic nature, and perceived intuitiveness. However, permutation feature importance has been shown to be misleading in the presence of dependent features as a result of the creation of unrealistic observations when permuting the dependent features. In this work, we develop TRIP (Test for Reliable Interpretation via Permutation), a test requiring minimal assumptions that is able to detect unreliable permutation feature importance scores that are the result of model extrapolation. To build on this, we demonstrate how the test can be complemented in order to allow its use in high dimensional settings. Through testing on simulated data and applications, our results show that the test can be used to reliably detect when permutation feature importance scores are unreliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07276v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Foote, Danny Krizanc</dc:creator>
    </item>
    <item>
      <title>Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning</title>
      <link>https://arxiv.org/abs/2507.07359</link>
      <description>arXiv:2507.07359v1 Announce Type: cross 
Abstract: We present GO-CBED, a goal-oriented Bayesian framework for sequential causal experimental design. Unlike conventional approaches that select interventions aimed at inferring the full causal model, GO-CBED directly maximizes the expected information gain (EIG) on user-specified causal quantities of interest, enabling more targeted and efficient experimentation. The framework is both non-myopic, optimizing over entire intervention sequences, and goal-oriented, targeting only model aspects relevant to the causal query. To address the intractability of exact EIG computation, we introduce a variational lower bound estimator, optimized jointly through a transformer-based policy network and normalizing flow-based variational posteriors. The resulting policy enables real-time decision-making via an amortized network. We demonstrate that GO-CBED consistently outperforms existing baselines across various causal reasoning and discovery tasks-including synthetic structural causal models and semi-synthetic gene regulatory networks-particularly in settings with limited experimental budgets and complex causal mechanisms. Our results highlight the benefits of aligning experimental design objectives with specific research goals and of forward-looking sequential planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07359v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheyu Zhang (University of Michigan), Jiayuan Dong (University of Michigan), Jie Liu (University of Michigan), Xun Huan (University of Michigan)</dc:creator>
    </item>
    <item>
      <title>Feature-free regression kriging</title>
      <link>https://arxiv.org/abs/2507.07382</link>
      <description>arXiv:2507.07382v1 Announce Type: cross 
Abstract: Spatial interpolation is a crucial task in geography. As perhaps the most widely used interpolation methods, geostatistical models -- such as Ordinary Kriging (OK) -- assume spatial stationarity, which makes it difficult to capture the nonstationary characteristics of geographic variables. A common solution is trend surface modeling (e.g., Regression Kriging, RK), which relies on external explanatory variables to model the trend and then applies geostatistical interpolation to the residuals. However, this approach requires high-quality and readily available explanatory variables, which are often lacking in many spatial interpolation scenarios -- such as estimating heavy metal concentrations underground. This study proposes a Feature-Free Regression Kriging (FFRK) method, which automatically extracts geospatial features -- including local dependence, local heterogeneity, and geosimilarity -- to construct a regression-based trend surface without requiring external explanatory variables. We conducted experiments on the spatial distribution prediction of three heavy metals in a mining area in Australia. In comparison with 17 classical interpolation methods, the results indicate that FFRK, which does not incorporate any explanatory variables and relies solely on extracted geospatial features, consistently outperforms both conventional Kriging techniques and machine learning models that depend on explanatory variables. This approach effectively addresses spatial nonstationarity while reducing the cost of acquiring explanatory variables, improving both prediction accuracy and generalization ability. This finding suggests that an accurate characterization of geospatial features based on domain knowledge can significantly enhance spatial prediction performance -- potentially yielding greater improvements than merely adopting more advanced statistical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07382v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Luo, Yilong Wu, Yongze Song</dc:creator>
    </item>
    <item>
      <title>Non-conjugate variational Bayes for pseudo-likelihood mixed effect models</title>
      <link>https://arxiv.org/abs/2206.09444</link>
      <description>arXiv:2206.09444v4 Announce Type: replace 
Abstract: We propose a unified, yet simple to code, non-conjugate variational Bayes algorithm for posterior approximation of generic Bayesian generalized mixed effect models. Specifically, we consider regression models identified by a linear predictor, eventually transformed using a bijective link, where the prediction misfit is measured using, possibly non-differentiable, loss functions. Examples include generalized linear models, quasi-likelihood models, and robust regression. To address the limitations of non-conjugate settings, we employ an efficient message passing optimization strategy under a Gaussian variational approximation of the posterior. The resulting algorithms automatically account for non-conjugate priors and non-smooth losses, without requiring model-specific data-augmented representations. Besides the general formulation, we provide closed-form updates for popular model specifications, including quantile regression and support vector machines. Overall, theoretical and empirical results highlight the effectiveness of the proposed method, demonstrating its computational efficiency and approximation accuracy as an alternative to existing Bayesian techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09444v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2527925</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 1-18 (2025)</arxiv:journal_reference>
      <dc:creator>Cristian Castiglione, Mauro Bernardi</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Learning for Spatiotemporal Mechanistic Models</title>
      <link>https://arxiv.org/abs/2208.06528</link>
      <description>arXiv:2208.06528v5 Announce Type: replace 
Abstract: We develop an approach for Bayesian learning of spatiotemporal dynamical mechanistic models. Such learning consists of statistical emulation of the mechanistic system that can efficiently interpolate the output of the system from arbitrary inputs. The emulated learner can then be used to train the system from noisy data achieved by melding information from observed data with the emulated mechanistic system. This joint melding of mechanistic systems employ hierarchical state-space models with Gaussian process regression. Assuming the dynamical system is controlled by a finite collection of inputs, Gaussian process regression learns the effect of these parameters through a number of training runs, driving the stochastic innovations of the spatiotemporal state-space component. This enables efficient modeling of the dynamics over space and time. This article details exact inference with analytically accessible posterior distributions in hierarchical matrix-variate Normal and Wishart models in designing the emulator. This step obviates expensive iterative algorithms such as Markov chain Monte Carlo or variational approximations. We also show how emulation is applicable to large-scale emulation by designing a dynamic Bayesian transfer learning framework. Inference on mechanistic model parameters proceeds using Markov chain Monte Carlo as a post-emulation step using the emulator as a regression component. We demonstrate this framework through solving inverse problems arising in the analysis of ordinary and partial nonlinear differential equations and, in addition, to a black-box computer model generating spatiotemporal dynamics across a graphical model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.06528v5</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sudipto Banerjee, Xiang Chen, Ian Frankenburg, Daniel Zhou</dc:creator>
    </item>
    <item>
      <title>Batch Predictive Inference</title>
      <link>https://arxiv.org/abs/2409.13990</link>
      <description>arXiv:2409.13990v5 Announce Type: replace 
Abstract: Constructing prediction sets with coverage guarantees for unobserved outcomes is a core problem in modern statistics. Methods for predictive inference have been developed for a wide range of settings, but usually only consider test data points one at a time. Here we study the problem of distribution-free predictive inference for a batch of multiple test points, aiming to construct prediction sets for functions -- such as the mean or median -- of any number of unobserved test datapoints. This setting includes constructing simultaneous prediction sets with a high probability of coverage, and selecting datapoints satisfying a specified condition while controlling the number of false claims.
  For the general task of predictive inference on a function of a batch of test points, we introduce a methodology called batch predictive inference (batch PI), and provide a distribution-free coverage guarantee under exchangeability of the calibration and test data. Batch PI requires the quantiles of a rank ordering function defined on certain subsets of ranks. While computing these quantiles is NP-hard in general, we show that it can be done efficiently in many cases of interest, most notably for batch score functions with a compositional structure -- which includes examples of interest such as the mean -- via a dynamic programming algorithm that we develop. Batch PI has advantages over naive approaches (such as partitioning the calibration data or directly extending conformal prediction) in many settings, as it can deliver informative prediction sets even using small calibration sample sizes. We illustrate that our procedures provide informative inference across the use cases mentioned above, through experiments on both simulated data and a drug-target interaction dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13990v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Handling Missingness, Failures, and Non-Convergence in Simulation Studies: A Review of Current Practices and Recommendations</title>
      <link>https://arxiv.org/abs/2409.18527</link>
      <description>arXiv:2409.18527v3 Announce Type: replace 
Abstract: Simulation studies are commonly used in methodological research for the empirical evaluation of data analysis methods. They generate artificial data sets under specified mechanisms and compare the performance of methods across conditions. However, simulation repetitions do not always produce valid outputs, e.g., due to non-convergence or other algorithmic failures. This phenomenon complicates the interpretation of results, especially when its occurrence differs between methods and conditions. Despite the potentially serious consequences of such "missingness", quantitative data on its prevalence and specific guidance on how to deal with it are currently limited. To this end, we reviewed 482 simulation studies published in various methodological journals and systematically assessed the prevalence and handling of missingness. We found that only 23% (111/482) of the reviewed simulation studies mention missingness, with even fewer reporting frequency (92/482 = 19%) or how it was handled (67/482 = 14%). We propose a classification of missingness and possible solutions. We give various recommendations, most notably to always quantify and report missingness, even if none was observed, to align missingness handling with study goals, and to share code and data for reproduction and reanalysis. Using a case study on publication bias adjustment methods, we illustrate common pitfalls and solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18527v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel, Franti\v{s}ek Barto\v{s}, Bj\"orn S. Siepe, Anna Lohmann</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Functional Principal Components Analysis</title>
      <link>https://arxiv.org/abs/2412.11340</link>
      <description>arXiv:2412.11340v3 Announce Type: replace 
Abstract: Functional Principal Components Analysis (FPCA) is a widely used analytic tool for dimension reduction of functional data. Traditional implementations of FPCA estimate the principal components from the data, then treat these estimates as fixed in subsequent analyses. To account for the uncertainty of PC estimates, we propose FAST, a fully-Bayesian FPCA with three core components: (1) projection of eigenfunctions onto an orthonormal spline basis; (2) efficient sampling of the orthonormal spline coefficient matrix using polar decomposition; and (3) ordering eigenvalues during sampling. Extensive simulation studies show that FAST is very stable and performs better compared to existing methods. FAST is motivated by and applied to a study of the variability in mealtime glucose from the Dietary Approaches to Stop Hypertension for Diabetes Continuous Glucose Monitoring (DASH4D CGM) study. All relevant STAN code and simulation routines are available as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11340v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Sartini, Xinkai Zhou, Liz Selvin, Scott Zeger, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Spatial vertical regression for spatial panel data: Evaluating the effect of the Florentine tramway's first line on commercial vitality</title>
      <link>https://arxiv.org/abs/2505.00450</link>
      <description>arXiv:2505.00450v2 Announce Type: replace 
Abstract: Synthetic control methods are commonly used in panel data settings to evaluate the effect of an intervention. In many of these cases, the treated and control units correspond to spatial units such as regions or neighborhoods. Our approach addresses the challenge of understanding how an intervention applied at specific locations influences the surrounding area. Traditional synthetic control applications may struggle with defining the effective area of impact, the extent of treatment propagation across space, and the variation of effects with distance from the treatment sites. To address these challenges, we introduce Spatial Vertical Regression (SVR) within the Bayesian paradigm. This innovative approach allows us to accurately predict the outcomes in varying proximities to the treatment sites, while meticulously accounting for the spatial structure inherent in the data. Specifically, rooted on the vertical regression framework of the synthetic control method, SVR employs a Gaussian process to ensure that the imputation of missing potential outcomes for areas of different distance around the treatment sites is spatially coherent, reflecting the expectation that nearby areas experience similar outcomes and have similar relationships to control areas. This approach is particularly pertinent to our study on the Florentine tramway's first line construction. We study its influence on the local commercial landscape, focusing on how business prevalence varies at different distances from the tram stops.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00450v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Alessandra Mattei, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>Coefficient Shape Transfer Learning for Functional Linear Regression</title>
      <link>https://arxiv.org/abs/2506.11367</link>
      <description>arXiv:2506.11367v2 Announce Type: replace 
Abstract: In this paper, we develop a novel transfer learning methodology to tackle the challenge of data scarcity in functional linear models. The methodology incorporates samples from the target model (target domain) alongside those from auxiliary models (source domains), transferring knowledge of coefficient shape from the source domains to the target domain. This shape-based knowledge transfer offers two key advantages. First, it is robust to covariate scaling, ensuring effectiveness despite variations in data distributions across different source domains. Second, the notion of coefficient shape homogeneity represents a meaningful advance beyond traditional coefficient homogeneity, allowing the method to exploit a wider range of source domains and achieve significantly improved model estimation. We rigorously analyze the convergence rates of the proposed estimator and examine the minimax optimality. Our findings show that the degree of improvement depends not only on the similarity of coefficient shapes between the target and source domains, but also on coefficient magnitudes and the spectral decay rates of the functional covariates covariance operators. To address situations where only a subset of auxiliary models is informative for the target model, we further develop a data-driven procedure for identifying such informative sources. The effectiveness of the proposed methodology is demonstrated through comprehensive simulation studies and an application to occupation time analysis using physical activity data from the U.S. National Health and Nutrition Examination Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11367v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Jiao, Ian W. Mckeague, N. -H. Chan</dc:creator>
    </item>
    <item>
      <title>Structural Classification of Locally Stationary Time Series Based on Second-order Characteristics</title>
      <link>https://arxiv.org/abs/2507.04237</link>
      <description>arXiv:2507.04237v2 Announce Type: replace 
Abstract: Time series classification is crucial for numerous scientific and engineering applications. In this article, we present a numerically efficient, practically competitive, and theoretically rigorous classification method for distinguishing between two classes of locally stationary time series based on their time-domain, second-order characteristics. Our approach builds on the autoregressive approximation for locally stationary time series, combined with an ensemble aggregation and a distance-based threshold for classification. It imposes no requirement on the training sample size, and is shown to achieve zero misclassification error rate asymptotically when the underlying time series differ only mildly in their second-order characteristics. The new method is demonstrated to outperform a variety of state-of-the-art solutions, including wavelet-based, tree-based, convolution-based methods, as well as modern deep learning methods, through intensive numerical simulations and a real EEG data analysis for epilepsy classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04237v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Qian, Xiucai Ding, Lexin Li</dc:creator>
    </item>
    <item>
      <title>Convexity Not Required: Estimation of Smooth Moment Condition Models</title>
      <link>https://arxiv.org/abs/2304.14386</link>
      <description>arXiv:2304.14386v2 Announce Type: replace-cross 
Abstract: Generalized and Simulated Method of Moments are often used to estimate structural Economic models. Yet, it is commonly reported that optimization is challenging because the corresponding objective function is non-convex. For smooth problems, this paper shows that convexity is not required: under conditions involving the Jacobian of the moments, certain algorithms are globally convergent. These include a gradient-descent and a Gauss-Newton algorithm with appropriate choice of tuning parameters. The results are robust to 1) non-convexity, 2) one-to-one moderately non-linear reparameterizations, and 3) moderate misspecification. The conditions preclude non-global optima. Numerical and empirical examples illustrate the condition, non-convexity, and convergence properties of different optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.14386v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron, Liang Zhong</dc:creator>
    </item>
    <item>
      <title>No $D_{\text{train}}$: Model-Agnostic Counterfactual Explanations Using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2405.18563</link>
      <description>arXiv:2405.18563v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) methods have experienced significant growth in the past decade, yet their practical application in high-impact real-world domains has been hindered by their opacity. When ML methods are responsible for making critical decisions, stakeholders often require insights into how to alter these decisions. Counterfactual explanations (CFEs) have emerged as a solution, offering interpretations of opaque ML models and providing a pathway to transition from one decision to another. However, most existing CFE methods require access to the model's training dataset, few methods can handle multivariate time-series, and none of model-agnostic CFE methods can handle multivariate time-series without training datasets. These limitations can be formidable in many scenarios. In this paper, we present NTD-CFE, a novel model-agnostic CFE method based on reinforcement learning (RL) that generates CFEs when training datasets are unavailable. NTD-CFE is suitable for both static and multivariate time-series datasets with continuous and discrete features. NTD-CFE reduces the CFE search space from a multivariate time-series domain to a lower dimensional space and addresses the problem using RL. Users have the flexibility to specify non-actionable, immutable, and preferred features, as well as causal constraints. We demonstrate the performance of NTD-CFE against four baselines on several datasets and find that, despite not having access to a training dataset, NTD-CFE finds CFEs that make significantly fewer and significantly smaller changes to the input time-series. These properties make CFEs more actionable, as the magnitude of change required to alter an outcome is vastly reduced. The code is available in the supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18563v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiangyu Sun, Raquel Aoki, Kevin H. Wilson</dc:creator>
    </item>
    <item>
      <title>Parametric Scaling Law of Tuning Bias in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2502.03023</link>
      <description>arXiv:2502.03023v2 Announce Type: replace-cross 
Abstract: Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional holdout set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03023v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zeng, Kangdao Liu, Bingyi Jing, Hongxin Wei</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Confidence</title>
      <link>https://arxiv.org/abs/2502.10653</link>
      <description>arXiv:2502.10653v2 Announce Type: replace-cross 
Abstract: This paper introduces a framework for selecting policies that maximize expected welfare under estimation uncertainty. The proposed method explicitly balances the size of the estimated welfare against the uncertainty inherent in its estimation, ensuring that chosen policies meet a reporting guarantee, namely, that actual welfare is guaranteed not to fall below the reported estimate with a pre-specified confidence level. We produce the efficient decision frontier, describing policies that offer maximum estimated welfare for a given acceptable level of estimation risk. We apply this approach to a variety of settings, including the selection of policy rules that allocate individuals to treatments and the allocation of limited budgets among competing social programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10653v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Sokbae Lee, Adam M. Rosen, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation</title>
      <link>https://arxiv.org/abs/2507.02275</link>
      <description>arXiv:2507.02275v2 Announce Type: replace-cross 
Abstract: Structure-agnostic causal inference studies how well one can estimate a treatment effect given black-box machine learning estimates of nuisance functions (like the impact of confounders on treatment and outcomes). Here, we find that the answer depends in a surprising way on the distribution of the treatment noise. Focusing on the partially linear model of \citet{robinson1988root}, we first show that the widely adopted double machine learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise, resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for independent non-Gaussian treatment noise, we show that DML is always suboptimal by constructing new practical procedures with higher-order robustness to nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant estimators to achieve $r$-th order insensitivity to nuisance errors whenever the $(r+1)$-st treatment cumulant is non-zero. We complement these core results with novel minimax guarantees for binary treatments in the partially linear model. Finally, using synthetic demand estimation experiments, we demonstrate the practical benefits of our higher-order robust estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02275v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Lester Mackey, Vasilis Syrgkanis</dc:creator>
    </item>
  </channel>
</rss>
