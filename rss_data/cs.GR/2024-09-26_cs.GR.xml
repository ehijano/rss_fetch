<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 01:59:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing</title>
      <link>https://arxiv.org/abs/2409.16629</link>
      <description>arXiv:2409.16629v1 Announce Type: new 
Abstract: We present a novel approach to synthesize dexterous motions for physically simulated hands in tasks that require coordination between the control of two hands with high temporal precision. Instead of directly learning a joint policy to control two hands, our approach performs bimanual control through cooperative learning where each hand is treated as an individual agent. The individual policies for each hand are first trained separately, and then synchronized through latent space manipulation in a centralized environment to serve as a joint policy for two-hand control. By doing so, we avoid directly performing policy learning in the joint state-action space of two hands with higher dimensions, greatly improving the overall training efficiency. We demonstrate the effectiveness of our proposed approach in the challenging guitar-playing task. The virtual guitarist trained by our approach can synthesize motions from unstructured reference data of general guitar-playing practice motions, and accurately play diverse rhythms with complex chord pressing and string picking patterns based on the input guitar tabs that do not exist in the references. Along with this paper, we provide the motion capture data that we collected as the reference for policy training. Code is available at: https://pei-xu.github.io/guitar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16629v1</guid>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680528.3687692</arxiv:DOI>
      <dc:creator>Pei Xu, Ruocheng Wang</dc:creator>
    </item>
    <item>
      <title>pyGANDALF -- An open-source, Geometric, ANimation, Directed, Algorithmic, Learning Framework for Computer Graphics</title>
      <link>https://arxiv.org/abs/2409.16724</link>
      <description>arXiv:2409.16724v1 Announce Type: new 
Abstract: In computer graphics (CG) education, the challenge of finding modern, versatile tools is significant, particularly when integrating both legacy and advanced technologies. Traditional frameworks, often reliant on solid, yet outdated APIs like OpenGL, limit the exploration of cutting-edge graphics techniques. To address this, we introduce pyGANDALF, a unique, lightweight, open-source CG framework built on three pillars: Entity-Component-System (ECS) architecture, Python programming, and WebGPU integration. This combination sets pyGANDALF apart by providing a streamlined ECS design with an editor layer, compatibility with WebGPU for state-of-the-art features like compute and ray tracing pipelines, and a programmer-friendly Python environment. The framework supports modern features, such as Physically Based Rendering (PBR) capabilities and integration with Universal Scene Description (USD) formats, making it suitable for both educational demonstrations and real-world applications. Evaluations by expert users confirmed that pyGANDALF effectively balances ease of use with advanced functionality, preparing students for contemporary CG development challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16724v1</guid>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Petropoulos, Manos Kamarianakis, Antonis Protopsaltis, George Papagiannakis</dc:creator>
    </item>
    <item>
      <title>Rapid Prototyping of 3D Microstructures: A Simplified Grayscale Lithography Encoding Method Using Blender</title>
      <link>https://arxiv.org/abs/2409.16749</link>
      <description>arXiv:2409.16749v1 Announce Type: new 
Abstract: The democratization of fabrication equipment has spurred recent interest in maskless grayscale lithography for both 2D and 3D microfabrication. However, the design of suitable template images remains a challenge. This work presents a simplified method for encoding 3D objects into grayscale image files optimized for grayscale lithography. Leveraging the widely used, open-source 3D modeling software Blender, we developed a robust approach to convert geometric heights into grayscale levels and generate image files through top-view rendering. Our method accurately reproduced the overall shape of simple structures like stairs and ramps compared to the original designs. We extended this approach to complex 3D sinusoidal surfaces, achieving similar results. Given the increasing accessibility and user-friendliness of digital rendering tools, this study offers a promising strategy for rapid prototyping of initial designs with minimal effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16749v1</guid>
      <category>cs.GR</category>
      <category>physics.app-ph</category>
      <category>physics.optics</category>
      <category>q-bio.CB</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabricio Frizera Borghi, Mohammed Bendimerad, Marie-Ly Chapon, Tatiana Petithory, Laurent Vonna, Laurent Pieuchot</dc:creator>
    </item>
    <item>
      <title>GenCAD: Image-Conditioned Computer-Aided Design Generation with Transformer-Based Contrastive Representation and Diffusion Priors</title>
      <link>https://arxiv.org/abs/2409.16294</link>
      <description>arXiv:2409.16294v1 Announce Type: cross 
Abstract: The creation of manufacturable and editable 3D shapes through Computer-Aided Design (CAD) remains a highly manual and time-consuming task, hampered by the complex topology of boundary representations of 3D solids and unintuitive design tools. This paper introduces GenCAD, a generative model that employs autoregressive transformers and latent diffusion models to transform image inputs into parametric CAD command sequences, resulting in editable 3D shape representations. GenCAD integrates an autoregressive transformer-based architecture with a contrastive learning framework, enhancing the generation of CAD programs from input images and providing a representation learning framework for multiple data modalities relevant to engineering designs. Extensive evaluations demonstrate that GenCAD significantly outperforms existing state-of-the-art methods in terms of the precision and modifiability of generated 3D shapes. Notably, GenCAD shows a marked improvement in the accuracy of 3D shape generation for long sequences, supporting its application in complex design tasks. Additionally, the contrastive embedding feature of GenCAD facilitates the retrieval of CAD models using image queries from databases which is a critical challenge within the CAD community. While most work in the 3D shape generation literature focuses on representations like meshes, voxels, or point clouds, practical engineering applications demand modifiability and the ability for multi-modal conditional generation. Our results provide a significant step forward in this direction, highlighting the potential of generative models to expedite the entire design-to-production pipeline and seamlessly integrate different design modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16294v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Ferdous Alam, Faez Ahmed</dc:creator>
    </item>
    <item>
      <title>LiDAR-3DGS: LiDAR Reinforced 3D Gaussian Splatting for Multimodal Radiance Field Rendering</title>
      <link>https://arxiv.org/abs/2409.16296</link>
      <description>arXiv:2409.16296v1 Announce Type: cross 
Abstract: In this paper, we explore the capabilities of multimodal inputs to 3D Gaussian Splatting (3DGS) based Radiance Field Rendering. We present LiDAR-3DGS, a novel method of reinforcing 3DGS inputs with LiDAR generated point clouds to significantly improve the accuracy and detail of 3D models. We demonstrate a systematic approach of LiDAR reinforcement to 3DGS to enable capturing of important features such as bolts, apertures, and other details that are often missed by image-based features alone. These details are crucial for engineering applications such as remote monitoring and maintenance. Without modifying the underlying 3DGS algorithm, we demonstrate that even a modest addition of LiDAR generated point cloud significantly enhances the perceptual quality of the models. At 30k iterations, the model generated by our method resulted in an increase of 7.064% in PSNR and 0.565% in SSIM, respectively. Since the LiDAR used in this research was a commonly used commercial-grade device, the improvements observed were modest and can be further enhanced with higher-grade LiDAR systems. Additionally, these improvements can be supplementary to other derivative works of Radiance Field Rendering and also provide a new insight for future LiDAR and computer vision integrated modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16296v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansol Lim, Hanbeom Chang, Jongseong Brad Choi, Chul Min Yeum</dc:creator>
    </item>
    <item>
      <title>Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model</title>
      <link>https://arxiv.org/abs/2409.16689</link>
      <description>arXiv:2409.16689v1 Announce Type: cross 
Abstract: Layout generation is a task to synthesize a harmonious layout with elements characterized by attributes such as category, position, and size. Human designers experiment with the placement and modification of elements to create aesthetic layouts, however, we observed that current discrete diffusion models (DDMs) struggle to correct inharmonious layouts after they have been generated. In this paper, we first provide novel insights into layout sticking phenomenon in DDMs and then propose a simple yet effective layout-assessment module Layout-Corrector, which works in conjunction with existing DDMs to address the layout sticking problem. We present a learning-based module capable of identifying inharmonious elements within layouts, considering overall layout harmony characterized by complex composition. During the generation process, Layout-Corrector evaluates the correctness of each token in the generated layout, reinitializing those with low scores to the ungenerated state. The DDM then uses the high-scored tokens as clues to regenerate the harmonized tokens. Layout-Corrector, tested on common benchmarks, consistently boosts layout-generation performance when in conjunction with various state-of-the-art DDMs. Furthermore, our extensive analysis demonstrates that the Layout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates control over the fidelity-diversity trade-off, and (3) significantly mitigates the performance drop associated with fast sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16689v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shoma Iwai, Atsuki Osanai, Shunsuke Kitada, Shinichiro Omachi</dc:creator>
    </item>
    <item>
      <title>Limitations of (Procrustes) Alignment in Assessing Multi-Person Human Pose and Shape Estimation</title>
      <link>https://arxiv.org/abs/2409.16861</link>
      <description>arXiv:2409.16861v1 Announce Type: cross 
Abstract: We delve into the challenges of accurately estimating 3D human pose and shape in video surveillance scenarios. Beginning with the advocacy for metrics like W-MPJPE and W-PVE, which omit the (Procrustes) realignment step, to improve model evaluation, we then introduce RotAvat. This technique aims to enhance these metrics by refining the alignment of 3D meshes with the ground plane. Through qualitative comparisons, we demonstrate RotAvat's effectiveness in addressing the limitations of existing aproaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16861v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Drazic Martin, Pierre Perrault</dc:creator>
    </item>
    <item>
      <title>Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model</title>
      <link>https://arxiv.org/abs/2409.16938</link>
      <description>arXiv:2409.16938v1 Announce Type: cross 
Abstract: Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object insertion in 3D content represented by Gaussian Splatting. Our approach introduces a multi-view diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable video diffusion model to facilitate view-consistent object inpainting. Within MVInpainter, we incorporate a ControlNet-based conditional injection module to enable controlled and more predictable multi-view generation. After generating the multi-view inpainted results, we further propose a mask-aware 3D reconstruction technique to refine Gaussian Splatting reconstruction from these sparse inpainted views. By leveraging these fabricate techniques, our approach yields diverse results, ensures view-consistent and harmonious insertions, and produces better object quality. Extensive experiments demonstrate that our approach outperforms existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16938v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongliang Zhong, Can Wang, Jingbo Zhang, Jing Liao</dc:creator>
    </item>
    <item>
      <title>Go-SLAM: Grounded Object Segmentation and Localization with Gaussian Splatting SLAM</title>
      <link>https://arxiv.org/abs/2409.16944</link>
      <description>arXiv:2409.16944v1 Announce Type: cross 
Abstract: We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting SLAM to reconstruct dynamic environments while embedding object-level information within the scene representations. This framework employs advanced object segmentation techniques, assigning a unique identifier to each Gaussian splat that corresponds to the object it represents. Consequently, our system facilitates open-vocabulary querying, allowing users to locate objects using natural language descriptions. Furthermore, the framework features an optimal path generation module that calculates efficient navigation paths for robots toward queried objects, considering obstacles and environmental uncertainties. Comprehensive evaluations in various scene settings demonstrate the effectiveness of our approach in delivering high-fidelity scene reconstructions, precise object segmentation, flexible object querying, and efficient robot path planning. This work represents an additional step forward in bridging the gap between 3D scene reconstruction, semantic object understanding, and real-time environment interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16944v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phu Pham, Dipam Patel, Damon Conover, Aniket Bera</dc:creator>
    </item>
    <item>
      <title>Text2CAD: Generating Sequential CAD Models from Beginner-to-Expert Level Text Prompts</title>
      <link>https://arxiv.org/abs/2409.17106</link>
      <description>arXiv:2409.17106v1 Announce Type: cross 
Abstract: Prototyping complex computer-aided design (CAD) models in modern softwares can be very time-consuming. This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts. We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels. Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The dataset contains $\sim170$K models and $\sim660$K text annotations, from abstract CAD descriptions (e.g., generate two concentric cylinders) to detailed specifications (e.g., draw two circles with center $(x,y)$ and radius $r_{1}$, $r_{2}$, and extrude along the normal by $d$...). Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts. We evaluate the performance of our model through a mixture of metrics, including visual quality, parametric precision, and geometrical accuracy. Our proposed framework shows great potential in AI-aided design applications. Our source code and annotations will be publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17106v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Sadil Khan, Sankalp Sinha, Talha Uddin Sheikh, Didier Stricker, Sk Aziz Ali, Muhammad Zeshan Afzal</dc:creator>
    </item>
    <item>
      <title>DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion</title>
      <link>https://arxiv.org/abs/2409.17145</link>
      <description>arXiv:2409.17145v1 Announce Type: cross 
Abstract: Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17145v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu</dc:creator>
    </item>
    <item>
      <title>Efficient Nearest Neighbor Search Using Dynamic Programming</title>
      <link>https://arxiv.org/abs/2409.15023</link>
      <description>arXiv:2409.15023v3 Announce Type: replace-cross 
Abstract: When dealing with point clouds distributed on manifold surfaces in 3D space, or when the query point is far from the data, the efficiency of traditional nearest neighbor search algorithms (e.g., KD Tree and R Tree) may degrade. In extreme cases, the complexity of the query can approach O(n). In this paper, we propose a novel dynamic programming technique that precomputes a Directed Acyclic Graph (DAG) to enable more efficient nearest neighbor queries for 2D manifold data. By leveraging this structure, only a small number of distance comparisons between point pairs are required to accurately identify the nearest neighbor. Extensive experimental results demonstrate that our method achieves query speeds that are 1x-10x faster than traditional methods. Moreover, our algorithm exhibits significant potential. It achieves query efficiency comparable to KD-trees on uniformly distributed point clouds. Additionally, our algorithm supports nearest neighbor queries among the first k points. Coupled with our algorithm, a farthest point sampling algorithm with lower complexity can also be implemented. Furthermore, our method has the potential to support nearest neighbor queries with different types of primitives and distance metrics. We believe that the method proposed in this paper represents the most concise and straightforward exact nearest neighbor search algorithm currently available, and it will contribute significantly to advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15023v3</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Wang, Jiantao Song, Shiqing Xin, Shuangmin Chen, Changhe Tu, Wenping Wang, Jiaye Wang</dc:creator>
    </item>
  </channel>
</rss>
