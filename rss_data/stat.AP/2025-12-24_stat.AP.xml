<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 02:15:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Energy Independence: Critical Minerals in the Indian Context</title>
      <link>https://arxiv.org/abs/2512.20317</link>
      <description>arXiv:2512.20317v1 Announce Type: new 
Abstract: The global impetus for extracting rare earth elements (REEs) is shaping the future of green technologies. From high-efficiency magnets in wind turbines to advanced batteries and solar photovoltaics, REEs are indispensable for a greener world through the energy transition. However, supply chains remain a barrier for the majority of the global population. India is mainly dependent on imports for most REEs. Innovation and recycling efforts in most REEs are still in their early stages. For India, aspiring to Viksit Bharat by 2047, securing sustainable REE access is critical to national energy security and technological independence. This paper explores India's opportunities and challenges in the REE domain, high-lighting underutilized resources such as copper tailings, fly ash, and e-waste. We argue for circular economy pathways that can reduce environmental impacts and strengthen domestic supply. Hindustan Copper Limited (HCL), as India's sole vertically integrated copper producer, is uniquely positioned to pioneer co-recovery of REEs, advance research and development partnerships, and build a comprehensive supply chain. By embedding sustainability, ESG principles, and community trust in its strategy, HCL can evolve into a national champion in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20317v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Ranjan Kumar, Sekhar Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Topic-informed dynamic mixture model for occupational heterogeneity in health risk behaviors</title>
      <link>https://arxiv.org/abs/2512.20408</link>
      <description>arXiv:2512.20408v1 Announce Type: new 
Abstract: Behavioral risk factors, i.e., smoking, poor nutrition, alcohol misuse, and physical inactivity (SNAP), are leading contributors to chronic diseases and healthcare costs worldwide. Their prevalence is shaped %not only by demographic characteristics %but and also by contextual ones such as socioeconomic and occupational environments. In this study, we leverage data from the Italian health and behavioral surveillance system PASSI to model SNAP behaviors through a Bayesian framework that integrates textual information on occupations. We use Structural Topic Modeling (STM) to cluster free-text job descriptions into latent occupational groups, which inform mixture weights in a multivariate ordered probit model. Covariate effects are allowed to vary across occupational clusters and evolve over time. To enhance interpretability and variable selection, we impose non-local spike-and-slab priors on regression coefficients. Finally, an online learning algorithm based on sequential Monte Carlo enables efficient updating as new data become available. This dynamic, scalable, and interpretable approach permits observing how occupational contexts modulate the impact of socio-demographic factors on health behaviors, providing valuable insights for targeted public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20408v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Schiavon, Mattia Stival, Angela Andreella, Stefano Campostrini</dc:creator>
    </item>
    <item>
      <title>DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</title>
      <link>https://arxiv.org/abs/2512.19744</link>
      <description>arXiv:2512.19744v1 Announce Type: cross 
Abstract: We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p&lt;0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent''), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at https://github.com/deepbridge/deepbridge, with complete documentation at https://deepbridge.readthedocs.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19744v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gustavo Coelho Haase, Paulo Henrique Dourado da Silva</dc:creator>
    </item>
    <item>
      <title>RANSAC Scoring Functions: Analysis and Reality Check</title>
      <link>https://arxiv.org/abs/2512.19850</link>
      <description>arXiv:2512.19850v1 Announce Type: cross 
Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19850v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>A. Shekhovtsov</dc:creator>
    </item>
    <item>
      <title>Multidimensional Stochastic Dominance Test Based on Center-outward Quantiles</title>
      <link>https://arxiv.org/abs/2512.19966</link>
      <description>arXiv:2512.19966v1 Announce Type: cross 
Abstract: Stochastic dominance (SD) provides a quantile-based partial ordering of random variables and has broad applications. Its extension to multivariate settings, however, is challenging due to the lack of a canonical ordering in $\mathbb{R}^d$ ($d \ge 2$) and the set-valued character of multivariate quantiles. Based on the multivariate center-outward quantile function in Hallin et al. (2021), this paper proposes new first- and second-order multivariate stochastic dominance (MSD) concepts through comparing contribution functions defined over quantile contours and regions. To address computational and inferential challenges, we incorporate entropy-regularized optimal transport, which ensures faster convergence rate and tractable estimation. We further develop consistent Kolmogorov-Smirnov and Cram\'er- von Mises type test statistics for MSD, establish bootstrap validity, and demonstrate through extensive simulations good finite-sample performance of the tests. Our approach offers a theoretically rigorous, and computationally feasible solution for comparing multivariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19966v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Ma, Hang Liu, Weiwei Zhuang</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Causal Explainability</title>
      <link>https://arxiv.org/abs/2512.20219</link>
      <description>arXiv:2512.20219v2 Announce Type: cross 
Abstract: Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20219v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihan Zhang, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Adaptive Multi-task Learning for Probabilistic Load Forecasting</title>
      <link>https://arxiv.org/abs/2512.20232</link>
      <description>arXiv:2512.20232v1 Announce Type: cross 
Abstract: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20232v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onintze Zaballa, Ver\'onica \'Alvarez, Santiago Mazuelas</dc:creator>
    </item>
    <item>
      <title>Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</title>
      <link>https://arxiv.org/abs/2512.20363</link>
      <description>arXiv:2512.20363v1 Announce Type: cross 
Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $\alpha$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20363v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</dc:creator>
    </item>
    <item>
      <title>Generalized method of L-moment estimation for stationary and nonstationary extreme value models</title>
      <link>https://arxiv.org/abs/2512.20385</link>
      <description>arXiv:2512.20385v1 Announce Type: cross 
Abstract: Precisely estimating out-of-sample upper quantiles is very important in risk assessment and in engineering practice for structural design to prevent a greater disaster. For this purpose, the generalized extreme value (GEV) distribution has been broadly used. To estimate the parameters of GEV distribution, the maximum likelihood estimation (MLE) and L-moment estimation (LME) methods have been primarily employed. For a better estimation using the MLE, several studies considered the generalized MLE (penalized likelihood or Bayesian) methods to cooperate with a penalty function or prior information for parameters. However, a generalized LME method for the same purpose has not been developed yet in the literature. We thus propose the generalized method of L-moment estimation (GLME) to cooperate with a penalty function or prior information. The proposed estimation is based on the generalized L-moment distance and a multivariate normal likelihood approximation. Because the L-moment estimator is more efficient and robust for small samples than the MLE, we reasonably expect the advantages of LME to continue to hold for GLME. The proposed method is applied to the stationary and nonstationary GEV models with two novel (data-adaptive) penalty functions to correct the bias of LME. A simulation study indicates that the biases of LME are considerably corrected by the GLME with slight increases in the standard error. Applications to US flood damage data and maximum rainfall at Phliu Agromet in Thailand illustrate the usefulness of the proposed method. This study may promote further work on penalized or Bayesian inferences based on L-moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20385v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yonggwan Shin, Yire Shin, Jihong Park, Jeong-Soo Park</dc:creator>
    </item>
    <item>
      <title>The Aligned Economic Index &amp; The State Switching Model</title>
      <link>https://arxiv.org/abs/2512.20460</link>
      <description>arXiv:2512.20460v1 Announce Type: cross 
Abstract: A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20460v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.5933216</arxiv:DOI>
      <arxiv:journal_reference>Financieel Forum Bank en Financiewezen 2020 3 pp 252-261</arxiv:journal_reference>
      <dc:creator>Ilias Aarab</dc:creator>
    </item>
    <item>
      <title>Switching between states and the COVID-19 turbulence</title>
      <link>https://arxiv.org/abs/2512.20477</link>
      <description>arXiv:2512.20477v1 Announce Type: cross 
Abstract: In Aarab (2020), I examine U.S. stock return predictability across economic regimes and document evidence of time-varying expected returns across market states in the long run. The analysis introduces a state-switching specification in which the market state is proxied by the slope of the yield curve, and proposes an Aligned Economic Index built from the popular predictors of Welch and Goyal (2008) (augmented with bond and equity premium measures). The Aligned Economic Index under the state-switching model exhibits statistically and economically meaningful in-sample ($R^2 = 5.9\%$) and out-of-sample ($R^2_{\text{oos}} = 4.12\%$) predictive power across both recessions and expansions, while outperforming a range of widely used predictors. In this work, I examine the added value for professional practitioners by computing the economic gains for a mean-variance investor and find substantial added benefit of using the new index under the state switching model across all market states. The Aligned Economic Index can thus be implemented on a consistent real-time basis. These findings are crucial for both academics and practitioners as expansions are much longer-lived than recessions. Finally, I extend the empirical exercises by incorporating data through September 2020 and document sizable gains from using the Aligned Economic Index, relative to more traditional approaches, during the COVID-19 market turbulence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20477v1</guid>
      <category>q-fin.ST</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Financieel Forum Bank en Financiewezen 2021 1 pp 10-20</arxiv:journal_reference>
      <dc:creator>Ilias Aarab</dc:creator>
    </item>
    <item>
      <title>Digital N-of-1 Trials and their Application in Experimental Physiology</title>
      <link>https://arxiv.org/abs/2412.15076</link>
      <description>arXiv:2412.15076v3 Announce Type: replace 
Abstract: Traditionally, studies in experimental physiology have been conducted in small groups of human participants, animal models or cell lines. Identifying optimal study designs that achieve sufficient power for drawing proper statistical inferences to detect group level effects with small sample sizes has been challenging. Moreover, average effects derived from traditional group-level inference do not necessarily apply to individual participants. Here, we introduce N-of-1 trials as an innovative study design that can be used to draw valid statistical inference about the effects of interventions on individual participants and can be aggregated across multiple study participants to provide population-level inferences more efficiently than standard group randomized trials. N-of-1 trials have been used since the late 1980s, but without large-scale adoption and with few applications in experimental physiology research settings. In this manuscript, we introduce the key components and design features of N-of-1 trials, describe statistical analysis and interpretations of the results, and describe some available digital tools to facilitate their use using examples from experimental physiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15076v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Konigorski, Mathias Ried-Larsen, Christopher H Schmid</dc:creator>
    </item>
    <item>
      <title>The Impact of Question Framing on the Performance of Automatic Occupation Coding</title>
      <link>https://arxiv.org/abs/2501.05584</link>
      <description>arXiv:2501.05584v3 Announce Type: replace 
Abstract: Occupational data play a vital role in research, official statistics, and policymaking, yet their collection and accurate classification remain a challenge. This study investigates the effects of occupational question wording on data variability and the performance of automatic coding tools. We conducted and replicated a split-ballot survey experiment in Germany using two common occupational question formats: one focusing on "job title" (Berufsbezeichnung) and another on "berufliche T\"atigkeit" (loosely translated as occupation or occupational task). Our analysis reveals that automatic coding tools, such as CASCOT and OccuCoDe, exhibit sensitivity to the form and origin of the data. Specifically, these tools were more efficient when coding responses to the job title question format than the occupational task format, suggesting a potential way to improve the respective questions for many German surveys. In a subsequent "detailed tasks and duties" question, providing a guiding example prompted respondents to give longer answers without broadening the range of unique words they used. These findings highlight the importance of harmonising survey questions and and ensuring that automatic coding tools are robust to differences in question wording. Further research is needed to optimise question design and coding tools for greater accuracy and applicability in occupational data collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05584v3</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Olga Kononykhina, Frauke Kreuter, Malte Schierholz</dc:creator>
    </item>
    <item>
      <title>Modeling and forecasting of European Carbon Emission Allowance futures by ARIMA-TX-GARCH models with correlation threshold</title>
      <link>https://arxiv.org/abs/2510.07568</link>
      <description>arXiv:2510.07568v3 Announce Type: replace 
Abstract: We propose an ARIMA-TX-GARCH model and use it to forecast European Carbon Emission Allowance futures prices, incorporating Brent crude oil futures prices as an exogenous variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07568v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaeho Lee, Eunju Hwang</dc:creator>
    </item>
    <item>
      <title>Do Generalized-Gamma Scale Mixtures of Normals Fit Large Image Datasets?</title>
      <link>https://arxiv.org/abs/2512.17038</link>
      <description>arXiv:2512.17038v2 Announce Type: replace 
Abstract: A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17038v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brandon Marks, Yash Dave, Zixun Wang, Hannah Chung, Riya Patwa, Simon Cha, Michael Murphy, Alexander Strang</dc:creator>
    </item>
    <item>
      <title>Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models</title>
      <link>https://arxiv.org/abs/2512.17119</link>
      <description>arXiv:2512.17119v2 Announce Type: replace 
Abstract: This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17119v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Pardo, Juan Sosa, Juan Pablo Torres-Clavijo, Andr\'es Felipe Ar\'evalo-Ar\'evalo</dc:creator>
    </item>
    <item>
      <title>Unraveling time-varying causal effects of multiple exposures: integrating Functional Data Analysis with Multivariable Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2512.19064</link>
      <description>arXiv:2512.19064v2 Announce Type: replace 
Abstract: Mendelian Randomization is a widely used instrumental variable method for assessing causal effects of lifelong exposures on health outcomes. Many exposures, however, have causal effects that vary across the life course and often influence outcomes jointly with other exposures or indirectly through mediating pathways. Existing approaches to multivariable Mendelian Randomization assume constant effects over time and therefore fail to capture these dynamic relationships. We introduce Multivariable Functional Mendelian Randomization (MV-FMR), a new framework that extends functional Mendelian Randomization to simultaneously model multiple time-varying exposures. The method combines functional principal component analysis with a data-driven cross-validation strategy for basis selection and accounts for overlapping instruments and mediation effects. Through extensive simulations, we assessed MV-FMR's ability to recover time-varying causal effects under a range of data-generating scenarios and compared the performance of joint versus separate exposure effect estimation strategies. Across scenarios involving nonlinear effects, horizontal pleiotropy, mediation, and sparse data, MV-FMR consistently recovered the true causal functions and outperformed univariable approaches. To demonstrate its practical value, we applied MV-FMR to UK Biobank data to investigate the time-varying causal effects of systolic blood pressure and body mass index on coronary artery disease. MV-FMR provides a flexible and interpretable framework for disentangling complex time-dependent causal processes and offers new opportunities for identifying life-course critical periods and actionable drivers relevant to disease prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19064v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicole Fontana, Francesca Ieva, Luisa Zuccolo, Emanuele Di Angelantonio, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>Hypergraph Representations of scRNA-seq Data for Improved Clustering with Random Walks</title>
      <link>https://arxiv.org/abs/2501.11760</link>
      <description>arXiv:2501.11760v4 Announce Type: replace-cross 
Abstract: Analysis of single-cell RNA sequencing data is often conducted through network projections such as coexpression networks, primarily due to the abundant availability of network analysis tools for downstream tasks. However, this approach has several limitations: loss of higher-order information, inefficient data representation caused by converting a sparse dataset to a fully connected network, and overestimation of coexpression due to zero-inflation. To address these limitations, we propose conceptualizing scRNA-seq expression data as hypergraphs, which are generalized graphs in which the hyperedges can connect more than two vertices. In the context of scRNA-seq data, the hypergraph nodes represent cells and the edges represent genes. Each hyperedge connects all cells where its corresponding gene is actively expressed and records the expression of the gene across different cells. This hypergraph conceptualization enables us to explore multi-way relationships beyond the pairwise interactions in coexpression networks without loss of information. We propose two novel clustering methods: (1) the Dual-Importance Preference Hypergraph Walk (DIPHW) and (2) the Coexpression and Memory-Integrated Dual-Importance Preference Hypergraph Walk (CoMem-DIPHW). They outperform established methods on both simulated and real scRNA-seq datasets. The improvement brought by our proposed methods is especially significant when data modularity is weak. Furthermore, CoMem-DIPHW incorporates the gene coexpression network, cell coexpression network, and the cell-gene expression hypergraph from the single-cell abundance counts data altogether for embedding computation. This approach accounts for both the local level information from single-cell level gene expression and the global level information from the pairwise similarity in the two coexpression networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11760v4</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan He, Daniel I. Bolnick, Samuel V. Scarpino, Tina Eliassi-Rad</dc:creator>
    </item>
  </channel>
</rss>
