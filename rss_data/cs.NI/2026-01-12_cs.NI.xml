<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 03:30:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>2BRobust -- Overcoming TCP BBR Performance Degradation in Virtual Machines under CPU Contention</title>
      <link>https://arxiv.org/abs/2601.05665</link>
      <description>arXiv:2601.05665v1 Announce Type: new 
Abstract: Motivated by the recent introduction and large-scale deployment of BBR congestion control algorithms, multiple studies have investigated the performance and fairness implications of this shift from loss-based to delay-based congestion control. Given the potential Internet-wide adoption of BBR, we must also consider its robustness in network and system scenarios. One such scenario is Cloud-based Virtual Machine (VM) networking - highly relevant in today's CDN-centric Internet. Interestingly, previous work has shown significant performance problems of BBRv1-2 running in Xen VMs, with BBR performance dropping to almost zero when CPU credit is low. In this paper, we develop a framework for measuring TCP throughput under fully controlled CPU contention, which uses Linux deadline scheduling to emulate generalized CPU contention conditions. Our measurements reveal that - in stark contrast to Cubic! - BBR throughput can break down during CPU contention under any hypervisor and all tested BDP conditions. Characterizing this performance degradation on a fine-granular level, we show that CPU limited BBR senders are capped at very low throughput levels below 10-20 Mbps. This finding implies that an Internet-wide shift from Cubic to BBR could harm the Internet's overall robustness, if not deployed with caution. To detect and overcome CPU-limited throughput, we propose a minimal BBR patch which detects the problematic situation by monitoring inflight bytes and reacts by increasing the pacing rate to make better use of the available CPU time. We show that our BBR patch overcomes the throughput problem for the most critical cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05665v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kathrin Elmenhorst, Nils Aschenbruck</dc:creator>
    </item>
    <item>
      <title>AWaRe-SAC: Proactive Slice Admission Control under Weather-Induced Capacity Uncertainty</title>
      <link>https://arxiv.org/abs/2601.05978</link>
      <description>arXiv:2601.05978v1 Announce Type: new 
Abstract: As emerging applications demand higher throughput and lower latencies, operators are increasingly deploying millimeter-wave (mmWave) links within x-haul transport networks, spanning fronthaul, midhaul, and backhaul segments. However, the inherent susceptibility of mmWave frequencies to weather-related attenuation, particularly rain fading, complicates the maintenance of stringent Quality of Service (QoS) requirements. This creates a critical challenge: making admission decisions under uncertainty regarding future network capacity. To address this, we develop a proactive slice admission control framework for mmWave x-haul networks subject to rain-induced fluctuations. Our objective is to improve network performance, ensure QoS, and optimize revenue, thereby surpassing the limitations of standard reactive approaches. The proposed framework integrates a deep learning predictor of future network conditions with a proactive Q-learning-based slice admission control mechanism. We validate our solution using real-world data from a mmWave x-haul deployment in a dense urban area, incorporating realistic models of link capacity attenuation and dynamic slice demands. Extensive evaluations demonstrate that our proactive solution achieves 2-3x higher long-term average revenue under dynamic link conditions, providing a scalable and resilient framework for adaptive admission control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05978v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dror Jacoby, Yanzhi Li, Shuyue Yu, Nicola Di Cicco, Hagit Messer, Gil Zussman, Igor Kadota</dc:creator>
    </item>
    <item>
      <title>SPARK: Sparse Parametric Antenna Representation using Kernels</title>
      <link>https://arxiv.org/abs/2601.05440</link>
      <description>arXiv:2601.05440v1 Announce Type: cross 
Abstract: Channel state information (CSI) acquisition and feedback overhead grows with the number of antennas, users, and reported subbands. This growth becomes a bottleneck for many antenna and reconfigurable intelligent surface (RIS) systems as arrays and user densities scale. Practical CSI feedback and beam management rely on codebooks, where beams are selected via indices rather than explicitly transmitting radiation patterns. Hardware-aware operation requires an explicit representation of the measured antenna/RIS response, yet high-fidelity measured patterns are high-dimensional and costly to handle. We present SPARK (Sparse Parametric Antenna Representation using Kernels), a training-free compression model that decomposes patterns into a smooth global base and sparse localized lobes. For 3D patterns, SPARK uses low-order spherical harmonics for global directivity and anisotropic Gaussian kernels for localized features. For RIS 1D azimuth cuts, it uses a Fourier-series base with 1D Gaussians. On patterns from the AERPAW testbed and a public RIS dataset, SPARK achieves up to 2.8$\times$ and 10.4$\times$ reductions in reconstruction MSE over baselines, respectively. Simulation shows that amortizing a compact pattern description and reporting sparse path descriptors can produce 12.65% mean uplink goodput gain under a fixed uplink budget. Overall, SPARK turns dense patterns into compact, parametric models for scalable, hardware-aware beam management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05440v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Bjorndahl, Mark O'Hair, Ben Zoghi, Joseph Camp</dc:creator>
    </item>
    <item>
      <title>LACIN: Linearly Arranged Complete Interconnection Networks</title>
      <link>https://arxiv.org/abs/2601.05668</link>
      <description>arXiv:2601.05668v1 Announce Type: cross 
Abstract: Several interconnection networks are based on the complete graph topology. Networks with a moderate size can be based on a single complete graph. However, large-scale networks such as Dragonfly and HyperX use, respectively, a hierarchical or a multi-dimensional composition of complete graphs.
  The number of links in these networks is huge and grows rapidly with their size. This paper introduces LACIN, a set of complete graph implementations that use identically indexed ports to link switches. This way of implementing the network reduces the complexity of its cabling and its routing. LACIN eases the deployment of networks for parallel computers of different scales, from VLSI systems to the largest supercomputers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05668v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LCA.2025.3649284</arxiv:DOI>
      <arxiv:journal_reference>Architecture Letters, vol., no. 01, pp. 1-4, PrePrints 5555</arxiv:journal_reference>
      <dc:creator>Ram\'on Beivide (Universidad de Cantabria, SPAIN, Barcelona Supercomputing Center, SPAIN), Crist\'obal Camarero (Universidad de Cantabria, SPAIN), Carmen Mart\'inez (Universidad de Cantabria, SPAIN), Enrique Vallejo (Universidad de Cantabria, SPAIN), Mateo Valero (Barcelona Supercomputing Center, SPAIN)</dc:creator>
    </item>
    <item>
      <title>Age of Gossip With Cellular Drone Mobility</title>
      <link>https://arxiv.org/abs/2601.05983</link>
      <description>arXiv:2601.05983v1 Announce Type: cross 
Abstract: We consider a cellular network containing $n$ nodes where nodes within a cell gossip with each other in a fully-connected fashion and a source shares updates with these nodes via a mobile drone. The mobile drone receives updates directly from the source and shares them with nodes in the cell where it currently resides. The drone moves between cells according to an underlying continuous-time Markov chain (CTMC). In this work, we evaluate the impact of the number of cells $f(n)$, drone speed $\lambda_m(n)$ and drone dissemination rate $\lambda_d(n)$ on the freshness of information of nodes in the network. We utilize the version age of information metric to quantify the freshness of information. We observe that the expected duration between two drone-to-cell service times depends on the stationary distribution of the underlying CTMC and $\lambda_d(n)$, but not on $\lambda_m(n)$. However, the version age instability in slow moving CTMCs makes high probability analysis for a general underlying CTMC difficult. Therefore, next we focus on the fully-connected drone mobility model. Under this model, we uncover a dual-bottleneck between drone mobility and drone dissemination speed: the version age is constrained by the slower of these two processes. If $\lambda_d(n) \gg \lambda_m(n)$, then the version age scaling of nodes is dominated by the inverse of $\lambda_m(n)$ and is independent of $\lambda_d(n)$. If $\lambda_m(n) \gg \lambda_d(n)$, then the version age scaling of nodes is dominated by the inverse of $\lambda_d(n)$ and is independent of $\lambda_m(n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05983v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arunabh Srivastava, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Inter-Satellite Link Configuration for Fast Delivery in Low-Earth-Orbit Constellations</title>
      <link>https://arxiv.org/abs/2511.15861</link>
      <description>arXiv:2511.15861v2 Announce Type: replace 
Abstract: End-to-end latency in large low-Earth-orbit (LEO) constellations is dominated by propagation delay, making total delay roughly proportional to the network diameter, the longest shortest path in hops. Current inter-satellite link (ISL) layouts have rarely been optimized to minimize network diameter while simultaneously satisfying physical and operational constraints, including maximum link distance, line-of-sight, per-satellite hardware limits, and long-term link viability over orbital periods. In this study, the selection and assignment of inter-plane ISLs is formulated as a diameter-minimization problem on a Starlink-inspired Walker-Delta constellation in which each satellite is equipped with two fixed intra-plane links and may activate up to two inter-plane links. Beginning with a feasible baseline, the topology is iteratively refined by a local-search procedure that replaces or reinforces links to shrink the diameter. The resulting ISL configuration meets all geometric and hardware limits, preserves link stability across multiple orbital periods, and yields a sparse, diameter-aware graph with potential for centralized routing capabilities. Simulations demonstrate that the proposed algorithm achieves low worst-case latency without compromising ISL stability, and the trade-off between hop count and long-term link stability is empirically measured for guidance of future LEO network deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15861v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arman Mollakhani, Jerayu Tiamraj, Shu-Jie Cao, Dongning Guo</dc:creator>
    </item>
    <item>
      <title>Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning</title>
      <link>https://arxiv.org/abs/2505.00918</link>
      <description>arXiv:2505.00918v3 Announce Type: replace-cross 
Abstract: IoT networks often face conflicting routing goals such as maximizing packet delivery, minimizing delay, and conserving limited battery energy. These priorities can also change dynamically: for example, an emergency alert requires high reliability, while routine monitoring prioritizes energy efficiency to prolong network lifetime. Existing works, including many deep reinforcement learning approaches, are typically centralized and assume static objectives, making them slow to adapt when preferences shift. We propose a dynamic and fully distributed multi-objective Q-learning routing algorithm that learns multiple per-preference Q-tables in parallel and introduces a novel greedy interpolation policy to act near-optimally for unseen preferences without retraining or central coordination. A theoretical analysis further shows that the optimal value function is Lipschitz-continuous in the preference parameter, ensuring that the proposed greedy interpolation policy yields provably near-optimal behavior. Simulations show that our approach adapts in real time to shifting priorities and achieves up to 80-90\% lower energy consumption and more than 2-5x higher cumulative rewards and packet delivery compared to six baseline protocols, under dynamic and distributed settings. Sensitivity analysis across varying preference window lengths confirms that the proposed DPQ framework consistently achieves higher composite reward than all baseline methods, demonstrating robustness to changes in operating conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00918v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Vaishnav, Praveen Kumar Donta, Sindri Magn\'usson</dc:creator>
    </item>
    <item>
      <title>Collective Communication for 100k+ GPUs</title>
      <link>https://arxiv.org/abs/2510.20171</link>
      <description>arXiv:2510.20171v4 Announce Type: replace-cross 
Abstract: The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20171v4</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Regina Ren, Deep Shah, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Yulun Wang, Bruce Wu, Xinfeng Xie, Jingyi Yang, Mingran Yang, Kenny Yu, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Prashanth Kannan, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Srikanth Sundaresan, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng</dc:creator>
    </item>
  </channel>
</rss>
