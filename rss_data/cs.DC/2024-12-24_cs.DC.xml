<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SYMPHONY: Improving Memory Management for LLM Inference Workloads</title>
      <link>https://arxiv.org/abs/2412.16434</link>
      <description>arXiv:2412.16434v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being deployed in applications such as chatbots, code editors, and conversational agents. A key feature of LLMs is their ability to engage in multi-turn interactions with humans or external tools, enabling a wide range of tasks. Each new request in a multi-turn interaction depends on the intermediate state, specifically the key-value (K,V) caches, from previous requests in the ongoing interaction. Existing serving engines either recompute the K,V caches or offload them to main memory. Profiling reveals that recomputation can result in over 99% of processed tokens being redundant. On the other hand, offloading K,V caches from GPU memory makes inference serving stateful, leading to load imbalances across the cluster. To address these challenges, we developed SYMPHONY. SYMPHONY leverages the observation that multi-turn work loads provide additional hints that allow K,V caches to be migrated off the critical serving path. By utilizing these hints, SYMPHONY dynamically migrates K,V caches to enable finegrained scheduling of inference requests. Our experiments demonstrate that SYMPHONY can handle over 8x the number of requests compared to state-of-the-art baselines, with a similar latency profile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16434v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Agarwal, Anyong Mao, Aditya Akella, Shivaram Venkataraman</dc:creator>
    </item>
    <item>
      <title>Distributed Inference on Mobile Edge and Cloud: A Data-Cartography based Clustering Approach</title>
      <link>https://arxiv.org/abs/2412.16616</link>
      <description>arXiv:2412.16616v1 Announce Type: new 
Abstract: The large size of DNNs poses a significant challenge for deployment on devices with limited resources, such as mobile, edge, and IoT platforms. To address this issue, a distributed inference framework can be utilized. In this framework, a small-scale DNN (initial layers) is deployed on mobile devices, a larger version on edge devices, and the full DNN on the cloud. Samples with low complexity (easy) can be processed on mobile, those with moderate complexity (medium) on edge devices, and high complexity (hard) samples on the cloud. Given that the complexity of each sample is unknown in advance, the crucial question in distributed inference is determining the sample complexity for appropriate DNN processing. We introduce a novel method named \our{}, which leverages the Data Cartography approach initially proposed for enhancing DNN generalization. By employing data cartography, we assess sample complexity. \our{} aims to boost accuracy while considering the offloading costs from mobile to edge/cloud. Our experimental results on GLUE datasets, covering a variety of NLP tasks, indicate that our approach significantly lowers inference costs by more than 43\% while maintaining a minimal accuracy drop of less than 0.5\% compared to performing all inferences on the cloud. The source code is available at https://anonymous.4open.science/r/DIMEC-1B04.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16616v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divya Jyoti Bajpai, Manjesh Kumar Hanawal</dc:creator>
    </item>
    <item>
      <title>Raft Distributed System for Multi-access Edge Computing Sharing Resources</title>
      <link>https://arxiv.org/abs/2412.16774</link>
      <description>arXiv:2412.16774v1 Announce Type: new 
Abstract: Researchers all over the world are employing a variety of analysis approaches in attempt to provide a safer and faster solution for sharing resources via a Multi-access Edge Computing system. Multi-access Edge Computing (MEC) is a job-sharing method within the edge server network whose main aim is to maximize the pace of the computing process, resulting in a more powerful and enhanced user experience. Although there are many other options when it comes to determining the fastest method for computing processes, our paper introduces a rather more extensive change to the system model to assure no data loss and/or task failure due to any scrutiny in the edge node cluster. RAFT, a powerful consensus algorithm, can be used to introduce an auction theory approach in our system, which enables the edge device to make the best decision possible regarding how to respond to a request from the client. Through the use of the RAFT consensus, blockchain may be used to improve the safety, security, and efficiency of applications by deploying it on trustful edge base stations. In addition to discussing the best-distributed system approach for our (MEC) system, a Deep Deterministic Policy Gradient (DDPG) algorithm is also presented in order to reduce overall system latency. Assumed in our proposal is the existence of a cluster of N Edge nodes, each containing a series of tasks that require execution. A DDPG algorithm is implemented in this cluster so that an auction can be held within the cluster of edge nodes to decide which edge node is best suited for performing the task provided by the client.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16774v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zain Khaliq, Ahmed Refaey Hussein</dc:creator>
    </item>
    <item>
      <title>BladeDISC++: Memory Optimizations Based On Symbolic Shape</title>
      <link>https://arxiv.org/abs/2412.16985</link>
      <description>arXiv:2412.16985v1 Announce Type: new 
Abstract: Recent deep learning workloads exhibit dynamic characteristics, leading to the rising adoption of dynamic shape compilers. These compilers can generate efficient kernels for dynamic shape graphs characterized by a fixed graph topology and uncertain tensor shapes. However, memory optimization, although particularly crucial in this large model era, remains relatively underexplored for dynamic shape graphs. The fundamental challenge lies in the lack of precise tensor shapes which are essential in conventional methods such as operation scheduling(op scheduling) and rematerialization. To address this challenge, we propose op scheduling and rematerialization approaches based on symbolic shapes and developed BladeDISC++. Besides, since rematerialization decisions cannot be made solely at compile time when tensor shapes are unknown, BladeDISC++ employs a compilation-runtime combined strategy to optimally address shape dynamics. Evaluations indicate that BladeDISC++ effectively reduces memory usage for dynamic shape graphs, achieving memory consumption comparable to optimizations using precise shapes, thereby promoting the broader adoption of dynamic shape compilers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16985v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>[1]"NeurIPS BladeDISC++: Memory Optimizations Based On Symbolic Shape" Neurips.cc, 2024. https://neurips.cc/virtual/2024/103601 (accessed Dec. 22, 2024)</arxiv:journal_reference>
      <dc:creator>Xiulong Yuan, Xu Yan, Wenting Shen, Xiafei Qiu, Ang Wang, Jie Zhang, Yong Li, Wei Lin</dc:creator>
    </item>
    <item>
      <title>Efficient Data Labeling and Optimal Device Scheduling in HWNs Using Clustered Federated Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2412.17081</link>
      <description>arXiv:2412.17081v1 Announce Type: new 
Abstract: Clustered Federated Multi-task Learning (CFL) has emerged as a promising technique to address statistical challenges, particularly with non-independent and identically distributed (non-IID) data across users. However, existing CFL studies entirely rely on the impractical assumption that devices possess access to accurate ground-truth labels. This assumption becomes problematic in hierarchical wireless networks (HWNs), with vast unlabeled data and dual-level model aggregation, slowing convergence speeds, extending processing times, and increasing resource consumption. To this end, we propose Clustered Federated Semi-Supervised Learning (CFSL), a novel framework tailored for realistic scenarios in HWNs. We leverage specialized models from device clustering and present two prediction model schemes: the best-performing specialized model and the weighted-averaging ensemble model. The former assigns the most suitable specialized model to label unlabeled data, while the latter unifies specialized models to capture broader data distributions. CFSL introduces two novel prediction time schemes, split-based and stopping-based, for accurate labeling timing, and two device selection strategies, greedy and round-robin. Extensive testing validates CFSL's superiority in labeling/testing accuracy and resource efficiency, achieving up to 51% energy savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17081v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moqbel Hamood, Abdullatif Albaseer, Mohamed Abdallah, Ala Al-Fuqaha</dc:creator>
    </item>
    <item>
      <title>Fast and Live Model Auto Scaling with O(1) Host Caching</title>
      <link>https://arxiv.org/abs/2412.17246</link>
      <description>arXiv:2412.17246v1 Announce Type: new 
Abstract: Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. We first show that data plane can be made fast with no/O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable host cache and is underutilized; (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled instance with cooperative execution, thus handles cases even when the compute network is not sufficiently fast. Our system BLITZSCALE reduces the serving tail latencies by up to 86% without caching, and we achieve comparable performance (or even better) to an optimal setup where all the parameters are cached at all the host for autoscaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17246v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingyan Zhang, Haotian Wang, Yang Liu, Xingda Wei, Yizhou Shan, Rong Chen, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>Dynamic Scheduling Strategies for Resource Optimization in Computing Environments</title>
      <link>https://arxiv.org/abs/2412.17301</link>
      <description>arXiv:2412.17301v1 Announce Type: new 
Abstract: The rapid development of cloud-native architecture has promoted the widespread application of container technology, but the optimization problems in container scheduling and resource management still face many challenges. This paper proposes a container scheduling method based on multi-objective optimization, which aims to balance key performance indicators such as resource utilization, load balancing and task completion efficiency. By introducing optimization models and heuristic algorithms, the scheduling strategy is comprehensively improved, and experimental verification is carried out using the real Google Cluster Data dataset. The experimental results show that compared with traditional static rule algorithms and heuristic algorithms, the optimized scheduling scheme shows significant advantages in resource utilization, load balancing and burst task completion efficiency. This shows that the proposed method can effectively improve resource management efficiency and ensure service quality and system stability in complex dynamic cloud environments. At the same time, this paper also explores the future development direction of scheduling algorithms in multi-tenant environments, heterogeneous cloud computing, and cross-edge and cloud collaborative computing scenarios, and proposes research prospects for energy consumption optimization, adaptive scheduling and fairness. The research results not only provide a theoretical basis and practical reference for container scheduling under cloud-native architecture, but also lay a foundation for further realizing intelligent and efficient resource management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17301v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoye Wang</dc:creator>
    </item>
    <item>
      <title>Power- and Fragmentation-aware Online Scheduling for GPU Datacenters</title>
      <link>https://arxiv.org/abs/2412.17484</link>
      <description>arXiv:2412.17484v1 Announce Type: new 
Abstract: The rise of Artificial Intelligence and Large Language Models is driving increased GPU usage in data centers for complex training and inference tasks, impacting operational costs, energy demands, and the environmental footprint of large-scale computing infrastructures. This work addresses the online scheduling problem in GPU datacenters, which involves scheduling tasks without knowledge of their future arrivals. We focus on two objectives: minimizing GPU fragmentation and reducing power consumption. GPU fragmentation occurs when partial GPU allocations hinder the efficient use of remaining resources, especially as the datacenter nears full capacity. A recent scheduling policy, Fragmentation Gradient Descent (FGD), leverages a fragmentation metric to address this issue. Reducing power consumption is also crucial due to the significant power demands of GPUs. To this end, we propose PWR, a novel scheduling policy to minimize power usage by selecting power-efficient GPU and CPU combinations. This involves a simplified model for measuring power consumption integrated into a Kubernetes score plugin. Through an extensive experimental evaluation in a simulated cluster, we show how PWR, when combined with FGD, achieves a balanced trade-off between reducing power consumption and minimizing GPU fragmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17484v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Lettich, Emanuele Carlini, Franco Maria Nardini, Raffaele Perego, Salvatore Trani</dc:creator>
    </item>
    <item>
      <title>Synergistic Integration of Blockchain and Software-Defined Networking in the Internet of Energy Systems</title>
      <link>https://arxiv.org/abs/2412.17530</link>
      <description>arXiv:2412.17530v1 Announce Type: new 
Abstract: Peer-to-peer (P2P) energy trading, Smart Grids (SG), and electric vehicle energy management are integral components of the Internet of Energy (IoE) field. The integration of Software-Defined Networks (SDNs) and Blockchain (BC) technologies into the IoE domain offers potential benefits that have only been studied in the literature in a few works. In this paper, we investigate the state-of-art solutions that leverage both SDNs and blockchain within the realm of the IoE. We categorize these solutions based on the method of integrating SDN and BC into two categories. The first category is the blockchain for SDN, where blockchain enhances the SDN directly. The second category is blockchain and SDN, where both technologies are used to enhance the proposed solutions. We identify three distinct blockchain applications based on their usage: decentralizing the SDN control plane, serving as a decentralized platform, and improving security measures. Similarly, we observe that SDN serves as a performance enhancer, a substitute for traditional networking, and solely as a control and management framework. It is posited that integrating SDNs and blockchain into IoE leads to performance enhancements, improves security, enables decentralized operations, and eliminates single points of failure in the SDN control plane. Additionally, some unaddressed issues, such as energy efficiency, smart contract management, and scalability, are discussed as potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17530v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>https://bcca-conference.org/2024/</arxiv:journal_reference>
      <dc:creator>Vahideh Hayyolalam, Abdulrezzak Zekiye, Hamza Abuzahra, Oznur Ozkasap, Murat Karakus, Evrim Guler, Suleyman Uludag</dc:creator>
    </item>
    <item>
      <title>Accelerated Methods with Compressed Communications for Distributed Optimization Problems under Data Similarity</title>
      <link>https://arxiv.org/abs/2412.16414</link>
      <description>arXiv:2412.16414v1 Announce Type: cross 
Abstract: In recent years, as data and problem sizes have increased, distributed learning has become an essential tool for training high-performance models. However, the communication bottleneck, especially for high-dimensional data, is a challenge. Several techniques have been developed to overcome this problem. These include communication compression and implementation of local steps, which work particularly well when there is similarity of local data samples. In this paper, we study the synergy of these approaches for efficient distributed optimization. We propose the first theoretically grounded accelerated algorithms utilizing unbiased and biased compression under data similarity, leveraging variance reduction and error feedback frameworks. Our results are of record and confirmed by experiments on different average losses and datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16414v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Bylinkin, Aleksandr Beznosikov</dc:creator>
    </item>
    <item>
      <title>Fractional Spending: VRF&amp;Ring Signatures As Efficient Primitives For Secret Quorums</title>
      <link>https://arxiv.org/abs/2412.16648</link>
      <description>arXiv:2412.16648v1 Announce Type: cross 
Abstract: Digital currencies have emerged as a significant evolution in the financial system, yet they face challenges in distributed settings, particularly regarding double spending. Traditional approaches, such as Bitcoin, use consensus to establish a total order of transactions, ensuring that no more than the currency held by an account is spent in the order. However, consensus protocols are costly, especially when coping with Byzantine faults. It was shown that solving Consensus is not needed to perform currency's transfer, for instance using byzantine quorum systems but validation remains per-account sequential. Recent research also introduced the fractional spending problem, which enables concurrent but non-conflicting transactions i.e., transactions that spend from the same account but cannot lead to a double spending because each is only spending a small fraction of the balance. A solution was proposed based on a new quorum system and specific cryptographic primitives to protect against an adaptive adversary. The quorum system, called (k1, k2)-quorum system, guarantees that at least k1 transactions can be validated concurrently but that no more than k2 can. Employing such quorums, a payer can validate concurrently multiple fractional spending transactions in parallel with high probability. Subsequently, the payer reclaims any remaining sum through a settlement. This paper enhances such solution by integrating different cryptographic primitives, VRF and Ring Signatures, into a similar protocol. But contrarily, these tools ensure quorums to remain secret during settlements, allowing to reduces its communication costs from cubic to quadratic in messages. We also achieve payment transaction with 3 message delays rather then 5. Additionally, we propose a refined formalization of the fractional spending problem, introducing coupons, which simplifies the theoretical framework and proof structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16648v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxence Perion, Sara Tucci-Piergiovanni, Rida Bazzi</dc:creator>
    </item>
    <item>
      <title>Gradient-based Trajectory Optimization with Parallelized Differentiable Traffic Simulation</title>
      <link>https://arxiv.org/abs/2412.16750</link>
      <description>arXiv:2412.16750v1 Announce Type: cross 
Abstract: We present a parallelized differentiable traffic simulator based on the Intelligent Driver Model (IDM), a car-following framework that incorporates driver behavior as key variables. Our simulator efficiently models vehicle motion, generating trajectories that can be supervised to fit real-world data. By leveraging its differentiable nature, IDM parameters are optimized using gradient-based methods. With the capability to simulate up to 2 million vehicles in real time, the system is scalable for large-scale trajectory optimization. We show that we can use the simulator to filter noise in the input trajectories (trajectory filtering), reconstruct dense trajectories from sparse ones (trajectory reconstruction), and predict future trajectories (trajectory prediction), with all generated trajectories adhering to physical laws. We validate our simulator and algorithm on several datasets including NGSIM and Waymo Open Dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16750v1</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanghyun Son, Laura Zheng, Brian Clipp, Connor Greenwell, Sujin Philip, Ming C. Lin</dc:creator>
    </item>
    <item>
      <title>Rethinking Performance Analysis for Configurable Software Systems: A Case Study from a Fitness Landscape Perspective</title>
      <link>https://arxiv.org/abs/2412.16888</link>
      <description>arXiv:2412.16888v1 Announce Type: cross 
Abstract: Modern software systems are often highly configurable to tailor varied requirements from diverse stakeholders. Understanding the mapping between configurations and the desired performance attributes plays a fundamental role in advancing the controllability and tuning of the underlying system, yet has long been a dark hole of knowledge due to its black-box nature. While there have been previous efforts in performance analysis for these systems, they analyze the configurations as isolated data points without considering their inherent spatial relationships. This renders them incapable of interrogating many important aspects of the configuration space like local optima. In this work, we advocate a novel perspective to rethink performance analysis -- modeling the configuration space as a structured ``landscape''. To support this proposition, we designed \our, an open-source, graph data mining empowered fitness landscape analysis (FLA) framework. By applying this framework to $86$M benchmarked configurations from $32$ running workloads of $3$ real-world systems, we arrived at $6$ main findings, which together constitute a holistic picture of the landscape topography, with thorough discussions about their implications on both configuration tuning and performance modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16888v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ISSTA 2025</arxiv:journal_reference>
      <dc:creator>Mingyu Huang, Peili Mao, Ke Li</dc:creator>
    </item>
    <item>
      <title>FedCross: Intertemporal Federated Learning Under Evolutionary Games</title>
      <link>https://arxiv.org/abs/2412.16968</link>
      <description>arXiv:2412.16968v1 Announce Type: cross 
Abstract: Federated Learning (FL) mitigates privacy leakage in decentralized machine learning by allowing multiple clients to train collaboratively locally. However, dynamic mobile networks with high mobility, intermittent connectivity, and bandwidth limitation severely hinder model updates to the cloud server. Although previous studies have typically addressed user mobility issue through task reassignment or predictive modeling, frequent migrations may result in high communication overhead. Overcoming this obstacle involves not only dealing with resource constraints, but also finding ways to mitigate the challenges posed by user migrations. We therefore propose an intertemporal incentive framework, FedCross, which ensures the continuity of FL tasks by migrating interrupted training tasks to feasible mobile devices. Specifically, FedCross comprises two distinct stages. In Stage 1, we address the task allocation problem across regions under resource constraints by employing a multi-objective migration algorithm to quantify the optimal task receivers. Moreover, we adopt evolutionary game theory to capture the dynamic decision-making of users, forecasting the evolution of user proportions across different regions to mitigate frequent migrations. In Stage 2, we utilize a procurement auction mechanism to allocate rewards among base stations, ensuring that those providing high-quality models receive optimal compensation. This approach incentivizes sustained user participation, thereby ensuring the overall feasibility of FedCross. Finally, experimental results validate the theoretical soundness of FedCross and demonstrate its significant reduction in communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16968v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfeng Lu, Ying Zhang, Riheng Jia, Shuqin Cao, Jing Liu, Hao Fu</dc:creator>
    </item>
    <item>
      <title>Quantum Approximate Optimisation Applied to Graph Similarity</title>
      <link>https://arxiv.org/abs/2412.17309</link>
      <description>arXiv:2412.17309v1 Announce Type: cross 
Abstract: Quantum computing promises solutions to classically difficult and new-found problems through controlling the subtleties of quantum computing. The Quantum Approximate Optimisation Algorithm (QAOA) is a recently proposed quantum algorithm designed to tackle difficult combinatorial optimisation problems utilising both quantum and classical computation. The hybrid nature, generality and typically low gate-depth make it a strong candidate for near-term implementation in quantum computing. Finding the practical limits of the algorithm is currently an open problem. Until now, no tools to facilitate the design and validation of probabilistic quantum optimisation algorithms such as the QAOA on a non-trivial scale exist. Graph similarity is a long standing classically difficult problem withstanding decades of research from academia and industry. Determining the maximal edge overlap between all possible node label permutations is an NP-Complete task and provides an apt measure of graph similarity. We introduce a novel quantum optimisation simulation package facilitating investigation of all constituent components of the QAOA from desktop to cluster scale using graph similarity as an example. Our simulation provides flexibility and performance. We investigate eight classical optimisation methods each at six levels of decomposition. Moreover an encoding for permutation based problems such as graph similarity through edge overlap to the QAOA allows for significant quantum memory savings at the cost of additional operations. This compromise extends into the classical portion of the algorithm as the inclusion of infeasible solutions creates a challenging cost-function landscape. We present performance analysis of our simulation and of the QAOA setting a precedent for investigating and validating numerous other difficult problems to the QAOA as we move towards realising practical quantum computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17309v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas J. Pritchard</dc:creator>
    </item>
    <item>
      <title>FedTLU: Federated Learning with Targeted Layer Updates</title>
      <link>https://arxiv.org/abs/2412.17692</link>
      <description>arXiv:2412.17692v1 Announce Type: cross 
Abstract: Federated learning (FL) addresses privacy concerns in language modeling by enabling multiple clients to contribute to training language models. However, non-IID (identically and independently distributed) data across clients often limits FL's performance. This issue is especially challenging during model fine-tuning, as noise due to variations in clients' data distributions can harm model convergence near the optimum. This paper proposes a targeted layer update strategy for fine-tuning in FL. Instead of randomly updating layers of the language model, as often done in practice, we use a scoring mechanism to identify and update the most critical layers, avoiding excessively noisy or even poisoned updates by freezing the parameters in other layers. We show in extensive experiments that our method improves convergence and performance in non-IID settings, offering a more efficient approach to fine-tuning federated language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17692v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jong-Ik Park, Carlee Joe-Wong</dc:creator>
    </item>
    <item>
      <title>ShotQC: Reducing Sampling Overhead in Quantum Circuit Cutting</title>
      <link>https://arxiv.org/abs/2412.17704</link>
      <description>arXiv:2412.17704v1 Announce Type: cross 
Abstract: The recent \emph{quantum circuit cutting} technique enables simulating large quantum circuits on distributed smaller devices, significantly extending the capabilities of current noisy intermediate-scale quantum (NISQ) hardware. However, this method incurs substantial classical postprocessing and additional quantum resource demands, as both postprocessing complexity and sampling overhead scale exponentially with the number of cuts introduced. In this work, we propose an enhanced circuit cutting framework \emph{ShotQC} with effective sampling overhead reduction. It effectively reduces sampling overhead through two key optimizations: \emph{shot distribution} and \emph{cut parameterization}. The former employs an adaptive Monte Carlo method to dynamically allocate more quantum resources to subcircuit configurations that contribute more to variance in the final outcome. The latter leverages additional degrees of freedom in postprocessing to further suppress variance. By integrating these optimization methods, ShotQC achieves significant reductions in sampling overhead without increasing classical postprocessing complexity, as demonstrated on a range of benchmark circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17704v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po-Hung Chen, Dah-Wei Chiou, Jie-Hong Roland Jiang</dc:creator>
    </item>
    <item>
      <title>On Completely Edge-Independent Spanning Trees in Locally Twisted Cubes</title>
      <link>https://arxiv.org/abs/2401.01585</link>
      <description>arXiv:2401.01585v4 Announce Type: replace 
Abstract: A network can contain numerous spanning trees. If two spanning trees $T_i,T_j$ do not share any common edges, $T_i$ and $T_j$ are said to be pairwisely edge-disjoint. For spanning trees $T_1, T_2, ..., T_m$, if every two of them are pairwisely edge-disjoint, they are called completely edge-independent spanning trees (CEISTs for short). CEISTs can facilitate many network functionalities, and constructing CEISTs as maximally allowed as possible in a given network is a worthy undertaking. In this paper, we establish the maximal number of CEISTs in the locally twisted cube network, and propose an algorithm to construct $\lfloor \frac{n}{2} \rfloor$ CEISTs in $LTQ_n$, the $n$-dimensional locally twisted cube. The proposed algorithm has been actually implemented, and we present the outputs. Network broadcasting in the $LTQ_n$ was simulated using $\lfloor\frac{n}{2}\rfloor$ CEISTs, and the performance compared with broadcasting using a single tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01585v4</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaorui Li, Baolei Cheng, Jianxi Fan, Yan Wang, Dajin Wang</dc:creator>
    </item>
    <item>
      <title>A Note on Solving Problems of Substantially Super-linear Complexity in $N^{o(1)}$ Rounds of the Congested Clique</title>
      <link>https://arxiv.org/abs/2405.15270</link>
      <description>arXiv:2405.15270v2 Announce Type: replace 
Abstract: We study the possibility of designing $N^{o(1)}$-round protocols for problems of substantially super-linear polynomial-time (sequential) complexity on the congested clique with about $N^{1/2}$ nodes, where $N$ is the input size. We show that the average time complexity of the local computation performed at a clique node (in terms of the size of the data received by the node) in such protocols has to be substantially larger than the time complexity of the given problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15270v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrzej Lingas</dc:creator>
    </item>
    <item>
      <title>Formal Definition and Implementation of Reproducibility Tenets for Computational Workflows</title>
      <link>https://arxiv.org/abs/2406.01146</link>
      <description>arXiv:2406.01146v2 Announce Type: replace 
Abstract: Computational workflow management systems power contemporary data-intensive sciences. The slowly resolving reproducibility crisis presents both a sobering warning and an opportunity to iterate on what science and data processing entails. The Square Kilometre Array (SKA), the world's largest radio telescope, is among the most extensive scientific projects underway and presents grand scientific collaboration and data-processing challenges. In this work, we aim to improve the ability of workflow management systems to facilitate reproducible, high-quality science. This work presents a scale and system-agnostic computational workflow model and extends five well-known reproducibility concepts into seven well-defined tenets for this workflow model. Additionally, we present a method to construct workflow execution signatures using cryptographic primitives in amortized constant time. We combine these three concepts and provide a concrete implementation in Data Activated Flow Graph Engine (DALiuGE), a workflow management system for the SKA to embed specific provenance information into workflow signatures, demonstrating the possibility of facilitating automatic formal verification of scientific quality in amortized constant time. We validate our approach with a simple yet representative astronomical processing task: filtering a noisy signal with a lowpass filter using CPU and GPU methods. This example shows the practicality and efficacy of combining formal tenet definitions with a workflow signature generation mechanism. Our framework, spanning formal UML specification, principled provenance information collection based on reproducibility tenets, and finally, a concrete example implementation in DALiuGE illuminates otherwise obscure scientific discrepancies and similarities between principally identical workflow executions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01146v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2024.107684</arxiv:DOI>
      <dc:creator>Nicholas J. Pritchard, Andreas Wicenec</dc:creator>
    </item>
    <item>
      <title>MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool</title>
      <link>https://arxiv.org/abs/2406.17565</link>
      <description>arXiv:2406.17565v3 Announce Type: replace 
Abstract: Large language model (LLM) serving has transformed from stateless to stateful systems, utilizing techniques like context caching and disaggregated inference. These optimizations extend the lifespan and domain of the KV cache, necessitating a new architectural approach. We present MemServe, a unified system that integrates both inter-request and intra-request optimizations. MemServe introduces MemPool, an elastic memory pool managing distributed memory and KV caches across serving instances. Using MemPool APIs, MemServe combines context caching with disaggregated inference for the first time, supported by a global scheduler that enhances cache reuse through a global prompt tree-based locality-aware policy. Tests show that MemServe significantly improves job completion time and time-to-first-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17565v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, Yizhou Shan</dc:creator>
    </item>
    <item>
      <title>Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2410.18125</link>
      <description>arXiv:2410.18125v2 Announce Type: replace 
Abstract: Edge Intelligence (EI) has been instrumental in delivering real-time, localized services by leveraging the computational capabilities of edge networks. The integration of Large Language Models (LLMs) empowers EI to evolve into the next stage: Edge General Intelligence (EGI), enabling more adaptive and versatile applications that require advanced understanding and reasoning capabilities. However, systematic exploration in this area remains insufficient. This survey delineates the distinctions between EGI and traditional EI, categorizing LLM-empowered EGI into three conceptual systems: centralized, hybrid, and decentralized. For each system, we detail the framework designs and review existing implementations. Furthermore, we evaluate the performance and throughput of various Small Language Models (SLMs) that are more suitable for development on edge devices. This survey provides researchers with a comprehensive vision of EGI, offering insights into its vast potential and establishing a foundation for future advancements in this rapidly evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18125v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Handi Chen, Weipeng Deng, Shuo Yang, Jinfeng Xu, Zhihan Jiang, Edith C. H. Ngai, Jiangchuan Liu, Xue Liu</dc:creator>
    </item>
    <item>
      <title>Matryoshka: Optimization of Dynamic Diverse Quantum Chemistry Systems via Elastic Parallelism Transformation</title>
      <link>https://arxiv.org/abs/2412.13203</link>
      <description>arXiv:2412.13203v2 Announce Type: replace 
Abstract: AI infrastructures, predominantly GPUs, have delivered remarkable performance gains for deep learning. Conversely, scientific computing, exemplified by quantum chemistry systems, suffers from dynamic diversity, where computational patterns are more diverse and vary dynamically, posing a significant challenge to sponge acceleration off GPUs.
  In this paper, we propose Matryoshka, a novel elastically-parallel technique for the efficient execution of quantum chemistry system with dynamic diversity on GPU. Matryoshka capitalizes on Elastic Parallelism Transformation, a property prevalent in scientific systems yet underexplored for dynamic diversity, to elastically realign parallel patterns with GPU architecture. Structured around three transformation primitives (Permutation, Deconstruction, and Combination), Matryoshka encompasses three core components. The Block Constructor serves as the central orchestrator, which reformulates data structures accommodating dynamic inputs and constructs fine-grained GPU-efficient compute blocks. Within each compute block, the Graph Compiler operates offline, generating high-performance code with clear computational path through an automated compilation process. The Workload Allocator dynamically schedules workloads with varying operational intensities to threads online. It achieves highly efficient parallelism for compute-intensive operations and facilitates fusion with neighboring memory-intensive operations automatically. Extensive evaluation shows that Matryoshka effectively addresses dynamic diversity, yielding acceleration improvements of up to 13.86x (average 9.41x) over prevailing state-of-the-art approaches on 13 quantum chemistry systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13203v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuowei Wang, Kun Li, Donglin Bai, Fusong Ju, Leo Xia, Ting Cao, Ju Ren, Yaoxue Zhang, Mao Yang</dc:creator>
    </item>
    <item>
      <title>Minimizing speculation overhead in a parallel recognizer for regular texts</title>
      <link>https://arxiv.org/abs/2412.14975</link>
      <description>arXiv:2412.14975v2 Announce Type: replace 
Abstract: Speculative data-parallel algorithms for language recognition have been widely experimented for various types of finite-state automata (FA), deterministic (DFA) and nondeterministic (NFA), often derived from regular expressions (RE). Such an algorithm cuts the input string into chunks, independently recognizes each chunk in parallel by means of identical FAs, and at last joins the chunk results and checks overall consistency. In chunk recognition, it is necessary to speculatively start the FAs in any state, thus causing an overhead that reduces the speedup compared to a serial algorithm. Existing data-parallel DFA-based recognizers suffer from the excessive number of starting states, and the NFA-based ones suffer from the number of nondeterministic transitions. Our data-parallel algorithm is based on the new FA type called reduced interface DFA (RI-DFA), which minimizes the speculation overhead without incurring in the penalty of nondeterministic transitions or of impractically enlarged DFA machines. The algorithm is proved to be correct and theoretically efficient, because it combines the state-reduction of an NFA with the speed of deterministic transitions, thus improving on both DFA-based and NFA-based existing implementations. The practical applicability of the RI-DFA approach is confirmed by a quantitative comparison of the number of starting states for a large public benchmark of complex FAs. On multi-core computing architectures, the RI-DFA recognizer is much faster than the NFA-based one on all benchmarks, while it matches the DFA-based one on some benchmarks and performs much better on some others. The extra time cost needed to construct an RI-DFA compared to a DFA is moderate and is compatible with a practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14975v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Angelo Borsotti, Luca Breveglieri, Stefano Crespi Reghizzi, Angelo Morzenti</dc:creator>
    </item>
    <item>
      <title>Distributed Subgraph Finding: Progress and Challenges</title>
      <link>https://arxiv.org/abs/2203.06597</link>
      <description>arXiv:2203.06597v4 Announce Type: replace-cross 
Abstract: This is a survey of the exciting recent progress made in understanding the complexity of distributed subgraph finding problems. It overviews the results and techniques for assorted variants of subgraph finding problems in various models of distributed computing, and states intriguing open questions. This version contains some updates over the ICALP 2021 version, and I will try to keep updating it as additional progress is made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.06597v4</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Censor-Hillel</dc:creator>
    </item>
    <item>
      <title>Worst-Case Input Generation for Concurrent Programs under Non-Monotone Resource Metrics</title>
      <link>https://arxiv.org/abs/2309.01261</link>
      <description>arXiv:2309.01261v4 Announce Type: replace-cross 
Abstract: Worst-case input generation aims to automatically generate inputs that exhibit the worst-case performance of programs. It has several applications, and can, for example, detect vulnerabilities to denial-of-service (DoS) attacks. However, it is non-trivial to generate worst-case inputs for concurrent programs, particularly for resources like memory where the peak cost depends on how processes are scheduled. This article presents the first sound worst-case input generation algorithm for concurrent programs under non-monotone resource metrics like memory. The key insight is to leverage resource-annotated session types and symbolic execution. Session types describe communication protocols on channels in process calculi. Equipped with resource annotations, resource-annotated session types not only encode cost bounds but also indicate how many resources can be reused and transferred between processes. This information is critical for identifying a worst-case execution path during symbolic execution. The algorithm is sound: if it returns any input, it is guaranteed to be a valid worst-case input. The algorithm is also relatively complete: as long as resource-annotated session types are sufficiently expressive and the background theory for SMT solving is decidable, a worst-case input is guaranteed to be returned. A simple case study of a web server's memory usage demonstrates the utility of the worst-case input generation algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01261v4</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long Pham, Jan Hoffmann</dc:creator>
    </item>
    <item>
      <title>Tackling the Unlimited Staleness in Federated Learning with Intertwined Data and Device Heterogeneities</title>
      <link>https://arxiv.org/abs/2309.13536</link>
      <description>arXiv:2309.13536v3 Announce Type: replace-cross 
Abstract: Federated Learning (FL) can be affected by data and device heterogeneities, caused by clients' different local data distributions and latencies in uploading model updates (i.e., staleness). Traditional schemes consider these heterogeneities as two separate and independent aspects, but this assumption is unrealistic in practical FL scenarios where these heterogeneities are intertwined. In these cases, traditional FL schemes are ineffective, and a better approach is to convert a stale model update into a unstale one. In this paper, we present a new FL framework that ensures the accuracy and computational efficiency of this conversion, hence effectively tackling the intertwined heterogeneities that may cause unlimited staleness in model updates. Our basic idea is to estimate the distributions of clients' local training data from their uploaded stale model updates, and use these estimations to compute unstale client model updates. In this way, our approach does not require any auxiliary dataset nor the clients' local models to be fully trained, and does not incur any additional computation or communication overhead at client devices. We compared our approach with the existing FL strategies on mainstream datasets and models, and showed that our approach can improve the trained model accuracy by up to 25% and reduce the number of required training epochs by up to 35%. Source codes can be found at: https://github.com/pittisl/FL-with-intertwined-heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13536v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haoming Wang, Wei Gao</dc:creator>
    </item>
    <item>
      <title>Revising Apetrei's bounding volume hierarchy construction algorithm to allow stackless traversal</title>
      <link>https://arxiv.org/abs/2402.00665</link>
      <description>arXiv:2402.00665v2 Announce Type: replace-cross 
Abstract: Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal. One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index). In general, this operation requires an additional tree traversal during the tree construction. For some tree structures, however, it is possible achieve the same result at a reduced cost. We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012]. Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes. We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes. Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00665v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Prokopenko, Damien Lebrun-Grandi\'e</dc:creator>
    </item>
    <item>
      <title>A Framework for Effective Invocation Methods of Various LLM Services</title>
      <link>https://arxiv.org/abs/2402.03408</link>
      <description>arXiv:2402.03408v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive abilities in solving various natural language processing tasks and are now widely offered as services. LLM services enable users to accomplish tasks without requiring specialized knowledge, simply by paying service providers. However, numerous providers offer various LLM services with variations in pricing, latency, and performance. These factors are also affected by different invocation methods, such as the choice of context and the use of cache, which lead to unpredictable and uncontrollable service cost and quality. Consequently, utilizing various LLM services invocation methods to construct an effective (cost-saving, low-latency and high-performance) invocation strategy that best meets task demands becomes a pressing challenge. This paper provides a comprehensive overview of methods help LLM services to be invoked efficiently. Technically, we define the problem of constructing an effective LLM services invocation strategy, and based on this, propose a unified LLM service invocation framework. The framework classifies existing methods into four categories: input abstraction, semantic cache, solution design, and output enhancement, which can be used separately or jointly during the invocation life cycle. We discuss the methods in each category and compare them to provide valuable guidance for researchers. Finally, we emphasize the open challenges in this domain and shed light on future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03408v3</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Can Wang, Dianbo Sui, Bolin Zhang, Xiaoyu Liu, Jiabao Kang, Zhidong Qiao, Zhiying Tu</dc:creator>
    </item>
    <item>
      <title>mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture</title>
      <link>https://arxiv.org/abs/2404.12135</link>
      <description>arXiv:2404.12135v3 Announce Type: replace-cross 
Abstract: Root cause analysis (RCA) in Micro-services architecture (MSA) with escalating complexity encounters complex challenges in maintaining system stability and efficiency due to fault propagation and circular dependencies among nodes. Diverse root cause analysis faults require multi-agents with diverse expertise. To mitigate the hallucination problem of large language models (LLMs), we design blockchain-inspired voting to ensure the reliability of the analysis by using a decentralized decision-making process. To avoid non-terminating loops led by common circular dependency in MSA, we objectively limit steps and standardize task processing through Agent Workflow. We propose a pioneering framework, multi-Agent Blockchain-inspired Collaboration for root cause analysis in micro-services architecture (mABC), where multiple agents based on the powerful LLMs follow Agent Workflow and collaborate in blockchain-inspired voting. Specifically, seven specialized agents derived from Agent Workflow each provide valuable insights towards root cause analysis based on their expertise and the intrinsic software knowledge of LLMs collaborating within a decentralized chain. Our experiments on the AIOps challenge dataset and a newly created Train-Ticket dataset demonstrate superior performance in identifying root causes and generating effective resolutions. The ablation study further highlights Agent Workflow, multi-agent, and blockchain-inspired voting is crucial for achieving optimal performance. mABC offers a comprehensive automated root cause analysis and resolution in micro-services architecture and significantly improves the IT Operation domain. The code and dataset are in https://github.com/zwpride/mABC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12135v3</guid>
      <category>cs.MA</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhang, Hongcheng Guo, Jian Yang, Zhoujin Tian, Yi Zhang, Chaoran Yan, Zhoujun Li, Tongliang Li, Xu Shi, Liangfan Zheng, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>PixelsDB: Serverless and NL-Aided Data Analytics with Flexible Service Levels and Prices</title>
      <link>https://arxiv.org/abs/2405.19784</link>
      <description>arXiv:2405.19784v2 Announce Type: replace-cross 
Abstract: Serverless query processing has become increasingly popular due to its advantages, including automated resource management, high elasticity, and pay-as-you-go pricing. For users who are not system experts, serverless query processing greatly reduces the cost of owning a data analytic system. However, it is still a significant challenge for non-expert users to transform their complex and evolving data analytic needs into proper SQL queries and select a serverless query service that delivers satisfactory performance and price for each type of query.
  This paper presents PixelsDB, an open-source data analytic system that allows users who lack system or SQL expertise to explore data efficiently. It allows users to generate and debug SQL queries using a natural language interface powered by fine-tuned language models. The queries are then executed by a serverless query engine that offers varying prices for different performance service levels (SLAs). The performance SLAs are natively supported by dedicated architecture design and heterogeneous resource scheduling that can apply cost-efficient resources to process non-urgent queries. We demonstrate that the combination of a serverless paradigm, a natural-language-aided interface, and flexible SLAs and prices will substantially improve the usability of cloud data analytic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19784v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoqiong Bian, Dongyang Geng, Haoyang Li, Yunpeng Chai, Anastasia Ailamaki</dc:creator>
    </item>
    <item>
      <title>Graph Neural Network Training Systems: A Performance Comparison of Full-Graph and Mini-Batch</title>
      <link>https://arxiv.org/abs/2406.00552</link>
      <description>arXiv:2406.00552v4 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have gained significant attention in recent years due to their ability to learn representations of graph-structured data. Two common methods for training GNNs are mini-batch training and full-graph training. Since these two methods require different training pipelines and systems optimizations, two separate classes of GNN training systems emerged, each tailored for one method. Works that introduce systems belonging to a particular category predominantly compare them with other systems within the same category, offering limited or no comparison with systems from the other category. Some prior work also justifies its focus on one specific training method by arguing that it achieves higher accuracy than the alternative. The literature, however, has incomplete and contradictory evidence in this regard.
  In this paper, we provide a comprehensive empirical comparison of representative full-graph and mini-batch GNN training systems. We find that the mini-batch training systems consistently converge faster than the full-graph training ones across multiple datasets, GNN models, and system configurations. We also find that mini-batch training techniques converge to similar to or often higher accuracy values than full-graph training ones, showing that mini-batch sampling is not necessarily detrimental to accuracy. Our work highlights the importance of comparing systems across different classes, using time-to-accuracy rather than epoch time for performance comparison, and selecting appropriate hyperparameters for each training method separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00552v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Bajaj, Hojae Son, Juelin Liu, Hui Guan, Marco Serafini</dc:creator>
    </item>
  </channel>
</rss>
