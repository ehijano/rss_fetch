<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Aug 2024 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Measuring the Impact of New Risk Factors Within Survival Models</title>
      <link>https://arxiv.org/abs/2408.02770</link>
      <description>arXiv:2408.02770v1 Announce Type: new 
Abstract: Survival is poor for patients with metastatic cancer, and it is vital to examine new biomarkers that can improve patient prognostication and identify those who would benefit from more aggressive therapy. In metastatic prostate cancer, two new assays have become available: one that quantifies the number of cancer cells circulating in the peripheral blood, and the other a marker of the aggressiveness of the disease. It is critical to determine the magnitude of the effect of these biomarkers on the discrimination of a model-based risk score. To do so, most analysts frequently consider the discrimination of two separate survival models: one that includes both the new and standard factors and a second that includes the standard factors alone. However, this analysis is ultimately incorrect for many of the scale-transformation models ubiquitous in survival, as the reduced model is misspecified if the full model is specified correctly. To circumvent this issue, we developed a projection-based approach to estimate the impact of the two prostate cancer biomarkers. The results indicate that the new biomarkers can influence model discrimination and justify their inclusion in the risk model; however, the hunt remains for an applicable model to risk-stratify patients with metastatic prostate cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02770v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Glenn Heller, Sean M. Devlin</dc:creator>
    </item>
    <item>
      <title>Continuous Monitoring via Repeated Significance</title>
      <link>https://arxiv.org/abs/2408.02821</link>
      <description>arXiv:2408.02821v1 Announce Type: new 
Abstract: Requiring statistical significance at multiple interim analyses to declare a statistically significant result for an AB test allows less stringent requirements for significance at each interim analysis. Repeated repeated significance competes well with methods built on assumptions about the test -- assumptions that may be impossible to evaluate a priori and may require extra data to evaluate empirically.
  Instead, requiring repeated significance allows the data itself to prove directly that the required results are not due to chance alone. We explain how to apply tests with repeated significance to continuously monitor unbounded tests -- tests that do not have an a priori bound on running time or number of observations. We show that it is impossible to maintain a constant requirement for significance for unbounded tests, but that we can come arbitrarily close to that goal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02821v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Bax, Arundhyoti Sarkar, Alex Shtoff</dc:creator>
    </item>
    <item>
      <title>Setting the duration of online A/B experiments</title>
      <link>https://arxiv.org/abs/2408.02830</link>
      <description>arXiv:2408.02830v1 Announce Type: new 
Abstract: In designing an online A/B experiment, it is crucial to select a sample size and duration that ensure the resulting confidence interval (CI) for the treatment effect is the right width to detect an effect of meaningful magnitude with sufficient statistical power without wasting resources. While the relationship between sample size and CI width is well understood, the effect of experiment duration on CI width remains less clear. This paper provides an analytical formula for the width of a CI based on a ratio treatment effect estimator as a function of both sample size (N) and duration (T). The formula is derived from a mixed effects model with two variance components. One component, referred to as the temporal variance, persists over time for experiments where the same users are kept in the same experiment arm across different days. The remaining error variance component, by contrast, decays to zero as T gets large. The formula we derive introduces a key parameter that we call the user-specific temporal correlation (UTC), which quantifies the relative sizes of the two variance components and can be estimated from historical experiments. Higher UTC indicates a slower decay in CI width over time. On the other hand, when the UTC is 0 -- as for experiments where users shuffle in and out of the experiment across days -- the CI width decays at the standard parametric 1/T rate. We also study how access to pre-period data for the users in the experiment affects the CI width decay. We show our formula closely explains CI widths on real A/B experiments at YouTube.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02830v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison H. Li, Chaoyu Yu</dc:creator>
    </item>
    <item>
      <title>Weighted shape-constrained estimation for the autocovariance sequence from a reversible Markov chain</title>
      <link>https://arxiv.org/abs/2408.03024</link>
      <description>arXiv:2408.03024v1 Announce Type: new 
Abstract: We present a novel weighted $\ell_2$ projection method for estimating autocovariance sequences and spectral density functions from reversible Markov chains. Berg and Song (2023) introduced a least-squares shape-constrained estimation approach for the autocovariance function by projecting an initial estimate onto a shape-constrained space using an $\ell_2$ projection. While the least-squares objective is commonly used in shape-constrained regression, it can be suboptimal due to correlation and unequal variances in the input function. To address this, we propose a weighted least-squares method that defines a weighted norm on transformed data. Specifically, we transform an input autocovariance sequence into the Fourier domain and apply weights based on the asymptotic variance of the sample periodogram, leveraging the asymptotic independence of periodogram ordinates. Our proposal can equivalently be viewed as estimating a spectral density function by applying shape constraints to its Fourier series. We demonstrate that our weighted approach yields strongly consistent estimates for both the spectral density and the autocovariance sequence. Empirical studies show its effectiveness in uncertainty quantification for Markov chain Monte Carlo estimation, outperforming the unweighted moment LS estimator and other state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03024v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyebin Song, Stephen Berg</dc:creator>
    </item>
    <item>
      <title>Predictive Performance Test based on the Exhaustive Nested Cross-Validation for High-dimensional data</title>
      <link>https://arxiv.org/abs/2408.03138</link>
      <description>arXiv:2408.03138v1 Announce Type: new 
Abstract: It is crucial to assess the predictive performance of a model in order to establish its practicality and relevance in real-world scenarios, particularly for high-dimensional data analysis. Among data splitting or resampling methods, cross-validation (CV) is extensively used for several tasks such as estimating the prediction error, tuning the regularization parameter, and selecting the most suitable predictive model among competing alternatives. The K-fold cross-validation is a popular CV method but its limitation is that the risk estimates are highly dependent on the partitioning of the data (for training and testing). Here, the issues regarding the reproducibility of the K-fold CV estimator is demonstrated in hypothesis testing wherein different partitions lead to notably disparate conclusions. This study presents an alternative novel predictive performance test and valid confidence intervals based on exhaustive nested cross-validation for determining the difference in prediction error between two model-fitting algorithms. A naive implementation of the exhaustive nested cross-validation is computationally costly. Here, we address concerns regarding computational complexity by devising a computationally tractable closed-form expression for the proposed cross-validation estimator using ridge regularization. Our study also investigates strategies aimed at enhancing statistical power within high-dimensional scenarios while controlling the Type I error rate. To illustrate the practical utility of our method, we apply it to an RNA sequencing study and demonstrate its effectiveness in the context of biological data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03138v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iris Ivy Gauran, Hernando Ombao, Zhaoxia Yu</dc:creator>
    </item>
    <item>
      <title>Regression analysis of elliptically symmetric direction data</title>
      <link>https://arxiv.org/abs/2408.03268</link>
      <description>arXiv:2408.03268v1 Announce Type: new 
Abstract: A comprehensive toolkit is developed for regression analysis of directional data based on a flexible class of angular Gaussian distributions. Informative testing procedures for isotropy and covariate effects on the directional response are proposed. Moreover, a prediction region that achieves the smallest volume in a class of ellipsoidal prediction regions of the same coverage probability is constructed. The efficacy of these inference procedures is demonstrated in simulation experiments. Finally, this new toolkit is used to analyze directional data originating from a hydrology study and a bioinformatics application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03268v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zehao Yu, Xianzheng Huang</dc:creator>
    </item>
    <item>
      <title>Visual Analysis of Multi-outcome Causal Graphs</title>
      <link>https://arxiv.org/abs/2408.02679</link>
      <description>arXiv:2408.02679v1 Announce Type: cross 
Abstract: We introduce a visual analysis method for multiple causal graphs with different outcome variables, namely, multi-outcome causal graphs. Multi-outcome causal graphs are important in healthcare for understanding multimorbidity and comorbidity. To support the visual analysis, we collaborated with medical experts to devise two comparative visualization techniques at different stages of the analysis process. First, a progressive visualization method is proposed for comparing multiple state-of-the-art causal discovery algorithms. The method can handle mixed-type datasets comprising both continuous and categorical variables and assist in the creation of a fine-tuned causal graph of a single outcome. Second, a comparative graph layout technique and specialized visual encodings are devised for the quick comparison of multiple causal graphs. In our visual analysis approach, analysts start by building individual causal graphs for each outcome variable, and then, multi-outcome causal graphs are generated and visualized with our comparative technique for analyzing differences and commonalities of these causal graphs. Evaluation includes quantitative measurements on benchmark datasets, a case study with a medical expert, and expert user studies with real-world health research data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02679v1</guid>
      <category>cs.LG</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengjie Fan, Jinlu Yu, Daniel Weiskopf, Nan Cao, Huai-Yu Wang, Liang Zhou</dc:creator>
    </item>
    <item>
      <title>The multivariate fractional Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2408.03051</link>
      <description>arXiv:2408.03051v1 Announce Type: cross 
Abstract: Starting from the notion of multivariate fractional Brownian Motion introduced in [F. Lavancier, A. Philippe, and D. Surgailis. Covariance function of vector self-similar processes. Statistics &amp; Probability Letters, 2009] we define a multivariate version of the fractional Ornstein-Uhlenbeck process. This multivariate Gaussian process is stationary, ergodic and allows for different Hurst exponents on each component. We characterize its correlation matrix and its short and long time asymptotics. Besides the marginal parameters, the cross correlation between one-dimensional marginal components is ruled by two parameters. We consider the problem of their inference, proposing two types of estimator, constructed from discrete observations of the process. We establish their asymptotic theory, in one case in the long time asymptotic setting, in the other case in the infill and long time asymptotic setting. The limit behavior can be asymptotically Gaussian or non-Gaussian, depending on the values of the Hurst exponents of the marginal components. The technical core of the paper relies on the analysis of asymptotic properties of functionals of Gaussian processes, that we establish using Malliavin calculus and Stein's method. We provide numerical experiments that support our theoretical analysis and also suggest a conjecture on the application of one of these estimators to the multivariate fractional Brownian Motion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03051v1</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ranieri Dugo, Giacomo Giorgio, Paolo Pigato</dc:creator>
    </item>
    <item>
      <title>Advances in projection predictive inference</title>
      <link>https://arxiv.org/abs/2306.15581</link>
      <description>arXiv:2306.15581v2 Announce Type: replace 
Abstract: The concepts of Bayesian prediction, model comparison, and model selection have developed significantly over the last decade. As a result, the Bayesian community has witnessed a rapid growth in theoretical and applied contributions to building and selecting predictive models. Projection predictive inference in particular has shown promise to this end, finding application across a broad range of fields. It is less prone to over-fitting than na\"ive selection based purely on cross-validation or information criteria performance metrics, and has been known to out-perform other methods in terms of predictive performance. We survey the core concept and contemporary contributions to projection predictive inference, and present a safe, efficient, and modular workflow for prediction-oriented model selection therein. We also provide an interpretation of the projected posteriors achieved by projection predictive inference in terms of their limitations in causal settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15581v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann McLatchie, S\"olvi R\"ognvaldsson, Frank Weber, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Credible Distributions of Overall Ranking of Entities</title>
      <link>https://arxiv.org/abs/2401.01833</link>
      <description>arXiv:2401.01833v2 Announce Type: replace 
Abstract: Inference on overall ranking of a set of entities, such as athletes or players, schools and universities, hospitals, cities, restaurants, movies or books, companies, states, countries or subpopulations, based on appropriate characteristics or performances, is an important problem. Estimation of ranks based on point estimates of means does not account for the uncertainty in those estimates. Treating estimated ranks without any regard for uncertainty is problematic. We propose a novel solution using the Bayesian approach. It is easily implementable, competitive with a popular frequentist method, more effective and informative. Using suitable joint credible sets for entity means, we appropriately create {\it credible distributions} (CDs, a phrase we coin), which are probability distributions, for the rank vector of entities. As a byproduct, the supports of the CDs are credible sets for overall ranking. We evaluate our proposed procedure in terms of accuracy and stability using a number of applications and a simulation study. While the frequentist approach cannot utilize covariates, the proposed method handles them routinely to its benefit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01833v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gauri Sankar Datta, Yiren Hou, Abhyuday Mandal</dc:creator>
    </item>
    <item>
      <title>Hybrid Smoothing for Anomaly Detection in Time Series</title>
      <link>https://arxiv.org/abs/2402.03459</link>
      <description>arXiv:2402.03459v2 Announce Type: replace 
Abstract: Many industrial and engineering processes monitored as times series have smooth trends that indicate normal behavior and occasionally anomalous patterns that can indicate a problem. This kind of behavior can be modeled by a smooth trend, such as a spline or Gaussian process, and a disruption based on a sparser representation. Our approach is to expand the process signal into two sets of basis functions: one set uses L2 penalties on the coefficients, and the other set uses L1 penalties to control sparsity. From a frequentist perspective, this results in a hybrid smoother that combines cubic smoothing splines and the LASSO. As a Bayesian hierarchical model (BHM), this is equivalent to priors giving a Gaussian process and a Laplace distribution for anomaly coefficients. For the hybrid smoother, we propose two new ways of determining the penalty parameters that use effective degrees of freedom and contrast this with the BHM that uses loosely informative inverse gamma priors. Several reformulations are used to make sampling the BHM posterior more efficient, including some novel features in orthogonalizing and regularizing the model basis functions. This methodology is motivated by a substantive application, offline monitoring of a water treatment process for municipal water filtration. We also test the robustness of these methods with a Monte Carlo study designed to inspect a range trended time series under an array of conditions and compare this new approach to multiple existing modern methods. Both the hybrid smoother and the full BHM give comparable results with small false positive and false negative rates. Besides being successful in the water treatment application, this work can be easily extended to other Gaussian process models and other features that represent process disruptions in offline data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03459v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Hofkes, Douglas Nychka, Tzahi Cath, Amanda Hering, Craig McGonagill</dc:creator>
    </item>
    <item>
      <title>Spatial autoregressive model with measurement error in covariates</title>
      <link>https://arxiv.org/abs/2402.04593</link>
      <description>arXiv:2402.04593v3 Announce Type: replace 
Abstract: The Spatial AutoRegressive model (SAR) is commonly used in studies involving spatial and network data to estimate the spatial or network peer influence and the effects of covariates on the response, taking into account the dependence among units. While the model can be efficiently estimated with a Quasi maximum likelihood approach (QMLE), the detrimental effect of covariate measurement error on the QMLE and how to remedy it is currently unknown. If covariates are measured with error, then the QMLE may not have the $\sqrt{n}$ convergence and may even be inconsistent even when a node is influenced by only a limited number of other nodes or spatial units. We develop a measurement error-corrected ML estimator (ME-QMLE) for the parameters of the SAR model when covariates are measured with error. The ME-QMLE possesses statistical consistency and asymptotic normality properties and we derive its limiting covariance. We consider two types of applications. The first is when the true covariate is imprecisely measured with replicated measurements or cannot be measured directly, and a proxy is observed instead. The second one involves including latent homophily factors estimated with error from the network for estimating peer influence. Our numerical results verify the bias correction property of the estimator and the accuracy of the standard error estimates in finite samples. We illustrate the method on two real datasets; i) peer influence in GPA for middle school students in New Jersey and ii) county-level death rates from the COVID-19 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04593v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhadeep Paul, Shanjukta Nath</dc:creator>
    </item>
    <item>
      <title>Instrumental Variable Estimation of Distributional Causal Effects</title>
      <link>https://arxiv.org/abs/2406.19986</link>
      <description>arXiv:2406.19986v2 Announce Type: replace 
Abstract: Estimating the causal effect of a treatment on the entire response distribution is an important yet challenging task. For instance, one might be interested in how a pension plan affects not only the average savings among all individuals but also how it affects the entire savings distribution. While sufficiently large randomized studies can be used to estimate such distributional causal effects, they are often either not feasible in practice or involve non-compliance. A well-established class of methods for estimating average causal effects from either observational studies with unmeasured confounding or randomized studies with non-compliance are instrumental variable (IV) methods. In this work, we develop an IV-based approach for identifying and estimating distributional causal effects. We introduce a distributional IV model with corresponding assumptions, which leads to a novel identification result for the interventional cumulative distribution function (CDF) under a binary treatment. We then use this identification to construct a nonparametric estimator, called DIVE, for estimating the interventional CDFs under both treatments. We empirically assess the performance of DIVE in a simulation experiment and illustrate the usefulness of distributional causal effects on two real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19986v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kook, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>Enhanced Local Explainability and Trust Scores with Random Forest Proximities</title>
      <link>https://arxiv.org/abs/2310.12428</link>
      <description>arXiv:2310.12428v3 Announce Type: replace-cross 
Abstract: We initiate a novel approach to explain the predictions and out of sample performance of random forest (RF) regression and classification models by exploiting the fact that any RF can be mathematically formulated as an adaptive weighted K nearest-neighbors model. Specifically, we employ a recent result that, for both regression and classification tasks, any RF prediction can be rewritten exactly as a weighted sum of the training targets, where the weights are RF proximities between the corresponding pairs of data points. We show that this linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established feature-based methods like SHAP, which generate attributions for a model prediction across input features. We show how this proximity-based approach to explainability can be used in conjunction with SHAP to explain not just the model predictions, but also out-of-sample performance, in the sense that proximities furnish a novel means of assessing when a given model prediction is more or less likely to be correct. We demonstrate this approach in the modeling of US corporate bond prices and returns in both regression and classification cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12428v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Rosaler, Dhruv Desai, Bhaskarjit Sarmah, Dimitrios Vamvourellis, Deran Onay, Dhagash Mehta, Stefano Pasquali</dc:creator>
    </item>
  </channel>
</rss>
