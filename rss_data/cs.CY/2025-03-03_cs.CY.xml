<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Broken Letters, Broken Narratives: A Case Study on Arabic Script in DALL-E 3</title>
      <link>https://arxiv.org/abs/2502.20459</link>
      <description>arXiv:2502.20459v1 Announce Type: new 
Abstract: Text-to-image generative AI systems exhibit significant limitations when engaging with under-represented domains, including non-Western art forms, often perpetuating biases and misrepresentations. We present a focused case study on the generative AI system DALL-E 3, examining its inability to properly represent calligraphic Arabic script, a culturally significant art form. Through a critical analysis of the generated outputs, we explore these limitations, emerging biases, and the broader implications in light of Edward Said's concept of Orientalism as well as historical examples of pseudo-Arabic. We discuss how misrepresentations persist in new technological contexts and what consequences they may have.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20459v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arshia Sobhan, Philippe Pasquier, Gabriela Aceves Sepulveda</dc:creator>
    </item>
    <item>
      <title>Software development projects as a way for multidisciplinary soft and future skills education</title>
      <link>https://arxiv.org/abs/2502.21114</link>
      <description>arXiv:2502.21114v1 Announce Type: new 
Abstract: Soft and future skills are in high demand in the modern job market. These skills are required for both technical and non-technical people. It is difficult to teach these competencies in a classical academic environment.
  The paper presents a possible approach to teaching in soft and future skills in a short, intensive joint project. In our case, it is a project within the Erasmus+ framework, but it can be organized in many different frameworks.
  In the project we use problem based learning, active learning and group-work teaching methodologies. Moreover, the approach put high emphasizes diversity. We arrange a set of multidisciplinary students in groups. Each group is working on software development tasks. This type of projects demand diversity, and only a part of the team needs technical skills. In our case less than half of participants had computer science background. Additionally, software development projects are usually interesting for non-technical students.
  The multicultural, multidisciplinary and international aspects are very important in a modern global working environment. On the other hand, short time of the project and its intensity allow to simulate stressful situations in a real word tasks. The effects of the project on the required competencies are measured using the KYSS method.
  The results prove that the presented method increased participants soft skills in communication, cooperation, digital skills and self reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21114v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krzysztof Podlaski, Michal Beczkowski, Katharina Simbeck, Katrin Dziergwa, Derek O'Reilly, Shane Dowdall, Joao Monteiro, Catarina Oliveira Lucas, Johanna Hautamaki, Heikki Ahonen, Hiram Bollaert, Philippe Possemiers, Zofia Stawska</dc:creator>
    </item>
    <item>
      <title>Digital Doppelgangers: Ethical and Societal Implications of Pre-Mortem AI Clones</title>
      <link>https://arxiv.org/abs/2502.21248</link>
      <description>arXiv:2502.21248v1 Announce Type: new 
Abstract: The rapid advancement of generative AI has enabled the creation of pre-mortem digital twins, AI-driven replicas that mimic the behavior, personality, and knowledge of living individuals. These digital doppelgangers serve various functions, including enhancing productivity, enabling creative collaboration, and preserving personal legacies. However, their development raises critical ethical, legal, and societal concerns. Issues such as identity fragmentation, psychological effects on individuals and their social circles, and the risks of unauthorized cloning and data exploitation demand careful examination. Additionally, as these AI clones evolve into more autonomous entities, concerns about consent, ownership, and accountability become increasingly complex.
  This paper differentiates pre-mortem AI clones from post-mortem generative ghosts, examining their unique ethical and legal implications. We explore key challenges, including the erosion of personal identity, the implications of AI agency, and the regulatory gaps in digital rights and privacy laws. Through a research-driven approach, we propose a framework for responsible AI governance, emphasizing identity preservation, consent mechanisms, and autonomy safeguards. By aligning technological advancements with societal values, this study contributes to the growing discourse on AI ethics and provides policy recommendations for the ethical deployment of pre-mortem AI clones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21248v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijayalaxmi Methuku, Praveen Kumar Myakala</dc:creator>
    </item>
    <item>
      <title>Among Them: A game-based framework for assessing persuasion capabilities of LLMs</title>
      <link>https://arxiv.org/abs/2502.20426</link>
      <description>arXiv:2502.20426v1 Announce Type: cross 
Abstract: The proliferation of large language models (LLMs) and autonomous AI agents has raised concerns about their potential for automated persuasion and social influence. While existing research has explored isolated instances of LLM-based manipulation, systematic evaluations of persuasion capabilities across different models remain limited. In this paper, we present an Among Us-inspired game framework for assessing LLM deception skills in a controlled environment. The proposed framework makes it possible to compare LLM models by game statistics, as well as quantify in-game manipulation according to 25 persuasion strategies from social psychology and rhetoric. Experiments between 8 popular language models of different types and sizes demonstrate that all tested models exhibit persuasive capabilities, successfully employing 22 of the 25 anticipated techniques. We also find that larger models do not provide any persuasion advantage over smaller models and that longer model outputs are negatively correlated with the number of games won. Our study provides insights into the deception capabilities of LLMs, as well as tools and data for fostering future research on the topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20426v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Idziejczak, Vasyl Korzavatykh, Mateusz Stawicki, Andrii Chmutov, Marcin Korcz, Iwo B{\l}\k{a}dek, Dariusz Brzezinski</dc:creator>
    </item>
    <item>
      <title>Large Language Model Strategic Reasoning Evaluation through Behavioral Game Theory</title>
      <link>https://arxiv.org/abs/2502.20432</link>
      <description>arXiv:2502.20432v1 Announce Type: cross 
Abstract: Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20432v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen</dc:creator>
    </item>
    <item>
      <title>HELENE: An Open-Source High-Security Privacy-Preserving Blockchain Based System for Automating and Managing Laboratory Health Tests</title>
      <link>https://arxiv.org/abs/2502.20477</link>
      <description>arXiv:2502.20477v1 Announce Type: cross 
Abstract: In the last years, especially since the COVID-19 pandemic, precision medicine platforms emerged as useful tools for supporting new tests like the ones that detect the presence of antibodies and antigens with better sensitivity and specificity than traditional methods. In addition, the pandemic has also influenced the way people interact (decentralization), behave (digital world) and purchase health services (online). Moreover, there is a growing concern in the way health data are managed, especially in terms of privacy. To tackle such issues, this article presents a sustainable direct-to-consumer health-service open-source platform called HELENE that is supported by blockchain and by a novel decentralized oracle that protects patient data privacy. Specifically, HELENE enables health test providers to compete through auctions, allowing patients to bid for their services and to keep the control over their health test results. Moreover, data exchanges among the involved stakeholders can be performed in a trustworthy, transparent and standardized way to ease software integration and to avoid incompatibilities. After providing a thorough description of the platform, the proposed health platform is assessed in terms of smart contract performance. In addition, the response time of the developed oracle is evaluated and NIST SP 800-22 tests are executed to demonstrate the adequacy of the devised random number generator. Thus, this article shows the capabilities and novel propositions of HELENE for delivering health services providing an open-source platform for future researchers, who can enhance it and adapt it to their needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20477v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Fern\'andez-Blanco, Pedro Garc\'ia-Cereijo, David Lema-N\'u\~nez, Diego Ramil-L\'opez, Paula Fraga-Lamas, Leire Egia-Mendikute, As\'is Palaz\'on, Tiago M. Fern\'andez-Caram\'es</dc:creator>
    </item>
    <item>
      <title>Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in Programming Education</title>
      <link>https://arxiv.org/abs/2502.20527</link>
      <description>arXiv:2502.20527v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being explored in higher education, yet their effectiveness as teaching agents remains underexamined. In this paper, we present the development of GuideLM, a fine-tuned LLM designed for programming education. GuideLM has been integrated into the Debugging C Compiler (DCC), an educational C compiler that leverages LLMs to generate pedagogically sound error explanations. Previously, DCC relied on off-the-shelf OpenAI models, which, while accurate, often over-assisted students by directly providing solutions despite contrary prompting.
  To address this, we employed supervised fine-tuning (SFT) on a dataset of 528 student-question/teacher-answer pairs, creating two models: GuideLM and GuideLM-mini, fine-tuned on ChatGPT-4o and 4o-mini, respectively. We conducted an expert analysis of 400 responses per model, comparing their pedagogical effectiveness against base OpenAI models. Our evaluation, grounded in constructivism and cognitive load theory, assessed factors such as conceptual scaffolding, clarity, and Socratic guidance.
  Results indicate that GuideLM and GuideLM-mini improve pedagogical performance, with an 8% increase in Socratic guidance and a 58% improvement in economy of words compared to GPT-4o. However, this refinement comes at the cost of a slight reduction in general accuracy. While further work is needed, our findings suggest that fine-tuning LLMs with targeted datasets is a promising approach for developing models better suited to educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20527v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily Ross, Yuval Kansal, Jake Renzella, Alexandra Vassar, Andrew Taylor</dc:creator>
    </item>
    <item>
      <title>Displaying Fear, Sadness, and Joy in Public: Schizophrenia Vloggers' Video Narration of Emotion and Online Care-Seeking</title>
      <link>https://arxiv.org/abs/2502.20658</link>
      <description>arXiv:2502.20658v1 Announce Type: cross 
Abstract: Individuals with severe mental illnesses (SMI), particularly schizophrenia, experience complex and intense emotions frequently. They increasingly turn to vlogging as an authentic medium for emotional disclosure and online support-seeking. While previous research has primarily focused on text-based disclosure, little is known about how people construct narratives around emotions and emotional experiences through video blogs. Our study analyzed 401 YouTube videos created by schizophrenia vloggers, revealing that vloggers disclosed their fear, sadness, and joy through verbal narration by explicit expressions or storytelling. Visually, they employed various framing styles, including Anonymous, Talk-to-Camera, and In-the-Moment approaches, along with diverse visual narration techniques. Notably, we uncovered a concerning 'visual appeal disparity' in audience engagement, with visually appealing videos receiving significantly more views, likes, and comments. This study discusses the role of video-sharing platforms in emotional expression and offers design implications for fostering online care-seeking for emotionally vulnerable populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20658v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaying "Lizzy" Liu, Yunlong Wang, Allen Jue, Yao Lyu, Yiheng Su, Shuo Niu, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Why Trust in AI May Be Inevitable</title>
      <link>https://arxiv.org/abs/2502.20701</link>
      <description>arXiv:2502.20701v1 Announce Type: cross 
Abstract: In human-AI interactions, explanation is widely seen as necessary for enabling trust in AI systems. We argue that trust, however, may be a pre-requisite because explanation is sometimes impossible. We derive this result from a formalization of explanation as a search process through knowledge networks, where explainers must find paths between shared concepts and the concept to be explained, within finite time. Our model reveals that explanation can fail even under theoretically ideal conditions - when actors are rational, honest, motivated, can communicate perfectly, and possess overlapping knowledge. This is because successful explanation requires not just the existence of shared knowledge but also finding the connection path within time constraints, and it can therefore be rational to cease attempts at explanation before the shared knowledge is discovered. This result has important implications for human-AI interaction: as AI systems, particularly Large Language Models, become more sophisticated and able to generate superficially compelling but spurious explanations, humans may default to trust rather than demand genuine explanations. This creates risks of both misplaced trust and imperfect knowledge integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20701v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nghi Truong, Phanish Puranam, Ilia Testlin</dc:creator>
    </item>
    <item>
      <title>Optimizing Large Language Models for ESG Activity Detection in Financial Texts</title>
      <link>https://arxiv.org/abs/2502.21112</link>
      <description>arXiv:2502.21112v1 Announce Type: cross 
Abstract: The integration of Environmental, Social, and Governance (ESG) factors into corporate decision-making is a fundamental aspect of sustainable finance. However, ensuring that business practices align with evolving regulatory frameworks remains a persistent challenge. AI-driven solutions for automatically assessing the alignment of sustainability reports and non-financial disclosures with specific ESG activities could greatly support this process. Yet, this task remains complex due to the limitations of general-purpose Large Language Models (LLMs) in domain-specific contexts and the scarcity of structured, high-quality datasets. In this paper, we investigate the ability of current-generation LLMs to identify text related to environmental activities. Furthermore, we demonstrate that their performance can be significantly enhanced through fine-tuning on a combination of original and synthetically generated data. To this end, we introduce ESG-Activities, a benchmark dataset containing 1,325 labelled text segments classified according to the EU ESG taxonomy. Our experimental results show that fine-tuning on ESG-Activities significantly enhances classification accuracy, with open models such as Llama 7B and Gemma 7B outperforming large proprietary solutions in specific configurations. These findings have important implications for financial analysts, policymakers, and AI researchers seeking to enhance ESG transparency and compliance through advanced natural language processing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21112v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattia Birti, Francesco Osborne, Andrea Maurino</dc:creator>
    </item>
    <item>
      <title>Identifying Emerging Concepts in Large Corpora</title>
      <link>https://arxiv.org/abs/2502.21315</link>
      <description>arXiv:2502.21315v1 Announce Type: cross 
Abstract: We introduce a new method to identify emerging concepts in large text corpora. By analyzing changes in the heatmaps of the underlying embedding space, we are able to detect these concepts with high accuracy shortly after they originate, in turn outperforming common alternatives. We further demonstrate the utility of our approach by analyzing speeches in the U.S. Senate from 1941 to 2015. Our results suggest that the minority party is more active in introducing new concepts into the Senate discourse. We also identify specific concepts that closely correlate with the Senators' racial, ethnic, and gender identities. An implementation of our method is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21315v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sibo Ma, Julian Nyarko</dc:creator>
    </item>
    <item>
      <title>Towards AI Accountability Infrastructure: Gaps and Opportunities in AI Audit Tooling</title>
      <link>https://arxiv.org/abs/2402.17861</link>
      <description>arXiv:2402.17861v3 Announce Type: replace 
Abstract: Audits are critical mechanisms for identifying the risks and limitations of deployed artificial intelligence (AI) systems. However, the effective execution of AI audits remains incredibly difficult, and practitioners often need to make use of various tools to support their efforts. Drawing on interviews with 35 AI audit practitioners and a landscape analysis of 435 tools, we compare the current ecosystem of AI audit tooling to practitioner needs. While many tools are designed to help set standards and evaluate AI systems, they often fall short in supporting accountability. We outline challenges practitioners faced in their efforts to use AI audit tools and highlight areas for future tool development beyond evaluation -- from harms discovery to advocacy. We conclude that the available resources do not currently support the full scope of AI audit practitioners' needs and recommend that the field move beyond tools for just evaluation and towards more comprehensive infrastructure for AI accountability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17861v3</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713301</arxiv:DOI>
      <dc:creator>Victor Ojewale, Ryan Steed, Briana Vecchione, Abeba Birhane, Inioluwa Deborah Raji</dc:creator>
    </item>
    <item>
      <title>Generative AI Policies under the Microscope: How CS Conferences Are Navigating the New Frontier in Scholarly Writing</title>
      <link>https://arxiv.org/abs/2410.11977</link>
      <description>arXiv:2410.11977v3 Announce Type: replace 
Abstract: As the use of Generative AI (Gen-AI) in scholarly writing and peer reviews continues to rise, it is essential for the computing field to establish and adopt clear Gen-AI policies. This study examines the landscape of Gen-AI policies across 64 major Computer Science conferences and offers recommendations for promoting more effective and responsible use of Gen-AI in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11977v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahjabin Nahar, Sian Lee, Rebekah Guillen, Dongwon Lee</dc:creator>
    </item>
    <item>
      <title>The Third Moment of AI Ethics: Developing Relatable and Contextualized Tools</title>
      <link>https://arxiv.org/abs/2501.16954</link>
      <description>arXiv:2501.16954v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) ethics has gained significant momentum, evidenced by the growing body of published literature, policy guidelines, and public discourse. However, the practical implementation and adoption of AI ethics principles among practitioners has not kept pace with this theoretical development. Common barriers to adoption include overly abstract language, poor accessibility, and insufficient practical guidance for implementation. Through participatory design with industry practitioners, we developed an open-source tool that bridges this gap. Our tool is firmly grounded in normative ethical frameworks while offering concrete, actionable guidance in an intuitive format that aligns with established software development workflows. We validated this approach through a proof of concept study in the United States autonomous driving industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16954v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Hladikova, Yuling Wang, Andreia Martinho</dc:creator>
    </item>
    <item>
      <title>Maximal Extractable Value in Decentralized Finance: Taxonomy, Detection, and Mitigation</title>
      <link>https://arxiv.org/abs/2411.03327</link>
      <description>arXiv:2411.03327v2 Announce Type: replace-cross 
Abstract: Decentralized Finance (DeFi) leverages blockchain-enabled smart contracts to deliver automated and trustless financial services without the need for intermediaries. However, the public visibility of financial transactions on the blockchain can be exploited, as participants can reorder, insert, or remove transactions to extract value, often at the expense of others. This extracted value is known as the Maximal Extractable Value (MEV). MEV causes financial losses and consensus instability, disrupting the security, efficiency, and decentralization goals of the DeFi ecosystem. Therefore, it is crucial to analyze, detect, and mitigate MEV to safeguard DeFi. Our comprehensive survey offers a holistic view of the MEV landscape in the DeFi ecosystem. We present an in-depth understanding of MEV through a novel taxonomy of MEV transactions supported by real transaction examples. We perform a critical comparative analysis of various MEV detection approaches, evaluating their effectiveness in identifying different transaction types. Furthermore, we assess different categories of MEV mitigation strategies and discuss their limitations. We identify the challenges of current mitigation and detection approaches and discuss potential solutions. This survey provides valuable insights for researchers, developers, stakeholders, and policymakers, helping to curb and democratize MEV for a more secure and efficient DeFi ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03327v2</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huned Materwala, Shraddha M. Naik, Aya Taha, Tala Abdulrahman Abed, Davor Svetinovic</dc:creator>
    </item>
    <item>
      <title>Web Privacy based on Contextual Integrity: Measuring the Collapse of Online Contexts</title>
      <link>https://arxiv.org/abs/2412.16246</link>
      <description>arXiv:2412.16246v2 Announce Type: replace-cross 
Abstract: The collapse of social contexts has been amplified by digital infrastructures but surprisingly received insufficient attention from Web privacy scholars. Users are persistently identified within and across distinct Web contexts, in varying degrees, through and by different websites and trackers, losing the ability to maintain a fragmented identity. To systematically evaluate this structural privacy harm, we operationalize the theory of Privacy as Contextual Integrity and measure persistent user identification within and between distinct Web contexts. We crawl the top-700 popular websites across the contexts of health, finance, news \&amp; media, LGBTQ, eCommerce, adult, and education websites, for 27 days, and created network graphs to learn how persistent browser identification via third-party cookies and JavaScript fingerprinting is diffused within and between Web contexts. Past work measured Web tracking in bulk, highlighting the volume of trackers and tracking techniques. These measurements miss a crucial privacy implication of Web tracking - the collapse of online contexts. Our findings reveal how persistent browser identification varies between and within contexts, diffusing user IDs to different distances, contrasting known tracking distributions across websites, and conducted as a joint or separate effort via cookie IDs and JS fingerprinting. Our network analysis informs the construction of browsers' storage containers to protect users against real-time context collapse. This is a first modest step in measuring Web privacy as Contextual Integrity, opening new avenues for contextual Web privacy research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16246v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ido Sivan-Sevilla, Parthav Poudel</dc:creator>
    </item>
    <item>
      <title>Firewalls to Secure Dynamic LLM Agentic Networks</title>
      <link>https://arxiv.org/abs/2502.01822</link>
      <description>arXiv:2502.01822v2 Announce Type: replace-cross 
Abstract: Future LLM agents are likely to communicate on behalf of users with other entity-representing agents on tasks that entail long-horizon plans with interdependent goals. Current work does not focus on such agentic networks, nor does it address their challenges. Thus, we first identify the required properties of agents' communication, which should be proactive and adaptable. It needs to satisfy 1) privacy: agents should not share more than what is needed for the task, and 2) security: the communication must preserve integrity and maintain utility against selfish entities. We design a use case (travel planning) as a testbed that exemplifies these requirements, and we show examples of how this can go wrong. Next, we propose a practical design, inspired by established network security principles, for constrained LLM agentic networks that balance adaptability, security, and privacy. Our framework automatically constructs and updates task-specific rules from prior simulations to build firewalls. We offer layers of defense to 1) convert free-form input to a task-specific protocol, 2) dynamically abstract users' data to a task-specific degree of permissiveness, and 3) self-correct the agents' trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01822v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, Reza Shokri</dc:creator>
    </item>
  </channel>
</rss>
