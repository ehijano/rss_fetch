<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2024 02:43:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Understanding algorithmic fairness for clinical prediction in terms of subgroup net benefit and health equity</title>
      <link>https://arxiv.org/abs/2412.07879</link>
      <description>arXiv:2412.07879v1 Announce Type: new 
Abstract: There are concerns about the fairness of clinical prediction models. 'Fair' models are defined as those for which their performance or predictions are not inappropriately influenced by protected attributes such as ethnicity, gender, or socio-economic status. Researchers have raised concerns that current algorithmic fairness paradigms enforce strict egalitarianism in healthcare, levelling down the performance of models in higher-performing subgroups instead of improving it in lower-performing ones.
  We propose assessing the fairness of a prediction model by expanding the concept of net benefit, using it to quantify and compare the clinical impact of a model in different subgroups. We use this to explore how a model distributes benefit across a population, its impact on health inequalities, and its role in the achievement of health equity. We show how resource constraints might introduce necessary trade-offs between health equity and other objectives of healthcare systems.
  We showcase our proposed approach with the development of two clinical prediction models: 1) a prognostic type 2 diabetes model used by clinicians to enrol patients into a preventive care lifestyle intervention programme, and 2) a lung cancer screening algorithm used to allocate diagnostic scans across the population. This approach helps modellers better understand if a model upholds health equity by considering its performance in a clinical and social context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07879v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Benitez-Aurioles, Alice Joules, Irene Brusini, Niels Peek, Matthew Sperrin</dc:creator>
    </item>
    <item>
      <title>The continuous net benefit: Assessing the clinical utility of prediction models when informing a continuum of decisions</title>
      <link>https://arxiv.org/abs/2412.07882</link>
      <description>arXiv:2412.07882v1 Announce Type: new 
Abstract: Clinical prognostic models help inform decision-making by estimating a patient's risk of experiencing an outcome in the future. The net benefit is increasingly being used to assess the clinical utility of models. By calculating an appropriately weighted average of the true and false positives of a model, the net benefit assesses the value added by a binary decision policy obtained when thresholding a model. Although such 'treat or not' decisions are common, prognostic models are also often used to tailor and personalise the care of patients, which implicitly involves the consideration of multiple interventions at different risk thresholds.
  We extend the net benefit to consider multiple decision thresholds simultaneously, by taking a weighted area under a rescaled version of the net benefit curve, deriving the continuous net benefit. In addition to the consideration of a continuum of interventions, we also show how the continuous net benefit can be used for populations with a range of optimal thresholds for a single treatment, due to individual variations in expected treatment benefit or harm, highlighting limitations of current proposed methods that calculate the area under the decision curve. We showcase the continuous net benefit through two examples of cardiovascular preventive care, comparing two modelling choices using the continuous net benefit.
  The continuous net benefit informs researchers of the clinical utility of models during selection, development, and validation, and helps decision makers understand their usefulness, improving their viability towards implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07882v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Benitez-Aurioles, Laure Wynants, Niels Peek, Patrick Goodley, Philip Crosbie, Matthew Sperrin</dc:creator>
    </item>
    <item>
      <title>Unsupervised Detection of Anomalous Driving Patterns Using High Resolution Telematics Time Series Data</title>
      <link>https://arxiv.org/abs/2412.08106</link>
      <description>arXiv:2412.08106v1 Announce Type: new 
Abstract: Vehicle telematics provides granular data for dynamic driving risk assessment, but current methods often rely on aggregated metrics (e.g., harsh braking counts) and do not fully exploit the rich time-series structure of telematics data. In this paper, we introduce a flexible framework using continuous-time hidden Markov model (CTHMM) to model and analyze trip-level telematics data. Unlike existing methods, the CTHMM models raw time-series data without predefined thresholds on harsh driving events or assumptions about accident probabilities. Moreover, our analysis is based solely on telematics data, requiring no traditional covariates such as driver or vehicle characteristics. Through unsupervised anomaly detection based on pseudo-residuals, we identify deviations from normal driving patterns -- defined as the prevalent behaviour observed in a driver's history or across the population -- which are linked to accident risk. Validated on both controlled and real-world datasets, the CTHMM effectively detects abnormal driving behaviour and trips with increased accident likelihood. In real-data analysis, higher anomaly levels in longitudinal and lateral accelerations consistently correlate with greater accident risk, with classification models using this information achieving ROC-AUC values as high as 0.86 for trip-level analysis and 0.78 for distinguishing drivers with claims. Furthermore, the methodology reveals significant behavioural differences between drivers with and without claims, offering valuable insights for insurance applications, accident analysis, and prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08106v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Weng Chan, Andrei L. Badescu, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Spatial similarity index for scouting in football</title>
      <link>https://arxiv.org/abs/2412.08303</link>
      <description>arXiv:2412.08303v1 Announce Type: new 
Abstract: Finding players with similar profiles is an important problem in sports such as football. Scouting for new players requires a wealth of information about the available players so that similar profiles to that of a target player can be identified. However, information about the position of the players in the field is seldom used. For this reason, a novel approach based on spatial data analysis is introduced to produce a spatial similarity index that can help to identify similar players. The use of this new spatial similarity index is illustrated to identify similar players using spatial data from the Spanish competition "La Liga", season 2019-2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08303v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Virgilio G\'omez-Rubio, Jes\'us Lagos, Francisco Palm\'i-Perales</dc:creator>
    </item>
    <item>
      <title>Application of Markov Chains to Multiple Sclerosis Clinical Trial Data to Estimate Disease Trajectories</title>
      <link>https://arxiv.org/abs/2412.08364</link>
      <description>arXiv:2412.08364v1 Announce Type: new 
Abstract: Background: Multiple Sclerosis (MS), an autoimmune disease affecting millions worldwide, is characterized by its variable course, in which some patients will experience a more benign disease course and others a more active one, with the latter leading to permanent neural damage and disability. Methods: This study uses a Markov Chain model to demonstrate the probability of movement across different states on the Expanded Disability Status Scale (EDSS) and attempted to define worsening, improvement, cycling, and stability of these different pathways. Most importantly we were interested in assessing the lack of impermanence of confirmed disability worsening and if it could be estimated from the Markov model. Results: The study identified only 8.1% were considered worsening, 5.6% consistent improving and 86% cyclers and less than 1% consistently stable. More importantly we also found that many (approximately 30%) of participants with confirmed disability worsening (CDW) regressed to stages that were not considered worsening, on subsequent visits after CDW. Conclusions: These finding are similar to what has been reported previously as predictors of worsening, and also for a lack of durability of CDW, but our results suggest that clinical trial endpoints may need to be modified to more accurately capture differences between the treatment and control groups. Further, this suggests that the rate of worsening in trials that use time to CDW are overestimating the extent of CDW. The trials remain valid since the regressing applies to both treatment and control groups, but that the results may be underestimating the treatment benefit due to misclassification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08364v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Uma Sthanu, Gary Cutter PhD</dc:creator>
    </item>
    <item>
      <title>Outcome-guided spike-and-slab Lasso Biclustering: A Novel Approach for Enhancing Biclustering Techniques for Gene Expression Analysis</title>
      <link>https://arxiv.org/abs/2412.08416</link>
      <description>arXiv:2412.08416v1 Announce Type: new 
Abstract: Biclustering has gained interest in gene expression data analysis due to its ability to identify groups of samples that exhibit similar behaviour in specific subsets of genes (or vice versa), in contrast to traditional clustering methods that classify samples based on all genes. Despite advances, biclustering remains a challenging problem, even with cutting-edge methodologies. This paper introduces an extension of the recently proposed Spike-and-Slab Lasso Biclustering (SSLB) algorithm, termed Outcome-Guided SSLB (OG-SSLB), aimed at enhancing the identification of biclusters in gene expression analysis. Our proposed approach integrates disease outcomes into the biclustering framework through Bayesian profile regression. By leveraging additional clinical information, OG-SSLB improves the interpretability and relevance of the resulting biclusters. Comprehensive simulations and numerical experiments demonstrate that OG-SSLB achieves superior performance, with improved accuracy in estimating the number of clusters and higher consensus scores compared to the original SSLB method. Furthermore, OG-SSLB effectively identifies meaningful patterns and associations between gene expression profiles and disease states. These promising results demonstrate the effectiveness of OG-SSLB in advancing biclustering techniques, providing a powerful tool for uncovering biologically relevant insights. The OGSSLB software can be found as an R/C++ package at https://github.com/luisvargasmieles/OGSSLB .</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08416v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis A. Vargas-Mieles, Paul D. W. Kirk, Chris Wallace</dc:creator>
    </item>
    <item>
      <title>Adaptive Phase 2/3 Design with Dose Optimization</title>
      <link>https://arxiv.org/abs/2412.08439</link>
      <description>arXiv:2412.08439v1 Announce Type: new 
Abstract: FDA's Project Optimus initiative for oncology drug development emphasizes selecting a dose that optimizes both efficacy and safety. When an inferentially adaptive Phase 2/3 design with dose selection is implemented to comply with the initiative, the conventional inverse normal combination test is commonly used for Type I error control. However, indiscriminate application of this overly conservative test can lead to substantial increase in sample size and timeline delays, which undermines the appeal of the adaptive approach. This, in turn, frustrates drug developers regarding Project Optimus.
  The inflation of Type I error depends on the probability of selecting a dose with better long-term efficacy outcome at end of the study based on limited follow-up data at dose selection. In this paper, we discuss the estimation of this probability and its impact on Type I error control in realistic settings. Incorporating it explicitly into the two methods we have proposed result in improved designs, potentially motivating drug developers to adhere more closely to an initiative that has the potential to revolutionize oncology drug development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08439v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cong Chen, Mo Huang, Xuekui Zhang</dc:creator>
    </item>
    <item>
      <title>Enhancing the use of family planning service statistics using a Bayesian modelling approach to inform estimates of modern contraceptive use in low- and middle-income countries</title>
      <link>https://arxiv.org/abs/2412.08606</link>
      <description>arXiv:2412.08606v1 Announce Type: new 
Abstract: Monitoring family planning indicators, such as modern contraceptive prevalence rate (mCPR), is essential for family planning programming. The Family Planning Estimation Tool (FPET) uses survey data to estimate and forecast family planning indicators, including mCPR, over time. However, sole reliance on large-scale surveys, carried out on average every 3-5 years, can lead to data gaps. Service statistics are a readily available data source, routinely collected in conjunction with service delivery. Various service statistics data types can be used to derive a family planning indicator called Estimated Modern Use (EMU). In a number of countries, annual rates of change in EMU have been found to be predictive of true rates of change in mCPR. However, it has been challenging to capture the varying levels of uncertainty associated with the EMU indicator across different countries and service statistics data types and to subsequently quantify this uncertainty when using EMU in FPET. We present a new approach to using EMUs in FPET to inform mCPR estimates, using annual EMU rates of change as input, and accounting for uncertainty associated with the EMU derivation process. The approach also considers additional country-type-specific uncertainty. We assess the EMU type-specific uncertainty at the country level, via a Bayesian hierarchical modelling approach. Validation results and anonymised country-level case studies highlight improved predictive performance and provide insights into the impact of including EMU data on mCPR estimates compared to using survey data alone. Together, they demonstrate that EMUs can help countries monitor progress toward their family planning goals more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08606v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shauna Mooney, Leontine Alkema, Emily Sonneveldt, Kristin Bietsch, Jessica Williamson, Niamh Cahill</dc:creator>
    </item>
    <item>
      <title>Dynamical modelling of the frailty index indicates that health reaches a tipping point near age 75</title>
      <link>https://arxiv.org/abs/2412.07795</link>
      <description>arXiv:2412.07795v1 Announce Type: cross 
Abstract: The frailty index (FI) serves as a useful quantitative summary of age-related health. We quantitatively modelled FI trajectories with age. We fit directly to longitudinal transitions in health attributes from normal to deficit and vice-versa. We used data from two large longitudinal studies: the Health and Retirement Study and the English Longitudinal Study of Ageing. The studies included 47592 individuals with 254357 total visits. Using damage (deficit emergence) and repair (deficit recovery) transitions we estimated changes to robustness and resilience, respectively. We find that both robustness and resilience decrease continuously with both increasing age and FI. Remarkably, these declines caused a tipping point in health near age 75, when damage and repair rates are equal. Beyond this tipping point, the ongoing loss of both robustness and resilience leads to a sharp increase in the FI and a commensurate increase in risk of mortality. This tipping point was observed in both sexes, noting that males showed higher initial robustness and resilience, and commensurately steeper decline, consistent with the sex-frailty paradox. We infer that robustness and resilience mitigate environmental stressors only up to an age of 75, beyond which health deficits will increasingly accumulate leading to death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07795v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen Pridham, Kenneth Rockwood, Andrew D. Rutenberg</dc:creator>
    </item>
    <item>
      <title>How Should We Represent History in Interpretable Models of Clinical Policies?</title>
      <link>https://arxiv.org/abs/2412.07895</link>
      <description>arXiv:2412.07895v1 Announce Type: cross 
Abstract: Modeling policies for sequential clinical decision-making based on observational data is useful for describing treatment practices, standardizing frequent patterns in treatment, and evaluating alternative policies. For each task, it is essential that the policy model is interpretable. Learning accurate models requires effectively capturing the state of a patient, either through sequence representation learning or carefully crafted summaries of their medical history. While recent work has favored the former, it remains a question as to how histories should best be represented for interpretable policy modeling. Focused on model fit, we systematically compare diverse approaches to summarizing patient history for interpretable modeling of clinical policies across four sequential decision-making tasks. We illustrate differences in the policies learned using various representations by breaking down evaluations by patient subgroups, critical states, and stages of treatment, highlighting challenges specific to common use cases. We find that interpretable sequence models using learned representations perform on par with black-box models across all tasks. Interpretable models using hand-crafted representations perform substantially worse when ignoring history entirely, but are made competitive by incorporating only a few aggregated and recent elements of patient history. The added benefits of using a richer representation are pronounced for subgroups and in specific use cases. This underscores the importance of evaluating policy models in the context of their intended use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07895v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Matsson, Lena Stempfle, Yaochen Rao, Zachary R. Margolin, Heather J. Litman, Fredrik D. Johansson</dc:creator>
    </item>
    <item>
      <title>Dynamic Classification of Latent Disease Progression with Auxiliary Surrogate Labels</title>
      <link>https://arxiv.org/abs/2412.08088</link>
      <description>arXiv:2412.08088v1 Announce Type: cross 
Abstract: Disease progression prediction based on patients' evolving health information is challenging when true disease states are unknown due to diagnostic capabilities or high costs. For example, the absence of gold-standard neurological diagnoses hinders distinguishing Alzheimer's disease (AD) from related conditions such as AD-related dementias (ADRDs), including Lewy body dementia (LBD). Combining temporally dependent surrogate labels and health markers may improve disease prediction. However, existing literature models informative surrogate labels and observed variables that reflect the underlying states using purely generative approaches, limiting the ability to predict future states. We propose integrating the conventional hidden Markov model as a generative model with a time-varying discriminative classification model to simultaneously handle potentially misspecified surrogate labels and incorporate important markers of disease progression. We develop an adaptive forward-backward algorithm with subjective labels for estimation, and utilize the modified posterior and Viterbi algorithms to predict the progression of future states or new patients based on objective markers only. Importantly, the adaptation eliminates the need to model the marginal distribution of longitudinal markers, a requirement in traditional algorithms. Asymptotic properties are established, and significant improvement with finite samples is demonstrated via simulation studies. Analysis of the neuropathological dataset of the National Alzheimer's Coordinating Center (NACC) shows much improved accuracy in distinguishing LBD from AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08088v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zexi Cai, Donglin Zeng, Karen S. Marder, Lawrence S. Honig, Yuanjia Wang</dc:creator>
    </item>
    <item>
      <title>A general approach to optimal imperfect maintenance activities of a repairable equipment with imperfect maintenance and multiple failure modes</title>
      <link>https://arxiv.org/abs/2412.08380</link>
      <description>arXiv:2412.08380v1 Announce Type: cross 
Abstract: In this paper we describe a general approach to optimal imperfect maintenance activities of a repairable equipment with independent components. Most of the existing works on optimal imperfect maintenance activities of a repairable equipment with independent components. In addition, it is assumed that all the components of the equipment share the same model and the same maintenance intervals and that effectiveness of maintenance is known. In this paper we take a different approach. In order to formalize the uncertainty on the occurrence of failures and on the effect of maintenance activities we consider, for each component, a class of candidate models obtained combining models for failure rate with models for imperfect maintenance and let the data select the best model (that might be different for the different components). All the parameters are assumed to be unknown and are jointly estimated via maximum likelihood. Model selection is performed, separately for each component, using standard selection criteria that take into account the problem of over-parametrization. The selected models are used to derive the cost per unit time and the average reliability of the equipment, the objective functions of a Multi-Objective Optimization Problem with maintenance intervals of each single component as decision variables. The proposed procedure is illustrated using a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08380v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cie.2018.12.032</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Industrial Engineering 128 (2019) 24-31</arxiv:journal_reference>
      <dc:creator>Rub\'en Mullor, Julio Mulero, Mario Trottini</dc:creator>
    </item>
    <item>
      <title>Understanding North Atlantic Climate Instabilities and Complex Interactions using Data Science</title>
      <link>https://arxiv.org/abs/2001.10171</link>
      <description>arXiv:2001.10171v4 Announce Type: replace 
Abstract: The North Atlantic Oscillation (NAO) index, a measure of sea-level atmospheric pressure variability, holds significant influence over weather patterns in North America and Northern Europe. A negative (positive) NAO value signifies increased cold air outbreaks and storm occurrences (reduced occurrences) in these regions. NAO, a product of multiple climate factors, demonstrates intricate dynamics with sea surface temperature (SST) and sea ice extent (SIE). In this study, we adopt a data-driven approach to explore the complex interplay between NAO, SST, and SIE, revealing a critical instability rooted in positive feedback loops among these climate variables. Our statistical machine learning methodology examines the impacts of melting Arctic SIE and rising SST on NAO, thereby understanding the weather patterns across the North Atlantic region. The skewness analysis yields a negative skewness in NAO across various time intervals -- daily, weekly, and monthly. This skewness, coupled with NAO's mean zero stationary nature, accentuates system instability. To capture these dynamics, we formulate a Bayesian Granger-causal dynamic linear model, which effectively updates the predictor-dependent variable relationship over time. The findings underscore an impending critical instability, indicative of more frequent occurrences of intensely cold climates in eastern North America and northern Europe, theory signifies a notable climate shift. By delving into the intricate feedback mechanisms of NAO, SST, and SIE, our study enhances our comprehension of climate variability, fostering a more informed perspective on the imminent climate changes that lie ahead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.10171v4</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alka Yadav, Sourish Das, Anirban Chakraborti, Sudeep Shukla</dc:creator>
    </item>
    <item>
      <title>Large Row-Constrained Supersaturated Designs for High-throughput Screening</title>
      <link>https://arxiv.org/abs/2407.06173</link>
      <description>arXiv:2407.06173v2 Announce Type: replace-cross 
Abstract: High-throughput screening, in which multiwell plates are used to test large numbers of compounds against specific targets, is widely used across many areas of the biological sciences and most prominently in drug discovery. We propose a statistically principled approach to these screening experiments, using the machinery of supersaturated designs and the Lasso. To accommodate limitations on the number of biological entities that can be applied to a single microplate well, we present a new class of row-constrained supersaturated designs. We develop a computational procedure to construct these designs, provide some initial lower bounds on the average squared off-diagonal values of their main-effects information matrix, and study the impact of the constraint on design quality. We also show via simulation that the proposed constrained row screening method is statistically superior to existing methods and demonstrate the use of the new methodology on a real drug-discovery system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06173v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byran J. Smucker, Stephen E. Wright, Isaac Williams, Richard C. Page, Andor J. Kiss, Surendra Bikram Silwal, Maria Weese, David J. Edwards</dc:creator>
    </item>
    <item>
      <title>Toward Model-Agnostic Detection of New Physics Using Data-Driven Signal Regions</title>
      <link>https://arxiv.org/abs/2409.06960</link>
      <description>arXiv:2409.06960v2 Announce Type: replace-cross 
Abstract: In the search for new particles in high-energy physics, it is crucial to select the Signal Region (SR) in such a way that it is enriched with signal events if they are present. While most existing search methods set the region relying on prior domain knowledge, it may be unavailable for a completely novel particle that falls outside the current scope of understanding. We address this issue by proposing a method built upon a model-agnostic but often realistic assumption about the localized topology of the signal events, in which they are concentrated in a certain area of the feature space. Considering the signal component as a localized high-frequency feature, our approach employs the notion of a low-pass filter. We define the SR as an area which is most affected when the observed events are smeared with additive random noise. We overcome challenges in density estimation in the high-dimensional feature space by learning the density ratio of events that potentially include a signal to the complementary observation of events that closely resemble the target events but are free of any signals. By applying our method to simulated $\mathrm{HH} \rightarrow 4b$ events, we demonstrate that the method can efficiently identify a data-driven SR in a high-dimensional feature space in which a high portion of signal events concentrate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06960v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheun Yi, John Alison, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v2 Announce Type: replace-cross 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by a factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simple hypothesis tests, as well as an algorithm to determine the necessary inflation factor for model parameter fits and Goodness of Fit tests and composite hypothesis tests. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v2</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
  </channel>
</rss>
