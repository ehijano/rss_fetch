<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jun 2024 01:47:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Cyclic Scheduler Design for Minimizing Age of Information in Massive Scale Networks Susceptible to Packet Errors</title>
      <link>https://arxiv.org/abs/2406.15541</link>
      <description>arXiv:2406.15541v1 Announce Type: new 
Abstract: In multi-source status update systems, sources need to be scheduled appropriately to maintain timely communication between each of the sources and the monitor. A cyclic schedule is an age-agnostic schedule in which the sources are served according to a fixed finite transmission pattern, which upon completion, repeats itself. Such a scheme has a low $O(1)$ runtime complexity, which is desirable in large networks. This paper's focus is on designing transmission patterns so as to be used in massive scale networking scenarios involving a very large number of sources, e.g., up to thousands of IoT sources, with service time requirements and weights being heterogeneous in nature. The goal is to minimize the weighted sum age of information (AoI), called weighted AoI, when transmitting users' packets over a channel susceptible to heterogeneous packet errors. The main tool we use is a stochastic modeling framework using either Markov chains (MC) or moment generating functions (MGF), by which we obtain the weighted AoI for a given transmission pattern, which is not straightforward in the presence of packet drops. Using this framework, we provide a lower bound on the weighted AoI for the particular case of two sources, and also an algorithm to attain this lower bound. Then, by using the same framework, we design a cyclic scheduler for general number of sources with reasonable complexity using convex optimization and well-established packet spreading algorithms, and comparatively evaluate the proposed algorithm and existing age-agnostic scheduling schemes for general number of sources (resp.~two sources) when the lower bound is not available (resp.~when it is available). We present extensive numerical results to validate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15541v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahan Liyanaarachchi, Sennur Ulukus, Nail Akar</dc:creator>
    </item>
    <item>
      <title>Wireless MapReduce Arrays for Coded Distributed Computing</title>
      <link>https://arxiv.org/abs/2406.15791</link>
      <description>arXiv:2406.15791v1 Announce Type: new 
Abstract: We consider a wireless distributed computing system based on the MapReduce framework, which consists of three phases: \textit{Map}, \textit{Shuffle}, and \textit{Reduce}. The system consists of a set of distributed nodes assigned to compute arbitrary output functions depending on a file library. The computation of the output functions is decomposed into Map and Reduce functions, and the Shuffle phase, which involves the data exchange, links the two. In our model, the Shuffle phase communication happens over a full-duplex wireless interference channel. For this setting, a coded wireless MapReduce distributed computing scheme exists in the literature, achieving optimal performance under one-shot linear schemes. However, the scheme requires the number of input files to be very large, growing exponentially with the number of nodes. We present schemes that require the number of files to be in the order of the number of nodes and achieve the same performance as the existing scheme. The schemes are obtained by designing a structure called wireless MapReduce array that succinctly represents all three phases in a single array. The wireless MapReduce arrays can also be obtained from the extended placement delivery arrays known for multi-antenna coded caching schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15791v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabath Peter, K. K. Krishnan Namboodiri, B. Sundar Rajan</dc:creator>
    </item>
    <item>
      <title>Linear complementary pairs of codes over a finite non-commutative Frobenius ring</title>
      <link>https://arxiv.org/abs/2406.15794</link>
      <description>arXiv:2406.15794v1 Announce Type: new 
Abstract: In this paper, we study linear complementary pairs (LCP) of codes over finite non-commutative local rings. We further provide a necessary and sufficient condition for a pair of codes $(C,D)$ to be LCP of codes over finite non-commutative Frobenius rings. The minimum distances $d(C)$ and $d(D^\perp)$ are defined as the security parameter for an LCP of codes $(C, D).$ It was recently demonstrated that if $C$ and $D$ are both $2$-sided LCP of group codes over a finite commutative Frobenius rings, $D^\perp$ and $C$ are permutation equivalent in \cite{LL23}. As a result, the security parameter for a $2$-sided group LCP $(C, D)$ of codes is simply $d(C)$. Towards this, we deliver an elementary proof of the fact that for a linear complementary pair of codes $(C,D)$, where $C$ and $D$ are linear codes over finite non-commutative Frobenius rings, under certain conditions, the dual code $D^\perp$ is equivalent to $C.$</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15794v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Bhowmick, Xiusheng Liu</dc:creator>
    </item>
    <item>
      <title>Coded Beam Training for RIS Assisted Wireless Communications</title>
      <link>https://arxiv.org/abs/2406.15802</link>
      <description>arXiv:2406.15802v1 Announce Type: new 
Abstract: Reconfigurable intelligent surface (RIS) is considered as one of the key technologies for future 6G communications. To fully unleash the performance of RIS, accurate channel state information (CSI) is crucial. Beam training is widely utilized to acquire the CSI. However, before aligning the beam correctly to establish stable connections, the signal-to-noise ratio (SNR) at UE is inevitably low, which reduces the beam training accuracy. To deal with this problem, we exploit the coded beam training framework for RIS systems, which leverages the error correction capability of channel coding to improve the beam training accuracy under low SNR. Specifically, we first extend the coded beam training framework to RIS systems by decoupling the base station-RIS channel and the RIS-user channel. For this framework, codewords that accurately steer to multiple angles is essential for fully unleashing the error correction capability. In order to realize effective codeword design in RIS systems, we then propose a new codeword design criterion, based on which we propose a relaxed Gerchberg-Saxton (GS) based codeword design scheme by considering the constant modulus constraints of RIS elements. In addition, considering the two dimensional structure of RIS, we further propose a dimension reduced encoder design scheme, which can not only guarentee a better beam shape, but also enable a stronger error correction capability. Simulation results reveal that the proposed scheme can realize effective and accurate beam training in low SNR scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15802v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhao Chen, Linglong Dai</dc:creator>
    </item>
    <item>
      <title>Permutation Codes Correcting Multiple Deletions</title>
      <link>https://arxiv.org/abs/2406.16656</link>
      <description>arXiv:2406.16656v1 Announce Type: new 
Abstract: Permutation codes in the Ulam metric, which can correct multiple deletions, have been investigated extensively recently owing to their applications. In this work, we are interested in the maximum size of the permutation codes in the Ulam metric and aim to design permutation codes that can correct multiple deletions with efficient decoding algorithms. We first present an improvement on the Gilbert--Varshamov bound of the maximum size of these permutation codes which is the best-known lower bound. Next, we focus on designing permutation codes in the Ulam metric with a decoding algorithm. These constructed codes are the best-known permutation codes that can correct multiple deletions. In particular, the constructed permutation codes can correct $t$ deletions with at most $(3t-1) \log n+o(\log n)$ bits of redundancy where $n$ is the length of the code. Finally, we provide an efficient decoding algorithm for our constructed permutation codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16656v1</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuche Wang, The Nguyen, Yeow Meng Chee, Van Khu Vu</dc:creator>
    </item>
    <item>
      <title>Adaptive Coding for Two-Way Wiretap Channel under Strong Secrecy</title>
      <link>https://arxiv.org/abs/2406.16662</link>
      <description>arXiv:2406.16662v1 Announce Type: new 
Abstract: This paper studies adaptive coding for the two-way wiretap channel. Especially, the strong secrecy metric is of our interest that is defined by the information leakage of transmitted messages to the eavesdropper. First, we consider an adaptive coding, the construction of which is based on running the well studied non-adaptive coding in several rounds and the dependency between the adjacent rounds of transmission is introduced by the key exchange mechanism that is embedded in the non-adaptive coding in each transmission round. As a result, we analyze the reliability and strong secrecy that are measured by the decoding error probability and information leakage, characterize them in terms of the conditional R\'enyi mutual information, and derive inner bounds on the secrecy capacity regions for the TW-WC under strong joint and individual secrecy constraints. Second, we introduce another adaptive coding method that explores the correlation among the outputs at the receivers. With this approach, we show that for the two-way wiretap channel that fulfills the conditionally independent condition, positive transmission rates can be always guaranteed even under the joint secrecy constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16662v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanling Chen, Masahito Hayashi</dc:creator>
    </item>
    <item>
      <title>Decentralized and Centralized IDD Schemes for Cell-Free Networks</title>
      <link>https://arxiv.org/abs/2406.16675</link>
      <description>arXiv:2406.16675v1 Announce Type: new 
Abstract: In this paper, we propose iterative interference cancellation schemes with access points selection (APs-Sel) for cell-free massive multiple-input multiple-output (CF-mMIMO) systems. Closed-form expressions for centralized and decentralized linear minimum mean square error (LMMSE) receive filters with APs-Sel are derived assuming imperfect channel state information (CSI). Furthermore, we develop a list-based detector based on LMMSE receive filters that exploits interference cancellation and the constellation points. A message-passing-based iterative detection and decoding (IDD) scheme that employs low-density parity-check (LDPC) codes is then developed. Moreover, log-likelihood ratio (LLR) refinement strategies based on censoring and a linear combination of local LLRs are proposed to improve the network performance. We compare the cases with centralized and decentralized processing in terms of bit error rate (BER) performance, complexity, and signaling under perfect CSI (PCSI) and imperfect CSI (ICSI) and verify the superiority of the distributed architecture with LLR refinements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16675v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Ssettumba, Z. Shao, L. Landau, R. de Lamare</dc:creator>
    </item>
    <item>
      <title>The EM Algorithm in Information Geometry</title>
      <link>https://arxiv.org/abs/2406.15398</link>
      <description>arXiv:2406.15398v1 Announce Type: cross 
Abstract: The purpose of this thesis is to convey the basic concepts of information geometry and its applications to non-specialists and those in applied fields, assuming only a first-year undergraduate background in calculus, linear algebra, and probability theory / statistics. We first begin with an introduction to the EM algorithm, providing a typical use case in Python, before moving to an overview of basic Riemannian geometry. We then introduce the core concepts of information geometry and the $em$ algorithm, with an explicit calculation of both the $e$ and $m$ projection, before closing with a discussion of an important application of this research to the field of deep learning, providing a novel implementation in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15398v1</guid>
      <category>math.HO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sammy Suliman</dc:creator>
    </item>
    <item>
      <title>Computable one-way functions on the reals</title>
      <link>https://arxiv.org/abs/2406.15817</link>
      <description>arXiv:2406.15817v1 Announce Type: cross 
Abstract: A major open problem in computational complexity is the existence of a one-way function, namely a function from strings to strings which is computationally easy to compute but hard to invert. Levin (2023) formulated the notion of one-way functions from reals (infinite bit-sequences) to reals in terms of computability, and asked whether partial computable one-way functions exist. We give a strong positive answer using the hardness of the halting problem and exhibiting a total computable one-way function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15817v1</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Barmpalias, Xiaoyan Zhang</dc:creator>
    </item>
    <item>
      <title>Automatic AI Model Selection for Wireless Systems: Online Learning via Digital Twinning</title>
      <link>https://arxiv.org/abs/2406.15819</link>
      <description>arXiv:2406.15819v1 Announce Type: cross 
Abstract: In modern wireless network architectures, such as O-RAN, artificial intelligence (AI)-based applications are deployed at intelligent controllers to carry out functionalities like scheduling or power control. The AI "apps" are selected on the basis of contextual information such as network conditions, topology, traffic statistics, and design goals. The mapping between context and AI model parameters is ideally done in a zero-shot fashion via an automatic model selection (AMS) mapping that leverages only contextual information without requiring any current data. This paper introduces a general methodology for the online optimization of AMS mappings. Optimizing an AMS mapping is challenging, as it requires exposure to data collected from many different contexts. Therefore, if carried out online, this initial optimization phase would be extremely time consuming. A possible solution is to leverage a digital twin of the physical system to generate synthetic data from multiple simulated contexts. However, given that the simulator at the digital twin is imperfect, a direct use of simulated data for the optimization of the AMS mapping would yield poor performance when tested in the real system. This paper proposes a novel method for the online optimization of AMS mapping that corrects for the bias of the simulator by means of limited real data collected from the physical system. Experimental results for a graph neural network-based power control app demonstrate the significant advantages of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15819v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushuo Hou, Matteo Zecchin, Sangwoo Park, Yunlong Cai, Guanding Yu, Kaushik Chowdhury, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Full-Space Wireless Sensing Enabled by Multi-Sector Intelligent Surfaces</title>
      <link>https://arxiv.org/abs/2406.15945</link>
      <description>arXiv:2406.15945v2 Announce Type: cross 
Abstract: The multi-sector intelligent surface (IS), benefiting from a smarter wave manipulation capability, has been shown to enhance channel gain and offer full-space coverage in communications. However, the benefits of multi-sector IS in wireless sensing remain unexplored. This paper introduces the application of multi-sector IS for wireless sensing/localization. Specifically, we propose a new self-sensing system, where an active source controller uses the multi-sector IS geometry to reflect/scatter the emitted signals towards the entire space, thereby achieving full-space coverage for wireless sensing. Additionally, dedicated sensors are installed aligned with the IS elements at each sector, which collect echo signals from the target and cooperate to sense the target angle. In this context, we develop a maximum likelihood estimator of the target angle for the proposed multi-sector IS self-sensing system, along with the corresponding theoretical limits defined by the Cram\'er-Rao Bound. The analysis reveals that the advantages of the multi-sector IS self-sensing system stem from two aspects: enhancing the probing power on targets (thereby improving power efficiency) and increasing the rate of target angle (thereby enhancing the transceiver's sensitivity to target angles). Finally, our analysis and simulations confirm that the multi-sector IS self-sensing system, particularly the 4-sector architecture, achieves full-space sensing capability beyond the single-sector IS configuration. Furthermore, similarly to communications, employing directive antenna patterns on each sector's IS elements and sensors significantly enhances sensing capabilities. This enhancement originates from both aspects of improved power efficiency and target angle sensitivity, with the former also being observed in communications while the latter being unique in sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15945v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumeng Zhang, Xiaodan Shao, Hongyu Li, Bruno Clerckx, Rui Zhang</dc:creator>
    </item>
    <item>
      <title>Weak recovery, hypothesis testing, and mutual information in stochastic block models and planted factor graphs</title>
      <link>https://arxiv.org/abs/2406.15957</link>
      <description>arXiv:2406.15957v1 Announce Type: cross 
Abstract: The stochastic block model is a canonical model of communities in random graphs. It was introduced in the social sciences and statistics as a model of communities, and in theoretical computer science as an average case model for graph partitioning problems under the name of the ``planted partition model.'' Given a sparse stochastic block model, the two standard inference tasks are: (i) Weak recovery: can we estimate the communities with non trivial overlap with the true communities? (ii) Detection/Hypothesis testing: can we distinguish if the sample was drawn from the block model or from a random graph with no community structure with probability tending to $1$ as the graph size tends to infinity?
  In this work, we show that for sparse stochastic block models, the two inference tasks are equivalent except at a critical point. That is, weak recovery is information theoretically possible if and only if detection is possible. We thus find a strong connection between these two notions of inference for the model. We further prove that when detection is impossible, an explicit hypothesis test based on low degree polynomials in the adjacency matrix of the observed graph achieves the optimal statistical power. This low degree test is efficient as opposed to the likelihood ratio test, which is not known to be efficient. Moreover, we prove that the asymptotic mutual information between the observed network and the community structure exhibits a phase transition at the weak recovery threshold.
  Our results are proven in much broader settings including the hypergraph stochastic block models and general planted factor graphs. In these settings we prove that the impossibility of weak recovery implies contiguity and provide a condition which guarantees the equivalence of weak recovery and detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15957v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elchanan Mossel, Allan Sly, Youngtak Sohn</dc:creator>
    </item>
    <item>
      <title>Towards unlocking the mystery of adversarial fragility of neural networks</title>
      <link>https://arxiv.org/abs/2406.16200</link>
      <description>arXiv:2406.16200v1 Announce Type: cross 
Abstract: In this paper, we study the adversarial robustness of deep neural networks for classification tasks. We look at the smallest magnitude of possible additive perturbations that can change the output of a classification algorithm. We provide a matrix-theoretic explanation of the adversarial fragility of deep neural network for classification. In particular, our theoretical results show that neural network's adversarial robustness can degrade as the input dimension $d$ increases. Analytically we show that neural networks' adversarial robustness can be only $1/\sqrt{d}$ of the best possible adversarial robustness. Our matrix-theoretic explanation is consistent with an earlier information-theoretic feature-compression-based explanation for the adversarial fragility of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16200v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jingchao Gao, Raghu Mudumbai, Xiaodong Wu, Jirong Yi, Catherine Xu, Hui Xie, Weiyu Xu</dc:creator>
    </item>
    <item>
      <title>An Optimal Decentralized Multi-access Coded Caching System</title>
      <link>https://arxiv.org/abs/2203.16845</link>
      <description>arXiv:2203.16845v3 Announce Type: replace 
Abstract: In this paper, we consider a multi-access coded caching system with decentralized prefetching, where a server hosts $N$ files, each of size $F$ bits, and is connected to $K$ users through a shared link. There are $c$ caches distributed across the network and each of the $K$ users connects to a random set of $r\leq c$ caches. Initially, we consider the model in which each of the cache subsets is accessed by exactly a specific number of users. For this model, a novel linear delivery scheme is introduced, using which the closed-form expression for the per-user delivery rate is computed. Furthermore, using techniques from index coding, the optimality of the proposed linear delivery scheme among all linear delivery schemes is proved. The results of the decentralized shared caching and conventional decentralized caching schemes are recovered as special cases of the proposed model. The model is further generalized by allowing each cache subset to serve any number of users. This enhances the flexibility of the system, enabling it to accommodate any arbitrary number of users. A delivery scheme is proposed for the generalized model and is shown to be optimal for certain user-to-cache associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.16845v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monolina Dutta, Anoop Thomas, B. Sundar Rajan</dc:creator>
    </item>
    <item>
      <title>Capacity of Finite-State Channels with Delayed Feedback</title>
      <link>https://arxiv.org/abs/2303.18008</link>
      <description>arXiv:2303.18008v2 Announce Type: replace 
Abstract: In this paper, we investigate the capacity of finite-state channels (FSCs) in presence of delayed feedback. We show that the capacity of a FSC with delayed feedback can be computed as that of a new FSC with instantaneous feedback and an extended state. Consequently, graph-based methods to obtain computable upper and lower bounds on the delayed feedback capacity of unifilar FSCs are proposed. Based on these methods, we establish that the capacity of the trapdoor channel with delayed feedback of two time instances is given by $\log_2(3/2)$. In addition, we derive an analytical upper bound on the delayed feedback capacity of the binary symmetric channel with a no consecutive ones input constraint. This bound also serves as a novel upper bound on its non-feedback capacity, which outperforms all previously known bounds. Lastly, we demonstrate that feedback does improve the capacity of the dicode erasure channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.18008v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bashar Huleihel, Oron Sabag, Haim H. Permuter, Victoria Kostina</dc:creator>
    </item>
    <item>
      <title>Beamforming Design for IRS-and-UAV-Aided Two-Way Amplify-and-Forward Relay Networks in Maritime IoT</title>
      <link>https://arxiv.org/abs/2306.00412</link>
      <description>arXiv:2306.00412v4 Announce Type: replace 
Abstract: In this paper, an intelligent reflecting surface (IRS)-and-unmanned aerial vehicle (UAV)-assisted two-way amplify-and-forward (AF) relay network in maritime Internet of Things (IoT) is proposed, where ship1 ($\text{S}_1$) and ship2 ($\text{S}_2$) can be viewed as data collecting centers. To enhance the message exchange rate between $\text{S}_1$ and $\text{S}_2$, a problem of maximizing minimum rate is cast, where the variables, namely AF relay beamforming matrix and IRS phase shifts of two time slots, need to be optimized. To achieve a maximum rate, a low-complexity alternately iterative (AI) scheme based on zero forcing and successive convex approximation (LC-ZF-SCA) algorithm is presented. To obtain a significant rate enhancement, a high-performance AI method based on one step, semidefinite programming and penalty SCA (ONS-SDP-PSCA) is proposed. Simulation results show that by the proposed LC-ZF-SCA and ONS-SDP-PSCA methods, the rate of the IRS-and-UAV-assisted AF relay network surpass those of with random phase and only AF relay networks. Moreover, ONS-SDP-PSCA perform better than LC-ZF-SCA in aspect of rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00412v4</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuehui Wang, Feng Shu, Yuanyuan Wu, Weiping Shi, Shihao Yan, Yifan Zhao, Qiankun Cheng, Zhongwen Sun, Jiangzhou Wang</dc:creator>
    </item>
    <item>
      <title>Learning and Communications Co-Design for Remote Inference Systems: Feature Length Selection and Transmission Scheduling</title>
      <link>https://arxiv.org/abs/2308.10094</link>
      <description>arXiv:2308.10094v3 Announce Type: replace 
Abstract: In this paper, we consider a remote inference system, where a neural network is used to infer a time-varying target (e.g., robot movement), based on features (e.g., video clips) that are progressively received from a sensing node (e.g., a camera). Each feature is a temporal sequence of sensory data. The inference error is determined by (i) the timeliness and (ii) the sequence length of the feature, where we use Age of Information (AoI) as a metric for timeliness. While a longer feature can typically provide better inference performance, it often requires more channel resources for sending the feature. To minimize the time-averaged inference error, we study a learning and communication co-design problem that jointly optimizes feature length selection and transmission scheduling. When there is a single sensor-predictor pair and a single channel, we develop low-complexity optimal co-designs for both the cases of time-invariant and time-variant feature length. When there are multiple sensor-predictor pairs and multiple channels, the co-design problem becomes a restless multi-arm multi-action bandit problem that is PSPACE-hard. For this setting, we design a low-complexity algorithm to solve the problem. Trace-driven evaluations demonstrate the potential of these co-designs to reduce inference error by up to 10000 times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10094v3</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSAIT.2023.3322620</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal on Selected Areas in Information Theory, vol. 4, pp. 524-538, 2023</arxiv:journal_reference>
      <dc:creator>Md Kamran Chowdhury Shisher, Bo Ji, I-Hong Hou, Yin Sun</dc:creator>
    </item>
    <item>
      <title>Glued lattices are better quantizers than $K_{12}$</title>
      <link>https://arxiv.org/abs/2312.00481</link>
      <description>arXiv:2312.00481v3 Announce Type: replace 
Abstract: 40 years ago, Conway and Sloane proposed using the highly symmetrical Coxeter-Todd lattice $K_{12}$ for quantization, and estimated its second moment. Since then, all published lists identify $K_{12}$ as the best 12-dimensional lattice quantizer. Surprisingly, $K_{12}$ is not optimal: we construct two new 12-dimensional lattices with lower normalized second moments. The new lattices are obtained by gluing together 6-dimensional lattices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00481v3</guid>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MG</category>
      <category>math.MP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3398421</arxiv:DOI>
      <dc:creator>Erik Agrell, Daniel Pook-Kolb, Bruce Allen</dc:creator>
    </item>
    <item>
      <title>Permutation Entropy for Signal Analysis</title>
      <link>https://arxiv.org/abs/2312.00964</link>
      <description>arXiv:2312.00964v2 Announce Type: replace 
Abstract: Shannon Entropy is the preeminent tool for measuring the level of uncertainty (and conversely, information content) in a random variable. In the field of communications, entropy can be used to express the information content of given signals (represented as time series) by considering random variables which sample from specified subsequences. In this paper, we will discuss how an entropy variant, the \textit{permutation entropy} can be used to study and classify radio frequency signals in a noisy environment. The permutation entropy is the entropy of the random variable which samples occurrences of permutation patterns from time series given a fixed window length, making it a function of the distribution of permutation patterns. Since the permutation entropy is a function of the relative order of data, it is (global) amplitude agnostic and thus allows for comparison between signals at different scales. This article is intended to describe a permutation patterns approach to a data driven problem in radio frequency communications research, and includes a primer on all non-permutation pattern specific background. An empirical analysis of the methods herein on radio frequency data is included. No prior knowledge of signals analysis is assumed, and permutation pattern specific notation will be included. This article serves as a self-contained introduction to the relationship between permutation patterns, entropy, and signals analysis for studying radio frequency signals and includes results on a classification task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00964v2</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bill Kay, Audun Myers, Thad Boydston, Emily Ellwein, Cameron Mackenzie, Iliana Alvarez, Erik Lentz</dc:creator>
    </item>
    <item>
      <title>Belief Propagation Decoding of Quantum LDPC Codes with Guided Decimation</title>
      <link>https://arxiv.org/abs/2312.10950</link>
      <description>arXiv:2312.10950v2 Announce Type: replace 
Abstract: Quantum low-density parity-check (QLDPC) codes have emerged as a promising technique for quantum error correction. A variety of decoders have been proposed for QLDPC codes and many of them utilize belief propagation (BP) decoding in some fashion. However, the use of BP decoding for degenerate QLDPC codes is known to have issues with convergence. These issues are typically attributed to short cycles in the Tanner graph and code degeneracy (i.e. multiple error patterns with the same syndrome). Although various methods have been proposed to mitigate the non-convergence issue, such as BP with ordered statistics decoding (BP-OSD) and BP with stabilizer inactivation (BP-SI), achieving better performance with lower complexity remains an active area of research.
  In this work, we propose a decoder for QLDPC codes based on BP guided decimation (BPGD), which has been previously studied for constraint satisfaction and lossy compression problems. The decimation process is applicable to both binary and quaternary BP and it involves sequentially fixing the value of the most reliable qubits to encourage BP convergence. Despite its simplicity, We find that BPGD significantly reduces the BP failure rate due to non-convergence, achieving performance on par with BP with ordered statistics decoding and BP with stabilizer inactivation, without the need to solve systems of linear equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10950v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanwen Yao, Waleed Abu Laban, Christian H\"ager, Alexandre Graell i Amat, Henry D. Pfister</dc:creator>
    </item>
    <item>
      <title>Optimization and Identification of Lattice Quantizers</title>
      <link>https://arxiv.org/abs/2401.01799</link>
      <description>arXiv:2401.01799v2 Announce Type: replace 
Abstract: Lattices with minimal normalized second moments are designed using a new numerical optimization algorithm. Starting from a random lower-triangular generator matrix and applying stochastic gradient descent, all elements are updated towards the negative gradient, which makes it the most efficient algorithm proposed so far for this purpose. A graphical illustration of the theta series, called theta image, is introduced and shown to be a powerful tool for converting numerical lattice representations into their underlying exact forms. As a proof of concept, optimized lattices are designed in dimensions up to 16. In all dimensions, the algorithm converges to either the previously best known lattice or a better one. The dual of the 15-dimensional laminated lattice is conjectured to be optimal in its dimension and its exact normalized second moment is computed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01799v2</guid>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MG</category>
      <category>math.MP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Erik Agrell, Daniel Pook-Kolb, Bruce Allen</dc:creator>
    </item>
    <item>
      <title>Binary Codes for Correcting Two Edits</title>
      <link>https://arxiv.org/abs/2403.11766</link>
      <description>arXiv:2403.11766v2 Announce Type: replace 
Abstract: An edit refers to a single insertion, deletion, or substitution. This paper aims to construct binary codes that can correct two edits. To do this, a necessary and sufficient condition for a code to be two-edit correctable is provided, showing that a code is a two-edit correcting code if and only if it can correct two deletions, up to two substitutions, and one deletion and up to one substitution, separately. This criterion allows for the construction of two-edit correcting codes leveraging these three types of error correcting codes. In the field of constructing codes for correcting two deletions, we present a construction with $4\log n+O(\log\log n)$ redundant bits that can be viewed as a subcode proposed by Guruswami and H{\aa}stad, and provide an alternative proof. Moreover, our two-deletion correcting codes can also correct up to two substitutions after making a slight modification. In the field of constructing codes for correcting one deletion and up to one substitution, we present a construction with $4 \log n+O(\log\log n)$ redundant bits, which outperforms the best previously known results $6 \log n+O(1)$. Leveraging these codes, we obtain a construction of two-edit correcting codes with $6 \log n+O(\log\log n)$ redundant bits. This outperforms the best previously known result, which requires at least $8\log n$ redundant bits. Moreover, we also consider the list-decoding problem under the two-edit channel and construct a two-edit list-decodable code with a list size of two employing $4 \log n+O(\log\log n)$ redundant bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11766v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yubo Sun, Gennian Ge</dc:creator>
    </item>
    <item>
      <title>Region-Specific Coarse Quantization with Check Node Awareness in 5G-LDPC Decoding</title>
      <link>https://arxiv.org/abs/2406.14233</link>
      <description>arXiv:2406.14233v2 Announce Type: replace 
Abstract: This paper presents novel techniques for improving the error correction performance and reducing the complexity of coarsely quantized 5G-LDPC decoders. The proposed decoder design supports arbitrary message-passing schedules on a base-matrix level by modeling exchanged messages with entry-specific discrete random variables. Variable nodes (VNs) and check nodes (CNs) involve compression operations designed using the information bottleneck method to maximize preserved mutual information between code bits and quantized messages. We introduce alignment regions that assign the messages to groups with aligned reliability levels to decrease the number of individual design parameters. Group compositions with degree-specific separation of messages improve performance by up to 0.4 dB. Further, we generalize our recently proposed CN-aware quantizer design to irregular LDPC codes and layered schedules. The method optimizes the VN quantizer to maximize preserved mutual information at the output of the subsequent CN update, enhancing performance by up to 0.2 dB. A schedule optimization modifies the order of layer updates, reducing the average iteration count by up to 35 %. We integrate all new techniques in a rate-compatible decoder design by extending the alignment regions along a rate-dimension. Our complexity analysis for 2-bit decoding estimates up to 64 % higher throughput versus 4-bit decoding at similar performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14233v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Mohr, Gerhard Bauch</dc:creator>
    </item>
    <item>
      <title>Algebraic compressed sensing</title>
      <link>https://arxiv.org/abs/2108.13208</link>
      <description>arXiv:2108.13208v2 Announce Type: replace-cross 
Abstract: We introduce the broad subclass of algebraic compressed sensing problems, where structured signals are modeled either explicitly or implicitly via polynomials. This includes, for instance, low-rank matrix and tensor recovery. We employ powerful techniques from algebraic geometry to study well-posedness of sufficiently general compressed sensing problems, including existence, local recoverability, global uniqueness, and local smoothness. Our main results are summarized in thirteen questions and answers in algebraic compressed sensing. Most of our answers concerning the minimum number of required measurements for existence, recoverability, and uniqueness of algebraic compressed sensing problems are optimal and depend only on the dimension of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.13208v2</guid>
      <category>math.NA</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.AG</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.acha.2023.03.006</arxiv:DOI>
      <dc:creator>Paul Breiding, Fulvio Gesmundo, Mateusz Micha{\l}ek, Nick Vannieuwenhoven</dc:creator>
    </item>
    <item>
      <title>Reliable Simulation of Quantum Channels: the Error Exponent</title>
      <link>https://arxiv.org/abs/2112.04475</link>
      <description>arXiv:2112.04475v4 Announce Type: replace-cross 
Abstract: The Quantum Reverse Shannon Theorem has been a milestone in quantum information theory. It states that asymptotically reliable simulation of a quantum channel, assisted by unlimited shared entanglement, requires a rate of classical communication equal to the channel's entanglement-assisted classical capacity. In this paper, we study the error exponent of quantum channel simulation, which characterizes the optimal speed of exponential convergence of the performance towards the perfect, as the blocklength increases. Based on channel purified distance, we derive lower and upper bounds for the error exponent. Then we show that the two bounds coincide when the classical communication rate is below a critical value, and hence, we have determined the exact formula of the error exponent in the low-rate case. This enables us to obtain an operational interpretation to the channel's sandwiched R\'enyi information of order from 1 to 2, since our formula is expressed as a transform of this quantity. In the derivation, we have also obtained an achievability bound for quantum channel simulation in the finite-blocklength setting, which is of realistic significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.04475v4</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Li, Yongsheng Yao</dc:creator>
    </item>
    <item>
      <title>Smoothed Analysis of the Koml\'os Conjecture: Rademacher Noise</title>
      <link>https://arxiv.org/abs/2307.06285</link>
      <description>arXiv:2307.06285v5 Announce Type: replace-cross 
Abstract: The {\em discrepancy} of a matrix $M \in \mathbb{R}^{d \times n}$ is given by $\mathrm{DISC}(M) := \min_{\boldsymbol{x} \in \{-1,1\}^n} \|M\boldsymbol{x}\|_\infty$. An outstanding conjecture, attributed to Koml\'os, stipulates that $\mathrm{DISC}(M) = O(1)$, whenever $M$ is a Koml\'os matrix, that is, whenever every column of $M$ lies within the unit sphere. Our main result asserts that $\mathrm{DISC}(M + R/\sqrt{d}) = O(d^{-1/2})$ holds asymptotically almost surely, whenever $M \in \mathbb{R}^{d \times n}$ is Koml\'os, $R \in \mathbb{R}^{d \times n}$ is a Rademacher random matrix, $d = \omega(1)$, and $n = \omega(d \log d)$. The factor $d^{-1/2}$ normalising $R$ is essentially best possible and the dependency between $n$ and $d$ is asymptotically best possible. Our main source of inspiration is a result by Bansal, Jiang, Meka, Singla, and Sinha (ICALP 2022). They obtained an assertion similar to the one above in the case that the smoothing matrix is Gaussian. They asked whether their result can be attained with the optimal dependency $n = \omega(d \log d)$ in the case of Bernoulli random noise or any other types of discretely distributed noise; the latter types being more conducive for Smoothed Analysis in other discrepancy settings such as the Beck-Fiala problem. For Bernoulli noise, their method works if $n = \omega(d^2)$. In the case of Rademacher noise, we answer the question posed by Bansal, Jiang, Meka, Singla, and Sinha. Our proof builds upon their approach in a strong way and provides a discrete version of the latter. Breaking the $n = \omega(d^2)$ barrier and reaching the optimal dependency $n = \omega(d \log d)$ for Rademacher noise requires additional ideas expressed through a rather meticulous counting argument, incurred by the need to maintain a high level of precision all throughout the discretisation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06285v5</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elad Aigner-Horev, Dan Hefetz, Michael Trushkin</dc:creator>
    </item>
    <item>
      <title>Optimal Best Arm Identification with Fixed Confidence in Restless Bandits</title>
      <link>https://arxiv.org/abs/2310.13393</link>
      <description>arXiv:2310.13393v2 Announce Type: replace-cross 
Abstract: We study best arm identification in a restless multi-armed bandit setting with finitely many arms. The discrete-time data generated by each arm forms a homogeneous Markov chain taking values in a common, finite state space. The state transitions in each arm are captured by an ergodic transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs. The real-valued parameters of the arm TPMs are unknown and belong to a given space. Given a function $f$ defined on the common state space of the arms, the goal is to identify the best arm -- the arm with the largest average value of $f$ evaluated under the arm's stationary distribution -- with the fewest number of samples, subject to an upper bound on the decision's error probability (i.e., the fixed-confidence regime). A lower bound on the growth rate of the expected stopping time is established in the asymptote of a vanishing error probability. Furthermore, a policy for best arm identification is proposed, and its expected stopping time is proved to have an asymptotic growth rate that matches the lower bound. It is demonstrated that tracking the long-term behavior of a certain Markov decision process and its state-action visitation proportions are the key ingredients in analyzing the converse and achievability bounds. It is shown that under every policy, the state-action visitation proportions satisfy a specific approximate flow conservation constraint and that these proportions match the optimal proportions dictated by the lower bound under any asymptotically optimal policy. The prior studies on best arm identification in restless bandits focus on independent observations from the arms, rested Markov arms, and restless Markov arms with known arm TPMs. In contrast, this work is the first to study best arm identification in restless bandits with unknown arm TPMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13393v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. N. Karthik, Vincent Y. F. Tan, Arpan Mukherjee, Ali Tajer</dc:creator>
    </item>
    <item>
      <title>Multi-functional OFDM Signal Design for Integrated Sensing, Communications, and Power Transfer</title>
      <link>https://arxiv.org/abs/2311.00104</link>
      <description>arXiv:2311.00104v2 Announce Type: replace-cross 
Abstract: The wireless domain is witnessing a flourishing of integrated systems, e.g. (a) integrated sensing and communications, and (b) simultaneous wireless information and power transfer, due to their potential to use resources (spectrum, power) judiciously. Inspired by this trend, we investigate integrated sensing, communications and powering (ISCAP), through the design of a wideband OFDM signal to power a sensor while simultaneously performing target-sensing and communication. To characterize the ISCAP performance region, we assume symbols with non-zero mean asymmetric Gaussian distribution (i.e., the input distribution), and optimize its mean and variance at each subcarrier to maximize the harvested power, subject to constraints on the achievable rate (communications) and the average side-to-peak-lobe difference (sensing). The resulting input distribution, through simulations, achieves a larger performance region than that of (i) a symmetric complex Gaussian input distribution with identical mean and variance for the real and imaginary parts, (ii) a zero-mean symmetric complex Gaussian input distribution, and (iii) the superposed power-splitting communication and sensing signal (the coexisting solution). In particular, the optimized input distribution balances the three functions by exhibiting the following features: (a) symbols in subcarriers with strong communication channels have high variance to satisfy the rate constraint, while the other symbols are dominated by the mean, forming a relatively uniform sum of mean and variance across subcarriers for sensing; (b) with looser communication and sensing constraints, large absolute means appear on subcarriers with stronger powering channels for higher harvested power. As a final note, the results highlight the great potential of the co-designed ISCAP system for further efficiency enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00104v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumeng Zhang, Sundar Aditya, Bruno Clerckx</dc:creator>
    </item>
    <item>
      <title>Theory of Compression Channels for Postselected Quantum Metrology</title>
      <link>https://arxiv.org/abs/2311.06679</link>
      <description>arXiv:2311.06679v3 Announce Type: replace-cross 
Abstract: Postselected quantum metrological scheme is especially advantageous when the final measurements are either very noisy or expensive in practical experiments. In this work, we put forward a general theory on the compression channels in postselected quantum metrology. We define the basic notions characterizing the compression quality and illuminate the underlying structure of lossless compression channels. Previous experiments on Postselected optical phase estimation and weak-value amplification are shown to be particular cases of this general theory. Furthermore, for two categories of bipartite systems, we show that the compression loss can be made arbitrarily small even when the compression channel acts only on one subsystem. These findings can be employed to distribute quantum measurements so that the measurement noise and cost are dramatically reduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06679v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevLett.132.250802</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Lett. 132, 250802(2024)</arxiv:journal_reference>
      <dc:creator>Jing Yang</dc:creator>
    </item>
    <item>
      <title>Quantum Channel Simulation in Fidelity is no more difficult than State Splitting</title>
      <link>https://arxiv.org/abs/2403.14416</link>
      <description>arXiv:2403.14416v2 Announce Type: replace-cross 
Abstract: Characterizing the minimal communication needed for the quantum channel simulation is a fundamental task in the quantum information theory. In this paper, we show that, in fidelity, the quantum channel simulation can be directly achieved via quantum state splitting without using a technique known as the de~Finetti reduction, and thus provide a pair of tighter one-shot bounds. Using the bounds, we also recover the quantum reverse Shannon theorem in a much simpler way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14416v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael X. Cao, Rahul Jain, Marco Tomamichel</dc:creator>
    </item>
    <item>
      <title>Universal Batch Learning Under The Misspecification Setting</title>
      <link>https://arxiv.org/abs/2405.07252</link>
      <description>arXiv:2405.07252v2 Announce Type: replace-cross 
Abstract: In this paper we consider the problem of universal {\em batch} learning in a misspecification setting with log-loss. In this setting the hypothesis class is a set of models $\Theta$. However, the data is generated by an unknown distribution that may not belong to this set but comes from a larger set of models $\Phi \supset \Theta$. Given a training sample, a universal learner is requested to predict a probability distribution for the next outcome and a log-loss is incurred. The universal learner performance is measured by the regret relative to the best hypothesis matching the data, chosen from $\Theta$. Utilizing the minimax theorem and information theoretical tools, we derive the optimal universal learner, a mixture over the set of the data generating distributions, and get a closed form expression for the min-max regret. We show that this regret can be considered as a constrained version of the conditional capacity between the data and its generating distributions set. We present tight bounds for this min-max regret, implying that the complexity of the problem is dominated by the richness of the hypotheses models $\Theta$ and not by the data generating distributions set $\Phi$. We develop an extension to the Arimoto-Blahut algorithm for numerical evaluation of the regret and its capacity achieving prior distribution. We demonstrate our results for the case where the observations come from a $K$-parameters multinomial distributions while the hypothesis class $\Theta$ is only a subset of this family of distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07252v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shlomi Vituri, Meir Feder</dc:creator>
    </item>
    <item>
      <title>Subspace-Informed Matrix Completion</title>
      <link>https://arxiv.org/abs/2405.07890</link>
      <description>arXiv:2405.07890v3 Announce Type: replace-cross 
Abstract: In this work, we consider the matrix completion problem, where the objective is to reconstruct a low-rank matrix from a few observed entries. A commonly employed approach involves nuclear norm minimization. For this method to succeed, the number of observed entries needs to scale at least proportional to both the rank of the ground-truth matrix and the coherence parameter. While the only prior information is oftentimes the low-rank nature of the ground-truth matrix, in various real-world scenarios, additional knowledge about the ground-truth low-rank matrix is available. For instance, in collaborative filtering, Netflix problem, and dynamic channel estimation in wireless communications, we have partial or full knowledge about the signal subspace in advance. Specifically, we are aware of some subspaces that form multiple angles with the column and row spaces of the ground-truth matrix. Leveraging this valuable information has the potential to significantly reduce the required number of observations. To this end, we introduce a multi-weight nuclear norm optimization problem that concurrently promotes the low-rank property as well the information about the available subspaces. The proposed weights are tailored to penalize each angle corresponding to each basis of the prior subspace independently. We further propose an optimal weight selection strategy by minimizing the coherence parameter of the ground-truth matrix, which is equivalent to minimizing the required number of observations. Simulation results validate the advantages of incorporating multiple weights in the completion procedure. Specifically, our proposed multi-weight optimization problem demonstrates a substantial reduction in the required number of observations compared to the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07890v3</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamideh. Sadat Fazael Ardakani, Sajad Daei, Arash Amini, Mikael Skoglund, Gabor Fodor</dc:creator>
    </item>
    <item>
      <title>Optimal Transmitter Design and Pilot Spacing in MIMO Non-Stationary Aging Channels</title>
      <link>https://arxiv.org/abs/2405.07895</link>
      <description>arXiv:2405.07895v2 Announce Type: replace-cross 
Abstract: This work considers an uplink wireless communication system where multiple users with multiple antennas transmit data frames over dynamic channels. Previous studies have shown that multiple transmit and receive antennas can substantially enhance the sum-capacity of all users when the channel is known at the transmitter and in the case of uncorrelated transmit and receive antennas. However, spatial correlations stemming from close proximity of transmit antennas and channel variation between pilot and data time slots, known as channel aging, can substantially degrade the transmission rate if they are not properly into account. In this work, we provide an analytical framework to concurrently exploit both of these features. Specifically, we first propose a beamforming framework to capture spatial correlations. Then, based on random matrix theory tools, we introduce a deterministic expression that approximates the average sum-capacity of all users. Subsequently, we obtain the optimal values of pilot spacing and beamforming vectors upon maximizing this expression. Simulation results show the impacts of path loss, velocity of mobile users and Rician factor on the resulting sum-capacity and underscore the efficacy of our methodology compared to prior works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07895v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sajad Daei, Gabor Fodor, Mikael Skoglund</dc:creator>
    </item>
  </channel>
</rss>
