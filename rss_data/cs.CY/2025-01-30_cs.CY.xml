<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 12:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant</title>
      <link>https://arxiv.org/abs/2501.17176</link>
      <description>arXiv:2501.17176v1 Announce Type: new 
Abstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17176v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Ballestero-Rib\'o, Daniel Ortiz-Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Programming in Brazilian Higher Education and High School: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2501.17278</link>
      <description>arXiv:2501.17278v1 Announce Type: new 
Abstract: Programming, which is both economically significant and mentally stimulating, has been found to benefit the aging brain and to enhance cognitive function at various educational levels. Despite its advantages, challenges persist in standardizing and implementing programming education effectively across both the higher and secondary education levels in Brazil. To shed light on these issues, we carried out a systematic review of programming teaching methods in the Brazilian context, examining gaps, common techniques, approaches, and action opportunities in programming education. Our findings provide valuable recommendations for educational policymakers and educators to develop effective and updated national policies to teach programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17278v1</guid>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>19th Latin American Conference on Learning Technologies (LACLO 2024)</arxiv:journal_reference>
      <dc:creator>Sofia C. Latini Gon\c{c}alves, Rodrigo Moreira, Larissa F. Rodrigues Moreira, Andr\'e R. Backes, Adriana Zanella Martinhago</dc:creator>
    </item>
    <item>
      <title>International AI Safety Report</title>
      <link>https://arxiv.org/abs/2501.17805</link>
      <description>arXiv:2501.17805v1 Announce Type: new 
Abstract: The first International AI Safety Report comprehensively synthesizes the current evidence on the capabilities, risks, and safety of advanced AI systems. The report was mandated by the nations attending the AI Safety Summit in Bletchley, UK. Thirty nations, the UN, the OECD, and the EU each nominated a representative to the report's Expert Advisory Panel. A total of 100 AI experts contributed, representing diverse perspectives and disciplines. Led by the report's Chair, these independent experts collectively had full discretion over the report's content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17805v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoshua Bengio, S\"oren Mindermann, Daniel Privitera, Tamay Besiroglu, Rishi Bommasani, Stephen Casper, Yejin Choi, Philip Fox, Ben Garfinkel, Danielle Goldfarb, Hoda Heidari, Anson Ho, Sayash Kapoor, Leila Khalatbari, Shayne Longpre, Sam Manning, Vasilios Mavroudis, Mantas Mazeika, Julian Michael, Jessica Newman, Kwan Yee Ng, Chinasa T. Okolo, Deborah Raji, Girish Sastry, Elizabeth Seger, Theodora Skeadas, Tobin South, Emma Strubell, Florian Tram\`er, Lucia Velasco, Nicole Wheeler, Daron Acemoglu, Olubayo Adekanmbi, David Dalrymple, Thomas G. Dietterich, Edward W. Felten, Pascale Fung, Pierre-Olivier Gourinchas, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Andreas Krause, Susan Leavy, Percy Liang, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Alice Oh, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Sch\"olkopf, Dawn Song, Alvaro Soto, Lee Tiedrich, Ga\"el Varoquaux, Andrew Yao, Ya-Qin Zhang, Fahad Albalawi, Marwan Alserkal, Olubunmi Ajala, Guillaume Avrin, Christian Busch, Andr\'e Carlos Ponce de Leon Ferreira de Carvalho, Bronwyn Fox, Amandeep Singh Gill, Ahmet Halit Hatip, Juha Heikkil\"a, Gill Jolly, Ziv Katzir, Hiroaki Kitano, Antonio Kr\"uger, Chris Johnson, Saif M. Khan, Kyoung Mu Lee, Dominic Vincent Ligot, Oleksii Molchanovskyi, Andrea Monti, Nusu Mwamanzi, Mona Nemer, Nuria Oliver, Jos\'e Ram\'on L\'opez Portillo, Balaraman Ravindran, Raquel Pezoa Rivera, Hammam Riza, Crystal Rugege, Ciar\'an Seoighe, Jerry Sheehan, Haroon Sheikh, Denise Wong, Yi Zeng</dc:creator>
    </item>
    <item>
      <title>Dialogue Systems for Emotional Support via Value Reinforcement</title>
      <link>https://arxiv.org/abs/2501.17182</link>
      <description>arXiv:2501.17182v1 Announce Type: cross 
Abstract: Emotional support dialogue systems aim to reduce help-seekers' distress and help them overcome challenges. While human values$\unicode{x2013}$core beliefs that shape an individual's priorities$\unicode{x2013}$are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored. To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positive values in seekers. Our model learns to identify which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit. The model demonstrated superior performance in emotional support capabilities, outperforming various baselines. Notably, it more effectively explored and elicited values from seekers. Expert assessments by therapists highlighted two key strengths of our model: its ability to validate users' challenges and its effectiveness in emphasizing positive aspects of their situations$\unicode{x2013}$both crucial elements of value reinforcement. Our work validates the effectiveness of value reinforcement for emotional support systems and establishes a foundation for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17182v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juhee Kim, Chunghu Mok, Jisun Lee, Hyang Sook Kim, Yohan Jo</dc:creator>
    </item>
    <item>
      <title>"Ownership, Not Just Happy Talk": Co-Designing a Participatory Large Language Model for Journalism</title>
      <link>https://arxiv.org/abs/2501.17299</link>
      <description>arXiv:2501.17299v1 Announce Type: cross 
Abstract: Journalism has emerged as an essential domain for understanding the uses, limitations, and impacts of large language models (LLMs) in the workplace. News organizations face divergent financial incentives: LLMs already permeate newswork processes within financially constrained organizations, even as ongoing legal challenges assert that AI companies violate their copyright. At stake are key questions about what LLMs are created to do, and by whom: How might a journalist-led LLM work, and what can participatory design illuminate about the present-day challenges about adapting ``one-size-fits-all'' foundation models to a given context of use? In this paper, we undertake a co-design exploration to understand how a participatory approach to LLMs might address opportunities and challenges around AI in journalism. Our 20 interviews with reporters, data journalists, editors, labor organizers, product leads, and executives highlight macro, meso, and micro tensions that designing for this opportunity space must address. From these desiderata, we describe the result of our co-design work: organizational structures and functionality for a journalist-controlled LLM. In closing, we discuss the limitations of commercial foundation models for workplace use, and the methodological implications of applying participatory methods to LLM co-design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17299v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emily Tseng, Meg Young, Marianne Aubin Le Qu\'er\'e, Aimee Rinehart, Harini Suresh</dc:creator>
    </item>
    <item>
      <title>On the Coexistence and Ensembling of Watermarks</title>
      <link>https://arxiv.org/abs/2501.17356</link>
      <description>arXiv:2501.17356v1 Announce Type: cross 
Abstract: Watermarking, the practice of embedding imperceptible information into media such as images, videos, audio, and text, is essential for intellectual property protection, content provenance and attribution. The growing complexity of digital ecosystems necessitates watermarks for different uses to be embedded in the same media. However, to detect and decode all watermarks, they need to coexist well with one another. We perform the first study of coexistence of deep image watermarking methods and, contrary to intuition, we find that various open-source watermarks can coexist with only minor impacts on image quality and decoding robustness. The coexistence of watermarks also opens the avenue for ensembling watermarking methods. We show how ensembling can increase the overall message capacity and enable new trade-offs between capacity, accuracy, robustness and image quality, without needing to retrain the base models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17356v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aleksandar Petrov, Shruti Agarwal, Philip H. S. Torr, Adel Bibi, John Collomosse</dc:creator>
    </item>
    <item>
      <title>Are you a DePIN? A Decision Tree to Classify Decentralized Physical Infrastructure Networks</title>
      <link>https://arxiv.org/abs/2501.17416</link>
      <description>arXiv:2501.17416v1 Announce Type: cross 
Abstract: Decentralized physical infrastructure networks (DePINs) are an emerging vertical within "Web3" replacing the traditional method that physical infrastructures are constructed. Yet, the boundaries between DePIN and traditional method of building crowd-sourced infrastructures such as citizen science initiatives or other Web3 verticals are not always so clear cut. In this work, we systematically analyze the differences between DePIN and other Web2 and Web3 verticals. For this, the study proposes a novel decision tree for classifying systems as DePIN. This tree is informed by prior studies and differentiates DePIN from related concepts using criteria such as the presence of a three-sided market, token-based incentives for supply, and the requirement for physical asset placement in those systems.
  The paper demonstrates the application of the decision tree to various blockchain systems, including Helium and Bitcoin, showcasing its practical utility in differentiating DePIN systems.
  This research offers significant contributions towards establishing a more objective and systematic approach to identifying and categorizing DePIN systems. It lays the groundwork for creating a comprehensive and unbiased database of DePIN systems, which will inform future research and development within this emerging sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17416v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael S. Andrew, Mark C. Ballandies</dc:creator>
    </item>
    <item>
      <title>Throwaway Accounts and Moderation on Reddit</title>
      <link>https://arxiv.org/abs/2501.17430</link>
      <description>arXiv:2501.17430v1 Announce Type: cross 
Abstract: Social media platforms (SMPs) facilitate information sharing across varying levels of sensitivity. A crucial design decision for SMP administrators is the platform's identity policy, with some opting for real-name systems while others allow anonymous participation. Content moderation on these platforms is conducted by both humans and automated bots. This paper examines the relationship between anonymity, specifically through the use of ``throwaway'' accounts, and the extent and nature of content moderation on Reddit. Our findings indicate that content originating from anonymous throwaway accounts is more likely to violate rules on Reddit. Thus, they are more likely to be removed by moderation than standard pseudonymous accounts. However, the moderation actions applied to throwaway accounts are consistent with those applied to ordinary accounts, suggesting that the use of anonymous accounts does not necessarily necessitate increased human moderation. We conclude by discussing the implications of these findings for identity policies and content moderation strategies on SMPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17430v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Guo, Kelly Caine</dc:creator>
    </item>
    <item>
      <title>A Tale of Three Location Trackers: AirTag, SmartTag, and Tile</title>
      <link>https://arxiv.org/abs/2501.17452</link>
      <description>arXiv:2501.17452v1 Announce Type: cross 
Abstract: Bluetooth Low Energy (BLE) location trackers, or "tags", are popular consumer devices for monitoring personal items. These tags rely on their respective network of companion devices that are capable of detecting their BLE signals and relay location information back to the owner. While manufacturers claim that such crowd-sourced approach yields accurate location tracking, the tags' real-world performance characteristics remain insufficiently understood. To this end, this study presents a comprehensive analysis of three major players in the market: Apple's AirTag, Samsung's SmartTag, and Tile. Our methodology combines controlled experiments -- with a known large distribution of location-reporting devices -- as well as in-the-wild experiments -- with no control on the number and kind of reporting devices encountered, thus emulating real-life use-cases. Leveraging data collection techniques improved from prior research, we recruit 22 volunteers traveling across 29 countries, examining the tags' performance under various environments and conditions. Our findings highlight crucial updates in device behavior since previous studies, with AirTag showing marked improvements in location report frequency. Companion device density emerged as the primary determinant of tag performance, overshadowing technological differences between products. Additionally, we find that post-COVID-19 mobility trends could have contributed to enhanced performance for AirTag and SmartTag. Tile, despite its cross-platform compatibility, exhibited notably lower accuracy, particularly in Asia and Africa, due to limited global adoption. Statistical modeling of spatial errors -- measured as the distance between reported and actual tag locations -- shows log-normal distributions across all tags, highlighting the need for improved location estimation methods to reduce occasional significant inaccuracies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17452v1</guid>
      <category>cs.PF</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>HyunSeok Daniel Jang, Hazem Ibrahim, Rohail Asim, Matteo Varvello, Yasir Zaki</dc:creator>
    </item>
    <item>
      <title>CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs</title>
      <link>https://arxiv.org/abs/2501.17581</link>
      <description>arXiv:2501.17581v1 Announce Type: cross 
Abstract: Counterspeech has been popular as an effective approach to counter online hate speech, leading to increasing research interest in automated counterspeech generation using language models. However, this field lacks standardised evaluation protocols and robust automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduce CSEval, a novel dataset and framework for evaluating counterspeech quality across four dimensions: contextual-relevance, aggressiveness, argument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated COT for Counterspeech Evaluation (ACE), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show that ACE outperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant advancement in automated counterspeech evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17581v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty</dc:creator>
    </item>
    <item>
      <title>The Imitation Game According To Turing</title>
      <link>https://arxiv.org/abs/2501.17629</link>
      <description>arXiv:2501.17629v1 Announce Type: cross 
Abstract: The current cycle of hype and anxiety concerning the benefits and risks to human society of Artificial Intelligence is fuelled, not only by the increasing use of generative AI and other AI tools by the general public, but also by claims made on behalf of such technology by popularizers and scientists. In particular, recent studies have claimed that Large Language Models (LLMs) can pass the Turing Test-a goal for AI since the 1950s-and therefore can "think". Large-scale impacts on society have been predicted as a result. Upon detailed examination, however, none of these studies has faithfully applied Turing's original instructions. Consequently, we conducted a rigorous Turing Test with GPT-4-Turbo that adhered closely to Turing's instructions for a three-player imitation game. We followed established scientific standards where Turing's instructions were ambiguous or missing. For example, we performed a Computer-Imitates-Human Game (CIHG) without constraining the time duration and conducted a Man-Imitates-Woman Game (MIWG) as a benchmark. All but one participant correctly identified the LLM, showing that one of today's most advanced LLMs is unable to pass a rigorous Turing Test. We conclude that recent extravagant claims for such models are unsupported, and do not warrant either optimism or concern about the social impact of thinking machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17629v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharon Temtsin (The University of Canterbury), Diane Proudfoot (The University of Canterbury), David Kaber (Oregon State University), Christoph Bartneck (The University of Canterbury)</dc:creator>
    </item>
    <item>
      <title>An eco-driving approach for ride comfort improvement</title>
      <link>https://arxiv.org/abs/2501.17658</link>
      <description>arXiv:2501.17658v1 Announce Type: cross 
Abstract: New challenges on transport systems are emerging due to the advances that the current paradigm is experiencing. The breakthrough of the autonomous car brings concerns about ride comfort, while the pollution concerns have arisen in recent years. In the model of automated automobiles, drivers are expected to become passengers, so, they will be more prone to suffer from ride discomfort or motion sickness. Conversely, the eco-driving implications should not be set aside because of the influence of pollution on climate and people's health. For that reason, a joint assessment of the aforementioned points would have a positive impact. Thus, this work presents a self-organised map-based solution to assess ride comfort features of individuals considering their driving style from the viewpoint of eco-driving. For this purpose, a previously acquired dataset from an instrumented car was used to classify drivers regarding the causes of their lack of ride comfort and eco-friendliness. Once drivers are classified regarding their driving style, natural-language-based recommendations are proposed to increase the engagement with the system. Hence, potential improvements of up to the 57.7% for ride comfort evaluation parameters, as well as up to the 47.1% in greenhouse-gasses emissions are expected to be reached.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17658v1</guid>
      <category>cs.RO</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1049/itr2.12137</arxiv:DOI>
      <dc:creator>\'Oscar Mata-Carballeira, In\'es del Campo, Estibalitz Asua</dc:creator>
    </item>
    <item>
      <title>In-IDE Programming Courses: Learning Software Development in a Real-World Setting</title>
      <link>https://arxiv.org/abs/2501.17747</link>
      <description>arXiv:2501.17747v1 Announce Type: cross 
Abstract: While learning programming languages is crucial for software engineers, mastering the necessary tools is equally important. To facilitate this, JetBrains recently released the JetBrains Academy plugin, which customizes the IDE for learners, allowing tutors to create courses entirely within IDE.
  In this work, we provide the first exploratory study of this learning format. We carried out eight one-hour interviews with students and developers who completed at least one course using the plugin, inquiring about their experience with the format, the used IDE features, and the current shortcomings. Our results indicate that learning inside the IDE is overall welcomed by the learners, allowing them to study in a more realistic setting, using features such as debugging and code analysis, which are crucial for real software development. With the collected results and the analysis of the current drawbacks, we aim to contribute to teaching students more practical skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17747v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasiia Birillo, Ilya Vlasov, Katsiaryna Dzialets, Hieke Keuning, Timofey Bryksin</dc:creator>
    </item>
    <item>
      <title>TikTok's recommendations skewed towards Republican content during the 2024 U.S. presidential race</title>
      <link>https://arxiv.org/abs/2501.17831</link>
      <description>arXiv:2501.17831v1 Announce Type: cross 
Abstract: TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States. The platform's status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally. Despite these concerns, there is scant research investigating TikTok's recommendation algorithm for political biases. We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections. Specifically, we create hundreds of "sock puppet" TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them. Collectively, these accounts viewed ~394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content. Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received ~11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to ~7.5% more opposite-party recommendations on average. These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content. Our findings provide insights into the inner workings of TikTok's recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17831v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hazem Ibrahim, HyunSeok Daniel Jang, Nouar Aldahoul, Aaron R. Kaufman, Talal Rahwan, Yasir Zaki</dc:creator>
    </item>
    <item>
      <title>Auditing Yelp's Business Ranking and Review Recommendation Through the Lens of Fairness</title>
      <link>https://arxiv.org/abs/2308.02129</link>
      <description>arXiv:2308.02129v2 Announce Type: replace 
Abstract: Auditing is critical to ensuring the fairness and reliability of decision-making systems. However, auditing a black-box system for bias can be challenging due to the lack of transparency in the model's internal workings. In many web applications, such as Yelp, it is challenging, if not impossible, to manipulate their inputs systematically to identify bias in the output. Yelp connects users and businesses, where users identify new businesses and simultaneously express their experiences through reviews. Yelp recommendation software moderates user-provided content by categorizing it into recommended and not-recommended sections. The recommended reviews, among other attributes, are used by Yelp's ranking algorithm to rank businesses in a neighborhood. Due to Yelp's substantial popularity and its high impact on local businesses' success, understanding the bias of its algorithms is crucial.
  This data-driven study, for the first time, investigates the bias of Yelp's business ranking and review recommendation system. We examine three hypotheses to assess if Yelp's recommendation software shows bias against reviews of less established users with fewer friends and reviews and if Yelp's business ranking algorithm shows bias against restaurants located in specific neighborhoods, particularly in hotspot regions, with specific demographic compositions. Our findings show that reviews of less-established users are disproportionately categorized as not-recommended. We also find a positive association between restaurants' location in hotspot regions and their average exposure. Furthermore, we observed some cases of severe disparity bias in cities where the hotspots are in neighborhoods with less demographic diversity or higher affluence and education levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02129v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Singhal, Javier Pacheco, Seyyed Mohammad Sadegh Moosavi Khorzooghi, Tanushree Debi, Abolfazl Asudeh, Gautam Das, Shirin Nilizadeh</dc:creator>
    </item>
    <item>
      <title>Transforming Medical Regulations into Numbers: Vectorizing a Decade of Medical Device Regulatory Shifts in the USA, EU, and China</title>
      <link>https://arxiv.org/abs/2411.00567</link>
      <description>arXiv:2411.00567v2 Announce Type: replace 
Abstract: Navigating the regulatory frameworks that ensure the safety and efficacy of medical devices can be challenging, especially across different regions. These frameworks often require redundant testing, slowing down the process of getting innovations to patients. This study leverages Natural Language Processing (NLP) to analyze 664 regulations and guidelines from the USA, EU, and China over the past decade, covering over 200 million tokens. We categorize regulations into key phases, such as animal studies, clinical trials, and other testing stages, and use Bidirectional Encoder Representations from Transformers (BERT) to perform Named Entity Recognition (NER), identifying key regulatory terms and entities. By converting these texts into numerical representations and segmenting them by phase, country, and year, we compare jurisdictional requirements and assess their alignment. Additionally, we apply Latent Dirichlet Allocation (LDA) for theme analysis to observe changes in regulatory focus over time, reflecting evolving priorities and challenges. Our analysis reveals notable semantic similarities and differences between countries and phases. For instance, the closest alignment in animal study regulations is between China and the USA, with a mean cosine distance of 0.33. These findings highlight the computational potential in regulatory science, offering valuable insights for researchers, policymakers, and industry professionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00567v2</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Han, Jeroen Bergmann</dc:creator>
    </item>
    <item>
      <title>Design Patterns for the Common Good: Building Better Technologies Using the Wisdom of Virtue Ethics</title>
      <link>https://arxiv.org/abs/2501.10288</link>
      <description>arXiv:2501.10288v2 Announce Type: replace 
Abstract: Virtue ethics is a philosophical tradition that emphasizes the cultivation of virtues in achieving the common good. It has been suggested to be an effective framework for envisioning more ethical technology, yet previous work on virtue ethics and technology design has remained at theoretical recommendations. Therefore, we propose an approach for identifying user experience design patterns that embody particular virtues to more concretely articulate virtuous technology designs. As a proof of concept for our approach, we documented seven design patterns for social media that uphold the virtues of Catholic Social Teaching. We interviewed 24 technology researchers and industry practitioners to evaluate these patterns. We found that overall the patterns enact the virtues they were identified to embody; our participants valued that the patterns fostered intentional conversations and personal connections. We pave a path for technology professionals to incorporate diverse virtue traditions into the development of technologies that support human flourishing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10288v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713546</arxiv:DOI>
      <dc:creator>Louisa Conwill, Megan K. Levis, Karla Badillo-Urquiola, Walter J. Scheirer</dc:creator>
    </item>
    <item>
      <title>Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development</title>
      <link>https://arxiv.org/abs/2501.16946</link>
      <description>arXiv:2501.16946v2 Announce Type: replace 
Abstract: This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human interests that often arise from societal systems' reliance on human participation to function. Furthermore, to the extent that these systems incentivise outcomes that do not line up with human preferences, AIs may optimize for those outcomes more aggressively. These effects may be mutually reinforcing across different domains: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16946v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud</dc:creator>
    </item>
    <item>
      <title>Susceptibility to Unreliable Information Sources: Swift Adoption with Minimal Exposure</title>
      <link>https://arxiv.org/abs/2311.05724</link>
      <description>arXiv:2311.05724v2 Announce Type: replace-cross 
Abstract: Misinformation proliferation on social media platforms is a pervasive threat to the integrity of online public discourse. Genuine users, susceptible to others' influence, often unknowingly engage with, endorse, and re-share questionable pieces of information, collectively amplifying the spread of misinformation. In this study, we introduce an empirical framework to investigate users' susceptibility to influence when exposed to unreliable and reliable information sources. Leveraging two datasets on political and public health discussions on Twitter, we analyze the impact of exposure on the adoption of information sources, examining how the reliability of the source modulates this relationship. Our findings provide evidence that increased exposure augments the likelihood of adoption. Users tend to adopt low-credibility sources with fewer exposures than high-credibility sources, a trend that persists even among non-partisan users. Furthermore, the number of exposures needed for adoption varies based on the source credibility, with extreme ends of the spectrum (very high or low credibility) requiring fewer exposures for adoption. Additionally, we reveal that the adoption of information sources often mirrors users' prior exposure to sources with comparable credibility levels. Our research offers critical insights for mitigating the endorsement of misinformation by vulnerable users, offering a framework to study the dynamics of content exposure and adoption on social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05724v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3589334.3648154</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM Web Conference 2024</arxiv:journal_reference>
      <dc:creator>Jinyi Ye, Luca Luceri, Julie Jiang, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>Modeling the amplification of epidemic spread by individuals exposed to misinformation on social media</title>
      <link>https://arxiv.org/abs/2402.11351</link>
      <description>arXiv:2402.11351v4 Announce Type: replace-cross 
Abstract: Understanding how misinformation affects the spread of disease is crucial for public health, especially given recent research indicating that misinformation can increase vaccine hesitancy and discourage vaccine uptake. However, it is difficult to investigate the interaction between misinformation and epidemic outcomes due to the dearth of data-informed holistic epidemic models. Here, we employ an epidemic model that incorporates a large, mobility-informed physical contact network as well as the distribution of misinformed individuals across counties derived from social media data. The model allows us to simulate various scenarios to understand how epidemic spreading can be affected by misinformation spreading through one particular social media platform. Using this model, we compare a worst-case scenario, in which individuals become misinformed after a single exposure to low-credibility content, to a best-case scenario where the population is highly resilient to misinformation. We estimate the additional portion of the U.S. population that would become infected over the course of the COVID-19 epidemic in the worst-case scenario. This work can provide policymakers with insights about the potential harms of exposure to online vaccine misinformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11351v4</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew R. DeVerna, Francesco Pierri, Yong-Yeol Ahn, Santo Fortunato, Alessandro Flammini, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Fairness in Social Influence Maximization via Optimal Transport</title>
      <link>https://arxiv.org/abs/2406.17736</link>
      <description>arXiv:2406.17736v4 Announce Type: replace-cross 
Abstract: We study fairness in social influence maximization, whereby one seeks to select seeds that spread a given information throughout a network, ensuring balanced outreach among different communities (e.g. demographic groups). In the literature, fairness is often quantified in terms of the expected outreach within individual communities. In this paper, we demonstrate that such fairness metrics can be misleading since they overlook the stochastic nature of information diffusion processes. When information diffusion occurs in a probabilistic manner, multiple outreach scenarios can occur. As such, outcomes such as ``In 50% of the cases, no one in group 1 gets the information, while everyone in group 2 does, and in the other 50%, it is the opposite'', which always results in largely unfair outcomes, are classified as fair by a variety of fairness metrics in the literature. We tackle this problem by designing a new fairness metric, mutual fairness, that captures variability in outreach through optimal transport theory. We propose a new seed-selection algorithm that optimizes both outreach and mutual fairness, and we show its efficacy on several real datasets. We find that our algorithm increases fairness with only a minor decrease (and at times, even an increase) in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17736v4</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>math.CO</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Chowdhary, Giulia De Pasquale, Nicolas Lanzetti, Ana-Andreea Stoica, Florian Dorfler</dc:creator>
    </item>
    <item>
      <title>Collapsed Language Models Promote Fairness</title>
      <link>https://arxiv.org/abs/2410.04472</link>
      <description>arXiv:2410.04472v3 Announce Type: replace-cross 
Abstract: To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more. Despite the development, it is nontrivial to reach a principled understanding of fairness and an effective algorithm that can consistently debias language models. In this work, by rigorous evaluations of Neural Collapse -- a learning phenomenon happen in last-layer representations and classifiers in deep networks -- on fairness-related words, we find that debiased language models exhibit collapsed alignment between token representations and word embeddings. More importantly, this observation inspires us to design a principled fine-tuning method that can effectively improve fairness in a wide range of debiasing methods, while still preserving the performance of language models on standard natural language understanding tasks. We attach our code at https://github.com/Xujxyang/Fairness-NC-main.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04472v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, Yunchao Wei</dc:creator>
    </item>
  </channel>
</rss>
