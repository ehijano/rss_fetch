<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 07:19:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP Specifications</title>
      <link>https://arxiv.org/abs/2406.01768</link>
      <description>arXiv:2406.01768v1 Announce Type: new 
Abstract: Understanding telecom standards involves sorting through numerous technical documents, such as those produced by the 3rd Generation Partnership Project (3GPP), which is time-consuming and labor-intensive. While large language models (LLMs) can assist with the extensive 3GPP knowledge base, an inclusive dataset is crucial for their effective pre-training and fine-tuning. In this paper, we introduce \textit{TSpec-LLM}, an open-source comprehensive dataset covering all 3GPP documents from Release 8 to Release 19 (1999--2023). To evaluate its efficacy, we first select a representative sample of 3GPP documents, create corresponding technical questions, and assess the baseline performance of various LLMs. We then incorporate a retrieval-augmented generation (RAG) framework to enhance LLM capabilities by retrieving relevant context from the \textit{TSpec-LLM} dataset. Our evaluation shows that using a naive-RAG framework on \textit{TSpec-LLM} improves the accuracy of GPT-3.5, Gemini 1.0 Pro, and GPT-4 from 44\%, 46\%, and 51\% to 71\%, 75\%, and 72\%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01768v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rasoul Nikbakht, Mohamed Benzaghta, Giovanni Geraci</dc:creator>
    </item>
    <item>
      <title>Non-uniformity is All You Need: Efficient and Timely Encrypted Traffic Classification With ECHO</title>
      <link>https://arxiv.org/abs/2406.01852</link>
      <description>arXiv:2406.01852v1 Announce Type: new 
Abstract: With 95% of Internet traffic now encrypted, an effective approach to classifying this traffic is crucial for network security and management. This paper introduces ECHO -- a novel optimization process for ML/DL-based encrypted traffic classification. ECHO targets both classification time and memory utilization and incorporates two innovative techniques.
  The first component, HO (Hyperparameter Optimization of binnings), aims at creating efficient traffic representations. While previous research often uses representations that map packet sizes and packet arrival times to fixed-sized bins, we show that non-uniform binnings are significantly more efficient. These non-uniform binnings are derived by employing a hyperparameter optimization algorithm in the training stage. HO significantly improves accuracy given a required representation size, or, equivalently, achieves comparable accuracy using smaller representations.
  Then, we introduce EC (Early Classification of traffic), which enables faster classification using a cascade of classifiers adapted for different exit times, where classification is based on the level of confidence. EC reduces the average classification latency by up to 90\%. Remarkably, this method not only maintains classification accuracy but also, in certain cases, improves it.
  Using three publicly available datasets, we demonstrate that the combined method, Early Classification with Hyperparameter Optimization (ECHO), leads to a significant improvement in classification efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01852v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shilo Daum, Tal Shapira, David Hay, Anat Bremler-Barr</dc:creator>
    </item>
    <item>
      <title>On-Demand Routing in LEO Mega-Constellations with Dynamic Laser Inter-Satellite Links</title>
      <link>https://arxiv.org/abs/2406.01953</link>
      <description>arXiv:2406.01953v1 Announce Type: new 
Abstract: Low Earth orbit (LEO) satellite mega constellations are beginning to include laser inter-satellite links (LISLs) to extend the Internet to the most remote locations on Earth. Since the process of establishing these links incurs a setup delay on the order of seconds, a static network topology is generally established well in advance, which is then used for the routing calculations. However, this involves keeping links active even when they are not being used to forward traffic, leading to poor energy efficiency. Motivated by technological advances that are gradually decreasing the LISL setup delays, we foresee scenarios where it will be possible to compute routes and establish dynamic LISLs on demand. This will require considering setup delays as penalties that will affect the end-to-end latency. In this paper, we present a nonlinear optimization model that considers these penalties in the cost function and propose three heuristic algorithms that solve the problem in a tractable way. The algorithms establish different trade-offs in terms of performance and computational complexity. We extensively analyze metrics including average latency, route change rate, outage probability, and jitter in Starlink's Phase I version 2 constellation. The results show the benefit of adaptive routing schemes according to the link setup delay. In particular, more complex schemes can decrease the average end-to-end latency in exchange for an increase in execution time. On the other hand, depending on the maximum tolerated latency, it is possible to use less computationally complex schemes which will be more scalable for the satellite mega constellations of the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01953v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhiraj Bhattacharjee, Pablo G. Madoery, Aizaz U. Chaudhry, Halim Yanikomeroglu, Gunes Karabulut Kurt, Peng Hu, Khaled Ahmed, Stephane Martel</dc:creator>
    </item>
    <item>
      <title>Advancing Ultra-Reliable 6G: Transformer and Semantic Localization Empowered Robust Beamforming in Millimeter-Wave Communications</title>
      <link>https://arxiv.org/abs/2406.02000</link>
      <description>arXiv:2406.02000v1 Announce Type: new 
Abstract: Advancements in 6G wireless technology have elevated the importance of beamforming, especially for attaining ultra-high data rates via millimeter-wave (mmWave) frequency deployment. Although promising, mmWave bands require substantial beam training to achieve precise beamforming. While initial deep learning models that use RGB camera images demonstrated promise in reducing beam training overhead, their performance suffers due to sensitivity to lighting and environmental variations. Due to this sensitivity, Quality of Service (QoS) fluctuates, eventually affecting the stability and dependability of networks in dynamic environments. This emphasizes a critical need for more robust solutions. This paper proposes a robust beamforming technique to ensure consistent QoS under varying environmental conditions. An optimization problem has been formulated to maximize users' data rates. To solve the formulated NP-hard optimization problem, we decompose it into two subproblems: the semantic localization problem and the optimal beam selection problem. To solve the semantic localization problem, we propose a novel method that leverages the k-means clustering and YOLOv8 model. To solve the beam selection problem, we propose a novel lightweight hybrid architecture that utilizes various data sources and a weighted entropy-based mechanism to predict the optimal beams. Rapid and accurate beam predictions are needed to maintain QoS. A novel metric, Accuracy-Complexity Efficiency (ACE), has been proposed to quantify this. Six testing scenarios have been developed to evaluate the robustness of the proposed model. Finally, the simulation result demonstrates that the proposed model outperforms several state-of-the-art baselines regarding beam prediction accuracy, received power, and ACE in the developed test scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02000v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Avi Deb Raha, Kitae Kim, Apurba Adhikary, Mrityunjoy Gain, Choong Seon Hong</dc:creator>
    </item>
    <item>
      <title>WHOIS Right? An Analysis of WHOIS and RDAP Consistency</title>
      <link>https://arxiv.org/abs/2406.02046</link>
      <description>arXiv:2406.02046v1 Announce Type: new 
Abstract: Public registration information on domain names, such as the accredited registrar, the domain name expiration date, or the abusecontact is crucial for many security tasks, from automated abuse notifications to botnet or phishing detection and classification systems. Various domain registration data is usually accessible through the WHOIS or RDAP protocols-a priori they provide the same data but use distinct formats and communication protocols. While WHOIS aims to provide human-readable data, RDAP uses a machine-readable format. Therefore, deciding which protocol to use is generally considered a straightforward technical choice, depending on the use case and the required automation and security level. In this paper, we examine the core assumption that WHOIS and RDAP offer the same data and that users can query them interchangeably. By collecting, processing, and comparing 164 million WHOIS and RDAP records for a sample of 55 million domain names, we reveal that while the data obtained through WHOIS and RDAP is generally consistent, 7.6% of the observed domains still present inconsistent data on important fields like IANA ID, creation date, or nameservers. Such variances should receive careful consideration from security stakeholders reliant on the accuracy of these fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02046v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-56249-5_9</arxiv:DOI>
      <arxiv:journal_reference>International Conference on Passive and Active Network Measurement, Mar 2024, Virtual Event, United States. pp.206-231</arxiv:journal_reference>
      <dc:creator>Simon Fernandez (LIG, Grenoble INP), Olivier Hureau (LIG, Grenoble INP), Andrzej Duda (Grenoble INP, LIG), Maciej Korczynski</dc:creator>
    </item>
    <item>
      <title>Towards Railways Remote Driving: Analysis of Video Streaming Latency and Adaptive Rate Control</title>
      <link>https://arxiv.org/abs/2406.02062</link>
      <description>arXiv:2406.02062v1 Announce Type: new 
Abstract: Remote driving aims to improve transport systems by promoting efficiency, sustainability, and accessibility. In the railway sector, remote driving makes it possible to increase flexibility, as the driver no longer has to be in the cab. However, this brings several challenges, as it has to provide at least the same level of safety obtained when the driver is in the cab. To achieve it, wireless networks and video streaming technologies gain importance as they should provide real-time track visualization and obstacle detection capabilities to the remote driver. Low latency camera capture, onboard media processing devices, and streaming protocols adapted for wireless links are the necessary enablers to be developed and integrated into the railway infrastructure. This paper compares video streaming protocols such as Real-Time Streaming Protocol (RTSP) and Web Real-Time Communication (WebRTC), as they are the main alternatives based on Real-time Transport Protocol (RTP) protocol to enable low latency. As latency is the main performance metric, this paper also provides a solution to calculate the End-to-End video streaming latency analytically. Finally, the paper proposes a rate control algorithm to adapt the video stream depending on the network capacity. The objective is to keep the latency as low as possible while avoiding any visual artifacts. The proposed solutions are tested in different setups and scenarios to prove their effectiveness before the planned field testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02062v1</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Mejias, Zaloa Fernandez, Roberto Viola, Ander Aramburu, Igor Lopez, Andoni Diaz</dc:creator>
    </item>
    <item>
      <title>Achieving Stability for Aloha Networks with Multiple Receivers</title>
      <link>https://arxiv.org/abs/2406.02186</link>
      <description>arXiv:2406.02186v1 Announce Type: new 
Abstract: Slotted Aloha has been widely adopted in various communication networks. Yet if the transmission probabilities and traffic input rates of transmitters are not properly regulated, their data queues may easily become unstable. For stability analysis of Aloha networks with multiple receivers, the focus of previous studies has been placed on the maximum input rate of each transmitter, below which the network is guaranteed to be stabilized under any given topology. By assuming a fixed and identical transmission probability across the network, however, network stability is found to be unachievable when the input rate exceeds zero.
  As we will demonstrate in this paper, the key to stabilizing the network lies in proper selection of transmission probabilities according to the traffic input rates and locations of all transmitters and receivers. Specifically, for an Aloha network with multiple capture receivers, by establishing and solving the fixed-point equations of the steady-state probabilities of successful transmissions of Head-of-Line (HOL) packets, the exact service rates of all transmitters' queues are obtained, based on which the operating region of transmission probabilities for achieving stability and the stability region of input rates are further characterized. The results are illustrated in various scenarios of multi-cell and ad-hoc networks. Simulation results validate the analysis and corroborate that the network can be stabilized as long as the traffic input rates are within the stability region, and the transmission probabilities are properly adjusted according to the traffic input rates and network topology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02186v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunshan Yang, Lin Dai</dc:creator>
    </item>
    <item>
      <title>Decentralized Physical Infrastructure Network (DePIN): Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2406.02239</link>
      <description>arXiv:2406.02239v1 Announce Type: new 
Abstract: The widespread use of the Internet has posed challenges to existing centralized physical infrastructure networks. Issues such as data privacy risks, service disruptions, and substantial expansion costs have emerged. To address these challenges, an innovative network architecture called Decentralized Physical Infrastructure Network (DePIN) has emerged. DePIN leverages blockchain technology to decentralize the control and management of physical devices, addressing limitations of traditional infrastructure network. This article provides a comprehensive exploration of DePIN, presenting its five-layer architecture, key design principles. Furthermore, it presents a detailed survey of the extant applications, operating mechanisms, and provides an in-depth analysis of market data pertaining to DePIN. Finally, it discusses a wide range of the open challenges faced by DePIN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02239v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibin Lin, Taotao Wang, Long Shi, Shengli Zhang, Bin Cao</dc:creator>
    </item>
    <item>
      <title>Quantum Computing in Wireless Communications and Networking: A Tutorial-cum-Survey</title>
      <link>https://arxiv.org/abs/2406.02240</link>
      <description>arXiv:2406.02240v1 Announce Type: new 
Abstract: Owing to its outstanding parallel computing capabilities, quantum computing (QC) has been a subject of continuous attention. With the gradual maturation of QC platforms, it has increasingly played a significant role in various fields such as transportation, pharmaceuticals, and industrial manufacturing,achieving unprecedented milestones. In modern society, wireless communication stands as an indispensable infrastructure, with its essence lying in optimization. Although artificial intelligence (AI) algorithms such as Reinforcement Learning (RL) and mathematical optimization have greatly enhanced the performance of wireless communication, the rapid attainment of optimal solutions for wireless communication problems remains an unresolved challenge. QC, however, presents a new alternative. This paper aims to elucidate the fundamentals of QC and explore its applications in wireless communications and networking. First, we will provide a tutorial on QC, covering its basics, characteristics, and popular QC algorithms. Next, we will introduce the applications of QC in communication and networking, followed by its applications in miscellaneous areas such as security and privacy, localization and tracking, and video streaming. Finally,we will discuss remaining open issues before concluding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02240v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhao, Tangjie Weng, Yue Ruan, Zhi Liu, Xuangou Wu, Xiao Zheng, Nei Kato</dc:creator>
    </item>
    <item>
      <title>A deep-learning-based MAC for integrating channel access, rate adaptation and channel switch</title>
      <link>https://arxiv.org/abs/2406.02291</link>
      <description>arXiv:2406.02291v1 Announce Type: new 
Abstract: With increasing density and heterogeneity in unlicensed wireless networks, traditional MAC protocols, such as carrier-sense multiple access with collision avoidance (CSMA/CA) in Wi-Fi networks, are experiencing performance degradation. This is manifested in increased collisions and extended backoff times, leading to diminished spectrum efficiency and protocol coordination. Addressing these issues, this paper proposes a deep-learning-based MAC paradigm, dubbed DL-MAC, which leverages spectrum sensing data readily available from energy detection modules in wireless devices to achieve the MAC functionalities of channel access, rate adaptation and channel switch. First, we utilize DL-MAC to realize a joint design of channel access and rate adaptation. Subsequently, we integrate the capability of channel switch into DL-MAC, enhancing its functionality from single-channel to multi-channel operation. Specifically, the DL-MAC protocol incorporates a deep neural network (DNN) for channel selection and a recurrent neural network (RNN) for the joint design of channel access and rate adaptation. We conducted real-world data collection within the 2.4 GHz frequency band to validate the effectiveness of DL-MAC, and our experiments reveal that DL-MAC exhibits superior performance over traditional algorithms in both single and multi-channel environments and also outperforms single-function approaches in terms of overall performance. Additionally, the performance of DL-MAC remains robust, unaffected by channel switch overhead within the evaluated range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02291v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiantao Xin, Wei Xu, Bin Cao, Taotao Wang, Shengli Zhang</dc:creator>
    </item>
    <item>
      <title>Development and Validation of a Proximity-based Wearable Computing Testbed for Community-oriented Wearable Systems</title>
      <link>https://arxiv.org/abs/2406.02311</link>
      <description>arXiv:2406.02311v1 Announce Type: new 
Abstract: In the rapidly evolving digital technology landscape, community-oriented wearable computing systems are emerging as a key tool for enhancing connectivity and interaction within communal spaces. This paper contributes to this burgeoning field by presenting the development and implementation of a proximity-based wearable computing testbed designed to forge stronger links within communities. The testbed exploits Ultra-Wideband (UWB) position sensors, 9-axis motion sensors, edge nodes, and a centralized server, forming a cohesive network that actively facilitates community interactions and engagements. By employing anchors and targets within the UWB sensors, the system achieves high precision in location and distance measurements, laying the groundwork for various proximity-based applications. Integrating 9-axis motion sensors and advanced edge nodes further underscores the system's versatility and robustness in wearable and edge computing. This paper delves into an in-depth exploration and evaluation of the proposed system's architecture, design, and implementation processes. It provides a comprehensive analysis of experimental results and discusses the system's potential impact on enhancing community networks, along with the future directions this technology could take.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02311v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/PerComWorkshops59983.2024.10502607</arxiv:DOI>
      <dc:creator>Qimeng Li, Fabrizio Mangione, Francesco Porreca, Raffaele Gravina, Giancarlo Fortino</dc:creator>
    </item>
    <item>
      <title>Towards Neural Architecture Search for Transfer Learning in 6G Networks</title>
      <link>https://arxiv.org/abs/2406.02333</link>
      <description>arXiv:2406.02333v1 Announce Type: new 
Abstract: The future 6G network is envisioned to be AI-native, and as such, ML models will be pervasive in support of optimizing performance, reducing energy consumption, and in coping with increasing complexity and heterogeneity. A key challenge is automating the process of finding optimal model architectures satisfying stringent requirements stemming from varying tasks, dynamicity and available resources in the infrastructure and deployment positions. In this paper, we describe and review the state-of-the-art in Neural Architecture Search and Transfer Learning and their applicability in networking. Further, we identify open research challenges and set directions with a specific focus on three main requirements with elements unique to the future network, namely combining NAS and TL, multi-objective search, and tabular data. Finally, we outline and discuss both near-term and long-term work ahead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02333v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Orucu, Farnaz Moradi, Masoumeh Ebrahimi, Andreas Johnsson</dc:creator>
    </item>
    <item>
      <title>A Novel Paradigm Shift for Next-Generation: Symbiotic Backscatter Rate-Splitting Multiple Access Systems</title>
      <link>https://arxiv.org/abs/2406.01921</link>
      <description>arXiv:2406.01921v1 Announce Type: cross 
Abstract: Next-generation wireless networks are projected to empower a broad range of Internet-of-things (IoT) applications and services with extreme data rates, posing new challenges in delivering large-scale connectivity at a low cost to current communication paradigms. Rate-splitting multiple access (RSMA) is one of the most spotlight nominees, conceived to address spectrum scarcity while reaching massive connectivity. Meanwhile, symbiotic communication is said to be an inexpensive way to realize future IoT on a large scale. To reach the goal of spectrum efficiency improvement and low energy consumption, we merge these advances by means of introducing a novel paradigm shift, called symbiotic backscatter RSMA, for the next generation. Specifically, we first establish the way to operate the symbiotic system to assist the readers in apprehending the proposed paradigm, then guide detailed design in beamforming weights with four potential gain-control (GC) strategies for enhancing symbiotic communication, and finally provide an information-theoretic framework using a new metric, called symbiotic outage probability (SOP) to characterize the proposed system performance. Through numerical result experiments, we show that the developed framework can accurately predict the actual SOP and the efficacy of the proposed GC strategies in improving the SOP performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01921v1</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.NA</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thai-Hoc Vu, Daniel Benevides da Costa, Bao Vo Nguyen Quoc, Sunghwan Kim</dc:creator>
    </item>
    <item>
      <title>ONCache: A Cache-Based Low-Overhead Container Overlay Network</title>
      <link>https://arxiv.org/abs/2305.05455</link>
      <description>arXiv:2305.05455v3 Announce Type: replace 
Abstract: Recent years have witnessed a widespread adoption of containers. While containers simplify and accelerate application development, existing container network technologies either incur significant overhead, which hurts performance for distributed applications, or lose flexibility or compatibility, which hinders the widespread deployment in production.
  We carefully analyze the kernel data path of an overlay network, quantifying the time consumed by each segment of the data path and identifying the \emph{extra overhead} in an overlay network compared to bare metal. We observe that this extra overhead generates repetitive results among packets, which inspires us to introduce caches within an overlay network.
  We design and implement ONCache (\textbf{O}verlay \textbf{N}etwork \textbf{Cache}), a cache-based container overlay network, to eliminate the extra overhead while maintaining flexibility and compatibility. We implement ONCache using the extended Berkeley Packet Filter (eBPF) with only 524 lines of code, and integrate it as a plugin of Antrea. With ONCache, containers attain networking performance akin to that of bare metal. Compared to the standard overlay networks, ONCache improves throughput and request-response transaction rate by 12\% and 36\% for TCP (20\% and 34\% for UDP), respectively, while significantly reducing per-packet CPU overhead. Popular distributed applications also benefit from ONCache.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05455v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengkai Lin, Shizhen Zhao, Peirui Cao, Xinchi Han, Quan Tian, Wenfeng Liu, Qi Wu, Donghai Han, Xinbing Wang</dc:creator>
    </item>
    <item>
      <title>Blockchains for Internet of Things: Fundamentals, Applications, and Challenges</title>
      <link>https://arxiv.org/abs/2405.04803</link>
      <description>arXiv:2405.04803v4 Announce Type: replace-cross 
Abstract: Internet of Things (IoT) services necessitate the storage, transmission, and analysis of diverse data for inference, autonomy, and control. Blockchains, with their inherent properties of decentralization and security, offer efficient database solutions for these devices through consensus-based data sharing. However, it's essential to recognize that not every blockchain system is suitable for specific IoT applications, and some might be more beneficial when excluded with privacy concerns. For example, public blockchains are not suitable for storing sensitive data. This paper presents a detailed review of three distinct blockchains tailored for enhancing IoT applications. We initially delve into the foundational aspects of three blockchain systems, highlighting their strengths, limitations, and implementation needs. Additionally, we discuss the security issues in different blockchains. Subsequently, we explore the blockchain's application in three pivotal IoT areas: edge AI, communications, and healthcare. We underscore potential challenges and the future directions for integrating different blockchains in IoT. Ultimately, this paper aims to offer a comprehensive perspective on the synergies between blockchains and the IoT ecosystem, highlighting the opportunities and complexities involved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04803v4</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusen Wu, Ye Hu, Mingzhe Chen, Yelena Yesha, M\'erouane Debbah</dc:creator>
    </item>
  </channel>
</rss>
