<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Feb 2025 05:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Soccer Goalkeeper Performance Evaluation: Clustering Approach</title>
      <link>https://arxiv.org/abs/2502.05548</link>
      <description>arXiv:2502.05548v1 Announce Type: new 
Abstract: The objective of this work was to help the soccer field managers to evaluate the performance of a goalkeeper in saving the penalty kicks. To this end, based on the concept of clustering, four measures were proposed for evaluating the goalkeeper's performance in terms of both the saved kicks and detecting the direction of kicked ball. The well-known measures ignore the goalkeeper's ability in detecting the directional jump, while the forth proposed measure in this work was regarded this fact. The effectiveness of the proposed measures were analyzed and demonstrated by evaluating the performance of goalkeepers participated in four important soccer matches. The results were consistent with the proposed measures. In summary, as well as the known point statistics for evaluating the performance of goalkeeper, the measures were proposed in this work that regard as well the ability of goalkeeper in detecting the direction of kicks are also suggested.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05548v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Teimouri</dc:creator>
    </item>
    <item>
      <title>Predicting Energy Demand with Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2502.06213</link>
      <description>arXiv:2502.06213v1 Announce Type: new 
Abstract: Hourly consumption from multiple providers displays pronounced intra-day, intra-week, and annual seasonalities, as well as strong cross-sectional correlations. We introduce a novel approach for forecasting high-dimensional U.S. electricity demand data by accounting for multiple seasonal patterns via tensor factor models. To this end, we restructure the hourly electricity demand data into a sequence of weekly tensors. Each weekly tensor is a three-mode array whose dimensions correspond to the hours of the day, the days of the week, and the number of providers. This multi-dimensional representation enables a factor decomposition that distinguishes among the various seasonal patterns along each mode: factor loadings over the hour dimension highlight intra-day cycles, factor loadings over the day dimension capture differences across weekdays and weekends, and factor loadings over the provider dimension reveal commonalities and shared dynamics among the different entities. We rigorously compare the predictive performance of our tensor factor model against several benchmarks, including traditional vector factor models and cutting-edge functional time series methods. The results consistently demonstrate that the tensor-based approach delivers superior forecasting accuracy at different horizons and provides interpretable factors that align with domain knowledge. Beyond its empirical advantages, our framework offers a systematic way to gain insight into the underlying processes that shape electricity demand patterns. In doing so, it paves the way for more nuanced, data-driven decision-making and can be adapted to address similar challenges in other high-dimensional time series applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06213v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattia Banin, Matteo Barigozzi, Luca Trapin</dc:creator>
    </item>
    <item>
      <title>Gaussian Process-driven Hidden Markov Models for Early Diagnosis of Infant Gait Anomalies</title>
      <link>https://arxiv.org/abs/2502.06334</link>
      <description>arXiv:2502.06334v1 Announce Type: new 
Abstract: Gait analysis is critical in the early detection and intervention of motor neurological disorders in infants. Despite its importance, traditional methods often struggle to model the high variability and rapid developmental changes inherent to infant gait. To address these challenges, we propose a probabilistic Gaussian Process (GP)-driven Hidden Markov Model (HMM) to capture the complex temporal dynamics of infant gait cycles and enable automatic recognition of gait anomalies. We use a Multi-Output GP (MoGP) framework to model interdependencies between multiple gait signals, with a composite kernel designed to account for smooth, non-smooth, and periodic behaviors exhibited in gait cycles. The HMM segments gait phases into normal and abnormal states, facilitating the precise identification of pathological movement patterns in stance and swing phases. The proposed model is trained and assessed using a dataset of infants with and without motor neurological disorders via leave-one-subject-out cross-validation. Results demonstrate that the MoGP outperforms Long Short-Term Memory (LSTM) based neural networks in modeling gait dynamics, offering improved accuracy, variance explanation, and temporal alignment. Further, the predictive performance of MoGP provides a principled framework for uncertainty quantification, allowing confidence estimation in gait trajectory predictions. Additionally, the HMM enhances interpretability by explicitly modeling gait phase transitions, improving the detection of subtle anomalies across multiple gait cycles. These findings highlight the MoGP-HMM framework as a robust automatic gait analysis tool, allowing early diagnosis and intervention strategies for infants with neurological motor disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06334v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Torres-Torres F. (UTP), Jonatan Arias-Garc\'ia (UTP), Hern\'an F. Garc\'ia (UPHF, CERAMATHS), Andr\'es F. L\'opez-Lopera (UPHF, CERAMATHS), Jes\'us F. Vargas-Bonilla</dc:creator>
    </item>
    <item>
      <title>Pulling back the curtain: the road from statistical estimand to machine-learning based estimator for epidemiologists (no wizard required)</title>
      <link>https://arxiv.org/abs/2502.05363</link>
      <description>arXiv:2502.05363v1 Announce Type: cross 
Abstract: Epidemiologists increasingly use causal inference methods that rely on machine learning, as these approaches can relax unnecessary model specification assumptions. While deriving and studying asymptotic properties of such estimators is a task usually associated with statisticians, it is useful for epidemiologists to understand the steps involved, as epidemiologists are often at the forefront of defining important new research questions and translating them into new parameters to be estimated. In this paper, our goal was to provide a relatively accessible guide through the process of (i) deriving an estimator based on the so-called efficient influence function (which we define and explain), and (ii) showing such an estimator's ability to validly incorporate machine learning, by demonstrating the so-called rate double robustness property. The derivations in this paper rely mainly on algebra and some foundational results from statistical inference, which are explained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05363v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Audrey Renson, Lina Montoya, Dana E. Goin, Iv\'an D\'iaz, Rachael K. Ross</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v1 Announce Type: cross 
Abstract: The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Mike Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>A state-space framework for causal detection of hippocampal ripple-replay events</title>
      <link>https://arxiv.org/abs/2502.05394</link>
      <description>arXiv:2502.05394v1 Announce Type: cross 
Abstract: Hippocampal ripple-replay events are typically identified using a two-step process that at each time point uses past and future data to determine whether an event is occurring. This prevents researchers from identifying these events in real time for closed-loop experiments. It also prevents the identification of periods of nonlocal representation that are not accompanied by large changes in the spectral content of the local field potentials (LFPs). In this work, we present a new state-space model framework that is able to detect concurrent changes in the rhythmic structure of LFPs with nonlocal activity in place cells to identify ripple-replay events in a causal manner. The model combines latent factors related to neural oscillations, represented space, and switches between coding properties to explain simultaneously the spiking activity from multiple units and the rhythmic content of LFPs recorded from multiple sources. The model is temporally causal, meaning that estimates of the switching state can be made at each instant using only past information from the spike and LFP signals, or can be combined with future data to refine those estimates. We applied this model framework to simulated and real hippocampal data to demonstrate its performance in identifying ripple-replay events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05394v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sirui Zeng, Uri T. Eden</dc:creator>
    </item>
    <item>
      <title>Multi-scale Masked Autoencoder for Electrocardiogram Anomaly Detection</title>
      <link>https://arxiv.org/abs/2502.05494</link>
      <description>arXiv:2502.05494v1 Announce Type: cross 
Abstract: Electrocardiogram (ECG) analysis is a fundamental tool for diagnosing cardiovascular conditions, yet anomaly detection in ECG signals remains challenging due to their inherent complexity and variability. We propose Multi-scale Masked Autoencoder for ECG anomaly detection (MMAE-ECG), a novel end-to-end framework that effectively captures both global and local dependencies in ECG data. Unlike state-of-the-art methods that rely on heartbeat segmentation or R-peak detection, MMAE-ECG eliminates the need for such pre-processing steps, enhancing its suitability for clinical deployment. MMAE-ECG partitions ECG signals into non-overlapping segments, with each segment assigned learnable positional embeddings. A novel multi-scale masking strategy and multi-scale attention mechanism, along with distinct positional embeddings, enable a lightweight Transformer encoder to effectively capture both local and global dependencies. The masked segments are then reconstructed using a single-layer Transformer block, with an aggregation strategy employed during inference to refine the outputs. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art approaches while significantly reducing computational complexity-approximately 1/78 of the floating-point operations (FLOPs) required for inference. Ablation studies further validate the effectiveness of each component, highlighting the potential of multi-scale masked autoencoders for anomaly detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05494v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya Zhou, Yujie Yang, Jianhuang Gan, Xiangjie Li, Jing Yuan, Wei Zhao</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis</title>
      <link>https://arxiv.org/abs/2502.06173</link>
      <description>arXiv:2502.06173v1 Announce Type: cross 
Abstract: Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06173v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanket Jantre, Tianle Wang, Gilchan Park, Kriti Chopra, Nicholas Jeon, Xiaoning Qian, Nathan M. Urban, Byung-Jun Yoon</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Proportions for Binary Responses: Insights from Incorporating the Absent Perspective of Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v1 Announce Type: cross 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trail while maintaining a predefined power level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Random Forest for Dynamic Risk Prediction or Recurrent Events: A Pseudo-Observation Approach</title>
      <link>https://arxiv.org/abs/2312.00770</link>
      <description>arXiv:2312.00770v3 Announce Type: replace 
Abstract: Recurrent events are common in clinical, healthcare, social and behavioral studies. A recent analysis framework for potentially censored recurrent event data is to construct a censored longitudinal data set consisting of times to the first recurrent event in multiple prespecified follow-up windows of length $\tau$. With the staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest are growing in popularity, as they can incorporate information from highly correlated predictors with non-standard relationships. In this paper, we bridge this gap by developing a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent $\tau$-duration follow-up period from a reconstructed censored longitudinal data set. We demonstrate the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a $\tau$-duration follow-up period when compared to the recurrent event modeling framework of Xia et al. (2020) in settings where association between predictors and recurrent event outcomes is complex in nature. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease (Albert et al., 2011).</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00770v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abigail Loe, Susan Murray, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>Optimal starting point for time series forecasting</title>
      <link>https://arxiv.org/abs/2409.16843</link>
      <description>arXiv:2409.16843v2 Announce Type: replace 
Abstract: Recent advances on time series forecasting mainly focus on improving the forecasting models themselves. However, when the time series data suffer from potential structural breaks or concept drifts, the forecasting performance might be significantly reduced. In this paper, we introduce a novel approach called Optimal Starting Point Time Series Forecast (OSP-TSP) for optimal forecasting, which can be combined with existing time series forecasting models. By adjusting the sequence length via leveraging the XGBoost and LightGBM models, the proposed approach can determine the optimal starting point (OSP) of the time series and then enhance the prediction performances of the base forecasting models. To illustrate the effectiveness of the proposed approach, comprehensive empirical analysis have been conducted on the M4 dataset and other real world datasets. Empirical results indicate that predictions based on the OSP-TSP approach consistently outperform those using the complete time series dataset. Moreover, comparison results reveals that combining our approach with existing forecasting models can achieve better prediction accuracy, which also reflect the advantages of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16843v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Zhong, Yinuo Ren, Guangyao Cao, Feng Li, Haobo Qi</dc:creator>
    </item>
    <item>
      <title>Development of COVID-19 Booster Vaccine Policy by Microsimulation and Q-learning</title>
      <link>https://arxiv.org/abs/2410.12936</link>
      <description>arXiv:2410.12936v3 Announce Type: replace 
Abstract: The COVID-19 pandemic highlighted the urgent need for effective vaccine policies, but traditional clinical trials often lack sufficient data to capture the diverse population characteristics necessary for comprehensive public health strategies. Ethical concerns around randomized trials during a pandemic further complicate policy development for public health. Reinforcement Learning (RL) offers a promising alternative for vaccine policy development. However, direct online RL exploration in real-world scenarios can result in suboptimal and potentially harmful decisions. This study proposes a novel framework combining tabular Q-learning with microsimulation (i.e., a Recurrent Neural Network (RNN) environment simulator) to address these challenges in public health vaccine policymaking, which enables effective vaccine policy learning without real-world interaction, addressing both ethical and exploration challenges. The RNN environment simulator captures temporal associations between infection and patient characteristics, generating realistic simulation data. Our tabular Q-learning model produces an interpretable policy table that balances the risks of severe infection against vaccination side effects. Applied to COVID-19 booster policies, the learned Q-learning-based policy outperforms current practices, offering a path toward more effective vaccination strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12936v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guoxuan Ma, Lili Zhao, Jian Kang</dc:creator>
    </item>
    <item>
      <title>The Loser's Curse Revisited: A Case Study of the Critical Role of Specifying a Utility Function</title>
      <link>https://arxiv.org/abs/2411.10400</link>
      <description>arXiv:2411.10400v2 Announce Type: replace 
Abstract: A longstanding question in the judgment and decision making literature is whether experts, even in high-stakes environments, exhibit the same cognitive biases observed in controlled experiments with inexperienced participants. In their seminal work, Massey and Thaler (2013) provide a notable example of bias and irrationality in expert decision making: general managers' behavior in the National Football League draft pick trade market. They argue that general managers systematically overvalue top draft picks, which generate less surplus value on average than later first-round picks, a phenomenon known as the loser's curse. Their conclusion hinges on the assumption that general managers should use expected surplus value as their utility function for evaluating draft picks. This assumption, however, is neither explicitly justified nor necessarily aligned with the strategic complexities of constructing a National Football League roster. In this paper, we challenge their framework by considering alternative utility functions, particularly those that emphasize the acquisition of transformational players--those capable of dramatically increasing a team's chances of winning the Super Bowl. Under a decision rule that prioritizes the probability of acquiring elite players, which we construct from a novel Bayesian hierarchical Beta regression model, general managers' draft trade behavior appears rational rather than systematically flawed. More broadly, our findings highlight the critical role of carefully specifying a utility function when evaluating the quality of decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10400v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Posterior linearisation smoothing with robust iterations</title>
      <link>https://arxiv.org/abs/2112.03969</link>
      <description>arXiv:2112.03969v3 Announce Type: replace-cross 
Abstract: This paper considers the problem of iterative Bayesian smoothing in nonlinear state-space models with additive noise using Gaussian approximations. Iterative methods are known to improve smoothed estimates but are not guaranteed to converge, motivating the development of methods with better convergence properties. The aim of this article is to extend Levenberg-Marquardt (LM) and line-search versions of the classical iterated extended Kalman smoother (IEKS) to the iterated posterior linearisation smoother (IPLS). The IEKS has previously been shown to be equivalent to the Gauss-Newton (GN) method. We derive a similar GN interpretation for the IPLS and use this to develop extensions to the IPLS, with improved convergence properties. We show that an LM extension for the IPLS can be achieved with a simple modification of the smoothing iterations, enabling algorithms with efficient implementations. We also derive the Armijo--Wolfe step length conditions for the IPLS enabling an efficient inexact line-search method. Our numerical experiments show the benefits of these extensions in highly nonlinear scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03969v3</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Lindqvist, Simo S\"arkk\"a, \'Angel F. Garc\'ia-Fern\'andez, Matti Raitoharju, Lennart Svensson</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v2 Announce Type: replace-cross 
Abstract: Treatment effect heterogeneity is of major interest in economics, but its assessment is often hindered by the fundamental lack of identification of the individual treatment effects. For example, we may want to assess the effect of a poverty reduction measure at different levels of poverty, but the causal effects on wealth at different wealth levels are not identified. Or, we may be interested in the proportion of workers who benefit from the minimum wage increase, but the proportion is not identified in the absence of counterfactuals. This paper derives bounds useful in such situations, which only depend on the marginal distributions of the outcomes. The bounds are nonparametrically sharp, making clear the maximum extent to which the data can speak about the heterogeneity of the treatment effects. An application to microfinance shows that the bounds can be informative even when the average treatment effects are not significant. Another application to the welfare reform identifies a nonnegligible portion of workers who increased and decreased working hours due to the reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Self-Regulating Random Walks for Resilient Decentralized Learning on Graphs</title>
      <link>https://arxiv.org/abs/2407.11762</link>
      <description>arXiv:2407.11762v2 Announce Type: replace-cross 
Abstract: Consider the setting of multiple random walks (RWs) on a graph executing a certain computational task. For instance, in decentralized learning via RWs, a model is updated at each iteration based on the local data of the visited node and then passed to a randomly chosen neighbor. RWs can fail due to node or link failures. The goal is to maintain a desired number of RWs to ensure failure resilience. Achieving this is challenging due to the lack of a central entity to track which RWs have failed to replace them with new ones by forking (duplicating) surviving ones. Without duplications, the number of RWs will eventually go to zero, causing a catastrophic failure of the system. We propose two decentralized algorithms called DecAFork and DecAFork+ that can maintain the number of RWs in the graph around a desired value even in the presence of arbitrary RW failures. Nodes continuously estimate the number of surviving RWs by estimating their return time distribution and fork the RWs when failures are likely to happen. DecAFork+ additionally allows terminations to avoid overloading the network by forking too many RWs. We present extensive numerical simulations that show the performance of DecAFork and DecAFork+ regarding fast detection and reaction to failures compared to a baseline, and establish theoretical guarantees on the performance of both algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11762v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Egger, Rawad Bitar, Ghadir Ayache, Antonia Wachter-Zeh, Salim El Rouayheb</dc:creator>
    </item>
    <item>
      <title>Optimal Visual Search with Highly Heuristic Decision Rules</title>
      <link>https://arxiv.org/abs/2409.12124</link>
      <description>arXiv:2409.12124v3 Announce Type: replace-cross 
Abstract: Visual search is a fundamental natural task for humans and other animals. We investigated the decision processes humans use in covert (single-fixation) search with briefly presented displays having well-separated potential target locations. Performance was compared with the Bayesian-optimal decision process under the assumption that the information from the different potential target locations is statistically independent. Surprisingly, humans performed slightly better than optimal, despite humans' substantial loss of sensitivity in the fovea (foveal neglect), and the implausibility of the human brain replicating the optimal computations. We show that three factors can quantitatively explain these seemingly paradoxical results. Most importantly, simple and fixed heuristic decision rules reach near optimal search performance. Secondly, foveal neglect primarily affects only the central potential target location. Finally, spatially correlated neural noise can cause search performance to exceed that predicted for independent noise. These findings have broad implications for understanding visual search tasks and other identification tasks in humans and other animals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12124v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anqi Zhang, Wilson S. Geisler</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Test of Bias Acceptance: Examining in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2411.18481</link>
      <description>arXiv:2411.18481v2 Announce Type: replace-cross 
Abstract: This study introduces Bhirkuti's Test of Bias Acceptance, a systematic graphical framework for evaluating bias and determining its acceptability under varying experimental conditions. Absolute Relative Bias (ARB), while useful for understanding bias, is sensitive to outliers and population parameter magnitudes, often overstating bias for small values and understating it for larger ones. Similarly, Relative Efficiency (RE) can be influenced by variance differences and outliers, occasionally producing counterintuitive values exceeding 100%, which complicates interpretation. By addressing the limitations of traditional metrics such as Absolute Relative Bias (ARB) and Relative Efficiency (RE), the proposed graphical methodology framework leverages ridgeline plots and standardized estimate to provide a comprehensive visualization of parameter estimate distributions. Ridgeline plots done this way offer a robust alternative by visualizing full distributions, highlighting variability, trends, outliers, descriptives and facilitating more informed decision-making. This study employs multivariate Latent Growth Models (LGM) and Monte Carlo simulations to examine the performance of growth curve modeling under planned missing data designs, focusing on parameter estimate recovery and efficiency. By combining innovative visualization techniques with rigorous simulation methods, Bhirkuti's Test of Bias Acceptance provides two methods of versatile and interpretable toolset for advancing quantitative research in bias evaluation and efficiency assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18481v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>A Kernel Score Perspective on Forecast Disagreement and the Linear Pool</title>
      <link>https://arxiv.org/abs/2412.09430</link>
      <description>arXiv:2412.09430v2 Announce Type: replace-cross 
Abstract: The variance of a linearly combined forecast distribution (or linear pool) consists of two components: The average variance of the component distributions (`average uncertainty'), and the average squared difference between the components' means and the pool's mean (`disagreement'). This paper shows that similar decompositions hold for a class of uncertainty measures that can be constructed as entropy functions of kernel scores. The latter are a rich family of scoring rules that covers point and distribution forecasts for univariate and multivariate, discrete and continuous settings. We further show that the disagreement term is useful for understanding the ex-post performance of the linear pool (as compared to the component distributions), and motivates using the linear pool instead of other forecast combination techniques. From a practical perspective, the results in this paper suggest principled measures of forecast disagreement in a wide range of applied settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09430v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Kr\"uger</dc:creator>
    </item>
  </channel>
</rss>
