<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Dec 2024 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Parallel Greedy Best-First Search with a Bound on the Number of Expansions Relative to Sequential Search</title>
      <link>https://arxiv.org/abs/2412.12221</link>
      <description>arXiv:2412.12221v1 Announce Type: new 
Abstract: Parallelization of non-admissible search algorithms such as GBFS poses a challenge because straightforward parallelization can result in search behavior which significantly deviates from sequential search. Previous work proposed PUHF, a parallel search algorithm which is constrained to only expand states that can be expanded by some tie-breaking strategy for GBFS. We show that despite this constraint, the number of states expanded by PUHF is not bounded by a constant multiple of the number of states expanded by sequential GBFS with the worst-case tie-breaking strategy. We propose and experimentally evaluate One Bench At a Time (OBAT), a parallel greedy search which guarantees that the number of states expanded is within a constant factor of the number of states expanded by sequential GBFS with some tie-breaking policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12221v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takumi Shimoda, Alex Fukunaga</dc:creator>
    </item>
    <item>
      <title>Cluster Editing on Cographs and Related Classes</title>
      <link>https://arxiv.org/abs/2412.12454</link>
      <description>arXiv:2412.12454v1 Announce Type: new 
Abstract: In the Cluster Editing problem, sometimes known as (unweighted) Correlation Clustering, we must insert and delete a minimum number of edges to achieve a graph in which every connected component is a clique. Owing to its applications in computational biology, social network analysis, machine learning, and others, this problem has been widely studied for decades and is still undergoing active research. There exist several parameterized algorithms for general graphs, but little is known about the complexity of the problem on specific classes of graphs.
  Among the few important results in this direction, if only deletions are allowed, the problem can be solved in polynomial time on cographs, which are the $P_4$-free graphs. However, the complexity of the broader editing problem on cographs is still open. We show that even on a very restricted subclass of cographs, the problem is NP-hard, W[1]-hard when parameterized by the number $p$ of desired clusters, and that time $n^{o(p/\log p)}$ is forbidden under the ETH. This shows that the editing variant is substantially harder than the deletion-only case, and that hardness holds for the many superclasses of cographs (including graphs of clique-width at most $2$, perfect graphs, circle graphs, permutation graphs). On the other hand, we provide an almost tight upper bound of time $n^{O(p)}$, which is a consequence of a more general $n^{O(cw \cdot p)}$ time algorithm, where $cw$ is the clique-width. Given that forbidding $P_4$s maintains NP-hardness, we look at $\{P_4, C_4\}$-free graphs, also known as trivially perfect graphs, and provide a cubic-time algorithm for this class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12454v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Lafond, Alitzel L\'opez S\'anchez, Weidong Luo</dc:creator>
    </item>
    <item>
      <title>Round and Communication Efficient Graph Coloring</title>
      <link>https://arxiv.org/abs/2412.12589</link>
      <description>arXiv:2412.12589v1 Announce Type: new 
Abstract: In the context of communication complexity, we explore randomized protocols for graph coloring, focusing specifically on the vertex and edge coloring problems in $n$-vertex graphs $G$ with a maximum degree $\Delta$. We consider a scenario where the edges of $G$ are partitioned between two players. Our first contribution is a randomized protocol that efficiently finds a $(\Delta + 1)$-vertex coloring of $G$, utilizing $O(n)$ bits of communication in expectation and completing in $O(\log \log n \cdot \log \Delta)$ rounds in the worst case. This advancement represents a significant improvement over the work of Flin and Mittal [PODC 2024], who achieved the same communication cost but required $O(n)$ rounds in expectation, thereby making a significant reduction in the round complexity. We also present a randomized protocol for a $(2\Delta - 1)$-edge coloring of $G$, which maintains the same $O(n)$ bits of communication in expectation over $O(\log^\ast \Delta)$ rounds in the worst case. We complement the result with a tight $\Omega(n)$-bit lower bound on the communication complexity of the $(2\Delta-1)$-edge coloring, while a similar $\Omega(n)$ lower bound for the $(\Delta+1)$-vertex coloring has been established by Flin and Mittal [PODC 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12589v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Gopinath Mishra, Hung Thuan Nguyen, Farrel D Salim</dc:creator>
    </item>
    <item>
      <title>Cuckoo Heavy Keeper and the balancing act of maintaining heavy-hitters in stream processing</title>
      <link>https://arxiv.org/abs/2412.12873</link>
      <description>arXiv:2412.12873v1 Announce Type: new 
Abstract: Finding heavy hitters in databases and data streams is a fundamental problem, with applications ranging from network monitoring to database query optimization, anomaly detection, and more. Approximation algorithms offer practical solutions, but they present multi-faceted trade-offs involving throughput, memory usage, and accuracy. Moreover, evolving applications demand capabilities beyond sequential processing - they require both parallel performance scaling and support for concurrent queries/updates.
  To address these challenges holistically, we first propose a new algorithm, Cuckoo Heavy Keeper, that combines careful algorithmic design with system-aware perspectives, to effectively balance competing trade-offs. Recognizing the diverse needs of different applications, we then propose two parallel algorithms optimized for different workload patterns: one prioritizes insertion throughput while the other targets efficient heavy hitter queries, both achieving high performance via efficient parallel scaling while supporting concurrent operations. Besides discussing the algorithms' bounds, through extensive evaluation, we demonstrate that Cuckoo Heavy Keeper improves throughput by 1.7X to 5.6X and accuracy by 1.9X to 27,542X compared to state-of-the-art methods under tight memory constraints, maintaining these advantages even with low-skew datasets. The parallel variants achieve near-linear scaling up to 70 threads while maintaining heavy hitters query latencies as low as 36 {\mu}sec to 350 {\mu}sec.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12873v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinh Quang Ngo, Marina Papatriantafilou</dc:creator>
    </item>
    <item>
      <title>Convergence of the QuickVal Residual</title>
      <link>https://arxiv.org/abs/2412.12599</link>
      <description>arXiv:2412.12599v1 Announce Type: cross 
Abstract: QuickSelect (aka Find), introduced by Hoare (1961), is a randomized algorithm for selecting a specified order statistic from an input sequence of $n$ objects, or rather their identifying labels usually known as keys. The keys can be numeric or symbol strings, or indeed any labels drawn from a given linearly ordered set. We discuss various ways in which the cost of comparing two keys can be measured, and we can measure the efficiency of the algorithm by the total cost of such comparisons.
  We define and discuss a closely related algorithm known as QuickVal and a natural probabilistic model for the input to this algorithm; QuickVal searches (almost surely unsuccessfully) for a specified population quantile $\alpha \in [0, 1]$ in an input sample of size $n$. Call the total cost of comparisons for this algorithm $S_n$. We discuss a natural way to define the random variables $S_1, S_2, \ldots$ on a common probability space. For a general class of cost functions, Fill and Nakama (2013) proved under mild assumptions that the scaled cost $S_n / n$ of QuickVal converges in $L^p$ and almost surely to a limit random variable $S$. For a general cost function, we consider what we term the QuickVal residual: \[
  R_n := \frac{S_n}n - S. \] The residual is of natural interest, especially in light of the previous analogous work on the sorting algorithm QuickSort. In the case $\alpha = 0$ of QuickMin with unit cost per key-comparison, we are able to calculate -- \`a la Bindjeme and Fill (2012) for QuickSort -- the exact (and asymptotic) $L^2$-norm of the residual. We take the result as motivation for the scaling factor $\sqrt{n}$ for the QuickVal residual for general population quantiles and for general cost. We then prove in general (under mild conditions on the cost function) that $\sqrt{n}\,R_n$ converges in law to a scale-mixture of centered Gaussians, and we also prove convergence of moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12599v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Allen Fill, Jason Matterer</dc:creator>
    </item>
    <item>
      <title>Computing crossing numbers with topological and geometric restrictions</title>
      <link>https://arxiv.org/abs/2412.13092</link>
      <description>arXiv:2412.13092v1 Announce Type: cross 
Abstract: Computing the crossing number of a graph is one of the most classical problems in computational geometry. Both it and numerous variations of the problem have been studied, and overcoming their frequent computational difficulty is an active area of research. Particularly recently, there has been increased effort to show and understand the parameterized tractability of various crossing number variants. While many results in this direction use a similar approach, a general framework remains elusive. We suggest such a framework that generalizes important previous results, and can even be used to show the tractability of deciding crossing number variants for which this was stated as an open problem in previous literature. Our framework targets variants that prescribe a partial predrawing and some kind of topological restrictions on crossings. Additionally, to provide evidence for the non-generalizability of previous approaches for the partially crossing number problem to allow for geometric restrictions, we show a new more constrained hardness result for partially predrawn rectilinear crossing number. In particular, we show W-hardness of deciding Straight-Line Planarity Extension parameterized by the number of missing edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13092v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thekla Hamm, Fabian Klute, Irene Parada</dc:creator>
    </item>
    <item>
      <title>Fast Computation of the Discrete Fourier Transform Square Index Coefficients</title>
      <link>https://arxiv.org/abs/2407.00182</link>
      <description>arXiv:2407.00182v2 Announce Type: replace 
Abstract: The $N$-point discrete Fourier transform (DFT) is a cornerstone for several signal processing applications. Many of these applications operate in real-time, making the computational complexity of the DFT a critical performance indicator to be optimized. Unfortunately, whether the $\mathcal{O}(N\log_2 N)$ time complexity of the fast Fourier transform (FFT) can be outperformed remains an unresolved question in the theory of computation. However, in many applications of the DFT -- such as compressive sensing, image processing, and wideband spectral analysis -- only a small fraction of the output signal needs to be computed because the signal is sparse. This motivates the development of algorithms that compute specific DFT coefficients more efficiently than the FFT algorithm. In this article, we show that the number of points of some DFT coefficients can be dramatically reduced by means of elementary mathematical properties. We present an algorithm that compacts the square index coefficients (SICs) of DFT (i.e., $X_{k\sqrt{N}}$, $k=0,1,\cdots, \sqrt{N}-1$, for a square number $N$) from $N$ to $\sqrt{N}$ points at the expense of $N-1$ complex sums and no multiplication. Based on this, any regular DFT algorithm can be straightforwardly applied to compute the SICs with a reduced number of complex multiplications. If $N$ is a power of two, one can combine our algorithm with the FFT to calculate all SICs in $\mathcal{O}(\sqrt{N}\log_2\sqrt{N})$ time complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00182v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>eess.SP</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MSP.2024.3511930</arxiv:DOI>
      <dc:creator>Saulo Queiroz, Jo\~ao P. Vilela, Edmundo Monteiro</dc:creator>
    </item>
    <item>
      <title>Geometry helps in routing scalability</title>
      <link>https://arxiv.org/abs/2412.07964</link>
      <description>arXiv:2412.07964v2 Announce Type: replace 
Abstract: Delay Tolerant Networking (DTN) aims to address a myriad of significant networking challenges that appear in time-varying settings, such as mobile and satellite networks, wherein changes in network topology are frequent and often subject to environmental constraints. Within this paradigm, routing problems are often solved by extending classical graph-theoretic path finding algorithms, such as the Bellman-Ford or Floyd-Warshall algorithms, to the time-varying setting; such extensions are simple to understand, but they have strict optimality criteria and can exhibit non-polynomial scaling. Acknowledging this, we study time-varying shortest path problems on metric graphs whose vertices are traced by semi-algebraic curves. As an exemplary application, we establish a polynomial upper bound on the number of topological critical events encountered by a set of $n$ satellites moving along elliptic curves in low Earth orbit (per orbital period). Experimental evaluations on networks derived from STARLINK satellite TLE's demonstrate that not only does this geometric framework allow for routing schemes between satellites requiring recomputation an order of magnitude less than graph-based methods, but it also demonstrates metric spanner properties exist in metric graphs derived from real-world data, opening the door for broader applications of geometric DTN routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07964v2</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Piekenbrock</dc:creator>
    </item>
    <item>
      <title>Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML</title>
      <link>https://arxiv.org/abs/2405.15913</link>
      <description>arXiv:2405.15913v3 Announce Type: replace-cross 
Abstract: Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF, which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may exceed $10^4$ and the number of model parameters may exceed $10^7$. In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to handle settings with virtually any number of model parameters and training iterations, with negligible utility degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15913v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan McKenna</dc:creator>
    </item>
    <item>
      <title>Enumeration of minimal transversals of hypergraphs of bounded VC-dimension</title>
      <link>https://arxiv.org/abs/2407.00694</link>
      <description>arXiv:2407.00694v3 Announce Type: replace-cross 
Abstract: We consider the problem of enumerating all minimal transversals (also called minimal hitting sets) of a hypergraph $\mathcal{H}$. An equivalent formulation of this problem known as the \emph{transversal hypergraph} problem (or \emph{hypergraph dualization} problem) is to decide, given two hypergraphs, whether one corresponds to the set of minimal transversals of the other. The existence of a polynomial time algorithm to solve this problem is a long standing open question. In \cite{fredman_complexity_1996}, the authors present the first sub-exponential algorithm to solve the transversal hypergraph problem which runs in quasi-polynomial time, making it unlikely that the problem is (co)NP-complete.
  In this paper, we show that when one of the two hypergraphs is of bounded VC-dimension, the transversal hypergraph problem can be solved in polynomial time, or equivalently that if $\mathcal{H}$ is a hypergraph of bounded VC-dimension, then there exists an incremental polynomial time algorithm to enumerate its minimal transversals. This result generalizes most of the previously known polynomial cases in the literature since they almost all consider classes of hypergraphs of bounded VC-dimension. As a consequence, the hypergraph transversal problem is solvable in polynomial time for any class of hypergraphs closed under partial subhypergraphs. We also show that the proposed algorithm runs in quasi-polynomial time in general hypergraphs and runs in polynomial time if the conformality of the hypergraph is bounded, which is one of the few known polynomial cases where the VC-dimension is unbounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00694v3</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Mary</dc:creator>
    </item>
    <item>
      <title>Six Candidates Suffice to Win a Voter Majority</title>
      <link>https://arxiv.org/abs/2411.03390</link>
      <description>arXiv:2411.03390v3 Announce Type: replace-cross 
Abstract: A cornerstone of social choice theory is Condorcet's paradox which says that in an election where $n$ voters rank $m$ candidates it is possible that, no matter which candidate is declared the winner, a majority of voters would have preferred an alternative candidate. Instead, can we always choose a small committee of winning candidates that is preferred to any alternative candidate by a majority of voters?
  Elkind, Lang, and Saffidine raised this question and called such a committee a Condorcet winning set. They showed that winning sets of size $2$ may not exist, but sets of size logarithmic in the number of candidates always do. In this work, we show that Condorcet winning sets of size $6$ always exist, regardless of the number of candidates or the number of voters. More generally, we show that if $\frac{\alpha}{1 - \ln \alpha} \geq \frac{2}{k + 1}$, then there always exists a committee of size $k$ such that less than an $\alpha$ fraction of the voters prefer an alternate candidate. These are the first nontrivial positive results that apply for all $k \geq 2$.
  Our proof uses the probabilistic method and the minimax theorem, inspired by recent work on approximately stable committee selection. We construct a distribution over committees that performs sufficiently well (when compared against any candidate on any small subset of the voters) so that this distribution must contain a committee with the desired property in its support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03390v3</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moses Charikar, Alexandra Lassota, Prasanna Ramakrishnan, Adrian Vetta, Kangning Wang</dc:creator>
    </item>
  </channel>
</rss>
