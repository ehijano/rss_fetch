<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 01:38:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification</title>
      <link>https://arxiv.org/abs/2510.24749</link>
      <description>arXiv:2510.24749v1 Announce Type: new 
Abstract: The escalating complexity of modern codebases has intensified the need for retrieval systems capable of interpreting cross-component change intents, a capability fundamentally absent in conventional function-level search paradigms. While recent studies have improved the alignment between natural language queries and code snippets, retrieving contextually relevant code for specific change requests remains largely underexplored. To address this gap, we introduce RepoAlign-Bench, the first benchmark specifically designed to evaluate repository-level code retrieval under change request driven scenarios, encompassing 52k annotated instances. This benchmark shifts the retrieval paradigm from function-centric matching to holistic repository-level reasoning. Furthermore, we propose ReflectCode, an adversarial reflection augmented dual-tower architecture featuring disentangled code_encoder and doc_encoder components. ReflectCode dynamically integrates syntactic patterns, function dependencies, and semantic expansion intents through large language model guided reflection. Comprehensive experiments demonstrate that ReflectCode achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over state-of-the-art baselines, establishing a new direction for context-aware code retrieval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24749v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aofan Liu, Shiyuan Song, Haoxuan Li, Cehao Yang, Yiyan Qi</dc:creator>
    </item>
    <item>
      <title>Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering</title>
      <link>https://arxiv.org/abs/2510.24799</link>
      <description>arXiv:2510.24799v1 Announce Type: new 
Abstract: The rapid advancement of AI-assisted software engineering has brought transformative potential to the field of software engineering, but existing tools and paradigms remain limited by cognitive overload, inefficient tool integration, and the narrow capabilities of AI copilots. In response, we propose Compiler.next, a novel search-based compiler designed to enable the seamless evolution of AI-native software systems as part of the emerging Software Engineering 3.0 era. Unlike traditional static compilers, Compiler.next takes human-written intents and automatically generates working software by searching for an optimal solution. This process involves dynamic optimization of cognitive architectures and their constituents (e.g., prompts, foundation model configurations, and system parameters) while finding the optimal trade-off between several objectives, such as accuracy, cost, and latency. This paper outlines the architecture of Compiler.next and positions it as a cornerstone in democratizing software development by lowering the technical barrier for non-experts, enabling scalable, adaptable, and reliable AI-powered software. We present a roadmap to address the core challenges in intent compilation, including developing quality programming constructs, effective search heuristics, reproducibility, and interoperability between compilers. Our vision lays the groundwork for fully automated, search-driven software development, fostering faster innovation and more efficient AI-driven systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24799v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Filipe R. Cogo, Gustavo A. Oliva, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>A Roadmap for Tamed Interactions with Large Language Models</title>
      <link>https://arxiv.org/abs/2510.24819</link>
      <description>arXiv:2510.24819v1 Announce Type: new 
Abstract: We are witnessing a bloom of AI-powered software driven by Large Language Models (LLMs). Although the applications of these LLMs are impressive and seemingly countless, their unreliability hinders adoption. In fact, the tendency of LLMs to produce faulty or hallucinated content makes them unsuitable for automating workflows and pipelines. In this regard, Software Engineering (SE) provides valuable support, offering a wide range of formal tools to specify, verify, and validate software behaviour. Such SE tools can be applied to define constraints over LLM outputs and, consequently, offer stronger guarantees on the generated content. In this paper, we argue that the development of a Domain Specific Language (DSL) for scripting interactions with LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based applications. Currently, LLMs and LLM-based software still lack reliability, robustness, and trustworthiness, and the tools or frameworks to cope with these issues suffer from fragmentation. In this paper, we present our vision of LSL. With LSL, we aim to address the limitations above by exploring ways to control LLM outputs, enforce structure in interactions, and integrate these aspects with verification, validation, and explainability. Our goal is to make LLM interaction programmable and decoupled from training or implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24819v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincenzo Scotti, Jan Keim, Tobias Hey, Andreas Metzger, Anne Koziolek, Raffaela Mirandola</dc:creator>
    </item>
    <item>
      <title>VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus</title>
      <link>https://arxiv.org/abs/2510.25015</link>
      <description>arXiv:2510.25015v1 Announce Type: new 
Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated verification from single functions to more complex data structure modules in Verus. VeriStruct employs a planner module to orchestrate the systematic generation of abstractions, type invariants, specifications, and proof code. To address the challenge that LLMs often misunderstand Verus' annotation syntax and verification-specific semantics, VeriStruct embeds syntax guidance within prompts and includes a repair stage to automatically correct annotation errors. In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in total. These results represent an important step toward the goal of automatic AI-assisted formal verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25015v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuyue Sun, Yican Sun, Daneshvar Amrollahi, Ethan Zhang, Shuvendu Lahiri, Shan Lu, David Dill, Clark Barrett</dc:creator>
    </item>
    <item>
      <title>Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study</title>
      <link>https://arxiv.org/abs/2510.25016</link>
      <description>arXiv:2510.25016v1 Announce Type: new 
Abstract: The future of Requirements Engineering (RE) is increasingly driven by artificial intelligence (AI), reshaping how we elicit, analyze, and validate requirements. Traditional RE is based on labor-intensive manual processes prone to errors and complexity. AI-powered approaches, specifically large language models (LLMs), natural language processing (NLP), and generative AI, offer transformative solutions and reduce inefficiencies. However, the use of AI in RE also brings challenges like algorithmic bias, lack of explainability, and ethical concerns related to automation. To address these issues, this study introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that integrates AI-driven analysis with human oversight to improve requirements elicitation, analysis, and validation. The model emphasizes ethical AI use through transparency, explainability, and bias mitigation. We outline a multi-phase research methodology focused on preparing RE datasets, fine-tuning AI models, and designing collaborative human-AI workflows. This preliminary study presents the conceptual framework and early-stage prototype implementation, establishing a research agenda and practical design direction for applying intelligent data science techniques to semi-structured and unstructured RE data in collaborative environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25016v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IDSTA66210.2025.11202850</arxiv:DOI>
      <arxiv:journal_reference>2025 Sixth International Conference on Intelligent Data Science Technologies and Applications (IDSTA 2025), IEEE</arxiv:journal_reference>
      <dc:creator>Mateen Ahmed Abbasi, Petri Ihantola, Tommi Mikkonen, Niko M\"akitalo</dc:creator>
    </item>
    <item>
      <title>Automating Benchmark Design</title>
      <link>https://arxiv.org/abs/2510.25039</link>
      <description>arXiv:2510.25039v1 Announce Type: new 
Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are the primary tool for assessing model capabilities, but these quickly become saturated. In contrast, dynamic benchmarks evolve alongside the models they evaluate, but are expensive to create and continuously update. To address these challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a framework that leverages environment design principles to automate the process of dynamic benchmark design. BeTaL works by parameterizing key design choices in base benchmark templates and uses LLMs to reason through the resulting parameter space to obtain target properties (such as difficulty and realism) in a cost-efficient manner. We validate this approach on its ability to create benchmarks with desired difficulty levels. Using BeTaL, we create two new benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive evaluation on these three tasks and multiple target difficulty levels shows that BeTaL produces benchmarks much closer to the desired difficulty, with average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25039v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amanda Dsouza, Harit Vishwakarma, Zhengyang Qi, Justin Bauer, Derek Pham, Thomas Walshe, Armin Parchami, Frederic Sala, Paroma Varma</dc:creator>
    </item>
    <item>
      <title>Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection</title>
      <link>https://arxiv.org/abs/2510.25057</link>
      <description>arXiv:2510.25057v1 Announce Type: new 
Abstract: Plagiarism detection in programming education faces growing challenges due to increasingly sophisticated obfuscation techniques, particularly automated refactoring-based attacks. While code plagiarism detection systems used in education practice are resilient against basic obfuscation, they struggle against structural modifications that preserve program behavior, especially caused by refactoring-based obfuscation. This paper presents a novel and extensible framework that enhances state-of-the-art detectors by leveraging code property graphs and graph transformations to counteract refactoring-based obfuscation. Our comprehensive evaluation of real-world student submissions, obfuscated using both algorithmic and AI-based obfuscation attacks, demonstrates a significant improvement in detecting plagiarized code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25057v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3744916.3773225</arxiv:DOI>
      <dc:creator>Robin Maisch, Larissa Schmid, Timur Sa\u{g}lam, Nils Niehues</dc:creator>
    </item>
    <item>
      <title>Adaptive Proof Refinement with LLM-Guided Strategy Selection</title>
      <link>https://arxiv.org/abs/2510.25103</link>
      <description>arXiv:2510.25103v1 Announce Type: new 
Abstract: Formal verification via theorem proving enables the expressive specification and rigorous proof of software correctness, but it is difficult to scale due to the significant manual effort and expertise required. While Large Language Models (LLMs) show potential in proof generation, they frequently produce incorrect proofs on the first attempt and require additional strategies for iterative refinement. However, existing approaches employ fixed refinement strategies and cannot dynamically choose an effective strategy based on the particular issues in a generated proof, which limits their performance. To overcome this limitation, we introduce Adapt, a novel proof refinement framework that leverages an LLM-guided decision-maker to dynamically select a suitable refinement strategy according to the state of the proof assistant and available context of an incorrect proof. We evaluate Adapt on two benchmarks against four existing methods and find that it significantly outperforms the best baseline on both by proving 16.63% and 18.58% more theorems, respectively. Furthermore, we demonstrate Adapt's generalizability by evaluating it across five different LLMs. We also conduct ablation studies to measure the contribution of each component and compare the trade-offs of alternative decision-maker designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25103v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minghai Lu, Zhe Zhou, Danning Xie, Songlin Jia, Benjamin Delaware, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>Automated Program Repair Based on REST API Specifications Using Large Language Models</title>
      <link>https://arxiv.org/abs/2510.25148</link>
      <description>arXiv:2510.25148v1 Announce Type: new 
Abstract: Many cloud services provide REST API accessible to client applications. However, developers often identify specification violations only during testing, as error messages typically lack the detail necessary for effective diagnosis. Consequently, debugging requires trial and error. This study proposes dcFix, a method for detecting and automatically repairing REST API misuses in client programs. In particular, dcFix identifies non-conforming code fragments, integrates them with the relevant API specifications into prompts, and leverages a Large Language Model (LLM) to produce the corrected code. Our evaluation demonstrates that dcFix accurately detects misuse and outperforms the baseline approach, in which prompts to the LLM omit any indication of code fragments non conforming to REST API specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25148v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Katsuki Yamagishi, Norihiro Yoshida, Erina Makihara, Katsuro Inoue</dc:creator>
    </item>
    <item>
      <title>Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2510.25195</link>
      <description>arXiv:2510.25195v1 Announce Type: new 
Abstract: Code comment generation aims to produce a generic overview of a code snippet, helping developers understand and maintain code. However, generic summaries alone are insufficient to meet the diverse needs of practitioners; for example, developers expect the implementation insights to be presented in an untangled manner, while users seek clear usage instructions. This highlights the necessity of multi-intent comment generation. With the widespread adoption of Large Language Models (LLMs) for code-related tasks, these models have been leveraged to tackle the challenge of multi-intent comment generation. Despite their successes, state-of-the-art LLM-based approaches often struggle to construct correct relationships among intents, code, and comments within a smaller number of demonstration examples. To mitigate this issue, we propose a framework named KUMIC for multi-intent comment generation. Built upon in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize knowledge utilization for LLMs to generate intent-specific comments. Specifically, KUMIC first designs a retrieval mechanism to obtain similar demonstration examples, which exhibit high code-comment consistency. Then, KUMIC leverages CoT to guide LLMs to focus on statements facilitating the derivation of code comments aligned with specific intents. In this context, KUMIC constructs a mapping knowledge chain, linking code to intent-specific statements to comments, which enables LLMs to follow similar reasoning steps when generating the desired comments. We conduct extensive experiments to evaluate KUMIC, and the results demonstrate that KUMIC outperforms state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms of BLEU, METEOR, ROUGE-L, and SBERT, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25195v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuochuan Li, Zan Wang, Xiaoning Du, Zhuo Wu, Jiuqiao Yu, Junjie Chen</dc:creator>
    </item>
    <item>
      <title>TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices</title>
      <link>https://arxiv.org/abs/2510.25242</link>
      <description>arXiv:2510.25242v1 Announce Type: new 
Abstract: The diversification of functionalities and the development of the IoT are making embedded systems larger and more complex in structure. Ensuring system reliability, especially in terms of security, necessitates selecting an appropriate programming language. As part of existing research, TECS/Rust has been proposed as a framework that combines Rust and component-based development (CBD) to enable scalable system design and enhanced reliability. This framework represents system structures using static mutable variables, but excessive exclusive controls applied to ensure thread safety have led to performance degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework utilizing call flows to address these limitations. The proposed Rust code leverages real-time OS exclusive control mechanisms, optimizing performance without compromising reusability. Rust code is automatically generated based on component descriptions. Evaluations demonstrate reduced overhead due to optimized exclusion control and high reusability of the generated code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25242v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ISORC65339.2025.00051</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 28th IEEE International Symposium on Real-Time Distributed Computing (ISORC), 2025, pp. 325-328</arxiv:journal_reference>
      <dc:creator>Nao Yoshimura, Hiroshi Oyama, Takuya Azumi</dc:creator>
    </item>
    <item>
      <title>TECS/Rust: Memory-safe Component Framework for Embedded Systems</title>
      <link>https://arxiv.org/abs/2510.25270</link>
      <description>arXiv:2510.25270v1 Announce Type: new 
Abstract: As embedded systems grow in complexity and scale due to increased functional diversity, component-based development (CBD) emerges as a solution to streamline their architecture and enhance functionality reuse. CBD typically utilizes the C programming language for its direct hardware access and low-level operations, despite its susceptibility to memory-related issues. To address these concerns, this paper proposes TECS/Rust, a Rust-based framework specifically designed for TECS, which is a component framework for embedded systems. It leverages Rust's compile-time memory-safe features, such as lifetime and borrowing, to mitigate memory vulnerabilities common with C. The proposed framework not only ensures memory safety but also maintains the flexibility of CBD, automates Rust code generation for CBD components, and supports efficient integration with real-time operating systems. An evaluation of the amount of generated code indicates that the code generated by this paper framework accounts for a large percentage of the actual code. Compared to code developed without the proposed framework, the difference in execution time is minimal, indicating that the overhead introduced by the proposed framework is negligible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25270v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ISORC61049.2024.10551370</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 27th IEEE International Symposium on Real-Time Distributed Computing (ISORC), 2024, pp. 1-11</arxiv:journal_reference>
      <dc:creator>Nao Yoshimura, Hiroshi Oyama, Takuya Azumi</dc:creator>
    </item>
    <item>
      <title>Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases</title>
      <link>https://arxiv.org/abs/2510.25297</link>
      <description>arXiv:2510.25297v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) increasingly generate code in software development, ensuring the quality of LLM-generated code has become important. Traditional testing approaches using Example-based Testing (EBT) often miss edge cases -- defects that occur at boundary values, special input patterns, or extreme conditions. This research investigates the characteristics of LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge cases. We analyze 16 HumanEval problems where standard solutions failed on extended test cases, generating both PBT and EBT test codes using Claude-4-sonnet. Our experimental results reveal that while each method individually achieved a 68.75\% bug detection rate, combining both approaches improved detection to 81.25\%. The analysis demonstrates complementary characteristics: PBT effectively detects performance issues and edge cases through extensive input space exploration, while EBT effectively detects specific boundary conditions and special patterns. These findings suggest that a hybrid approach leveraging both testing methods can improve the reliability of LLM-generated code, providing guidance for test generation strategies in LLM-based code generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25297v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hidetake Tanaka, Haruto Tanaka, Kazumasa Shimari, Kenichi Matsumoto</dc:creator>
    </item>
    <item>
      <title>Dissect-and-Restore: AI-based Code Verification with Transient Refactoring</title>
      <link>https://arxiv.org/abs/2510.25406</link>
      <description>arXiv:2510.25406v2 Announce Type: new 
Abstract: Formal verification is increasingly recognized as a critical foundation for building reliable software systems. However, the need for specialized expertise to write precise specifications, navigate complex proof obligations, and learn annotations often makes verification an order of magnitude more expensive than implementation. While modern AI systems can recognize patterns in mathematical proofs and interpret natural language, effectively integrating them into the formal verification process remains an open challenge. We present Prometheus, a novel AI-assisted system that facilitates automated code verification with current AI capabilities in conjunction with modular software engineering principles (e.g., modular refactoring). Our approach begins by decomposing complex program logic, such as nested loops, into smaller, verifiable components. Once verified, these components are recomposed to construct a proof of the original program. This decomposition-recomposition workflow is non-trivial. Prometheus addresses this by guiding the proof search through structured decomposition of complex lemmas into smaller, verifiable sub-lemmas. When automated tools are insufficient, users can provide lightweight natural language guidance to steer the proof process effectively. Our evaluation demonstrates that transiently applying modular restructuring to the code substantially improves the AI's effectiveness in verifying individual components. This approach successfully verifies 86% of tasks in our curated dataset, compared to 68% for the baseline. Gains are more pronounced with increasing specification complexity, improving from 30% to 69%, and when integrating proof outlines for complex programs, from 25% to 87%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25406v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changjie Wang, Mariano Scazzariello, Anoud Alshnakat, Roberto Guanciale, Dejan Kosti\'c, Marco Chiesa</dc:creator>
    </item>
    <item>
      <title>What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow</title>
      <link>https://arxiv.org/abs/2510.25423</link>
      <description>arXiv:2510.25423v1 Announce Type: new 
Abstract: AI agents have rapidly gained popularity across research and industry as systems that extend large language models with additional capabilities to plan, use tools, remember, and act toward specific goals. Yet despite their promise, developers face persistent and often underexplored challenges when building, deploying, and maintaining these emerging systems. To identify these challenges, we study developer discussions on Stack Overflow, the world's largest developer-focused Q and A platform with about 60 million questions and answers and 30 million users. We construct a taxonomy of developer challenges through tag expansion and filtering, apply LDA-MALLET for topic modeling, and manually validate and label the resulting themes. Our analysis reveals seven major areas of recurring issues encompassing 77 distinct technical challenges related to runtime integration, dependency management, orchestration complexity, and evaluation reliability. We further quantify topic popularity and difficulty to identify which issues are most common and hardest to resolve, map the tools and programming languages used in agent development, and track their evolution from 2021 to 2025 in relation to major AI model and framework releases. Finally, we present the implications of our results, offering concrete guidance for practitioners, researchers, and educators on agent reliability and developer support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25423v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Asgari, Annibale Panichella, Pouria Derakhshanfar, Mitchell Olsthoorn</dc:creator>
    </item>
    <item>
      <title>Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies</title>
      <link>https://arxiv.org/abs/2510.25506</link>
      <description>arXiv:2510.25506v1 Announce Type: new 
Abstract: Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both other researchers and practitioners. One important step towards excelling in empirical research on LLMs and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 86 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were fit for reproduction. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25506v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Angermeir, Maximilian Amougou, Mark Kreitz, Andreas Bauer, Matthias Linhuber, Davide Fucci, Fabiola Moy\'on C., Daniel Mendez, Tony Gorschek</dc:creator>
    </item>
    <item>
      <title>Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL</title>
      <link>https://arxiv.org/abs/2510.25665</link>
      <description>arXiv:2510.25665v1 Announce Type: new 
Abstract: Fuzzing has become a key search-based technique for software testing, but continuous fuzzing campaigns consume substantial computational resources and generate significant carbon footprints. Existing grey-box fuzzing approaches like AFL++ focus primarily on coverage maximisation, without considering the energy costs of exploring different execution paths. This paper presents GreenAFL, an energy-aware framework that incorporates power consumption into the fuzzing heuristics to reduce the environmental impact of automated testing whilst maintaining coverage. GreenAFL introduces two key modifications to traditional fuzzing workflows: energy-aware corpus minimisation considering power consumption when reducing initial corpora, and energy-guided heuristics that direct mutation towards high-coverage, low-energy inputs. We conduct an ablation study comparing vanilla AFL++, energy-based corpus minimisation, and energy-based heuristics to evaluate the individual contributions of each component. Results show that highest coverage, and lowest energy usage is achieved whenever at least one of our modifications is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25665v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayse Irmak Ercevik, Aidan Dakhama, Melane Navaratnarajah, Yazhuo Cao, Leo Fernandes</dc:creator>
    </item>
    <item>
      <title>A Configuration-First Framework for Reproducible, Low-Code Localization</title>
      <link>https://arxiv.org/abs/2510.25692</link>
      <description>arXiv:2510.25692v1 Announce Type: new 
Abstract: Machine learning is increasingly permeating radio-based localization services. To keep results credible and comparable, everyday workflows should make rigorous experiment specification and exact repeatability the default, without blocking advanced experimentation. However, in practice, researchers face a three-way gap that could be filled by a framework that offers (i) low coding effort for end-to-end studies, (ii) reproducibility by default including versioned code, data, and configurations, controlled randomness, isolated runs, and recorded artifacts, and (iii) built-in extensibility so new models, metrics, and stages can be added with minimal integration effort. Existing tools rarely deliver all three for machine learning in general and localization workflows in particular. In this paper we introduce LOCALIZE, a low-code, configuration-first framework for radio localization in which experiments are declared in human-readable configuration, a workflow orchestrator runs standardized pipelines from data preparation to reporting, and all artifacts, such as datasets, models, metrics, and reports, are versioned. The preconfigured, versioned datasets reduce initial setup and boilerplate, speeding up model development and evaluation. The design, with clear extension points, allows experts to add components without reworking the infrastructure. In a qualitative comparison and a head-to-head study against a plain Jupyter notebook baseline, we show that the framework reduces authoring effort while maintaining comparable runtime and memory behavior. Furthermore, using a Bluetooth Low Energy dataset, we show that scaling across training data (1x to 10x) keeps orchestration overheads bounded as data grows. Overall, the framework makes reproducible machine-learning-based localization experimentation practical, accessible, and extensible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25692v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Strnad (Jo\v{z}ef Stefan Institute, Slovenia), Bla\v{z} Bertalani\v{c} (Jo\v{z}ef Stefan Institute, Slovenia), Carolina Fortuna (Jo\v{z}ef Stefan Institute, Slovenia)</dc:creator>
    </item>
    <item>
      <title>Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents</title>
      <link>https://arxiv.org/abs/2510.25694</link>
      <description>arXiv:2510.25694v1 Announce Type: new 
Abstract: Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25694v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayi Kuang, Yinghui Li, Xin Zhang, Yangning Li, Di Yin, Xing Sun, Ying Shen, Philip S. Yu</dc:creator>
    </item>
    <item>
      <title>Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation</title>
      <link>https://arxiv.org/abs/2510.01225</link>
      <description>arXiv:2510.01225v1 Announce Type: cross 
Abstract: The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01225v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SUMMA64428.2024.10803746</arxiv:DOI>
      <arxiv:journal_reference>2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA), Lipetsk, Russian Federation, 2024, pp. 317-321</arxiv:journal_reference>
      <dc:creator>Andrei Lazarev, Dmitrii Sedov</dc:creator>
    </item>
    <item>
      <title>ODataX: A Progressive Evolution of the Open Data Protocol</title>
      <link>https://arxiv.org/abs/2510.24761</link>
      <description>arXiv:2510.24761v1 Announce Type: cross 
Abstract: The Open Data Protocol (OData) provides a standardized approach for building and consuming RESTful APIs with rich query capabilities. Despite its power and maturity, OData adoption remains confined primarily to enterprise environments, particularly within Microsoft and SAP ecosystems. This paper analyzes the key barriers preventing wider OData adoption and introduces ODataX, an evolved version of the protocol designed to address these limitations. ODataX maintains backward compatibility with OData v4 while introducing progressive complexity disclosure through simplified query syntax, built-in performance guardrails via query cost estimation, and enhanced caching mechanisms. This work aims to bridge the gap between enterprise-grade query standardization and the simplicity demanded by modern web development practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24761v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirudh Ganesh, Nitin Sood</dc:creator>
    </item>
    <item>
      <title>Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2510.24909</link>
      <description>arXiv:2510.24909v1 Announce Type: cross 
Abstract: Requirements engineering increasingly occurs in multi-stakeholder environments where organizations simultaneously cooperate and compete, creating coopetitive relationships in which trust evolves dynamically based on observed behavior over repeated interactions. While conceptual modeling languages like i* represent trust relationships qualitatively, they lack computational mechanisms for analyzing how trust changes with behavioral evidence. Conversely, computational trust models from multi-agent systems provide algorithmic updating but lack grounding in requirements engineering contexts and conceptual models. This technical report bridges this gap by developing a computational trust model that extends game-theoretic foundations for strategic coopetition with dynamic trust evolution. We introduce trust as a two-layer system with immediate trust responding to current behavior and reputation tracking violation history. Trust evolves through asymmetric updating where cooperation builds trust gradually while violations erode it sharply, creating hysteresis effects and trust ceilings that constrain relationship recovery. We develop a structured translation framework enabling requirements engineers to instantiate computational trust models from i* dependency networks and organizational contexts. Comprehensive experimental validation across 78,125 parameter configurations establishes robust emergence of negativity bias, hysteresis effects, and cumulative damage amplification. Empirical validation using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60 validation points (81.7%), successfully reproducing documented trust evolution across five distinct relationship phases including crisis and recovery periods. This technical report builds upon its foundational companion work in arXiv:2510.18802.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24909v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vik Pant, Eric Yu</dc:creator>
    </item>
    <item>
      <title>Proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis</title>
      <link>https://arxiv.org/abs/2510.25468</link>
      <description>arXiv:2510.25468v1 Announce Type: cross 
Abstract: This volume contains the post-proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis (HCVS 2025), which took place in Zagreb, Croatia, on July 22, 2025, as affiliated workshop of the 37th International Conference on Computer Aided Verification (CAV 2025).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25468v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.434</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 434, 2025</arxiv:journal_reference>
      <dc:creator>Emanuele De Angelis (CNR-IASI, Italy), Florian Frohn (RWTH Aachen, Germany)</dc:creator>
    </item>
    <item>
      <title>The Strength of Weak Ties Between Open-Source Developers</title>
      <link>https://arxiv.org/abs/2411.05646</link>
      <description>arXiv:2411.05646v2 Announce Type: replace 
Abstract: In a real-world social network, weak ties (reflecting low-intensity, infrequent interactions) act as bridges and connect people to different social circles, giving them access to diverse information and opportunities that are not available within one's immediate, close-knit vicinity. Weak ties can be crucial for creativity and innovation, as they introduce ideas and approaches that people can then combine in novel ways, leading to innovative solutions. Do weak ties facilitate creativity in software in similar ways? This paper suggests that the answer is "yes." Concretely, we study the correlation between developers' knowledge acquisition through three distinct interaction networks on GitHub and the innovativeness of the projects they develop, across over 37,000 Python projects hosted on GitHub. Our findings suggest that the topical diversity of projects in which developers engage, rather than the volume, correlates positively with the innovativeness of their future code. Notably, exposure through weak interactions (e.g., starring) emerges as a stronger predictor of future novelty than via strong ones (e.g., committing)</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05646v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongbo Fang, Patrick Park, James Evans, James Herbsleb, Bogdan Vasilescu</dc:creator>
    </item>
    <item>
      <title>Security Implications of User Non-compliance Behavior to Software Updates: A Risk Assessment Study</title>
      <link>https://arxiv.org/abs/2411.06262</link>
      <description>arXiv:2411.06262v2 Announce Type: replace 
Abstract: Software updates are essential to enhance security, fix bugs, and add better features to the existing software. While some users accept software updates, non-compliance remains a widespread issue. While some users accept software updates, non-compliance remains a widespread issue. End users' systems remain vulnerable to security threats when security updates are not installed or are installed with a delay. Despite research efforts, users' noncompliance behavior with software updates is still prevalent. In this study, we explored how psychological factors influence users' perception and behavior toward software updates. In addition, we investigated how information about potential vulnerabilities and risk scores influences their behavior. Next, we proposed a model that utilizes attributes from the National Vulnerability Database (NVD) to effectively assess the overall risk score associated with delaying software updates. Next, we conducted a user study with Windows OS users, showing that providing a risk score for not updating their systems and information about vulnerabilities significantly increased users' willingness to update their systems. Additionally, we examined the influence of demographic factors, gender, on users' decision-making regarding software updates. Our results show no statistically significant difference in male and female users' responses in terms of concerns about securing their systems. The implications of this study are relevant for software developers and manufacturers as they can use this information to design more effective software update notification messages. The communication of the potential risks and their corresponding risk scores may motivate users to take action and update their systems in a timely manner, which can ultimately improve the overall security of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06262v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jisa.2025.104152</arxiv:DOI>
      <arxiv:journal_reference>Journal of Information Security and Applications, Volume 93, September 2025, 104152, ISSN 2214-2126</arxiv:journal_reference>
      <dc:creator>Mahzabin Tamanna, Mohd Anwar, Joseph D. W. Stephens</dc:creator>
    </item>
    <item>
      <title>Efficient Function Orchestration for Large Language Models</title>
      <link>https://arxiv.org/abs/2504.14872</link>
      <description>arXiv:2504.14872v2 Announce Type: replace 
Abstract: Function calling is a fundamental capability of today's large language models, but sequential function calling posed efficiency problems. Recent studies have proposed to request function calls with parallelism support in order to alleviate this issue. However, they either delegate the concurrent function calls to users for execution which are conversely executed sequentially, or overlook the relations among various function calls, rending limited efficiency. This paper introduces LLMOrch, an advanced framework for automated, parallel function calling in large language models. The key principle behind LLMOrch is to identify an available processor to execute a function call while preventing any single processor from becoming overburdened. To this end, LLMOrch models the data relations (i.e., def-use) among different function calls and coordinates their executions by their control relations (i.e., mutual-exclusion) as well as the working status of the underlying processors. When comparing with state-of-the-art techniques, LLMOrch demonstrated comparable efficiency improvements in orchestrating I/O-intensive functions, while significantly outperforming (2$\times$) them with compute-intensive functions. LLMOrch's performance even showed a linear correlation to the number of allocated processors. We believe that these results highlight the potential of LLMOrch as an efficient solution for parallel function orchestration in the context of large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14872v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxia Liu, Peng Di, Cong Li, Jun Sun, Jingyi Wang</dc:creator>
    </item>
    <item>
      <title>10 quick tips for making your software outlive your job</title>
      <link>https://arxiv.org/abs/2505.06484</link>
      <description>arXiv:2505.06484v2 Announce Type: replace 
Abstract: Loss of key personnel has always been a risk for research software projects. Key members of the team may have to step away due to illness or burnout, to care for a family member, from a loss of financial support, or because their career is going in a new direction. Today, though, political and financial changes are putting large numbers of researchers out of work simultaneously, potentially leaving large amounts of research software abandoned. This article presents ten tips to help researchers ensure that the software they have built will continue to be usable after they have left their present job -- whether in the course of voluntary career moves or researcher mobility, but particularly in cases of involuntary departure due to political or institutional changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06484v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Richard Littauer, Greg Wilson, Jan Ainali, Eman Abdullah AlOmar, Sylwester Arabas, Yanina Bellini Saibene, Kris Bubendorfer, Kaylea Champion, Clare Dillon, Jouni Helske, Pieter Huybrechts, Daniel S. Katz, Chang Liao, David Lippert, Fang Liu, Pierre Marshall, Daniel R. McCloy, Ian McInerney, Mohamed Wiem Mkaouer, Priyanka Ojha, Christoph Treude, Ethan P. White</dc:creator>
    </item>
    <item>
      <title>LLM-Guided Scenario-based GUI Testing</title>
      <link>https://arxiv.org/abs/2506.05079</link>
      <description>arXiv:2506.05079v2 Announce Type: replace 
Abstract: The assurance of mobile app GUIs has become increasingly important, as the GUI serves as the primary medium of interaction between users and apps. Although numerous automated GUI testing approaches have been developed with diverse strategies, a substantial gap remains between these approaches and the underlying app business logic. Most existing approaches focus on general exploration rather than the completion of specific testing scenarios, often missing critical functionalities. Inspired by manual testing, which treats business logic-driven scenarios as the fundamental unit of testing, this paper introduces an approach that leverages large language models to comprehend GUI semantics and contextual relevance to given scenarios. Building on this capability, we propose ScenGen, an LLM-guided scenario-based GUI testing framework employing multi-agent collaboration to simulate and automate manual testing phases.
  Specifically, ScenGen integrates five agents: the Observer, Decider, Executor, Supervisor, and Recorder. The Observer perceives the app GUI state by extracting and structuring GUI widgets and layouts, interpreting semantic information. This is passed to the Decider, which makes scenario-driven decisions with LLM guidance to identify target widgets and determine actions toward fulfilling specific goals. The Executor performs these operations, while the Supervisor verifies alignment with intended scenario completion, ensuring traceability and consistency. Finally, the Recorder logs GUI operations into context memory as a knowledge base for subsequent decision-making and monitors runtime bugs. Comprehensive evaluations demonstrate that ScenGen effectively generates scenario-based GUI tests guided by LLM collaboration, achieving higher relevance to business logic and improving the completeness of automated GUI testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05079v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengcheng Yu, Yuchen Ling, Chunrong Fang, Quan Zhou, Chunyang Chen, Shaomin Zhu, Zhenyu Chen</dc:creator>
    </item>
    <item>
      <title>OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents</title>
      <link>https://arxiv.org/abs/2506.14866</link>
      <description>arXiv:2506.14866v2 Announce Type: replace 
Abstract: Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14866v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko</dc:creator>
    </item>
    <item>
      <title>Qualitative Analysis of the Teacher and Student Roles in Pair Programming</title>
      <link>https://arxiv.org/abs/2507.10305</link>
      <description>arXiv:2507.10305v2 Announce Type: replace 
Abstract: Background: Pair programming is a well-established and versatile agile practice. Previous research has found it to involve far more different roles than the well-known Driver and Observer/Navigator roles. Pair programming often involves heavy knowledge transfer from mainly one partner to the other. Objective: Understand how to fill the ensuing Teacher and Student roles well (positive behavioral patterns). Understand how they may break (anti-patterns). Method: Open coding and axial coding of 17 recorded pair programming sessions with 18 developers from 5 German software companies, plus interviews with 6 different developers from 4 other German companies. Results: We describe six facets of effective Teacher behavior (e.g. Prioritizing Knowledge Transfer) and two facets of effective Student behavior (e.g. Expressing Knowledge Wants). We describe four harmful would-be-Teacher behaviors (e.g. Pushing Unwanted Knowledge), and one harmful would-be-Student behavior (Failing to Provide a Back Channel). Conclusions: The role facets can serve as learning goals and to-do list for developers who want to develop strong pair programming skill. The anti-patterns can serve as warnings for one's own general behavior and as triggers for immediate meta-discussion if they occur within a pairing session.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10305v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linus Ververs, Trang Linh Lam, Lutz Prechelt</dc:creator>
    </item>
    <item>
      <title>Predicting Abandonment of Open Source Software Projects with An Integrated Feature Framework</title>
      <link>https://arxiv.org/abs/2507.21678</link>
      <description>arXiv:2507.21678v2 Announce Type: replace 
Abstract: Open Source Software (OSS) is a cornerstone of contemporary software development, yet the increasing prevalence of OSS project abandonment threatens global software supply chains. Although previous research has explored abandonment prediction methods, these methods often demonstrate unsatisfactory predictive performance, further plagued by imprecise abandonment discrimination, limited interpretability, and a lack of large, generalizable datasets. In this work, we address these challenges by reliably detecting OSS project abandonment through a dual approach: explicit archival status and rigorous semantic analysis of project documentation or description. Leveraging a precise and scalable labeling pipeline, we curate a comprehensive longitudinal dataset of 115,466 GitHub repositories, encompassing 57,733 confirmed abandonment repositories, enriched with detailed, timeline-based behavioral features. Building on this foundation, we introduce an integrated, multi-perspective feature framework for abandonment prediction, capturing user-centric, maintainer-centric, and project evolution features. Survival analysis using an AFT model yields a high C-index of 0.846, substantially outperforming models confined to surface features. Further, feature ablation and SHAP analyses confirm both the predictive power and interpretability of our approach. We further demonstrate practical deployment of a GBSA classifier for package risk in openEuler. By unifying precise labeling, multi-perspective features, and interpretable modeling, our work provides reproducible, scalable, and practitioner-oriented support for understanding and managing abandonment risk in large OSS ecosystems. Our tool not only predicts abandonment but also enhances program comprehension by providing actionable insights into the health and sustainability of OSS projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21678v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiming Xu, Runzhi He, Hengzhi Ye, Minghui Zhou, Huaimin Wang</dc:creator>
    </item>
    <item>
      <title>RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots</title>
      <link>https://arxiv.org/abs/2507.22664</link>
      <description>arXiv:2507.22664v2 Announce Type: replace 
Abstract: The presence of autonomous systems is growing at a fast pace and it is impacting many aspects of our lives. Designed to learn and act independently, these systems operate and perform decision-making without human intervention. However, they lack the ability to incorporate users' ethical preferences, which are unique for each individual in society and are required to personalize the decision-making processes. This reduces user trust and prevents autonomous systems from behaving according to the moral beliefs of their end-users. When multiple systems interact with differing ethical preferences, they must negotiate to reach an agreement that satisfies the ethical beliefs of all the parties involved and adjust their behavior consequently. To address this challenge, this paper proposes RobEthiChor, an approach that enables autonomous systems to incorporate user ethical preferences and contextual factors into their decision-making through ethics-based negotiation. RobEthiChor features a domain-agnostic reference architecture for designing autonomous systems capable of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an implementation of RobEthiChor within the Robot Operating System (ROS), which can be deployed on robots to provide them with ethics-based negotiation capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real robots and ran scenarios where a pair of robots negotiate upon resource contention. Experimental results demonstrate the feasibility and effectiveness of the system in realizing ethics-based negotiation. RobEthiChor allowed robots to reach an agreement in more than 73% of the scenarios with an acceptable negotiation time (0.67s on average). Experiments also demonstrate that the negotiation approach implemented in RobEthiChor is scalable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22664v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mashal Afzal Memon, Gianluca Filippone, Gian Luca Scoccia, Marco Autili, Paola Inverardi</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice</title>
      <link>https://arxiv.org/abs/2510.17894</link>
      <description>arXiv:2510.17894v2 Announce Type: replace 
Abstract: The ability to comprehend code has long been recognized as an essential skill in software engineering. As programmers lean more heavily on generative artificial intelligence (GenAI) assistants to develop code solutions, it is becoming increasingly important for programmers to comprehend GenAI solutions so that they can verify their appropriateness and properly integrate them into existing code. At the same time, GenAI tools are increasingly being enlisted to provide programmers with tailored explanations of code written both by GenAI and humans. Thus, in computing education, GenAI presents new challenges and opportunities for learners who are trying to comprehend computer programs. To provide computing educators with evidence-based guidance on the use of GenAI to facilitate code comprehension and to identify directions for future research, we present a systematic literature review (SLR) of state-of-the-art approaches and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on 31 studies published between 2022 and 2024. Despite their potential, GenAI assistants often yield inaccurate or unclear explanations, and novice programmers frequently struggle to craft effective prompts, thereby impeding their ability to leverage GenAI to aid code comprehension. Our review classifies GenAI-based approaches and tools, identifies methods used to study them, and summarizes the empirical evaluations of their effectiveness. We consider the implications of our findings for computing education research and practice, and identify directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17894v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunhan Qiao, Md Istiak Hossain Shihab, Christopher Hundhausen</dc:creator>
    </item>
    <item>
      <title>BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills</title>
      <link>https://arxiv.org/abs/2510.19898</link>
      <description>arXiv:2510.19898v2 Announce Type: replace 
Abstract: High quality bugs are key to training the next generation of language model based software engineering (SWE) agents. We introduce a novel method for synthetic generation of difficult and diverse bugs. Our method instructs SWE Agents to introduce a feature into the codebase whereby they may unintentionally break tests, resulting in bugs. Prior approaches often induce an out-of-distribution effect by generating bugs intentionally (e.g. by introducing local perturbation to existing code), which does not reflect realistic development processes. We perform qualitative analysis to demonstrate that our approach for generating bugs more closely reflects the patterns found in human-authored edits. Through extensive experiments, we demonstrate that our bugs provide more efficient training data for supervised fine-tuning, outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k bugs). We train on our newly generated bugs in addition to existing bug datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over three seeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19898v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atharv Sonwane, Isadora White, Hyunji Lee, Matheus Pereira, Lucas Caccia, Minseon Kim, Zhengyan Shi, Chinmay Singh, Alessandro Sordoni, Marc-Alexandre C\^ot\'e, Xingdi Yuan</dc:creator>
    </item>
    <item>
      <title>Securing AI Agent Execution</title>
      <link>https://arxiv.org/abs/2510.21236</link>
      <description>arXiv:2510.21236v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21236v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph B\"uhler, Matteo Biagiola, Luca Di Grazia, Guido Salvaneschi</dc:creator>
    </item>
  </channel>
</rss>
