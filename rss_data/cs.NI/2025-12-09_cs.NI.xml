<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 02:44:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AIMNET: An IoT-Empowered Digital Twin for Continuous Gas Emission Monitoring and Early Hazard Detection</title>
      <link>https://arxiv.org/abs/2512.06148</link>
      <description>arXiv:2512.06148v1 Announce Type: new 
Abstract: A Digital Twin (DT) framework to enhance carbon-based gas plume monitoring is critical for supporting timely and effective mitigation responses to environmental hazards such as industrial gas leaks, or wildfire outbreaks carrying large carbon emissions. We present AIMNET, a one-of-a-kind DT framework that integrates a built-in-house Internet of Things (IoT)-based continuous sensing network with a physics-based multi-scale weather-gas transport model, that enables high-resolution and real-time simulation and detection of carbon gas emissions. AIMNET features a three-layer system architecture: (i) physical world: custom-built devices for continuous monitoring; (ii) bidirectional information feedback links: intelligent data transmission and reverse control; and (iii) digital twin world: AI-driven analytics for prediction, anomaly detection, and dynamic weather-gas coupled molecule transport modeling. Designed for scalable, energy-efficient deployment in remote environments, AIMNET architecture is realized through a small-scale distributed sensing network over an oil and gas production basin. To demonstrate the high-resolution, fast-responding concept, an equivalent mobile-based emission monitoring network was deployed around a wastewater treatment plant that constantly emits methane plumes. Our preliminary results through which, have successfully captured the methane emission events whose dynamics have been further resolved by the tiered model simulations. This work supports our position that AIMNET provides a promising DT framework for reliable, real-time monitoring and predictive risk assessment. In the end, we also discuss key implementation challenges and outline future directions for advancing such a new DT framework for translation deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06148v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MIOT.2025.3638271</arxiv:DOI>
      <dc:creator>Zifan Zhou, Xuan Wang, Yang Yan, Lkhanaajav Mijiddorj, Yu Ding, Tyler Beringer, Parisa Masnadi Khiabani, Wolfgang G. Jentner, Xiao-Ming Hu, Chenghao Wang, Bryan M. Carroll, Ming Xue, David Ebert, Bin Li, Binbin Weng</dc:creator>
    </item>
    <item>
      <title>Programmable and GPU-Accelerated Edge Inference for Real-Time ISAC on NVIDIA ARC-OTA</title>
      <link>https://arxiv.org/abs/2512.06493</link>
      <description>arXiv:2512.06493v1 Announce Type: new 
Abstract: The transition of cellular networks to (i) software-based systems on commodity hardware and (ii) platforms for services beyond connectivity introduces critical system-level challenges. As sensing emerges as a key feature toward 6G standardization, supporting Integrated Sensing and Communication (ISAC) with limited bandwidth and piggybacking on communication signals, while maintaining high reliability and performance, remains a fundamental challenge. In this paper, we provide two key contributions. First, we present a programmable, plug-and-play framework for processing PHY/MAC signals through real-time, GPU-accelerated Artificial Intelligence (AI) applications on the edge Radio Access Network (RAN) infrastructure. Building on the Open RAN dApp architecture, the framework interfaces with a GPU-accelerated gNB based on NVIDIA ARC-OTA, feeding PHY/MAC data to custom AI logic with latency under 0.5 ms for complex channel state information extraction. Second, we demonstrate the framework's capabilities through cuSense, an indoor localization dApp that consumes uplink DMRS channel estimates, removes static multipath components, and runs a neural network to infer the position of a moving person. Evaluated on a 3GPP-compliant 5G NR deployment, cuSense achieves a mean localization error of 77 cm, with 75% of predictions falling within 1 meter. This is without dedicated sensing hardware or modifications to the RAN stack or signals. We plan to release both the framework and cuSense pipelines as open source, providing a reference design for future AI-native RANs and ISAC applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06493v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Villa, Mauro Belgiovine, Nicholas Hedberg, Michele Polese, Chris Dick, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>AQUILA: A QUIC-Based Link Architecture for Resilient Long-Range UAV Communication</title>
      <link>https://arxiv.org/abs/2512.06889</link>
      <description>arXiv:2512.06889v1 Announce Type: new 
Abstract: The proliferation of autonomous Unmanned Aerial Vehicles (UAVs) in Beyond Visual Line of Sight (BVLOS) applications is critically dependent on resilient, high-bandwidth, and low-latency communication links. Existing solutions face critical limitations: TCP's head-of-line blocking stalls time-sensitive data, UDP lacks reliability and congestion control, and cellular networks designed for terrestrial users degrade severely for aerial platforms. This paper introduces AQUILA, a cross-layer communication architecture built on QUIC to address these challenges. AQUILA contributes three key innovations: (1) a unified transport layer using QUIC's reliable streams for MAVLink Command and Control (C2) and unreliable datagrams for video, eliminating head-of-line blocking under unified congestion control; (2) a priority scheduling mechanism that structurally ensures C2 latency remains bounded and independent of video traffic intensity; (3) a UAV-adapted congestion control algorithm extending SCReAM with altitude-adaptive delay targeting and telemetry headroom reservation. AQUILA further implements 0-RTT connection resumption to minimize handover blackouts with application-layer replay protection, deployed over an IP-native architecture enabling global operation. Experimental validation demonstrates that AQUILA significantly outperforms TCP- and UDP-based approaches in C2 latency, video quality, and link resilience under realistic conditions, providing a robust foundation for autonomous BVLOS missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06889v1</guid>
      <category>cs.NI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ximing Huang, Yirui Rao</dc:creator>
    </item>
    <item>
      <title>Hyperflex: A SIMD-based DFA Model for Deep Packet Inspection</title>
      <link>https://arxiv.org/abs/2512.07123</link>
      <description>arXiv:2512.07123v1 Announce Type: new 
Abstract: Deep Packet Inspection (DPI) has been extensively employed for network security. It examines traffic payloads by searching for regular expressions (regex) with the Deterministic Finite Automaton (DFA) model. However, as the network bandwidth and ruleset size are increasing rapidly, the conventional DFA model has emerged as a significant performance bottleneck of DPI. Leveraging the Single-Instruction-Multiple-Data (SIMD) instruction to perform state transitions can substantially boost the efficiency of the DFA model. In this paper, we propose Hyperflex, a novel SIMD-based DFA model designed for high-performance regex matching. Hyperflex incorporates a region detection algorithm to identify regions suitable for acceleration by SIMD instructions across the whole DFA graph. Also, we design a hybrid state transition algorithm that enables state transition in both SIMD-accelerated and normal regions, and ensures seamless state transition across the two types of regions. We have implemented Hyperflex on the commodity CPU and evaluated it with real network traffic and DPI regexes. Our evaluation results indicate that Hyperflex reaches a throughput of 8.89Gbit/s, representing an improvement of up to 2.27 times over Mcclellan, the default DFA model of the prominent multi-pattern regex matching engine Hyperscan. As a result, Hyperflex has been successfully deployed in Hyperscan, significantly enhancing its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07123v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Wenjun Zhu, Harry Chang, Yang Hong, Geoff Langdale, Kun Qiu, Jin Zhao</dc:creator>
    </item>
    <item>
      <title>Implementation of Honeynet and Honeypot in Network Infrastructure in Production Network</title>
      <link>https://arxiv.org/abs/2512.07180</link>
      <description>arXiv:2512.07180v1 Announce Type: new 
Abstract: Network infrastructure in a production environment is increasingly targeted by attackers every day. Many resources and services now rely on the internet, making network infrastructure one of the most critical parts to protect, as it hosts numerous company resources and services. Several solutions have already been proposed to prevent attacks, minimize damage, and divert hackers and intruders. Among these, the honeypot stands out as a highly effective tool; it is designed to mimic both a scanner and an attacker, diverting and misleading them within a simulated, production-level environment. This paper will demonstrate the use of a honeynet where a honeypot acts like a real resource to deceive the attacker and analyze their behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07180v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nawshad Ahmed Evan, Md Raihan Uddin</dc:creator>
    </item>
    <item>
      <title>WaggleNet: A LoRa and MQTT-Based Monitoring System for Internal and External Beehive Conditions</title>
      <link>https://arxiv.org/abs/2512.07408</link>
      <description>arXiv:2512.07408v1 Announce Type: new 
Abstract: Bee populations are declining globally due to habitat loss, pesticide exposure, and climate change, threatening agricultural productivity and food security. While existing smart beehive systems monitor internal conditions, they typically overlook external environmental factors that significantly influence colony health, and are constrained by high cost, limited scalability, and inadequate contextual analysis. We present WaggleNet, a novel dual-scope monitoring system that simultaneously captures both internal hive conditions and external environmental parameters using a cost-effective LoRa-MQTT architecture. Our system deploys modular worker nodes ($\sim$\$15 each) equipped with temperature, humidity, light, and GPS sensors both inside and around beehives. A master node functions as a LoRa-MQTT gateway, forwarding data to a cloud server with a mobile application interface. Field experiments confirmed reliable operation with 100\% packet delivery over 110 meters in line-of-sight conditions and 95 meters in obstructed environments, including successful deployment inside wooden hive structures. Our system demonstrated stable end-to-end latency under 5 seconds and continuous operation over a two-month period across diverse environmental conditions. By bridging the gap between internal and external monitoring, WaggleNet enables contextual anomaly detection and supports data-driven precision beekeeping in resource-constrained settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07408v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minju Jeon, Jiyun Kim, Sewon Kim, Seongmin Park, Bo Zhang, Anthony H. Smith</dc:creator>
    </item>
    <item>
      <title>Service Registration, Indexing, Discovery &amp; Selection; An Architectural Survey Toward a GenAI-Driven Future</title>
      <link>https://arxiv.org/abs/2512.07638</link>
      <description>arXiv:2512.07638v1 Announce Type: new 
Abstract: The emergence of sixth-generation (6G) networks marks a paradigm shift: by unifying an edge-to-cloud computing continuum with ultra-high-performance networking, 6G will enable capabilities far beyond today's boundaries. As use-case diversity grows exponentially and user adoption drives traffic to unprecedented and highly dynamic levels, novel service orchestration mechanisms are indispensable. In this paper, we adopt an architectural viewpoint, examining Service Registration, Indexing, Discovery, and Selection (SRIDS) as fundamental elements of 6G service provision. We first establish the theoretical foundations of SRIDS in 6G by defining its core concepts, detailing its end-to-end workflow, reviewing current standardization efforts, and projecting its future design objectives, including reliability, scalability, automaticity and adaptability, determinism, efficiency, sustainability, semantic-awareness, security, privacy, and trust. We then perform a comprehensive literature review and gap analysis encompassing both existing surveys and recent research efforts, identifying conceptual and methodological gaps that hinder unified SRIDS in 6G. Next, we introduce a taxonomy that classifies SRIDS mechanisms into centralized, distributed, decentralized, and hybrid architectures, and systematically examine the relevant studies within each category. Each work is evaluated against the extracted design objectives. Building on these findings, we propose a hybrid architectural framework, combining centralized data management to ensure consistency and agility with distributed coordination to enhance scalability in emerging 6G use cases. The framework incorporates innovative technologies, such as Generative Artificial Intelligence (GenAI). We conclude by highlighting open challenges and suggesting directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07638v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Farhoudi, Masoud Shokrnezhad, Tarik Taleb</dc:creator>
    </item>
    <item>
      <title>Multi-Generator Continual Learning for Robust Delay Prediction in 6G</title>
      <link>https://arxiv.org/abs/2512.07726</link>
      <description>arXiv:2512.07726v1 Announce Type: new 
Abstract: In future 6G networks, dependable networks will enable telecommunication services such as remote control of robots or vehicles with strict requirements on end-to-end network performance in terms of delay, delay variation, tail distributions, and throughput. With respect to such networks, it is paramount to be able to determine what performance level the network segment can guarantee at a given point in time. One promising approach is to use predictive models trained using machine learning (ML). Predicting performance metrics such as one-way delay (OWD), in a timely manner, provides valuable insights for the network, user equipments (UEs), and applications to address performance trends, deviations, and violations. Over the course of time, a dynamic network environment results in distributional shifts, which causes catastrophic forgetting and drop of ML model performance. In continual learning (CL), the model aims to achieve a balance between stability and plasticity, enabling new information to be learned while preserving previously learned knowledge. In this paper, we target on the challenges of catastrophic forgetting of OWD prediction model. We propose a novel approach which introducing the concept of multi-generator for the state-of-the-art CL generative replay framework, along with tabular variational autoencoders (TVAE) as generators. The domain knowledge of UE capabilities is incorporated into the learning process for determining generator setup and relevance. The proposed approach is evaluated across a diverse set of scenarios with data that is collected in a realistic 5G testbed, demonstrating its outstanding performance in comparison to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07726v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Lan, Jalil Taghia, Hannes Larsson, Andreas Johnsson</dc:creator>
    </item>
    <item>
      <title>Deadline-Aware Scheduling of Distributed Quantum Circuits in Near-Term Quantum Cloud</title>
      <link>https://arxiv.org/abs/2512.06157</link>
      <description>arXiv:2512.06157v1 Announce Type: cross 
Abstract: Distributed quantum computing (DQC) enables scalable quantum computations by distributing large quantum circuits on multiple quantum processing units (QPUs) in the quantum cloud. In DQC, after partitioning quantum circuits, they must be scheduled and executed on heterogenous QPUs while balancing latency, overhead, QPU communication resource limits. However, since fully functioning quantum communication networks have not been realized yet, near-term quantum clouds will only rely on local operations and classical communication settings between QPUs, without entangled quantum links. Additionally, existing DQC scheduling frameworks do not account for user-defined execution deadlines and adopt inefficient wire cutting techniques. Accordingly, in this work, a deadline aware DQC scheduling framework with efficient wire cutting for near-term quantum cloud is proposed. The proposed framework schedules partitioned quantum subcircuits while accounting for circuit deadlines and QPU capacity limits. It also captures dependencies between partitioned subcircuits and distributes the execution of the sampling shots on different QPUs to have efficient wire cutting and faster execution. In this regard, a deadline-aware circuit scheduling optimization problem is formulated, and solved using simulated annealing. Simulation results show a marked improvement over existing shot-agnostic frameworks under urgent deadlines, reaching a 12.8% increase in requests served before their deadlines. Additionally, the proposed framework serves 8.16% more requests, on average, compared to state-of-the-art dependency-agnostic baseline frameworks, and by 9.60% versus the dependency-and-shot-agnostic baseline, all while achieving a smaller makespan of the DQC execution. Moreover, the proposed framework serves 23.7%, 24.5%, and 25.38% more requests compared to greedy, list scheduling, and random schedulers, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06157v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nour Dehaini, Christia Chahoud, Mahdi Chehimi</dc:creator>
    </item>
    <item>
      <title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title>
      <link>https://arxiv.org/abs/2512.06390</link>
      <description>arXiv:2512.06390v1 Announce Type: cross 
Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06390v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/APWiMob67231.2025.11269122</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob), Bali, Indonesia, 2025, pp. 180-186</arxiv:journal_reference>
      <dc:creator>Mehrab Hosain, Sabbir Alom Shuvo, Matthew Ogbe, Md Shah Jalal Mazumder, Yead Rahman, Md Azizul Hakim, Anukul Pandey</dc:creator>
    </item>
    <item>
      <title>Managed TLS Under Migration: Authentication Authority Across CDN and Hosting Transitions</title>
      <link>https://arxiv.org/abs/2512.07033</link>
      <description>arXiv:2512.07033v1 Announce Type: cross 
Abstract: Managed TLS has become a common approach for deploying HTTPS, with platforms generating and storing private keys and automating certificate issuance on behalf of domain operators. This model simplifies operational management but shifts control of authentication material from the domain owner to the platform. The implications of this shift during provider transitions remain insufficiently examined. This study investigates how managed TLS platforms behave when a domain is moved away from the platform that originally issued and stored its certificate. A controlled measurement environment was used to monitor multiple platforms after migration. Each platform was observed for the full remaining lifetime of the certificate that had been active during delegation. The measurements show that platforms continue to serve the same certificate until it expires, even after DNS resolvers direct traffic toward new infrastructure. No platform revoked, replaced, or retired the certificate, and no new certificate was issued after delegation ended. Direct connections to the previous platform continued to complete TLS handshakes with the stale certificate, which confirms that authentication capability persisted independently of DNS state. These findings indicate that authentication authority remains with the previous platform for the entire lifetime of certificates issued during the delegation period. The gap between DNS control and control of authentication material introduces a window in which multiple environments can authenticate the same domain. As managed TLS adoption grows, clearer mechanisms for key retirement and certificate invalidation are needed to ensure that the authentication authority follows operational authority during transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07033v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniyal Ganiuly, Nurzhau Bolatbek, Assel Smaiyl</dc:creator>
    </item>
    <item>
      <title>Discrete Time Credit-Based Shaping for Time-Sensitive Applications in 5G/6G Networks</title>
      <link>https://arxiv.org/abs/2505.12091</link>
      <description>arXiv:2505.12091v2 Announce Type: replace 
Abstract: Future wireless networks must deliver deterministic end-to-end delays for workloads such as smart-factory control loops. On Ethernet these guarantees are delivered by the set of tools within IEEE 802.1 time sensitive networking~(TSN) standards. Credit-based shaper (CBS) is one such tool which enforces bounded latency. Directly porting CBS to 5G/6G New Radio (NR) is non-trivial because NR schedules traffic in discrete-time, modulation-dependent resource allocation, whereas CBS assumes a continuous, fixed-rate link. Existing TSN-over-5G translators map Ethernet priorities to 5G quality of service (QoS) identifiers but leave the radio scheduler unchanged, so deterministic delay is lost within the radio access network (RAN). To address this challenge, we propose a novel slot-native approach that adapts CBS to operate natively in discrete NR slots. We first propose a per-slot credit formulation for each user-equipment ({UE}) queue that debits credit by the granted transport block size~(TBS); we call this discrete-time CBS (CBS-DT). Recognizing that debiting the full {TBS} can unduly penalize transmissions that actually use only part of their grant, we then introduce and analyze {CBS} with Partial Usage ({CBS-PU}). {CBS-PU} scales the credit debit in proportion to the actual bytes dequeued from the downlink queue. The resulting CBS-PU algorithm is shown to maintain bounded credit, preserve long-term rate reservations, and guarantees worst-case delay performance no worse than {CBS-DT}. Simulation results show that slot-level credit gating--particularly CBS-PU--enables NR to export TSN class QoS while maximizing resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12091v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anudeep Karnam, Kishor C. Joshi, Jobish John, George Exarchakos, Sonia Heemstra de Groot, Ignas Niemegeers</dc:creator>
    </item>
    <item>
      <title>A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation</title>
      <link>https://arxiv.org/abs/2508.01047</link>
      <description>arXiv:2508.01047v2 Announce Type: replace 
Abstract: This paper presents a novel TCP congestion control algorithm based on Deep Reinforcement Learning. The proposed approach utilizes Deep Q-Networks to optimize the congestion window (cWnd) by observing key network parameters and taking real-time actions. The algorithm is trained and evaluated within the NS-3 network simulator using the OpenGym interface. The results demonstrate significant improvements over traditional TCP New Reno in terms of latency and throughput, with better adaptability to changing network conditions. This study emphasizes the potential of reinforcement learning techniques for solving complex congestion control problems in modern networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01047v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Efe A\u{g}lamazlar, Emirhan Eken, Harun Batur Ge\c{c}ici</dc:creator>
    </item>
    <item>
      <title>MuMeNet: A Network Simulator for Musical Metaverse Communications</title>
      <link>https://arxiv.org/abs/2512.05201</link>
      <description>arXiv:2512.05201v2 Announce Type: replace 
Abstract: The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using "live audience interaction in a virtual concert" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05201v2</guid>
      <category>cs.NI</category>
      <category>cs.SD</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Al Housseini, Jaime Llorca, Luca Turchet, Tiziano Leidi, Cristina Rottondi, Omran Ayoub</dc:creator>
    </item>
    <item>
      <title>DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices</title>
      <link>https://arxiv.org/abs/2502.14011</link>
      <description>arXiv:2502.14011v2 Announce Type: replace-cross 
Abstract: The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. While extensions of the Very Fast Decision Tree (VFDT) remain state-of-the-art for tabular stream mining, their unregulated growth limits efficiency, particularly in ensemble settings where post-pruning at the individual tree level is seldom applied. This paper presents DFDT, a novel memory-constrained algorithm for online learning. DFDT employs activity-aware pre-pruning, dynamically adjusting splitting criteria based on leaf node activity: low-activity nodes are deactivated to conserve resources, moderately active nodes split under stricter conditions, and highly active nodes leverage a skipping mechanism for accelerated growth. Additionally, adaptive grace periods and tie thresholds allow DFDT to modulate splitting decisions based on observed data variability, enhancing the accuracy-memory-runtime trade-off while minimizing the need for hyperparameter tuning. An ablation study reveals three DFDT variants suited to different resource profiles. Fully compatible with existing ensemble frameworks, DFDT provides a drop-in alternative to standard VFDT-based learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14011v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afonso Louren\c{c}o, Jo\~ao Rodrigo, Jo\~ao Gama, Goreti Marreiros</dc:creator>
    </item>
    <item>
      <title>Optimizing Age of Information in Networks with Large and Small Updates</title>
      <link>https://arxiv.org/abs/2503.23658</link>
      <description>arXiv:2503.23658v2 Announce Type: replace-cross 
Abstract: Modern sensing and monitoring applications typically consist of sources transmitting updates of different sizes, ranging from a few bytes (position, temperature, etc.) to multiple megabytes (images, video frames, LIDAR point scans, etc.). Existing approaches to wireless scheduling for information freshness typically ignore this mix of large and small updates, leading to suboptimal performance. In this paper, we consider a single-hop wireless broadcast network with sources transmitting updates of different sizes to a base station over unreliable links. Some sources send large updates spanning many time slots while others send small updates spanning only a few time slots. Due to medium access constraints, only one source can transmit to the base station at any given time, thus requiring careful design of scheduling policies that takes the sizes of updates into account. First, we derive a lower bound on the achievable Age of Information (AoI) by any transmission scheduling policy. Second, we develop optimal randomized policies that consider both switching and no-switching during the transmission of large updates. Third, we introduce a novel Lyapunov function and associated analysis to propose an AoI-based Max-Weight policy that has provable constant factor optimality guarantees. Finally, we evaluate and compare the performance of our proposed scheduling policies through simulations, which show that our Max-Weight policy achieves near-optimal AoI performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23658v2</guid>
      <category>eess.SY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoyi Zhao, Vishrant Tripathi, Igor Kadota</dc:creator>
    </item>
  </channel>
</rss>
