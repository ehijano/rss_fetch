<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2024 01:34:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Surrogate Endpoint Based Provisional Approval Causal Roadmap</title>
      <link>https://arxiv.org/abs/2407.06350</link>
      <description>arXiv:2407.06350v1 Announce Type: new 
Abstract: For many rare diseases with no approved preventive interventions, promising interventions exist, yet it has been difficult to conduct a pivotal phase 3 trial that could provide direct evidence demonstrating a beneficial effect on the target disease outcome. When a promising putative surrogate endpoint(s) for the target outcome is available, surrogate-based provisional approval of an intervention may be pursued. We apply the Causal Roadmap rubric to define a surrogate endpoint based provisional approval causal roadmap, which combines observational study data that estimates the relationship between the putative surrogate and the target outcome, with a phase 3 surrogate endpoint study that collects the same data but is very under-powered to assess the treatment effect (TE) on the target outcome. The objective is conservative estimation/inference for the TE with an estimated lower uncertainty bound that allows (through two bias functions) for an imperfect surrogate and imperfect transport of the conditional target outcome risk in the untreated between the observational and phase 3 studies. Two estimators of TE (plug-in, nonparametric efficient one-step) with corresponding inference procedures are developed. Finite-sample performance of the plug-in estimator is evaluated in two simulation studies, with R code provided. The roadmap is illustrated with contemporary Group B Streptococcus vaccine development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06350v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter B. Gilbert, James Peng, Larry Han, Theis Lange, Yun Lu, Lei Nie, Mei-Chiung Shih, Salina P. Waddy, Ken Wiley, Margot Yann, Zafar Zafari, Debashis Ghosh, Dean Follmann, Michal Juraska, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Logit unfolding choice models for binary data</title>
      <link>https://arxiv.org/abs/2407.06395</link>
      <description>arXiv:2407.06395v1 Announce Type: new 
Abstract: Discrete choice models with non-monotonic response functions are important in many areas of application, especially political sciences and marketing. This paper describes a novel unfolding model for binary data that allows for heavy-tailed shocks to the underlying utilities. One of our key contributions is a Markov chain Monte Carlo algorithm that requires little or no parameter tuning, fully explores the support of the posterior distribution, and can be used to fit various extensions of our core model that involve (Bayesian) hypothesis testing on the latent construct. Our empirical evaluations of the model and the associated algorithm suggest that they provide better complexity-adjusted fit to voting data from the United States House of Representatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06395v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rayleigh Lei, Abel Rodriguez</dc:creator>
    </item>
    <item>
      <title>Increased risk of type I errors for detecting heterogeneity of treatment effects in cluster-randomized trials using mixed-effect models</title>
      <link>https://arxiv.org/abs/2407.06466</link>
      <description>arXiv:2407.06466v1 Announce Type: new 
Abstract: Evaluating heterogeneity of treatment effects (HTE) across subgroups is common in both randomized trials and observational studies. Although several statistical challenges of HTE analyses including low statistical power and multiple comparisons are widely acknowledged, issues arising for clustered data, including cluster randomized trials (CRTs), have received less attention. Notably, the potential for model misspecification is increased given the complex clustering structure (e.g., due to correlation among individuals within a subgroup and cluster), which could impact inference and type 1 errors. To illicit this issue, we conducted a simulation study to evaluate the performance of common analytic approaches for testing the presence of HTE for continuous, binary, and count outcomes: generalized linear mixed models (GLMM) and generalized estimating equations (GEE) including interaction terms between treatment group and subgroup. We found that standard GLMM analyses that assume a common correlation of participants within clusters can lead to severely elevated type 1 error rates of up to 47.2% compared to the 5% nominal level if the within-cluster correlation varies across subgroups. A flexible GLMM, which allows subgroup-specific within-cluster correlations, achieved the nominal type 1 error rate, as did GEE (though rates were slightly elevated even with as many as 50 clusters). Applying the methods to a real-world CRT using the count outcome utilization of healthcare, we found a large impact of the model specification on inference: the standard GLMM yielded highly significant interaction by sex (P=0.01), whereas the interaction was non-statistically significant under the flexible GLMM and GEE (P=0.64 and 0.93, respectively). We recommend that HTE analyses using GLMM account for within-subgroup correlation to avoid anti-conservative inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06466v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noorie Hyun, Abisola E. Idu, Andrea J. Cook, Jennifer F. Bobb</dc:creator>
    </item>
    <item>
      <title>Bayesian design for mathematical models of fruit growth based on misspecified prior information</title>
      <link>https://arxiv.org/abs/2407.06497</link>
      <description>arXiv:2407.06497v1 Announce Type: new 
Abstract: Bayesian design can be used for efficient data collection over time when the process can be described by the solution to an ordinary differential equation (ODE). Typically, Bayesian designs in such settings are obtained by maximising the expected value of a utility function that is derived from the joint probability distribution of the parameters and the response, given prior information about an appropriate ODE. However, in practice, appropriately defining such information \textit{a priori} can be difficult due to incomplete knowledge about the mechanisms that govern how the process evolves over time. In this paper, we propose a method for finding Bayesian designs based on a flexible class of ODEs. Specifically, we consider the inclusion of spline terms into ODEs to provide flexibility in modelling how the process changes over time. We then propose to leverage this flexibility to form designs that are efficient even when the prior information is misspecified. Our approach is motivated by a sampling problem in agriculture where the goal is to provide a better understanding of fruit growth where prior information is based on studies conducted overseas, and therefore is potentially misspecified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06497v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nushrath Najimuddin, David J. Warne, Helen Thompson, James M. McGree</dc:creator>
    </item>
    <item>
      <title>Independent Approximates provide a maximum likelihood estimate for heavy-tailed distributions</title>
      <link>https://arxiv.org/abs/2407.06522</link>
      <description>arXiv:2407.06522v1 Announce Type: new 
Abstract: Heavy-tailed distributions are infamously difficult to estimate because their moments tend to infinity as the shape of the tail decay increases. Nevertheless, this study shows the utilization of a modified group of moments for estimating a heavy-tailed distribution. These modified moments are determined from powers of the original distribution. The nth-power distribution is guaranteed to have finite moments up to n-1. Samples from the nth-power distribution are drawn from n-tuple Independent Approximates, which are the set of independent samples grouped into n-tuples and sub-selected to be approximately equal to each other. We show that Independent Approximates are a maximum likelihood estimator for the generalized Pareto and the Student's t distributions, which are members of the family of coupled exponential distributions. We use the first (original), second, and third power distributions to estimate their zeroth (geometric mean), first, and second power-moments respectively. In turn, these power-moments are used to estimate the scale and shape of the distributions. A least absolute deviation criteria is used to select the optimal set of Independent Approximates. Estimates using higher powers and moments are possible though the number of n-tuples that are approximately equal may be limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06522v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amenah AL-Najafi, Ugur Tirnakli, Kenric P. Nelson</dc:creator>
    </item>
    <item>
      <title>A flexible model for Record Linkage</title>
      <link>https://arxiv.org/abs/2407.06835</link>
      <description>arXiv:2407.06835v1 Announce Type: new 
Abstract: Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06835v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kayan\'e Robach, St\'ephanie van der Pas, Mark van de Wiel, Michel H. Hof</dc:creator>
    </item>
    <item>
      <title>Distributionally robust risk evaluation with an isotonic constraint</title>
      <link>https://arxiv.org/abs/2407.06867</link>
      <description>arXiv:2407.06867v1 Announce Type: new 
Abstract: Statistical learning under distribution shift is challenging when neither prior knowledge nor fully accessible data from the target distribution is available. Distributionally robust learning (DRL) aims to control the worst-case statistical performance within an uncertainty set of candidate distributions, but how to properly specify the set remains challenging. To enable distributional robustness without being overly conservative, in this paper, we propose a shape-constrained approach to DRL, which incorporates prior information about the way in which the unknown target distribution differs from its estimate. More specifically, we assume the unknown density ratio between the target distribution and its estimate is isotonic with respect to some partial order. At the population level, we provide a solution to the shape-constrained optimization problem that does not involve the isotonic constraint. At the sample level, we provide consistency results for an empirical estimator of the target in a range of different settings. Empirical studies on both synthetic and real data examples demonstrate the improved accuracy of the proposed shape-constrained approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06867v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Gui, Rina Foygel Barber, Cong Ma</dc:creator>
    </item>
    <item>
      <title>When Knockoffs fail: diagnosing and fixing non-exchangeability of Knockoffs</title>
      <link>https://arxiv.org/abs/2407.06892</link>
      <description>arXiv:2407.06892v1 Announce Type: new 
Abstract: Knockoffs are a popular statistical framework that addresses the challenging problem of conditional variable selection in high-dimensional settings with statistical control. Such statistical control is essential for the reliability of inference. However, knockoff guarantees rely on an exchangeability assumption that is difficult to test in practice, and there is little discussion in the literature on how to deal with unfulfilled hypotheses. This assumption is related to the ability to generate data similar to the observed data. To maintain reliable inference, we introduce a diagnostic tool based on Classifier Two-Sample Tests. Using simulations and real data, we show that violations of this assumption occur in common settings for classical Knockoffs generators, especially when the data have a strong dependence structure. We show that the diagnostic tool correctly detects such behavior. To fix knockoff generation, we propose a nonparametric, computationally-efficient alternative knockoff construction, which is based on constructing a predictor of each variable based on all others. We show that this approach achieves asymptotic exchangeability with the original variables under standard assumptions on the predictive model. We show empirically that the proposed approach restores error control on simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06892v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexandre Blain, Bertrand Thirion, Julia Linhart, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Effect estimation in the presence of a misclassified binary mediator</title>
      <link>https://arxiv.org/abs/2407.06970</link>
      <description>arXiv:2407.06970v1 Announce Type: new 
Abstract: Mediation analyses allow researchers to quantify the effect of an exposure variable on an outcome variable through a mediator variable. If a binary mediator variable is misclassified, the resulting analysis can be severely biased. Misclassification is especially difficult to deal with when it is differential and when there are no gold standard labels available. Previous work has addressed this problem using a sensitivity analysis framework or by assuming that misclassification rates are known. We leverage a variable related to the misclassification mechanism to recover unbiased parameter estimates without using gold standard labels. The proposed methods require the reasonable assumption that the sum of the sensitivity and specificity is greater than 1. Three correction methods are presented: (1) an ordinary least squares correction for Normal outcome models, (2) a multi-step predictive value weighting method, and (3) a seamless expectation-maximization algorithm. We apply our misclassification correction strategies to investigate the mediating role of gestational hypertension on the association between maternal age and pre-term birth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06970v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kimberly A. Hochstedler Webb, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Aggregate Bayesian Causal Forests: The ABCs of Flexible Causal Inference for Hierarchically Structured Data</title>
      <link>https://arxiv.org/abs/2407.07067</link>
      <description>arXiv:2407.07067v1 Announce Type: new 
Abstract: This paper introduces aggregate Bayesian Causal Forests (aBCF), a new Bayesian model for causal inference using aggregated data. Aggregated data are common in policy evaluations where we observe individuals such as students, but participation in an intervention is determined at a higher level of aggregation, such as schools implementing a curriculum. Interventions often have millions of individuals but far fewer higher-level units, making aggregation computationally attractive. To analyze aggregated data, a model must account for heteroskedasticity and intraclass correlation (ICC). Like Bayesian Causal Forests (BCF), aBCF estimates heterogeneous treatment effects with minimal parametric assumptions, but accounts for these aggregated data features, improving estimation of average and aggregate unit-specific effects.
  After introducing the aBCF model, we demonstrate via simulation that aBCF improves performance for aggregated data over BCF. We anchor our simulation on an evaluation of a large-scale Medicare primary care model. We demonstrate that aBCF produces treatment effect estimates with a lower root mean squared error and narrower uncertainty intervals while achieving the same level of coverage. We show that aBCF is not sensitive to the prior distribution used and that estimation improvements relative to BCF decline as the ICC approaches one. Code is available at https://github.com/mathematica-mpr/bcf-1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07067v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan R. C. Thal, Lauren V. Forrow, Erin R. Lipman, Jennifer E. Starling, Mariel M. Finucane</dc:creator>
    </item>
    <item>
      <title>Conditional Rank-Rank Regression</title>
      <link>https://arxiv.org/abs/2407.06387</link>
      <description>arXiv:2407.06387v1 Announce Type: cross 
Abstract: Rank-rank regressions are widely used in economic research to evaluate phenomena such as intergenerational income persistence or mobility. However, when covariates are incorporated to capture between-group persistence, the resulting coefficients can be difficult to interpret as such. We propose the conditional rank-rank regression, which uses conditional ranks instead of unconditional ranks, to measure average within-group income persistence. This property is analogous to that of the unconditional rank-rank regression that measures the overall income persistence. The difference between conditional and unconditional rank-rank regression coefficients therefore can measure between-group persistence. We develop a flexible estimation approach using distribution regression and establish a theoretical framework for large sample inference. An empirical study on intergenerational income mobility in Switzerland demonstrates the advantages of this approach. The study reveals stronger intergenerational persistence between fathers and sons compared to fathers and daughters, with the within-group persistence explaining 62% of the overall income persistence for sons and 52% for daughters. Families of small size or with highly educated fathers exhibit greater persistence in passing on their economic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06387v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>JANET: Joint Adaptive predictioN-region Estimation for Time-series</title>
      <link>https://arxiv.org/abs/2407.06390</link>
      <description>arXiv:2407.06390v1 Announce Type: cross 
Abstract: Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06390v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eshant English, Eliot Wong-Toi, Matteo Fontana, Stephan Mandt, Padhraic Smyth, Christoph Lippert</dc:creator>
    </item>
    <item>
      <title>LETS-C: Leveraging Language Embedding for Time Series Classification</title>
      <link>https://arxiv.org/abs/2407.06533</link>
      <description>arXiv:2407.06533v1 Announce Type: cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a language embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on well-established time series classification benchmark datasets. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging language encoders to embed time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06533v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachneet Kaur, Zhen Zeng, Tucker Balch, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>Extending the blended generalized extreme value distribution</title>
      <link>https://arxiv.org/abs/2407.06875</link>
      <description>arXiv:2407.06875v1 Announce Type: cross 
Abstract: The generalized extreme value (GEV) distribution is commonly employed to help estimate the likelihood of extreme events in many geophysical and other application areas. The recently proposed blended generalized extreme value (bGEV) distribution modifies the GEV with positive shape parameter to avoid a hard lower bound that complicates fitting and inference. Here, the bGEV is extended to the GEV with negative shape parameter, avoiding a hard upper bound that is unrealistic in many applications. This extended bGEV is shown to improve on the GEV for forecasting future heat extremes based on past data. Software implementing this bGEV and applying it to the example temperature data is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06875v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nir Y. Krakauer</dc:creator>
    </item>
    <item>
      <title>End-To-End Causal Effect Estimation from Unstructured Natural Language Data</title>
      <link>https://arxiv.org/abs/2407.07018</link>
      <description>arXiv:2407.07018v1 Announce Type: cross 
Abstract: Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce NATURAL, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07018v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Dhawan, Leonardo Cotta, Karen Ullrich, Rahul G. Krishnan, Chris J. Maddison</dc:creator>
    </item>
    <item>
      <title>Assumption Smuggling in Intermediate Outcome Tests of Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2407.07072</link>
      <description>arXiv:2407.07072v1 Announce Type: cross 
Abstract: Political scientists are increasingly attuned to the promises and pitfalls of establishing causal effects. But the vital question for many is not if a causal effect exists but why and how it exists. Even so, many researchers avoid causal mediation analyses due to the assumptions required, instead opting to explore causal mechanisms through what we call intermediate outcome tests. These tests use the same research design used to estimate the effect of treatment on the outcome to estimate the effect of the treatment on one or more mediators, with authors often concluding that evidence of the latter is evidence of a causal mechanism. We show in this paper that, without further assumptions, this can neither establish nor rule out the existence of a causal mechanism. Instead, such conclusions about the indirect effect of treatment rely on implicit and usually very strong assumptions that are often unmet. Thus, such causal mechanism tests, though very common in political science, should not be viewed as a free lunch but rather should be used judiciously, and researchers should explicitly state and defend the requisite assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07072v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Ruofan Ma, Aleksei Opacic</dc:creator>
    </item>
    <item>
      <title>Factor modelling for high-dimensional functional time series</title>
      <link>https://arxiv.org/abs/2112.13651</link>
      <description>arXiv:2112.13651v3 Announce Type: replace 
Abstract: Many economic and scientific problems involve the analysis of high-dimensional functional time series, where the number of functional variables $p$ diverges as the number of serially dependent observations $n$ increases. In this paper, we present a novel functional factor model for high-dimensional functional time series that maintains and makes use of the functional and dynamic structure to achieve great dimension reduction and find the latent factor structure. To estimate the number of functional factors and the factor loadings, we propose a fully functional estimation procedure based on an eigenanalysis for a nonnegative definite and symmetric matrix. Our proposal involves a weight matrix to improve the estimation efficiency and tackle the issue of heterogeneity, the rationale of which is illustrated by formulating the estimation from a novel regression perspective. Asymptotic properties of the proposed method are studied when $p$ diverges at some polynomial rate as $n$ increases. To provide a parsimonious model and enhance interpretability for near-zero factor loadings, we impose sparsity assumptions on the factor loading space and then develop a regularized estimation procedure with theoretical guarantees when $p$ grows exponentially fast relative to $n.$ Finally, we demonstrate that our proposed estimators significantly outperform the competing methods through both simulations and applications to a U.K. temperature data set and a Japanese mortality data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13651v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaojun Guo, Xinghao Qiao, Qingsong Wang, Zihan Wang</dc:creator>
    </item>
    <item>
      <title>Sparse high-dimensional linear mixed modeling with a partitioned empirical Bayes ECM algorithm</title>
      <link>https://arxiv.org/abs/2310.12285</link>
      <description>arXiv:2310.12285v2 Announce Type: replace 
Abstract: High-dimensional longitudinal data is increasingly used in a wide range of scientific studies. To properly account for dependence between longitudinal observations, statistical methods for high-dimensional linear mixed models (LMMs) have been developed. However, few packages implementing these high-dimensional LMMs are available in the statistical software R. Additionally, some packages suffer from scalability issues. This work presents an efficient and accurate Bayesian framework for high-dimensional LMMs. We use empirical Bayes estimators of hyperparameters for increased flexibility and an Expectation-Conditional-Minimization (ECM) algorithm for computationally efficient maximum a posteriori probability (MAP) estimation of parameters. The novelty of the approach lies in its partitioning and parameter expansion as well as its fast and scalable computation. We illustrate Linear Mixed Modeling with PaRtitiOned empirical Bayes ECM (LMM-PROBE) in simulation studies evaluating fixed and random effects estimation along with computation time. A real-world example is provided using data from a study of lupus in children, where we identify genes and clinical factors associated with a new lupus biomarker and predict the biomarker over time. Supplementary materials are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12285v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja Zgodic, Ray Bai, Jiajia Zhang, Peter Olejua, Alexander C. McLain</dc:creator>
    </item>
    <item>
      <title>Inference for bivariate extremes via a semi-parametric angular-radial model</title>
      <link>https://arxiv.org/abs/2401.07259</link>
      <description>arXiv:2401.07259v2 Announce Type: replace 
Abstract: The modelling of multivariate extreme events is important in a wide variety of applications, including flood risk analysis, metocean engineering and financial modelling. A wide variety of statistical techniques have been proposed in the literature; however, many such methods are limited in the forms of dependence they can capture, or make strong parametric assumptions about data structures. In this article, we introduce a novel inference framework for multivariate extremes based on a semi-parametric angular-radial model. This model overcomes the limitations of many existing approaches and provides a unified paradigm for assessing joint tail behaviour. Alongside inferential tools, we also introduce techniques for assessing uncertainty and goodness of fit. Our proposed technique is tested on simulated data sets alongside observed metocean time series', with results indicating generally good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07259v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Callum John Rowlandson Murphy-Barltrop, Ed Mackay, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>DGP-LVM: Derivative Gaussian process latent variable models</title>
      <link>https://arxiv.org/abs/2404.04074</link>
      <description>arXiv:2404.04074v2 Announce Type: replace 
Abstract: We develop a framework for derivative Gaussian process latent variable models (DGP-LVM) that can handle multi-dimensional output data using modified derivative covariance functions. The modifications account for complexities in the underlying data generating process such as scaled derivatives, varying information across multiple output dimensions as well as interactions between outputs. Further, our framework provides uncertainty estimates for each latent variable samples using Bayesian inference. Through extensive simulations, we demonstrate that latent variable estimation accuracy can be drastically increased by including derivative information due to our proposed covariance function modifications. The developments are motivated by a concrete biological research problem involving the estimation of the unobserved cellular ordering from single-cell RNA (scRNA) sequencing data for gene expression and its corresponding derivative information known as RNA velocity. Since the RNA velocity is only an estimate of the exact derivative information, the derivative covariance functions need to account for potential scale differences. In a real-world case study, we illustrate the application of DGP-LVMs to such scRNA sequencing data. While motivated by this biological problem, our framework is generally applicable to all kinds of latent variable estimation problems involving derivative information irrespective of the field of study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04074v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Soham Mukherjee, Manfred Claassen, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Estimating Metocean Environments Associated with Extreme Structural Response to Demonstrate the Dangers of Environmental Contour Methods</title>
      <link>https://arxiv.org/abs/2404.16775</link>
      <description>arXiv:2404.16775v3 Announce Type: replace 
Abstract: Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Alternatively, environmental contours provide an approximate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We demonstrate a methodology for efficient fully probabilistic analysis of structural failure. From this, we estimate the joint conditional probability density of the environment (CDE), given the occurrence of an extreme structural response. We use CDE as a diagnostic to highlight the deficiencies of environmental contour methods for design; none of the IFORM environmental contours considered characterise CDE well for three example structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16775v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, David Randell, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Computation with Deep Learning and Conformal prediction</title>
      <link>https://arxiv.org/abs/2406.04874</link>
      <description>arXiv:2406.04874v2 Announce Type: replace 
Abstract: Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these "user-choices". In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).
  Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04874v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meili Baragatti, Bertrand Cloez, David M\'etivier, Isabelle Sanchez</dc:creator>
    </item>
    <item>
      <title>Semiparametric Modeling and Analysis for Longitudinal Network Data</title>
      <link>https://arxiv.org/abs/2308.12227</link>
      <description>arXiv:2308.12227v2 Announce Type: replace-cross 
Abstract: We introduce a semiparametric latent space model for analyzing longitudinal network data. The model consists of a static latent space component and a time-varying node-specific baseline component. We develop a semiparametric efficient score equation for the latent space parameter by adjusting for the baseline nuisance component. Estimation is accomplished through a one-step update estimator and an appropriately penalized maximum likelihood estimator. We derive oracle error bounds for the two estimators and address identifiability concerns from a quotient manifold perspective. Our approach is demonstrated using the New York Citi Bike Dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12227v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinqiu He, Jiajin Sun, Yuang Tian, Zhiliang Ying, Yang Feng</dc:creator>
    </item>
  </channel>
</rss>
