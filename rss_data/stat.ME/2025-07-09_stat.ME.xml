<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jul 2025 01:25:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Density Discontinuity Regression</title>
      <link>https://arxiv.org/abs/2507.05581</link>
      <description>arXiv:2507.05581v1 Announce Type: new 
Abstract: Many policies hinge on a continuous variable exceeding a threshold, prompting strategic behavior by agents to stay on the favorable side. This creates density discontinuities at cutoffs, evident in contexts like taxable income, corporate regulations, and academic grading. Existing methods detect these discontinuities, but systematic approaches to examine how they vary with observable characteristics are lacking. We propose a novel, interpretable Bayesian framework that jointly estimates both the log-density ratio at the cutoff and the local shape of the density, as functions of covariates, within a data-driven window. This formulation yields regression-style estimates of covariate effects on the discontinuity. An adaptive window selection balances bias and variance. Our approach improves upon common methods that target only the log-density ratio around the threshold while ignoring the local density shape. We constrain the density jump to be non-negative, reflecting that agents would not aim to be on the losing side of the threshold. Applied to corporate shareholder voting data, our method identifies substantial variation in strategic behavior, notably stronger discontinuities for proposals facing negative recommendations from Institutional Shareholder Services, larger firms, and firms with lower analyst coverage. Overall, our method provides an interpretable framework to quantify heterogeneous agent responses to threshold-based policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05581v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Surya T Tokdar, Rik Sen, Haoliang Zheng, Shuangjie Zhang</dc:creator>
    </item>
    <item>
      <title>Area-based epigraph and hypograph indices for functional outlier detection</title>
      <link>https://arxiv.org/abs/2507.05701</link>
      <description>arXiv:2507.05701v1 Announce Type: new 
Abstract: Detecting outliers in Functional Data Analysis is challenging because curves can stray from the majority in many different ways. The Modified Epigraph Index (MEI) and Modified Hypograph Index (MHI) rank functions by the fraction of the domain on which one curve lies above or below another. While effective for spotting shape anomalies, their construction limits their ability to flag magnitude outliers. This paper introduces two new metrics, the Area-Based Epigraph Index (ABEI) and Area-Based Hypograph Index (ABHI) that quantify the area between curves, enabling simultaneous sensitivity to both magnitude and shape deviations. Building on these indices, we present EHyOut, a robust procedure that recasts functional outlier detection as a multivariate problem: for every curve, and for its first and second derivatives, we compute ABEI and ABHI and then apply multivariate outlier-detection techniques to the resulting feature vectors. Extensive simulations show that EHyOut remains stable across a wide range of contamination settings and often outperforms established benchmark methods. Moreover, applications to Spanish weather data and United Nations world population data further illustrate the practical utility and meaningfulness of this methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05701v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Belen Pulido, Alba M. Franco-Pereira, Rosa E. Lillo, Fabian Scheipl</dc:creator>
    </item>
    <item>
      <title>Multivariate regression with missing response data for modelling regional DNA methylation QTLs</title>
      <link>https://arxiv.org/abs/2507.05990</link>
      <description>arXiv:2507.05990v1 Announce Type: new 
Abstract: Identifying genetic regulators of DNA methylation (mQTLs) with multivariate models enhances statistical power, but is challenged by missing data from bisulfite sequencing. Standard imputation-based methods can introduce bias, limiting reliable inference. We propose \texttt{missoNet}, a novel convex estimation framework that jointly estimates regression coefficients and the precision matrix from data with missing responses. By using unbiased surrogate estimators, our three-stage procedure avoids imputation while simultaneously performing variable selection and learning the conditional dependence structure among responses. We establish theoretical error bounds, and our simulations demonstrate that \texttt{missoNet} consistently outperforms existing methods in both prediction and sparsity recovery. In a real-world mQTL analysis of the CARTaGENE cohort, \texttt{missoNet} achieved superior predictive accuracy and false-discovery control on a held-out validation set, identifying known and credible novel genetic associations. The method offers a robust, efficient, and theoretically grounded tool for genomic analyses, and is available as an R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05990v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shomoita Alam, Yixiao Zeng, Sasha Bernatsky, Marie Hudson, In\'es Colmegna, David A. Stephens, Celia M. T. Greenwood, Archer Y. Yang</dc:creator>
    </item>
    <item>
      <title>Permutations accelerate Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2507.06037</link>
      <description>arXiv:2507.06037v1 Announce Type: new 
Abstract: Approximate Bayesian Computation (ABC) methods have become essential tools for performing inference when likelihood functions are intractable or computationally prohibitive. However, their scalability remains a major challenge in hierarchical or high-dimensional models. In this paper, we introduce permABC, a new ABC framework designed for settings with both global and local parameters, where observations are grouped into exchangeable compartments.
  Building upon the Sequential Monte Carlo ABC (ABC-SMC) framework, permABC exploits the exchangeability of compartments through permutation-based matching, significantly improving computational efficiency.
  We then develop two further, complementary sequential strategies: Over Sampling, which facilitates early-stage acceptance by temporarily increasing the number of simulated compartments, and Under Matching, which relaxes the acceptance condition by matching only subsets of the data.
  These techniques allow for robust and scalable inference even in high-dimensional regimes. Through synthetic and real-world experiments -- including a hierarchical Susceptible-Infectious-Recover model of the early COVID-19 epidemic across 94 French departments -- we demonstrate the practical gains in accuracy and efficiency achieved by our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06037v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Luciano, Charly Andral, Christian P. Robert, Robin J. Ryder</dc:creator>
    </item>
    <item>
      <title>FDR controlling procedures with dimension reduction and their application to GWAS with linkage disequilibrium score</title>
      <link>https://arxiv.org/abs/2507.06049</link>
      <description>arXiv:2507.06049v1 Announce Type: new 
Abstract: Genome-wide association studies (GWAS) have led to the discovery of numerous single nucleotide polymorphisms (SNPs) associated with various phenotypes and complex diseases. However, the identified genetic variants do not fully explain the heritability of complex traits, known as the missing heritability problem. To address this challenge and accurately control false positives while maximizing true associations, we propose two approaches involving linkage disequilibrium (LD) scores as covariates. We apply principal component analysis (PCA), one of the dimensionality reduction techniques, to control the False Discovery Rate (FDR) in the presence of high-dimensional covariates. This method not only provides a convenient interpretation of how multiple covariates in high dimensions affect the control of FDR but also offers higher statistical power compared to cases where covariates are not used. Furthermore, we aim to investigate how covariates contribute to increasing the statistical power through various simulation experiments, comparing the results with real data examples to derive better interpretations. Using real-world datasets, including GWAS with Body Mass Index (BMI) as the phenotype, we evaluate the performance of our proposed approaches. By incorporating LD scores as covariates in FDR-controlled GWAS analyzes, we demonstrate their effectiveness in selecting informative LD scores and improving the identification of significant SNPs. Our methods alleviate computational burden and enhance interpretability while retaining essential information from LD scores. In general, our study contributes to the advancement of statistical methods in GWAS and provides practical guidance for researchers looking to improve the precision of genetic association analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06049v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dayeon Jung, Yewon Kim, Junyong Park</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell RNA-Seq Data</title>
      <link>https://arxiv.org/abs/2507.06113</link>
      <description>arXiv:2507.06113v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. Subject-level mediators are aggregated from cell-level data to perform mediation analysis assessing causal pathways linking gene expression to clinical outcomes. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06113v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>seMCD: Sequentially implemented Monte Carlo depth computation with statistical guarantees</title>
      <link>https://arxiv.org/abs/2507.06227</link>
      <description>arXiv:2507.06227v1 Announce Type: new 
Abstract: Statistical depth functions provide center-outward orderings in spaces of dimension larger than one, where a natural ordering does not exist. The numerical evaluation of such depth functions can be computationally prohibitive, even for relatively low dimensions. We present a novel sequentially implemented Monte Carlo methodology for the computation of, theoretical and empirical, depth functions and related quantities (seMCD), that outputs an interval, a so-called seMCD-bucket, to which the quantity of interest belongs with a high probability prespecified by the user. For specific classes of depth functions, we adapt algorithms from sequential testing, providing finite-sample guarantees. For depth functions dependent on unknown distributions, we offer asymptotic guarantees using non-parametric statistical methods. In contrast to plain-vanilla Monte Carlo methodology the number of samples required in the algorithm is random but typically much smaller than standard choices suggested in the literature. The seMCD method can be applied to various depth functions, covering multivariate and functional spaces. We demonstrate the efficiency and reliability of our approach through empirical studies, highlighting its applicability in outlier or anomaly detection, classification, and depth region computation. In conclusion, the seMCD-algorithm can achieve accurate depth approximations with few Monte Carlo samples while maintaining rigorous statistical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06227v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Gnettner, Claudia Kirch, Alicia Nieto-Reyes</dc:creator>
    </item>
    <item>
      <title>Large scale study of primary school student performance relative to their LMS activity and socioeconomic demographics using a Bayesian Additive Regression Trees containing random effects</title>
      <link>https://arxiv.org/abs/2507.05262</link>
      <description>arXiv:2507.05262v1 Announce Type: cross 
Abstract: Using data collected on almost every 9-12 years old student in Uruguay, we show how to apply Bayesian Additive Regression Trees (BART) with random effects to study performance association with Learning Managment System (LMS) activity and socioeconomic status. Performance data is joined with LMS activity pattern data. BART is chosen because it is possible to include school-level random effects. The model can be used for early identification of at-risk students, and highlights schools that are successful or need intervention. An interesting finding is that high levels of LMS usage show larger positive effects on performance in low socioeconomic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05262v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia da Silva, Bruno Tancredi, Ignacio Alvarez-Castro</dc:creator>
    </item>
    <item>
      <title>Increasing Systemic Resilience to Socioeconomic Challenges: Modeling the Dynamics of Liquidity Flows and Systemic Risks Using Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2507.05287</link>
      <description>arXiv:2507.05287v1 Announce Type: cross 
Abstract: Modern economic systems face unprecedented socioeconomic challenges, making systemic resilience and effective liquidity flow management essential. Traditional models such as CAPM, VaR, and GARCH often fail to reflect real market fluctuations and extreme events. This study develops and validates an innovative mathematical model based on the Navier-Stokes equations, aimed at the quantitative assessment, forecasting, and simulation of liquidity flows and systemic risks. The model incorporates 13 macroeconomic and financial parameters, including liquidity velocity, market pressure, internal stress, stochastic fluctuations, and risk premiums, all based on real data and formally included in the modified equation. The methodology employs econometric testing, Fourier analysis, stochastic simulation, and AI-based calibration to enable dynamic testing and forecasting. Simulation-based sensitivity analysis evaluates the impact of parameter changes on financial balance. The model is empirically tested using Georgian macroeconomic and financial data from 2010-2024, including GDP, inflation, the Gini index, CDS spreads, and LCR metrics. Results show that the model effectively describes liquidity dynamics, systemic risk, and extreme scenarios, while also offering a robust framework for multifactorial analysis, crisis prediction, and countercyclical policy planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05287v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>math.AP</category>
      <category>q-fin.EC</category>
      <category>q-fin.MF</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.61093/sec.9(2).92-113.2025</arxiv:DOI>
      <arxiv:journal_reference>SocioEconomic Challenges, 9(2), 92-113 (2025)</arxiv:journal_reference>
      <dc:creator>Davit Gondauri</dc:creator>
    </item>
    <item>
      <title>Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift</title>
      <link>https://arxiv.org/abs/2507.05412</link>
      <description>arXiv:2507.05412v1 Announce Type: cross 
Abstract: We consider the problem of learning robust discriminative representations of causally-related latent variables. In addition to observational data, the training dataset also includes interventional data obtained through targeted interventions on some of these latent variables to learn representations robust against the resulting interventional distribution shifts. Existing approaches treat interventional data like observational data, even when the underlying causal model is known, and ignore the independence relations that arise from these interventions. Since these approaches do not fully exploit the causal relational information resulting from interventions, they learn representations that produce large disparities in predictive performance on observational and interventional data, which worsens when the number of interventional training samples is limited. In this paper, (1) we first identify a strong correlation between this performance disparity and adherence of the representations to the independence conditions induced by the interventional causal model. (2) For linear models, we derive sufficient conditions on the proportion of interventional data in the training dataset, for which enforcing interventional independence between representations corresponding to the intervened node and its non-descendants lowers the error on interventional data. Combining these insights, (3) we propose RepLIn, a training algorithm to explicitly enforce this statistical independence during interventions. We demonstrate the utility of RepLIn on a synthetic dataset and on real image and text datasets on facial attribute classification and toxicity detection, respectively. Our experiments show that RepLIn is scalable with the number of nodes in the causal graph and is suitable to improve the robust representations against interventional distribution shifts of both continuous and discrete latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05412v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Sreekumar, Vishnu Naresh Boddeti</dc:creator>
    </item>
    <item>
      <title>Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2507.05511</link>
      <description>arXiv:2507.05511v1 Announce Type: cross 
Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted across a wide range of scientific and industrial applications, the treatment action space can naturally expand, from a binary treatment variable to a structured treatment policy. This policy may include several policy factors such as a continuous treatment intensity variable, or discrete treatment assignments. From first principles, we derive the formulation for incorporating multiple treatment policy variables into the functional forms of individual and average treatment effects. Building on this, we develop a methodology to directly rank subjects using aggregated HTE functions. In particular, we construct a Neural-Augmented Naive Bayes layer within a deep learning framework to incorporate an arbitrary number of factors that satisfies the Naive Bayes assumption. The factored layer is then applied with continuous treatment variables, treatment assignment, and direct ranking of aggregated treatment effect functions. Together, these algorithms build towards a generic framework for deep learning of heterogeneous treatment policies, and we show their power to improve performance with public datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05511v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jennifer Y. Zhang, Shuyang Du, Will Y. Zou</dc:creator>
    </item>
    <item>
      <title>Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning</title>
      <link>https://arxiv.org/abs/2507.05526</link>
      <description>arXiv:2507.05526v1 Announce Type: cross 
Abstract: In scientific domains -- from biology to the social sciences -- many questions boil down to \textit{What effect will we observe if we intervene on a particular variable?} If the causal relationships (e.g.~a causal graph) are known, it is possible to estimate the intervention distributions. In the absence of this domain knowledge, the causal structure must be discovered from the available observational data. However, observational data are often compatible with multiple causal graphs, making methods that commit to a single structure prone to overconfidence. A principled way to manage this structural uncertainty is via Bayesian inference, which averages over a posterior distribution on possible causal structures and functional mechanisms. Unfortunately, the number of causal structures grows super-exponentially with the number of nodes in the graph, making computations intractable. We propose to circumvent these challenges by using meta-learning to create an end-to-end model: the Model-Averaged Causal Estimation Transformer Neural Process (MACE-TNP). The model is trained to predict the Bayesian model-averaged interventional posterior distribution, and its end-to-end nature bypasses the need for expensive calculations. Empirically, we demonstrate that MACE-TNP outperforms strong Bayesian baselines. Our work establishes meta-learning as a flexible and scalable paradigm for approximating complex Bayesian causal inference, that can be scaled to increasingly challenging settings in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05526v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anish Dhir, Cristiana Diaconu, Valentinian Mihai Lungu, James Requeima, Richard E. Turner, Mark van der Wilk</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Longitudinal Data under Unknown Interference</title>
      <link>https://arxiv.org/abs/2106.15074</link>
      <description>arXiv:2106.15074v4 Announce Type: replace 
Abstract: In longitudinal studies where units are embedded in space or a social network, interference may arise, meaning that a unit's outcome can depend on treatment histories of others. The presence of interference poses significant challenges for causal inference, particularly when the interference structure -- how a unit's outcome responds to others' influences -- is complex, heterogeneous, and unknown to researchers. This paper develops a general framework for identifying and estimating both direct and spillover effects of treatment histories under minimal assumptions about the interference structure. We define a class of policy-relevant causal estimands and show that they can be represented by a modified marginal structural model (MSM). Under the standard assumption of sequential exchangeability, these estimands are identifiable and can be estimated using inverse probability weighting (IPW). We derive conditions for consistency and asymptotic normality of the estimators and provide procedures for constructing Wald-type confidence intervals with valid coverage in large samples. The method's utility is demonstrated through applications in both social science and biomedical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15074v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Michael Jetsupphasuk</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic BART for Rank-Order Data</title>
      <link>https://arxiv.org/abs/2308.10231</link>
      <description>arXiv:2308.10231v4 Announce Type: replace 
Abstract: Ranking lists are often provided at regular time intervals in a range of applications, including economics, sports, marketing, and politics. Most popular methods for rank-order data postulate a linear specification for the latent scores, which determine the observed ranks, and ignore the temporal dependence of the ranking lists. To address these issues, novel nonparametric static (ROBART) and autoregressive (ARROBART) models are developed, with latent scores defined as nonlinear Bayesian additive regression tree functions of covariates. To make inferences in the dynamic ARROBART model, closed-form filtering, predictive, and smoothing distributions for the latent time-varying scores are derived. These results are applied in a Gibbs sampler with data augmentation for posterior inference. The proposed methods are shown to outperform existing competitors in simulation studies, static data applications to electoral data, stated preferences for sushi and movies, and dynamic data applications to economic complexity rankings of countries and weekly pollster rankings of NCAA football teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10231v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Eoghan O'Neill, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>A Bayes Factor Framework for Unified Parameter Estimation and Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2403.09350</link>
      <description>arXiv:2403.09350v4 Announce Type: replace 
Abstract: The Bayes factor, the data-based updating factor of the prior to posterior odds of two hypotheses, is a natural measure of statistical evidence for one hypothesis over the other. We show how Bayes factors can also be used for parameter estimation. The key idea is to consider the Bayes factor as a function of the parameter value under the null hypothesis. This `support curve' is inverted to obtain point estimates (`maximum evidence estimates') and interval estimates (`support intervals'), similar to how P-value functions are inverted to obtain point estimates and confidence intervals. This provides data analysts with a unified inference framework as Bayes factors (for any tested parameter value), support intervals (at any level), and point estimates can be easily read off from a plot of the support curve. This approach shares similarities but is also distinct from conventional Bayesian and frequentist approaches: It uses the Bayesian evidence calculus, but without synthesizing data and prior, and it defines statistical evidence in terms of (integrated) likelihood ratios, but also includes a natural way for dealing with nuisance parameters. Applications to meta-analysis, replication studies, and logistic regression illustrate how our framework is of practical value for making quantitative inferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09350v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Defining Dispersion: A Fundamental Order for Univariate Discrete Distributions</title>
      <link>https://arxiv.org/abs/2406.02124</link>
      <description>arXiv:2406.02124v3 Announce Type: replace 
Abstract: The measurement of dispersion is one of the most fundamental and ubiquitous statistical concepts, in both applied and theoretical contexts. For dispersion measures, such as the standard deviation, to effectively capture the variability of a given distribution, they must, by definition, preserve some stochastic order of dispersion. The so-called dispersive order is the most basic order that serves as a foundation underneath the concept of dispersion measures. However, this order is incompatible with almost all discrete distributions, including lattice and most empirical distributions. As a result, popular measures may fail to accurately capture the dispersion of such distributions.
  In this paper, discrete adaptations of the dispersive order are defined and analyzed. They are shown to be a compromise between being equivalent to the original dispersive order on their joint area of applicability and other crucial properties. Moreover, they share many characteristic properties with the dispersive order, validating their role as a foundation for measuring discrete dispersion in a manner closely aligned with the continuous setting. Their behaviour on well-known families of lattice distribution is generally as expected when parameter differences are sufficiently large. Most popular dispersion measures preserve both discrete dispersive orders, rigorously ensuring that they are also meaningful in discrete settings. However, the interquantile range fails to preserve either discrete order, indicating that it is unsuitable for measuring the dispersion of discrete distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02124v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberl, Bernhard Klar</dc:creator>
    </item>
    <item>
      <title>Robust Score-Based Quickest Change Detection</title>
      <link>https://arxiv.org/abs/2407.11094</link>
      <description>arXiv:2407.11094v4 Announce Type: replace 
Abstract: Methods in the field of quickest change detection rapidly detect in real-time a change in the data-generating distribution of an online data stream. Existing methods have been able to detect this change point when the densities of the pre- and post-change distributions are known. Recent work has extended these results to the case where the pre- and post-change distributions are known only by their score functions. This work considers the case where the pre- and post-change score functions are known only to correspond to distributions in two disjoint sets. This work selects a pair of least-favorable distributions from these sets to robustify the existing score-based quickest change detection algorithm, the properties of which are studied. This paper calculates the least-favorable distributions for specific model classes and provides methods of estimating the least-favorable distributions for common constructions. Simulation results are provided demonstrating the performance of our robust change detection algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11094v4</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3566677</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. on Information Theory, vol. 71, no. 7, pp. 5539-5555, July 2025</arxiv:journal_reference>
      <dc:creator>Sean Moushegian, Suya Wu, Enmao Diao, Jie Ding, Taposh Banerjee, Vahid Tarokh</dc:creator>
    </item>
    <item>
      <title>A Bayesian workflow for securitizing casualty insurance risk</title>
      <link>https://arxiv.org/abs/2407.14666</link>
      <description>arXiv:2407.14666v3 Announce Type: replace 
Abstract: Casualty insurance-linked securities (ILS) are appealing to investors because the underlying insurance claims, which are directly related to resulting security performance, are uncorrelated with most other asset classes. Conversely, casualty ILS are appealing to insurers as an efficient capital management tool. However, securitizing casualty insurance risk is non-trivial, as it requires forecasting loss ratios for pools of insurance policies that have not yet been written, in addition to estimating how the underlying losses will develop over time within future accident years. In this paper, we lay out a Bayesian workflow that tackles these complexities by using: (1) theoretically informed time-series and state-space models to capture how loss ratios develop and change over time; (2) historic industry data to inform prior distributions of models fit to individual programs; (3) stacking to combine loss ratio predictions from candidate models, and (4) both prior predictive simulations and simulation-based calibration to aid model specification. Using historic Schedule P filings, we then show how our proposed Bayesian workflow can be used to assess and compare models across a variety of key model performance metrics evaluated on future accident year losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14666v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Haines, Conor Goold, J. Mark Shoun</dc:creator>
    </item>
    <item>
      <title>Incorporating Memory into Continuous-Time Spatial Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2408.17278</link>
      <description>arXiv:2408.17278v2 Announce Type: replace 
Abstract: Obtaining reliable and precise estimates of wildlife species abundance and distribution is essential for the conservation and management of animal populations and natural reserves. Spatial capture-recapture (SCR) models provide estimates of population size and spatial density from data collected from remote sensors such as camera traps. Such data contain spatial correlation between observations of the same individual, which SCR models partly account for through a latent individual-specific activity centre, a location near which the individual is more likely detected. However, SCR models assume that the observations of an individual are independent over time and space, conditional on its activity centre, so that observed sightings at a given time and location do not influence the probability of being seen at future times and/or locations. This assumption is ecologically unrealistic given the smooth movement of animals over space through time. We propose a new continuous-time modelling framework that incorporates both an individual's (latent) activity centre and its (known) previous location and time of detection. By formulating the detections of an individual as an inhomogeneous temporal Poisson process, we develop a model drawing inspiration from the Ornstein-Uhlenbeck process, which is commonly used to model animal movement. Applying our model to a camera-trap survey of American martens, we observe a substantial improvement in model fit and notable differences in the estimated spatial distribution of activity centres. A simulation study shows that standard SCR models can produce substantially biased population estimates when spatio-temporal dependence is ignored, while the memory-based model remains robust. These findings highlight the importance of accounting for memory of previous detections in SCR models to improve ecological interpretation and inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17278v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Panchaud, Ruth King, David Borchers, Hannah Worthington, Ian Durbach, Paul Van Dam-Bates</dc:creator>
    </item>
    <item>
      <title>Improving Wald's (approximate) sequential probability ratio test by avoiding overshoot</title>
      <link>https://arxiv.org/abs/2410.16076</link>
      <description>arXiv:2410.16076v4 Announce Type: replace 
Abstract: Wald's sequential probability ratio test (SPRT) is a cornerstone of sequential analysis. Based on desired type-I, II error levels $\alpha, \beta$, it stops when the likelihood ratio crosses certain thresholds, guaranteeing optimality of the expected sample size. However, these thresholds are not closed form and the test is often applied with approximate thresholds $(1-\beta)/\alpha$ and $\beta/(1-\alpha)$ (approximate SPRT). When $\beta &gt; 0$, this neither guarantees error control at $\alpha,\beta$ nor optimality. When $\beta=0$ (power-one SPRT), this method is conservative and not optimal. The looseness in both cases is caused by \emph{overshoot}: the test statistic overshoots the thresholds at the stopping time. Numerically calculating thresholds may be infeasible, and most software packages do not do this. We improve the approximate SPRT by modifying the test statistic to avoid overshoot. Our `sequential boosting' technique uniformly improves power-one SPRTs $(\beta=0)$ for simple nulls and alternatives, or for one-sided nulls and alternatives in exponential families. When $\beta &gt; 0$, our techniques provide guaranteed error control at $\alpha,\beta$, while needing less samples than the approximate SPRT in our simulations. We also provide several nontrivial extensions: confidence sequences, sampling without replacement and conformal martingales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16076v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Target Aggregate Data Adjustment Method for Transportability Analysis Utilizing Summary-Level Data from the Target Population</title>
      <link>https://arxiv.org/abs/2412.12335</link>
      <description>arXiv:2412.12335v2 Announce Type: replace 
Abstract: Transportability analysis is a causal inference framework used to evaluate the external validity of randomized clinical trials (RCTs) or observational studies. Most existing transportability analysis methods require individual patient-level data (IPD) for both the source and the target population, narrowing its applicability when only target aggregate-level data (AgD) is available. Besides, accounting for censoring is essential to reduce bias in longitudinal data, yet AgD-based transportability methods in the presence of censoring remain underexplored. Here, we propose a two-stage weighting framework named "Target Aggregate Data Adjustment" (TADA) to address the mentioned challenges simultaneously. TADA is designed as a two-stage weighting scheme to simultaneously adjust for both censoring bias and distributional imbalances of effect modifiers (EM), where the final weights are the product of the inverse probability of censoring weights and participation weights derived using the method of moments. We have conducted an extensive simulation study to evaluate TADA's performance. Our results indicate that TADA can effectively control the bias resulting from censoring within a non-extreme range suitable for most practical scenarios, and enhance the application and clinical interpretability of transportability analyses in settings with limited data availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12335v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichen Yan, Quang Vuong, Rebecca K Metcalfe, Tianyu Guan, Haolun Shi, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>Ridge-Regularized Largest Root Test For High-Dimensional General Linear Hypotheses</title>
      <link>https://arxiv.org/abs/2504.15510</link>
      <description>arXiv:2504.15510v2 Announce Type: replace 
Abstract: A fundamental problem in multivariate analysis is testing general linear hypotheses for regression coefficients in a multivariate linear model. This framework encompasses a wide range of well-studied tasks, including MANOVA, joint significance testing of predictors, and detection of trends or seasonal effects. Among classical approaches, Roy's largest root test is particularly effective for detecting concentrated signals, relying on the largest eigenvalue of an F matrix constructed from residual covariance matrices. However, in high-dimensional settings, these matrices often become ill-conditioned or singular, rendering the test infeasible. To address this, we propose a ridge-regularized Roy's test that stabilizes the covariance estimation via a ridge term. We establish the asymptotic Tracy-Widom distribution of the largest eigenvalue of the regularized F-matrix under a high-dimensional regime, where both the dimension and hypotheses are comparable to the sample size, assuming only finite-moment conditions. A computationally efficient procedure is developed to estimate the associated centering and scaling parameters. We further analyze the power of the test under a class of low-rank alternatives and examine the influence of the regularization parameter. The method demonstrates strong performance in simulations and is applied to data from the Human Connectome Project to assess associations between volumetric brain measurements and behavioral variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15510v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoran Li</dc:creator>
    </item>
    <item>
      <title>Regularized Estimation of the Loading Matrix in Factor Models for High-Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2506.11232</link>
      <description>arXiv:2506.11232v2 Announce Type: replace 
Abstract: High-dimensional data analysis using traditional models suffers from overparameterization. Two types of techniques are commonly used to reduce the number of parameters - regularization and dimension reduction. In this project, we combine them by imposing a sparse factor structure and propose a regularized estimator to further reduce the number of parameters in factor models. A challenge limiting the widespread application of factor models is that factors are hard to interpret, as both factors and the loading matrix are unobserved. To address this, we introduce a penalty term when estimating the loading matrix for a sparse estimate. As a result, each factor only drives a smaller subset of time series that exhibit the strongest correlation, improving the factor interpretability. The theoretical properties of the proposed estimator are investigated. The simulation results are presented to confirm that our algorithm performs well. We apply our method to Hawaii tourism data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11232v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xialu Liu, Xin Wang</dc:creator>
    </item>
    <item>
      <title>The Zeta Tail Distribution: A Novel Event-Count Model</title>
      <link>https://arxiv.org/abs/2506.17496</link>
      <description>arXiv:2506.17496v2 Announce Type: replace 
Abstract: We introduce the Zeta Tail(a) probability distribution as a new model for random damage-event counts in risk analysis. Although readily motivated as an analogue of the Geometric(p) distribution, Zeta Tail(a) has received little attention in the scholarly literature. In the present work, we begin by deriving various fundamental properties of this novel distribution. We then assess its usefulness as an alternative to Geometric(p), both theoretically and through application to a set of meteorological data. Lastly, we discuss conceptual differences between employing the Zeta Tail(a) model conditionally (i.e., given observed data with certain known characteristics) and unconditionally (i.e., for arbitrary, as yet unobserved data).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17496v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers</dc:creator>
    </item>
    <item>
      <title>Computationally efficient variational-like approximations of possibilistic inferential models</title>
      <link>https://arxiv.org/abs/2404.19224</link>
      <description>arXiv:2404.19224v3 Announce Type: replace-cross 
Abstract: Inferential models (IMs) offer provably reliable, data-driven, possibilistic statistical inference. But despite the IM framework's theoretical and foundational advantages, efficient computation is a challenge. This paper presents a simple yet powerful numerical strategy for approximating the IM's possibility contour, or at least its $\alpha$-cut for a specified $\alpha \in (0,1)$. Our proposal starts with the specification of a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure. Akin to variational inference, we then propose to tune the parameters of that parametric family so that its $100(1-\alpha)\%$ credible set roughly matches the IM contour's $\alpha$-cut. This parametric $\alpha$-cut matching strategy implies a full approximation to the IM's possibility contour at a fraction of the computational cost associated with previous strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19224v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijar.2025.109506</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Approximate Reasoning, volume 186, paper 109506, 2025</arxiv:journal_reference>
      <dc:creator>Leonardo Cella, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>An efficient Monte Carlo method for valid prior-free possibilistic statistical inference</title>
      <link>https://arxiv.org/abs/2501.10585</link>
      <description>arXiv:2501.10585v3 Announce Type: replace-cross 
Abstract: Inferential models (IMs) offer prior-free, Bayesian-like posterior degrees of belief designed for statistical inference, which feature a frequentist-like calibration property that ensures reliability of said inferences. The catch is that IMs' degrees of belief are possibilistic rather than probabilistic and, since the familiar Monte Carlo methods approximate probabilistic quantities, there are significant computational challenges associated with putting this framework into practice. The present paper overcomes these challenges by developing a new Monte Carlo method designed specifically to approximate the IM's possibilistic output. The proposal is based on a characterization of the possibilistic IM's credal set, which identifies the "best probabilistic approximation" of the IM as a mixture distribution that can be readily approximated and sampled from. These samples can then be transformed into an approximation of the possibilistic IM. Numerical results are presented highlighting the proposed approximation's accuracy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10585v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Counterfactual Inference under Thompson Sampling</title>
      <link>https://arxiv.org/abs/2504.08773</link>
      <description>arXiv:2504.08773v2 Announce Type: replace-cross 
Abstract: Recommender systems exemplify sequential decision-making under uncertainty, strategically deciding what content to serve to users, to optimise a range of potential objectives. To balance the explore-exploit trade-off successfully, Thompson sampling provides a natural and widespread paradigm to probabilistically select which action to take. Questions of causal and counterfactual inference, which underpin use-cases like offline evaluation, are not straightforward to answer in these contexts. Specifically, whilst most existing estimators rely on action propensities, these are not readily available under Thompson sampling procedures.
  We derive exact and efficiently computable expressions for action propensities under a variety of parameter and outcome distributions, enabling the use of off-policy estimators in Thompson sampling scenarios. This opens up a range of practical use-cases where counterfactual inference is crucial, including unbiased offline evaluation of recommender systems, as well as general applications of causal inference in online advertising, personalisation, and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08773v2</guid>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Jeunen</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Invariant Prediction</title>
      <link>https://arxiv.org/abs/2505.11211</link>
      <description>arXiv:2505.11211v2 Announce Type: replace-cross 
Abstract: We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We leverage the hierarchical structure to explicitly test invariance of causal mechanisms under heterogeneous data, resulting in improved computational scalability for a larger number of predictors compared to ICP. Moreover, given its Bayesian nature BHIP enables the use of prior information. In this paper, we test two sparsity inducing priors: horseshoe and spike-and-slab, both of which allow us a more reliable identification of causal features. We test BHIP in synthetic and real-world data showing its potential as an alternative inference method to ICP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11211v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia</dc:creator>
    </item>
  </channel>
</rss>
