<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>"We do use it, but not how hearing people think": How the Deaf and Hard of Hearing Community Uses Large Language Model Tools</title>
      <link>https://arxiv.org/abs/2410.21358</link>
      <description>arXiv:2410.21358v1 Announce Type: new 
Abstract: Generative AI tools, particularly those utilizing large language models (LLMs), have become increasingly prevalent in both professional and personal contexts, offering powerful capabilities for text generation and communication support. While these tools are widely used to enhance productivity and accessibility, there has been limited exploration of how Deaf and Hard of Hearing (DHH) individuals engage with text-based generative AI tools, as well as the challenges they may encounter. This paper presents a mixed-method survey study investigating how the DHH community uses Text AI tools, such as ChatGPT, to reduce communication barriers, bridge Deaf and hearing cultures, and improve access to information. Through a survey of 80 DHH participants and separate interviews with 11 other participants, we found that while these tools provide significant benefits, including enhanced communication and mental health support, they also introduce barriers, such as a lack of American Sign Language (ASL) support and understanding of Deaf cultural nuances. Our findings highlight unique usage patterns within the DHH community and underscore the need for inclusive design improvements. We conclude by offering practical recommendations to enhance the accessibility of Text AI for the DHH community and suggest directions for future research in AI and accessibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21358v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuxu Huffman, Si Chen, Kelly Avery Mack, Haotian Su, Qi Wang, Raja Kushalnagar</dc:creator>
    </item>
    <item>
      <title>A New Heuristic Algorithm for Balanced Deliberation Groups</title>
      <link>https://arxiv.org/abs/2410.21451</link>
      <description>arXiv:2410.21451v1 Announce Type: new 
Abstract: We here present an improved version of the Sortition Foundation's GROUPSELECT software package, which aims to repeatedly allocate participants of a deliberative process to discussion groups in a way that balances demographics in each group and maximises distinct meetings over time. Our result, GROUPOPT, significantly outperforms the prior algorithmic approach LEGACY. We also add functionalities to the GROUPSELECT software to help the end user. The GROUPOPT algorithm utilises random shuffles and Pareto swaps to find a locally optimal solution that maximises demographic balance and minimises the number of pairwise previous meetings, with the relative importance of these two metrics defined by the user.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21451v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake Barrett, Philipp C Verpoort, Kobi Gal</dc:creator>
    </item>
    <item>
      <title>Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users</title>
      <link>https://arxiv.org/abs/2410.21596</link>
      <description>arXiv:2410.21596v1 Announce Type: new 
Abstract: As artificial intelligence becomes increasingly sophisticated, companion chatbots have been proposed as a potential solution to the growing epidemic of loneliness. However, the impact of these AI companions on users' psychological well-being and social behaviors remains poorly understood. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and their experience of loneliness. We find a small but significant direct correlation between session length with chatbots and loneliness, and we develop a model of this relationship through multiple regression analysis, finding social attraction and neuroticism as moderators. We find seven clusters of users, including socially fulfilled dependent users to lonely dependent users. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for what kind of human-AI connections might lead to emotional well-being and complement rather than replace human connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21596v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Auren R. Liu, Pat Pataranutaporn, Sherry Turkle, Pattie Maes</dc:creator>
    </item>
    <item>
      <title>Accessible Nonverbal Cues to Support Conversations in VR for Blind and Low Vision People</title>
      <link>https://arxiv.org/abs/2410.21652</link>
      <description>arXiv:2410.21652v1 Announce Type: new 
Abstract: Social VR has increased in popularity due to its affordances for rich, embodied, and nonverbal communication. However, nonverbal communication remains inaccessible for blind and low vision people in social VR. We designed accessible cues with audio and haptics to represent three nonverbal behaviors: eye contact, head shaking, and head nodding. We evaluated these cues in real-time conversation tasks where 16 blind and low vision participants conversed with two other users in VR. We found that the cues were effective in supporting conversations in VR. Participants had statistically significantly higher scores for accuracy and confidence in detecting attention during conversations with the cues than without. We also found that participants had a range of preferences and uses for the cues, such as learning social norms. We present design implications for handling additional cues in the future, such as the challenges of incorporating AI. Through this work, we take a step towards making interpersonal embodied interactions in VR fully accessible for blind and low vision people.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21652v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3663548.3675663</arxiv:DOI>
      <arxiv:journal_reference>ASSETS 2024</arxiv:journal_reference>
      <dc:creator>Crescentia Jung, Jazmin Collins, Ricardo E. Gonzalez Penuela, Jonathan Isaac Segal, Andrea Stevenson Won, Shiri Azenkot</dc:creator>
    </item>
    <item>
      <title>"The Guide Has Your Back": Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People</title>
      <link>https://arxiv.org/abs/2410.21659</link>
      <description>arXiv:2410.21659v1 Announce Type: new 
Abstract: As social VR applications grow in popularity, blind and low vision users encounter continued accessibility barriers. Yet social VR, which enables multiple people to engage in the same virtual space, presents a unique opportunity to allow other people to support a user's access needs. To explore this opportunity, we designed a framework based on physical sighted guidance that enables a guide to support a blind or low vision user with navigation and visual interpretation. A user can virtually hold on to their guide and move with them, while the guide can describe the environment. We studied the use of our framework with 16 blind and low vision participants and found that they had a wide range of preferences. For example, we found that participants wanted to use their guide to support social interactions and establish a human connection with a human-appearing guide. We also highlight opportunities for novel guidance abilities in VR, such as dynamically altering an inaccessible environment. Through this work, we open a novel design space for a versatile approach for making VR fully accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21659v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3597638.3608386</arxiv:DOI>
      <arxiv:journal_reference>ASSETS 2023</arxiv:journal_reference>
      <dc:creator>Jazmin Collins, Crescentia Jung, Yeonju Jang, Danielle Montour, Andrea Stevenson Won, Shiri Azenkot</dc:creator>
    </item>
    <item>
      <title>Cross-Domain Transfer Learning Method for Thermal Adaptive Behavior Recognition with WiFi</title>
      <link>https://arxiv.org/abs/2410.21827</link>
      <description>arXiv:2410.21827v1 Announce Type: new 
Abstract: A reliable comfort model is essential to improve occupant satisfaction and reduce building energy consumption. As two types of the most common and intuitive thermal adaptive behaviors, precise recognition of dressing and undressing can effectively support thermal comfort prediction. However, traditional activity recognition suffers from shortcomings in privacy, cost, and performance. To address the above issues, this study proposes a cross-domain transfer learning method for human dressing and undressing adaptive behavior recognition with WiFi. First, we determine the activity interval by calculating the sliding variance for denoised WiFi signals. Subsequently, short-time Fourier transform and discrete wavelet transform are performed to extract action information on the basis of time-frequency analysis. Ultimately, an efficient 1D CNN pre-trained model is integrated with the SVM algorithm as a hybrid model to enhance the identification robustness in new scenarios. Experiment results show that the hybrid model based on transfer learning provides a more accurate prediction for the adaptative behavior of target subjects, achieving 96.9% and 94.9% accuracy in two cases, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21827v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaohe Lv, Guoliang Zhao, Zhanbo Xu, Jiang Wu, Yadong Zhou, Kun Liu</dc:creator>
    </item>
    <item>
      <title>Effects of Human Avatar Representation in Virtual Reality on Inter-Brain Connection</title>
      <link>https://arxiv.org/abs/2410.21894</link>
      <description>arXiv:2410.21894v1 Announce Type: new 
Abstract: Increasing advances in affordable consumer hardware and accessible software frameworks are now bringing Virtual Reality (VR) to the masses. Especially collaborative VR applications where different people can work together are gaining momentum. In this context, human avatars and their representations are a crucial aspect of collaborative VR applications as they represent a digital twin of the end-users and determine how one is perceived in a virtual environment. When it comes to the effect of avatar representation on the end-users of collaborative VR applications, so far mostly questionnaires have been used to assess the quality of avatar representations. A promising alternative to objectively measure the effect of avatar representation is the investigation of inter-brain connections during the usage of a collaborative VR application. However, the combination of immersive VR applications and inter-brain connections has not been fully researched yet. Thus, our work investigates how different human avatar representations (real (RL), full-body (FB), and head-hand (HH)) affect inter-brain connections. For this purpose, we have designed and conducted a hyperscanning study with eight pairs. The main results of our hyperscanning study show that the number of significant sensor pairs was the highest in the RL, medium in the FB, and lowest in the HH condition indicating that an avatar that looks more like a real human enables more significant sensor pairs to appear in an EEG analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21894v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Enes Yigitbas, Christian Kaltschmidt</dc:creator>
    </item>
    <item>
      <title>Evaluating Perceptual Deviations in Video See-Through Head-Mounted Displays while Utilizing Physical Touchscreens</title>
      <link>https://arxiv.org/abs/2410.21944</link>
      <description>arXiv:2410.21944v1 Announce Type: new 
Abstract: Extended reality technology has become a useful tool in many applications, but still suffers from visual deviations that can hamper the utility of the technology. This paper discusses the types of persisting visual deviations experienced when observing the natural world through video see-through head-mounted displays. A generalizable method to measure the effect of these deviations on real-world interaction is designed and used in a human-in-the-loop experiment. The experiment compared video see-through sight through an head-mounted display with normal eyesight in a static set-up, focusing on (camera) lens distortions and display deviations. Participants interacted with a real touchscreen, locating the position of flashed markers shortly after disappearance comparing both conditions to check for deviations in position and time. Results show significant larger mean distance errors between the interaction locations and the original marker positions for video see-through compared to normal eyesight. Moreover, errors increase towards the screen periphery. No significant distance error improvement over time was found, however, response times did significantly decrease for both types of sight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21944v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rudy De-Xin de Lange, Roemer Martin Bien Bakker, Tanja Johanna Juliana Bos</dc:creator>
    </item>
    <item>
      <title>An LLM-based Simulation Framework for Embodied Conversational Agents in Psychological Counseling</title>
      <link>https://arxiv.org/abs/2410.22041</link>
      <description>arXiv:2410.22041v1 Announce Type: new 
Abstract: Simulation is crucial for validating algorithmic strategies in real-world scenarios. While LLM-based social simulation shows promise as a mainstream tool, simulating complex scenarios like psychological counseling remains challenging. We present ECAs (short for Embodied Conversational Agents), a framework for simulating psychological counseling clients' embodied memory, integrating embodied cognition and counseling theories. We formulate six design goals based on a comprehensive review of psychological counseling theories. Using LLMs, we expand real counseling case data into a nuanced embodied cognitive memory space and generate dialogues based on high-frequency counseling questions. We validate our framework using the D4 dataset, with evaluations by licensed counselors. Results show our approach significantly outperforms baselines in simulation authenticity and necessity. To demonstrate scalability, we created a public ECAs dataset through batch simulations. This research provides valuable insights for future social simulation studies in psychological counseling and Embodied Counseling Agents research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22041v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lixiu Wu, Yuanrong Tang, Qisen Pan, Xianyang Zhan, Yucheng Han, Mingyang You, Lanxi Xiao, Tianhong Wang, Chen Zhong, Jiangtao Gong</dc:creator>
    </item>
    <item>
      <title>Towards Data-Informed Interventions: Opportunities and Challenges of Street-level Multimodal Sensing</title>
      <link>https://arxiv.org/abs/2410.22092</link>
      <description>arXiv:2410.22092v1 Announce Type: new 
Abstract: Over the past decades, improvements in data collection hardware coupled with novel artificial intelligence algorithms have made it possible for researchers to understand urban environments at an unprecedented scale. From local interactions between actors to city-wide infrastructural problems, this new data-driven approach enables a more informed and trustworthy decision-making process aiming at transforming cities into safer and more equitable places for living. This new moment unfolded new opportunities to understand various phenomena that directly impact how accessible cities are to heterogeneous populations. Specifically, sensing localized physical interactions among actors under different scenarios can drive substantial interventions in urban environments to make them safer for all. In this manuscript, we list opportunities and associated challenges to leverage street-level multimodal sensing data to empower domain experts in making more informed decisions and, ultimately, supporting a data-informed policymaking framework. The challenges presented here can motivate research in different areas, such as computer vision and human-computer interaction, to support cities in growing more sustainably.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22092v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joao Rulff, Giancarlo Pereira, Maryam Hosseini, Marcos Lage, Claudio Silva</dc:creator>
    </item>
    <item>
      <title>Analyzing Multimodal Interaction Strategies for LLM-Assisted Manipulation of 3D Scenes</title>
      <link>https://arxiv.org/abs/2410.22177</link>
      <description>arXiv:2410.22177v1 Announce Type: new 
Abstract: As more applications of large language models (LLMs) for 3D content for immersive environments emerge, it is crucial to study user behaviour to identify interaction patterns and potential barriers to guide the future design of immersive content creation and editing systems which involve LLMs. In an empirical user study with 12 participants, we combine quantitative usage data with post-experience questionnaire feedback to reveal common interaction patterns and key barriers in LLM-assisted 3D scene editing systems. We identify opportunities for improving natural language interfaces in 3D design tools and propose design recommendations for future LLM-integrated 3D content creation systems. Through an empirical study, we demonstrate that LLM-assisted interactive systems can be used productively in immersive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22177v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junlong Chen, Jens Grubert, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>GPT-4o reads the mind in the eyes</title>
      <link>https://arxiv.org/abs/2410.22309</link>
      <description>arXiv:2410.22309v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are capable of reproducing human-like inferences, including inferences about emotions and mental states, from text. Whether this capability extends beyond text to other modalities remains unclear. Humans possess a sophisticated ability to read the mind in the eyes of other people. Here we tested whether this ability is also present in GPT-4o, a multimodal LLM. Using two versions of a widely used theory of mind test, the Reading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes Test, we found that GPT-4o outperformed humans in interpreting mental states from upright faces but underperformed humans when faces were inverted. While humans in our sample showed no difference between White and Non-white faces, GPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's errors were not random but revealed a highly consistent, yet incorrect, processing of mental-state information across trials, with an orientation-dependent error structure that qualitatively differed from that of humans for inverted faces but not for upright faces. These findings highlight how advanced mental state inference abilities and human-like face processing signatures, such as inversion effects, coexist in GPT-4o alongside substantial differences in information processing compared to humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22309v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James W. A. Strachan, Oriana Pansardi, Eugenio Scaliti, Marco Celotto, Krati Saxena, Chunzhi Yi, Fabio Manzi, Alessandro Rufo, Guido Manzi, Michael S. A. Graziano, Stefano Panzeri, Cristina Becchio</dc:creator>
    </item>
    <item>
      <title>Assessing User Needs in Non-Visual Text Input: Perceptions of Blind Adults on Current and Experimental Mobile Interfaces</title>
      <link>https://arxiv.org/abs/2410.22324</link>
      <description>arXiv:2410.22324v1 Announce Type: new 
Abstract: Text input on mobile devices without physical key boundaries can be challenging for people who are blind or low-vision. We interview 12 blind adults about their experiences with mobile text input to provide insight into which research direction may be the most beneficial. We identify three primary themes that were experiences or opinions shared by many of our participants: the poor accuracy of dictation, difficulty entering text in noisy environments, and difficulty correcting errors in entered text. We discuss an experimental non-visual text input method with each participant to solicit opinions and find that the largest concern is the time it would take to learn the technique. We find that the majority of our participants do not use word predictions while performing text input with an onscreen keyboard, finding it faster and easier to finish typing each word manually.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22324v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Gaines, Keith Vertanen</dc:creator>
    </item>
    <item>
      <title>Less Cybersickness, Please: Demystifying and Detecting Stereoscopic Visual Inconsistencies in Virtual Reality Apps</title>
      <link>https://arxiv.org/abs/2406.09313</link>
      <description>arXiv:2406.09313v2 Announce Type: cross 
Abstract: The quality of Virtual Reality (VR) apps is vital, particularly the rendering quality of the VR Graphical User Interface (GUI). Different from traditional 2D apps, VR apps create a 3D digital scene for users, by rendering two distinct 2D images for the user's left and right eyes, respectively. Stereoscopic visual inconsistency (denoted as "SVI") issues, however, undermine the rendering process of the user's brain, leading to user discomfort and even adverse health effects. Such issues commonly exist but remain underexplored. We conduct an empirical analysis on 282 SVI bug reports from 15 VR platforms, summarizing 15 types of manifestations. The empirical analysis reveals that automatically detecting SVI issues is challenging, mainly because: (1) lack of training data; (2) the manifestations of SVI issues are diverse, complicated, and often application-specific; (3) most accessible VR apps are closed-source commercial software. Existing pattern-based supervised classification approaches may be inapplicable or ineffective in detecting the SVI issues. To counter these challenges, we propose an unsupervised black-box testing framework named StereoID to identify the stereoscopic visual inconsistencies, based only on the rendered GUI states. StereoID generates a synthetic right-eye image based on the actual left-eye image and computes distances between the synthetic right-eye image and the actual right-eye image to detect SVI issues. We propose a depth-aware conditional stereo image translator to power the image generation process, which captures the expected perspective shifts between left-eye and right-eye images. We build a large-scale unlabeled VR stereo screenshot dataset with larger than 171K images from 288 real-world VR apps for experiments. After substantial experiments, StereoID demonstrates superior performance for detecting SVI issues in both user reports and wild VR apps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09313v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3660803</arxiv:DOI>
      <dc:creator>Shuqing Li, Cuiyun Gao, Jianping Zhang, Yujia Zhang, Yepang Liu, Jiazhen Gu, Yun Peng, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>Enhancing EHR Systems with data from wearables: An end-to-end Solution for monitoring post-Surgical Symptoms in older adults</title>
      <link>https://arxiv.org/abs/2410.21507</link>
      <description>arXiv:2410.21507v1 Announce Type: cross 
Abstract: Mobile health (mHealth) apps have gained popularity over the past decade for patient health monitoring, yet their potential for timely intervention is underutilized due to limited integration with electronic health records (EHR) systems. Current EHR systems lack real-time monitoring capabilities for symptoms, medication adherence, physical and social functions, and community integration. Existing systems typically rely on static, in-clinic measures rather than dynamic, real-time patient data. This highlights the need for automated, scalable, and human-centered platforms to integrate patient-generated health data (PGHD) within EHR. Incorporating PGHD in a user-friendly format can enhance patient symptom surveillance, ultimately improving care management and post-surgical outcomes. To address this barrier, we have developed an mHealth platform, ROAMM-EHR, to capture real-time sensor data and Patient Reported Outcomes (PROs) using a smartwatch. The ROAMM-EHR platform can capture data from a consumer smartwatch, send captured data to a secure server, and display information within the Epic EHR system using a user-friendly interface, thus enabling healthcare providers to monitor post-surgical symptoms effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21507v1</guid>
      <category>q-bio.QM</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3636534.3698118</arxiv:DOI>
      <dc:creator>Heng Sun, Sai Manoj Jalam, Havish Kodali, Subhash Nerella, Ruben D. Zapata, Nicole Gravina, Jessica Ray, Erik C. Schmidt, Todd Matthew Manini, Rashidi Parisa</dc:creator>
    </item>
    <item>
      <title>Diagnosis of Knee Osteoarthritis Using Bioimpedance and Deep Learning</title>
      <link>https://arxiv.org/abs/2410.21512</link>
      <description>arXiv:2410.21512v1 Announce Type: cross 
Abstract: Diagnosing knee osteoarthritis (OA) early is crucial for managing symptoms and preventing further joint damage, ultimately improving patient outcomes and quality of life. In this paper, a bioimpedance-based diagnostic tool that combines precise hardware and deep learning for effective non-invasive diagnosis is proposed. system features a relay-based circuit and strategically placed electrodes to capture comprehensive bioimpedance data. The data is processed by a neural network model, which has been optimized using convolutional layers, dropout regularization, and the Adam optimizer. This approach achieves a 98% test accuracy, making it a promising tool for detecting knee osteoarthritis musculoskeletal disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21512v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamal Al-Nabulsi, Mohammad Al-Sayed Ahmad, Baraa Hasaneiah, Fayhaa AlZoubi</dc:creator>
    </item>
    <item>
      <title>USpeech: Ultrasound-Enhanced Speech with Minimal Human Effort via Cross-Modal Synthesis</title>
      <link>https://arxiv.org/abs/2410.22076</link>
      <description>arXiv:2410.22076v1 Announce Type: cross 
Abstract: Speech enhancement is crucial in human-computer interaction, especially for ubiquitous devices. Ultrasound-based speech enhancement has emerged as an attractive choice because of its superior ubiquity and performance. However, inevitable interference from unexpected and unintended sources during audio-ultrasound data acquisition makes existing solutions rely heavily on human effort for data collection and processing. This leads to significant data scarcity that limits the full potential of ultrasound-based speech enhancement. To address this, we propose USpeech, a cross-modal ultrasound synthesis framework for speech enhancement with minimal human effort. At its core is a two-stage framework that establishes correspondence between visual and ultrasonic modalities by leveraging audible audio as a bridge. This approach overcomes challenges from the lack of paired video-ultrasound datasets and the inherent heterogeneity between video and ultrasound data. Our framework incorporates contrastive video-audio pre-training to project modalities into a shared semantic space and employs an audio-ultrasound encoder-decoder for ultrasound synthesis. We then present a speech enhancement network that enhances speech in the time-frequency domain and recovers the clean speech waveform via a neural vocoder. Comprehensive experiments show USpeech achieves remarkable performance using synthetic ultrasound data comparable to physical data, significantly outperforming state-of-the-art ultrasound-based speech enhancement baselines. USpeech is open-sourced at https://github.com/aiot-lab/USpeech/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22076v1</guid>
      <category>cs.SD</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Jiang-Tao Yu, Running Zhao, Sijie Ji, Edith C. H. Ngai, Chenshu Wu</dc:creator>
    </item>
    <item>
      <title>An Actor-Critic Approach to Boosting Text-to-SQL Large Language Model</title>
      <link>https://arxiv.org/abs/2410.22082</link>
      <description>arXiv:2410.22082v1 Announce Type: cross 
Abstract: Text-To-SQL (T2S) conversion based on large language models (LLMs) has found a wide range of applications, by leveraging the capabilities of LLMs in interpreting the query intent expressed in natural language. Existing research focuses on suitable representations for data schema and/or questions, task-specific instructions and representative examples, and complicated inference pipelines. All these methods are empirical and task specific, without a theoretical bound on performance. In this paper, we propose a simple, general, and performance guaranteed T2S enhancement approach called Actor-Critic (AC). Specifically, we design two roles using the same LLM: an Actor to produce SQL queries and a Critic to evaluate the produced SQL. If the Critic believes the produced SQL is wrong, it notifies the Actor to reproduce the SQL and perform evaluation again. By this simple iterative process, expected performance can be derived in theory. We conducted extensive experiments on the Spider and related datasets with eleven LLMs, and demonstrated that the Actor-Critic method consistently improves the performance of T2S, thus serving as a general enhancement approach for T2S conversion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22082v1</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Zheng, Haipeng Jing, Canyu Rui, Askar Hamdulla, Dong Wang</dc:creator>
    </item>
    <item>
      <title>A Data-Driven Analysis of the Sovereign Citizens Movement on Telegram</title>
      <link>https://arxiv.org/abs/2410.22142</link>
      <description>arXiv:2410.22142v1 Announce Type: cross 
Abstract: Online communities of known extremist groups like the alt-right and QAnon have been well explored in past work. However, we find that an extremist group called Sovereign Citizens is relatively unexplored despite its existence since the 1970s. Their main belief is delegitimizing the established government with a tactic called paper terrorism, clogging courts with pseudolegal claims. In recent years, their activities have escalated to threats like forcefully claiming property ownership and participating in the Capitol Riot. This paper aims to shed light on Sovereign Citizens' online activities by examining two Telegram channels, each belonging to an identified Sovereign Citizen individual. We collect over 888K text messages and apply NLP techniques. We find that the two channels differ in the topics they discussed, demonstrating different focuses. Further, the two channels exhibit less toxic content compared to other extremist groups like QAnon. Finally, we find indications of overlapping beliefs between the two channels and QAnon, suggesting a merging or complementing of beliefs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22142v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.36190/2024.08</arxiv:DOI>
      <arxiv:journal_reference>Workshop Proceedings of the 18th International AAAI Conference on Web and Social Media (ICWSM) -- Workshop: CySoc 2024: 5th International Workshop on Cyber Social Threats</arxiv:journal_reference>
      <dc:creator>Satrio Yudhoatmojo, Utkucan Balci, Jeremy Blackburn</dc:creator>
    </item>
    <item>
      <title>Democratizing Reward Design for Personal and Representative Value-Alignment</title>
      <link>https://arxiv.org/abs/2410.22203</link>
      <description>arXiv:2410.22203v1 Announce Type: cross 
Abstract: Aligning AI agents with human values is challenging due to diverse and subjective notions of values. Standard alignment methods often aggregate crowd feedback, which can result in the suppression of unique or minority preferences. We introduce Interactive-Reflective Dialogue Alignment, a method that iteratively engages users in reflecting on and specifying their subjective value definitions. This system learns individual value definitions through language-model-based preference elicitation and constructs personalized reward models that can be used to align AI behaviour. We evaluated our system through two studies with 30 participants, one focusing on "respect" and the other on ethical decision-making in autonomous vehicles. Our findings demonstrate diverse definitions of value-aligned behaviour and show that our system can accurately capture each person's unique understanding. This approach enables personalized alignment and can inform more representative and interpretable collective alignment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22203v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carter Blair, Kate Larson, Edith Law</dc:creator>
    </item>
    <item>
      <title>Effective Guidance for Model Attention with Simple Yes-no Annotations</title>
      <link>https://arxiv.org/abs/2410.22312</link>
      <description>arXiv:2410.22312v1 Announce Type: cross 
Abstract: Modern deep learning models often make predictions by focusing on irrelevant areas, leading to biased performance and limited generalization. Existing methods aimed at rectifying model attention require explicit labels for irrelevant areas or complex pixel-wise ground truth attention maps. We present CRAYON (Correcting Reasoning with Annotations of Yes Or No), offering effective, scalable, and practical solutions to rectify model attention using simple yes-no annotations. CRAYON empowers classical and modern model interpretation techniques to identify and guide model reasoning: CRAYON-ATTENTION directs classic interpretations based on saliency maps to focus on relevant image regions, while CRAYON-PRUNING removes irrelevant neurons identified by modern concept-based methods to mitigate their influence. Through extensive experiments with both quantitative and human evaluation, we showcase CRAYON's effectiveness, scalability, and practicality in refining model attention. CRAYON achieves state-of-the-art performance, outperforming 12 methods across 3 benchmark datasets, surpassing approaches that require more complex annotations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22312v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongmin Lee (Polo), Ali Payani (Polo), Duen Horng (Polo),  Chau</dc:creator>
    </item>
    <item>
      <title>Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions</title>
      <link>https://arxiv.org/abs/2302.07248</link>
      <description>arXiv:2302.07248v2 Announce Type: replace 
Abstract: Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers identify potential errors is to highlight uncertain tokens. However, there have been no empirical studies exploring the effectiveness of this technique -- nor investigating the different and not-yet-agreed-upon notions of uncertainty in the context of generative models. We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools, and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07248v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3702320</arxiv:DOI>
      <dc:creator>Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, Jennifer Wortman Vaughan</dc:creator>
    </item>
    <item>
      <title>When combinations of humans and AI are useful: A systematic review and meta-analysis</title>
      <link>https://arxiv.org/abs/2405.06087</link>
      <description>arXiv:2405.06087v2 Announce Type: replace 
Abstract: Inspired by the increasing use of AI to augment humans, researchers have studied human-AI systems involving different tasks, systems, and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here, we addressed this question by conducting a meta-analysis of over 100 recent experimental studies reporting over 300 effect sizes. First, we found that, on average, human-AI combinations performed significantly worse than the best of humans or AI alone. Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when the AI outperformed humans alone we found losses. These findings highlight the heterogeneity of the effects of human-AI collaboration and point to promising avenues for improving human-AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06087v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41562-024-02024-1</arxiv:DOI>
      <arxiv:journal_reference>Nat Hum Behav (2024)</arxiv:journal_reference>
      <dc:creator>Michelle Vaccaro, Abdullah Almaatouq, Thomas Malone</dc:creator>
    </item>
    <item>
      <title>A Spatial-Spectral and Temporal Dual Prototype Network for Motor Imagery Brain-Computer Interface</title>
      <link>https://arxiv.org/abs/2407.03177</link>
      <description>arXiv:2407.03177v2 Announce Type: replace 
Abstract: Motor imagery electroencephalogram (MI-EEG) decoding plays a crucial role in developing motor imagery brain-computer interfaces (MI-BCIs). However, decoding intentions from MI remains challenging due to the inherent complexity of EEG signals relative to the small-sample size. To address this issue, we propose a spatial-spectral and temporal dual prototype network (SST-DPN). First, we design a lightweight attention mechanism to uniformly model the spatial-spectral relationships across multiple EEG electrodes, enabling the extraction of powerful spatial-spectral features. Then, we develop a multi-scale variance pooling module tailored for EEG signals to capture long-term temporal features. This module is parameter-free and computationally efficient, offering clear advantages over the widely used transformer models. Furthermore, we introduce dual prototype learning to optimize the feature space distribution and training process, thereby improving the model's generalization ability on small-sample MI datasets. Our experimental results show that the SST-DPN outperforms state-of-the-art models with superior classification accuracy (84.11% for dataset BCI4-2A, 86.65% for dataset BCI4-2B). Additionally, we use the BCI3-4A dataset with fewer training data to further validate the generalization ability of the proposed SST-DPN. We also achieve superior performance with 82.03% classification accuracy. Benefiting from the lightweight parameters and superior decoding accuracy, our SST-DPN shows great potential for practical MI-BCI applications. The code is publicly available at https://github.com/hancan16/SST-DPN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03177v2</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Can Han, Chen Liu, Yaqi Wang, Crystal Cai, Jun Wang, Dahong Qian</dc:creator>
    </item>
    <item>
      <title>PAIGE: Examining Learning Outcomes and Experiences with Personalized AI-Generated Educational Podcasts</title>
      <link>https://arxiv.org/abs/2409.04645</link>
      <description>arXiv:2409.04645v2 Announce Type: replace 
Abstract: Generative AI is revolutionizing content creation and has the potential to enable real-time, personalized educational experiences. We investigated the effectiveness of converting textbook chapters into AI-generated podcasts and explored the impact of personalizing these podcasts for individual learner profiles. We conducted a 3x3 user study with 180 college students in the United States, comparing traditional textbook reading with both generalized and personalized AI-generated podcasts across three textbook subjects. The personalized podcasts were tailored to students' majors, interests, and learning styles. Our findings show that students found the AI-generated podcast format to be more enjoyable than textbooks and that personalized podcasts led to significantly improved learning outcomes, although this was subject-specific. These results highlight that AI-generated podcasts can offer an engaging and effective modality transformation of textbook material, with personalization enhancing content relevance. We conclude with design recommendations for leveraging AI in education, informed by student feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04645v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiffany D. Do, Usama Bin Shafqat, Elsie Ling, Nikhil Sarda</dc:creator>
    </item>
    <item>
      <title>AeroHaptix: A Wearable Vibrotactile Feedback System for Enhancing Collision Avoidance in UAV Teleoperation</title>
      <link>https://arxiv.org/abs/2407.12105</link>
      <description>arXiv:2407.12105v2 Announce Type: replace-cross 
Abstract: Haptic feedback enhances collision avoidance by providing directional obstacle information to operators during unmanned aerial vehicle (UAV) teleoperation. However, such feedback is often rendered via haptic joysticks, which are unfamiliar to UAV operators and limited to single direction force feedback. Additionally, the direct coupling between the input device and the feedback method diminishes an operators' sense of control and causes oscillatory movements. To overcome these limitations, we propose AeroHaptix, a wearable haptic feedback system that uses spatial vibrations to communicate multiple obstacle directions to operators simultaneously, without interfering the input control. The layout of vibrotactile actuators was determined via a perceptual study to eliminate perceptual biases and achieve uniform spatial coverage. A novel rendering algorithm, MultiCBF, extends control barrier functions to support multi-directional feedback. Our system evaluation showed that compared to the baseline condition, AeroHaptix effectively reduced the number of collisions and input disagreement. Additionally, operators reported that AeroHaptix was more helpful than the force feedback method, with comparable workload and situational awareness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12105v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bingjian Huang, Zhecheng Wang, Qilong Cheng, Siyi Ren, Hanfeng Cai, Antonio Alvarez Valdivia, Karthik Mahadevan, Daniel Wigdor</dc:creator>
    </item>
    <item>
      <title>Is the Lecture Engaging for Learning? Lecture Voice Sentiment Analysis for Knowledge Graph-Supported Intelligent Lecturing Assistant (ILA) System</title>
      <link>https://arxiv.org/abs/2408.10492</link>
      <description>arXiv:2408.10492v2 Announce Type: replace-cross 
Abstract: This paper introduces an intelligent lecturing assistant (ILA) system that utilizes a knowledge graph to represent course content and optimal pedagogical strategies. The system is designed to support instructors in enhancing student learning through real-time analysis of voice, content, and teaching methods. As an initial investigation, we present a case study on lecture voice sentiment analysis, in which we developed a training set comprising over 3,000 one-minute lecture voice clips. Each clip was manually labeled as either engaging or non-engaging. Utilizing this dataset, we constructed and evaluated several classification models based on a variety of features extracted from the voice clips. The results demonstrate promising performance, achieving an F1-score of 90% for boring lectures on an independent set of over 800 test voice clips. This case study lays the groundwork for the development of a more sophisticated model that will integrate content analysis and pedagogical practices. Our ultimate goal is to aid instructors in teaching more engagingly and effectively by leveraging modern artificial intelligence techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10492v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan An, Samarth Kolanupaka, Jacob An, Matthew Ma, Unnat Chhatwal, Alex Kalinowski, Michelle Rogers, Brian Smith</dc:creator>
    </item>
    <item>
      <title>Enhancing Preference-based Linear Bandits via Human Response Time</title>
      <link>https://arxiv.org/abs/2409.05798</link>
      <description>arXiv:2409.05798v2 Announce Type: replace-cross 
Abstract: Interactive preference learning systems present humans with queries as pairs of options; humans then select their preferred choice, allowing the system to infer preferences from these binary choices. While binary choice feedback is simple and widely used, it offers limited information about preference strength. To address this, we leverage human response times, which inversely correlate with preference strength, as complementary information. We introduce a computationally efficient method based on the EZ-diffusion model, combining choices and response times to estimate the underlying human utility function. Theoretical and empirical comparisons with traditional choice-only estimators show that for queries where humans have strong preferences (i.e., "easy" queries), response times provide valuable complementary information and enhance utility estimates. We integrate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that incorporating response times significantly accelerates preference learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05798v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shen Li, Yuyang Zhang, Zhaolin Ren, Claire Liang, Na Li, Julie A. Shah</dc:creator>
    </item>
    <item>
      <title>AI Can Enhance Creativity in Social Networks</title>
      <link>https://arxiv.org/abs/2410.15264</link>
      <description>arXiv:2410.15264v2 Announce Type: replace-cross 
Abstract: Can peer recommendation engines elevate people's creative performances in self-organizing social networks? Answering this question requires resolving challenges in data collection (e.g., tracing inspiration links and psycho-social attributes of nodes) and intervention design (e.g., balancing idea stimulation and redundancy in evolving information environments). We trained a model that predicts people's ideation performances using semantic and network-structural features in an online platform. Using this model, we built SocialMuse, which maximizes people's predicted performances to generate peer recommendations for them. We found treatment networks leveraging SocialMuse outperforming AI-agnostic control networks in several creativity measures. The treatment networks were more decentralized than the control, as SocialMuse increasingly emphasized network-structural features at large network sizes. This decentralization spreads people's inspiration sources, helping inspired ideas stand out better. Our study provides actionable insights into building intelligent systems for elevating creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15264v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raiyan Abdul Baten, Ali Sarosh Bangash, Krish Veera, Gourab Ghoshal, Ehsan Hoque</dc:creator>
    </item>
    <item>
      <title>Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation Scenario</title>
      <link>https://arxiv.org/abs/2410.19072</link>
      <description>arXiv:2410.19072v2 Announce Type: replace-cross 
Abstract: The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is promising, but existing implementations overlook key insights from human-robot interaction (HRI) research. Studies on human-machine teams demonstrate that human perceptions of a machine teammate are critical in governing the machine's performance. Here, we present a mixed factorial design to assess human perceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants were assigned to the role of casualty (CAS) or bystander (BYS) and subjected to three within-subjects conditions based on the MEDEVAC robot's operating mode: autonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each trial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and transporting them to an ambulance exchange point while avoiding an idle bystander. Following each trial, subjects completed a questionnaire measuring their emotional states, perceived safety, and social compatibility with the robot. Results indicate a consistent main effect of operating mode on reported emotional states and perceived safety. Pairwise analyses suggest that the employment of the AF operating mode negatively impacted perceptions along these dimensions. There were no persistent differences between casualty and bystander responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19072v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tyson Jordan, Pranav Pandey, Prashant Doshi, Ramviyas Parasuraman, Adam Goodie</dc:creator>
    </item>
  </channel>
</rss>
