<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Approximation of the Gompertz function with a multilogistic function</title>
      <link>https://arxiv.org/abs/2405.12984</link>
      <description>arXiv:2405.12984v1 Announce Type: new 
Abstract: The paper deals with the comparison of the Gompertz function and the logistic function. We show that the Gompertz function can be approximated with high accuracy by a sum of three logistic functions (multilogistic function). Two of them are increasing and one is decreasing. We use second-order logistic wavelets to estimate the parameters of the multilogistic function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12984v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grzegorz Rzadkowski</dc:creator>
    </item>
    <item>
      <title>On Convergence of the Alternating Directions SGHMC Algorithm</title>
      <link>https://arxiv.org/abs/2405.13140</link>
      <description>arXiv:2405.13140v1 Announce Type: new 
Abstract: We study convergence rates of Hamiltonian Monte Carlo (HMC) algorithms with leapfrog integration under mild conditions on stochastic gradient oracle for the target distribution (SGHMC). Our method extends standard HMC by allowing the use of general auxiliary distributions, which is achieved by a novel procedure of Alternating Directions.
  The convergence analysis is based on the investigations of the Dirichlet forms associated with the underlying Markov chain driving the algorithms. For this purpose, we provide a detailed analysis on the error of the leapfrog integrator for Hamiltonian motions with both the kinetic and potential energy functions in general form. We characterize the explicit dependence of the convergence rates on key parameters such as the problem dimension, functional properties of both the target and auxiliary distributions, and the quality of the oracle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13140v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyadip Ghsoh, Yingdong Lu, Tomasz Nowicki</dc:creator>
    </item>
    <item>
      <title>Max-sliced Wasserstein concentration and uniform ratio bounds of empirical measures on RKHS</title>
      <link>https://arxiv.org/abs/2405.13153</link>
      <description>arXiv:2405.13153v1 Announce Type: new 
Abstract: Optimal transport and the Wasserstein distance $\mathcal{W}_p$ have recently seen a number of applications in the fields of statistics, machine learning, data science, and the physical sciences. These applications are however severely restricted by the curse of dimensionality, meaning that the number of data points needed to estimate these problems accurately increases exponentially in the dimension. To alleviate this problem, a number of variants of $\mathcal{W}_p$ have been introduced. We focus here on one of these variants, namely the max-sliced Wasserstein metric $\overline{\mathcal{W}}_p$. This metric reduces the high-dimensional minimization problem given by $\mathcal{W}_p$ to a maximum of one-dimensional measurements in an effort to overcome the curse of dimensionality. In this note we derive concentration results and upper bounds on the expectation of $\overline{\mathcal{W}}_p$ between the true and empirical measure on unbounded reproducing kernel Hilbert spaces. We show that, under quite generic assumptions, probability measures concentrate uniformly fast in one-dimensional subspaces, at (nearly) parametric rates. Our results rely on an improvement of currently known bounds for $\overline{\mathcal{W}}_p$ in the finite-dimensional case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13153v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Han, Cynthia Rush, Johannes Wiesel</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of FBSDEs with random terminal time</title>
      <link>https://arxiv.org/abs/2405.13266</link>
      <description>arXiv:2405.13266v1 Announce Type: new 
Abstract: This paper investigates the nonparametric estimation of the functional coefficients of the FBSDEs with random terminal time, including the local constant and local linear estimators. We provide complete two-dimensional asymptotics in both the time span and the sampling interval, allowing for the precise characterization of their distribution. Moreover, the empirical likelihood (EL) method to construct the data-driven confidence intervals for these estimators is provided. Some numerical simulations investigate the finite-sample properties of the estimators and compare the performance of the EL method and the conventional method in constructing confidence intervals based on asymptotic normality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13266v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaolin Ji, Chenyao Yu, Linlin Zhu</dc:creator>
    </item>
    <item>
      <title>A stereographic test of spherical uniformity</title>
      <link>https://arxiv.org/abs/2405.13531</link>
      <description>arXiv:2405.13531v1 Announce Type: new 
Abstract: We introduce a test of uniformity for (hyper)spherical data motivated by the stereographic projection. The closed-form expression of the test statistic and its null asymptotic distribution are derived using Gegenbauer polynomials. The power against rotationally symmetric local alternatives is provided, and simulations illustrate the non-null asymptotic results. The stereographic test outperforms other tests in a testing scenario with antipodal dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13531v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alberto Fern\'andez-de-Marcos, Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>The effect of regularization in high dimensional Cox regression</title>
      <link>https://arxiv.org/abs/2405.13690</link>
      <description>arXiv:2405.13690v1 Announce Type: new 
Abstract: We investigate analytically the behaviour of the penalized maximum partial likelihood estimator (PMPLE). Our results are derived for a generic separable regularization, but we focus on the elastic net. This penalization is routinely adopted for survival analysis in the high dimensional regime, where the Maximum Partial Likelihood estimator (no regularization) might not even exist. Previous theoretical results require that the number $s$ of non-zero association coefficients is $O(n^{\alpha})$, with $\alpha \in (0,1)$ and $n$ the sample size. Here we accurately characterize the behaviour of the PMPLE when $s$ is proportional to $n$ via the solution of a system of six non-linear equations that can be easily obtained by fixed point iteration. These equations are derived by means of the replica method and under the assumption that the covariates $\mathbf{X}\in \mathbb{R}^p$ follow a multivariate Gaussian law with covariance $\mathbf{I}_p/p$.
  The solution of the previous equations allows us to investigate the dependency of various metrics of interest and hence their dependency on the ratio $\zeta = p/n$, the fraction of true active components $\nu = s/p$, and the regularization strength. We validate our results by extensive numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13690v1</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Massa</dc:creator>
    </item>
    <item>
      <title>Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods</title>
      <link>https://arxiv.org/abs/2405.13912</link>
      <description>arXiv:2405.13912v1 Announce Type: new 
Abstract: We study the matrix denoising problem of estimating the singular vectors of a rank-$1$ signal corrupted by noise with both column and row correlations. Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal. On top of this, most of the literature has focused on the special case of estimating the left singular vector of the signal when the noise only possesses row correlation (one-sided heteroscedasticity). In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise. We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error. Numerical experiments demonstrate the significant advantage of our theoretically principled method with the state of the art. The proofs draw connections with statistical physics and approximate message passing, departing drastically from standard random matrix theory techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13912v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan Zhang, Marco Mondelli</dc:creator>
    </item>
    <item>
      <title>High-dimensional (Group) Adversarial Training in Linear Regression</title>
      <link>https://arxiv.org/abs/2405.13940</link>
      <description>arXiv:2405.13940v1 Announce Type: new 
Abstract: Adversarial training can achieve robustness against adversarial perturbations and has been widely used in machine learning models. This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under $\ell_\infty$-perturbation in high-dimensional linear regression. It will be shown that the associated convergence rate of prediction error can achieve the minimax rate up to a logarithmic factor in the high-dimensional linear regression on the class of sparse parameters. Additionally, the group adversarial training procedure is analyzed. Compared with classic adversarial training, it will be proved that the group adversarial training procedure enjoys a better prediction error upper bound under certain group-sparsity patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13940v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Xie, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Quantifying Multivariate Graph Dependencies: Theory and Estimation for Multiplex Graphs</title>
      <link>https://arxiv.org/abs/2405.14482</link>
      <description>arXiv:2405.14482v1 Announce Type: new 
Abstract: Multiplex graphs, characterised by their layered structure, exhibit informative interdependencies within layers that are crucial for understanding complex network dynamics. Quantifying the interaction and shared information among these layers is challenging due to the non-Euclidean structure of graphs. Our paper introduces a comprehensive theory of multivariate information measures for multiplex graphs. We introduce graphon mutual information for pairs of graphs and expand this to graphon interaction information for three or more graphs, including their conditional variants. We then define graphon total correlation and graphon dual total correlation, along with their conditional forms, and introduce graphon $O-$information. We discuss and quantify the concepts of synergy and redundancy in graphs for the first time, introduce consistent nonparametric estimators for these multivariate graphon information--theoretic measures, and provide their convergence rates. We also conduct a simulation study to illustrate our theoretical findings and demonstrate the relationship between the introduced measures, multiplex graph structure, and higher--order interdependecies. Real-world applications further show the utility of our estimators in revealing shared information and dependence structures in real-world multiplex graphs. This work not only answers fundamental questions about information sharing across multiple graphs but also sets the stage for advanced pattern analysis in complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14482v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anda Skeja, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>Entrywise error bounds for low-rank approximations of kernel matrices</title>
      <link>https://arxiv.org/abs/2405.14494</link>
      <description>arXiv:2405.14494v1 Announce Type: new 
Abstract: In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14494v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Modell</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference for heat kernel Gaussian processes on manifolds</title>
      <link>https://arxiv.org/abs/2405.13342</link>
      <description>arXiv:2405.13342v1 Announce Type: cross 
Abstract: We develop scalable manifold learning methods and theory, motivated by the problem of estimating manifold of fMRI activation in the Human Connectome Project (HCP). We propose the Fast Graph Laplacian Estimation for Heat Kernel Gaussian Processes (FLGP) in the natural exponential family model. FLGP handles large sample sizes $ n $, preserves the intrinsic geometry of data, and significantly reduces computational complexity from $ \mathcal{O}(n^3) $ to $ \mathcal{O}(n) $ via a novel reduced-rank approximation of the graph Laplacian's transition matrix and truncated Singular Value Decomposition for eigenpair computation. Our numerical experiments demonstrate FLGP's scalability and improved accuracy for manifold learning from large-scale complex data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13342v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhui He, Guoxuan Ma, Jian Kang, Ying Yang</dc:creator>
    </item>
    <item>
      <title>Nonparametric quantile regression for spatio-temporal processes</title>
      <link>https://arxiv.org/abs/2405.13783</link>
      <description>arXiv:2405.13783v1 Announce Type: cross 
Abstract: In this paper, we develop a new and effective approach to nonparametric quantile regression that accommodates ultrahigh-dimensional data arising from spatio-temporal processes. This approach proves advantageous in staving off computational challenges that constitute known hindrances to existing nonparametric quantile regression methods when the number of predictors is much larger than the available sample size. We investigate conditions under which estimation is feasible and of good overall quality and obtain sharp approximations that we employ to devising statistical inference methodology. These include simultaneous confidence intervals and tests of hypotheses, whose asymptotics is borne by a non-trivial functional central limit theorem tailored to martingale differences. Additionally, we provide finite-sample results through various simulations which, accompanied by an illustrative application to real-worldesque data (on electricity demand), offer guarantees on the performance of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13783v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soudeep Deb, Claudia Neves, Subhrajyoty Roy</dc:creator>
    </item>
    <item>
      <title>Extending Kernel Testing To General Designs</title>
      <link>https://arxiv.org/abs/2405.13799</link>
      <description>arXiv:2405.13799v1 Announce Type: cross 
Abstract: Kernel-based testing has revolutionized the field of non-parametric tests through the embedding of distributions in an RKHS. This strategy has proven to be powerful and flexible, yet its applicability has been limited to the standard two-sample case, while practical situations often involve more complex experimental designs. To extend kernel testing to any design, we propose a linear model in the RKHS that allows for the decomposition of mean embeddings into additive functional effects. We then introduce a truncated kernel Hotelling-Lawley statistic to test the effects of the model, demonstrating that its asymptotic distribution is chi-square, which remains valid with its Nystrom approximation. We discuss a homoscedasticity assumption that, although absent in the standard two-sample case, is necessary for general designs. Finally, we illustrate our framework using a single-cell RNA sequencing dataset and provide kernel-based generalizations of classical diagnostic and exploration tools to broaden the scope of kernel testing in any experimental design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13799v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Ozier-Lafontaine, Franck Picard, Bertrand Michel</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Cocycles</title>
      <link>https://arxiv.org/abs/2405.13844</link>
      <description>arXiv:2405.13844v1 Announce Type: cross 
Abstract: Many interventions in causal inference can be represented as transformations. We identify a local symmetry property satisfied by a large class of causal models under such interventions. Where present, this symmetry can be characterized by a type of map called a cocycle, an object that is central to dynamical systems theory. We show that such cocycles exist under general conditions and are sufficient to identify interventional and counterfactual distributions. We use these results to derive cocycle-based estimators for causal estimands and show they achieve semiparametric efficiency under typical conditions. Since (infinitely) many distributions can share the same cocycle, these estimators make causal inference robust to mis-specification by sidestepping superfluous modelling assumptions. We demonstrate both robustness and state-of-the-art performance in several simulations, and apply our method to estimate the effects of 401(k) pension plan eligibility on asset accumulation using a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13844v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>Geometry of rational quasi-independence models as toric fiber products</title>
      <link>https://arxiv.org/abs/2405.13897</link>
      <description>arXiv:2405.13897v1 Announce Type: cross 
Abstract: We investigate the geometry of a family of log-linear statistical models called quasi-independence models. The toric fiber product is useful for understanding the geometry of parameter inference in these models because the maximum likelihood degree is multiplicative under the TFP. We define the coordinate toric fiber product, or cTFP, and give necessary and sufficient conditions under which a quasi-independence model is a cTFP of lower-order models. We show that the vanishing ideal of every 2-way quasi-independence model with ML-degree 1 can be realized as an iterated toric fiber product of linear ideals. We also classify which Lawrence lifts of 2-way quasi-independence models are cTFPs and give a necessary condition under which a $k$-way model has ML-degree 1 using its facial submodels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13897v1</guid>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jane Ivy Coons, Heather A. Harrington, Niharika Chakrabarty Paul</dc:creator>
    </item>
    <item>
      <title>Analysis of Corrected Graph Convolutions</title>
      <link>https://arxiv.org/abs/2405.13987</link>
      <description>arXiv:2405.13987v1 Announce Type: cross 
Abstract: Machine learning for node classification on graphs is a prominent area driven by applications such as recommendation systems. State-of-the-art models often use multiple graph convolutions on the data, as empirical evidence suggests they can enhance performance. However, it has been shown empirically and theoretically, that too many graph convolutions can degrade performance significantly, a phenomenon known as oversmoothing. In this paper, we provide a rigorous theoretical analysis, based on the contextual stochastic block model (CSBM), of the performance of vanilla graph convolution from which we remove the principal eigenvector to avoid oversmoothing. We perform a spectral analysis for $k$ rounds of corrected graph convolutions, and we provide results for partial and exact classification. For partial classification, we show that each round of convolution can reduce the misclassification error exponentially up to a saturation level, after which performance does not worsen. For exact classification, we show that the separability threshold can be improved exponentially up to $O({\log{n}}/{\log\log{n}})$ corrected convolutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13987v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Robert Wang, Aseem Baranwal, Kimon Fountoulakis</dc:creator>
    </item>
    <item>
      <title>FLIPHAT: Joint Differential Privacy for High Dimensional Sparse Linear Bandits</title>
      <link>https://arxiv.org/abs/2405.14038</link>
      <description>arXiv:2405.14038v1 Announce Type: cross 
Abstract: High dimensional sparse linear bandits serve as an efficient model for sequential decision-making problems (e.g. personalized medicine), where high dimensional features (e.g. genomic data) on the users are available, but only a small subset of them are relevant. Motivated by data privacy concerns in these applications, we study the joint differentially private high dimensional sparse linear bandits, where both rewards and contexts are considered as private data. First, to quantify the cost of privacy, we derive a lower bound on the regret achievable in this setting. To further address the problem, we design a computationally efficient bandit algorithm, \textbf{F}orgetfu\textbf{L} \textbf{I}terative \textbf{P}rivate \textbf{HA}rd \textbf{T}hresholding (FLIPHAT). Along with doubling of episodes and episodic forgetting, FLIPHAT deploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a sparse linear regression oracle to ensure both privacy and regret-optimality. We show that FLIPHAT achieves optimal regret up to logarithmic factors. We analyze the regret by providing a novel refined analysis of the estimation error of N-IHT, which is of parallel interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14038v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunrit Chakraborty, Saptarshi Roy, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>A Concentration Inequality for Maximum Mean Discrepancy (MMD)-based Statistics and Its Application in Generative Models</title>
      <link>https://arxiv.org/abs/2405.14051</link>
      <description>arXiv:2405.14051v1 Announce Type: cross 
Abstract: Maximum Mean Discrepancy (MMD) is a probability metric that has found numerous applications in machine learning. In this work, we focus on its application in generative models, including the minimum MMD estimator, Generative Moment Matching Network (GMMN), and Generative Adversarial Network (GAN). In these cases, MMD is part of an objective function in a minimization or min-max optimization problem. Even if its empirical performance is competitive, the consistency and convergence rate analysis of the corresponding MMD-based estimators has yet to be carried out.
  We propose a uniform concentration inequality for a class of Maximum Mean Discrepancy (MMD)-based estimators, that is, a maximum deviation bound of empirical MMD values over a collection of generated distributions and adversarially learned kernels. Here, our inequality serves as an efficient tool in the theoretical analysis for MMD-based generative models. As elaborating examples, we applied our main result to provide the generalization error bounds for the MMD-based estimators in the context of the minimum MMD estimator and MMD GAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14051v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijin Ni, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Building a stable classifier with the inflated argmax</title>
      <link>https://arxiv.org/abs/2405.14064</link>
      <description>arXiv:2405.14064v1 Announce Type: cross 
Abstract: We propose a new framework for algorithmic stability in the context of multiclass classification. In practice, classification algorithms often operate by first assigning a continuous score (for instance, an estimated probability) to each possible label, then taking the maximizer -- i.e., selecting the class that has the highest score. A drawback of this type of approach is that it is inherently unstable, meaning that it is very sensitive to slight perturbations of the training data, since taking the maximizer is discontinuous. Motivated by this challenge, we propose a pipeline for constructing stable classifiers from data, using bagging (i.e., resampling and averaging) to produce stable continuous scores, and then using a stable relaxation of argmax, which we call the "inflated argmax," to convert these scores to a set of candidate labels. The resulting stability guarantee places no distributional assumptions on the data, does not depend on the number of classes or dimensionality of the covariates, and holds for any base classifier. Using a common benchmark data set, we demonstrate that the inflated argmax provides necessary protection against unstable classifiers, without loss of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14064v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake A. Soloff, Rina Foygel Barber, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Testing Quantumness via Photon Statistics for Time-Bin based Quantum Random Number Generators</title>
      <link>https://arxiv.org/abs/2405.14085</link>
      <description>arXiv:2405.14085v1 Announce Type: cross 
Abstract: Randomness is one of the essential components in many fields including cryptography and simulations. Several Quantum Random Number Generator (QRNG) models have been proposed to produce quantum random numbers, which, due to the quantum theory, are more secure than their classical counterparts. However, QRNGs can not produce true random numbers without deterministic classical post-processing. If the underlying distribution of the QRNG is close to a uniform distribution, a small amount of post-processing is sufficient to produce good random numbers retaining quantumness. In this work, we address the randomness and quantumness in the random numbers generated by the QRNGs. We consider two models of QRNGs, which ideally produce random numbers following different distributions (exponential and uniform), and show that, in practice, they are following similar distributions. These empirical photon distributions can be used to test the quantumness of a QRNG. In this letter, we suggest the $\chi^2$ goodness-of-fit to test quantumness, as it is known to be an effective method to test if sample data follows a known distribution. We derive a relation when the underlying sampling distributions of the QRNGs will be $\epsilon$-random. Depending on this relation, a suitable post-processing algorithm can be chosen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14085v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nirupam Basak, Soumya Das, Goutam Paul</dc:creator>
    </item>
    <item>
      <title>Generalised Bayes Linear Inference</title>
      <link>https://arxiv.org/abs/2405.14145</link>
      <description>arXiv:2405.14145v1 Announce Type: cross 
Abstract: Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14145v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lachlan Astfalck, Cassandra Bird, Daniel Williamson</dc:creator>
    </item>
    <item>
      <title>Cumulant-based approximation for fast and efficient prediction for species distribution</title>
      <link>https://arxiv.org/abs/2405.14456</link>
      <description>arXiv:2405.14456v1 Announce Type: cross 
Abstract: Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables. For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences. However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations. To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\gamma$-divergence. Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution. The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy. Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption. This equivalence leads to an highly efficient computational method for estimating species distribution. The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species. The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy. A R package {\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14456v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Osamu Komori, Yusuke Saigusa, Shinto Eguchi, Yasuhiro Kubota</dc:creator>
    </item>
    <item>
      <title>Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem</title>
      <link>https://arxiv.org/abs/2405.14532</link>
      <description>arXiv:2405.14532v1 Announce Type: cross 
Abstract: The Procrustes-Wasserstein problem consists in matching two high-dimensional point clouds in an unsupervised setting, and has many applications in natural language processing and computer vision. We consider a planted model with two datasets $X,Y$ that consist of $n$ datapoints in $\mathbb{R}^d$, where $Y$ is a noisy version of $X$, up to an orthogonal transformation and a relabeling of the data points. This setting is related to the graph alignment problem in geometric models. In this work, we focus on the euclidean transport cost between the point clouds as a measure of performance for the alignment. We first establish information-theoretic results, in the high ($d \gg \log n$) and low ($d \ll \log n$) dimensional regimes. We then study computational aspects and propose the Ping-Pong algorithm, alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation. We give sufficient conditions for the method to retrieve the planted signal after one single step. We provide experimental results to compare the proposed approach with the state-of-the-art method of Grave et al. (2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14532v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Even, Luca Ganassali, Jakob Maier, Laurent Massouli\'e</dc:creator>
    </item>
    <item>
      <title>Efficient Algorithms for the Sensitivities of the Pearson Correlation Coefficient and Its Statistical Significance to Online Data</title>
      <link>https://arxiv.org/abs/2405.14686</link>
      <description>arXiv:2405.14686v1 Announce Type: cross 
Abstract: Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford's online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14686v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Harary</dc:creator>
    </item>
    <item>
      <title>Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models</title>
      <link>https://arxiv.org/abs/2405.14861</link>
      <description>arXiv:2405.14861v1 Announce Type: cross 
Abstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14861v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gen Li, Yuling Yan</dc:creator>
    </item>
    <item>
      <title>Limit results for distributed estimation of invariant subspaces in multiple networks inference and PCA</title>
      <link>https://arxiv.org/abs/2206.04306</link>
      <description>arXiv:2206.04306v4 Announce Type: replace 
Abstract: We study the problem of distributed estimation of the leading singular vectors for a collection of matrices with shared invariant subspaces. In particular we consider an algorithm that first estimates the projection matrices corresponding to the leading singular vectors for each individual matrix, then computes the average of the projection matrices, and finally returns the leading eigenvectors of the sample averages. We show that the algorithm, when applied to (1) parameters estimation for a collection of independent edge random graphs with shared singular vectors but possibly heterogeneous edge probabilities or (2) distributed PCA for independent sub-Gaussian random vectors with spiked covariance structure, yields estimates whose row-wise fluctuations are normally distributed around the rows of the true singular vectors. Leveraging these results we also consider a two-sample test for the null hypothesis that a pair of random graphs have the same edge probabilities and we present a test statistic whose limiting distribution converges to a central (resp. non-central) $\chi^2$ under the null (resp. local alternative) hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04306v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Minh Tang</dc:creator>
    </item>
    <item>
      <title>Posterior contraction and uncertainty quantification for the multivariate spike-and-slab LASSO</title>
      <link>https://arxiv.org/abs/2209.04389</link>
      <description>arXiv:2209.04389v2 Announce Type: replace 
Abstract: We study the asymptotic properties of Deshpande et al.\ (2019)'s multivariate spike-and-slab LASSO (mSSL) procedure for simultaneous variable and covariance selection in the sparse multivariate linear regression problem. In that problem, $q$ correlated responses are regressed onto $p$ covariates and the mSSL works by placing separate spike-and-slab priors on the entries in the matrix of marginal covariate effects and off-diagonal elements in the upper triangle of the residual precision matrix. Under mild assumptions about these matrices, we establish the posterior contraction rate for the mSSL posterior in the asymptotic regime where both $p$ and $q$ diverge with $n.$ By ``de-biasing'' the corresponding MAP estimates, we obtain confidence intervals for each covariate effect and residual partial correlation. In extensive simulation studies, these intervals displayed close-to-nominal frequentist coverage in finite sample settings but tended to be substantially longer than those obtained using a version of the Bayesian bootstrap that randomly re-weights the prior. We further show that the de-biased intervals for individual covariate effects are asymptotically valid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04389v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunyi Shen, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Moment-Based Adjustments of Statistical Inference in High-Dimensional Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2305.17731</link>
      <description>arXiv:2305.17731v4 Announce Type: replace 
Abstract: We developed a statistical inference method applicable to a broad range of generalized linear models (GLMs) in high-dimensional settings, where the number of unknown coefficients scales proportionally with the sample size. Although a pioneering inference method has been developed for logistic regression, which is a specific instance of GLMs, we cannot apply this method directly to other GLMs because of unknown hyper-parameters. In this study, we addressed this limitation by developing a new inference method designed for a certain class of GLMs. Our method is based on the adjustment of asymptotic normality in high dimensions and is feasible in the sense that it is possible even with unknown hyper-parameters. Specifically, we introduce a novel convex loss-based estimator and its associated system, which are essential components of inference. Next, we devise a moment-based method for estimating the system parameters required by the method. Consequently, we construct confidence intervals for GLMs in a high-dimensional regime. We prove that our proposed method has desirable theoretical properties, such as strong consistency and exact coverage probability. Finally, we experimentally confirmed its validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17731v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuma Sawaya, Yoshimasa Uematsu, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Lower Complexity Adaptation for Empirical Entropic Optimal Transport</title>
      <link>https://arxiv.org/abs/2306.13580</link>
      <description>arXiv:2306.13580v3 Announce Type: replace 
Abstract: Entropic optimal transport (EOT) presents an effective and computationally viable alternative to unregularized optimal transport (OT), offering diverse applications for large-scale data analysis. In this work, we derive novel statistical bounds for empirical plug-in estimators of the EOT cost and show that their statistical performance in the entropy regularization parameter $\epsilon$ and the sample size $n$ only depends on the simpler of the two probability measures. For instance, under sufficiently smooth costs this yields the parametric rate $n^{-1/2}$ with factor $\epsilon^{-d/2}$, where $d$ is the minimum dimension of the two population measures. This confirms that empirical EOT also adheres to the lower complexity adaptation principle, a hallmark feature only recently identified for unregularized OT. As a consequence of our theory, we show that the empirical entropic Gromov-Wasserstein distance and its unregularized version for measures on Euclidean spaces also obey this principle. Additionally, we comment on computational aspects and complement our findings with Monte Carlo simulations. Our techniques employ empirical process theory and rely on a dual formulation of EOT over a single function class. Crucial to our analysis is the observation that the entropic cost-transformation of a function class does not increase its uniform metric entropy by much.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13580v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Groppe, Shayan Hundrieser</dc:creator>
    </item>
    <item>
      <title>On the sample complexity of parameter estimation in logistic regression with normal design</title>
      <link>https://arxiv.org/abs/2307.04191</link>
      <description>arXiv:2307.04191v4 Announce Type: replace 
Abstract: The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04191v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Hsu, Arya Mazumdar</dc:creator>
    </item>
    <item>
      <title>Quasi-Maximum Likelihood Estimation of long-memory linear processes</title>
      <link>https://arxiv.org/abs/2310.14711</link>
      <description>arXiv:2310.14711v2 Announce Type: replace 
Abstract: The purpose of this paper is to study the convergence of the quasi-maximum likelihood (QML) estimator for long memory linear processes. We first establish a correspondence between the long-memory linear process representation and the long-memory AR$(\infty)$ process representation. We then establish the almost sure consistency and asymptotic normality of the QML estimator. Numerical simulations illustrate the theoretical results and confirm the good performance of the estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14711v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Marc Bardet (SAMM), Yves Gael Tchabo Mbienkeu (UY1)</dc:creator>
    </item>
    <item>
      <title>Likelihood Geometry of Reflexive Polytopes</title>
      <link>https://arxiv.org/abs/2311.13572</link>
      <description>arXiv:2311.13572v2 Announce Type: replace 
Abstract: We study the problem of maximum likelihood (ML) estimation for statistical models defined by reflexive polytopes. Our focus is on the maximum likelihood degree of these models as an algebraic measure of complexity of the corresponding optimization problem. We compute the ML degrees of all 4319 classes of three-dimensional reflexive polytopes, and observe some surprising behavior in terms of the presence of gaps between ML degrees and degrees of the associated toric varieties. We interpret these drops in the context of discriminants and prove formulas for the ML degree for families of reflexive polytopes, including the hypercube and its dual, the cross polytope, in arbitrary dimension. In particular, we determine a family of embeddings for the $d$-cube that implies ML degree one. Finally, we discuss generalized constructions of families of reflexive polytopes in terms of their ML degrees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13572v2</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Janike Oldekop</dc:creator>
    </item>
    <item>
      <title>Spectral gap bounds for reversible hybrid Gibbs chains</title>
      <link>https://arxiv.org/abs/2312.12782</link>
      <description>arXiv:2312.12782v2 Announce Type: replace 
Abstract: Hybrid Gibbs samplers represent a prominent class of approximated Gibbs algorithms that utilize Markov chains to approximate conditional distributions, with the Metropolis-within-Gibbs algorithm standing out as a well-known example. Despite their widespread use in both statistical and non-statistical applications, very little is known about their convergence properties. This article introduces novel methods for establishing bounds on the convergence rates of hybrid Gibbs samplers. In particular, we examine the convergence characteristics of hybrid random-scan Gibbs and data augmentation algorithms. Our analysis confirms that the absolute spectral gap of a hybrid chain can be bounded based on the absolute spectral gap of the exact Gibbs chain and the absolute spectral gaps of the Markov chains employed for conditional distribution approximations. For application, we study the convergence properties of four practical hybrid Gibbs algorithms: a random-scan Metropolis-within-Gibbs sampler, a hybrid proximal sampler, random-scan Gibbs samplers with block updates, and a hybrid slice sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12782v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Qin, Nianqiao Ju, Guanyang Wang</dc:creator>
    </item>
    <item>
      <title>R\'enyi entropy, R\'enyi divergence and Jensen-R\'enyi information generating functions, and associated properties and estimation</title>
      <link>https://arxiv.org/abs/2401.04418</link>
      <description>arXiv:2401.04418v2 Announce Type: replace 
Abstract: In this paper, we propose R\'enyi information generating function (RIGF) and discuss its various properties. The relation between the RIGF and Shannon entropy of order $q&gt;0$ is established. Several bounds are obtained. The RIGF of escort distribution is also derived. Furthermore, we introduce R\'enyi divergence information generating function (RDIGF) and discuss its effect under monotone transformations. Next, we propose Jensen-R\'enyi information generating function (JRIGF) and establish its properties. In addition, we present non-parametric and parametric estimators of the RIGF. For illustrative purpose, a simulation study is carried out and a real data relating to the failure times of electronic components is analyzed. Finally, a comparison study between the non-parametric and parametric estimators is made in terms of absolute bias and mean square error (MSE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04418v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal, N. Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Universal Graph Compression: Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2006.02643</link>
      <description>arXiv:2006.02643v3 Announce Type: replace-cross 
Abstract: Motivated by the prevalent data science applications of processing large-scale graph data such as social networks and biological networks, this paper investigates lossless compression of data in the form of a labeled graph. Particularly, we consider a widely used random graph model, stochastic block model (SBM), which captures the clustering effects in social networks. An information-theoretic universal compression framework is applied, in which one aims to design a single compressor that achieves the asymptotically optimal compression rate, for every SBM distribution, without knowing the parameters of the SBM. Such a graph compressor is proposed in this paper, which universally achieves the optimal compression rate with polynomial time complexity for a wide class of SBMs. Existing universal compression techniques are developed mostly for stationary ergodic one-dimensional sequences. However, the adjacency matrix of SBM has complex two-dimensional correlations. The challenge is alleviated through a carefully designed transform that converts two-dimensional correlated data into almost i.i.d. submatrices. The sequence of submatrices is then compressed by a Krichevsky--Trofimov compressor, whose length analysis is generalized to identically distributed but arbitrarily correlated sequences. In four benchmark graph datasets, the compressed files from competing algorithms take 2.4 to 27 times the space needed by the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.02643v3</guid>
      <category>cs.IT</category>
      <category>cs.DB</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alankrita Bhatt, Ziao Wang, Chi Wang, Lele Wang</dc:creator>
    </item>
    <item>
      <title>The Spectral Approach to Linear Rational Expectations Models</title>
      <link>https://arxiv.org/abs/2007.13804</link>
      <description>arXiv:2007.13804v5 Announce Type: replace-cross 
Abstract: This paper considers linear rational expectations models in the frequency domain. The paper characterizes existence and uniqueness of solutions to particular as well as generic systems. The set of all solutions to a given system is shown to be a finite dimensional affine space in the frequency domain. It is demonstrated that solutions can be discontinuous with respect to the parameters of the models in the context of non-uniqueness, invalidating mainstream frequentist and Bayesian methods. The ill-posedness of the problem motivates regularized solutions with theoretically guaranteed uniqueness, continuity, and even differentiability properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.13804v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majid M. Al-Sadoon</dc:creator>
    </item>
    <item>
      <title>HMC and underdamped Langevin united in the unadjusted convex smooth case</title>
      <link>https://arxiv.org/abs/2202.00977</link>
      <description>arXiv:2202.00977v5 Announce Type: replace-cross 
Abstract: We consider a family of unadjusted generalized HMC samplers, which includes standard position HMC samplers and discretizations of the underdamped Langevin process. A detailed analysis and optimization of the parameters is conducted in the Gaussian case, which shows an improvement from $1/\kappa$ to $1/\sqrt{\kappa}$ for the convergence rate in terms of the condition number $\kappa$ by using partial velocity refreshment, with respect to classical full refreshments. A similar effect is observed empirically for two related algorithms, namely Metropolis-adjusted gHMC and kinetic piecewise-deterministic Markov processes. Then, a stochastic gradient version of the samplers is considered, for which dimension-free convergence rates are established for log-concave smooth targets over a large range of parameters, gathering in a unified framework previous results on position HMC and underdamped Langevin and extending them to HMC with inertia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.00977v5</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola\"i Gouraud, Pierre Le Bris, Adrien Majka, Pierre Monmarch\'e</dc:creator>
    </item>
    <item>
      <title>Calibrated Model Criticism Using Split Predictive Checks</title>
      <link>https://arxiv.org/abs/2203.15897</link>
      <description>arXiv:2203.15897v3 Announce Type: replace-cross 
Abstract: Checking how well a fitted model explains the data is one of the most fundamental parts of a Bayesian data analysis. However, existing model checking methods suffer from trade-offs between being well-calibrated, automated, and computationally efficient. To overcome these limitations, we propose split predictive checks (SPCs), which combine the ease-of-use and speed of posterior predictive checks with the good calibration properties of predictive checks that rely on model-specific derivations or inference schemes. We develop an asymptotic theory for two types of SPCs: single SPCs and the divided SPCs. Our results demonstrate that they offer complementary strengths. Single SPCs work well with smaller datasets and provide excellent power when there is substantial misspecification, such as when the estimate uncertainty in the test statistic is significantly underestimated. When the sample size is large, divided SPCs can provide better power and are able to detect more subtle form of misspecification. We validate the finite-sample utility of SPCs through extensive simulation experiments in exponential family and hierarchical models, and provide three real-data examples where SPCs offer novel insights and additional flexibility beyond what is available when using posterior predictive checks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.15897v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Li, Jonathan H. Huggins</dc:creator>
    </item>
    <item>
      <title>Efficient Concentration with Gaussian Approximation</title>
      <link>https://arxiv.org/abs/2208.09922</link>
      <description>arXiv:2208.09922v5 Announce Type: replace-cross 
Abstract: Concentration inequalities for the sample mean, like those due to Bernstein and Hoeffding, are valid for any sample size but overly conservative, yielding confidence intervals that are unnecessarily wide. The central limit theorem (CLT) provides asymptotic confidence intervals with optimal width, but these are invalid for all sample sizes. To resolve this tension, we develop new computable concentration inequalities with asymptotically optimal size, finite-sample validity, and sub-Gaussian decay. These bounds enable the construction of efficient confidence intervals with correct coverage for any sample size and efficient empirical Berry-Esseen bounds that require no prior knowledge of the population variance. We derive our inequalities by tightly bounding non-uniform Kolmogorov and Wasserstein distances to a Gaussian using zero-bias couplings and Stein's method of exchangeable pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09922v5</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morgane Austern, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2304.07278</link>
      <description>arXiv:2304.07278v2 Announce Type: replace-cross 
Abstract: This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon inhomogeneous Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}
  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log factor), our algorithm is able to yield $\varepsilon$ accuracy for arbitrarily many reward functions (even when they are adversarially designed), a task commonly dubbed as ``reward-free exploration.'' The novelty of our algorithm design draws on insights from offline RL: the exploration scheme attempts to maximize a critical reward-agnostic quantity that dictates the performance of offline RL, while the policy learning paradigm leverages ideas from sample-optimal offline RL paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07278v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Li, Yuling Yan, Yuxin Chen, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Inference for Rank-Rank Regressions</title>
      <link>https://arxiv.org/abs/2310.15512</link>
      <description>arXiv:2310.15512v2 Announce Type: replace-cross 
Abstract: Slope coefficients in rank-rank regressions are popular measures of intergenerational mobility. In this paper, we first point out two important properties of the OLS estimator in such regressions: commonly used variance estimators do not consistently estimate the asymptotic variance of the OLS estimator and, when the underlying distribution is not continuous, the OLS estimator may be highly sensitive to the way in which ties are handled. Motivated by these findings we derive the asymptotic theory for the OLS estimator in a general rank-rank regression specification without making assumptions about the continuity of the underlying distribution. We then extend the asymptotic theory to other regressions involving ranks that have been used in empirical work. Finally, we apply our new inference methods to three empirical studies. We find that the confidence intervals based on estimators of the correct variance may sometimes be substantially shorter and sometimes substantially longer than those based on commonly used variance estimators. The differences in confidence intervals concern economically meaningful values of mobility and thus may lead to different conclusions when comparing mobility across different regions or countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15512v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Chetverikov, Daniel Wilhelm</dc:creator>
    </item>
    <item>
      <title>Causal Discovery under Latent Class Confounding</title>
      <link>https://arxiv.org/abs/2311.07454</link>
      <description>arXiv:2311.07454v4 Announce Type: replace-cross 
Abstract: An acyclic causal structure can be described using a directed acyclic graph (DAG) with arrows indicating causation. The task of learning this structure from data is known as "causal discovery." Diverse populations or changing environments can sometimes give rise to heterogeneous data. This heterogeneity can be thought of as a mixture model with multiple "sources," each exerting their own distinct signature on the observed variables. From this perspective, the source is a latent common cause for every observed variable. While some methods for causal discovery are able to work around unobserved confounding in special cases, the only known ways to deal with a global confounder (such as a latent class) involve parametric assumptions. Focusing on discrete observables, we demonstrate that globally confounded causal structures can still be identifiable without parametric assumptions, so long as the number of latent classes remains small relative to the size and sparsity of the underlying DAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07454v4</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bijan Mazaheri, Spencer Gordon, Yuval Rabani, Leonard Schulman</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Tests of Group Invariance through Conformal Prediction</title>
      <link>https://arxiv.org/abs/2401.15461</link>
      <description>arXiv:2401.15461v3 Announce Type: replace-cross 
Abstract: We develop anytime-valid tests of invariance under the action of compact groups. The resulting test statistics are optimal in a logarithmic-growth sense. We apply our method to extend recent anytime-valid tests of independence and to construct tests of normality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15461v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyron Lardy, Muriel Felipe P\'erez-Ortiz</dc:creator>
    </item>
    <item>
      <title>Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees</title>
      <link>https://arxiv.org/abs/2405.10289</link>
      <description>arXiv:2405.10289v2 Announce Type: replace-cross 
Abstract: In nonsmooth, nonconvex stochastic optimization, understanding the uniform convergence of subdifferential mappings is crucial for analyzing stationary points of sample average approximations of risk as they approach the population risk. Yet, characterizing this convergence remains a fundamental challenge.
  This work introduces a novel perspective by connecting the uniform convergence of subdifferential mappings to that of subgradient mappings as empirical risk converges to the population risk. We prove that, for stochastic weakly-convex objectives, and within any open set, a uniform bound on the convergence of subgradients -- chosen arbitrarily from the corresponding subdifferential sets -- translates to a uniform bound on the convergence of the subdifferential sets itself, measured by the Hausdorff metric.
  Using this technique, we derive uniform convergence rates for subdifferential sets of stochastic convex-composite objectives. Our results do not rely on key distributional assumptions in the literature, which require the population and finite sample subdifferentials to be continuous in the Hausdorff metric, yet still provide tight convergence rates. These guarantees lead to new insights into the nonsmooth landscapes of such objectives within finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10289v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ruan</dc:creator>
    </item>
  </channel>
</rss>
