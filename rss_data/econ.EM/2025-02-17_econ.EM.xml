<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 04:20:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>High-dimensional censored MIDAS logistic regression for corporate survival forecasting</title>
      <link>https://arxiv.org/abs/2502.09740</link>
      <description>arXiv:2502.09740v1 Announce Type: new 
Abstract: This paper addresses the challenge of forecasting corporate distress, a problem marked by three key statistical hurdles: (i) right censoring, (ii) high-dimensional predictors, and (iii) mixed-frequency data. To overcome these complexities, we introduce a novel high-dimensional censored MIDAS (Mixed Data Sampling) logistic regression. Our approach handles censoring through inverse probability weighting and achieves accurate estimation with numerous mixed-frequency predictors by employing a sparse-group penalty. We establish finite-sample bounds for the estimation error, accounting for censoring, the MIDAS approximation error, and heavy tails. The superior performance of the method is demonstrated through Monte Carlo simulations. Finally, we present an extensive application of our methodology to predict the financial distress of Chinese-listed firms. Our novel procedure is implemented in the R package 'Survivalml'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09740v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Miao, Jad Beyhum, Jonas Striaukas, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>Prioritized Ranking Experimental Design Using Recommender Systems in Two-Sided Platforms</title>
      <link>https://arxiv.org/abs/2502.09806</link>
      <description>arXiv:2502.09806v1 Announce Type: new 
Abstract: Interdependencies between units in online two-sided marketplaces complicate estimating causal effects in experimental settings. We propose a novel experimental design to mitigate the interference bias in estimating the total average treatment effect (TATE) of item-side interventions in online two-sided marketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the recommender system as an instrument for experimentation. TSPR strategically prioritizes items based on their treatment status in the listings displayed to users. We designed TSPR to provide users with a coherent platform experience by ensuring access to all items and a consistent realization of their treatment by all users. We evaluate our experimental design through simulations using a search impression dataset from an online travel agency. Our methodology closely estimates the true simulated TATE, while a baseline item-side estimator significantly overestimates TATE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09806v1</guid>
      <category>econ.EM</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahyar Habibi, Zahra Khanalizadeh, Negar Ziaeian</dc:creator>
    </item>
    <item>
      <title>Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions for Time Series</title>
      <link>https://arxiv.org/abs/2502.10065</link>
      <description>arXiv:2502.10065v1 Announce Type: new 
Abstract: This paper is the first to propose valid inference tools, based on self-normalization, in time series expected shortfall regressions. In doing so, we propose a novel two-step estimator for expected shortfall regressions which is based on convex optimization in both steps (rendering computation easy) and it only requires minimization of quantile losses and squared error losses (methods for both of which are implemented in every standard statistical computing package). As a corollary, we also derive self-normalized inference tools in time series quantile regressions. Extant methods, based on a bootstrap or direct estimation of the long-run variance, are computationally more involved, require the choice of tuning parameters and have serious size distortions when the regression errors are strongly serially dependent. In contrast, our inference tools only require estimates of the quantile regression parameters that are computed on an expanding window and are correctly sized. Simulations show the advantageous finite-sample properties of our methods. Finally, two applications to stock return predictability and to Growth-at-Risk demonstrate the practical usefulness of the developed inference tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10065v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yannick Hoga, Christian Schulz</dc:creator>
    </item>
    <item>
      <title>Residualised Treatment Intensity and the Estimation of Average Partial Effects</title>
      <link>https://arxiv.org/abs/2502.10301</link>
      <description>arXiv:2502.10301v1 Announce Type: new 
Abstract: This paper introduces R-OLS, an estimator for the average partial effect (APE) of a continuous treatment variable on an outcome variable in the presence of non-linear and non-additively separable confounding of unknown form. Identification of the APE is achieved by generalising Stein's Lemma (Stein, 1981), leveraging an exogenous error component in the treatment along with a flexible functional relationship between the treatment and the confounders. The identification results for R-OLS are used to characterize the properties of Double/Debiased Machine Learning (Chernozhukov et al., 2018), specifying the conditions under which the APE is estimated consistently. A novel decomposition of the ordinary least squares estimand provides intuition for these results. Monte Carlo simulations demonstrate that the proposed estimator outperforms existing methods, delivering accurate estimates of the true APE and exhibiting robustness to moderate violations of its underlying assumptions. The methodology is further illustrated through an empirical application to Fetzer (2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10301v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julius Sch\"aper</dc:creator>
    </item>
    <item>
      <title>Comprehensive Causal Machine Learning</title>
      <link>https://arxiv.org/abs/2405.10198</link>
      <description>arXiv:2405.10198v2 Announce Type: replace 
Abstract: Uncovering causal effects in multiple treatment setting at various levels of granularity provides substantial value to decision makers. Comprehensive machine learning approaches to causal effect estimation allow to use a single causal machine learning approach for estimation and inference of causal mean effects for all levels of granularity. Focusing on selection-on-observables, this paper compares three such approaches, the modified causal forest (mcf), the generalized random forest (grf), and double machine learning (dml). It also compares the theoretical properties of the approaches and provides proven theoretical guarantees for the mcf. The findings indicate that dml-based methods excel for average treatment effects at the population level (ATE) and group level (GATE) with few groups, when selection into treatment is not too strong. However, for finer causal heterogeneity, explicitly outcome-centred forest-based approaches are superior. The mcf has three additional benefits: (i) It is the most robust estimator in cases when dml-based approaches underperform because of substantial selection into treatment; (ii) it is the best estimator for GATEs when the number of groups gets larger; and (iii), it is the only estimator that is internally consistent, in the sense that low-dimensional causal ATEs and GATEs are obtained as aggregates of finer-grained causal parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10198v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Michael Lechner, Jana Mareckova</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v3 Announce Type: replace-cross 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v3</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
  </channel>
</rss>
