<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Feb 2026 02:56:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Performance Evaluation of V2X Communication Using Large-Scale Traffic Data</title>
      <link>https://arxiv.org/abs/2602.07244</link>
      <description>arXiv:2602.07244v1 Announce Type: new 
Abstract: Vehicular communication (V2X) technologies are widely regarded as a cornerstone for cooperative and automated driving, yet their large-scale real-world deployment remains limited. As a result, understanding V2X performance under realistic, full-scale traffic conditions continues to be relevant. Most existing performance evaluations rely on synthetic traffic scenarios generated by simulators, which, while useful, may not fully capture the features of real-world traffic. In this paper, we present a large-scale, data-driven evaluation of V2X communication performance using real-world traffic datasets. Vehicle trajectories derived from the Highway Drone (HighD) and Intersection Drone (InD) datasets are converted into simulation-ready formats and coupled with a standardized V2X networking stack to enable message-level performance analysis for entire traffic populations comprising over hundred thousands vehicles across multiple locations. We evaluate key V2X performance indicators, including inter-generation gap, inter-packet gap, packet delivery ratio, and channel busy ratio, across both highway and urban intersection environments. Our results show that cooperative awareness services remain feasible at scale under realistic traffic conditions. In addition, the findings highlight how traffic density, mobility patterns, and communication range influence V2X performance and how synthetic traffic assumptions may overestimate channel congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07244v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Pravin Arockiasamy, Alexey Vinel</dc:creator>
    </item>
    <item>
      <title>Mirage: Transmitting a Video as a Perceptual Illusion for 50,000X Speedup</title>
      <link>https://arxiv.org/abs/2602.07396</link>
      <description>arXiv:2602.07396v1 Announce Type: new 
Abstract: The existing communication framework mainly aims at accurate reconstruction of source signals to ensure reliable transmission. However, this signal-level fidelity-oriented design often incurs high communication overhead and system complexity, particularly in video communication scenarios where mainstream frameworks rely on transmitting visual data itself, resulting in significant bandwidth consumption. To address this issue, we propose a visual data-free communication framework, Mirage, for extremely efficient video transmission while preserving semantic information. Mirage decomposes video content into two complementary components: temporal sequence information capturing motion dynamics and spatial appearance representations describing overall visual structure. Temporal information is preserved through video captioning, while key frames are encoded into compact semantic representations for spatial appearance. These representations are transmitted to the receiver, where videos are synthesized using generative video models. Since no raw visual data is transmitted, Mirage is inherently privacy-preserving. Mirage also supports personalized adaptation across deployment scenarios. The sender, network, and receiver can independently impose constraints on semantic representation, transmission, and generation, enabling flexible trade-offs between efficiency, privacy, control, and perceptual quality. Experimental results in video transmission demonstrate that Mirage achieves up to a 50000X data-level compression speedup over raw video transmission, with gains expected to scale with larger video content sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07396v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junjie Wu, Tianrui Li, Yi Zhang, Ziyuan Yang</dc:creator>
    </item>
    <item>
      <title>NOMA-Assisted Multi-BS MEC Networks for Delay-Sensitive and Computation-Intensive IoT Applications</title>
      <link>https://arxiv.org/abs/2602.07456</link>
      <description>arXiv:2602.07456v1 Announce Type: new 
Abstract: The burgeoning and ubiquitous deployment of the Internet of Things (IoT) landscape struggles with ultra-low latency demands for computation-intensive tasks in massive connectivity scenarios. In this paper, we propose an innovative uplink non-orthogonal multiple access (NOMA)-assisted multi-base station (BS) mobile edge computing (BS-MEC) network tailored for massive IoT connectivity. To fulfill the quality-of-service (QoS) requirements of delay-sensitive and computation-intensive IoT applications, we formulate a joint task offloading, user grouping, and power allocation optimization problem with the overarching objective of minimizing the system's total delay, aiming to address issues of unbalanced subchannel access, inter-group interference, computational load disparities, and device heterogeneity. To effectively tackle this problem, we first reformulate task offloading and user grouping into a non-cooperative game model and propose an exact potential game-based joint decision-making (EPG-JDM) algorithm, which dynamically selects optimal task offloading and subchannel access decisions for each IoT device based on its channel conditions, thereby achieving the Nash Equilibrium. Then, we propose a majorization-minimization (MM)-based power allocation algorithm, which transforms the original subproblem into a tractable convex optimization paradigm. Extensive simulation experiments demonstrate that our proposed EPG-JDM algorithm significantly outperforms state-of-the-art decision-making algorithms and classic heuristic algorithms, yielding performance improvements of up to 19.3% and 14.7% in terms of total delay and power consumption, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07456v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuang Chen, Fengqian Guo, Chang Wu, Mingyu Peng, Hancheng Lu, Chang Wen Chen</dc:creator>
    </item>
    <item>
      <title>LEO Topology Design Under Real-World Deployment Constraints</title>
      <link>https://arxiv.org/abs/2602.07756</link>
      <description>arXiv:2602.07756v1 Announce Type: new 
Abstract: The performance of large-scale Low-Earth-Orbit (LEO) networks, which consist of thousands of satellites interconnected by optical links, is dependent on its network topology. Existing topology designs often assume idealized conditions and do not account for real-world deployment dynamics, such as partial constellation deployment, daily node turnovers, and varying link availability, making them inapplicable to real LEO networks. In this paper, we develop two topology design methods that explicitly operate under real-world deployment constraints: the Long--Short Links (LSL) method, which systematically combines long-distance shortcut links with short-distance local links, and the Simulated Annealing (SA) method, which constructs topologies via stochastic optimization. Evaluated under both full deployment and partial deployment scenarios using 3-months of Starlink data, our methods achieve up to 45% lower average end-to-end delay, 65% fewer hops, and up to $2.3\times$ higher network capacity compared to +Grid. Both methods are designed to handle daily node turnovers by incrementally updating the topology, maintaining good network performance while avoiding costly full reconstruction of the topology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07756v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muaz Ali, Beichuan Zhang</dc:creator>
    </item>
    <item>
      <title>Interference Propagation Analysis for Large-Scale Multi-RIS-Empowered Wireless Communications:An Epidemiological Perspective</title>
      <link>https://arxiv.org/abs/2602.07922</link>
      <description>arXiv:2602.07922v1 Announce Type: new 
Abstract: Reconfigurable intelligent surfaces (RISs) have gained significant attention in recent years due to their ability to control the reflection of radio-frequency signals and reshape the wireless propagation environment. Unlike traditional studies that primarily focus on the advantages of RISs, this paper examines the negative impacts of RISs by investigating interference propagation caused by user mobility in downlink wireless systems. We employ a stochastic geometric model to simulate the locations of base stations and RISs using the Mat\'{e}rn hard core point process, while user locations are modeled with the homogeneous Poisson point process. We derive novel closed-form expressions for the power distributions of the received signal at the users and the interfering signal. Additionally, we present a novel expression for coverage probability and introduce the concept of interference propagation intensity. To characterize the dynamics of interference caused by user mobility, we adopt an epidemiological approach using the susceptible-infected-susceptible model. Finally, crucial factors influencing the propagation of interference are analyzed. Numerical results validate our theoretical analysis and provide suggestions for managing interference propagation in large-scale multi-RIS wireless communication networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07922v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaining Wang, Xueyao Zhang, Bo Yang, Xuelin Cao, Qiang Cheng, Zhiwen Yu, Bin Guo, George C. Alexandropoulos, Kai-Kit Wong, Chan-Byoung Chae, M\'erouane Debbah</dc:creator>
    </item>
    <item>
      <title>Trajectory-Aware Multi-RIS Activation and Configuration: A Riemannian Diffusion Method</title>
      <link>https://arxiv.org/abs/2602.07937</link>
      <description>arXiv:2602.07937v1 Announce Type: new 
Abstract: Reconfigurable intelligent surfaces (RISs) offer a low-cost, energy-efficient means for enhancing wireless coverage. Yet, their inherently programmable reflections may unintentionally amplify interference, particularly in large-scale, multi-RIS-enabled mobile communication scenarios where dense user mobility and frequent line-of-sight overlaps can severely degrade the signal-to-interference-plus-noise ratio (SINR). To address this challenge, this paper presents a novel generative multi-RIS control framework that jointly optimizes the ON/OFF activation patterns of multiple RISs in the smart wireless environment and the phase configurations of the activated RISs based on predictions of multi-user trajectories and interference patterns. We specially design a long short-term memory (LSTM) artificial neural network, enriched with speed and heading features, to forecast multi-user trajectories, thereby enabling reconstruction of future channel state information. To overcome the highly nonconvex nature of the multi-RIS control problem, we develop a Riemannian diffusion model on the torus to generate geometry-consistent phase-configuration, where the reverse diffusion process is dynamically guided by reinforcement learning. We then rigorously derive the optimal ON/OFF states of the metasurfaces by comparing predicted achievable rates under RIS activation and deactivation conditions. Extensive simulations demonstrate that the proposed framework achieves up to 30\% SINR improvement over learning-based control and up to 44\% gain compared with the RIS always-on scheme, while consistently outperforming state-of-the-art baselines across different transmit powers, RIS configurations, and interference densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07937v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaining Wang, Bo Yang, Yusheng Lei, Zhibo Li, Zhiwen Yu, Xuelin Cao, Bin Guo, George C. Alexandropoulos, Dusit Niyato, M\'erouane Debbah, Zhu Han</dc:creator>
    </item>
    <item>
      <title>DHEA-MECD: An Embodied Intelligence-Powered DRL Algorithm for AUV Tracking in Underwater Environments with High-Dimensional Features</title>
      <link>https://arxiv.org/abs/2602.07947</link>
      <description>arXiv:2602.07947v1 Announce Type: new 
Abstract: In recent years, autonomous underwater vehicle (AUV) systems have demonstrated significant potential in complex marine exploration. However, effective AUV-based tracking remains challenging in realistic underwater environments characterized by high-dimensional features, including coupled kinematic states, spatial constraints, time-varying environmental disturbances, etc. To address these challenges, this paper proposes a hierarchical embodied-intelligence (EI) architecture for underwater multi-target tracking with AUVs in complex underwater environments. Built upon this architecture, we introduce the Double-Head Encoder-Attention-based Multi-Expert Collaborative Decision (DHEA-MECD), a novel Deep Reinforcement Learning (DRL) algorithm designed to support efficient and robust multi-target tracking. Specifically, in DHEA-MECD, a Double-Head Encoder-Attention-based information extraction framework is designed to semantically decompose raw sensory observations and explicitly model complex dependencies among heterogeneous features, including spatial configurations, kinematic states, structural constraints, and stochastic perturbations. On this basis, a motion-stage-aware multi-expert collaborative decision mechanism with Top-k expert selection strategy is introduced to support stage-adaptive decision-making. Furthermore, we propose the DHEA-MECD-based underwater multitarget tracking algorithm to enable AUV smart, stable, and anti-interference multi-target tracking. Extensive experimental results demonstrate that the proposed approach achieves superior tracking success rates, faster convergence, and improved motion optimality compared with mainstream DRL-based methods, particularly in complex and disturbance-rich marine environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07947v1</guid>
      <category>cs.NI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Tian, Chuan Lin, Guangjie Han, Chen An, Qian Zhu, Shengzhao Zhu, Zhenyu Wang</dc:creator>
    </item>
    <item>
      <title>NeuroScaler: Towards Energy-Optimal Autoscaling for Container-Based Services</title>
      <link>https://arxiv.org/abs/2602.08191</link>
      <description>arXiv:2602.08191v1 Announce Type: new 
Abstract: Future networks must meet stringent requirements while operating within tight energy and carbon constraints. Current autoscaling mechanisms remain workload-centric and infrastructure-siloed, and are largely unaware of their environmental impact. We present NeuroScaler, an AI-native, energy-efficient, and carbon-aware orchestrator for green cloud and edge networks. NeuroScaler aggregates multi-tier telemetry, from Power Distribution Units (PDUs) through bare-metal servers to virtualized infrastructure with containers managed by Kubernetes, using distinct energy and computing metrics at each tier. It supports several machine learning pipelines that link load, performance, and power. Within this unified observability layer, a model-predictive control policy optimizes energy use while meeting service-level objectives. In a real testbed with production-grade servers supporting real services, NeuroScaler reduces energy consumption by 34.68% compared to the Horizontal Pod Autoscaler (HPA) while maintaining target latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08191v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisson O. Chaves, Rodrigo Moreira, Larissa F. Rodrigues Moreira, Joao Correia, David Santos, Rui Silva, Tiago Barros, Daniel Corujo, Miguel Rocha, Flavio de Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>MonkeyTree: Near-Minimal Congestion for Multi-tenant Training via Migration</title>
      <link>https://arxiv.org/abs/2602.08296</link>
      <description>arXiv:2602.08296v2 Announce Type: new 
Abstract: We present MonkeyTree, the first system to mitigate network congestion in multi-tenant GPU clusters through job-migration based defragmentation rather than network-layer techniques. As cloud operators co-locate ML training jobs on shared, oversubscribed networks, congestion degrades training throughput for over a third of jobs. Prior approaches either rely on routing and flow scheduling--which we show have fundamental limits when traffic exceeds capacity--or require costly full-bisection bandwidth topologies with packet spraying.
  MonkeyTree exploits characteristics of ML training traffic: ring-based collectives generate exactly one cross-rack flow per rack a job spans, making congestion-free placements achievable. The sparse constraint structure admits abundant valid configurations, making them easy to reach with few migrations. Once reached, low fragmentation is self-reinforcing, as new arrivals disturb only a few racks. MonkeyTree formulates defragmentation as an integer linear program that minimizes worker movements, subject to per-rack fragmentation bounds. We prove a tight bound showing any placement can be defragmented to at most two cross-rack fragments per ToR, and extend the formulation to hybrid parallelism with multiple rings per server. Migration is implemented via in-memory checkpoint-and-restore over RDMA, incurring only 9.02 seconds of system overhead end-to-end per worker. We evaluate MonkeyTree using a custom simulator modeling clusters of up to 2,048 H200 GPUs and prototype on a five-node A100 testbed. MonkeyTree improves average job completion time by 14 percent over the next best baseline on a cluster of 1,024 GPUs with a 4:1 oversubscription. With a high 16:1 oversubscription ratio and 2,048 GPUs, MonkeyTree keeps p99 job completion time within 5 percent of ideal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08296v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton A. Zabreyko, Weiyang Wang, Manya Ghobadi</dc:creator>
    </item>
    <item>
      <title>PACC: Protocol-Aware Cross-Layer Compression for Compact Network Traffic Representation</title>
      <link>https://arxiv.org/abs/2602.08331</link>
      <description>arXiv:2602.08331v1 Announce Type: new 
Abstract: Network traffic classification is a core primitive for network security and management, yet it is increasingly challenged by pervasive encryption and evolving protocols. A central bottleneck is representation: hand-crafted flow statistics are efficient but often too lossy, raw-bit encodings can be accurate but are costly, and recent pre-trained embeddings provide transfer but frequently flatten the protocol stack and entangle signals across layers. We observe that real traffic contains substantial redundancy both across network layers and within each layer; existing paradigms do not explicitly identify and remove this redundancy, leading to wasted capacity, shortcut learning, and degraded generalization. To address this, we propose PACC, a redundancy-aware, layer-aware representation framework. PACC treats the protocol stack as multi-view inputs and learns compact layer-wise projections that remain faithful to each layer while explicitly factorizing representations into shared (cross-layer) and private (layer-specific) components. We operationalize these goals with a joint objective that preserves layer-specific information via reconstruction, captures shared structure via contrastive mutual-information learning, and maximizes task-relevant information via supervised losses, yielding compact latents suitable for efficient inference. Across datasets covering encrypted application classification, IoT device identification, and intrusion detection, PACC consistently outperforms feature-engineered and raw-bit baselines. On encrypted subsets, it achieves up to a 12.9% accuracy improvement over nPrint. PACC matches or surpasses strong foundation-model baselines. At the same time, it improves end-to-end efficiency by up to 3.16x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08331v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhaochen Guo, Tianyufei Zhou, Honghao Wang, Ronghua Li, Shinan Liu</dc:creator>
    </item>
    <item>
      <title>Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach</title>
      <link>https://arxiv.org/abs/2602.08456</link>
      <description>arXiv:2602.08456v1 Announce Type: new 
Abstract: Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Service Sets (BSSs) is challenging due to the lack of global state information. In addition, the concurrent operation of multiple agents creates a highly non-stationary environment, often resulting in suboptimal global configurations (e.g., using the maximum possible transmission power by default). To overcome these limitations, this paper introduces a decentralized learning algorithm based on regret-matching, grounded in internal regret minimization. Unlike standard decentralized ``selfish'' approaches that often converge to inefficient Nash Equilibria (NE), internal regret minimization guides competing agents toward Correlated Equilibria (CE), effectively mimicking coordination without explicit communication. Through simulation results, we showcase the superiority of our proposed approach and its ability to reach near-optimal global performance. These results confirm the not-yet-unleashed potential of scalable decentralized solutions and question the need for the heavy signaling overheads and architectural complexity associated with emerging centralized solutions like Multi-Access Point Coordination (MAPC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08456v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesc Wilhelmi, Boris Bellalta, Miguel Casasnovas, Aleksandra Kijanka, Miguel Calvo-Fullana</dc:creator>
    </item>
    <item>
      <title>From Raw Data to Shared 3D Semantics: Task-Oriented Communication for Multi-Robot Collaboration</title>
      <link>https://arxiv.org/abs/2602.08624</link>
      <description>arXiv:2602.08624v1 Announce Type: new 
Abstract: Multi-robot systems (MRS) rely on exchanging raw sensory data to cooperate in complex three-dimensional (3D) environments. However, this strategy often leads to severe communication congestion and high transmission latency, significantly degrading collaboration efficiency. This paper proposes a decentralized task-oriented semantic communication framework for multi-robot collaboration in unknown 3D environments. Each robot locally extracts compact, task-relevant semantics using a lightweight Pixel Difference Network (PiDiNet) with geometric processing. It shares only these semantic updates to build a task-sufficient 3D scene representation that supports cooperative perception, navigation, and object transport. Our numerical results show that the proposed method exhibits a dramatic reduction in communication overhead from $858.6$ Mb to $4.0$ Mb (over $200\times$ compression gain) while improving collaboration efficiency by shortening task completion from $1,054$ to $281$ steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08624v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruibo Xue, Jiedan Tan, Fang Liu, Jingwen Tong, Taotao Wang, Shuoyao Wang</dc:creator>
    </item>
    <item>
      <title>6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks</title>
      <link>https://arxiv.org/abs/2602.08675</link>
      <description>arXiv:2602.08675v1 Announce Type: new 
Abstract: This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T, and the O-RAN Alliance, and organizes them into five standardization-aligned capability categories. Starting from 113,475 scenarios, we generate a balanced pool of 10,000 very-hard multiple-choice questions using task-conditioned prompts that enforce multi-step quantitative reasoning under uncertainty and worst-case regret minimization over multi-turn horizons. After automated filtering and expert human validation, 3,722 questions are retained as a high-confidence evaluation set, while the full pool is released to support training and fine-tuning of 6G-specialized models. Using 6G-Bench, we evaluate 22 foundation models spanning dense and mixture-of-experts architectures, short- and long-context designs (up to 1M tokens), and both open-weight and proprietary systems. Across models, deterministic single-shot accuracy (pass@1) spans a wide range from 0.22 to 0.82, highlighting substantial variation in semantic reasoning capability. Leading models achieve intent and policy reasoning accuracy in the range 0.87--0.89, while selective robustness analysis on reasoning-intensive tasks shows pass@5 values ranging from 0.20 to 0.91. To support open science and reproducibility, we release the 6G-Bench dataset on GitHub: https://github.com/maferrag/6G-Bench</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08675v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah</dc:creator>
    </item>
    <item>
      <title>Rethinking IPv6 Defense: A Unified Edge-Centric Zero-Trust Data-Plane Architecture</title>
      <link>https://arxiv.org/abs/2602.08891</link>
      <description>arXiv:2602.08891v1 Announce Type: new 
Abstract: IPv6 dependability is increasingly inseparable from IPv6 security: Neighbor Discovery (ND), Router Advertisements (RA), and ICMPv6 are essential for correct operation yet expose a broad attack surface for spoofing and flooding. Meanwhile, IPv6's massive address space breaks per-IP reputation and makes many defenses either non-scalable or narrowly scoped (e.g., only internal threats, only RA abuse, or only volumetric floods). We propose a zero-trust edge architecture implemented in a single programmable data-plane pipeline that unifies four modules: external spoofing, internal spoofing, external flooding, and internal flooding. A key design choice is to enforce identity plausibility before rate plausibility: stateless per-packet validation filters spoofed traffic early so that time-window statistics for flooding operate on credible identities. We outline a concrete P4 design (prefix Hop-Limit bands, DAD-anchored address-port bindings, and Count-Min Sketch windowed counting) and evaluate it across a systematic 15-scenario suite spanning single-, dual-, and multi-vector compositions. We report results from a BMv2 prototype and validate the same pipeline on a Netronome NFP-4000 SmartNIC, and we discuss limitations and open directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08891v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Walid Aljoby, Mohammed Alzayani, Md. Kamrul Hossain, Khaled A. Harras</dc:creator>
    </item>
    <item>
      <title>Zero Trust for Multi-RAT IoT: Trust Boundary Management in Heterogeneous Wireless Network Environments</title>
      <link>https://arxiv.org/abs/2602.08989</link>
      <description>arXiv:2602.08989v1 Announce Type: new 
Abstract: The proliferation of Multi-Radio Access Technology, Internet of Things devices, particularly Unmanned Aerial Vehicles operating across LoRaWAN, 5G/4G cellular, Meshtastic mesh, proprietary protocols such as DJI OcuSync, MAVLink telemetry links, Wi-Fi, and satellite, creates a fundamental and hitherto unexamined challenge for Zero Trust Architecture adoption. Each transition between radio access technologies constitutes a trust boundary crossing: the device exits one network trust domain and enters another, potentially invalidating authentication state, device attestation, and contextual trust signals. Current ZTA frameworks assume relatively stable network environments and do not address the trust implications of frequent, dynamic RAT switching in mobile IoT deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08989v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Shelby</dc:creator>
    </item>
    <item>
      <title>Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications</title>
      <link>https://arxiv.org/abs/2602.08242</link>
      <description>arXiv:2602.08242v1 Announce Type: cross 
Abstract: Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08242v1</guid>
      <category>cs.SE</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Hassaan Mughal, Muhammad Bilal</dc:creator>
    </item>
    <item>
      <title>RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks</title>
      <link>https://arxiv.org/abs/2602.08446</link>
      <description>arXiv:2602.08446v1 Announce Type: cross 
Abstract: Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08446v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pouria Arefijamal, Mahdi Ahmadlou, Bardia Safaei, J\"org Henkel</dc:creator>
    </item>
    <item>
      <title>DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce</title>
      <link>https://arxiv.org/abs/2602.08923</link>
      <description>arXiv:2602.08923v1 Announce Type: cross 
Abstract: Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.
  This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.
  We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08923v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Ran Ben Basat</dc:creator>
    </item>
    <item>
      <title>Lightweight Call Signaling and Peer-to-Peer Control of WebRTC Video Conferencing</title>
      <link>https://arxiv.org/abs/2602.08975</link>
      <description>arXiv:2602.08975v1 Announce Type: cross 
Abstract: We present the software architecture and implementation of our web-based multiparty video conference application. It does not use a media server. For call signaling, it either piggybacks on existing push notifications via a lightweight notification server, or utilizes email messages to further remove that server dependency. For conference control and data storage, it creates a peer-to-peer network of the clients participating in the call. Our prototype client web app can be installed as a browser extension, or a progressive web app on desktop and mobile. It uses WebRTC data channels and media streams for the control and media paths in implementing a full featured video conferencing with audio, video, text and screen sharing. The challenges faced and the techniques used in creating our lightweight or serverless system are useful to other low-end WebRTC applications that intend to save cost on server maintenance or paid subscriptions for multiparty video calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08975v1</guid>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kundan Singh</dc:creator>
    </item>
    <item>
      <title>Kugelblitz: Executable, Cost-Aware Design-Space Exploration for Programmable Packet Pipelines</title>
      <link>https://arxiv.org/abs/2305.08435</link>
      <description>arXiv:2305.08435v2 Announce Type: replace 
Abstract: Programmable packet-processing pipelines are a core building block of modern SmartNICs and switches, yet their design requires navigating intertwined trade-offs among program feasibility, hardware cost, and system-level performance. Existing approaches rely on proxy metrics such as stage or ALU count, which often mispredict capability and end-to-end behavior. We present Kugelblitz, a framework for executable, cost-aware design-space exploration of programmable packet pipelines. Kugelblitz decouples packet-processing programs from pipeline architectures and uses compiler-based feasibility checking to prune designs that cannot support target workloads. For feasible architectures, Kugelblitz automatically generates synthesizable RTL, enabling synthesis-backed area and timing estimation and cycle-accurate full-system evaluation with real application workloads. Using representative programs including NAT, firewalling, and an in-network key-value cache, we show that proxy metrics substantially overestimate capability, that performance rankings change under system-level evaluation, and that the cost of supporting richer workloads is highly non-linear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08435v2</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artem Ageev, Antoine Kaufmann</dc:creator>
    </item>
    <item>
      <title>On Resolving Non-Preemptivity in Multitask Scheduling: An Optimal Algorithm in Deterministic and Stochastic Worlds</title>
      <link>https://arxiv.org/abs/2411.06348</link>
      <description>arXiv:2411.06348v2 Announce Type: replace 
Abstract: The efficient scheduling of multi-task jobs across multiprocessor systems has become increasingly critical with the rapid expansion of computational systems. This challenge, known as Multiprocessor Multitask Scheduling (MPMS), is essential for optimizing the performance and scalability of applications in fields such as cloud computing and deep learning. In this paper, we study the MPMS problem under both deterministic and stochastic models, where each job is composed of multiple tasks and can only be completed when all its tasks are finished. We introduce $\mathsf{NP}$-$\mathsf{SRPT}$, a non-preemptive variant of the SRPT algorithm, designed to accommodate scenarios with non-preemptive tasks. Our algorithm achieves a competitive ratio of $\ln \alpha + \beta + 1$ for minimizing response time, where $\alpha$ represents the ratio of the largest to the smallest job workload, and $\beta$ captures the ratio of the largest non-preemptive task workload to the smallest job workload. We further establish that this competitive ratio is order-optimal when the number of processors is fixed. For the stochastic $\mathsf{M}$/$\mathsf{G}$/$\mathsf{N}$ system, we prove that $\mathsf{NP}$-$\mathsf{SRPT}$ achieves asymptotically optimal mean response time as the traffic intensity approaches $1$, assuming task size distribution with finite support. Moreover, the asymptotic optimality extends to infinite task size distributions under mild probabilistic assumptions, including the standard $\mathsf{M}$/$\mathsf{M}$/$\mathsf{N}$ model. Finally, we extend the analysis to the setting of unknown job sizes, proving that non-preemptive adaptations of the $\mathsf{M\text{-}Gittins}$ and $\mathsf{M\text{-}SERPT}$ policies achieve asymptotic optimality and near-optimality, respectively, for a broad class of job size distributions. Experimental results validate the effectiveness of $\mathsf{NP}$-$\mathsf{SRPT}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06348v2</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>math.PR</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Li</dc:creator>
    </item>
    <item>
      <title>Scaling Data Center TCP to Terabits with Laminar</title>
      <link>https://arxiv.org/abs/2504.19058</link>
      <description>arXiv:2504.19058v3 Announce Type: replace 
Abstract: We present Laminar, the first TCP stack that delivers ASIC-class performance and energy efficiency on programmable Reconfigurable Match-Action Table (RMT) pipelines, providing flexibility while retaining standard TCP semantics and POSIX socket compatibility. The key challenge to Laminar is reconciling TCP's complex dependent state updates with RMT's unidirectional, lock-step execution model. To overcome this challenge, Laminar introduces three novel techniques: optimistic concurrency (speculative updates validated downstream), pseudo-segment injection (circular dependency resolution without stalls), and bump-in-the-wire processing (single-pass segment handling). Together, these enable TCP processing, including retransmission, reassembly, flow, and congestion control, as a pipeline of simple match-action operations.
  Our Intel Tofino 2 prototype demonstrates Laminar's scalability to terabit speeds, flexibility, and robustness to network dynamics. Laminar matches RDMA performance and efficiency for both RPC and streaming workloads (including NVMe-oF with SPDK), while maintaining TCP/POSIX compatibility. Laminar saves up to 16 host CPU cores versus state-of-the-art kernel-bypass TCP, while achieving 5$\times$ lower 99.99p tail latency and 2$\times$ better throughput-per-watt for key-value stores. At scale, Laminar drives nearly $1$ Bpps at 20 $\mu$s RPC tail latency. Unlike fixed-function offloads, Laminar supports transport evolution through in-data-path extensions (selective ACKs, congestion control variants, application co-design for shared logs). Finally, Laminar generalizes to FPGA SmartNICs, outperforming ToNIC's monolithic design by $3\times$ under equal timing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19058v3</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajath Shashidhara, Antoine Kaufmann, Simon Peter</dc:creator>
    </item>
    <item>
      <title>UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach</title>
      <link>https://arxiv.org/abs/2511.07366</link>
      <description>arXiv:2511.07366v2 Announce Type: replace 
Abstract: This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07366v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dao Lan Vy Dinh, Anh Nguyen Thi Mai, Hung Tran, Giang Quynh Le Vu, Tu Dac Ho, Zhenni Pan, Vo Nhan Van, Symeon Chatzinotas, Dinh-Hieu Tran</dc:creator>
    </item>
    <item>
      <title>Optimal Oblivious Load-Balancing for Sparse Traffic in Large-Scale Satellite Networks</title>
      <link>https://arxiv.org/abs/2601.02537</link>
      <description>arXiv:2601.02537v4 Announce Type: replace 
Abstract: Oblivious load-balancing in networks involves routing traffic from sources to destinations using predetermined routes independent of the traffic, so that the maximum load on any link in the network is minimized. We investigate oblivious load-balancing schemes for a $N\times N$ torus network under sparse traffic where there are at most $k$ active source-destination pairs. We are motivated by the problem of load-balancing in large-scale LEO satellite networks, which can be modelled as a torus, where the traffic is known to be sparse and localized to certain hotspot areas. We formulate the problem as a linear program and show that no oblivious routing scheme can achieve a worst-case load lower than approximately $\frac{\sqrt{2k}}{4}$ when $1&lt;k \leq N^2/2$ and $\frac{N}{4}$ when $N^2/2\leq k\leq N^2$. Moreover, we demonstrate that the celebrated Valiant Load Balancing scheme is suboptimal under sparse traffic and construct an optimal oblivious load-balancing scheme that achieves the lower bound. Further, we discover a $\sqrt{2}$ multiplicative gap between the worst-case load of a non-oblivious routing and the worst-case load of any oblivious routing. The results can also be extended to general $N\times M$ tori with unequal link capacities along the vertical and horizontal directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02537v4</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rudrapatna Vallabh Ramakanth, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Path to Diversity: A Primer on ISAC-izing Commodity Wi-Fi for Practical Deployments</title>
      <link>https://arxiv.org/abs/2601.12980</link>
      <description>arXiv:2601.12980v2 Announce Type: replace 
Abstract: Integrated Sensing and Communication (ISAC) has emerged as a key paradigm in next-generation wireless networks. While the ubiquity and low cost of commodity Wi-Fi make it an ideal platform for wide-scale sensing, it is the continuous evolution of Wi-Fi standards-towards higher frequency bands, wider bandwidths, and larger antenna arrays-that fundamentally unlocks the physical resources required for high-performance ISAC. To structure this rapidly expanding field, numerous surveys have appeared. However, prevailing literature predominantly adopts a top-down perspective, emphasizing upper-layer applications or deep learning models while treating the physical layer as an opaque abstraction. Consequently, these works often fail to touch the bottom layer of signal formation and lack technical guidance on overcoming the physical barriers that constrain sensing performance. To bridge this gap, this tutorial takes a bottom-up approach, systematically analyzing the sensing gains brought by Wi-Fi advancements through the lens of physical-layer diversity. We organize the framework around four orthogonal dimensions: i) Temporal Diversity addresses synchronization gaps to enable absolute ranging; ii) Frequency Diversity expands the effective bandwidth to sharpen range resolution; iii) Link Diversity leverages distributed topologies and digital feedback to achieve ubiquitous observability; and iv) Spatial Diversity utilizes multi-antenna arrays to combine passive angular discrimination with active directional control. Collectively, these orthogonal dimensions resolve fundamental ambiguities in time, range, and space, bridging physical capabilities with challenging sensing diversities. By synthesizing these dimensions, this tutorial provides a comprehensive guide for "ISAC-izing" commodity Wi-Fi, paving the way for future standardization and robust deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12980v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongbo Wang, Xin Li, Yinghui He, Jingzhi Hu, Mingming Xu, Zhe Chen, Fu Xiao, Jun Luo</dc:creator>
    </item>
    <item>
      <title>Causal Online Learning of Safe Regions in Cloud Radio Access Networks</title>
      <link>https://arxiv.org/abs/2602.05280</link>
      <description>arXiv:2602.05280v2 Announce Type: replace 
Abstract: Cloud radio access networks (RANs) enable cost-effective management of mobile networks by dynamically scaling their capacity on demand. However, deploying adaptive controllers to implement such dynamic scaling in operational networks is challenging due to the risk of breaching service agreements and operational constraints. To mitigate this challenge, we present a novel method for learning the safe operating region of the RAN, i.e., the set of resource allocations and network configurations for which its specification is fulfilled. The method, which we call (C)ausal (O)nline (L)earning, operates in two online phases: an inference phase and an intervention phase. In the first phase, we passively observe the RAN to infer an initial safe region via causal inference and Gaussian process regression. In the second phase, we gradually expand this region through interventional Bayesian learning. We prove that COL ensures that the learned region is safe with a specified probability and that it converges to the full safe region under standard conditions. We experimentally validate COL on a 5G testbed. The results show that COL quickly learns the safe region while incurring low operational cost and being up to 10x more sample-efficient than current state-of-the-art methods for safe learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05280v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kim Hammar, Tansu Alpcan, Emil Lupu</dc:creator>
    </item>
    <item>
      <title>Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning</title>
      <link>https://arxiv.org/abs/2505.00918</link>
      <description>arXiv:2505.00918v5 Announce Type: replace-cross 
Abstract: IoT networks often face conflicting routing goals such as maximizing packet delivery, minimizing delay, and conserving limited battery energy. These priorities can also change dynamically: for example, an emergency alert requires high reliability, while routine monitoring prioritizes energy efficiency to prolong network lifetime. Existing works, including many deep reinforcement learning approaches, are typically centralized and assume static objectives, making them slow to adapt when preferences shift. We propose a dynamic and fully distributed multi-objective Q-learning routing algorithm that learns multiple per-preference Q-tables in parallel and introduces a novel greedy interpolation policy to act near-optimally for unseen preferences without retraining or central coordination. A theoretical analysis further shows that the optimal value function is Lipschitz-continuous in the preference parameter, ensuring that the proposed greedy interpolation policy yields provably near-optimal behavior. Simulations show that our approach adapts in real time to shifting priorities and achieves up to 80-90\% lower energy consumption and more than 2-5x higher cumulative rewards and packet delivery compared to six baseline protocols, under dynamic and distributed settings. Sensitivity analysis across varying preference window lengths confirms that the proposed DPQ framework consistently achieves higher composite reward than all baseline methods, demonstrating robustness to changes in operating conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00918v5</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Vaishnav, Praveen Kumar Donta, Sindri Magn\'usson</dc:creator>
    </item>
    <item>
      <title>Neural Gaussian Radio Fields for Channel Estimation</title>
      <link>https://arxiv.org/abs/2508.11668</link>
      <description>arXiv:2508.11668v3 Announce Type: replace-cross 
Abstract: Accurate channel state information (CSI) is a critical bottleneck in modern wireless networks, with pilot overhead consuming 11\% to 21\% of transmission bandwidth and feedback delays causing severe throughput degradation under mobility. Addressing this requires rethinking how neural fields represent coherent wave phenomena. This work introduces \textit{neural Gaussian radio fields (\textcolor{stanfordred}{nGRF})}, a physics-informed framework that fundamentally reframes neural field design by replacing view-dependent rasterization with direct complex-valued aggregation in 3D space. This approach natively models wave superposition rather than visual occlusion. The architectural shift transforms the learning objective from function-fitting to source-recovery, a well-posed inverse problem grounded in electromagnetic theory. While demonstrated for wireless channel estimation, the core principle of explicit primitive-based fields with physics-constrained aggregation extends naturally to any coherent wave-based domain, including acoustic propagation, seismic imaging, and ultrasound reconstruction. Evaluations show that the inductive bias of \textcolor{stanfordred}{nGRF} achieves 10.9 dB higher prediction SNR than state-of-the-art methods with 220$\times$ faster inference (1.1 ms vs. 242 ms), 18$\times$ lower measurement density, and 180$\times$ faster training. For large-scale outdoor environments where implicit methods fail, \textcolor{stanfordred}{nGRF} achieves 28.32 dB SNR, demonstrating that structured representations supplemented by domain physics can fundamentally outperform generic deep learning architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11668v3</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Umer, Muhammad Ahmed Mohsin, Ahsan Bilal, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions</title>
      <link>https://arxiv.org/abs/2509.23248</link>
      <description>arXiv:2509.23248v2 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) has enabled an emergence of agentic artificial intelligence (AI) with powerful reasoning and autonomous decision-making capabilities. This integration with edge computing has led to the development of Mobile Edge General Intelligence (MEGI), which brings real-time, privacy-preserving reasoning to the network edge. However, deploying LLM-based agentic AI reasoning in MEGI environments poses significant challenges due to the high computational demands of reasoning and the limited resources of edge devices. To address these challenges, we propose a joint optimization framework for efficient LLM reasoning deployment in MEGI. First, we systematically review enhancement methods to identify mechanisms suitable for edge adaptation. Subsequently, we present a distributed framework that synergizes reasoning enhancement via adaptive CoT prompting with scalable deployment through a distributed MoE architecture. An important innovation of this approach involves modeling reasoning depth as a dynamic network resource variable, which is optimized jointly with expert activation and transmission power. This mechanism allows the system to dynamically regulate expert networks and reasoning complexity according to task requirements and device capabilities. Experimental evaluations in mobile edge environments demonstrate that the proposed framework effectively balances reasoning quality and resource efficiency. The results show that with less than one second of additional inference time, both accuracy and latency satisfaction rate can reach 90\%, validating the practical viability of deploying sophisticated LLM reasoning in resource-constrained MEGI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23248v2</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyi Luo, Ruichen Zhang, Xiangwang Hou, Jun Du, Chunxiao Jiang, Yong Ren, Dusit Niyato, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments</title>
      <link>https://arxiv.org/abs/2510.06884</link>
      <description>arXiv:2510.06884v3 Announce Type: replace-cross 
Abstract: Accurate and real-time prediction of wireless channel conditions, particularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a foundational requirement for enabling Ultra-Reliable Low-Latency Communication (URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based or statistical models fail to cope with the spatio-temporal complexities introduced by mobile obstacles and transient interference inherent to smart warehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless Infrastructure for Smart Warehouse using VAE), a novel synergistic deep learning architecture that functions as a lightweight 2D predictive digital twin of the radio environment. Evo-WISVA integrates a memory-augmented Variational Autoencoder (VAE) featuring an Attention-driven Latent Memory Module (LMM) for robust, context-aware spatial feature extraction, with a Convolutional Long Short-Term Memory (ConvLSTM) network for precise temporal forecasting and sequential refinement. The entire pipeline is optimized end-to-end via a joint loss function, ensuring optimal feature alignment between the generative and predictive components. Rigorous experimental evaluation conducted on a high-fidelity ns-3-generated industrial warehouse dataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art baselines, achieving up to a 47.6\% reduction in average reconstruction error. Crucially, the model exhibits exceptional generalization capacity to unseen environments with vastly increased dynamic complexity (up to ten simultaneously moving obstacles) while maintaining amortized computational efficiency essential for real-time deployment. Evo-WISVA establishes a foundational technology for proactive wireless resource management, enabling autonomous optimization and advancing the realization of predictive digital twins in industrial communication networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06884v3</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Gulia, Amlan Ganguly, Michael E. Kuhl, Ehsan Rashedi, Clark Hochgraf</dc:creator>
    </item>
  </channel>
</rss>
