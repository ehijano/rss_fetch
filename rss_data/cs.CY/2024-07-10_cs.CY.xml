<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finding Fake News Websites in the Wild</title>
      <link>https://arxiv.org/abs/2407.07159</link>
      <description>arXiv:2407.07159v1 Announce Type: new 
Abstract: The battle against the spread of misinformation on the Internet is a daunting task faced by modern society. Fake news content is primarily distributed through digital platforms, with websites dedicated to producing and disseminating such content playing a pivotal role in this complex ecosystem. Therefore, these websites are of great interest to misinformation researchers. However, obtaining a comprehensive list of websites labeled as producers and/or spreaders of misinformation can be challenging, particularly in developing countries. In this study, we propose a novel methodology for identifying websites responsible for creating and disseminating misinformation content, which are closely linked to users who share confirmed instances of fake news on social media. We validate our approach on Twitter by examining various execution modes and contexts. Our findings demonstrate the effectiveness of the proposed methodology in identifying misinformation websites, which can aid in gaining a better understanding of this phenomenon and enabling competent entities to tackle the problem in various areas of society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07159v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Araujo Leandro, Cout Joao M. M., Nery Luiz Felipe, Rodrigues Isadora C., Almeida Jussara M., Reis Julio C. S., Benevenuto Fabricio</dc:creator>
    </item>
    <item>
      <title>From Principles to Rules: A Regulatory Approach for Frontier AI</title>
      <link>https://arxiv.org/abs/2407.07300</link>
      <description>arXiv:2407.07300v1 Announce Type: new 
Abstract: Several jurisdictions are starting to regulate frontier artificial intelligence (AI) systems, i.e. general-purpose AI systems that match or exceed the capabilities present in the most advanced systems. To reduce risks from these systems, regulators may require frontier AI developers to adopt safety measures. The requirements could be formulated as high-level principles (e.g. 'AI systems should be safe and secure') or specific rules (e.g. 'AI systems must be evaluated for dangerous model capabilities following the protocol set forth in...'). These regulatory approaches, known as 'principle-based' and 'rule-based' regulation, have complementary strengths and weaknesses. While specific rules provide more certainty and are easier to enforce, they can quickly become outdated and lead to box-ticking. Conversely, while high-level principles provide less certainty and are more costly to enforce, they are more adaptable and more appropriate in situations where the regulator is unsure exactly what behavior would best advance a given regulatory objective. However, rule-based and principle-based regulation are not binary options. Policymakers must choose a point on the spectrum between them, recognizing that the right level of specificity may vary between requirements and change over time. We recommend that policymakers should initially (1) mandate adherence to high-level principles for safe frontier AI development and deployment, (2) ensure that regulators closely oversee how developers comply with these principles, and (3) urgently build up regulatory capacity. Over time, the approach should likely become more rule-based. Our recommendations are based on a number of assumptions, including (A) risks from frontier AI systems are poorly understood and rapidly evolving, (B) many safety practices are still nascent, and (C) frontier AI developers are best placed to innovate on safety practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07300v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Schuett, Markus Anderljung, Alexis Carlier, Leonie Koessler, Ben Garfinkel</dc:creator>
    </item>
    <item>
      <title>Use of social networks to motivate computer-engineering students to participate in self-assessment activities</title>
      <link>https://arxiv.org/abs/2407.07460</link>
      <description>arXiv:2407.07460v1 Announce Type: new 
Abstract: Motivation is essential in the learning process of university students, and teachers should have a wide range of strategies to address this issue. The emergence of social technologies has had a considerable influence in e-learning systems, and a number of experts state that their use is a good method to motivate students and to increase their participation in activities. This study attempts to determine whether social networks and social applications should be viewed as many other tools or whether they can actually provide extra motivation for students to participate. The study compared the percentage of student participation in tasks of self-assessment. The experiments covered three traditional strategies of student motivation and another one in which social networks were used to introduce, explain and deliver the self-assessment tasks. The case with a higher participation was the one in which students obtained a reward from the completion of the activity. Despite this result, the statistical analysis indicated that the use of social networks obtained similar results as a strategy of continuous and regular motivational speeches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07460v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.24846/v23i2y201408</arxiv:DOI>
      <arxiv:journal_reference>Studies in Informatics and Control, ISSN 1220-1766, vol. 23(2), pp. 197-206, June 2014</arxiv:journal_reference>
      <dc:creator>Carlos Guerrero, Antoni Jaume-i-Cap\'o</dc:creator>
    </item>
    <item>
      <title>African Democracy in the Era of Generative Disinformation: Challenges and Countermeasures against AI-Generated Propaganda</title>
      <link>https://arxiv.org/abs/2407.07695</link>
      <description>arXiv:2407.07695v1 Announce Type: new 
Abstract: In light of prominent discourse around the negative implications of generative AI, an emerging area of research is investigating the current and estimated impacts of AI-generated propaganda on African citizens participating in elections. Throughout Africa, there have already been suspected cases of AI-generated propaganda influencing electoral outcomes or precipitating coups in countries like Nigeria, Burkina Faso, and Gabon, underscoring the need for comprehensive research in this domain. This paper aims to highlight the risks associated with the spread of generative AI-driven disinformation within Africa while concurrently examining the roles of government, civil society, academia, and the general public in the responsible development, practical use, and robust governance of AI. To understand how African governments might effectively counteract the impact of AI-generated propaganda, this paper presents case studies illustrating the current usage of generative AI for election-related propaganda in Africa. Subsequently, this paper discusses efforts by fact-checking organisations to mitigate the negative impacts of disinformation, explores the potential for new initiatives to actively engage citizens in literacy efforts to combat disinformation spread, and advocates for increased governmental regulatory measures. Overall, this research seeks to increase comprehension of the potential ramifications of AI-generated propaganda on democratic processes within Africa and propose actionable strategies for stakeholders to address these multifaceted challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07695v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chinasa T. Okolo</dc:creator>
    </item>
    <item>
      <title>Digital Twin sensors in cultural heritage applications</title>
      <link>https://arxiv.org/abs/2407.07750</link>
      <description>arXiv:2407.07750v1 Announce Type: new 
Abstract: The paper concerns the extension of the Heritage Digital Twin Ontology introduced in previous work to describe the reactivity of digital twins used for cultural heritage documentation by including the semantic description of sensors and activators and all the process of interacting with the real world. After analysing previous work on the use of digital twins in cultural heritage, a summary description of the Heritage Digital Twin Ontology is provided, and the existing applications of digital twins to cultural heritage are overviewed, with references to reviews summarizing the large production of scientific contributions on the topic. Then a novel ontology, named Reactive Digital Twin Ontology is described, in which sensors, activators and the decision processes are also semantically described, turning the previous synchronic approach to cultural heritage documentation into a diachronic one. Some case studies exemplify this theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07750v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/s24123978</arxiv:DOI>
      <arxiv:journal_reference>Sensors 2024, 24, 3978</arxiv:journal_reference>
      <dc:creator>Franco Niccolucci, Achille Felicetti</dc:creator>
    </item>
    <item>
      <title>Can ChatGPT Pass a Theory of Computing Course?</title>
      <link>https://arxiv.org/abs/2407.07757</link>
      <description>arXiv:2407.07757v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have had considerable difficulty when prompted with mathematical questions, especially those within theory of computing (ToC) courses. In this paper, we detail two experiments regarding our own ToC course and the ChatGPT LLM. For the first, we evaluated ChatGPT's ability to pass our own ToC course's exams. For the second, we created a database of sample ToC questions and responses to accommodate other ToC offerings' choices for topics and structure. We scored each of ChatGPT's outputs on these questions. Overall, we determined that ChatGPT can pass our ToC course, and is adequate at understanding common formal definitions and answering "simple"-style questions, e.g., true/false and multiple choice. However, ChatGPT often makes nonsensical claims in open-ended responses, such as proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07757v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matei A. Golesteanu, Garrett B. Vowinkel, Ryan E. Dougherty</dc:creator>
    </item>
    <item>
      <title>Learning and Motivational Impact of Game-Based Learning: Comparing Face-to-Face and Online Formats on Computer Science Education</title>
      <link>https://arxiv.org/abs/2407.07762</link>
      <description>arXiv:2407.07762v1 Announce Type: new 
Abstract: Contribution: This article analyzes the learning and motivational impact of teacher-authored educational video games on computer science education and compares its effectiveness in both face-to-face and online (remote) formats. This work presents comparative data and findings obtained from 217 students who played the game in a face-to-face format (control group) and 104 students who played the game in an online format (experimental group). Background: Serious video games have been proven effective at computer science education, however, it is still unknown whether the effectiveness of these games is the same regardless of their format, face-to-face or online. Moreover, the usage of games created through authoring tools has barely been explored. Research Questions: Are teacher-authored educational video games effective in terms of learning and motivation for computer science students? Does the effectiveness of teacher-authored educational video games depend on whether they are used in a face-to-face or online format? Methodology: A quasi-experiment has been conducted by using three instruments (pre-test, post-test, and questionnaire) with the purpose of comparing the effectiveness of game-based learning in face-to-face and online formats. A total of 321 computer science students played a teacher-authored educational video game aimed to learn about software design. Findings: The results reveal that teacher-authored educational video games are highly effective in terms of knowledge acquisition and motivation both in face-to-face and online formats. The results also show that some students' perceptions were more positive when a face-to-face format was used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07762v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TE.2023.3241099</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Education, Volume 66, Issue 4, 2023</arxiv:journal_reference>
      <dc:creator>Daniel L\'opez-Fern\'andez, Aldo Gordillo, Jennifer P\'erez, Edmundo Tovar</dc:creator>
    </item>
    <item>
      <title>Uncovering the Interaction Equation: Quantifying the Effect of User Interactions on Social Media Homepage Recommendations</title>
      <link>https://arxiv.org/abs/2407.07227</link>
      <description>arXiv:2407.07227v1 Announce Type: cross 
Abstract: Social media platforms depend on algorithms to select, curate, and deliver content personalized for their users. These algorithms leverage users' past interactions and extensive content libraries to retrieve and rank content that personalizes experiences and boosts engagement. Among various modalities through which this algorithmically curated content may be delivered, the homepage feed is the most prominent. This paper presents a comprehensive study of how prior user interactions influence the content presented on users' homepage feeds across three major platforms: YouTube, Reddit, and X (formerly Twitter). We use a series of carefully designed experiments to gather data capable of uncovering the influence of specific user interactions on homepage content. This study provides insights into the behaviors of the content curation algorithms used by each platform, how they respond to user interactions, and also uncovers evidence of deprioritization of specific topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07227v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hussam Habib, Ryan Stoldt, Raven Maragh-Lloyd, Brian Ekdale, Rishab Nithyanand</dc:creator>
    </item>
    <item>
      <title>Long-Term Fairness in Sequential Multi-Agent Selection with Positive Reinforcement</title>
      <link>https://arxiv.org/abs/2407.07350</link>
      <description>arXiv:2407.07350v1 Announce Type: cross 
Abstract: While much of the rapidly growing literature on fair decision-making focuses on metrics for one-shot decisions, recent work has raised the intriguing possibility of designing sequential decision-making to positively impact long-term social fairness. In selection processes such as college admissions or hiring, biasing slightly towards applicants from under-represented groups is hypothesized to provide positive feedback that increases the pool of under-represented applicants in future selection rounds, thus enhancing fairness in the long term. In this paper, we examine this hypothesis and its consequences in a setting in which multiple agents are selecting from a common pool of applicants. We propose the Multi-agent Fair-Greedy policy, that balances greedy score maximization and fairness. Under this policy, we prove that the resource pool and the admissions converge to a long-term fairness target set by the agents when the score distributions across the groups in the population are identical. We provide empirical evidence of existence of equilibria under non-identical score distributions through synthetic and adapted real-world datasets. We then sound a cautionary note for more complex applicant pool evolution models, under which uncoordinated behavior by the agents can cause negative reinforcement, leading to a reduction in the fraction of under-represented applicants. Our results indicate that, while positive reinforcement is a promising mechanism for long-term fairness, policies must be designed carefully to be robust to variations in the evolution model, with a number of open issues that remain to be explored by algorithm designers, social scientists, and policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07350v1</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSAIT.2024.3416078</arxiv:DOI>
      <dc:creator>Bhagyashree Puranik, Ozgur Guldogan, Upamanyu Madhow, Ramtin Pedarsani</dc:creator>
    </item>
    <item>
      <title>High-Precision, Fair University Course Scheduling During a Pandemic</title>
      <link>https://arxiv.org/abs/2407.07355</link>
      <description>arXiv:2407.07355v1 Announce Type: cross 
Abstract: Scheduling university courses is extra challenging when classroom capacities are reduced because of social distancing requirements that are implemented in response to a pandemic such as COVID-19. In this work, we propose an expanded taxonomy of course delivery modes, present an integer program, and develop a course scheduling algorithm to enable all course sections -- even the largest -- to have a significant classroom learning component during a pandemic. Our approach is fair by ensuring that a certain fraction of the instruction in every course section occurs in the classroom. Unlike previous studies, we do not allow rotating attendance and instead require simultaneous attendance in which all students in a section meet in 1-5 rooms at the same time but less often than in a normal semester. These mass meetings, which create opportunities for in-person midterm exams and group activities, are scheduled at high precision across all days of the semester rather than a single, repeating week. A fast heuristic algorithm makes the schedule in an hour. Results: We consider the 1834 in-person course sections, 172 classrooms, and 96 days in the fall 2022 semester at [UniversityXYZ]. If average classroom capacity is reduced by 75% due to a pandemic, our approach still allows at least 25% of the instruction in every section, and more than 49% of all instruction across the entire campus, to be in the classroom. Our method also produces excellent results for regular classroom assignment. Managerial implications: An algorithm based on the principles of fairness and simultaneous attendance can significantly improve university course schedules during a pandemic and in normal times. High-precision schedules that prepare a campus for various pandemic possibilities can be created with minimal administrative effort and activated at a moment's notice before or during a semester if an outbreak occurs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07355v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew E. H. Petering, Marshall Khamechian</dc:creator>
    </item>
    <item>
      <title>The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing</title>
      <link>https://arxiv.org/abs/2407.07786</link>
      <description>arXiv:2407.07786v1 Announce Type: cross 
Abstract: Rapid progress in general-purpose AI has sparked significant interest in "red teaming," a practice of adversarial testing originating in military and cybersecurity applications. AI red teaming raises many questions about the human factor, such as how red teamers are selected, biases and blindspots in how tests are conducted, and harmful content's psychological effects on red teamers. A growing body of HCI and CSCW literature examines related practices-including data labeling, content moderation, and algorithmic auditing. However, few, if any, have investigated red teaming itself. This workshop seeks to consider the conceptual and empirical challenges associated with this practice, often rendered opaque by non-disclosure agreements. Future studies may explore topics ranging from fairness to mental health and other areas of potential harm. We aim to facilitate a community of researchers and practitioners who can begin to meet these challenges with creativity, innovation, and thoughtful reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07786v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alice Qian Zhang, Ryland Shaw, Jacy Reese Anthis, Ashlee Milton, Emily Tseng, Jina Suh, Lama Ahmad, Ram Shankar Siva Kumar, Julian Posada, Benjamin Shestakofsky, Sarah T. Roberts, Mary L. Gray</dc:creator>
    </item>
    <item>
      <title>Coordinated Disclosure of Dual-Use Capabilities: An Early Warning System for Advanced AI</title>
      <link>https://arxiv.org/abs/2407.01420</link>
      <description>arXiv:2407.01420v2 Announce Type: replace 
Abstract: Advanced AI systems may be developed which exhibit capabilities that present significant risks to public safety or security. They may also exhibit capabilities that may be applied defensively in a wide set of domains, including (but not limited to) developing societal resilience against AI threats. We propose Coordinated Disclosure of Dual-Use Capabilities (CDDC) as a process to guide early information-sharing between advanced AI developers, US government agencies, and other private sector actors about these capabilities. The process centers around an information clearinghouse (the "coordinator") which receives evidence of dual-use capabilities from finders via mandatory and/or voluntary reporting pathways, and passes noteworthy reports to defenders for follow-up (i.e., further analysis and response). This aims to provide the US government, dual-use foundation model developers, and other actors with an overview of AI capabilities that could significantly impact public safety and security, as well as maximal time to respond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01420v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joe O'Brien, Shaun Ee, Jam Kraprayoon, Bill Anderson-Samways, Oscar Delaney, Zoe Williams</dc:creator>
    </item>
    <item>
      <title>Practical Guide for Causal Pathways and Sub-group Disparity Analysis</title>
      <link>https://arxiv.org/abs/2407.02702</link>
      <description>arXiv:2407.02702v2 Announce Type: replace 
Abstract: In this study, we introduce the application of causal disparity analysis to unveil intricate relationships and causal pathways between sensitive attributes and the targeted outcomes within real-world observational data. Our methodology involves employing causal decomposition analysis to quantify and examine the causal interplay between sensitive attributes and outcomes. We also emphasize the significance of integrating heterogeneity assessment in causal disparity analysis to gain deeper insights into the impact of sensitive attributes within specific sub-groups on outcomes. Our two-step investigation focuses on datasets where race serves as the sensitive attribute. The results on two datasets indicate the benefit of leveraging causal analysis and heterogeneity assessment not only for quantifying biases in the data but also for disentangling their influences on outcomes. We demonstrate that the sub-groups identified by our approach to be affected the most by disparities are the ones with the largest ML classification errors. We also show that grouping the data only based on a sensitive attribute is not enough, and through these analyses, we can find sub-groups that are directly affected by disparities. We hope that our findings will encourage the adoption of such methodologies in future ethical AI practices and bias audits, fostering a more equitable and fair technological landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02702v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farnaz Kohankhaki, Shaina Raza, Oluwanifemi Bamgbose, Deval Pandya, Elham Dolatabadi</dc:creator>
    </item>
    <item>
      <title>The Dark Side of NFTs: A Large-Scale Empirical Study of Wash Trading</title>
      <link>https://arxiv.org/abs/2312.12544</link>
      <description>arXiv:2312.12544v2 Announce Type: replace-cross 
Abstract: NFTs (Non-Fungible Tokens) have seen significant growth since they first captured public attention in 2021. However, the NFT market is plagued by fake transactions and economic bubbles, e.g., NFT wash trading. Wash trading typically refers to a transaction involving the same person or two colluding individuals, and has become a major threat to the NFT ecosystem. Previous studies only detect NFT wash trading from the financial aspect, while the real-world wash trading cases are much more complicated (e.g., not aiming at inflating the market value). There is still a lack of multi-dimension analysis to better understand NFT wash trading. Therefore, we present the most comprehensive study of NFT wash trading, analyzing 8,717,031 transfer events and 3,830,141 sale events from 2,701,883 NFTs. We first optimize the dataset collected via the OpenSea API. Next, we identify three types of NFT wash trading and propose identification algorithms. Our experimental results reveal 824 transfer events and 5,330 sale events (accounting for a total of \$8,857,070.41) and 370 address pairs related to NFT wash trading behaviors, causing a minimum loss of \$3,965,247.13. Furthermore, we provide insights from six aspects, i.e., marketplace design, profitability, NFT project design, payment token, user behavior, and NFT ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12544v2</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijian Chen, Jiachi Chen, Jiangshan Yu, Xiapu Luo, Yanlin Wang</dc:creator>
    </item>
    <item>
      <title>"Can You Play Anything Else?" Understanding Play Style Flexibility in League of Legends</title>
      <link>https://arxiv.org/abs/2402.05865</link>
      <description>arXiv:2402.05865v2 Announce Type: replace-cross 
Abstract: This study investigates the concept of flexibility within League of Legends, a popular online multiplayer game, focusing on the relationship between user adaptability and team success. Utilizing a dataset encompassing players of varying skill levels and play styles, we calculate two measures of flexibility for each player: overall flexibility and temporal flexibility. Our findings suggest that the flexibility of a user is dependent upon a user's preferred play style, and flexibility does impact match outcome. This work also shows that skill level not only indicates how willing a player is to adapt their play style but also how their adaptability changes over time. This paper highlights the duality and balance of specialization versus flexibility, providing insights that can inform strategic planning, collaboration and resource allocation in competitive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05865v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emily Chen, Alexander Bisberg, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>STAR: SocioTechnical Approach to Red Teaming Language Models</title>
      <link>https://arxiv.org/abs/2406.11757</link>
      <description>arXiv:2406.11757v2 Announce Type: replace-cross 
Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11757v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</dc:creator>
    </item>
  </channel>
</rss>
