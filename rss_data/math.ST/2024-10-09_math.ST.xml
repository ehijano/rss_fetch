<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Oct 2024 04:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Functional Extreme-PLS</title>
      <link>https://arxiv.org/abs/2410.05517</link>
      <description>arXiv:2410.05517v1 Announce Type: new 
Abstract: We propose an extreme dimension reduction method extending the Extreme-PLS approach to the case where the covariate lies in a possibly infinite-dimensional Hilbert space. The ideas are partly borrowed from both Partial Least-Squares and Sliced Inverse Regression techniques. As such, the method relies on the projection of the covariate onto a subspace and maximizes the covariance between its projection and the response conditionally to an extreme event driven by a random threshold to capture the tail-information. The covariate and the heavy-tailed response are supposed to be linked through a non-linear inverse single-index model and our goal is to infer the index in this regression framework. We propose a new family of estimators and show its asymptotic consistency with convergence rates under the model. Assuming mild conditions on the noise, most of the assumptions are stated in terms of regular variation unlike the standard literature on SIR and single-index regression. Finally, our results are illustrated on a finite-sample study with synthetic functional data as well as on real data from the financial realm, highlighting the effectiveness of the dimension reduction for estimating extreme risk measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05517v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>St\'ephane Girard, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>With random regressors, least squares inference is robust to correlated errors with unknown correlation structure</title>
      <link>https://arxiv.org/abs/2410.05567</link>
      <description>arXiv:2410.05567v1 Announce Type: new 
Abstract: Linear regression is arguably the most widely used statistical method. With fixed regressors and correlated errors, the conventional wisdom is to modify the variance-covariance estimator to accommodate the known correlation structure of the errors. We depart from the literature by showing that with random regressors, linear regression inference is robust to correlated errors with unknown correlation structure. The existing theoretical analyses for linear regression are no longer valid because even the asymptotic normality of the least-squares coefficients breaks down in this regime. We first prove the asymptotic normality of the t statistics by establishing their Berry-Esseen bounds based on a novel probabilistic analysis of self-normalized statistics. We then study the local power of the corresponding t tests and show that, perhaps surprisingly, error correlation can even enhance power in the regime of weak signals. Overall, our results show that linear regression is applicable more broadly than the conventional theory suggests, and further demonstrate the value of randomization to ensure robustness of inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05567v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zifeng Zhang, Peng Ding, Wen Zhou, Haonan Wang</dc:creator>
    </item>
    <item>
      <title>Statistical inference for highly correlated stationary point processes and noisy bivariate Neyman-Scott processes</title>
      <link>https://arxiv.org/abs/2410.05732</link>
      <description>arXiv:2410.05732v1 Announce Type: new 
Abstract: Motivated by estimating the lead-lag relationships in high-frequency financial data, we propose noisy bivariate Neyman-Scott point processes with gamma kernels (NBNSP-G). NBNSP-G tolerates noises that are not necessarily Poissonian and has an intuitive interpretation. Our experiments suggest that NBNSP-G can explain the correlation of orders of two stocks well. A composite-type quasi-likelihood is employed to estimate the parameters of the model. However, when one tries to prove consistency and asymptotic normality, NBNSP-G breaks the boundedness assumption on the moment density functions commonly assumed in the literature. Therefore, under more relaxed conditions, we show consistency and asymptotic normality for bivariate point process models, which include NBNSP-G. Our numerical simulations also show that the estimator is indeed likely to converge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05732v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Shiotani, Nakahiro Yoshida</dc:creator>
    </item>
    <item>
      <title>A Unifying Framework for Global Gaussianization: Asymptotic Equivalence of Locally Stationary Processes and Bivariate White Noise</title>
      <link>https://arxiv.org/abs/2410.05751</link>
      <description>arXiv:2410.05751v1 Announce Type: new 
Abstract: We consider a general class of statistical experiments, in which an $n$-dimensional centered Gaussian random variable is observed and its covariance matrix is the parameter of interest. The covariance matrix is assumed to be well-approximable in a linear space of lower dimension $K_n$ with eigenvalues uniformly bounded away from zero and infinity. We prove asymptotic equivalence of this experiment and a class of $K_n$-dimensional Gaussian models with informative expectation in Le Cam's sense when $n$ tends to infinity and $K_n$ is allowed to increase moderately in $n$ at a polynomial rate. For this purpose we derive a new localization technique for non-i.i.d. data and a novel high-dimensional Central Limit Law in total variation distance. These results are key ingredients to show asymptotic equivalence between the experiments of locally stationary Gaussian time series and a bivariate Wiener process with the log spectral density as its drift. Therein a novel class of matrices is introduced which generalizes circulant Toeplitz matrices traditionally used for strictly stationary time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05751v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristina Butucea, Alexander Meister, Angelika Rohde</dc:creator>
    </item>
    <item>
      <title>Uniform Convergence Rate of the Nonparametric Estimator for Integrated Diffusion Processes</title>
      <link>https://arxiv.org/abs/2410.05822</link>
      <description>arXiv:2410.05822v1 Announce Type: new 
Abstract: Nonparametric estimation of integrated diffusion processes has been thoroughly studied, primarily focusing on point-wise convergence. This paper firstly obtains the uniform convergence rates of the Nadaraya-Watson estimators for the coefficients of the integrated diffusion processes. We derive the uniform convergence rates over unbounded support under the shrinking observation interval and long time span assumption. While existing literature suggests that the convergence rate for the diffusion coefficient estimator in continuous-time diffusion processes can reach $(\log n/n)^{2/5}$, which corresponds to the minimax lower bound in sup-norm risk for nonparametric estimation with i.i.d. data. However, we find that the diffusion coefficient estimator of integrated diffusion processes can not attain this rate. Our results are necessary tools for specification testing and semiparametric estimation of certain types of diffusion processes and time series, based on nonparametric estimators, with applications in fields such as finance, geology, and physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05822v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaolin Ji, Linlin Zhu</dc:creator>
    </item>
    <item>
      <title>A Riemannian covariance for manifold-valued data</title>
      <link>https://arxiv.org/abs/2410.06164</link>
      <description>arXiv:2410.06164v1 Announce Type: new 
Abstract: The extension of bivariate measures of dependence to non-Euclidean spaces is a challenging problem. The non-linear nature of these spaces makes the generalisation of classical measures of linear dependence (such as the covariance) not trivial. In this paper, we propose a novel approach to measure stochastic dependence between two random variables taking values in a Riemannian manifold, with the aim of both generalising the classical concepts of covariance and correlation and building a connection to Fr\'echet moments of random variables on manifolds. We introduce generalised local measures of covariance and correlation and we show that the latter is a natural extension of Pearson correlation. We then propose suitable estimators for these quantities and we prove strong consistency results. Finally, we demonstrate their effectiveness through simulated examples and a real-world application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06164v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meshal Abuqrais, Davide Pigoli</dc:creator>
    </item>
    <item>
      <title>Maximum likelihood degree of the $\beta$-stochastic blockmodel</title>
      <link>https://arxiv.org/abs/2410.06223</link>
      <description>arXiv:2410.06223v1 Announce Type: new 
Abstract: Log-linear exponential random graph models are a specific class of statistical network models that have a log-linear representation. This class includes many stochastic blockmodel variants. In this paper, we focus on $\beta$-stochastic blockmodels, which combine the $\beta$-model with a stochastic blockmodel. Here, using recent results by Almendra-Hern\'{a}ndez, De Loera, and Petrovi\'{c}, which describe a Markov basis for $\beta$-stochastic block model, we give a closed form formula for the maximum likelihood degree of a $\beta$-stochastic blockmodel. The maximum likelihood degree is the number of complex solutions to the likelihood equations. In the case of the $\beta$-stochastic blockmodel, the maximum likelihood degree factors into a product of Eulerian numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06223v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cashous Bortner, Jennifer Garbett, Elizabeth Gross, Christopher McClain, Naomi Krawzik, Derek Young</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Low-Rank Tensors: Heteroskedasticity, Subgaussianity, and Applications</title>
      <link>https://arxiv.org/abs/2410.06381</link>
      <description>arXiv:2410.06381v1 Announce Type: new 
Abstract: In this paper, we consider inference and uncertainty quantification for low Tucker rank tensors with additive noise in the high-dimensional regime. Focusing on the output of the higher-order orthogonal iteration (HOOI) algorithm, a commonly used algorithm for tensor singular value decomposition, we establish non-asymptotic distributional theory and study how to construct confidence regions and intervals for both the estimated singular vectors and the tensor entries in the presence of heteroskedastic subgaussian noise, which are further shown to be optimal for homoskedastic Gaussian noise. Furthermore, as a byproduct of our theoretical results, we establish the entrywise convergence of HOOI when initialized via diagonal deletion. To further illustrate the utility of our theoretical results, we then consider several concrete statistical inference tasks. First, in the tensor mixed-membership blockmodel, we consider a two-sample test for equality of membership profiles, and we propose a test statistic with consistency under local alternatives that exhibits a power improvement relative to the corresponding matrix test considered in several previous works. Next, we consider simultaneous inference for small collections of entries of the tensor, and we obtain consistent confidence regions. Finally, focusing on the particular case of testing whether entries of the tensor are equal, we propose a consistent test statistic that shows how index overlap results in different asymptotic standard deviations. All of our proposed procedures are fully data-driven, adaptive to noise distribution and signal strength, and do not rely on sample-splitting, and our main results highlight the effect of higher-order structures on estimation relative to the matrix setting. Our theoretical results are demonstrated through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06381v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Anru Zhang</dc:creator>
    </item>
    <item>
      <title>On the maximum likelihood degree for Gaussian graphical models</title>
      <link>https://arxiv.org/abs/2410.07007</link>
      <description>arXiv:2410.07007v1 Announce Type: new 
Abstract: In this paper we revisit the likelihood geometry of Gaussian graphical models. We give a detailed proof that the ML-degree behaves monotonically on induced subgraphs. Furthermore, we complete a missing argument that the ML-degree of the $n$-th cycle is larger than one for any $n\geq 4$, therefore completing the characterization that the only Gaussian graphical models with rational maximum likelihood estimator are the ones corresponding to chordal (decomposable) graphs. Finally, we prove that the formula for the ML-degree of a cycle conjectured by Drton, Sturmfels and Sullivant provides a correct lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07007v1</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Rodica Andreea Dinu, Mateusz Micha{\l}ek, Martin Vodi\v{c}ka</dc:creator>
    </item>
    <item>
      <title>Sampling Spiked Wishart Eigenvalues</title>
      <link>https://arxiv.org/abs/2410.05280</link>
      <description>arXiv:2410.05280v1 Announce Type: cross 
Abstract: Efficient schemes for sampling from the eigenvalues of the Wishart distribution have recently been described for both the uncorrelated central case (where the covariance matrix is $\mathbf{I}$) and the spiked Wishart with a single spike (where the covariance matrix differs from $\mathbf{I}$ in a single entry on the diagonal). Here, we generalize these schemes to the spiked Wishart with an arbitrary number of spikes. This approach also applies to the spiked pseudo-Wishart distribution. We describe how to differentiate this procedure for the purposes of stochastic gradient descent, allowing the fitting of the eigenvalue distribution to some target distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05280v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas G. Brooks</dc:creator>
    </item>
    <item>
      <title>The Breakdown of Gaussian Universality in Classification of High-dimensional Mixtures</title>
      <link>https://arxiv.org/abs/2410.05609</link>
      <description>arXiv:2410.05609v1 Announce Type: cross 
Abstract: The assumption of Gaussian or Gaussian mixture data has been extensively exploited in a long series of precise performance analyses of machine learning (ML) methods, on large datasets having comparably numerous samples and features. To relax this restrictive assumption, subsequent efforts have been devoted to establish "Gaussian equivalent principles" by studying scenarios of Gaussian universality where the asymptotic performance of ML methods on non-Gaussian data remains unchanged when replaced with Gaussian data having the same mean and covariance. Beyond the realm of Gaussian universality, there are few exact results on how the data distribution affects the learning performance.
  In this article, we provide a precise high-dimensional characterization of empirical risk minimization, for classification under a general mixture data setting of linear factor models that extends Gaussian mixtures. The Gaussian universality is shown to break down under this setting, in the sense that the asymptotic learning performance depends on the data distribution beyond the class means and covariances. To clarify the limitations of Gaussian universality in classification of mixture data and to understand the impact of its breakdown, we specify conditions for Gaussian universality and discuss their implications for the choice of loss function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05609v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyi Mai, Zhenyu Liao</dc:creator>
    </item>
    <item>
      <title>Long-Context Linear System Identification</title>
      <link>https://arxiv.org/abs/2410.05690</link>
      <description>arXiv:2410.05690v1 Announce Type: cross 
Abstract: This paper addresses the problem of long-context linear system identification, where the state $x_t$ of a dynamical system at time $t$ depends linearly on previous states $x_s$ over a fixed context window of length $p$. We establish a sample complexity bound that matches the i.i.d. parametric rate up to logarithmic factors for a broad class of systems, extending previous works that considered only first-order dependencies. Our findings reveal a learning-without-mixing phenomenon, indicating that learning long-context linear autoregressive models is not hindered by slow mixing properties potentially associated with extended context windows. Additionally, we extend these results to (i) shared low-rank representations, where rank-regularized estimators improve rates with respect to dimensionality, and (ii) misspecified context lengths in strictly stable systems, where shorter contexts offer statistical advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05690v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>O\u{g}uz Kaan Y\"uksel, Mathieu Even, Nicolas Flammarion</dc:creator>
    </item>
    <item>
      <title>Simple Relative Deviation Bounds for Covariance and Gram Matrices</title>
      <link>https://arxiv.org/abs/2410.05754</link>
      <description>arXiv:2410.05754v1 Announce Type: cross 
Abstract: We provide non-asymptotic, relative deviation bounds for the eigenvalues of empirical covariance and gram matrices in general settings. Unlike typical uniform bounds, which may fail to capture the behavior of smaller eigenvalues, our results provide sharper control across the spectrum. Our analysis is based on a general-purpose theorem that allows one to convert existing uniform bounds into relative ones. The theorems and techniques emphasize simplicity and should be applicable across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05754v1</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Barzilai, Ohad Shamir</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Differentiable Structure Learning</title>
      <link>https://arxiv.org/abs/2410.06163</link>
      <description>arXiv:2410.06163v1 Announce Type: cross 
Abstract: Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers, thus paving the way for differentiable structure learning under general models and losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06163v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Deng, Kevin Bello, Pradeep Ravikumar, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling</title>
      <link>https://arxiv.org/abs/2410.06397</link>
      <description>arXiv:2410.06397v1 Announce Type: cross 
Abstract: Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference.
  In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06397v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew X. Burns, Qingyuan Hou, Michael C. Huang</dc:creator>
    </item>
    <item>
      <title>Adaptive Refinement Protocols for Distributed Distribution Estimation under $\ell^p$-Losses</title>
      <link>https://arxiv.org/abs/2410.06884</link>
      <description>arXiv:2410.06884v1 Announce Type: cross 
Abstract: Consider the communication-constrained estimation of discrete distributions under $\ell^p$ losses, where each distributed terminal holds multiple independent samples and uses limited number of bits to describe the samples. We obtain the minimax optimal rates of the problem in most parameter regimes. An elbow effect of the optimal rates at $p=2$ is clearly identified. To show the optimal rates, we first design estimation protocols to achieve them. The key ingredient of these protocols is to introduce adaptive refinement mechanisms, which first generate rough estimate by partial information and then establish refined estimate in subsequent steps guided by the rough estimate. The protocols leverage successive refinement, sample compression and thresholding methods to achieve the optimal rates in different parameter regimes. The optimality of the protocols is shown by deriving compatible minimax lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06884v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deheng Yuan, Tao Guo, Zhongyi Huang</dc:creator>
    </item>
    <item>
      <title>A Bias-Accuracy-Privacy Trilemma for Statistical Estimation</title>
      <link>https://arxiv.org/abs/2301.13334</link>
      <description>arXiv:2301.13334v3 Announce Type: replace 
Abstract: Differential privacy (DP) is a rigorous notion of data privacy, used for private statistics. The canonical algorithm for differentially private mean estimation is to first clip the samples to a bounded range and then add noise to their empirical mean. Clipping controls the sensitivity and, hence, the variance of the noise that we add for privacy. But clipping also introduces statistical bias. This tradeoff is inherent: we prove that no algorithm can simultaneously have low bias, low error, and low privacy loss for arbitrary distributions.
  Additionally, we show that under strong notions of DP (i.e., pure or concentrated DP), unbiased mean estimation is impossible, even if we assume that the data is sampled from a Gaussian. On the positive side, we show that unbiased mean estimation is possible under a more permissive notion of differential privacy (approximate DP) if we assume that the distribution is symmetric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13334v3</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Kamath, Argyris Mouzakis, Matthew Regehr, Vikrant Singhal, Thomas Steinke, Jonathan Ullman</dc:creator>
    </item>
    <item>
      <title>Cardinality Sparsity: Applications in Matrix-Matrix Multiplications and Machine Learning</title>
      <link>https://arxiv.org/abs/2302.08235</link>
      <description>arXiv:2302.08235v2 Announce Type: replace 
Abstract: High-dimensional data has become ubiquitous across the sciences but presents computational and statistical challenges. A common approach to addressing these challenges is through sparsity. In this paper, we introduce a new concept of sparsity, called cardinality sparsity. Broadly speaking, we define a tensor as sparse if it contains only a small number of unique values. We demonstrate that cardinality sparsity can improve deep learning and tensor regression both statistically and computationally. Along the way, we generalize recent statistical theories in these fields. Most importantly, we show that cardinality sparsity has a strikingly powerful application beyond high-dimensional data analysis: it can significantly speed up matrix-matrix multiplications. For instance, we demonstrate that cardinality sparsity leads to algorithms for binary-matrix multiplication that outperform state-of-the-art algorithms by a substantial margin. Additionally, another crucial aspect of this sparsity is minimizing memory usage. By executing matrix multiplication in the compressed domain, we can significantly lower the amount of memory needed to store the input data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08235v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ali Mohades, Johannes Lederer</dc:creator>
    </item>
    <item>
      <title>On Second-Order Statistics of the Log-Average Periodogram for Gaussian Processes</title>
      <link>https://arxiv.org/abs/2306.10920</link>
      <description>arXiv:2306.10920v2 Announce Type: replace 
Abstract: We present an approximate expression for the covariance of the log-average periodogram for a zero mean stationary Gaussian process. Our findings extend the work of [1] on the covariance of the log-periodogram by additionally taking averaging over adjacent frequencies into account. Moreover, we provide a simple expression for the non-integer moments of a non-central chi-squared distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10920v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karolina Klockmann, Tatyana Krivobokova</dc:creator>
    </item>
    <item>
      <title>Distributional Theory and Statistical Inference for Linear Functions of Eigenvectors with Small Eigengaps</title>
      <link>https://arxiv.org/abs/2308.02480</link>
      <description>arXiv:2308.02480v2 Announce Type: replace 
Abstract: Spectral methods have myriad applications in high-dimensional statistics and data science, and while previous works have primarily focused on $\ell_2$ or $\ell_{2,\infty}$ eigenvector and singular vector perturbation theory, in many settings these analyses fall short of providing the fine-grained guarantees required for various inferential tasks. In this paper we study statistical inference for linear functions of eigenvectors and principal components with a particular emphasis on the setting where gaps between eigenvalues may be extremely small relative to the corresponding spiked eigenvalue, a regime which has been oft-neglected in the literature. It has been previously established that linear functions of eigenvectors and principal components incur a non-negligible bias, so in this work we provide Berry-Esseen bounds for empirical linear forms and their debiased counterparts respectively in the matrix denoising model and the spiked principal component analysis model, both under Gaussian noise. Next, we propose data-driven estimators for the appropriate bias and variance quantities resulting in approximately valid confidence intervals, and we demonstrate our theoretical results through numerical simulations. We further apply our results to obtain distributional theory and confidence intervals for eigenvector entries, for which debiasing is not necessary. Crucially, our proposed confidence intervals and bias-correction procedures can all be computed directly from data without sample-splitting and are asymptotically valid under minimal assumptions on the eigengap and signal strength. Furthermore, our Berry-Esseen bounds clearly reflect the effects of both signal strength and eigenvalue closeness on the estimation and inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02480v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg</dc:creator>
    </item>
    <item>
      <title>On the stability of filtration functions for dependent data with applications to break detection</title>
      <link>https://arxiv.org/abs/2311.11259</link>
      <description>arXiv:2311.11259v2 Announce Type: replace 
Abstract: In this paper, we study the stability of commonly used filtration functions in topological data analysis under small perturbations of the underlying nonrandom point cloud. Relying on these stability results, we then develop a test procedure to detect and determine structural breaks in a sequence of topological data objects obtained from weakly dependent data. The proposed method applies for instance to statistics of persistence diagrams of $\mathbb{R}^d$-valued Bernoulli shift systems under the \v{C}ech or Vietoris-Rips filtration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11259v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Krebs, Daniel Rademacher</dc:creator>
    </item>
    <item>
      <title>An analysis of the noise schedule for score-based generative models</title>
      <link>https://arxiv.org/abs/2402.04650</link>
      <description>arXiv:2402.04650v3 Announce Type: replace 
Abstract: Score-based generative models (SGMs) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target.Recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the Kullback-Leibler (KL) divergence and Wasserstein distances.  Under mild assumptions on the data distribution, we establish an upper bound for the KL divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule. Under additional regularity assumptions, taking advantage of favorable underlying contraction mechanisms, we provide a tighter error bound in Wasserstein distance compared to state-of-the-art results. In addition to being tractable, this upper bound jointly incorporates properties of the target distribution and SGM hyperparameters that need to be tuned during training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04650v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislas Strasman (SU, LPSM), Antonio Ocello (CMAP), Claire Boyer (LPSM), Sylvain Le Corff (LPSM), Vincent Lemaire (LPSM)</dc:creator>
    </item>
    <item>
      <title>A Geometrical Analysis of Kernel Ridge Regression and its Applications</title>
      <link>https://arxiv.org/abs/2404.07709</link>
      <description>arXiv:2404.07709v2 Announce Type: replace 
Abstract: We obtain upper bounds for the estimation error of Kernel Ridge Regression (KRR) for all non-negative regularization parameters, offering a geometric perspective on various phenomena in KRR. As applications: 1. We address the multiple descent problem, unifying the proofs of arxiv:1908.10292 and arxiv:1904.12191 for polynomial kernels and we establish multiple descent for the upper bound of estimation error of KRR under sub-Gaussian design and non-asymptotic regimes. 2. For a sub-Gaussian design vector and for non-asymptotic scenario, we prove a one-sided isomorphic version of the Gaussian Equivalent Conjecture. 3. We offer a novel perspective on the linearization of kernel matrices of non-linear kernel, extending it to the power regime for polynomial kernels. 4. Our theory is applicable to data-dependent kernels, providing a convenient and accurate tool for the feature learning regime in deep learning theory. 5. Our theory extends the results in arxiv:2009.14286 under weak moment assumption.
  Our proof is based on three mathematical tools developed in this paper that can be of independent interest: 1. Dvoretzky-Milman theorem for ellipsoids under (very) weak moment assumptions. 2. Restricted Isomorphic Property in Reproducing Kernel Hilbert Spaces with embedding index conditions. 3. A concentration inequality for finite-degree polynomial kernel functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07709v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Gavrilopoulos, Guillaume Lecu\'e, Zong Shang</dc:creator>
    </item>
    <item>
      <title>The extended Ville's inequality for nonintegrable nonnegative supermartingales</title>
      <link>https://arxiv.org/abs/2304.01163</link>
      <description>arXiv:2304.01163v3 Announce Type: replace-cross 
Abstract: Following the initial work by Robbins, we rigorously present an extended theory of nonnegative supermartingales, requiring neither integrability nor finiteness. In particular, we derive a key maximal inequality foreshadowed by Robbins, which we call the extended Ville's inequality, that strengthens the classical Ville's inequality (for integrable nonnegative supermartingales), and also applies to our nonintegrable setting. We derive an extension of the method of mixtures, which applies to $\sigma$-finite mixtures of our extended nonnegative supermartingales. We present some implications of our theory for sequential statistics, such as the use of improper mixtures (priors) in deriving nonparametric confidence sequences and (extended) e-processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01163v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Statistical inference of convex order by Wasserstein projection</title>
      <link>https://arxiv.org/abs/2406.02840</link>
      <description>arXiv:2406.02840v2 Announce Type: replace-cross 
Abstract: Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention, convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency and concentration results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error of our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. With proper choices of families of distributions, we further attain that the power of the proposed test increases to one as the number of samples grows to infinity. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Experiments based on synthetic data sets illuminate the success of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02840v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Young-Heon Kim, Yuanlong Ruan, Andrew Warren</dc:creator>
    </item>
    <item>
      <title>Generalized FGM dependence: Geometrical representation and convex bounds on sums</title>
      <link>https://arxiv.org/abs/2406.10648</link>
      <description>arXiv:2406.10648v2 Announce Type: replace-cross 
Abstract: Building on the one-to-one relationship between generalized FGM copulas and multivariate Bernoulli distributions, we prove that the class of multivariate distributions with generalized FGM copulas is a convex polytope. Therefore, we find sharp bounds in this class for many aggregate risk measures, such as value-at-risk, expected shortfall, and entropic risk measure, by enumerating their values on the extremal points of the convex polytope. This is infeasible in high dimensions. We overcome this limitation by considering the aggregation of identically distributed risks with generalized FGM copula specified by a common parameter $p$. In this case, the analogy with the geometrical structure of the class of Bernoulli distribution allows us to provide sharp analytical bounds for convex risk measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10648v2</guid>
      <category>q-fin.MF</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\'el\`ene Cossette, Etienne Marceau, Alessandro Mutti, Patrizia Semeraro</dc:creator>
    </item>
    <item>
      <title>Probabilistic Conformal Prediction with Approximate Conditional Validity</title>
      <link>https://arxiv.org/abs/2407.01794</link>
      <description>arXiv:2407.01794v2 Announce Type: replace-cross 
Abstract: We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $P_{Y \mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide only a marginal coverage guarantee. In contrast, our approach extends these frameworks to achieve approximately conditional coverage, which is crucial for many practical applications. Our prediction sets adapt to the behavior of the predictive distribution, making them effective even under high heteroscedasticity. While exact conditional guarantees are infeasible without assumptions on the underlying data distribution, we derive non-asymptotic bounds that depend on the total variation distance of the conditional distribution and its estimate. Using extensive simulations, we show that our method consistently outperforms existing approaches in terms of conditional coverage, leading to more reliable statistical inference in a variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01794v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Plassier, Alexander Fishkov, Mohsen Guizani, Maxim Panov, Eric Moulines</dc:creator>
    </item>
  </channel>
</rss>
