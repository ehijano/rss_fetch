<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jul 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Heterogeneity-Aware Regression with Nonparametric Estimation and Structured Selection for Hospital Readmission Prediction</title>
      <link>https://arxiv.org/abs/2507.06388</link>
      <description>arXiv:2507.06388v1 Announce Type: new 
Abstract: Readmission prediction is a critical but challenging clinical task, as the inherent relationship between high-dimensional covariates and readmission is complex and heterogeneous. Despite this complexity, models should be interpretable to aid clinicians in understanding an individual's risk prediction. Readmissions are often heterogeneous, as individuals hospitalized for different reasons, particularly across distinct clinical diagnosis groups, exhibit materially different subsequent risks of readmission. To enable flexible yet interpretable modeling that accounts for patient heterogeneity, we propose a novel hierarchical-group structure kernel that uses sparsity-inducing kernel summation for variable selection. Specifically, we design group-specific kernels that vary across clinical groups, with the degree of variation governed by the underlying heterogeneity in readmission risk; when heterogeneity is minimal, the group-specific kernels naturally align, approaching a shared structure across groups. Additionally, by allowing variable importance to adapt across interactions, our approach enables more precise characterization of higher-order effects, improving upon existing methods that capture nonlinear and higher-order interactions via functional ANOVA. Extensive simulations and a hematologic readmission dataset (n=18,096) demonstrate superior performance across subgroups of patients (AUROC, PRAUC) over the lasso and XGBoost. Additionally, our model provides interpretable insights into variable importance and group heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06388v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Wang, Angela Bailey, Christopher Tignanelli, Jared D. Huling</dc:creator>
    </item>
    <item>
      <title>Differential Equation-Constrained Local Regression for Data with Sparse Design</title>
      <link>https://arxiv.org/abs/2507.06409</link>
      <description>arXiv:2507.06409v1 Announce Type: new 
Abstract: Local polynomial regression of order one or higher often performs poorly in areas with sparse data. In contrast, local constant regression tends to be more robust in these regions, although it is generally the least accurate approach, especially near the boundaries of the data. Incorporating information from differential equations, which may approximately or exactly hold, is one way of extending the sparse design capacity of local constant regression while reducing bias and variance. A nonparametric regression method that exploits first-order differential equations is studied in this paper and applied to noisy mouse tumour growth data. Asymptotic biases and variances of kernel estimators using Taylor polynomials with different degrees are discussed. Model comparison is performed for different estimators through simulation studies under various scenarios that simulate exponential-type growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06409v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunlei Ge, W. John Braun</dc:creator>
    </item>
    <item>
      <title>From Radar to Risk: Building a High-Resolution Hail Database for Austria And Estimating Risk Through the Integration of Distributional Neural Networks into the Metastatistical Framework</title>
      <link>https://arxiv.org/abs/2507.06429</link>
      <description>arXiv:2507.06429v1 Announce Type: new 
Abstract: This study makes significant contributions to the understanding of hail climatology in Austria.
  First, it introduces a comprehensive database of hailstone sizes, constructed from three-dimensional radar data spanning 2009 to 2022 and calibrated by approximately 5000 verified hail reports.
  The database serves as foundation for describing the short-term climatology of hail and provides the data necessary for estimating hail risk maps with enhanced spatial resolution and quality.
  Second, the study enables the spatio-temporal metastitical extreme value distribution (TMEVD) to feature return levels of up to 30 years on a high-resolution grid of 1km x 1km.
  Key advancements include the adaptation of the TMEVD, which now incorporates atmospheric input variables for robust estimations in data-sparse regions.
  Additionally, this paper presents a novel methodological approach that utilizes a distributional neural network, tailored with innovative sample weighting to efficiently handle the increased computational demands and complexities associated with modeling the distribution parameters.
  Together, these contributions provide a valuable resource for future research and risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06429v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregor Ehrensperger, Vera Katharina Meyer, Marc-Andr\'e Falkensteiner, Tobias Hell</dc:creator>
    </item>
    <item>
      <title>Determining vaccine responders in the presence of baseline immunity using single-cell assays and paired control samples</title>
      <link>https://arxiv.org/abs/2507.06451</link>
      <description>arXiv:2507.06451v1 Announce Type: new 
Abstract: A key objective in vaccine studies is to evaluate vaccine-induced immunogenicity and determine whether participants have mounted a response to the vaccine. Cellular immune responses are essential for assessing vaccine-induced immunogenicity, and single-cell assays, such as intracellular cytokine staining (ICS) are commonly employed to profile individual immune cell phenotypes and the cytokines they produce after stimulation. In this article, we introduce a novel statistical framework for identifying vaccine responders using ICS data collected before and after vaccination. This framework incorporates paired control data to account for potential unintended variations between assay runs, such as batch effects, that could lead to misclassification of participants as vaccine responders. To formally integrate paired control data for accounting for assay variation across different time points (i.e., before and after vaccination), our proposed framework calculates and reports two p-values, both adjusting for paired control data but in distinct ways: (i) the maximally adjusted p-value, which applies the most conservative adjustment to the unadjusted p-value, ensuring validity over all plausible batch effects consistent with the paired control samples' data, and (ii) the minimally adjusted p-value, which imposes only the minimal adjustment to the unadjusted p-value, such that the adjusted p-value cannot be falsified by the paired control samples' data. We apply this framework to analyze ICS data collected at baseline and 4 weeks post-vaccination from the COVID-19 Prevention Network (CoVPN) 3008 study. Our analysis helps address two clinical questions: 1) which participants exhibited evidence of an incident Omicron infection, and 2) which participants showed vaccine-induced T cell responses against the Omicron BA.4/5 Spike protein.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06451v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Siyu Heng, Asa Tapley, Stephen De Rosa, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>A powerful transformation of quantitative responses for biobank-scale association studies</title>
      <link>https://arxiv.org/abs/2507.06496</link>
      <description>arXiv:2507.06496v1 Announce Type: new 
Abstract: In linear regression models with non-Gaussian errors, transformations of the response variable are widely used in a broad range of applications. Motivated by various genetic association studies, transformation methods for hypothesis testing have received substantial interest. In recent years, the rise of biobank-scale genetic studies, which feature a vast number of participants that could be around half a million, spurred the need for new transformation methods that are both powerful for detecting weak genetic signals and computationally efficient for large-scale data. In this work, we propose a novel transformation method that leverages the information of the error density. This transformation leads to locally most powerful tests and therefore has strong power for detecting weak signals. To make the computation scalable to biobank-scale studies, we harnessed the nature of weak genetic signals and proposed a consistent and computationally efficient estimator of the transformation function. Through extensive simulations and a gene-based analysis of spirometry traits from the UK Biobank, we validate that our approach maintains stringent control over type I error rates and significantly enhances statistical power over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06496v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yaowu Liu, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Generalized Nonlinear Models Offer Basis Free SINDy With Model Uncertainty</title>
      <link>https://arxiv.org/abs/2507.06776</link>
      <description>arXiv:2507.06776v1 Announce Type: new 
Abstract: Sparse Identification of Nonlinear Dynamics (SINDy) has become a standard methodology for inferring governing equations of dynamical systems from observed data using statistical modeling. However, classical SINDy approaches rely on predefined libraries of candidate functions to model nonlinearities, which limits flexibility and excludes robust uncertainty quantification. This paper proposes Bayesian Generalized Nonlinear Models (BGNLMs) as a principled alternative for more flexible statistical modeling. BGNLMs employ spike-and-slab priors combined with binary inclusion indicators to automatically discover relevant nonlinearities without predefined basis functions. Moreover, BGNLMs quantify uncertainty in selected bases and final model predictions, enabling robust exploration of the model space. In this paper, the BGNLM framework is applied to several three-dimensional (3D) SINDy problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06776v1</guid>
      <category>stat.ME</category>
      <category>math.DS</category>
      <category>stat.CO</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>https://iwsm2025.ie/wp-content/uploads/2025/07/IWSM2025_Limerick_Proceedings.pdf</arxiv:journal_reference>
      <dc:creator>Aliaksandr Hubin</dc:creator>
    </item>
    <item>
      <title>Bayesian Bootstrap-based Gaussian Copula Model for Mixed Data with High Missing Rates</title>
      <link>https://arxiv.org/abs/2507.06785</link>
      <description>arXiv:2507.06785v1 Announce Type: new 
Abstract: Missing data is a common issue in various fields such as medicine, social sciences, and natural sciences, and it poses significant challenges for accurate statistical analysis. Although numerous imputation methods have been proposed to address this issue, many of them fail to adequately capture the complex dependency structure among variables. To overcome this limitation, models based on the Gaussian copula framework have been introduced. However, most existing copula-based approaches do not account for the uncertainty in the marginal distributions, which can lead to biased marginal estimates and degraded performance, especially under high missingness rates.
  In this study, we propose a Bayesian bootstrap-based Gaussian Copula model (BBGC) that explicitly incorporates uncertainty in the marginal distributions of each variable. The proposed BBGC combines the flexible dependency modeling capability of the Gaussian copula with the Bayesian uncertainty quantification of marginal cumulative distribution functions (CDFs) via the Bayesian bootstrap. Furthermore, it is extended to handle mixed data types by incorporating methods for ordinal variable modeling.
  Through simulation studies and experiments on real-world datasets from the UCI repository, we demonstrate that the proposed BBGC outperforms existing imputation methods across various missing rates and mechanisms (MCAR, MAR). Additionally, the proposed model shows superior performance on real semiconductor manufacturing process data compared to conventional imputation approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06785v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Jeunghun Oh, Hungkuk Ko, Jeongmin Park, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Metropolis-adjusted Subdifferential Langevin Algorithm</title>
      <link>https://arxiv.org/abs/2507.06950</link>
      <description>arXiv:2507.06950v1 Announce Type: new 
Abstract: The Metropolis-Adjusted Langevin Algorithm (MALA) is a widely used Markov Chain Monte Carlo (MCMC) method for sampling from high-dimensional distributions. However, MALA relies on differentiability assumptions that restrict its applicability. In this paper, we introduce the Metropolis-Adjusted Subdifferential Langevin Algorithm (MASLA), a generalization of MALA that extends its applicability to distributions whose log-densities are locally Lipschitz, generally non-differentiable, and non-convex. We evaluate the performance of MASLA by comparing it with other sampling algorithms in settings where they are applicable. Our results demonstrate the effectiveness of MASLA in handling a broader class of distributions while maintaining computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06950v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Ning</dc:creator>
    </item>
    <item>
      <title>Conformal Link Prediction with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2507.07025</link>
      <description>arXiv:2507.07025v1 Announce Type: new 
Abstract: We propose a new method for predicting multiple missing links in partially observed networks while controlling the false discovery rate (FDR), a largely unresolved challenge in network analysis. The main difficulty lies in handling complex dependencies and unknown, heterogeneous missing patterns. We introduce conformal link prediction ({\tt clp}), a distribution-free procedure grounded in the exchangeability structure of weighted graphon models. Our approach constructs conformal p-values via a novel multi-splitting strategy that restores exchangeability within local test sets, thereby ensuring valid row-wise FDR control, even under unknown missing mechanisms. To achieve FDR control across all missing links, we further develop a new aggregation scheme based on e-values, which accommodates arbitrary dependence across network predictions. Our method requires no assumptions on the missing rates, applies to weighted, unweighted, undirected, and bipartite networks, and enjoys finite-sample theoretical guarantees. Extensive simulations and real-world data study confirm the effectiveness and robustness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07025v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Wenqin Du, Wanteng Ma, Dong Xia, Yuan Zhang, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Non-Asymptotic Analysis of Online Local Private Learning with SGD</title>
      <link>https://arxiv.org/abs/2507.07041</link>
      <description>arXiv:2507.07041v1 Announce Type: new 
Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely used for solving optimization problems with privacy guarantees in machine learning and statistics. Despite this, a systematic non-asymptotic convergence analysis for DP-SGD, particularly in the context of online problems and local differential privacy (LDP) models, remains largely elusive. Existing non-asymptotic analyses have focused on non-private optimization methods, and hence are not applicable to privacy-preserving optimization problems. This work initiates the analysis to bridge this gap and opens the door to non-asymptotic convergence analysis of private optimization problems. A general framework is investigated for the online LDP model in stochastic optimization problems. We assume that sensitive information from individuals is collected sequentially and aim to estimate, in real-time, a static parameter that pertains to the population of interest. Most importantly, we conduct a comprehensive non-asymptotic convergence analysis of the proposed estimators in finite-sample situations, which gives their users practical guidelines regarding the effect of various hyperparameters, such as step size, parameter dimensions, and privacy budgets, on convergence rates. Our proposed estimators are validated in the theoretical and practical realms by rigorous mathematical derivations and carefully constructed numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07041v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enze Shi, Jinhan Xie, Bei Jiang, Linglong Kong, Xuming He</dc:creator>
    </item>
    <item>
      <title>Method: Using generalized additive models in the animal sciences</title>
      <link>https://arxiv.org/abs/2507.06281</link>
      <description>arXiv:2507.06281v1 Announce Type: cross 
Abstract: Nonlinear relationships between covariates and a response variable of interest are frequently encountered in animal science research. Within statistical models, these nonlinear effects have, traditionally, been handled using a range of approaches, including transformation of the response, parametric nonlinear models based on theory or phenomenological grounds (e.g., lactation curves), or through fixed spline or polynomial terms. If it is desirable to learn the shape of the relationship from the data directly, then generalized additive models (GAMs) are an excellent alternative to these traditional approaches. GAMs extend the generalized linear model such that the linear predictor includes one or more smooth functions, parameterised using penalised splines. A wiggliness penalty on each function is used to avoid over fitting while estimating the parameters of the spline basis functions to maximise fit to the data without producing an overly complex function. Modern GAMs include automatic smoothness selection methods to find an optimal balance between fit and complexity of the estimated functions. Because GAMs learn the shapes of functions from the data, the user can avoid forcing a particular model to their data. Here, I provide a brief description of GAMs and visually illustrate how they work. I then demonstrate the utility of GAMs on three example data sets of increasing complexity, to show i) how learning from data can produce a better fit to data than that of parametric models, ii) how hierarchical GAMs can be used to estimate growth data from multiple animals in a single model, and iii) how hierarchical GAMs can be used for formal statistical inference in a designed experiment of the effects of exposure to maternal hormones on subsequent growth in Japanese quail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06281v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin L. Simpson</dc:creator>
    </item>
    <item>
      <title>Fast Gaussian Processes under Monotonicity Constraints</title>
      <link>https://arxiv.org/abs/2507.06677</link>
      <description>arXiv:2507.06677v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) are widely used as surrogate models for complicated functions in scientific and engineering applications. In many cases, prior knowledge about the function to be approximated, such as monotonicity, is available and can be leveraged to improve model fidelity. Incorporating such constraints into GP models enhances predictive accuracy and reduces uncertainty, but remains a computationally challenging task for high-dimensional problems. In this work, we present a novel virtual point-based framework for building constrained GP models under monotonicity constraints, based on regularized linear randomize-then-optimize (RLRTO), which enables efficient sampling from a constrained posterior distribution by means of solving randomized optimization problems. We also enhance two existing virtual point-based approaches by replacing Gibbs sampling with the No U-Turn Sampler (NUTS) for improved efficiency. A Python implementation of these methods is provided and can be easily applied to a wide range of problems. This implementation is then used to validate the approaches on approximating a range of synthetic functions, demonstrating comparable predictive performance between all considered methods and significant improvements in computational efficiency with the two NUTS methods and especially with the RLRTO method. The framework is further applied to construct surrogate models for systems of differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06677v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Zhang, Jasper M. Everink, Jakob Sauer J{\o}rgensen</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Long-Tailed Classification</title>
      <link>https://arxiv.org/abs/2507.06867</link>
      <description>arXiv:2507.06867v1 Announce Type: cross 
Abstract: Many real-world classification problems, such as plant identification, have extremely long-tailed class distributions. In order for prediction sets to be useful in such settings, they should (i) provide good class-conditional coverage, ensuring that rare classes are not systematically omitted from the prediction sets, and (ii) be a reasonable size, allowing users to easily verify candidate labels. Unfortunately, existing conformal prediction methods, when applied to the long-tailed setting, force practitioners to make a binary choice between small sets with poor class-conditional coverage or sets with very good class-conditional coverage but that are extremely large. We propose methods with guaranteed marginal coverage that smoothly trade off between set size and class-conditional coverage. First, we propose a conformal score function, prevalence-adjusted softmax, that targets a relaxed notion of class-conditional coverage called macro-coverage. Second, we propose a label-weighted conformal prediction method that allows us to interpolate between marginal and class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet and iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06867v1</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiffany Ding, Jean-Baptiste Fermanian, Joseph Salmon</dc:creator>
    </item>
    <item>
      <title>A statistical approach to latent dynamic modeling with differential equations</title>
      <link>https://arxiv.org/abs/2311.16286</link>
      <description>arXiv:2311.16286v2 Announce Type: replace 
Abstract: Ordinary differential equations (ODEs) can provide mechanistic models of temporally local changes of processes, where parameters are often informed by external knowledge. While ODEs are popular in systems modeling, they are less established for statistical modeling of longitudinal cohort data, e.g., in a clinical setting. Yet, modeling of local changes could also be attractive for assessing the trajectory of an individual in a cohort in the immediate future given its current status, where ODE parameters could be informed by further characteristics of the individual. However, several hurdles so far limit such use of ODEs, as compared to regression-based function fitting approaches. The potentially higher level of noise in cohort data might be detrimental to ODEs, as the shape of the ODE solution heavily depends on the initial value. In addition, larger numbers of variables multiply such problems and might be difficult to handle for ODEs. To address this, we propose to use each observation in the course of time as the initial value to obtain multiple local ODE solutions and build a combined estimator of the underlying dynamics. Neural networks are used for obtaining a low-dimensional latent space for dynamic modeling from a potentially large number of variables, and for obtaining patient-specific ODE parameters from baseline variables. Simultaneous identification of dynamic models and of a latent space is enabled by recently developed differentiable programming techniques. We illustrate the proposed approach in an application with spinal muscular atrophy patients and a corresponding simulation study. In particular, modeling of local changes in health status at any point in time is contrasted to the interpretation of functions obtained from a global regression. This more generally highlights how different application settings might demand different modeling strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16286v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maren Hackenberg, Astrid Pechmann, Clemens Kreutz, Janbernd Kirschner, Harald Binder</dc:creator>
    </item>
    <item>
      <title>Multinomial Link Models</title>
      <link>https://arxiv.org/abs/2312.16260</link>
      <description>arXiv:2312.16260v5 Announce Type: replace 
Abstract: We propose a new family of regression models for analyzing categorical responses, called multinomial link models. It consists of four classes, namely, mixed-link models that generalize existing multinomial logistic models and their extensions, two-group models that can incorporate the observations with NA or unknown responses, dichotomous conditional link models that handle longitudinal binary responses, and po-npo mixture models that are more flexible than partial proportional odds models. By characterizing the feasible parameter space, deriving necessary and sufficient conditions, and developing validated algorithms to guarantee the finding of feasible maximum likelihood estimates, we solve the infeasibility issue of existing statistical software when estimating parameters for cumulative link models. We also provide explicit formulae and detailed algorithms for computing the Fisher information matrix and selecting the best models among the new family. The applications to real datasets show that the new models can fit the data significantly better, correct misleading conclusions due to missing responses, and make more informative statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16260v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianmeng Wang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Post-selection inference for quantifying uncertainty in changes in variance</title>
      <link>https://arxiv.org/abs/2405.15670</link>
      <description>arXiv:2405.15670v2 Announce Type: replace 
Abstract: Quantifying uncertainty in detected changepoints is an important problem. However it is challenging as the naive approach would use the data twice, first to detect the changes, and then to test them. This will bias the test, and can lead to anti-conservative p-values. One approach to avoid this is to use ideas from post-selection inference, which conditions on the information in the data used to choose which changes to test. As a result this produces valid p-values; that is, p-values that have a uniform distribution if there is no change. Currently such methods have been developed for detecting changes in mean only. This paper presents two approaches for constructing post-selection p-values for detecting changes in variance. These vary depending on the method use to detect the changes, but are general in terms of being applicable for a range of change-detection methods and a range of hypotheses that we may wish to test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15670v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Carrington, Paul Fearnhead</dc:creator>
    </item>
    <item>
      <title>Large multi-response linear regression estimation based on low-rank pre-smoothing</title>
      <link>https://arxiv.org/abs/2411.18334</link>
      <description>arXiv:2411.18334v2 Announce Type: replace 
Abstract: Pre-smoothing is a technique aimed at increasing the signal-to-noise ratio in data to improve subsequent estimation and model selection in regression problems. Motivated by the many scientific applications in which multi-response regression problems arise, particularly when the number of responses is large, we propose here to extend pre-smoothing methods to the multiple outcomne setting. Specifically, we introduce and study a simple technique for pre-smoothing based on low-rank approximation. We establish theoretical results on the performance of the proposed methodology, which show that in the large-response setting, the proposed technique outperforms ordinary least squares estimation with the mean squared error criterion, whilst being computationally more efficient than alternative approaches such as reduced rank regression. We quantify our estimator's benefit empirically in a number of simulated experiments. We also demonstrate our proposed low-rank pre-smoothing technique on real data arising from the environmental and biological sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18334v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinle Tian, Alex Gibberd, Matthew Nunes, Sandipan Roy</dc:creator>
    </item>
    <item>
      <title>Assessing treatment efficacy for interval-censored endpoints using multistate semi-Markov models fit to multiple data streams</title>
      <link>https://arxiv.org/abs/2501.14097</link>
      <description>arXiv:2501.14097v3 Announce Type: replace 
Abstract: We introduce a computationally efficient and general approach for utilizing multiple, possibly interval-censored, data streams to study complex biomedical endpoints using multistate semi-Markov models. Our motivating application is the REGEN-2069 trial, which investigated the protective efficacy (PE) of the monoclonal antibody combination REGEN-COV against SARS-CoV-2 when administered prophylactically to individuals in households at high risk of secondary transmission. Using data on symptom onset, episodic RT-qPCR sampling, and serological testing, we estimate the PE of REGEN-COV for asymptomatic infection, its effect on seroconversion following infection, and the duration of viral shedding. We find that REGEN-COV reduced the risk of asymptomatic infection and the duration of viral shedding, and led to lower rates of seroconversion among asymptomatically infected participants. Our algorithm for fitting semi-Markov models to interval-censored data employs a Monte Carlo expectation maximization (MCEM) algorithm combined with importance sampling to efficiently address the intractability of the marginal likelihood when data are intermittently observed. Our algorithm provide substantial computational improvements over existing methods and allows us to fit semi-parametric models despite complex coarsening of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14097v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, C. Jason Liang, Allyson Mateja, Dean A. Follmann, Meagan P. O'Brien, Chenguang Wang, Jonathan Fintzi</dc:creator>
    </item>
    <item>
      <title>The Case for Time in Causal DAGs</title>
      <link>https://arxiv.org/abs/2501.19311</link>
      <description>arXiv:2501.19311v3 Announce Type: replace 
Abstract: We make the case for incorporating a notion of time into causal directed acyclic graphs (DAGs). We demonstrate that nontemporal causal DAGs are ambiguous and obstruct justification of the acyclicity assumption. Assuming that causes precede effects, causal relationships are relative to the time order, and causal DAGs require temporal qualification. We propose a formalization via composite causal variables that refer to quantities at one or multiple time points. We emphasize that the acyclicity assumption requires different justifications depending on whether the time order allows cycles. We conclude by discussing implications for the interpretation and applicability of DAGs as causal models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19311v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander G. Reisach, Alberto Su\'arez, Sebastian Weichwald, Antoine Chambaz</dc:creator>
    </item>
    <item>
      <title>Ultrahigh-dimensional Quadratic Discriminant Analysis Using Random Projections</title>
      <link>https://arxiv.org/abs/2505.23324</link>
      <description>arXiv:2505.23324v2 Announce Type: replace 
Abstract: This paper investigates the effectiveness of using the Random Projection Ensemble (RPE) approach in Quadratic Discriminant Analysis (QDA) for ultrahigh-dimensional classification problems. Classical methods such as Linear Discriminant Analysis (LDA) and QDA are used widely, but face significant challenges in their implementation when the data dimension (say, $p$) exceeds the sample size (say, $n$). In particular, both LDA (using the Moore-Penrose inverse for covariance matrices) and QDA (even with known covariance matrices) may perform as poorly as random guessing when $p/n \to \infty$ as $n \to \infty$. The RPE method, known for addressing the curse of dimensionality, offers a fast and effective solution without relying on selective summary measures of the competing distributions. This paper demonstrates the practical advantages of employing RPE on QDA in terms of classification performance as well as computational efficiency. We establish results for limiting perfect classification in both the population and sample versions of the proposed RPE-QDA classifier, under fairly general assumptions that allow for sub-exponential growth of $p$ relative to $n$. Several simulated and gene expression data sets are analyzed to evaluate the performance of the proposed classifier in ultrahigh-dimensional~scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23324v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annesha Deb, Minerva Mukhopadhyay, Subhajit Dutta</dc:creator>
    </item>
    <item>
      <title>Heavy-tailed max-linear structural equation models in networks with hidden nodes</title>
      <link>https://arxiv.org/abs/2306.15356</link>
      <description>arXiv:2306.15356v2 Announce Type: replace-cross 
Abstract: Recursive max-linear vectors provide models for causal dependence between large values of random variables that are supported on directed acyclic graphs, but the standard assumption that all nodes of such a graph are observed can be unrealistic. We give necessary and sufficient conditions for a partially observed recursive max-linear vector to be representable as a recursive max-linear (sub-)model and provide a graphical algorithm to construct the latter. Our conditions concern the max-weighted paths of a directed acyclic graph and its minimal representation, which play a key role for such models. In the framework of regular variation we translate these conditions into checkable criteria and establish a connection between max-weighted paths and the extremal dependence measure of transformed variables for pairs of nodes. We propose a statistical algorithm to detect bivariate regularly varying recursive max-linear models among the node variables of a directed acyclic graph and show consistency and asymptotic normality of the estimators of the extremal dependence measure under a thresholding procedure. Simulations show that our algorithm performs satisfactorily. We apply it to nutrition intake data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15356v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Krali, Anthony C. Davison, Claudia Kl\"uppelberg</dc:creator>
    </item>
    <item>
      <title>Online Quantile Regression</title>
      <link>https://arxiv.org/abs/2402.04602</link>
      <description>arXiv:2402.04602v3 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of features is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only \textsf{ short-term} sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well-controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for squared loss, we also briefly discuss its properties and proper schemes. Extensive simulations support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04602v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Dong Xia, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Efficient Decision Trees for Tensor Regressions</title>
      <link>https://arxiv.org/abs/2408.01926</link>
      <description>arXiv:2408.01926v2 Announce Type: replace-cross 
Abstract: We proposed the tensor-input tree (TT) method for scalar-on-tensor and tensor-on-tensor regression problems. We first address scalar-on-tensor problem by proposing scalar-output regression tree models whose input variable are tensors (i.e., multi-way arrays). We devised and implemented fast randomized and deterministic algorithms for efficient fitting of scalar-on-tensor trees, making TT competitive against tensor-input GP models. Based on scalar-on-tensor tree models, we extend our method to tensor-on-tensor problems using additive tree ensemble approaches. Theoretical justification and extensive experiments on real and synthetic datasets are provided to illustrate the performance of TT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01926v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Luo, Akira Horiguchi, Li Ma</dc:creator>
    </item>
    <item>
      <title>Causal Inference Isn't Special: Why It's Just Another Prediction Problem</title>
      <link>https://arxiv.org/abs/2504.04320</link>
      <description>arXiv:2504.04320v3 Announce Type: replace-cross 
Abstract: Causal inference is often portrayed as fundamentally distinct from predictive modeling, with its own terminology, goals, and intellectual challenges. But at its core, causal inference is simply a structured instance of prediction under distribution shift. In both cases, we begin with labeled data from a source domain and seek to generalize to a target domain where outcomes are not observed. The key difference is that in causal inference, the labels -- potential outcomes -- are selectively observed based on treatment assignment, introducing bias that must be addressed through assumptions. This perspective reframes causal estimation as a familiar generalization problem and highlights how techniques from predictive modeling, such as reweighting and domain adaptation, apply directly to causal tasks. It also clarifies that causal assumptions are not uniquely strong -- they are simply more explicit. By viewing causal inference through the lens of prediction, we demystify its logic, connect it to familiar tools, and make it more accessible to practitioners and educators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04320v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez-Lor\'ia</dc:creator>
    </item>
    <item>
      <title>Eigenstructure inference for high-dimensional covariance with generalized shrinkage inverse-Wishart prior</title>
      <link>https://arxiv.org/abs/2505.20668</link>
      <description>arXiv:2505.20668v2 Announce Type: replace-cross 
Abstract: In multivariate statistics, estimating the covariance matrix is essential for understanding the interdependence among variables. In high-dimensional settings, where the number of covariates increases with the sample size, it is well known that the eigenstructure of the sample covariance matrix is inconsistent. The inverse-Wishart prior, a standard choice for covariance estimation in Bayesian inference, also suffers from posterior inconsistency. To address the issue of eigenvalue dispersion in high-dimensional settings, the shrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its conceptual appeal and empirical success, the asymptotic justification for the SIW prior has remained limited. In this paper, we propose a generalized shrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance modeling. By extending the SIW framework, the gSIW prior accommodates a broader class of prior distributions and facilitates the derivation of theoretical properties under specific parameter choices. In particular, under the spiked covariance assumption, we establish the asymptotic behavior of the posterior distribution for both eigenvalues and eigenvectors by directly evaluating the posterior expectations for two sets of parameter choices. This direct evaluation provides insights into the large-sample behavior of the posterior that cannot be obtained through general posterior asymptotic theorems. Finally, simulation studies illustrate that the proposed prior provides accurate estimation of the eigenstructure, particularly for spiked eigenvalues, achieving narrower credible intervals and higher coverage probabilities compared to existing methods. For spiked eigenvectors, the performance is generally comparable to that of competing approaches, including the sample covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20668v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Kwangmin Lee, Sewon Park, Jaeyong Lee</dc:creator>
    </item>
  </channel>
</rss>
