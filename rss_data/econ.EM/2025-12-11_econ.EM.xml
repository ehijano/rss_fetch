<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Dec 2025 05:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Screening in Experiments with Partial Compliance</title>
      <link>https://arxiv.org/abs/2512.09206</link>
      <description>arXiv:2512.09206v1 Announce Type: new 
Abstract: This note studies optimal experimental design under partial compliance when experimenters can screen participants prior to randomization. Theoretical results show that retaining all compliers and screening out all non-compliers achieves three complementary aims: (i) the Local Average Treatment Effect is the same as the standard 2SLS estimator with no screening; (ii) median bias is minimized; and (iii) statistical power is maximized. In practice, complier status is unobserved. We therefore discuss feasible screening strategies and propose a simple test for screening efficacy. Future work will conduct an experiment to demonstrate the feasibility and advantages of the optimal screening design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09206v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Carter, Adeline Delavande, Mario Fiorini, Peter Siminski, Patrick Vu</dc:creator>
    </item>
    <item>
      <title>Debiased Bayesian Inference for High-dimensional Regression Models</title>
      <link>https://arxiv.org/abs/2512.09257</link>
      <description>arXiv:2512.09257v1 Announce Type: new 
Abstract: There has been significant progress in Bayesian inference based on sparsity-inducing (e.g., spike-and-slab and horseshoe-type) priors for high-dimensional regression models. The resulting posteriors, however, in general do not possess desirable frequentist properties, and the credible sets thus cannot serve as valid confidence sets even asymptotically. We introduce a novel debiasing approach that corrects the bias for the entire Bayesian posterior distribution. We establish a new Bernstein-von Mises theorem that guarantees the frequentist validity of the debiased posterior. We demonstrate the practical performance of our proposal through Monte Carlo simulations and two empirical applications in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09257v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihui Chen, Zheng Fang, Ruixuan Liu</dc:creator>
    </item>
    <item>
      <title>New Approximation Results and Optimal Estimation for Fully Connected Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2512.09853</link>
      <description>arXiv:2512.09853v1 Announce Type: new 
Abstract: \citet{farrell2021deep} establish non-asymptotic high-probability bounds for general deep feedforward neural network (with rectified linear unit activation function) estimators, with \citet[Theorem 1]{farrell2021deep} achieving a suboptimal convergence rate for fully connected feedforward networks. The authors suggest that improved approximation of fully connected networks could yield sharper versions of \citet[Theorem 1]{farrell2021deep} without altering the theoretical framework. By deriving approximation bounds specifically for a narrower fully connected deep neural network, this note demonstrates that \citet[Theorem 1]{farrell2021deep} can be improved to achieve an optimal rate (up to a logarithmic factor). Furthermore, this note briefly shows that deep neural network estimators can mitigate the curse of dimensionality for functions with compositional structure and functions defined on manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09853v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaoji Tang</dc:creator>
    </item>
    <item>
      <title>Balancing Weights for Causal Mediation Analysis</title>
      <link>https://arxiv.org/abs/2512.09337</link>
      <description>arXiv:2512.09337v1 Announce Type: cross 
Abstract: This paper develops methods for estimating the natural direct and indirect effects in causal mediation analysis. The efficient influence function-based estimator (EIF-based estimator) and the inverse probability weighting estimator (IPW estimator), which are standard in causal mediation analysis, both rely on the inverse of the estimated propensity scores, and thus they are vulnerable to two key issues (i) instability and (ii) finite-sample covariate imbalance. We propose estimators based on the weights obtained by an algorithm that directly penalizes weight dispersion while enforcing approximate covariate and mediator balance, thereby improving stability and mitigating bias in finite samples. We establish the convergence rates of the proposed weights and show that the resulting estimators are asymptotically normal and achieve the semiparametric efficiency bound. Monte Carlo simulations demonstrate that the proposed estimator outperforms not only the EIF-based estimator and the IPW estimator but also the regression imputation estimator in challenging scenarios with model misspecification. Furthermore, the proposed method is applied to a real dataset from a study examining the effects of media framing on immigration attitudes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09337v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Kawato</dc:creator>
    </item>
    <item>
      <title>A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches</title>
      <link>https://arxiv.org/abs/2512.09360</link>
      <description>arXiv:2512.09360v1 Announce Type: cross 
Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09360v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boge Lyu, Qianye Yin, Iris Denise Tommelein, Hanyang Liu, Karnamohit Ranka, Karthik Yeluripati, Junzhe Shi</dc:creator>
    </item>
    <item>
      <title>Kernel Three Pass Regression Filter</title>
      <link>https://arxiv.org/abs/2405.07292</link>
      <description>arXiv:2405.07292v4 Announce Type: replace 
Abstract: We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07292v4</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rajveer Jat, Daanish Padha</dc:creator>
    </item>
    <item>
      <title>Automatic Inference for Value-Added Regressions</title>
      <link>https://arxiv.org/abs/2503.19178</link>
      <description>arXiv:2503.19178v2 Announce Type: replace 
Abstract: A large empirical literature regresses outcomes on empirical Bayes shrinkage estimates of value-added, yet little is known about whether this approach leads to unbiased estimates and valid inference for the downstream regression coefficients. We study a general class of empirical Bayes estimators and the properties of the resulting regression coefficients. We show that estimators can be asymptotically biased and inference can be invalid if the shrinkage estimator does not account for heteroskedasticity in the noise when estimating value added. By contrast, shrinkage estimators properly constructed to model this heteroskedasticity perform an automatic bias correction: the associated regression estimator is asymptotically unbiased, asymptotically normal, and efficient in the sense that it is asymptotically equivalent to regressing on the true (latent) value-added. Further, OLS standard errors from regressing on shrinkage estimates are consistent in this case. As such, efficient inference is easy for practitioners to implement: simply regress outcomes on shrinkage estimates of value-added that account for noise heteroskedasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19178v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Xie</dc:creator>
    </item>
    <item>
      <title>Inference on effect size after multiple hypothesis testing</title>
      <link>https://arxiv.org/abs/2503.22369</link>
      <description>arXiv:2503.22369v3 Announce Type: replace 
Abstract: Significant treatment effects are often emphasized when interpreting and summarizing empirical findings in studies that estimate multiple, possibly many, treatment effects. Under this kind of selective reporting, conventional treatment effect estimates may be biased and their corresponding confidence intervals may undercover the true effect sizes. We propose new estimators and confidence intervals that provide valid inferences on the effect sizes of the significant effects after multiple hypothesis testing. Our methods are based on the principle of selective conditional inference and complement a wide range of tests, including step-up tests and bootstrap-based step-down tests. Our approach is scalable, allowing us to study an application with over 370 estimated effects. We justify our procedure for asymptotically normal treatment effect estimators. We provide two empirical examples that demonstrate bias correction and confidence interval adjustments for significant effects. The magnitude and direction of the bias correction depend on the correlation structure of the estimated effects and whether the interpretation of the significant effects depends on the (in)significance of other effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22369v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Dzemski, Ryo Okui, Wenjie Wang</dc:creator>
    </item>
    <item>
      <title>Tests of exogeneity in duration models with censored data</title>
      <link>https://arxiv.org/abs/2510.26613</link>
      <description>arXiv:2510.26613v3 Announce Type: replace 
Abstract: Consider the setting in which a researcher is interested in the causal effect of a treatment $Z$ on a duration time $T$, which is subject to right censoring. We assume that $T=\varphi(X,Z,U)$, where $X$ is a vector of baseline covariates, $\varphi(X,Z,U)$ is strictly increasing in the error term $U$ for each $(X,Z)$ and $U\sim \mathcal{U}[0,1]$. Therefore, the model is nonparametric and nonseparable. We propose nonparametric tests for the hypothesis that $Z$ is exogenous, meaning that $Z$ is independent of $U$ given $X$. The test statistics rely on an instrumental variable $W$ that is independent of $U$ given $X$. We assume that $X,W$ and $Z$ are all categorical. Test statistics are constructed for the hypothesis that the conditional rank $V_T= F_{T \mid X,Z}(T \mid X,Z)$ is independent of $(X,W)$ jointly. Under an identifiability condition on $\varphi$, this hypothesis is equivalent to $Z$ being exogenous. However, note that $V_T$ is censored by $V_C =F_{T \mid X,Z}(C \mid X,Z)$, which complicates the construction of the test statistics significantly. We derive the limiting distributions of the proposed tests and prove that our estimator of the distribution of $V_T$ converges to the uniform distribution at a rate faster than the usual parametric $n^{-1/2}$-rate. We demonstrate that the test statistics and bootstrap approximations for the critical values have a good finite sample performance in various Monte Carlo settings. Finally, we illustrate the tests with an empirical application to the National Job Training Partnership Act (JTPA) Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26613v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilles Crommen, Jean-Pierre Florens, Ingrid Van Keilegom</dc:creator>
    </item>
  </channel>
</rss>
