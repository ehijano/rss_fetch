<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:02:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust extrapolation problem for random processes with stationary increments</title>
      <link>https://arxiv.org/abs/2510.14003</link>
      <description>arXiv:2510.14003v1 Announce Type: new 
Abstract: The problem of optimal estimation of linear functionals $A {\xi}=\int_{0}^{\infty} a(t)\xi(t)dt$ and $A_T{\xi}=\int_{0}^{T} a(t)\xi(t)dt$ depending on the unknown values of random process $\xi(t)$, $t\in R$, with stationary $n$th increments from observations of ttis process for $t&lt;0$ is considered. Formulas for calculating mean square error and spectral characteristic of optimal linear estimation of the functionals are proposed in the case when spectral density is exactly known. Formulas that determine the least favorable spectral densities are proposed for given sets of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14003v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Luz, Mikhail Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Filtering Problem for Random Processes with Stationary Increments</title>
      <link>https://arxiv.org/abs/2510.14023</link>
      <description>arXiv:2510.14023v1 Announce Type: new 
Abstract: This paper deals with the problem of optimal mean-square filtering of the linear functionals $A{\xi}=\int_{0}^{\infty}a(t)\xi(-t)dt$ and $A_T{\xi}=\int_{0}^Ta(t)\xi(-t)dt$ which depend on the unknown values of random process $\xi(t)$ with stationary $n$th increments from observations of process $\xi(t)+\eta(t)$ at points $t\leq0$, where $\eta(t)$ is a stationary process uncorrelated with $\xi(t)$. We propose the values of mean-square errors and spectral characteristics of optimal linear estimates of the functionals when spectral densities of the processes are known. In the case where we can operate only with a set of admissible spectral densities relations that determine the least favorable spectral densities and the minimax spectral characteristics are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14023v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.7726/cms.2015.1002</arxiv:DOI>
      <dc:creator>Maksym Luz, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Minimax Estimation for Periodically Correlated Stochastic Processes</title>
      <link>https://arxiv.org/abs/2510.14033</link>
      <description>arXiv:2510.14033v1 Announce Type: new 
Abstract: The problem of optimal linear estimation of functional depending on the unknown values of periodically correlated stochastic process from observations of this process for is considered.
  Formulas that determine the least favorable processes and the minimax estimation for functional are proposed for the given class of admissible processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14033v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iryna Dubovets'ka, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Minimum Hellinger Distance Estimators for Complex Survey Designs</title>
      <link>https://arxiv.org/abs/2510.14055</link>
      <description>arXiv:2510.14055v1 Announce Type: new 
Abstract: Reliable inference from complex survey samples can be derailed by outliers and high-leverage observations induced by unequal inclusion probabilities and calibration. We develop a minimum Hellinger distance estimator (MHDE) for parametric superpopulation models under complex designs, including Poisson PPS and fixed-size SRS/PPS without replacement, with possibly stochastic post-stratified or calibrated weights. Using a Horvitz-Thompson-adjusted kernel density plug-in, we show: (i) $L^1$-consistency of the KDE with explicit large-deviation tail bounds driven by a variance-adaptive effective sample size; (ii) uniform exponential bounds for the Hellinger affinity that yield MHDE consistency under mild identifiability; (iii) an asymptotic Normal distribution for the MHDE with covariance $\mathbf A^{-1}\boldsymbol\Sigma \mathbf A^{\intercal}$ (and a finite-population correction under without-replacement designs); and (iv) robustness via the influence function and $\alpha$-influence curves in the Hellinger topology. Simulations under Gamma and lognormal superpopulation models quantify efficiency-robustness trade-offs relative to weighted MLE under independent and high-leverage contamination. An application to NHANES 2021-2023 total water consumption shows that the MHDE remains stable despite extreme responses that markedly bias the MLE. The estimator is simple to implement via quadrature over a fixed grid and is extensible to other divergence families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14055v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David Keepplinger, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>The geometry of PLS shrinkages</title>
      <link>https://arxiv.org/abs/2510.14430</link>
      <description>arXiv:2510.14430v1 Announce Type: new 
Abstract: The geometrical structure of PLS shrinkages is here considered. Firstly, an explicit formula for the shrinkage vector is provided. In that expression, shrinkage factors are expressed a averages of a set of basic shrinkages that depend only on the data matrix. On the other hand, the weights of that average are multilinear functions of the observed responses. That representation allows to characterise the set of possible shrinkages and identify extreme situations where the PLS estimator has an highly nonlinear behaviour. In these situations, recently proposed measures for the degrees of freedom (DoF), that directly depend on the shrinkages, fail to provide reasonable values. It is also shown that the longstanding conjecture that the DoFs of PLS always exceeds the number PLS directions does not hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14430v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Foschi</dc:creator>
    </item>
    <item>
      <title>Strong consistency of pseudo-likelihood parameter estimator for univariate Gaussian mixture models</title>
      <link>https://arxiv.org/abs/2510.14482</link>
      <description>arXiv:2510.14482v1 Announce Type: new 
Abstract: We consider a new method for estimating the parameters of univariate Gaussian mixture models. The method relies on a nonparametric density estimator $\hat{f}_n$ (typically a kernel estimator). For every set of Gaussian mixture components, $\hat{f}_n$ is used to find the best set of mixture weights. That set is obtained by minimizing the $L_2$ distance between $\hat{f}_n$ and the Gaussian mixture density with the given component parameters. The densities together with the obtained weights are then plugged in to the likelihood function, resulting in the so-called pseudo-likelihood function. The final parameter estimators are the parameter values that maximize the pseudo-likelihood function together with the corresponding weights. The advantages of the pseudo-likelihood over the full likelihood are: 1) its arguments are the means and variances only, mixture weights are also functions of the means and variances; 2) unlike the likelihood function, it is always bounded above. Thus, the maximizer of the pseudo-likelihood function -- referred to as the pseudo-likelihood estimator -- always exists. In this article, we prove that the pseudo-likelihood estimator is strongly consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14482v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"uri Lember, Raul Kangro, Kristi Kuljus</dc:creator>
    </item>
    <item>
      <title>Regression Model Selection Under General Conditions</title>
      <link>https://arxiv.org/abs/2510.14822</link>
      <description>arXiv:2510.14822v1 Announce Type: new 
Abstract: Model selection criteria are one of the most important tools in statistics. Proofs showing a model selection criterion is asymptotically optimal are tailored to the type of model (linear regression, quantile regression, penalized regression, etc.), the estimation method (linear smoothers, maximum likelihood, generalized method of moments, etc.), the type of data (i.i.d., dependent, high dimensional, etc.), and the type of model selection criterion. Moreover, assumptions are often restrictive and unrealistic making it a slow and winding process for researchers to determine if a model selection criterion is selecting an optimal model. This paper provides general proofs showing asymptotic optimality for a wide range of model selection criteria under general conditions. This paper not only asymptotically justifies model selection criteria for most situations, but it also unifies and extends a range of previously disparate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14822v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amaze Lusompa</dc:creator>
    </item>
    <item>
      <title>On Time-subordinated Brownian Motion Processes for Financial Markets</title>
      <link>https://arxiv.org/abs/2510.14108</link>
      <description>arXiv:2510.14108v1 Announce Type: cross 
Abstract: The key purpose of this paper is to present Fourier method to model the stochastic time-change in this context of time-subordinated Brownian motion models. We review Gaussian Variance-Mean mixtures and time-subordinated models with a key example of the Gamma process. A non-parametric characteristic function decomposition of subordinated Brownian motion is presented. This allows one to characterise and study the stochastic time-change directly from the full process. Finally we provide an example empirical decomposition of S$\&amp;$P log-returns. We explore the Variance Gamma process as a key example throughout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14108v1</guid>
      <category>q-fin.MF</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Shenoy, Peter Kempthorne</dc:creator>
    </item>
    <item>
      <title>Debiased Kernel Estimation of Spot Volatility in the Presence of Infinite Variation Jumps</title>
      <link>https://arxiv.org/abs/2510.14285</link>
      <description>arXiv:2510.14285v1 Announce Type: cross 
Abstract: Volatility estimation is a central problem in financial econometrics, but becomes particularly challenging when jump activity is high, a phenomenon observed empirically in highly traded financial securities. In this paper, we revisit the problem of spot volatility estimation for an It\^o semimartingale with jumps of unbounded variation. We construct truncated kernel-based estimators and debiased variants that extend the efficiency frontier for spot volatility estimation in terms of the jump activity index $Y$, raising the previous bound $Y&lt;4/3$ to $Y&lt;20/11$, thereby covering nearly the entire admissible range $Y&lt;2$. Compared with earlier work, our approach attains smaller asymptotic variances through the use of unbounded kernels, is simpler to implement, and has broader applicability under more flexible model assumptions. A comprehensive simulation study confirms that our procedures substantially outperform competing methods in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14285v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. Cooper Boniece, Jos\'e E. Figueroa-L\'opez, Tianwei Zhou</dc:creator>
    </item>
    <item>
      <title>Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach</title>
      <link>https://arxiv.org/abs/2510.14415</link>
      <description>arXiv:2510.14415v1 Announce Type: cross 
Abstract: This paper develops a sensitivity analysis framework that transfers the average total treatment effect (ATTE) from source data with a fully observed network to target data whose network is completely unknown. The ATTE represents the average social impact of a policy that assigns the treatment to every individual in the dataset. We postulate a covariate-shift type assumption that both source and target datasets share the same conditional mean outcome. However, because the target network is unobserved, this assumption alone is not sufficient to pin down the ATTE for the target data. To address this issue, we consider a sensitivity analysis based on the uncertainty of the target network's degree distribution, where the extent of uncertainty is measured by the Wasserstein distance from a given reference degree distribution. We then construct bounds on the target ATTE using a linear programming-based estimator. The limiting distribution of the bound estimator is derived via the functional delta method, and we develop a wild bootstrap approach to approximate the distribution. As an empirical illustration, we revisit the social network experiment on farmers' weather insurance adoption in China by Cai et al. (2015).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14415v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tadao Hoshino</dc:creator>
    </item>
    <item>
      <title>On the Identifiability of Tensor Ranks via Prior Predictive Matching</title>
      <link>https://arxiv.org/abs/2510.14523</link>
      <description>arXiv:2510.14523v1 Announce Type: cross 
Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central challenge that often relies on heuristic methods. This paper introduces a rigorous approach to determine rank identifiability in probabilistic tensor models, based on prior predictive moment matching. We transform a set of moment matching conditions into a log-linear system of equations in terms of marginal moments, prior hyperparameters, and ranks; establishing an equivalence between rank identifiability and the solvability of such system. We apply this framework to four foundational tensor-models, demonstrating that the linear structure of the PARAFAC/CP model, the chain structure of the Tensor Train model, and the closed-loop structure of the Tensor Ring model yield solvable systems, making their ranks identifiable. In contrast, we prove that the symmetric topology of the Tucker model leads to an underdetermined system, rendering the ranks unidentifiable by this method. For the identifiable models, we derive explicit closed-form rank estimators based on the moments of observed data only. We empirically validate these estimators and evaluate the robustness of the proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14523v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliezer da Silva, Arto Klami, Diego Mesquita, I\~nigo Urteaga</dc:creator>
    </item>
    <item>
      <title>Testing by Betting while Borrowing and Bargaining</title>
      <link>https://arxiv.org/abs/2407.11465</link>
      <description>arXiv:2407.11465v2 Announce Type: replace 
Abstract: Testing by betting has been a cornerstone of the game-theoretic statistics literature. In this framework, a betting score (or more generally an e-process), as opposed to a traditional p-value, is used to quantify the evidence against a null hypothesis: the higher the betting score, the more money one has made betting against the null, and thus the larger the evidence that the null is false. A key ingredient assumed throughout past works is that one cannot bet more money than one currently has. In this paper, we ask what happens if the bettor is allowed to borrow money after going bankrupt, allowing further financial flexibility in this game of hypothesis testing. We propose various definitions of (adjusted) evidence relative to the wealth borrowed, indebted, and accumulated. We also ask what happens if the bettor can "bargain", in order to obtain odds bettor than specified by the null hypothesis. The adjustment of wealth in order to serve as evidence appeals to the characterization of arbitrage, interest rates, and num\'eraire-adjusted pricing in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11465v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Asymptotic optimality theory of confidence intervals of the mean</title>
      <link>https://arxiv.org/abs/2501.19126</link>
      <description>arXiv:2501.19126v2 Announce Type: replace 
Abstract: We address the classical problem of constructing confidence intervals (CIs) for the mean of a distribution, given \(N\) i.i.d. samples, such that the CI contains the true mean with probability at least \(1 - \delta\), where \(\delta \in (0,1)\). We characterize three distinct learning regimes based on the minimum achievable limiting width of any CI as the sample size \(N_{\delta} \to \infty\) and \(\delta \to 0\). In the first regime, where \(N_{\delta}\) grows slower than \(\log(1/\delta)\), the limiting width of any CI equals the width of the distribution's support, precluding meaningful inference. In the second regime, where \(N_{\delta}\) scales as \(\log(1/\delta)\), we precisely characterize the minimum limiting width, which depends on the scaling constant. In the third regime, where \(N_{\delta}\) grows faster than \(\log(1/\delta)\), complete learning is achievable, and the limiting width of the CI collapses to zero, converging to the true mean. We demonstrate that CIs derived from concentration inequalities based on Kullback--Leibler (KL) divergences achieve asymptotically optimal performance, attaining the minimum limiting width in both sufficient and complete learning regimes for distributions in two families: single-parameter exponential and bounded support. Additionally, these results extend to one-sided CIs, with the width notion adjusted appropriately. Finally, we generalize our findings to settings with random per-sample costs, motivated by practical applications such as stochastic simulators and cloud service selection. Instead of a fixed sample size, we consider a cost budget \(C_{\delta}\), identifying analogous learning regimes and characterizing the optimal CI construction policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19126v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vikas Deep, Achal Bassamboo, Sandeep Juneja</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Kernel Two-Sample Tests with Random Features</title>
      <link>https://arxiv.org/abs/2502.20755</link>
      <description>arXiv:2502.20755v2 Announce Type: replace 
Abstract: Reproducing Kernel Hilbert Space (RKHS) embedding of probability distributions has proved to be an effective approach, via MMD (maximum mean discrepancy), for nonparametric hypothesis testing problems involving distributions defined over general (non-Euclidean) domains. While a substantial amount of work has been done on this topic, only recently have minimax optimal two-sample tests been constructed that incorporate, unlike MMD, both the mean element and a regularized version of the covariance operator. However, as with most kernel algorithms, the optimal test scales cubically in the sample size, limiting its applicability. In this paper, we propose a spectral-regularized two-sample test based on random Fourier feature (RFF) approximation and investigate the trade-offs between statistical optimality and computational efficiency. We show the proposed test to be minimax optimal if the approximation order of RFF (which depends on the smoothness of the likelihood ratio and the decay rate of the eigenvalues of the integral operator) is sufficiently large. We develop a practically implementable permutation-based version of the proposed test with a data-adaptive strategy for selecting the regularization parameter. Finally, through numerical experiments on simulated and benchmark datasets, we demonstrate that the proposed RFF-based test is computationally efficient and performs almost similarly (with a small drop in power) to the exact test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20755v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Mukherjee, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>Geometric optics approximation sampling: near-field case</title>
      <link>https://arxiv.org/abs/2403.01655</link>
      <description>arXiv:2403.01655v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel gradient-free and dimensionality-independent sampler, the Geometric Optics Approximation Sampling (GOAS), based on a near-field reflector system. The key idea involves constructing a reflecting surface that redirects rays from a source with a prescribed simple distribution toward a target domain, achieving the desired target measure. Once this surface is constructed, an arbitrary number of independent, uncorrelated samples can be drawn by re-simulating (ray-tracing) the reflector system, i.e., push-forward samples from the source distribution under a reflecting map. To compute the reflecting surface, we employ an enhanced supporting ellipsoid method for the near-field reflector problem. This approach does not require gradient information of the target density and discretizes the target measure using either a low-discrepancy or random sequence, ensuring dimensionality independence. Since the resulting surface is non-smooth (being a union of ellipsoidal sheets) but continuous, we apply a softmin smoothing technique to enable sampling. Theoretically, we define the geometric optics approximation measure as the push-forward of the source measure through the reflecting map. We prove that this measure is well-defined and stable with respect to perturbations of the target domain, ensuring robustness in sampling. Additionally, we derive error bounds between the numerical geometric optics approximation measure and the target measure under the Hellinger metric. Our numerical experiments validate the theoretical claims of GOAS, demonstrate its superior performance compared to MCMC for complex distributions, and confirm its practical effectiveness and broad applicability in solving Bayesian inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01655v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zejun Sun, Guang-Hui Zheng</dc:creator>
    </item>
    <item>
      <title>$\ell_1$-Regularized Generalized Least Squares</title>
      <link>https://arxiv.org/abs/2405.10719</link>
      <description>arXiv:2405.10719v2 Announce Type: replace-cross 
Abstract: We study an $\ell_{1}$-regularized generalized least-squares (GLS) estimator for high-dimensional regressions with autocorrelated errors. Specifically, we consider the case where errors are assumed to follow an autoregressive process, alongside a feasible variant of GLS that estimates the structure of this process in a data-driven manner. The estimation procedure consists of three steps: performing a LASSO regression, fitting an autoregressive model to the realized residuals, and then running a second-stage LASSO regression on the rotated (whitened) data. We examine the theoretical performance of the method in a sub-Gaussian random-design setting, in particular assessing the impact of the rotation on the design matrix and how this impacts the estimation error of the procedure. We show that our proposed estimators maintain smaller estimation error than an unadjusted LASSO regression when the errors are driven by an autoregressive process. A simulation study verifies the performance of the proposed method, demonstrating that the penalized (feasible) GLS-LASSO estimator performs on par with the LASSO in the case of white noise errors, whilst outperforming when the errors exhibit significant autocorrelation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10719v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaveh S. Nobari, Alex Gibberd</dc:creator>
    </item>
    <item>
      <title>Planning for gold: Hypothesis screening with split samples for valid powerful testing in matched observational studies</title>
      <link>https://arxiv.org/abs/2406.00866</link>
      <description>arXiv:2406.00866v2 Announce Type: replace-cross 
Abstract: Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. One approach to mitigate this concern is to identify hypotheses likely to be more resilient to hidden biases by splitting the data into a planning sample for designing the study and an analysis sample for making inferences. We devise a powerful and flexible method for selecting hypotheses in the planning sample when an unknown number of outcomes are affected by the treatment, allowing researchers to gain the benefits of exploratory analysis and still conduct powerful inference under concerns of unmeasured confounding. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00866v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</dc:creator>
    </item>
  </channel>
</rss>
