<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Oct 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training</title>
      <link>https://arxiv.org/abs/2510.20012</link>
      <description>arXiv:2510.20012v1 Announce Type: new 
Abstract: This study develops an AI-based pose estimation pipeline to enable precise quantification of movement kinematics in resistance training. Using video data from Wolf et al. (2025), which compared lengthened partial (pROM) and full range-of-motion (fROM) training across eight upper-body exercises in 26 participants, 280 recordings were processed to extract frame-level joint-angle trajectories. After filtering and smoothing, per-set metrics were derived, including range of motion (ROM), tempo, and concentric/eccentric phase durations. A random-effects meta-analytic model was applied to account for within-participant and between-exercise variability. Results show that pROM repetitions were performed with a smaller ROM and shorter overall durations, particularly during the eccentric phase of movement. Variance analyses revealed that participant-level differences, rather than exercise-specific factors, were the primary driver of variation, although there is substantial evidence of heterogeneous treatment effects. We then introduce a novel metric, \%ROM, which is the proportion of full ROM achieved during pROM, and demonstrate that this definition of lengthened partials remains relatively consistent across exercises. Overall, these findings suggest that lengthened partials differ from full ROM training not only in ROM, but also in execution dynamics and consistency, highlighting the potential of AI-based methods for advancing research and improving resistance training prescription.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20012v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Adam Diamant</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Learning Under Sequential Randomization</title>
      <link>https://arxiv.org/abs/2510.20078</link>
      <description>arXiv:2510.20078v1 Announce Type: new 
Abstract: Sequential treatment assignments in online experiments lead to complex dependency structures, often rendering identification, estimation and inference over treatments a challenge. Treatments in one session (e.g., a user logging on) can have an effect that persists into subsequent sessions, leading to cumulative effects on outcomes measured at a later stage. This can render standard methods for identification and inference trivially misspecified. We propose T-Learners layered into the G-Formula for this setting, building on literature from causal machine learning and identification in sequential settings. In a simple simulation, this approach prevents decaying accuracy in the presence of carry-over effects, highlighting the importance of identification and inference strategies tailored to the nature of systems often seen in the tech domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20078v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rina Friedberg, Richard Mudd, Patrick Johnstone, Melissa Pothen, Vishal Vaingankar, Vishwanath Sangale, Abbas Zaidi</dc:creator>
    </item>
    <item>
      <title>Reorienting Age-Friendly Frameworks for Rural Contexts: A Spatial Competence-Press Framework for Aging in Chinese Villages</title>
      <link>https://arxiv.org/abs/2510.20343</link>
      <description>arXiv:2510.20343v1 Announce Type: new 
Abstract: While frameworks such as the WHO Age-Friendly Cities have advanced urban aging policy, rural contexts demand fundamentally different analytical approaches. The spatial dispersion, terrain variability, and agricultural labor dependencies that characterize rural aging experiences require moving beyond service-domain frameworks toward spatial stress assessment models. Current research on rural aging in China exhibits methodological gaps, systematically underrepresenting the spatial stressors that older adults face daily, including terrain barriers, infrastructure limitations, climate exposure, and agricultural labor burdens. Existing rural revitalization policies emphasize standardized interventions while inadequately addressing spatial heterogeneity and the spatially-differentiated needs of aging populations. This study developed a GIS-based spatial stress analysis framework that applies Lawton and Nahemow's competence-press model to quantify aging-related stressors and classify rural villages by intervention needs. Using data from 27 villages in Mamuchi Township, Shandong Province, we established four spatial stress indicators: slope gradient index (SGI), solar radiation exposure index (SREI), walkability index (WI), and agricultural intensity index (AII). Analysis of variance and hierarchical clustering revealed significant variation in spatial pressures across villages and identified distinct typologies that require targeted intervention strategies. The framework produces both quantitative stress measurements for individual villages and a classification system that groups villages with similar stress patterns, providing planners and policymakers with practical tools for designing spatially-targeted age-friendly interventions in rural China and similar contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20343v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyuan Gao</dc:creator>
    </item>
    <item>
      <title>Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models</title>
      <link>https://arxiv.org/abs/2510.19999</link>
      <description>arXiv:2510.19999v1 Announce Type: cross 
Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for solving generalized linear models with elastic net constraints that reduces training time in comparison to existing state-of-the-art methods. We redesign the CD method by performing a Taylor expansion around the current iterate to avoid nonlinear operations arising in the gradient computation. By introducing this approximation, we are able to unroll the vector recurrences occurring in the CD method and reformulate the resulting computations into more efficient batched computations. We show empirically that the recurrence can be unrolled by a tunable integer parameter, $s$, such that $s &gt; 1$ yields performance improvements without affecting convergence, whereas $s = 1$ yields the original CD method. A key advantage of ECCD is that it avoids the convergence delay and numerical instability exhibited by block coordinate descent. Finally, we implement our proposed method in C++ using Eigen to accelerate linear algebra computations. Comparison of our method against existing state-of-the-art solvers shows consistent performance improvements of $3\times$ in average for regularization path variant on diverse benchmark datasets. Our implementation is available at https://github.com/Yixiao-Wang-Stats/ECCD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19999v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixiao Wang, Zishan Shao, Ting Jiang, Aditya Devarakonda</dc:creator>
    </item>
    <item>
      <title>Multi-Task Deep Learning for Surface Metrology</title>
      <link>https://arxiv.org/abs/2510.20339</link>
      <description>arXiv:2510.20339v1 Announce Type: cross 
Abstract: A reproducible deep learning framework is presented for surface metrology to predict surface texture parameters together with their reported standard uncertainties. Using a multi-instrument dataset spanning tactile and optical systems, measurement system type classification is addressed alongside coordinated regression of Ra, Rz, RONt and their uncertainty targets (Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and heteroscedastic heads with post-hoc conformal calibration to yield calibrated intervals. On a held-out set, high fidelity was achieved by single-target regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and probability calibration was essentially unchanged after temperature scaling (ECE 0.00504 -&gt; 0.00503 on the test split). Negative transfer was observed for naive multi-output trunks, with single-target models performing better. These results provide calibrated predictions suitable to inform instrument selection and acceptance decisions in metrological workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20339v1</guid>
      <category>physics.app-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Kucharski, A. Gaska, T. Kowaluk, K. Stepien, M. Repalska, B. Gapinski, M. Wieczorowski, M. Nawotka, P. Sobecki, P. Sosinowski, J. Tomasik, A. Wojtowicz</dc:creator>
    </item>
    <item>
      <title>Clustering of multivariate tail dependence using conditional methods</title>
      <link>https://arxiv.org/abs/2510.20424</link>
      <description>arXiv:2510.20424v1 Announce Type: cross 
Abstract: The conditional extremes (CE) framework has proven useful for analysing the joint tail behaviour of random vectors. However, when applied across many locations or variables, it can be difficult to interpret or compare the resulting extremal dependence structures, particularly for high dimensional vectors. To address this, we propose a novel clustering method for multivariate extremes using the CE framework. Our approach introduces a closed-form, computationally efficient dissimilarity measure for multivariate tails, based on the skew-geometric Jensen-Shannon divergence, and is applicable in arbitrary dimensions. Applying standard clustering algorithms to a matrix of pairwise distances, we obtain interpretable groups of random vectors with homogeneous tail dependence. Simulation studies demonstrate that our method outperforms existing approaches for clustering bivariate extremes, and uniquely extends to the multivariate setting. In our application to Irish meteorological data, our clustering identifies spatially coherent regions with similar extremal dependence between precipitation and wind speeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20424v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick O'Toole, Christian Rohrbeck, Jordan Richards</dc:creator>
    </item>
    <item>
      <title>Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.20460</link>
      <description>arXiv:2510.20460v1 Announce Type: cross 
Abstract: Large language models (LLMs) produce outputs with varying levels of uncertainty, and, just as often, varying levels of correctness; making their practical reliability far from guaranteed. To quantify this uncertainty, we systematically evaluate four approaches for confidence estimation in LLM outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For the evaluation of the approaches, we conduct experiments on four question-answering tasks using a state-of-the-art open-source LLM. Our results show that each uncertainty metric captures a different facet of model confidence and that the hybrid CoCoA approach yields the best reliability overall, improving both calibration and discrimination of correct answers. We discuss the trade-offs of each method and provide recommendations for selecting uncertainty measures in LLM applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20460v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Hobelsberger, Theresa Winner, Andreas Nawroth, Oliver Mitevski, Anna-Carolina Haensch</dc:creator>
    </item>
    <item>
      <title>A comparison of methods for designing hybrid type 2 cluster-randomized trials with continuous effectiveness and implementation endpoints</title>
      <link>https://arxiv.org/abs/2510.20741</link>
      <description>arXiv:2510.20741v1 Announce Type: cross 
Abstract: Hybrid type 2 studies are gaining popularity for their ability to assess both implementation and health outcomes as co-primary endpoints. Often conducted as cluster-randomized trials (CRTs), five design methods can validly power these studies: p-value adjustment methods, combined outcomes approach, single weighted 1-DF test, disjunctive 2-DF test, and conjunctive test. We compared all of the methods theoretically and numerically. Theoretical comparisons of the power equations allowed us to identify if any method globally had more or less power than other methods. It was shown that the p-value adjustment methods are always less powerful than the combined outcomes approach and the single 1-DF test. We also identified the conditions under which the disjunctive 2-DF test is less powerful than the single 1-DF test. Because our theoretical comparison showed that some methods could be more powerful than others under certain conditions, and less powerful under others, we conducted a numerical study to understand these differences. The crt2power R package was created to calculate the power or sample size for CRTs with two continuous co-primary endpoints. Using this package, we conducted a numerical evaluation across 30,000 input scenarios to compare statistical power. Specific patterns were identified where a certain method consistently achieved the highest power. When the treatment effects are unequal, the disjunctive 2-DF test tends to have higher power. When the treatment effect sizes are the same, the single 1-DF test tends to have higher power. Together, these comparisons provide clearer insights to guide method selection for powering hybrid type 2 studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20741v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melody Owen, Fan Li, Ruyi Liu, Donna Spiegelman</dc:creator>
    </item>
    <item>
      <title>Integrated Bayesian non-parametric spatial modeling for cross-sample identification of spatially variable genes</title>
      <link>https://arxiv.org/abs/2504.09654</link>
      <description>arXiv:2504.09654v3 Announce Type: replace 
Abstract: Spatial transcriptomics has revolutionized tissue analysis by simultaneously mapping gene expression, spatial topography, and histological context across consecutive tissue sections, enabling systematic investigation of spatial heterogeneity. The detection of spatially variable (SV) genes, which are molecular signatures with position-dependent expression, provides critical insights into disease mechanisms spanning oncology, neurology, and cardiovascular research. Current methodologies, however, confront dual constraints: predominant reliance on predefined spatial pattern templates restricts detection of novel complex spatial architectures, and inconsistent sample selection strategies compromise analytical stability and biological interpretability. To overcome these challenges, we propose a novel Bayesian hierarchical framework incorporating non-parametric spatial modeling and across-sample integration. It takes advantage of the non-parametric technique and develops an adaptive spatial process accommodating complex pattern discovery while maintaining biological interpretability. A novel cross-sample bi-level shrinkage prior is further introduced for robust multi-sample SV gene detection, facilitating more effective information fusion. An efficient variational inference is developed for posterior inference ensuring computational scalability. Comprehensive simulations demonstrate the improved performance of our proposed method over existing analytical frameworks, and its application to DLPFC and SCC data reveals interpretable SV genes whose spatial patterns delineate relevant clusters and gradients, advancing human transcriptomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09654v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Zhou, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>One-sample survival tests in the presence of non-proportional hazards in oncology clinical trial</title>
      <link>https://arxiv.org/abs/2506.18608</link>
      <description>arXiv:2506.18608v2 Announce Type: replace 
Abstract: In oncology, conduct well-powered time-to-event randomized clinical trials may be challenging due to limited patietns number. Many designs for single-arm trials (SATs) have recently emerged as an alternative to overcome this issue. They rely on the (modified) one-sample log-rank test (OSLRT) under the proportional hazards to compare the survival curves of an experimental and an external control group. We extend Finkelstein's formulation of OSLRT as a score test by using a piecewise exponential model for early, middle and delayed treatment effects and an accelerated hazards model for crossing hazards. We adapt the restricted mean survival time based test and construct a combination test procedure (max-Combo) to SATs. The performance of the developed are evaluated through a simulation study. The score tests are as conservative as the OSLRT and have the highest power when the data generation matches the model underlying score tests. The max-Combo test is more powerful than the OSLRT whatever the scenarios and is thus an interesting approach as compared to a score test. Uncertainty on the survival curve estimated of the external control group and its model misspecification may have a significant impact on performance. For illustration, we apply the developed tests on real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18608v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chlo\'e Szurewsky (U1018), Guosheng Yin (DSAS), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>Prognostic Framework for Robotic Manipulators Operating Under Dynamic Task Severities</title>
      <link>https://arxiv.org/abs/2412.00538</link>
      <description>arXiv:2412.00538v2 Announce Type: replace-cross 
Abstract: Robotic manipulators are critical in many applications but are known to degrade over time. This degradation is influenced by the nature of the tasks performed by the robot. Tasks with higher severity, such as handling heavy payloads, can accelerate the degradation process. One way this degradation is reflected is in the position accuracy of the robot's end-effector. In this paper, we present a prognostic modeling framework that predicts a robotic manipulator's Remaining Useful Life (RUL) while accounting for the effects of task severity. Our framework represents the robot's position accuracy as a Brownian motion process with a random drift parameter that is influenced by task severity. The dynamic nature of task severity is modeled using a continuous-time Markov chain (CTMC). To evaluate RUL, we discuss two approaches -- (1) a novel closed-form expression for Remaining Lifetime Distribution (RLD), and (2) Monte Carlo simulations, commonly used in prognostics literature. Theoretical results establish the equivalence between these RUL computation approaches. We validate our framework through experiments using two distinct physics-based simulators for planar and spatial robot fleets. Our findings show that robots in both fleets experience shorter RUL when handling a higher proportion of high-severity tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00538v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Mohanty, Jason Dekarske, Stephen K. Robinson, Sanjay Joshi, Nagi Gebraeel</dc:creator>
    </item>
    <item>
      <title>On the Wisdom of Crowds (of Economists)</title>
      <link>https://arxiv.org/abs/2503.09287</link>
      <description>arXiv:2503.09287v2 Announce Type: replace-cross 
Abstract: We study the properties of macroeconomic survey forecast response averages as the number of survey respondents grows. Such averages are ``portfolios" of forecasts. We characterize the speed and pattern of the gains from diversification as a function of portfolio size (the number of survey respondents) in both (1) the key real-world data-based environment of the U.S. Survey of Professional Forecasters, and (2) the theoretical model-based environment of equicorrelated forecast errors. We proceed by proposing and comparing various direct and model-based ``crowd size signature plots", which summarize the forecasting performance of $k$-average forecasts as a function of $k$, where $k$ is the number of forecasts in the average. We then estimate the equicorrelation model for growth and inflation forecast errors by choosing model parameters to minimize the divergence between direct and model-based signature plots. The results indicate near-perfect equicorrelation model fit for both growth and inflation, which we explicate by showing analytically that, under very weak conditions, the direct and fitted equicorrelation model-based signature plots are identical at a particular model parameter configuration. That parameter configuration immediately suggests an analytic closed-form estimator for the direct signature plot, so that equicorrelation ultimately emerges as a device for convenient calculation of direct signature plots, rather than a separate ``model" producing separate signature plots. In any event we find that the gains from survey diversification are greater for inflation forecasts than for growth forecasts, and that they are largely exhausted with inclusion of 5-10 representative forecasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09287v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francis X. Diebold, Aaron Mora, Minchul Shin</dc:creator>
    </item>
    <item>
      <title>A Sensitivity Analysis Framework for Quantifying Confidence in Decisions in the Presence of Data Uncertainty</title>
      <link>https://arxiv.org/abs/2504.17043</link>
      <description>arXiv:2504.17043v2 Announce Type: replace-cross 
Abstract: Nearly all statistical analyses that inform policy-making are based on imperfect data. As examples, the data may suffer from measurement errors, missing values, sample selection bias, or record linkage errors. Analysts have to decide how to handle such data imperfections, e.g., analyze only the complete cases or impute values for the missing items via some posited model. Their choices can influence estimates and hence, ultimately, policy decisions. Thus, it is prudent for analysts to evaluate the sensitivity of estimates and policy decisions to the assumptions underlying their choices. To facilitate this goal, we propose that analysts define metrics and visualizations that target the sensitivity of the ultimate decision to the assumptions underlying their approach to handling the data imperfections. Using these visualizations, the analyst can assess their confidence in the policy decision under their chosen analysis. We illustrate metrics and corresponding visualizations with two examples, namely considering possible measurement error in the inputs of predictive models of presidential vote share and imputing missing values when evaluating the percentage of children exposed to high levels of lead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17043v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adway S. Wadekar, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift</title>
      <link>https://arxiv.org/abs/2505.17203</link>
      <description>arXiv:2505.17203v2 Announce Type: replace-cross 
Abstract: We study contextual dynamic pricing when a target market can leverage K auxiliary markets -- offline logs or concurrent streams -- whose mean utilities differ by a structured preference shift. We propose Cross-Market Transfer Dynamic Pricing (CM-TDP), the first algorithm that provably handles such model-shift transfer and delivers minimax-optimal regret for both linear and non-parametric utility models.
  For linear utilities of dimension d, where the difference between source- and target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret $\tilde{O}((d*K^{-1}+s_{0})\log T)$. For nonlinear demand residing in a reproducing kernel Hilbert space with effective dimension $\alpha$, complexity $\beta$ and task-similarity parameter $H$, the regret becomes $\tilde{O}\!(K^{-2\alpha\beta/(2\alpha\beta+1)}T^{1/(2\alpha\beta+1)} + H^{2/(2\alpha+1)}T^{1/(2\alpha+1)})$, matching information-theoretic lower bounds up to logarithmic factors. The RKHS bound is the first of its kind for transfer pricing and is of independent interest.
  Extensive simulations show up to 50% lower cumulative regret and 5 times faster learning relative to single-market pricing baselines. By bridging transfer learning, robust aggregation, and revenue optimization, CM-TDP moves toward pricing systems that transfer faster, price smarter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17203v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhang, Elynn Chen, Yujun Yan</dc:creator>
    </item>
    <item>
      <title>Two approaches to multiple canonical correlation analysis for repeated measures data</title>
      <link>https://arxiv.org/abs/2510.04457</link>
      <description>arXiv:2510.04457v2 Announce Type: replace-cross 
Abstract: In classical canonical correlation analysis (CCA), the goal is to determine the linear transformations of two random vectors into two new random variables that are most strongly correlated. Canonical variables are pairs of these new random variables, while canonical correlations are correlations between these pairs. In this paper, we propose and study two generalizations of this classical method:
  (1) Instead of two random vectors we study more complex data structures that appear in important applications. In these structures, there are $L$ features, each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objects over $T$ time points. We derive a suitable analog of the CCA for such data. Our approach relies on embeddings into Reproducing Kernel Hilbert Spaces, and covers several related data structures as well.
  (2) We develop an analogous approach for multidimensional random processes. In this case, the experimental units are multivariate continuous, square-integrable functions over a given interval. These functions are modeled as elements of a Hilbert space, so in this case, we define the multiple functional canonical correlation analysis, MFCCA.
  We justify our approaches by their application to two data sets and suitable large sample theory. We derive consistency rates for the related transformation and correlation estimators, and show that it is possible to relax two common assumptions on the compactness of the underlying cross-covariance operators and the independence of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04457v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz G\'orecki, Miros{\l}aw Krzy\'sko, Felix Gnettner, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access</title>
      <link>https://arxiv.org/abs/2510.15324</link>
      <description>arXiv:2510.15324v3 Announce Type: replace-cross 
Abstract: I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($\Delta \text{AIC} &gt; 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional difference-in-differences approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15324v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Network Contagion Dynamics in European Banking: A Navier-Stokes Framework for Systemic Risk Assessment</title>
      <link>https://arxiv.org/abs/2510.19630</link>
      <description>arXiv:2510.19630v2 Announce Type: replace-cross 
Abstract: This paper develops a continuous functional framework for analyzing contagion dynamics in financial networks, extending the Navier-Stokes-based approach to network-structured spatial processes. We model financial distress propagation as a diffusion process on weighted networks, deriving a network diffusion equation from first principles that predicts contagion decay depends on the network's algebraic connectivity through the relation $\kappa = \sqrt{\lambda_2/D}$, where $\lambda_2$ is the second-smallest eigenvalue of the graph Laplacian and $D$ is the diffusion coefficient. Applying this framework to European banking data from the EBA stress tests (2018, 2021, 2023), we estimate interbank exposure networks using maximum entropy methods and track the evolution of systemic risk through the COVID-19 crisis. Our key finding is that network connectivity declined by 45\% from 2018 to 2023, implying a 26\% reduction in the contagion decay parameter. Difference-in-differences analysis reveals this structural change was driven by regulatory-induced deleveraging of systemically important banks, which experienced differential asset reductions of 17\% relative to smaller institutions. The networks exhibit lognormal rather than scale-free degree distributions, suggesting greater resilience than previously assumed in the literature. Extensive robustness checks across parametric and non-parametric estimation methods confirm declining systemic risk, with cross-method correlations exceeding 0.95. These findings demonstrate that post-COVID-19 regulatory reforms effectively reduced network interconnectedness and systemic vulnerability in the European banking system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19630v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
