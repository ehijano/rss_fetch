<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Aug 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A User Manual for cuHALLaR: A GPU Accelerated Low-Rank Semidefinite Programming Solver</title>
      <link>https://arxiv.org/abs/2508.15951</link>
      <description>arXiv:2508.15951v1 Announce Type: new 
Abstract: We present a Julia-based interface to the precompiled HALLaR and cuHALLaR binaries for large-scale semidefinite programs (SDPs). Both solvers are established as fast and numerically stable, and accept problem data in formats compatible with SDPA and a new enhanced data format taking advantage of Hybrid Sparse Low-Rank (HSLR) structure. The interface allows users to load custom data files, configure solver options, and execute experiments directly from Julia. A collection of example problems is included, including the SDP relaxations of the Matrix Completion and Maximum Stable Set problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15951v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Aguirre, Diego Cifuentes, Vincent Guigues, Renato D. C. Monteiro, Victor Hugo Nascimento, Arnesh Sujanani</dc:creator>
    </item>
    <item>
      <title>A unified vertical alignment and earthwork model in road design with a new convex optimization model for road networks</title>
      <link>https://arxiv.org/abs/2508.15953</link>
      <description>arXiv:2508.15953v1 Announce Type: new 
Abstract: The vertical alignment optimization problem in road design seeks the optimal vertical alignment of a road at minimal cost, taking into account earthwork while meeting all safety and design requirements. In recent years, modelling techniques have been advanced to incorporate: side slopes, multiple material types, multiple hauling types, and road networks. However, the advancements were created disjointly with implementations that only made a single advancement to the basic model. Herein, we present a mixed-integer linear programming optimization model that unifies all previous advancements. The model further improves on previous work by maintaining convexity even in the multi-material setting. We compare our new model to previous models, validate it numerically, and demonstrate its capability in approximating material volumes. Our new model performs particularly well for determining the optimal vertical alignment for large road networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15953v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayan Sadhukhan, Warren Hare, Yves Lucet</dc:creator>
    </item>
    <item>
      <title>Variable Neighborhood Descent Methods for Large-scale Single-assignment Multi-level Facility Location Problem</title>
      <link>https://arxiv.org/abs/2508.15954</link>
      <description>arXiv:2508.15954v1 Announce Type: new 
Abstract: This paper addresses the single--assignment uncapacitated multi-level facility location (MFL) problem, which has numerous applications, including tactical and strategic supply chain management. We consider four-level and five-level facilities (4-LFL and 5-LFL). Although the MFL has been addressed in the literature in various settings, solutions to large-scale, realistic problems are still lacking. This paper considers several variants of the variable neighborhood descent (VND) method, including BVND, PVND, CVND, and UVND, for the problem. In each case, a multi-start strategy with strong diversification components is provided. Extensive computational experiments are presented to compare methods for large-scale problems involving up to 10,000 customers, 150 distribution centers, 50 warehouses, and 30 plants in the case of 4-LFL; and 8,000 customers, 150 distribution centers, 50 warehouses, 50 plants, and 100 suppliers in the case of 5-LFL. Sensitivity analyses, supported by appropriate statistical methods, validate the effectiveness of the heuristics' results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15954v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Wang, Bahram Alidaee</dc:creator>
    </item>
    <item>
      <title>Mean-Field Generalisation Bounds for Learning Controls in Stochastic Environments</title>
      <link>https://arxiv.org/abs/2508.16001</link>
      <description>arXiv:2508.16001v1 Announce Type: new 
Abstract: We consider a data-driven formulation of the classical discrete-time stochastic control problem. Our approach exploits the natural structure of many such problems, in which significant portions of the system are uncontrolled. Employing the dynamic programming principle and the mean-field interpretation of single-hidden layer neural networks, we formulate the control problem as a series of infinite-dimensional minimisation problems. When regularised carefully, we provide practically verifiable assumptions for non-asymptotic bounds on the generalisation error achieved by the minimisers to this problem, thus ensuring stability in overparametrised settings, for controls learned using finitely many observations. We explore connections to the traditional noisy stochastic gradient descent algorithm, and subsequently show promising numerical results for some classic control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16001v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Baros, Samuel N. Cohen, Christoph Reisinger</dc:creator>
    </item>
    <item>
      <title>Chapter 7 Multi-Criteria Decision-Making: Reference-Type Methods</title>
      <link>https://arxiv.org/abs/2508.16087</link>
      <description>arXiv:2508.16087v1 Announce Type: new 
Abstract: This chapter describes selected reference-type multi-criteria decision-making (MCDM) methods that rank alternatives by comparing them with one or more reference solutions derived from an alternatives-criteria matrix (ACM). After explaining the idea of constructing positive ideal, negative ideal and/or average reference solutions, the chapter details the algorithmic steps of each method, illustrating them with a common ACM example. The 9 methods covered are: Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), Gray/Grey Relational Analysis (GRA), VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR), Evaluation Based on Distance from Average Solution (EDAS), Multi-attributive Border Approximation Area Comparison (MABAC), Combinative Distance-based Assessment (CODAS), Proximity Indexed Value (PIV), Measurement of Alternatives and Ranking According to Compromise Solution (MARCOS) and Preference Ranking on the Basis of Ideal-average Distance (PROBID). The advantages (e.g., computational simplicity) and limitations (e.g., susceptibility to rank reversal) of each method are discussed. A consolidated summary highlights how the different treatments of reference solutions can ultimately drive variations in the ranking of alternatives, underscoring the value of applying several methods in practice. By studying this chapter, readers can (1) describe the principles and steps of each reference-type method, (2) implement them on an ACM, and (3) choose an appropriate reference-type method for their decision-making problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16087v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyuan Wang, Gade Pandu Rangaiah</dc:creator>
    </item>
    <item>
      <title>Exact Hull Reformulation for Quadratically Constrained Generalized Disjunctive Programs</title>
      <link>https://arxiv.org/abs/2508.16093</link>
      <description>arXiv:2508.16093v1 Announce Type: new 
Abstract: Generalized Disjunctive Programming (GDP) is a framework for optimization problems involving discrete decisions and nonlinear constraints. The widely used Hull Reformulation transforms GDPs into Mixed-Integer Nonlinear Programming (MINLP) problems with tighter continuous relaxations. However, it typically employs an epsilon approximation of the closure of the perspective function for nonlinear constraints, which can lead to numerical instability and a weaker continuous relaxation.
  We present an exact Hull Reformulation for GDPs with quadratic constraints, preserving the original quadratic structure and avoiding relaxation weakening. The method applies to both convex and non-convex constraints and extends earlier approaches developed for second-order cone representable functions. We prove equivalence to the conventional exact Hull Reformulation for quadratic constraints and demonstrate improved computational performance and numerical stability in extensive computational experiments. Benchmarks include random GDP instances, Continuously Stirred Tank Reactor network optimization, k-means clustering, and constrained layout optimization problems. The computational results demonstrate improved solution times and numerical stability of the proposed exact Hull Reformulation, making it a preferable approach for GDP problems with quadratic constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16093v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergey Gusev, David E. Bernal Neira</dc:creator>
    </item>
    <item>
      <title>GPU Implementation of Second-Order Linear and Nonlinear Programming Solvers</title>
      <link>https://arxiv.org/abs/2508.16094</link>
      <description>arXiv:2508.16094v1 Announce Type: new 
Abstract: In recent years, GPU-accelerated optimization solvers based on second-order methods (e.g., interior-point methods) have gained momentum with the advent of mature and efficient GPU-accelerated direct sparse linear solvers, such as cuDSS. This paper provides an overview of the state of the art in GPU-based second-order solvers, focusing on pivoting-free interior-point methods for large and sparse linear and nonlinear programs. We begin by highlighting the capabilities and limitations of the currently available GPU-accelerated sparse linear solvers. Next, we discuss different formulations of the Karush-Kuhn-Tucker systems for second-order methods and evaluate their suitability for pivoting-free GPU implementations. We also discuss strategies for computing sparse Jacobians and Hessians on GPUs for nonlinear programming. Finally, we present numerical experiments demonstrating the scalability of GPU-based optimization solvers. We observe speedups often exceeding 10x compared to comparable CPU implementations on large-scale instances when solved up to medium precision. Additionally, we examine the current limitations of existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16094v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Montoison, Fran\c{c}ois Pacaud, Sungho Shin, Mihai Anitescu</dc:creator>
    </item>
    <item>
      <title>Data-Driven Multi-Objective Optimization of Large-Diameter Si Floating-Zone Crystal Growth</title>
      <link>https://arxiv.org/abs/2508.16111</link>
      <description>arXiv:2508.16111v1 Announce Type: new 
Abstract: Floating Zone (FZ) silicon crystal growth is essential for high-power electronics and advanced detection systems. The increasing pressure to scale up the process is challenging due to competing objectives. This study presents a surrogate-based optimization framework to address Multi-Objective Optimization (MOO) in FZ growth, considering eight relevant objectives related to productivity, geometrical and growth parameters, and crystal quality. A Deep Ensemble (DE) of Neural Networks serves as a surrogate model, trained on numerical data from a Finite Element Model (FEM). Optimization is carried out using NSGA-II and NSGA-III, two variants of Genetic Algorithms that explore trade-offs between competing objectives and identify high-performing candidate solutions. Results show that NSGA-II outperforms NSGA-III. The optimal solutions correctly captured known trends, such as correlations between crystal size, pulling rate, and thermal stress. A subset of the more intricate solutions was validated through new simulations, showing excellent prediction performance. However, candidate solutions must still be verified by the FEM prior to experimental validation. This framework establishes a foundation for systematic, data-driven process optimization in FZ growth and can be extended to accelerate improvements in other crystal growth methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16111v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Vieira, Milena Petkovic, Robert Menzel, Natasha Dropka</dc:creator>
    </item>
    <item>
      <title>Optimal Data Reduction under Information-Theoretic Criteria</title>
      <link>https://arxiv.org/abs/2508.16123</link>
      <description>arXiv:2508.16123v1 Announce Type: new 
Abstract: Selecting an optimal subset of features or instances under an information theoretic criterion has become an effective preprocessing strategy for reducing data complexity while preserving essential information. This study investigates two representative problems within this paradigm: feature selection based on the maximum relevance minimum redundancy criterion, and instance selection grounded in the Kullback Leibler divergence. To address the intrinsic nonconvexities of these problems, we develop polyhedral relaxations that yield exact mixed integer linear programming formulations, thereby enabling globally optimal data reduction. By leveraging modern optimization techniques, we further design efficient algorithmic implementations capable of solving practically sized instances. Extensive numerical experiments on both real world and synthetic datasets demonstrate that our method efficiently solves data reduction problems to global optimality, significantly outperforming existing benchmark approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16123v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taotao He, Jun Luo, Junkai Zhao</dc:creator>
    </item>
    <item>
      <title>Innovative Distributed Maintenance Concept: From the design to cost optimisation</title>
      <link>https://arxiv.org/abs/2508.16160</link>
      <description>arXiv:2508.16160v1 Announce Type: new 
Abstract: This study proposes an integrated heuristic framework for the strategic optimization of distributed maintenance operations in geo-distributed production systems (GDPS). It introduces a dual-entity maintenance structure comprising a Centralized Maintenance Workshop (CMW) and a Mobile Maintenance Workshop (MMW), aimed at minimizing total long-term maintenance costs. The cost function incorporates transport, operations, and downtime penalties, optimized via a two-stage algorithmic approach: a Maintenance Planning Algorithm (MPA) based on predictive maintenance scheduling, and a Long-term Heuristic Scheduling Algorithm (LHSA) addressing a capacitated vehicle routing problem with time windows (CVRPTW). A novel contribution includes a heuristic for CMW location determination using the weighted barycentre of site failure probabilities and a discrete selection of MMW capacities. Mixed Integer Linear Programming (MILP) and a divide-and-conquer heuristic are utilized to handle the NP-hard nature of the problem. Experimental validation using Weibull-distributed failure data and various cost scenarios demonstrates that the proposed Optimised Maintenance and Capacitated Routing (OMCR) framework can reduce lifecycle maintenance costs by up to 50%, with increased scalability for systems exceeding 30 GDPS. The framework is applicable to sectors requiring high availability and centralized servicing, including aerospace, railway, and energy industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16160v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Journal of Prognostics and Health Management, 2025, 16 (2)</arxiv:journal_reference>
      <dc:creator>Maria Di Mascolo (G-SCOP, G-SCOP\_DOME2S), Zineb Simeu-Abazi (G-SCOP, G-SCOP\_DOME2S), Rony Ars\`ene Djeunang Mezafack (G-SCOP, G-SCOP\_DOME2S)</dc:creator>
    </item>
    <item>
      <title>Well-posedness of Lur'e systems with feedthrough</title>
      <link>https://arxiv.org/abs/2508.16221</link>
      <description>arXiv:2508.16221v1 Announce Type: new 
Abstract: For a large class of Lur'e systems with time-varying nonlinearities and feedthrough we consider several well-posedness issues, namely: existence, continuation, blow-up in finite-time, forward completeness and uniqueness of solutions. Lur'e systems with feedthrough are systems of forced, nonlinear ordinary differential equations coupled with a nonlinear algebraic equation determining the output of the system. The presence of feedthrough means that the algebraic equation is implicit in the output, and, in general, the output may not be expressible by an analytic formula in terms of the state and the input. Simple examples illustrate that the well-posedness properties of such systems are not necessarily guaranteed by assumptions sufficient for the corresponding well-posedness properties of Lur'e systems without feedthrough. We provide sufficient conditions for the well-posedness properties mentioned above, using global inversion theorems from real analysis and tools from non-smooth analysis and differential inclusions. The theory is illustrated with examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16221v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Guiver, Hartmut Logemann</dc:creator>
    </item>
    <item>
      <title>Elastoplasticity with softening as a state-dependent sweeping process: non-uniqueness of solutions and emergence of shear bands in lattices of springs</title>
      <link>https://arxiv.org/abs/2508.16333</link>
      <description>arXiv:2508.16333v1 Announce Type: new 
Abstract: Plasticity with softening and fracture mechanics lead to ill-posed mathematical problems due to the loss of monotonicity. Multiple co-existing solutions are possible when softening elements are coupled together, and solutions cannot be continued beyond the point of complete degradation of the set of admissible stresses. We present a state-dependent sweeping process which solves the evolution of elasto-plastic Lattice Spring Models with arbitrary placement of softening, hardening and perfectly plastic springs. Using numerical simulations of regular grid lattices with softening we demonstrate the emergence of non-symmetric shear bands with strain localization. At the same time, in toy examples it is easy to analytically derive multiple co-existing solutions. These solutions correspond to fixed points in the implicit catch-up algorithm and we observe a discontinuous bifurcation with the exchange of stability of those fixed points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16333v1</guid>
      <category>math.OC</category>
      <category>cond-mat.soft</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Gudoshnikov</dc:creator>
    </item>
    <item>
      <title>Exact penalty functions and global saddle points of augmented Lagrangians for well-posed constrained optimization problems</title>
      <link>https://arxiv.org/abs/2508.16462</link>
      <description>arXiv:2508.16462v1 Announce Type: new 
Abstract: The goal of this article is to study necessary and sufficient conditions for the exactness of penalty functions and the existence of global saddle points of augmented Lagrangians for well-posed (in a suitable sense) constrained optimization problems in infinite dimensional spaces. To this end, we present a new version of extended well-posedness of a constrained optimization problem and analyse how it relates to the more well-known types of well-posedness, such as Tykhonov and Levitin-Polyak well-posedness. This new version of extended well-posedness allows one to extend many existing results on exact penalty functions and global saddle points of augmented Lagrangians from the finite dimensional to the infinite dimensional case. Such extensions provide first verifiable sufficient conditions for the exactness of penalty functions and the existence of global saddle points of augmented Lagrangians in the infinite dimensional case that do not rely on very restrictive and difficult to verify assumptions (nonlocal metric regularity of constraints, existence of nonlocal error bounds, the Palais-Smale condition, abstract properties of the perturbation function, etc.) that are typically used in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16462v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. V. Dolgopolik</dc:creator>
    </item>
    <item>
      <title>LeAP-SSN: A Semismooth Newton Method with Global Convergence Rates</title>
      <link>https://arxiv.org/abs/2508.16468</link>
      <description>arXiv:2508.16468v1 Announce Type: new 
Abstract: We propose LeAP-SSN (Levenberg--Marquardt Adaptive Proximal Semismooth Newton method), a semismooth Newton-type method with a simple, parameter-free globalisation strategy that guarantees convergence from arbitrary starting points in nonconvex settings to stationary points, and under a Polyak--Lojasiewicz condition, to a global minimum, in Hilbert spaces. The method employs an adaptive Levenberg--Marquardt regularisation for the Newton steps, combined with backtracking, and does not require knowledge of problem-specific constants. We establish global nonasymptotic rates: $\mathcal{O}(1/k)$ for convex problems in terms of objective values, $\mathcal{O}(1/\sqrt{k})$ under nonconvexity in terms of subgradients, and linear convergence under a Polyak--Lojasiewicz condition. The algorithm achieves superlinear convergence under mild semismoothness and Dennis--Mor\'e or partial smoothness conditions, even for non-isolated minimisers. By combining strong global guarantees with superlinear local rates in a fully parameter-agnostic framework, LeAP-SSN bridges the gap between globally convergent algorithms and the fast asymptotics of Newton's method. The practical efficiency of the method is illustrated on representative problems from imaging, contact mechanics, and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16468v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amal Alphonse, Pavel Dvurechensky, Ioannis P. A. Papadopoulos, Clemens Sirotenko</dc:creator>
    </item>
    <item>
      <title>Complexity Analysis of the Regular Simplicial Search Method with Reflection and Shrinking Steps for Derivative-Free Optimization</title>
      <link>https://arxiv.org/abs/2508.16561</link>
      <description>arXiv:2508.16561v1 Announce Type: new 
Abstract: Simplex-type methods, such as the well-known Nelder-Mead algorithm, are widely used in derivative-free optimization (DFO), particularly in practice. Despite their popularity, the theoretical understanding of their convergence properties has been limited, and until very recently essentially no worst-case complexity bounds were available. Recently, Cao et al. provided a sharp error bound for linear interpolation and extrapolation and derived a worst-case complexity result for a basic simplex-type method. Motivated by this, we propose a practical and provable algorithm -- the regular simplicial search method (RSSM), that incorporates reflection and shrinking steps, akin to the original method of Spendley et al. We establish worst-case complexity bounds in nonconvex, convex, and strongly convex cases. These results provide guarantees on convergence rates and lay the groundwork for future complexity analysis of more advanced simplex-type algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16561v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liyuan Cao, Wei Hu, Jinxin Wang</dc:creator>
    </item>
    <item>
      <title>Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks</title>
      <link>https://arxiv.org/abs/2508.15695</link>
      <description>arXiv:2508.15695v1 Announce Type: cross 
Abstract: We present several advances to the physics and equality constrained artificial neural networks (PECANN) framework that substantially improve its capability to learn solutions of canonical partial differential equations (PDEs). First, we generalize the augmented Lagrangian method (ALM) to support multiple independent penalty parameters, enabling simultaneous enforcement of heterogeneous constraints. Second, we reformulate pointwise constraint enforcement and Lagrange multipliers as expectations over constraint terms, reducing memory overhead and permitting efficient mini-batch training. Third, to address PDEs with oscillatory, multi-scale features, we incorporate Fourier feature mappings and show that a single mapping suffices where multiple mappings or more costly architectures were required in related methods. Fourth, we introduce a time-windowing strategy for long-time evolution in which the terminal state of each window is enforced as an initial-condition constraint for the next, ensuring continuity without discrete time models. Crucially, we propose a conditionally adaptive penalty update (CAPU) strategy for ALM, which preserves the principle that larger constraint violations incur stronger penalties. CAPU accelerates the growth of Lagrange multipliers for selectively challenging constraints, enhancing constraint enforcement during training. We demonstrate the effectiveness of PECANN-CAPU on problems including the transonic rarefaction problem, reversible advection of a passive by a vortex, high-wavenumber Helmholtz and Poisson equations, and inverse identification of spatially varying heat sources. Comparisons with established methods and recent Kolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive accuracy across all cases. Collectively, these advances improve PECANN's robustness, efficiency, and applicability to demanding problems in scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15695v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qifeng Hu, Shamsulhaq Basir, Inanc Senocak</dc:creator>
    </item>
    <item>
      <title>An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem</title>
      <link>https://arxiv.org/abs/2508.15949</link>
      <description>arXiv:2508.15949v1 Announce Type: cross 
Abstract: Hybridizing machine learning techniques with metaheuristics has attracted significant attention in recent years. Many attempts employ supervised or reinforcement learning to support the decision-making of heuristic methods. However, in some cases, these techniques are deemed too time-consuming and not competitive with hand-crafted heuristics. This paper proposes a hybridization between metaheuristics and a less expensive learning strategy to extract the latent structure of graphs, known as Graph Representation Learning (GRL). For such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a hierarchical graph visualization problem. There is limited literature on methods for this problem, for which Greedy Randomized Search Procedures (GRASP) heuristics have shown promising results. In line with this, this paper investigates the gains of incorporating GRL into the construction phase of GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational experiments, we first analyze the results achieved considering different node embedding techniques, where deep learning-based strategies stood out. The evaluation considered the primal integral measure that assesses the quality of the solutions according to the required time for such. According to this measure, the best GL-GRASP heuristics demonstrated superior performance than state-of-the-art literature GRASP heuristics for the problem. A scalability test on newly generated denser instances under a fixed time limit further confirmed the robustness of the GL-GRASP heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15949v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruna C. B. Charytitsch, Mar\'ia C. V. Nascimento</dc:creator>
    </item>
    <item>
      <title>Data-Driven Analysis and Predictive Control of Descriptor Systems with Application to Power and Water Networks</title>
      <link>https://arxiv.org/abs/2508.16091</link>
      <description>arXiv:2508.16091v1 Announce Type: cross 
Abstract: Despite growing interest in data-driven analysis and control of linear systems, descriptor systems--which are essential for modeling complex engineered systems with algebraic constraints like power and water networks--have received comparatively little attention. This paper develops a comprehensive data-driven framework for analyzing and controlling discrete-time descriptor systems without relying on explicit state-space models. We address fundamental challenges posed by non-causality through the construction of forward and backward data matrices, establishing data-based sufficient conditions for controllability and observability in terms of input-output data, where both R-controllability and C-controllability (R-observability and C-observability) have been considered. We then extend Willems' fundamental lemma to incompletely controllable systems. These methodological advances enable Data-Enabled Predictive Control (DeePC) to achieve output tracking in descriptor systems and to maintain performance under incomplete controllability conditions, as demonstrated in two case studies: i) Frequency regulation in an IEEE 9-bus power system with 3 generators, where DeePC maintained the frequency stability of the power system despite deliberate violations of R-controllability; and ii) Pressure head control in an EPANET water network with 3 tanks, 2 reservoirs, and 117 pipes, where output tracking was successfully enforced under algebraic constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16091v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Zhang, Yu Wang, Jun Shang, Jinhui Zhang</dc:creator>
    </item>
    <item>
      <title>Is the optimal magnetic rectangle a square?</title>
      <link>https://arxiv.org/abs/2508.16152</link>
      <description>arXiv:2508.16152v1 Announce Type: cross 
Abstract: We are concerned with the dependence of the lowest eigenvalue of the magnetic Dirichlet Laplacian on the geometry of rectangles, subject to homogeneous fields. We conjecture that the square is a global minimiser both under the area or perimeter constraints. Contrary to the well-known magnetic-free analogue, the present spectral problem does not admit explicit solutions. By establishing lower and upper bound to the eigenvalue, we establish the conjecture for weak magnetic fields. Moreover, we relate the validity of the conjecture to the simplicity of the eigenvalue and symmetries of minimisers of a non-convex minimisation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16152v1</guid>
      <category>math.SP</category>
      <category>math-ph</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Krejcirik</dc:creator>
    </item>
    <item>
      <title>$\ell_{1}^{2}-\eta\ell_{2}^{2}$ sparsity regularization for nonlinear ill-posed problems</title>
      <link>https://arxiv.org/abs/2508.16163</link>
      <description>arXiv:2508.16163v1 Announce Type: cross 
Abstract: In this study, we investigate the $\left\|\cdot\right\|_{\ell_{1}}^{2}-\eta\left\|\cdot\right\|_{\ell_{2}}^{2}$ sparsity regularization with $0&lt; \eta\leq 1$, in the context of nonlinear ill-posed inverse problems. We focus on the examination of the well-posedness associated with this regularization approach. Notably, the case where $\eta=1$ presents weaker theoretical outcomes than $0&lt; \eta&lt;1$, primarily due to the absence of coercivity and the Radon-Riesz property associated with the regularization term. Under specific conditions pertaining to the nonlinearity of the operator $F$, we establish that every minimizer of the $\left\|\cdot\right\|_{\ell_{1}}^{2}-\eta\left\|\cdot\right\|_{\ell_{2}}^{2}$ regularization exhibits sparsity. Moreover, for the case where $0&lt;\eta&lt;1$, we demonstrate convergence rates of $\mathcal{O}\left(\delta^{1/2}\right)$ and $\mathcal{O}\left(\delta\right)$ for the regularized solution, concerning a sparse exact solution, under differing yet widely accepted conditions related to the nonlinearity of $F$. Additionally, we present the iterative half variation algorithm as an effective method for addressing the $\left\|\cdot\right\|_{\ell_{1}}^{2}-\eta\left\|\cdot\right\|_{\ell_{2}}^{2}$ regularization in the domain of nonlinear ill-posed equations. Numerical results provided corroborate the effectiveness of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16163v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long Li, Liang Ding</dc:creator>
    </item>
    <item>
      <title>Clifford Accelerated Adaptive QAOA</title>
      <link>https://arxiv.org/abs/2508.16443</link>
      <description>arXiv:2508.16443v1 Announce Type: cross 
Abstract: Clifford Circuit Initializaton improves on initial guess of parameters on Parametric Quantum Circuits (PQCs) by leveraging efficient simulation of circuits made out of gates from the Clifford Group. The parameter space is pre-optimized by exploring the Hilbert space in a reduced ensemble of Clifford-expressible points (Clifford Points), providing better initialization. Simultaneously, dynamical circuit reconfiguration algorithms, such as ADAPT-QAOA, improve on QAOA performances by providing a gate re-configuration routine while the optimization is being executed. In this article, we show that Clifford Point approximations at multiple levels of ADAPT allow for multiple improvements while increasing quantum-classical integration opportunities. First we show numerically that Clifford Point preoptimization offers non-trivial gate-selection behavior in ADAPT with some possible convergence improvement. Second, that Clifford Point approximations allows for more suited, fully parallel and fully classical ADAPT operator selection for MaxCut and the TFIM problem. Finally, we show that applying 10 to 30\% error approximation on T-gates using low-rank stabilizer decomposition can provide significative improvements in convergence quality for the MaxCut and TFIM problem. The latter hints at significant T-gate over-representation in antsatz design, opening opportunities for aggressive compilation optimizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16443v1</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Lisart-Liebermann, Arcesio Casta\~neda Medina</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)</title>
      <link>https://arxiv.org/abs/2508.16474</link>
      <description>arXiv:2508.16474v1 Announce Type: cross 
Abstract: This work presents a novel reinforcement learning (RL) algorithm based on Y-wise Affine Neural Networks (YANNs). YANNs provide an interpretable neural network which can exactly represent known piecewise affine functions of arbitrary input and output dimensions defined on any amount of polytopic subdomains. One representative application of YANNs is to reformulate explicit solutions of multi-parametric linear model predictive control. Built on this, we propose the use of YANNs to initialize RL actor and critic networks, which enables the resulting YANN-RL control algorithm to start with the confidence of linear optimal control. The YANN-actor is initialized by representing the multi-parametric control solutions obtained via offline computation using an approximated linear system model. The YANN-critic represents the explicit form of the state-action value function for the linear system and the reward function as the objective in an optimal control problem (OCP). Additional network layers are injected to extend YANNs for nonlinear expressions, which can be trained online by directly interacting with the true complex nonlinear system. In this way, both the policy and state-value functions exactly represent a linear OCP initially and are able to eventually learn the solution of a general nonlinear OCP. Continuous policy improvement is also implemented to provide heuristic confidence that the linear OCP solution serves as an effective lower bound to the performance of RL policy. The YANN-RL algorithm is demonstrated on a clipped pendulum and a safety-critical chemical-reactive system. Our results show that YANN-RL significantly outperforms the modern RL algorithm using deep deterministic policy gradient, especially when considering safety constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16474v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Austin Braniff, Yuhe Tian</dc:creator>
    </item>
    <item>
      <title>FraPPE: Fast and Efficient Preference-based Pure Exploration</title>
      <link>https://arxiv.org/abs/2508.16487</link>
      <description>arXiv:2508.16487v1 Announce Type: cross 
Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given confidence level the set of Pareto optimal arms in a vector-valued (aka multi-objective) bandit, where the reward vectors are ordered via a (given) preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied, there does not exist a computationally efficient algorithm that can optimally track the existing lower bound for arbitrary preference cones. We successfully fill this gap by efficiently solving the minimisation and maximisation problems in the lower bound. First, we derive three structural properties of the lower bound that yield a computationally tractable reduction of the minimisation problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation problem in the lower bound. Together, these techniques solve the maxmin optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with $K$ arms and $L$ dimensional reward, which is a significant acceleration over the literature. We further prove that our proposed PrePEx algorithm, FraPPE, asymptotically achieves the optimal sample complexity. Finally, we perform numerical experiments across synthetic and real datasets demonstrating that FraPPE achieves the lowest sample complexities to identify the exact Pareto set among the existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16487v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Udvas Das, Apurv Shukla, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation</title>
      <link>https://arxiv.org/abs/2508.16540</link>
      <description>arXiv:2508.16540v1 Announce Type: cross 
Abstract: We present a comprehensive theoretical analysis of first-order methods for escaping strict saddle points in smooth non-convex optimization. Our main contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully explicit constants and a rigorous separation between gradient-descent and saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with $\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient evaluations for the descent phase plus $O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode, with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our theoretical predictions through extensive experiments across both synthetic functions and practical machine learning tasks, confirming the logarithmic dimension dependence and the predicted per-episode function decrease. We also provide complete algorithmic specifications including a finite-difference variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch sizing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16540v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faruk Alpay, Hamdi Alakkad</dc:creator>
    </item>
    <item>
      <title>Majorization-minimization Bregman proximal gradient algorithms for NMF with the Kullback--Leibler divergence</title>
      <link>https://arxiv.org/abs/2405.11185</link>
      <description>arXiv:2405.11185v5 Announce Type: replace 
Abstract: Nonnegative matrix factorization (NMF) is a popular method in machine learning and signal processing to decompose a given nonnegative matrix into two nonnegative matrices. In this paper, we propose new algorithms, called majorization-minimization Bregman proximal gradient algorithm (MMBPG) and MMBPG with extrapolation (MMBPGe) to solve NMF. These iterative algorithms minimize the objective function and its potential function monotonically. Assuming the Kurdyka--\L{}ojasiewicz property, we establish that a sequence generated by MMBPG(e) globally converges to a stationary point. We apply MMBPG and MMBPGe to the Kullback--Leibler (KL) divergence-based NMF. While most existing KL-based NMF methods update two blocks or each variable alternately, our algorithms update all variables simultaneously. MMBPG and MMBPGe for KL-based NMF are equipped with a separable Bregman distance that satisfies the smooth adaptable property and that makes its subproblem solvable in closed form. Using this fact, we guarantee that a sequence generated by MMBPG(e) globally converges to a Karush--Kuhn--Tucker (KKT) point of KL-based NMF. In numerical experiments, we compare proposed algorithms with existing algorithms on synthetic data and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11185v5</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takahashi, Mirai Tanaka, Shiro Ikeda</dc:creator>
    </item>
    <item>
      <title>On the nonlinear programming problems subject to a system of generalized bipolar fuzzy relational equalities defined with continuous t-norms</title>
      <link>https://arxiv.org/abs/2411.15225</link>
      <description>arXiv:2411.15225v2 Announce Type: replace 
Abstract: As a starting point, this paper develops the system of bipolar fuzzy relational equations (FRE) to the most general case, where bipolar FREs are defined by an arbitrary continuous t-norm. Due to the fact that fuzzy relational equations are special cases of bipolar FREs, the proposed system can also be viewed as a generalization of traditional FREs, in which the fuzzy composition can be defined by a continuous t-norm. In order to determine the feasibility of the proposed system, some necessary and sufficient conditions are presented for studying continuous bipolar FREs. This is followed by a complete analysis of the set of feasible solutions to the problem. Contrary to FREs and bipolar FREs defined by continuous Archimedean t-norms, the feasible solutions set of generalized bipolar FREs consists of a finite number of compact sets that are not necessarily connected. Further, five techniques have been outlined in an attempt to simplify the current problem, and then an algorithm has been presented to find the feasible region of the problem. Next, we present a class of optimization models subject to continuous bipolar FRE constraints, in which the objective function incorporates a wide range of (non)linear functions, such as maximum functions, geometric mean functions, log-sum-exp functions, maximum eigenvalues of symmetric matrices, support functions for sets, etc. Considering that the problem has a finite number of local optimal solutions, the global optimal solution can always be obtained by choosing the point with the minimum objective value among these local optimal solutions. Lastly, as a means to illustrate the definitions, theorems, and algorithms presented in the paper, a step-by-step example is presented in several sections, in which the constraints are a system of bipolar FREs defined by the Dubois-Prade t-norm, which is a continuous non-Archimedean t-norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15225v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.22111/ijfs.2025.50433.8905</arxiv:DOI>
      <arxiv:journal_reference>Volume 22, Issue 3 May and June 2025 Pages 21-38</arxiv:journal_reference>
      <dc:creator>Amin Ghodousian, Mohammad Sedigh Chopannavaz</dc:creator>
    </item>
    <item>
      <title>Revisit Gradient Descent for Geodesically Convex Optimization</title>
      <link>https://arxiv.org/abs/2504.06814</link>
      <description>arXiv:2504.06814v3 Announce Type: replace 
Abstract: In a seminal work of Zhang and Sra, gradient descent methods for geodesically convex optimization were comprehensively studied. In particular, based on a refined use of the triangle comparison theorem of Toponogov, Zhang and Sra derived a comparison inequality that relates the current iterate, the next iterate and the optimum point. Since their seminal work, numerous follow-ups have studied different downstream usages of their comparison lemma. However, all results along this line relies on strong assumptions, such as bounded domain assumption or curvature bounded below assumption.
  In this work, we introduce the concept of quasilinearization to optimization, presenting a novel framework for analyzing geodesically convex optimization. By leveraging this technique, we establish state-of-the-art convergence rates -- for both deterministic and stochastic settings -- under substantially weaker assumptions than previously required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06814v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunlu Shu, Jiaxin Jiang, Lei Shi, Tianyu Wang</dc:creator>
    </item>
    <item>
      <title>Implicit Regularization Makes Overparameterized Asymmetric Matrix Sensing Robust to Perturbations</title>
      <link>https://arxiv.org/abs/2309.01796</link>
      <description>arXiv:2309.01796v2 Announce Type: replace-cross 
Abstract: Several key questions remain unanswered regarding overparameterized learning models. It is unclear how (stochastic) gradient descent finds solutions that generalize well, and in particular the role of small random initializations. Matrix sensing, which is the problem of reconstructing a low-rank matrix from a few linear measurements, has become a standard prototypical setting to study these phenomena. Previous works have shown that matrix sensing can be solved by factorized gradient descent, provided the random initialization is extremely small.
  In this paper, we find that factorized gradient descent is highly robust to certain perturbations. This lets us use a perturbation term to capture both the effects of imperfect measurements, discretization by gradient descent, and other noise, resulting in a general formulation which we call \textit{perturbed gradient flow}. We find that not only is this equivalent formulation easier to work with, but it leads to sharper sample and time complexities than previous work, handles moderately small initializations, and the results are naturally robust to perturbations such as noisy measurements or changing measurement matrices. Finally, we also analyze mini-batch stochastic gradient descent using the formulation, where we find improved sample complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01796v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan S. Wind</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Jump-Diffusions, with Financial Applications</title>
      <link>https://arxiv.org/abs/2405.16449</link>
      <description>arXiv:2405.16449v4 Announce Type: replace-cross 
Abstract: We study continuous-time reinforcement learning (RL) for stochastic control in which system dynamics are governed by jump-diffusion processes. We formulate an entropy-regularized exploratory control problem with stochastic policies to capture the exploration--exploitation balance essential for RL. Unlike the pure diffusion case initially studied by Wang et al. (2020), the derivation of the exploratory dynamics under jump-diffusions calls for a careful formulation of the jump part. Through a theoretical analysis, we find that one can simply use the same policy evaluation and $q$-learning algorithms in Jia and Zhou (2022a, 2023), originally developed for controlled diffusions, without needing to check a priori whether the underlying data come from a pure diffusion or a jump-diffusion. However, we show that the presence of jumps ought to affect parameterizations of actors and critics in general. We investigate as an application the mean--variance portfolio selection problem with stock price modelled as a jump-diffusion, and show that both RL algorithms and parameterizations are invariant with respect to jumps. Finally, we present a detailed study on applying the general theory to option hedging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16449v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefeng Gao, Lingfei Li, Xun Yu Zhou</dc:creator>
    </item>
    <item>
      <title>Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.01819</link>
      <description>arXiv:2502.01819v3 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area use a discrete-time formulation, which is prone to induced discretization errors, and often not applicable to models with higher-order/black-box solvers. The objective of this study is to develop a disciplined approach to fine-tune diffusion models using continuous-time RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby making connections to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models of Stable Diffusion v1.5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01819v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyang Zhao, Haoxian Chen, Ji Zhang, David D. Yao, Wenpin Tang</dc:creator>
    </item>
    <item>
      <title>Transient performance of MPC for tracking without terminal constraints</title>
      <link>https://arxiv.org/abs/2506.10589</link>
      <description>arXiv:2506.10589v2 Announce Type: replace-cross 
Abstract: Model predictive control (MPC) for tracking is a recently introduced approach, which extends standard MPC formulations by incorporating an artificial reference as an additional optimization variable, in order to track external and potentially time-varying references. In this work, we analyze the performance of such an MPC for tracking scheme without a terminal cost and terminal constraints. We derive a transient performance estimate, i.e. a bound on the closed-loop performance over an arbitrary time interval, yielding insights on how to select the scheme's parameters for performance. Furthermore, we show that in the asymptotic case, where the prediction horizon and observed time interval tend to infinity, the closed-loop solution of MPC for tracking recovers the infinite horizon optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10589v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCSYS.2025.3585945</arxiv:DOI>
      <arxiv:journal_reference>IEEE Control Systems Letters, vol. 9, pp. 2049-2054, 2025</arxiv:journal_reference>
      <dc:creator>Nadine Ehmann, Matthias K\"ohler, Frank Allg\"ower</dc:creator>
    </item>
    <item>
      <title>Flow Matching-Based Generative Modeling for Efficient and Scalable Data Assimilation</title>
      <link>https://arxiv.org/abs/2508.13313</link>
      <description>arXiv:2508.13313v2 Announce Type: replace-cross 
Abstract: Data assimilation (DA) is the problem of sequentially estimating the state of a dynamical system from noisy observations. Recent advances in generative modeling have inspired new approaches to DA in high-dimensional nonlinear settings, especially the ensemble score filter (EnSF). However, these come at a significant computational burden due to slow sampling. In this paper, we introduce a new filtering framework based on flow matching (FM) -- called the ensemble flow filter (EnFF) -- to accelerate sampling and enable flexible design of probability paths. EnFF -- a training-free DA approach -- integrates MC estimators for the marginal FM vector field (VF) and a localized guidance to assimilate observations. EnFF has faster sampling and more flexibility in VF design compared to existing generative modeling for DA. Theoretically, we show that EnFF encompasses classical filtering methods such as the bootstrap particle filter and the ensemble Kalman filter as special cases. Experiments on high-dimensional filtering benchmarks demonstrate improved cost-accuracy tradeoffs and the ability to leverage larger ensembles than prior methods. Our results highlight the promise of FM as a scalable tool for filtering in high-dimensional applications that enable the use of large ensembles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13313v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taos Transue, Bohan Chen, So Takao, Bao Wang</dc:creator>
    </item>
  </channel>
</rss>
