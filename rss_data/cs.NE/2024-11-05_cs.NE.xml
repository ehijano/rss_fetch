<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2024 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Improved Chicken Swarm Optimization Algorithm for Handwritten Document Image Enhancement</title>
      <link>https://arxiv.org/abs/2411.00802</link>
      <description>arXiv:2411.00802v1 Announce Type: new 
Abstract: Chicken swarm optimization is a new meta-heuristic algorithm which mimics the foraging hierarchical behavior of chicken. In this paper, we describe the preprocessing of handwritten document by contrast enhancement while preserving detail with an improved chicken swarm optimization algorithm.The results of the algorithm are compared with other existing meta heuristic algorithms like Cuckoo Search, Firefly Algorithm and the Artificial bee colony. The proposed algorithm considerably outperforms all the above by giving good results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00802v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Stanley Mugisha, Lynn tar Gutu, P Nagabhushan</dc:creator>
    </item>
    <item>
      <title>Designing a Dataset for Convolutional Neural Networks to Predict Space Groups Consistent with Extinction Laws</title>
      <link>https://arxiv.org/abs/2411.00803</link>
      <description>arXiv:2411.00803v1 Announce Type: new 
Abstract: In this paper, we utilize a dataset composed of one-dimensional powder diffraction patterns to train Convolutional Neural Networks for predicting space groups. We used a new strategy to design the dataset, the diffraction pattern was calculated based the lattice parameters and the Extinction Laws, instead of the traditional strategy that generating it from the crystallographic database. This paper demonstrated that the new strategy is more reasonable than the traditional one. As a result, the model trained on the cubic and tetragonal training set from the newly designed dataset achieves prediction accuracy that matches the theoretical maximums calculated based on Extinction Laws. This result demonstrates that the machine learning based prediction can be physically reasonable and reliable. Additionally, the model trained on our new designed dataset shows better generalization capability than the one trained on a traditionally designed dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00803v1</guid>
      <category>cs.NE</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Jiajun Zhong, Yikun Li, Junrong Zhang, Rong Du</dc:creator>
    </item>
    <item>
      <title>Differentiable architecture search with multi-dimensional attention for spiking neural networks</title>
      <link>https://arxiv.org/abs/2411.00902</link>
      <description>arXiv:2411.00902v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) have gained enormous popularity in the field of artificial intelligence due to their low power consumption. However, the majority of SNN methods directly inherit the structure of Artificial Neural Networks (ANN), usually leading to sub-optimal model performance in SNNs. To alleviate this problem, we integrate Neural Architecture Search (NAS) method and propose Multi-Attention Differentiable Architecture Search (MA-DARTS) to directly automate the search for the optimal network structure of SNNs. Initially, we defined a differentiable two-level search space and conducted experiments within micro architecture under a fixed layer. Then, we incorporated a multi-dimensional attention mechanism and implemented the MA-DARTS algorithm in this search space. Comprehensive experiments demonstrate our model achieves state-of-the-art performance on classification compared to other methods under the same parameters with 94.40% accuracy on CIFAR10 dataset and 76.52% accuracy on CIFAR100 dataset. Additionally, we monitored and assessed the number of spikes (NoS) in each cell during the whole experiment. Notably, the number of spikes of the whole model stabilized at approximately 110K in validation and 100k in training on datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00902v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2024.128181</arxiv:DOI>
      <dc:creator>Yilei Man, Linhai Xie, Shushan Qiao, Yumei Zhou, Delong Shang</dc:creator>
    </item>
    <item>
      <title>Deep memetic models for combinatorial optimization problems: application to the tool switching problem</title>
      <link>https://arxiv.org/abs/2411.01922</link>
      <description>arXiv:2411.01922v1 Announce Type: new 
Abstract: Memetic algorithms are techniques that orchestrate the interplay between population-based and trajectory-based algorithmic components. In particular, some memetic models can be regarded under this broad interpretation as a group of autonomous basic optimization algorithms that interact among them in a cooperative way in order to deal with a specific optimization problem, aiming to obtain better results than the algorithms that constitute it separately. Going one step beyond this traditional view of cooperative optimization algorithms, this work tackles deep meta-cooperation, namely the use of cooperative optimization algorithms in which some components can in turn be cooperative methods themselves, thus exhibiting a deep algorithmic architecture. The objective of this paper is to demonstrate that such models can be considered as an efficient alternative to other traditional forms of cooperative algorithms. To validate this claim, different structural parameters, such as the communication topology between the agents, or the parameter that influences the depth of the cooperative effort (the depth of meta-cooperation), have been analyzed. To do this, a comparison with the state-of-the-art cooperative methods to solve a specific combinatorial problem, the Tool Switching Problem, has been performed. Results show that deep models are effective to solve this problem, outperforming metaheuristics proposed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01922v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s12293-019-00294-1</arxiv:DOI>
      <arxiv:journal_reference>Memetic Computing 12:3-22, 2020</arxiv:journal_reference>
      <dc:creator>Jhon Edgar Amaya, Carlos Cotta, Antonio J. Fern\'andez-Leiva, Pablo Garc\'ia-S\'anchez</dc:creator>
    </item>
    <item>
      <title>Memetic collaborative approaches for finding balanced incomplete block designs</title>
      <link>https://arxiv.org/abs/2411.02250</link>
      <description>arXiv:2411.02250v1 Announce Type: new 
Abstract: The balanced incomplete block design (BIBD) problem is a difficult combinatorial problem with a large number of symmetries, which add complexity to its resolution. In this paper, we propose a dual (integer) problem representation that serves as an alternative to the classical binary formulation of the problem. We attack this problem incrementally: firstly, we propose basic algorithms (i.e. local search techniques and genetic algorithms) intended to work separately on the two different search spaces (i.e. binary and integer); secondly, we propose two hybrid schemes: an integrative approach (i.e. a memetic algorithm) and a collaborative model in which the previous methods work in parallel, occasionally exchanging information. Three distinct two-dimensional structures are proposed as communication topology among the algorithms involved in the collaborative model, as well as a number of migration and acceptance criteria for sending and receiving data. An empirical analysis comparing a large number of instances of our schemes (with algorithms possibly working on different search spaces and with/without symmetry breaking methods) shows that some of these algorithms can be considered the state of the art of the metaheuristic methods applied to finding BIBDs. Moreover, our cooperative proposal is a general scheme from which distinct algorithmic variants can be instantiated to handle symmetrical optimisation problems. For this reason, we have also analysed its key parameters, thereby providing general guidelines for the design of efficient/robust cooperative algorithms devised from our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02250v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cor.2019.104804</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Operations Research 144:104804, 2020</arxiv:journal_reference>
      <dc:creator>David Rodr\'iguez Rueda, Carlos Cotta, Antonio J. Fern\'andez-Leiva</dc:creator>
    </item>
    <item>
      <title>AI-Guided Codesign Framework for Novel Material and Device Design applied to MTJ-based True Random Number Generators</title>
      <link>https://arxiv.org/abs/2411.01008</link>
      <description>arXiv:2411.01008v1 Announce Type: cross 
Abstract: Novel devices and novel computing paradigms are key for energy efficient, performant future computing systems. However, designing devices for new applications is often time consuming and tedious. Here, we investigate the design and optimization of spin orbit torque and spin transfer torque magnetic tunnel junction models as the probabilistic devices for true random number generation. We leverage reinforcement learning and evolutionary optimization to vary key device and material properties of the various device models for stochastic operation. Our AI guided codesign methods generated different candidate devices capable of generating stochastic samples for a desired probability distribution, while also minimizing energy usage for the devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01008v1</guid>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karan P. Patel, Andrew Maicke, Jared Arzate, Jaesuk Kwon, J. Darby Smith, James B. Aimone, Jean Anne C. Incorvia, Suma G. Cardwell, Catherine D. Schuman</dc:creator>
    </item>
    <item>
      <title>Integrating Graph Neural Networks and Many-Body Expansion Theory for Potential Energy Surfaces</title>
      <link>https://arxiv.org/abs/2411.01578</link>
      <description>arXiv:2411.01578v1 Announce Type: cross 
Abstract: Rational design of next-generation functional materials relied on quantitative predictions of their electronic structures beyond single building blocks. First-principles quantum mechanical (QM) modeling became infeasible as the size of a material grew beyond hundreds of atoms. In this study, we developed a new computational tool integrating fragment-based graph neural networks (FBGNN) into the fragment-based many-body expansion (MBE) theory, referred to as FBGNN-MBE, and demonstrated its capacity to reproduce full-dimensional potential energy surfaces (FD-PES) for hierarchic chemical systems with manageable accuracy, complexity, and interpretability. In particular, we divided the entire system into basic building blocks (fragments), evaluated their single-fragment energies using a first-principles QM model and attacked many-fragment interactions using the structure-property relationships trained by FBGNNs. Our development of FBGNN-MBE demonstrated the potential of a new framework integrating deep learning models into fragment-based QM methods, and marked a significant step towards computationally aided design of large functional materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01578v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.NE</category>
      <category>physics.chem-ph</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siqi Chen, Zhiqiang Wang, Xianqi Deng, Yili Shen, Cheng-Wei Ju, Jun Yi, Lin Xiong, Guo Ling, Dieaa Alhmoud, Hui Guan, Zhou Lin</dc:creator>
    </item>
    <item>
      <title>Energy-Aware FPGA Implementation of Spiking Neural Network with LIF Neurons</title>
      <link>https://arxiv.org/abs/2411.01628</link>
      <description>arXiv:2411.01628v1 Announce Type: cross 
Abstract: Tiny Machine Learning (TinyML) has become a growing field in on-device processing for Internet of Things (IoT) applications, capitalizing on AI algorithms that are optimized for their low complexity and energy efficiency. These algorithms are designed to minimize power and memory footprints, making them ideal for the constraints of IoT devices. Within this domain, Spiking Neural Networks (SNNs) stand out as a cutting-edge solution for TinyML, owning to their event-driven processing paradigm which offers an efficient method of handling dataflow. This paper presents a novel SNN architecture based on the 1st Order Leaky Integrate-and-Fire (LIF) neuron model to efficiently deploy vision-based ML algorithms on TinyML systems. A hardware-friendly LIF design is also proposed, and implemented on a Xilinx Artix-7 FPGA. To evaluate the proposed model, a collision avoidance dataset is considered as a case study. The proposed SNN model is compared to the state-of-the-art works and Binarized Convolutional Neural Network (BCNN) as a baseline. The results show the proposed approach is 86% more energy efficient than the baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01628v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmer Hamid Ali, Mozhgan Navardi, Tinoosh Mohsenin</dc:creator>
    </item>
    <item>
      <title>Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation</title>
      <link>https://arxiv.org/abs/2411.02001</link>
      <description>arXiv:2411.02001v1 Announce Type: cross 
Abstract: Local learning, which trains a network through layer-wise local targets and losses, has been studied as an alternative to backpropagation (BP) in neural computation. However, its algorithms often become more complex or require additional hyperparameters because of the locality, making it challenging to identify desirable settings in which the algorithm progresses in a stable manner. To provide theoretical and quantitative insights, we introduce the maximal update parameterization ($\mu$P) in the infinite-width limit for two representative designs of local targets: predictive coding (PC) and target propagation (TP). We verified that $\mu$P enables hyperparameter transfer across models of different widths. Furthermore, our analysis revealed unique and intriguing properties of $\mu$P that are not present in conventional BP. By analyzing deep linear networks, we found that PC's gradients interpolate between first-order and Gauss-Newton-like gradients, depending on the parameterization. We demonstrate that, in specific standard settings, PC in the infinite-width limit behaves more similarly to the first-order gradient. For TP, even with the standard scaling of the last layer, which differs from classical $\mu$P, its local loss optimization favors the feature learning regime over the kernel regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02001v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoki Ishikawa, Rio Yokota, Ryo Karakida</dc:creator>
    </item>
    <item>
      <title>Discrete the solving model of time-variant standard Sylvester-conjugate matrix equations using Euler-forward formula</title>
      <link>https://arxiv.org/abs/2411.02333</link>
      <description>arXiv:2411.02333v1 Announce Type: cross 
Abstract: Time-variant standard Sylvester-conjugate matrix equations are presented as early time-variant versions of the complex conjugate matrix equations. Current solving methods include Con-CZND1 and Con-CZND2 models, both of which use ode45 for continuous model. Given practical computational considerations, discrete these models is also important. Based on Euler-forward formula discretion, Con-DZND1-2i model and Con-DZND2-2i model are proposed. Numerical experiments using step sizes of 0.1 and 0.001. The above experiments show that Con-DZND1-2i model and Con-DZND2-2i model exhibit different neural dynamics compared to their continuous counterparts, such as trajectory correction in Con-DZND2-2i model and the swallowing phenomenon in Con-DZND1-2i model, with convergence affected by step size. These experiments highlight the differences between optimizing sampling discretion errors and space compressive approximation errors in neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02333v1</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiakuang He, Dongqing Wu</dc:creator>
    </item>
    <item>
      <title>Alternate Loss Functions for Classification and Robust Regression Can Improve the Accuracy of Artificial Neural Networks</title>
      <link>https://arxiv.org/abs/2303.09935</link>
      <description>arXiv:2303.09935v3 Announce Type: replace 
Abstract: All machine learning algorithms use a loss, cost, utility or reward function to encode the learning objective and oversee the learning process. This function that supervises learning is a frequently unrecognized hyperparameter that determines how incorrect outputs are penalized and can be tuned to improve performance. This paper shows that training speed and final accuracy of neural networks can significantly depend on the loss function used to train neural networks. In particular derivative values can be significantly different with different loss functions leading to significantly different performance after gradient descent based Backpropagation (BP) training. This paper explores the effect on performance of using new loss functions that are also convex but penalize errors differently compared to the popular Cross-entropy loss. Two new classification loss functions that significantly improve performance on a wide variety of benchmark tasks are proposed. A new loss function call smooth absolute error that outperforms the Squared error, Huber and Log-Cosh losses on datasets with significantly many outliers is proposed. This smooth absolute error loss function is infinitely differentiable and more closely approximates the absolute error loss compared to the Huber and Log-Cosh losses used for robust regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09935v3</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathew Mithra Noel, Arindam Banerjee, Geraldine Bessie Amali D, Venkataraman Muthiah-Nakarajan</dc:creator>
    </item>
    <item>
      <title>Efficient Vectorized Backpropagation Algorithms for Training Feedforward Networks Composed of Quadratic Neurons</title>
      <link>https://arxiv.org/abs/2310.02901</link>
      <description>arXiv:2310.02901v3 Announce Type: replace 
Abstract: Higher order artificial neurons whose outputs are computed by applying an activation function to a higher order multinomial function of the inputs have been considered in the past, but did not gain acceptance due to the extra parameters and computational cost. However, higher order neurons have significantly greater learning capabilities since the decision boundaries of higher order neurons can be complex surfaces instead of just hyperplanes. The boundary of a single quadratic neuron can be a general hyper-quadric surface allowing it to learn many nonlinearly separable datasets. Since quadratic forms can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional parameters are needed instead of $n^2$. A quadratic Logistic regression model is first presented. Solutions to the XOR problem with a single quadratic neuron are considered. The complete vectorized equations for both forward and backward propagation in feedforward networks composed of quadratic neurons are derived. A reduced parameter quadratic neural network model with just $ n $ additional parameters per neuron that provides a compromise between learning ability and computational cost is presented. Comparison on benchmark classification datasets are used to demonstrate that a final layer of quadratic neurons enables networks to achieve higher accuracy with significantly fewer hidden layer neurons. In particular this paper shows that any dataset composed of $\mathcal{C}$ bounded clusters can be separated with only a single layer of $\mathcal{C}$ quadratic neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02901v3</guid>
      <category>cs.NE</category>
      <category>cs.CV</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathew Mithra Noel, Venkataraman Muthiah-Nakarajan</dc:creator>
    </item>
    <item>
      <title>ORS: A novel Olive Ridley Survival inspired Meta-heuristic Optimization Algorithm</title>
      <link>https://arxiv.org/abs/2409.09210</link>
      <description>arXiv:2409.09210v2 Announce Type: replace 
Abstract: Meta-heuristic algorithmic development has been a thrust area of research since its inception. In this paper, a novel meta-heuristic optimization algorithm, Olive Ridley Survival (ORS), is proposed which is inspired from survival challenges faced by hatchlings of Olive Ridley sea turtle. A major fact about survival of Olive Ridley reveals that out of one thousand Olive Ridley hatchlings which emerge from nest, only one survive at sea due to various environmental and other factors. This fact acts as the backbone for developing the proposed algorithm. The algorithm has two major phases: hatchlings survival through environmental factors and impact of movement trajectory on its survival. The phases are mathematically modelled and implemented along with suitable input representation and fitness function. The algorithm is analysed theoretically. To validate the algorithm, fourteen mathematical benchmark functions from standard CEC test suites are evaluated and statistically tested. Also, to study the efficacy of ORS on recent complex benchmark functions, ten benchmark functions of CEC-06-2019 are evaluated. Further, three well-known engineering problems are solved by ORS and compared with other state-of-the-art meta-heuristics. Simulation results show that in many cases, the proposed ORS algorithm outperforms some state-of-the-art meta-heuristic optimization algorithms. The sub-optimal behavior of ORS in some recent benchmark functions is also observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09210v2</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niranjan Panigrahi, Sourav Kumar Bhoi, Debasis Mohapatra, Rashmi Ranjan Sahoo, Kshira Sagar Sahoo, Anil Mohapatra</dc:creator>
    </item>
  </channel>
</rss>
