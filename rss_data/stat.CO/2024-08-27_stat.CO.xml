<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Aug 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating the Effective Sample Size for an inverse problem in subsurface flows</title>
      <link>https://arxiv.org/abs/2408.13411</link>
      <description>arXiv:2408.13411v1 Announce Type: cross 
Abstract: The Effective Sample Size (ESS) and Integrated Autocorrelation Time (IACT) are two popular criteria for comparing Markov Chain Monte Carlo (MCMC) algorithms and detecting their convergence. Our goal is to assess those two quantities in the context of an inverse problem in subsurface flows. We begin by presenting a review of some popular methods for their estimation, and then simulate their sample distributions on AR(1) sequences for which the exact values were known. We find that those ESS estimators may not be statistically consistent, because their variance grows linearly in the number of sample values of the MCMC. Next, we analyze the output of two distinct MCMC algorithms for the Bayesian approach to the simulation of an elliptic inverse problem. Here, the estimators cannot even agree about the order of magnitude of the ESS. Our conclusion is that the ESS has major limitations and should not be used on MCMC outputs of complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13411v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucas Seiffert, Felipe Pereira</dc:creator>
    </item>
    <item>
      <title>Robust Principal Components by Casewise and Cellwise Weighting</title>
      <link>https://arxiv.org/abs/2408.13596</link>
      <description>arXiv:2408.13596v1 Announce Type: cross 
Abstract: Principal component analysis (PCA) is a fundamental tool for analyzing multivariate data. Here the focus is on dimension reduction to the principal subspace, characterized by its projection matrix. The classical principal subspace can be strongly affected by the presence of outliers. Traditional robust approaches consider casewise outliers, that is, cases generated by an unspecified outlier distribution that differs from that of the clean cases. But there may also be cellwise outliers, which are suspicious entries that can occur anywhere in the data matrix. Another common issue is that some cells may be missing. This paper proposes a new robust PCA method, called cellPCA, that can simultaneously deal with casewise outliers, cellwise outliers, and missing cells. Its single objective function combines two robust loss functions, that together mitigate the effect of casewise and cellwise outliers. The objective function is minimized by an iteratively reweighted least squares (IRLS) algorithm. Residual cellmaps and enhanced outlier maps are proposed for outlier detection. The casewise and cellwise influence functions of the principal subspace are derived, and its asymptotic distribution is obtained. Extensive simulations and two real data examples illustrate the performance of cellPCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13596v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Centofanti, Mia Hubert, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Bayesian neural networks via MCMC: a Python-based tutorial</title>
      <link>https://arxiv.org/abs/2304.02595</link>
      <description>arXiv:2304.02595v3 Announce Type: replace-cross 
Abstract: Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling methods are used to implement Bayesian inference. In the past three decades, MCMC sampling methods have faced some challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposal distributions that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to statisticians and currently not well-known among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of libraries and tutorials to this end. This tutorial provides code in Python with data and instructions that enable their use and extension. We provide results for some benchmark problems showing the strengths and weaknesses of implementing the respective Bayesian models via MCMC. We highlight the challenges in sampling multi-modal posterior distributions for the case of Bayesian neural networks and the need for further improvement of convergence diagnosis methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02595v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3401234</arxiv:DOI>
      <dc:creator>Rohitash Chandra, Joshua Simmons</dc:creator>
    </item>
    <item>
      <title>Inferring Covariance Structure from Multiple Data Sources via Subspace Factor Analysis</title>
      <link>https://arxiv.org/abs/2305.04113</link>
      <description>arXiv:2305.04113v3 Announce Type: replace-cross 
Abstract: Factor analysis provides a canonical framework for imposing lower-dimensional structure such as sparse covariance in high-dimensional data. High-dimensional data on the same set of variables are often collected under different conditions, for instance in reproducing studies across research groups. In such cases, it is natural to seek to learn the shared versus condition-specific structure. Existing hierarchical extensions of factor analysis have been proposed, but face practical issues including identifiability problems. To address these shortcomings, we propose a class of SUbspace Factor Analysis (SUFA) models, which characterize variation across groups at the level of a lower-dimensional subspace. We prove that the proposed class of SUFA models lead to identifiability of the shared versus group-specific components of the covariance, and study their posterior contraction properties. Taking a Bayesian approach, these contributions are developed alongside efficient posterior computation algorithms. Our sampler fully integrates out latent variables, is easily parallelizable and has complexity that does not depend on sample size. We illustrate the methods through application to integration of multiple gene expression datasets relevant to immunology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04113v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noirrit Kiran Chandra, David B. Dunson, Jason Xu</dc:creator>
    </item>
    <item>
      <title>cpp11armadillo: An R Package to Use the Armadillo C++ Library</title>
      <link>https://arxiv.org/abs/2408.11074</link>
      <description>arXiv:2408.11074v2 Announce Type: replace-cross 
Abstract: This article introduces 'cpp11armadillo', a new R package that integrates the powerful Armadillo C++ library for linear algebra into the R programming environment. Targeted primarily at social scientists and other non-programmers, this article explains the computational benefits of moving code to C++ in terms of speed and syntax. We provide a comprehensive overview of Armadillo's capabilities, highlighting its user-friendly syntax akin to MATLAB and its efficiency for computationally intensive tasks. The 'cpp11armadillo' package simplifies a part of the process of using C++ within R by offering additional ease of integration for those who require high-performance linear algebra operations in their R workflows. This work aims to bridge the gap between computational efficiency and accessibility, making advanced linear algebra operations more approachable for R users without extensive programming backgrounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11074v2</guid>
      <category>cs.MS</category>
      <category>cs.PL</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda, Jonathan Schneider Malamud</dc:creator>
    </item>
  </channel>
</rss>
