<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Apr 2024 04:07:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Predictive Algorithms to Automatic Generation of Anomalies</title>
      <link>https://arxiv.org/abs/2404.10111</link>
      <description>arXiv:2404.10111v1 Announce Type: new 
Abstract: Machine learning algorithms can find predictive signals that researchers fail to notice; yet they are notoriously hard-to-interpret. How can we extract theoretical insights from these black boxes? History provides a clue. Facing a similar problem -- how to extract theoretical insights from their intuitions -- researchers often turned to ``anomalies:'' constructed examples that highlight flaws in an existing theory and spur the development of new ones. Canonical examples include the Allais paradox and the Kahneman-Tversky choice experiments for expected utility theory. We suggest anomalies can extract theoretical insights from black box predictive algorithms. We develop procedures to automatically generate anomalies for an existing theory when given a predictive algorithm. We cast anomaly generation as an adversarial game between a theory and a falsifier, the solutions to which are anomalies: instances where the black box algorithm predicts - were we to collect data - we would likely observe violations of the theory. As an illustration, we generate anomalies for expected utility theory using a large, publicly available dataset on real lottery choices. Based on an estimated neural network that predicts lottery choices, our procedures recover known anomalies and discover new ones for expected utility theory. In incentivized experiments, subjects violate expected utility theory on these algorithmically generated anomalies; moreover, the violation rates are similar to observed rates for the Allais paradox and Common ratio effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10111v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sendhil Mullainathan, Ashesh Rambachan</dc:creator>
    </item>
    <item>
      <title>Lorenz map, inequality ordering and curves based on multidimensional rearrangements</title>
      <link>https://arxiv.org/abs/2203.09000</link>
      <description>arXiv:2203.09000v4 Announce Type: replace 
Abstract: We propose a multivariate extension of the Lorenz curve based on multivariate rearrangements of optimal transport theory. We define a vector Lorenz map as the integral of the vector quantile map associated with a multivariate resource allocation. Each component of the Lorenz map is the cumulative share of each resource, as in the traditional univariate case. The pointwise ordering of such Lorenz maps defines a new multivariate majorization order, which is equivalent to preference by any social planner with inequality averse multivariate rank dependent social evaluation functional. We define a family of multi-attribute Gini index and complete ordering based on the Lorenz map. We propose the level sets of an Inverse Lorenz Function as a practical tool to visualize and compare inequality in two dimensions, and apply it to income-wealth inequality in the United States between 1989 and 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09000v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanqin Fan, Marc Henry, Brendan Pass, Jorge A. Rivero</dc:creator>
    </item>
    <item>
      <title>Causal Machine Learning for Moderation Effects</title>
      <link>https://arxiv.org/abs/2401.08290</link>
      <description>arXiv:2401.08290v2 Announce Type: replace 
Abstract: It is valuable for any decision maker to know the impact of decisions (treatments) on average and for subgroups. The causal machine learning literature has recently provided tools for estimating group average treatment effects (GATE) to understand treatment heterogeneity better. This paper addresses the challenge of interpreting such differences in treatment effects between groups while accounting for variations in other covariates. We propose a new parameter, the balanced group average treatment effect (BGATE), which measures a GATE with a specific distribution of a priori-determined covariates. By taking the difference of two BGATEs, we can analyse heterogeneity more meaningfully than by comparing two GATEs. The estimation strategy for this parameter is based on double/debiased machine learning for discrete treatments in an unconfoundedness setting, and the estimator is shown to be $\sqrt{N}$-consistent and asymptotically normal under standard conditions. Adding additional identifying assumptions allows specific balanced differences in treatment effects between groups to be interpreted causally, leading to the causal balanced group average treatment effect. We explore the finite sample properties in a small-scale simulation study and demonstrate the usefulness of these parameters in an empirical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08290v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nora Bearth, Michael Lechner</dc:creator>
    </item>
    <item>
      <title>Inference under partial identification with minimax test statistics</title>
      <link>https://arxiv.org/abs/2401.13057</link>
      <description>arXiv:2401.13057v2 Announce Type: replace 
Abstract: We provide a means of computing and estimating the asymptotic distributions of statistics based on an outer minimization of an inner maximization. Such test statistics, which arise frequently in moment models, are of special interest in providing hypothesis tests under partial identification. Under general conditions, we provide an asymptotic characterization of such test statistics using the minimax theorem, and a means of computing critical values using the bootstrap. Making some light regularity assumptions, our results augment several asymptotic approximations that have been provided for partially identified hypothesis tests, and extend them by mitigating their dependence on local linear approximations of the parameter space. These asymptotic results are generally simple to state and straightforward to compute (esp.\ adversarially).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13057v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Loh</dc:creator>
    </item>
    <item>
      <title>Extending the Scope of Inference About Predictive Ability to Machine Learning Methods</title>
      <link>https://arxiv.org/abs/2402.12838</link>
      <description>arXiv:2402.12838v2 Announce Type: replace 
Abstract: Though out-of-sample forecast evaluation is systematically employed with modern machine learning methods and there exists a well-established classic inference theory for predictive ability, see, e.g., West (1996, Asymptotic Inference About Predictive Ability, Econometrica, 64, 1067-1084), such theory is not directly applicable to modern machine learners such as the Lasso in the high dimensional setting. We investigate under which conditions such extensions are possible. Two key properties for standard out-of-sample asymptotic inference to be valid with machine learning are (i) a zero-mean condition for the score of the prediction loss function; and (ii) a fast rate of convergence for the machine learner. Monte Carlo simulations confirm our theoretical findings. We recommend a small out-of-sample vs in-sample size ratio for accurate finite sample inferences with machine learning. We illustrate the wide applicability of our results with a new out-of-sample test for the Martingale Difference Hypothesis (MDH). We obtain the asymptotic null distribution of our test and use it to evaluate the MDH of some major exchange rates at daily and higher frequencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12838v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Carlos Escanciano, Ricardo Parra</dc:creator>
    </item>
    <item>
      <title>Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model</title>
      <link>https://arxiv.org/abs/2403.07236</link>
      <description>arXiv:2403.07236v4 Announce Type: replace 
Abstract: It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary while imposing very few restrictions on the underlying data generating process. I construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available. This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified or seek out data aggregated at the finest level possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07236v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Moon</dc:creator>
    </item>
    <item>
      <title>Julia as a universal platform for statistical software development</title>
      <link>https://arxiv.org/abs/2404.09309</link>
      <description>arXiv:2404.09309v2 Announce Type: replace 
Abstract: Like Python and Java, which are integrated into Stata, Julia is a free programming language that runs on all major operating systems. The julia package links Stata to Julia as well. Users can transfer data between Stata and Julia at high speed, issue Julia commands from Stata to analyze and plot, and pass results back to Stata. Julia's econometric software ecosystem is not as mature as Stata's or R's, or even Python's. But Julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms. The boottest program for wild bootstrap-based inference (Roodman et al. 2019) can call a Julia back end for a 33-50% speed-up, even as the R package fwildclusterboot (Fischer and Roodman 2021) uses the same back end for inference after instrumental variables estimation. reghdfejl mimics reghdfe (Correia 2016) in fitting linear models with high-dimensional fixed effects but calls an independently developed Julia package for tenfold acceleration on hard problems. reghdfejl also supports nonlinear models--preliminarily, as the Julia package for that purpose matures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09309v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Roodman</dc:creator>
    </item>
  </channel>
</rss>
