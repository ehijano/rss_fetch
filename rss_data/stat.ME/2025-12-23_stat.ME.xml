<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Dec 2025 05:01:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deep Gaussian Processes with Gradients</title>
      <link>https://arxiv.org/abs/2512.18066</link>
      <description>arXiv:2512.18066v1 Announce Type: new 
Abstract: Deep Gaussian processes (DGPs) are popular surrogate models for complex nonstationary computer experiments. DGPs use one or more latent Gaussian processes (GPs) to warp the input space into a plausibly stationary regime, then use typical GP regression on the warped domain. While this composition of GPs is conceptually straightforward, the functional nature of the multi-dimensional latent warping makes Bayesian posterior inference challenging. Traditional GPs with smooth kernels are naturally suited for the integration of gradient information, but the integration of gradients within a DGP presents new challenges and has yet to be explored. We propose a novel and comprehensive Bayesian framework for DGPs with gradients that facilitates both gradient-enhancement and gradient posterior predictive distributions. We provide open-source software in the "deepgp" package on CRAN, with optional Vecchia approximation to circumvent cubic computational bottlenecks. We benchmark our DGPs with gradients on a variety of nonstationary simulations, showing improvement over both GPs with gradients and conventional DGPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18066v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annie S. Booth</dc:creator>
    </item>
    <item>
      <title>Data adaptive covariate balancing for causal effect estimation for high dimensional data</title>
      <link>https://arxiv.org/abs/2512.18069</link>
      <description>arXiv:2512.18069v1 Announce Type: new 
Abstract: A key challenge in estimating causal effects from observational data is handling confounding and is commonly achieved through weighting methods that balance distribution of covariates between treatment and control groups. Weighting approaches can be classified by whether weights are estimated using parametric or nonparametric methods, and by whether the model relies on modeling and inverting the propensity score or directly estimates weights to achieve distributional balance by minimizing a measure of dissimilarity between groups. Parametric methods, both for propensity score modeling and direct balancing, are prone to model misspecification. In addition, balancing approaches often suffer from the curse of dimensionality, as they assign equal importance to all covariates, thus potentially de-emphasizing true confounders. Several methods, such as the outcome adaptive lasso, attempt to mitigate this issue through variable selection, but are parametric and focus on propensity score estimation rather than direct balancing. In this paper, we propose a nonparametric direct balancing approach that uses random forests to adaptively emphasize confounders. Our method jointly models treatment and outcome using random forests, allowing the data to identify covariates that influence both processes. We construct a similarity measure, defined by the proportion of trees in which two observations fall into the same leaf node, yielding a distance between treatment and control distributions that is sensitive to relevant covariates and captures the structure of confounding. Under suitable assumptions, we show that the resulting weights converge to normalized inverse propensity scores in the L2 norm and provide consistent treatment effect estimates. We demonstrate the effectiveness of our approach through extensive simulations and an application to a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18069v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simion De, Jared D. Huling</dc:creator>
    </item>
    <item>
      <title>Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences</title>
      <link>https://arxiv.org/abs/2512.18119</link>
      <description>arXiv:2512.18119v1 Announce Type: new 
Abstract: Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18119v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kohei Watanabe</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian inference for two-stage models in environmental epidemiology</title>
      <link>https://arxiv.org/abs/2512.18143</link>
      <description>arXiv:2512.18143v1 Announce Type: new 
Abstract: Statistical models often require inputs that are not completely known. This can occur when inputs are measured with error, indirectly, or when they are predicted using another model. In environmental epidemiology, air pollution exposure is a key determinant of health, yet typically must be estimated for each observational unit by a complex model. Bayesian two-stage models combine this stage-one model with a stage-two model for the health outcome given the exposure. However, analysts usually only have access to the stage-one model output without all of its specifications or input data, making joint Bayesian inference apparently intractable. We show that two prominent workarounds-using a point estimate or using the posterior from the stage-one model without feedback from the stage-two model-lead to miscalibrated inference. Instead, we propose efficient algorithms to facilitate joint Bayesian inference and provide more accurate estimates and well-calibrated uncertainties. Comparing different approaches, we investigate the association between PM2.5 exposure and county-level mortality rates in the South-Central USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18143v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Larin, Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>Frequentist forecasting in regime-switching models with extended Hamilton filter</title>
      <link>https://arxiv.org/abs/2512.18149</link>
      <description>arXiv:2512.18149v1 Announce Type: new 
Abstract: Psychological change processes, such as university student dropout in math, often exhibit discrete latent state transitions and can be studied using regime-switching models with intensive longitudinal data (ILD). Recently, regime-switching state-space (RSSS) models have been extended to allow for latent variables and their autoregressive effects. Despite this progress, estimation methods for handling both intra-individual changes and inter-individual differences as predictors of regime-switches need further exploration. Specifically, there's a need for frequentist estimation methods in dynamic latent variable frameworks that allow real-time inferences and forecasts of latent or observed variables during ongoing data collection. Building on Chow and Zhang's (2013) extended Kim filter, we introduce a first frequentist filter for RSSS models which allows hidden Markov(-switching) models to depend on both latent within- and between-individual characteristics. As a counterpart of Kelava et al.'s (2022) Bayesian forecasting filter for nonlinear dynamic latent class structural equation models (NDLC-SEM), our proposed method is the first frequentist approach within this general class of models. In an empirical study, the filter is applied to forecast emotions and behavior related to student dropout in math. Parameter recovery and prediction of regime and dynamic latent variables are evaluated through simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18149v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kento Okuyama, Tim Fabian Schaffland, Pascal Kilian, Holger Brandt, Augustin Kelava</dc:creator>
    </item>
    <item>
      <title>quollr: An R Package for Visualizing 2-D Models from Nonlinear Dimension Reductions in High-Dimensional Space</title>
      <link>https://arxiv.org/abs/2512.18166</link>
      <description>arXiv:2512.18166v1 Announce Type: new 
Abstract: Nonlinear dimension reduction methods provide a low-dimensional representation of high-dimensional data by applying a Nonlinear transformation. However, the complexity of the transformations and data structures can create wildly different representations depending on the method and hyper-parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures. The R package quollr has been developed as a new visual tool to determine which method and which hyper-parameter choices provide the most accurate representation of high-dimensional data. The scurve data from the package is used to illustrate the algorithm. Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles are used to demonstrate the usability of the package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18166v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala</dc:creator>
    </item>
    <item>
      <title>Copula Entropy: Theory and Applications</title>
      <link>https://arxiv.org/abs/2512.18168</link>
      <description>arXiv:2512.18168v1 Announce Type: new 
Abstract: This is the monograph on the theory and applications of copula entropy (CE). This book first introduces the theory of CE, including its background, definition, theorems, properties, and estimation methods. The theoretical applications of CE to structure learning, association discovery, variable selection, causal discovery, system identification, time lag estimation, domain adaptation, multivariate normality test, copula hypothesis test, two-sample test, change point detection, and symmetry test are reviewed. The relationships between the theoretical applications and their connections to correlation and causality are discussed. The framework based on CE for measuring statistical independence and conditional independence is compared to the other similar ones. The advantages of CE based methodologies over the other comparable ones are evaluated with simulations. The mathematical generalizations of CE are reviewed. The real applications of CE to every branch of science and engineering are briefly introduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18168v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Ma</dc:creator>
    </item>
    <item>
      <title>cardinalR: Generating Interesting High-Dimensional Data Structures</title>
      <link>https://arxiv.org/abs/2512.18172</link>
      <description>arXiv:2512.18172v1 Announce Type: new 
Abstract: Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18172v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala</dc:creator>
    </item>
    <item>
      <title>Applying non-negative matrix factorization with covariates to structural equation modeling for blind input-output analysis</title>
      <link>https://arxiv.org/abs/2512.18250</link>
      <description>arXiv:2512.18250v1 Announce Type: new 
Abstract: Structural equation modeling (SEM) describes directed dependence and feedback, whereas non-negative matrix factorization (NMF) provides interpretable, parts-based representations for non-negative data. We propose NMF-SEM, a unified non-negative framework that embeds NMF within a simultaneous-equation structure, enabling latent feedback loops and a reduced-form input-output mapping when intermediate flows are unobserved. The mapping separates direct effects from cumulative propagation effects and summarizes reinforcement using an amplification ratio.
  We develop regularized multiplicative-update estimation with orthogonality and sparsity penalties, and introduce structural evaluation metrics for input-output fidelity, second-moment (covariance-like) agreement, and feedback strength. Applications show that NMF-SEM recovers the classical three-factor structure in the Holzinger-Swineford data, identifies climate- and pollutant-driven mortality pathways with negligible feedback in the Los Angeles system, and separates deprivation, general morbidity, and deaths-of-despair components with weak feedback in Mississippi health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18250v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>On Efficient Adjustment in Causal Graphs</title>
      <link>https://arxiv.org/abs/2512.18315</link>
      <description>arXiv:2512.18315v1 Announce Type: new 
Abstract: Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-\gamma}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18315v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isabela Belciug, Simon Ferreira, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>Bayesian Brain Edge-Based Connectivity (BBeC): a Bayesian model for brain edge-based connectivity inference</title>
      <link>https://arxiv.org/abs/2512.18403</link>
      <description>arXiv:2512.18403v1 Announce Type: new 
Abstract: Brain connectivity analysis based on magnetic resonance imaging is crucial for understanding neurological mechanisms. However, edge-based connectivity inference faces significant challenges, particularly the curse of dimensionality when estimating high-dimensional covariance matrices. Existing methods often struggle to account for the unknown latent topological structure among brain edges, leading to inaccurate parameter estimation and unstable inference. To address these issues, this study proposes a Bayesian model based on a finite-dimensional Dirichlet distribution. Unlike non-parametric approaches, our method utilizes a finite-dimensional Dirichlet distribution to model the topological structure of brain networks, ensuring constant parameter dimensionality and improving algorithmic stability. We reformulate the covariance matrix structure to guarantee positive definiteness and employ a Metropolis-Hastings algorithm to simultaneously infer network topology and correlation parameters. Simulations validated the recovery of both network topology and correlation parameters. When applied to the Alzheimer's Disease Neuroimaging Initiative dataset, the model successfully identified structural subnetworks. The identified clusters were not only validated by composite anatomical metrics but also consistent with established findings in the literature, collectively demonstrating the model's reliability. The estimated covariance matrix also revealed that intragroup connection strength is stronger than intergroup connection strength. This study introduces a Bayesian framework for inferring brain network topology and high-dimensional covariance structures. The model configuration reduces parameter dimensionality while ensuring the positive definiteness of covariance matrices. As a result, it offers a reliable tool for investigating intrinsic brain connectivity in large-scale neuroimaging studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18403v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zijing Li, Chenhao Zeng, Shufei Ge</dc:creator>
    </item>
    <item>
      <title>Calibrating hierarchical Bayesian domain inference for a proportion</title>
      <link>https://arxiv.org/abs/2512.18479</link>
      <description>arXiv:2512.18479v1 Announce Type: new 
Abstract: Small area estimation (SAE) improves estimates for local communities or groups, such as counties, neighborhoods, or demographic subgroups, when data are insufficient for each area. This is important for targeting local resources and policies, especially when national-level or large-area data mask variation at a more granular level. Researchers often fit hierarchical Bayesian models to stabilize SAE when data are sparse. Ideally, Bayesian procedures also exhibit good frequentist properties, as demonstrated by calibrated Bayes metrics. However, hierarchical Bayesian models tend to shrink domain estimates toward the overall mean and may produce credible intervals that do not maintain nominal coverage. Hoff et al. developed the Frequentist, but Assisted by Bayes (FAB) intervals for subgroup estimates with normally distributed outcomes. However, non-normally distributed data present new challenges, and multiple types of intervals have been proposed for estimating proportions. We examine domain inference with binary outcomes and extend FAB intervals to improve nominal coverage. We describe how to numerically compute FAB intervals for a proportion and evaluate their performance through repeated simulation studies. Leveraging multilevel regression and poststratification (MRP), we further refine SAE to correct for sample selection bias, construct the FAB intervals for MRP estimates and assess their repeated sampling properties. Finally, we apply the proposed inference methods to estimate COVID-19 infection rates across geographic and demographic subgroups. We find that the FAB intervals improve nominal coverage, at the cost of wider intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18479v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rayleigh Lei, Yajuan Si</dc:creator>
    </item>
    <item>
      <title>A Bayesian likely responder approach for the analysis of randomized controlled trials</title>
      <link>https://arxiv.org/abs/2512.18492</link>
      <description>arXiv:2512.18492v1 Announce Type: new 
Abstract: An important goal of precision medicine is to personalize medical treatment by identifying individuals who are most likely to benefit from a specific treatment. The Likely Responder (LR) framework, which identifies a subpopulation where treatment response is expected to exceed a certain clinical threshold, plays a role in this effort. However, the LR framework, and more generally, data-driven subgroup analyses, often fail to account for uncertainty in the estimation of model-based data-driven subgrouping. We propose a simple two-stage approach that integrates subgroup identification with subsequent subgroup-specific inference on treatment effects. We incorporate model estimation uncertainty from the first stage into subgroup-specific treatment effect estimation in the second stage, by utilizing Bayesian posterior distributions from the first stage. We evaluate our method through simulations, demonstrating that the proposed Bayesian two-stage model produces better calibrated confidence intervals than na\"ive approaches. We apply our method to an international COVID-19 treatment trial, which shows substantial variation in treatment effects across data-driven subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18492v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annan Deng, Carole Siegel, Hyung G. Park</dc:creator>
    </item>
    <item>
      <title>The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics</title>
      <link>https://arxiv.org/abs/2512.18508</link>
      <description>arXiv:2512.18508v1 Announce Type: new 
Abstract: Validation gating is a fundamental component of classical Kalman-based tracking systems. Only measurements whose normalized innovation squared (NIS) falls below a prescribed threshold are considered for state update. While this procedure is statistically motivated by the chi-square distribution, it implicitly replaces the unconditional innovation process with a conditionally observed one, restricted to the validation event. This paper shows that innovation statistics computed after gating converge to gate-conditioned rather than nominal quantities. Under classical linear--Gaussian assumptions, we derive exact expressions for the first- and second-order moments of the innovation conditioned on ellipsoidal gating, and show that gating induces a deterministic, dimension-dependent contraction of the innovation covariance. The analysis is extended to NN association, which is shown to act as an additional statistical selection operator. We prove that selecting the minimum-norm innovation among multiple in-gate measurements introduces an unavoidable energy contraction, implying that nominal innovation statistics cannot be preserved under nontrivial gating and association. Closed-form results in the two-dimensional case quantify the combined effects and illustrate their practical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18508v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barak Or</dc:creator>
    </item>
    <item>
      <title>State-Space Modeling of Time-Varying Spillovers on Networks</title>
      <link>https://arxiv.org/abs/2512.18584</link>
      <description>arXiv:2512.18584v1 Announce Type: new 
Abstract: Many modern time series arise on networks, where each component is attached to a node and interactions follow observed edges. Classical time-varying parameter VARs (TVP-VARs) treat all series symmetrically and ignore this structure, while network autoregressive models exploit a given graph but usually impose constant parameters and stationarity. We develop network state-space models in which a low-dimensional latent state controls time-varying network spillovers, own-lag persistence and nodal covariate effects. A key special case is a network time-varying parameter VAR (NTVP-VAR) that constrains each lag matrix to be a linear combination of known network operators, such as a row-normalised adjacency and the identity, and lets the associated coefficients evolve stochastically in time. The framework nests Gaussian and Poisson network autoregressions, network ARIMA models with graph differencing, and dynamic edge models driven by multivariate logistic regression. We give conditions ensuring that NTVP-VARs are well-defined in second moments despite nonstationary states, describe network versions of stability and local stationarity, and discuss shrinkage, thresholding and low-rank tensor structures for high-dimensional graphs. Conceptually, network state-space models separate where interactions may occur (the graph) from how strong they are at each time (the latent state), providing an interpretable alternative to both unstructured TVP-VARs and existing network time-series models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18584v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane, Theofanis Papamichalis</dc:creator>
    </item>
    <item>
      <title>Accuracy of Uniform Inference on Fine Grid Points</title>
      <link>https://arxiv.org/abs/2512.18627</link>
      <description>arXiv:2512.18627v1 Announce Type: new 
Abstract: Uniform confidence bands for functions are widely used in empirical analysis. A variety of simple implementation methods (most notably multiplier bootstrap) have been proposed and theoretically justified. However, an implementation over a literally continuous index set is generally computationally infeasible, and practitioners therefore compute the critical value by evaluating the statistic on a finite evaluation grid. This paper quantifies how fine the evaluation grid must be for a multiplier bootstrap procedure over finite grid points to deliver valid uniform confidence bands. We derive an explicit bound on the resulting coverage error that separates discretization effects from the intrinsic high-dimensional bootstrap approximation error on the grid. The bound yields a transparent workflow for choosing the grid size in practice, and we illustrate the implementation through an example of kernel density estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18627v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai</dc:creator>
    </item>
    <item>
      <title>Non-stationary Spatial Modeling Using Fractional SPDEs</title>
      <link>https://arxiv.org/abs/2512.18768</link>
      <description>arXiv:2512.18768v1 Announce Type: new 
Abstract: We construct a Gaussian random field (GRF) that combines fractional smoothness with spatially varying anisotropy. The GRF is defined through a stochastic partial differential equation (SPDE), where the range, marginal variance, and anisotropy vary spatially according to a spectral parametrization of the SPDE coefficients. Priors are constructed to reduce overfitting in this flexible covariance model, and parameter estimation is done with an efficient gradient-based optimization approach that combines automatic differentiation with sparse matrix operations. In a simulation study, we investigate how many observations are required to reliably estimate fractional smoothness and non-stationarity, and find that one realization containing 500 observations or more is needed in the scenario considered. We also find that the proposed penalization prevents overfitting across varying numbers of observation locations. Two case studies demonstrate that the relative importance of fractional smoothness and non-stationarity is application dependent. Non-stationarity improves predictions in an application to ocean salinity, whereas fractional smoothness improves predictions in an application to precipitation. Predictive ability is assessed using mean squared error and the continuous ranked probability score. In addition to prediction, the proposed approach can be used as a tool to explore the presence of fractional smoothness and non-stationarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18768v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elling Svee, Geir-Arne Fuglstad</dc:creator>
    </item>
    <item>
      <title>Consistent Bayesian meta-analysis on subgroup specific effects and interactions</title>
      <link>https://arxiv.org/abs/2512.18785</link>
      <description>arXiv:2512.18785v1 Announce Type: new 
Abstract: Commonly, clinical trials report effects not only for the full study population but also for patient subgroups. Meta-analyses of subgroup-specific effects and treatment-by-subgroup interactions may be inconsistent, especially when trials apply different subgroup weightings. We show that meta-regression can, in principle, with a contribution adjustment, recover the same interaction inference regardless of whether interaction data or subgroup data are used. Our Bayesian framework for subgroup-data interaction meta-analysis inherently (i) adjusts for varying relative subgroup contribution, quantified by the information fraction (IF) within a trial; (ii) is robust to prevalence imbalance and variation; (iii) provides a self-contained, model-based approach; and (iv) can be used to incorporate prior information into interaction meta-analyses with few studies.The method is demonstrated using an example with as few as seven trials of disease-modifying therapies in relapsing-remitting multiple sclerosis. The Bayesian Contribution-adjusted Meta-analysis by Subgroup (CAMS) indicates a stronger treatment-by-disability interaction (relapse rate reduction) in patients with lower disability (EDSS &lt;= 3.5) compared with the unadjusted model, while results for younger patients (age &lt; 40 years) are unchanged.By controlling subgroup contribution while retaining subgroup interpretability, this approach enables reliable interaction decision-making when published subgroup data are available.Although the proposed CAMS approach is presented in a Bayesian context, it can also be implemented in frequentist or likelihood frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18785v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Panaro, Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Effect measures for comparing paired event times</title>
      <link>https://arxiv.org/abs/2512.18860</link>
      <description>arXiv:2512.18860v1 Announce Type: new 
Abstract: The progression-free survival ratio (PFSr) is a widely used measure in personalized oncology trials. It evaluates the effectiveness of treatment by comparing two consecutive event times - one under standard therapy and one under an experimental treatment. However, most proposed tests based on the PFSr cannot control the nominal type I error rate, even under mild assumptions such as random right-censoring. Consequently the results of these tests are often unreliable.
  As a remedy, we propose to estimate the relevant probabilities related to the PFSr by adapting recently developed methodology for the relative treatment effect between paired event times. As an additional alternative, we develop inference procedures based on differences and ratios of restricted mean survival times.
  An extensive simulation study confirms that the proposed novel methodology provides reliable inference, whereas previously proposed techniques break down in many realistic settings. The utility of our methods is further illustrated through an analysis of real data from a molecularly aided tumor trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18860v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Merle Munko, Simon Mack, Marc Ditzhaus, Stefan Fr\"ohling, Dennis Dobler, Dominic Edelmann</dc:creator>
    </item>
    <item>
      <title>Testing for latent structure via the Wilcoxon--Wigner random matrix of normalized rank statistics</title>
      <link>https://arxiv.org/abs/2512.18924</link>
      <description>arXiv:2512.18924v1 Announce Type: new 
Abstract: This paper considers the problem of testing for latent structure in large symmetric data matrices. The goal here is to develop statistically principled methodology that is flexible in its applicability, computationally efficient, and insensitive to extreme data variation, thereby overcoming limitations facing existing approaches. To do so, we introduce and systematically study certain symmetric matrices, called Wilcoxon--Wigner random matrices, whose entries are normalized rank statistics derived from an underlying independent and identically distributed sample of absolutely continuous random variables. These matrices naturally arise as the matricization of one-sample problems in statistics and conceptually lie at the interface of nonparametrics, multivariate analysis, and data reduction. Among our results, we establish that the leading eigenvalue and corresponding eigenvector of Wilcoxon--Wigner random matrices admit asymptotically Gaussian fluctuations with explicit centering and scaling terms. These asymptotic results enable rigorous parameter-free and distribution-free spectral methodology for addressing two hypothesis testing problems, namely community detection and principal submatrix detection. Numerical examples illustrate the performance of the proposed approach. Throughout, our findings are juxtaposed with existing results based on the spectral properties of independent entry symmetric random matrices in signal-plus-noise data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18924v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonquil Z. Liao, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Integrating Prioritized and Non-Prioritized Structures in Win Statistics</title>
      <link>https://arxiv.org/abs/2512.18946</link>
      <description>arXiv:2512.18946v1 Announce Type: new 
Abstract: Composite endpoints are frequently used as primary or secondary analyses in cardiovascular clinical trials to increase clinical relevance and statistical efficiency. Alternatively, the Win Ratio (WR) and other Win Statistics (WS) analyses rely on a strict hierarchical ordering of endpoints, assigning higher priority to clinically important endpoints. However, determining a definitive endpoint hierarchy can be challenging and may not adequately reflect situations where endpoints have comparable importance. In this study, we discuss the challenges of endpoint prioritization, underscore its critical role in WS analyses, and propose Rotation WR (RWR), a hybrid prioritization framework that integrates both prioritized and non-prioritized structures. By permitting blocks of equally-prioritized endpoints, RWR accommodates endpoints of equal or near equal clinical importance, recurrent events, and contexts requiring individualized shared decision making. Statistical inference for RWR is developed using U-statistics theory, including the hypothesis testing procedure and confidence interval construction. Extensions to two additional WS measures, Rotation Net Benefit and Rotation Win Odds, are also provided. Through extensive simulation studies involving multiple time-to-event endpoints, including recurrent events, we demonstrate that RWR achieves valid type I error control, desirable statistical power, and accurate confidence interval coverage. We illustrate both the methodological and practical insights of our work in a case study on endpoint prioritization with the SPRINT clinical trial, highlighting its implications for real-world clinical trial studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18946v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Scott Hummel, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>A Universal Framework for Factorial Matched Observational Studies with General Treatment Types: Design, Analysis, and Applications</title>
      <link>https://arxiv.org/abs/2512.18997</link>
      <description>arXiv:2512.18997v1 Announce Type: new 
Abstract: Matching is one of the most widely used causal inference frameworks in observational studies. However, all the existing matching-based causal inference methods are designed for either a single treatment with general treatment types (e.g., binary, ordinal, or continuous) or factorial (multiple) treatments with binary treatments only. To our knowledge, no existing matching-based causal methods can handle factorial treatments with general treatment types. This critical gap substantially hinders the applicability of matching in many real-world problems, in which there are often multiple, potentially non-binary (e.g., continuous) treatment components. To address this critical gap, this work develops a universal framework for the design and analysis of factorial matched observational studies with general treatment types (e.g., binary, ordinal, or continuous). We first propose a two-stage non-bipartite matching algorithm that constructs matched sets of units with similar covariates but distinct combinations of treatment doses, thereby enabling valid estimation of both main and interaction effects. We then introduce a new class of generalized factorial Neyman-type estimands that provide model-free, finite-population-valid definitions of marginal and interaction causal effects under factorial treatments with general treatment types. Randomization-based Fisher-type and Neyman-type inference procedures are developed, including unbiased estimators, asymptotically valid variance estimators, and variance adjustments incorporating covariate information for improved efficiency. Finally, we illustrate the proposed framework through a county-level application that evaluates the causal impacts of work- and non-work-trip reductions (social distancing practices) on COVID-19-related and drug-related outcomes during the COVID-19 pandemic in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18997v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianan Zhu, Tianruo Zhang, Diana Silver, Ellicott Matthay, Omar El-Shahawy, Hyunseung Kang, Siyu Heng</dc:creator>
    </item>
    <item>
      <title>Smoothed Quantile Estimation: A Unified Framework Interpolating to the Mean</title>
      <link>https://arxiv.org/abs/2512.19187</link>
      <description>arXiv:2512.19187v1 Announce Type: new 
Abstract: This paper develops and analyzes three families of estimators that continuously interpolate between classical quantiles and the sample mean. The construction begins with a smoothed version of the $L_{1}$ loss, indexed by a location parameter $z$ and a smoothing parameter $h \ge 0$, whose minimizer $\hat q(z,h)$ yields a unified M-estimation framework. Depending on how $(z, h)$ is specified, this framework generates three distinct classes of estimators: fixed-parameter smoothed quantile estimators, plug-in estimators of fixed quantiles, and a new continuum of mean-estimating procedures. For all three families we establish consistency and asymptotic normality via a uniform asymptotic equicontinuity argument. The limiting variances admit closed forms, allowing a transparent comparison of efficiency across families and smoothing levels. A geometric decomposition of the parameter space shows that, for fixed quantile level $\tau$, admissible pairs $(z, h)$ lie on straight lines along which the estimator targets the same population quantile while its asymptotic variance evolves. The theoretical analysis reveals two efficiency regimes. Under light-tailed distributions (e.g., Gaussian), smoothing yields a monotone variance reduction. Under heavy-tailed distributions (e.g., Laplace), a finite smoothing parameter $h^{*}(\tau) &gt; 0$ strictly improves efficiency for quantile estimation. Numerical experiments -- based on simulated data and real financial returns -- validate these conclusions and show that, both asymptotically and in finite samples, the mean-estimating family does not improve upon the sample mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19187v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sa\"id Maanan (LPP), Azzouz Dermoune (LPP), Ahmed El Ghini</dc:creator>
    </item>
    <item>
      <title>Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices</title>
      <link>https://arxiv.org/abs/2512.19273</link>
      <description>arXiv:2512.19273v1 Announce Type: new 
Abstract: High-dimensional Kronecker-structured estimation faces a conflict between non-convex scaling ambiguities and statistical robustness. The arbitrary factor scaling distorts gradient magnitudes, rendering standard fixed-threshold robust methods ineffective. We resolve this via Scaled Robust Gradient Descent (SRGD), which stabilizes optimization by de-scaling gradients before truncation. To further enforce interpretability, we introduce Scaled Hard Thresholding (SHT) for invariant variable selection. A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails. Experiments on simulated data and two real-world applications confirm superior robustness and efficiency of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19273v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Zhang, Zhiyun Fan, Wenyang Zhang, Di Wang</dc:creator>
    </item>
    <item>
      <title>High dimensional matrix estimation through elliptical factor models</title>
      <link>https://arxiv.org/abs/2512.19325</link>
      <description>arXiv:2512.19325v1 Announce Type: new 
Abstract: Elliptical factor models play a central role in modern high-dimensional data analysis, particularly due to their ability to capture heavy-tailed and heterogeneous dependence structures. Within this framework, Tyler's M-estimator (Tyler, 1987a) enjoys several optimality properties and robustness advantages. In this paper, we develop high-dimensional scatter matrix, covariance matrix and precision matrix estimators grounded in Tyler's M-estimation. We first adapt the Principal Orthogonal complEment Thresholding (POET) framework (Fan et al., 2013) by incorporating the spatial-sign covariance matrix as an effective initial estimator. Building on this idea, we further propose a direct extension of POET tailored for Tyler's M-estimation, referred to as the POET-TME method. We establish the consistency rates for the resulting estimators under elliptical factor models. Comprehensive simulation studies and a real data application illustrate the superior performance of POET-TME, especially in the presence of heavy-tailed distributions, demonstrating the practical value of our methodological contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19325v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyue Xu, Huifang Ma, Hongfei Wang, Long Feng</dc:creator>
    </item>
    <item>
      <title>A Reduced Basis Decomposition Approach to Efficient Data Collection in Pairwise Comparison Studies</title>
      <link>https://arxiv.org/abs/2512.19398</link>
      <description>arXiv:2512.19398v1 Announce Type: new 
Abstract: Comparative judgement studies elicit quality assessments through pairwise comparisons, typically analysed using the Bradley-Terry model. A challenge in these studies is experimental design, specifically, determining the optimal pairs to compare to maximize statistical efficiency. Constructing static experimental designs for these studies requires spectral decomposition of a covariance matrix over pairs of pairs, which becomes computationally infeasible for studies with more than approximately 150 objects. We propose a scalable method based on reduced basis decomposition that bypasses explicit construction of this matrix, achieving computational savings of two to three orders of magnitude. We establish eigenvalue bounds guaranteeing approximation quality and characterise the rank structure of the design matrix. Simulations demonstrate speedup factors exceeding 100 for studies with 64 or more objects, with negligible approximation error. We apply the method to construct designs for a 452-region spatial study in under 7 minutes and enable real-time design updates for classroom peer assessment, reducing computation time from 15 minutes to 15 seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19398v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiahua Jiang, Joseph Marsh, Rowland G Seymour</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Understanding Causal Effects that Vary by Treatment Initiation Time in EHR-based Studies</title>
      <link>https://arxiv.org/abs/2512.19553</link>
      <description>arXiv:2512.19553v1 Announce Type: new 
Abstract: Comparative effectiveness studies using electronic health records (EHR) consider data from patients who could ``enter'' the study cohort at any point during an interval that spans many years in calendar time. Unlike treatments in tightly controlled trials, real-world treatments can evolve over calendar time, especially if comparators include standard of care, or procedures where techniques may improve. Efforts to assess whether treatment efficacy itself is changing are complicated by changing patient populations, with potential covariate shift in key effect modifiers. In this work, we propose a statistical framework to estimate calendar-time specific average treatment effects and describe both how and why effects vary across treatment initiation time in EHR-based studies. Our approach projects doubly robust, time-specific treatment effect estimates onto candidate marginal structural models and uses a model selection procedure to best describe how effects vary by treatment initiation time. We further introduce a novel summary metric, based on standardization analysis, to quantify the role of covariate shift in explaining observed effect changes and disentangle changes in treatment effects from changes in the patient population receiving treatment. Extensive simulations using EHR data from Kaiser Permanente are used to validate the utility of the framework, which we apply to study changes in relative weight loss following two bariatric surgical interventions versus no surgery among patients with severe obesity between 2005-2011.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19553v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Rui Wang, David Arterburn, Heidi Fischer, Catherine Lee, Susan M. Shortreed, Alexander W. Levis, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Possibilistic Inferential Models for Post-Selection Inference in High-Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2512.19588</link>
      <description>arXiv:2512.19588v1 Announce Type: new 
Abstract: Valid uncertainty quantification after model selection remains challenging in high-dimensional linear regression, especially within the possibilistic inferential model (PIM) framework. We develop possibilistic inferential models for post-selection inference based on a regularized split possibilistic construction (RSPIM) that combines generic high-dimensional selectors with PIM validification through sample splitting. A first subsample is used to select a sparse model; ordinary least-squares refits on an independent inference subsample yield classical t/F pivots, which are then turned into consonant plausibility contours. In Gaussian linear models this leads to coor-dinatewise intervals with exact finite-sample strong validity conditional on the split and selected model, uniformly over all selectors that use only the selection data. We further analyze RSPIM in a sparse p &gt;&gt; n regime under high-level screening conditions, develop orthogonalized and bootstrap-based extensions for low-dimensional targets with high-dimensional nuisance, and study a maxitive multi-split aggregation that stabilizes inference across random splits while preserving strong validity. Simulations and a riboflavin gene-expression example show that calibrated RSPIM intervals are well behaved under both Gaussian and heteroskedastic errors and are competitive with state-of-the-art post-selection methods, while plausibility contours provide transparent diagnostics of post-selection uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19588v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaohui Lin</dc:creator>
    </item>
    <item>
      <title>A Markov Chain Modeling Approach for Predicting Relative Risks of Spatial Clusters in Public Health</title>
      <link>https://arxiv.org/abs/2512.19635</link>
      <description>arXiv:2512.19635v1 Announce Type: new 
Abstract: Predicting relative risk (RR) of spatial clusters is a complex task in public health that can be achieved through various statistical and machine-learning methods for different time intervals. However, high-resolution longitudinal data is often unavailable to successfully apply such methods. The goal of the present study is to further develop and test a new methodology proposed in our previous work for accurate sequential RR predictions in the case of limited lon gitudinal data. In particular, we first use a well-known likelihood ratio test to identify significant spatial clusters over user-defined time intervals. Then we apply a Markov chain modeling ap approach to predict RR values for each time interval. Our findings demonstrate that the proposed approach yields better performance with COVID-19 morbidity data compared to the previous study on mortality data. Additionally, increasing the number of time intervals enhances the accuracy of the proposed Markov chain modeling method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19635v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lyza Iamrache, Kamel Rekab, Majid Bani-Yagoub, Julia Pluta, Abdelghani Mehailia</dc:creator>
    </item>
    <item>
      <title>Testing for Conditional Independence in Binary Single-Index Models</title>
      <link>https://arxiv.org/abs/2512.19641</link>
      <description>arXiv:2512.19641v1 Announce Type: new 
Abstract: We wish to test whether a real-valued variable $Z$ has explanatory power, in addition to a multivariate variable $X$, for a binary variable $Y$. Thus, we are interested in testing the hypothesis $\mathbb{P}(Y=1\, | \, X,Z)=\mathbb{P}(Y=1\, | \, X)$, based on $n$ i.i.d.\ copies of $(X,Y,Z)$. In order to avoid the curse of dimensionality, we follow the common approach of assuming that the dependence of both $Y$ and $Z$ on $X$ is through a single-index $X^\top\beta$ only. Splitting the sample on both $Y$-values, we construct a two-sample empirical process of transformed $Z$-variables, after splitting the $X$-space into parallel strips. Studying this two-sample empirical process is challenging: it does not converge weakly to a standard Brownian bridge, but after an appropriate normalization it does. We use this result to construct distribution-free tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19641v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John H. J. Einmahl, Denis Kojevnikov, Bas J. M. Werker</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Selection of Low-Risk Oncology Patients for Survival Beyond a Time Horizon</title>
      <link>https://arxiv.org/abs/2512.18118</link>
      <description>arXiv:2512.18118v1 Announce Type: cross 
Abstract: We study the problem of selecting a subset of patients who are unlikely to experience an event within a specified time horizon, by calibrating a screening rule based on the output of a black-box survival model. This statistics problem has many applications in medicine, including identifying candidates for treatment de-escalation and prioritizing the allocation of limited medical resources. In this paper, we compare two families of methods that can provide different types of distribution-free guarantees for this task: (i) high-probability risk control and (ii) expectation-based false discovery rate control using conformal $p$-values. We clarify the relation between these two frameworks, which have important conceptual differences, and explain how each can be adapted to analyze time-to-event data using inverse probability of censoring weighting. Through experiments on semi-synthetic and real oncology data from the Flatiron Health Research Database, we find that both approaches often achieve the desired survival rate among selected patients, but with distinct efficiency profiles. The conformal method tends to be more powerful, whereas high-probability risk control offers stronger guarantees at the cost of some additional conservativeness. Finally, we provide practical guidance on implementation and parameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18118v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>Graphon-Level Bayesian Predictive Synthesis for Random Network</title>
      <link>https://arxiv.org/abs/2512.18587</link>
      <description>arXiv:2512.18587v1 Announce Type: cross 
Abstract: Bayesian predictive synthesis provides a coherent Bayesian framework for combining multiple predictive distributions, or agents, into a single updated prediction, extending Bayesian model averaging to allow general pooling of full predictive densities. This paper develops a static, graphon level version of Bayesian predictive synthesis for random networks. At the graphon level we show that Bayesian predictive synthesis corresponds to the integrated squared error projection of the true graphon onto the linear span of the agent graphons. We derive nonasymptotic oracle inequalities and prove that least-squares graphon-BPS, based on a finite number of edge observations, achieves the minimax L^2 rate over this agent span. Moreover, we show that any estimator that selects a single agent graphon is uniformly inconsistent on a nontrivial subset of the convex hull of the agents, whereas graphon-level Bayesian predictive synthesis remains minimax-rate optimal-formalizing a combination beats components phenomenon. Structural properties of the underlying random graphs are controlled through explicit Lipschitz bounds that transfer graphon error into error for edge density, degree distributions, subgraph densities, clustering coefficients, and giant component phase transitions. Finally, we develop a heavy tail theory for Bayesian predictive synthesis, showing how mixtures and entropic tilts preserve regularly varying degree distributions and how exponential random graph model agents remain within their family under log linear tilting with Kullback-Leibler optimal moment calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18587v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane</dc:creator>
    </item>
    <item>
      <title>Wavelet Latent Position Exponential Random Graphs</title>
      <link>https://arxiv.org/abs/2512.18592</link>
      <description>arXiv:2512.18592v1 Announce Type: cross 
Abstract: Many network datasets exhibit connectivity with variance by resolution and large-scale organization that coexists with localized departures. When vertices have observed ordering or embedding, such as geography in spatial and village networks, or anatomical coordinates in connectomes, learning where and at what resolution connectivity departs from a baseline is crucial. Standard models typically emphasize a single representation, i.e. stochastic block models prioritize coarse partitions, latent space models prioritize global geometry, small-world generators capture local clustering with random shortcuts, and graphon formulations are fully general and do not solely supply a canonical multiresolution parameterization for interpretation and regularization. We introduce wavelet latent position exponential random graphs (WL-ERGs), an exchangeable logistic-graphon framework in which the log-odds connectivity kernel is represented in compactly supported orthonormal wavelet coordinates and mapped to edge probabilities through a logistic link. Wavelet coefficients are indexed by resolution and location, which allows multiscale structure to become sparse and directly interpretable. Although edges remain independent given latent coordinates, any finite truncation yields a conditional exponential family whose sufficient statistics are multiscale wavelet interaction counts and conditional laws admit a maximum-entropy characterization. These characteristics enable likelihood-based regularization and testing directly in coefficient space. The theory is naturally scale-resolved and includes universality for broad classes of logistic graphons, near-minimax estimation under multiscale sparsity, scale-indexed recovery and detection thresholds, and a band-limited regime in which canonical coefficient-space tilts are non-degenerate and satisfy a finite-dimensional large deviation principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18592v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane</dc:creator>
    </item>
    <item>
      <title>Model-Agnostic Bounds for Augmented Inverse Probability Weighted Estimators' Wald-Confidence Interval Coverage in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2512.18898</link>
      <description>arXiv:2512.18898v1 Announce Type: cross 
Abstract: Nonparametric estimators, such as the augmented inverse probability weighted (AIPW) estimator, have become increasingly popular in causal inference. Numerous nonparametric estimators have been proposed, but they are all asymptotically normal with the same asymptotic variance under similar conditions, leaving little guidance for practitioners to choose an estimator. In this paper, I focus on another important perspective of their asymptotic behaviors beyond asymptotic normality, the convergence of the Wald-confidence interval (CI) coverage to the nominal coverage. Such results have been established for simpler estimators (e.g., the Berry-Esseen Theorem), but are lacking for nonparametric estimators. I consider a simple but practical setting where the AIPW estimator based on a black-box nuisance estimator, with or without cross-fitting, is used to estimate the average treatment effect in randomized controlled trials. I derive non-asymptotic Berry-Esseen-type bounds on the difference between Wald-CI coverage and the nominal coverage. I also analyze the bias of variance estimators, showing that the cross-fit variance estimator might overestimate while the non-cross-fit variance estimator might underestimate, which might explain why cross-fitting has been empirically observed to improve Wald-CI coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18898v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu</dc:creator>
    </item>
    <item>
      <title>Dyadic Flow Models for Nonstationary Gene Flow in Landscape Genomics</title>
      <link>https://arxiv.org/abs/2512.19035</link>
      <description>arXiv:2512.19035v1 Announce Type: cross 
Abstract: The field of landscape genomics aims to infer how landscape features affect gene flow across space. Most landscape genomic frameworks assume the isolation-by-distance and isolation-by-resistance hypotheses, which propose that genetic dissimilarity increases as a function of distance and as a function of cumulative landscape resistance, respectively. While these hypotheses are valid in certain settings, other mechanisms may affect gene flow. For example, the gene flow of invasive species may depend on founder effects and multiple introductions. Such mechanisms are not considered in modern landscape genomic models. We extend dyadic models to allow for mechanisms that range-shifting and/or invasive species may experience by introducing dyadic spatially-varying coefficients (DSVCs) defined on source-destination pairs. The DSVCs allow the effects of landscape on gene flow to vary across space, capturing nonstationary and asymmetric connectivity. Additionally, we incorporate explicit landscape features as connectivity covariates, which are localized to specific regions of the spatial domain and may function as barriers or corridors to gene flow. Such covariates are central to colonization and invasion, where spread accelerates along corridors and slows across landscape barriers. The proposed framework accommodates colonization-specific processes while retaining the ability to assess landscape influences on gene flow. Our case study of the highly invasive cheatgrass (Bromus tectorum) demonstrates the necessity of accounting for nonstationarity gene flow in range-shifting species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19035v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Schwob, Nicholas M. Calzada, Justin J. Van Ee, Diana Gamba, Rebecca A. Nelson, Megan L. Vahsen, Peter B. Adler, Jesse R. Lasky, Mevin B. Hooten</dc:creator>
    </item>
    <item>
      <title>A hybrid-Hill estimator enabled by heavy-tailed block maxima</title>
      <link>https://arxiv.org/abs/2512.19338</link>
      <description>arXiv:2512.19338v1 Announce Type: cross 
Abstract: When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19338v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Neves, Chang Xu</dc:creator>
    </item>
    <item>
      <title>Causal inference with misspecified network interference structure</title>
      <link>https://arxiv.org/abs/2302.11322</link>
      <description>arXiv:2302.11322v3 Announce Type: replace 
Abstract: Under interference, the treatment of one unit may affect the outcomes of other units. Such interference patterns between units are typically represented by a network. Correctly specifying this network requires identifying which units can affect others -- an inherently challenging task. Nevertheless, most existing approaches assume that a known and accurate network specification is given. In this paper, we study the consequences of such misspecification.
  We derive bounds on the bias arising from estimating causal effects using a misspecified network, showing that the estimation bias grows with the divergence between the assumed and true networks, quantified through their induced exposure probabilities. To address this challenge, we propose a novel estimator that leverages multiple networks simultaneously and remains unbiased if at least one of the networks is correct, even when we do not know which one. Therefore, the proposed estimator provides robustness to network specification. We illustrate key properties and demonstrate the utility of our proposed estimator through simulations and analysis of a social network field experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11322v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bar Weinstein, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>Generalized Data Thinning Using Sufficient Statistics</title>
      <link>https://arxiv.org/abs/2303.12931</link>
      <description>arXiv:2303.12931v3 Announce Type: replace 
Abstract: Our goal is to develop a general strategy to decompose a random variable $X$ into multiple independent random variables, without sacrificing any information about unknown parameters. A recent paper showed that for some well-known natural exponential families, $X$ can be "thinned" into independent random variables $X^{(1)}, \ldots, X^{(K)}$, such that $X = \sum_{k=1}^K X^{(k)}$. These independent random variables can then be used for various model validation and inference tasks, including in contexts where traditional sample splitting fails. In this paper, we generalize their procedure by relaxing this summation requirement and simply asking that some known function of the independent random variables exactly reconstruct $X$. This generalization of the procedure serves two purposes. First, it greatly expands the families of distributions for which thinning can be performed. Second, it unifies sample splitting and data thinning, which on the surface seem to be very different, as applications of the same principle. This shared principle is sufficiency. We use this insight to perform generalized thinning operations for a diverse set of families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12931v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2353948</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association, 120(549), 511-523 (2025)</arxiv:journal_reference>
      <dc:creator>Ameer Dharamshi, Anna Neufeld, Keshav Motwani, Lucy L. Gao, Daniela Witten, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Approximate co-sufficient sampling with regularization</title>
      <link>https://arxiv.org/abs/2309.08063</link>
      <description>arXiv:2309.08063v3 Announce Type: replace 
Abstract: In this work, we consider the problem of goodness-of-fit (GoF) testing for parametric models. This testing problem involves a composite null hypothesis, due to the unknown values of the model parameters. In some special cases, co-sufficient sampling (CSS) can remove the influence of these unknown parameters via conditioning on a sufficient statistic -- often, the maximum likelihood estimator (MLE) of the unknown parameters. However, many common parametric settings do not permit this approach, since conditioning on a sufficient statistic leads to a powerless test. The recent approximate co-sufficient sampling (aCSS) framework of Barber and Janson (2022) offers an alternative, replacing sufficiency with an approximately sufficient statistic (namely, a noisy version of the MLE). This approach recovers power in a range of settings where CSS cannot be applied, but can only be applied in settings where the unconstrained MLE is well-defined and well-behaved, which implicitly assumes a low-dimensional regime. In this work, we extend aCSS to the setting of constrained and penalized maximum likelihood estimation, so that more complex estimation problems can now be handled within the aCSS framework, including examples such as mixtures-of-Gaussians (where the unconstrained MLE is not well-defined due to degeneracy) and high-dimensional Gaussian linear models (where the MLE can perform well under regularization, such as an $\ell_1$ penalty or a shape constraint).</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08063v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanrong Zhu, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Variational Markov chain mixtures with automatic component selection</title>
      <link>https://arxiv.org/abs/2406.04653</link>
      <description>arXiv:2406.04653v2 Announce Type: replace 
Abstract: Markov state modeling has gained popularity in various scientific fields since it reduces complex time-series data sets into transitions between a few states. Yet common Markov state modeling frameworks assume a single Markov chain describes the data, so they suffer from an inability to discern heterogeneities. As an alternative, this paper models time-series data using a mixture of Markov chains, and it automatically determines the number of mixture components using the variational expectation-maximization algorithm.Variational EM simultaneously identifies the number of Markov chains and the dynamics of each chain without expensive model comparisons or posterior sampling. As a theoretical contribution, this paper identifies the natural limits of Markov state mixture modeling by proving a lower bound on the classification error. It then presents numerical experiments where variational EM achieves performance consistent with the theoretically optimal error scaling. The experiments are based on synthetic and observational data sets including Last.fm music listening, ultramarathon running, and gene expression. In each of the three data sets, variational EM leads to the identification of meaningful heterogeneities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04653v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher E. Miles, Robert J. Webber</dc:creator>
    </item>
    <item>
      <title>The $\infty$-S test via regression quantile affine LASSO</title>
      <link>https://arxiv.org/abs/2409.04256</link>
      <description>arXiv:2409.04256v3 Announce Type: replace 
Abstract: A novel test in the linear $\ell_1$ (LAD) and quantile regressions is proposed, based on the scores provided by the dual variables (signs) arising in the calculation of the (so-called) affine-lasso estimate--a Rao-type, Lagrange multiplier test using the thresholding, towards the null hypothesis of the test, function of the latter estimate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04256v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sylvain Sardy, Ivan Mizera, Xiaoyu Ma, Hugo Gaible</dc:creator>
    </item>
    <item>
      <title>A generalized e-value feature detection method with FDR control at multiple resolutions</title>
      <link>https://arxiv.org/abs/2409.17039</link>
      <description>arXiv:2409.17039v5 Announce Type: replace 
Abstract: Multiple resolutions arise across a range of explanatory features due to domain-specific structures, leading to the formation of feature groups. It follows that the simultaneous detection of significant features and groups aimed at a specific response with false discovery rate (FDR) control stands as a crucial issue, such as the spatial genome-wide association studies. Nevertheless, existing detection methods with multilayer FDR control generally rely on valid p-values or knockoff statistics, which can be not flexible, powerful and stable in several settings. To fix this issue effectively, this article develops a novel method of Stabilized Flexible E-Filter Procedure (SFEFP), by constructing unified generalized e-values, leveraging a generalized e-filter, and adopting a stabilization treatment with power enhancement. This method flexibly incorporates diverse base detection procedures at different resolutions to provide consistent, powerful, and stable results, while controlling FDR at multiple resolutions simultaneously. Statistical properties of multilayer filtering procedure encompassing one-bit property, multilayer FDR control, and stability guarantee are established. We also develop several examples for SFEFP such as the eDS-filter. Simulation studies and the analysis of HIV mutation data demonstrate the efficacy of SFEFP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17039v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyao Yu, Ruixing Ming, Min Xiao, Zhanfeng Wang, Bingyi Jing</dc:creator>
    </item>
    <item>
      <title>Parameter-Specific Bias Diagnostics in Random-Effects Panel Data Models</title>
      <link>https://arxiv.org/abs/2412.20555</link>
      <description>arXiv:2412.20555v4 Announce Type: replace 
Abstract: The Hausman specification test detects inconsistency of the random-effects estimator by comparing it with an alternative fixed-effects estimator. This note shows how a recently proposed bias diagnostic for linear mixed models can complement this test in random-effects panel-data applications. The diagnostic delivers parameter-specific internal estimates of finite-sample bias of the random-effects estimator, together with permutation-based $p$-values, from a single fitted random-effects model. We illustrate its use in a gasoline-demand panel and in a value-added model for teacher evaluation, using publicly available R packages, and we discuss how the resulting bias summaries can be incorporated into routine practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20555v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew T. Karl</dc:creator>
    </item>
    <item>
      <title>Inverse sampling intensity weighting for preferential sampling adjustment</title>
      <link>https://arxiv.org/abs/2503.05067</link>
      <description>arXiv:2503.05067v2 Announce Type: replace 
Abstract: Traditional geostatistical methods assume independence between observation locations and the spatial process of interest. Violations of this independence assumption are referred to as preferential sampling (PS). Standard methods to address PS rely on estimating complex shared latent variable models and can be difficult to apply in practice. We study the use of inverse sampling intensity weighting (ISIW) for PS adjustment in model-based geostatistics. ISIW is a two-stage approach wherein we estimate the sampling intensity of the observation locations then define intensity-based weights within a weighted likelihood adjustment. Prediction follows by substituting the adjusted parameter estimates within kriging. We introduce an implementation of ISIW based on the Vecchia approximation, enabling computational gains while maintaining strong predictive accuracy. Interestingly, we found that ISIW outpredicts standard PS methods under misspecification of the sampling design, and that accurate parameter estimation had little correlation with predictive performance, raising questions about the conditions driving optimal implementation of kriging-based predictors under PS. Our work highlights the potential of ISIW to adjust for PS in an intuitive, fast, and effective manner. We illustrate these ideas on spatial prediction of lead concentrations measured through moss biomonitoring data in Galicia, Spain, and PM2.5 concentrations from the U.S. EPA Air Quality System network in California.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05067v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas W. Hsiao, Lance A. Waller</dc:creator>
    </item>
    <item>
      <title>Spearman's rho for zero-inflated count data: formulation and attainable bounds</title>
      <link>https://arxiv.org/abs/2503.13148</link>
      <description>arXiv:2503.13148v2 Announce Type: replace 
Abstract: We propose an alternative formulation of Spearman's rho for zero-inflated count data. The formulation yields an estimator with explicitly attainable bounds, facilitating interpretation in settings where the standard range [-1,1] is no longer informative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13148v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasper Arends, Guanjie Lyu, Mhamed Mesfioui, Elisa Perrone, Julien Trufin</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Invariant Tests of Multivariate Normality Based on Radial Concentration</title>
      <link>https://arxiv.org/abs/2504.09237</link>
      <description>arXiv:2504.09237v2 Announce Type: replace 
Abstract: While the problem of testing multivariate normality has received considerable attention in the classical low-dimensional setting where the sample size $n$ is much larger than the feature dimension $d$ of the data, there is presently a dearth of existing tests which are valid in the high-dimensional setting where $d$ is of comparable or larger order than $n$. This paper studies the hypothesis testing problem of determining whether $n$ i.i.d. samples are generated from a $d$-dimensional multivariate normal distribution, in settings where $d$ grows with $n$ at some rate under a broad regime. To this end, we propose a new class of computationally efficient tests which can be regarded as a high-dimensional adaptation of the classical radial approach to testing normality. A key member of this class is a range-type test which, under a very general rate of growth of $d$ with respect to $n$, is proven to achieve both type I error-control and consistency for three important classes of alternatives; namely, finite mixture model, non-Gaussian elliptical, and leptokurtic alternatives. Extensive simulation studies demonstrate the superiority of our test compared to existing methods, and two gene expression applications demonstrate the effectiveness of our procedure for detecting violations of multivariate normality which are of potentially practical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09237v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Derek Latremouille</dc:creator>
    </item>
    <item>
      <title>A stochastic method to estimate a zero-inflated two-part mixed model for human microbiome data</title>
      <link>https://arxiv.org/abs/2504.15411</link>
      <description>arXiv:2504.15411v2 Announce Type: replace 
Abstract: Human microbiome studies based on genetic sequencing techniques produce compositional longitudinal data of the relative abundances of microbial taxa over time, allowing to understand, through mixed-effects modeling, how microbial communities evolve in response to clinical interventions, environmental changes, or disease progression. In particular, the Zero-Inflated Beta Regression (ZIBR) models jointly and over time the presence and abundance of each microbe taxon, considering the compositional nature of the data, its skewness, and the over-abundance of zeros. However, as for other complex random effects models, maximum likelihood estimation suffers from the intractability of likelihood integrals. Available estimation methods rely on log-likelihood approximation, which is prone to potential limitations such as biased estimates or unstable convergence. In this work we develop an alternative maximum likelihood estimation approach for the ZIBR model, based on the Stochastic Approximation Expectation Maximization (SAEM) algorithm. The proposed methodology allows to model unbalanced data, which is not always possible in existing approaches. We also provide estimations of the standard errors and the log-likelihood of the fitted model. The performance of the algorithm is established through simulation, and its use is demonstrated on two microbiome studies, showing its ability to detect changes in both presence and abundance of bacterial taxa over time and in response to treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15411v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Barrera, Cristian Meza, Ana Arribas-Gil</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Community Detection and Model Selection in Blockmodels</title>
      <link>https://arxiv.org/abs/2505.22459</link>
      <description>arXiv:2505.22459v2 Announce Type: replace 
Abstract: Blockmodels are a foundational tool for modeling community structure in networks, with the stochastic blockmodel (SBM), degree-corrected blockmodel (DCBM), and popularity-adjusted blockmodel (PABM) forming a natural hierarchy of increasing generality. While community detection under these models has been extensively studied, much less attention has been paid to the model selection problem, i.e., determining which model best fits a given network. Building on recent theoretical insights about the spectral geometry of these models, we propose a unified framework for simultaneous community detection and model selection across the full blockmodel hierarchy. A key innovation is the use of loss functions that serve a dual role: they act as objective functions for community detection and as test statistics for hypothesis testing. We develop a greedy algorithm to minimize these loss functions and establish theoretical guarantees for exact label recovery and model selection consistency under each model. Extensive simulation studies demonstrate that our method achieves high accuracy in both tasks, outperforming or matching state-of-the-art alternatives. Applications to five real-world networks further illustrate the interpretability and practical utility of our approach. R code for implementing the method is available at https://github.com/subhankarbhadra/model-selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22459v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2590073</arxiv:DOI>
      <dc:creator>Subhankar Bhadra, Minh Tang, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Tree-based methods for length-biased survival data</title>
      <link>https://arxiv.org/abs/2508.16312</link>
      <description>arXiv:2508.16312v3 Announce Type: replace 
Abstract: Left-truncated survival data commonly arise in prevalent cohort studies, where only individuals who have experienced disease onset and survived until enrollment in the study. When the onset process follows a stationary Poisson process, the resulting data are length-biased. This sampling mechanism induces a selection bias towards longer survival individuals, and statistical methods for traditional survival data are not directly applicable. While tree-based methods developed for left-truncated data can be applied, they may be inefficient for length-biased data, as they do not account for the distribution of truncation times. To address this, we propose new survival trees and forests for length-biased right-censored data within the conditional inference framework. Our approach uses a score function derived from the full likelihood to construct permutation test statistics for variable splitting. For survival prediction, we consider two estimators of the unbiased survival function, differing in statistical efficiency and computational complexity. These elements enhance efficiency in tree construction and improve accuracy of survival prediction in ensemble settings. Simulation studies demonstrate efficiency gains in both tree recovery and survival prediction, often exceeding the gains from ensembling alone. We further illustrate the utility of the proposed methods using lung cancer data from the Cancer Public Library Database, a nationwide cancer registry in South Korea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16312v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinwoo Lee, Donghwan Lee, Hyunwoo Lee, Jiyu Sun</dc:creator>
    </item>
    <item>
      <title>Understanding Spatial Regression Models from a Weighting Perspective in an Observational Study of Superfund Remediation</title>
      <link>https://arxiv.org/abs/2508.19572</link>
      <description>arXiv:2508.19572v2 Announce Type: replace 
Abstract: A key challenge in environmental health research is unmeasured spatial confounding, driven by unobserved spatially structured variables that influence both treatment and outcome. A common approach is to fit a spatial regression that models the outcome as a linear function of treatment and covariates, with a spatially structured error term to account for unmeasured spatial confounding. However, it remains unclear to what extent spatial regression actually accounts for such forms of confounding in finite samples, and whether this regression adjustment can be reformulated from a design-based perspective. Motivated by an observational study on the effect of Superfund site remediation on birth outcomes, we present a weighting framework for causal inference that unifies three canonical classes of spatial regression models$\unicode{x2013}$random effects, conditional autoregressive, and Gaussian process models$\unicode{x2013}$and reveals how they implicitly construct causal contrasts across space. Specifically, we show that: (i) the spatial error term induces approximate balance on a latent set of covariates and therefore adjusts for a specific form of unmeasured confounding; and (ii) the covariance structure of the spatial error can be equivalently represented as regressors in a linear model. Building on these insights, we introduce a new estimator that jointly addresses multiple forms of unmeasured spatial confounding and develop visual diagnostics. Using our new estimator, we find evidence of a small but beneficial effect of remediation on the percentage of small vulnerable newborns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19572v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie M. Woodward, Francesca Dominici, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Monotone data augmentation algorithm for longitudinal continuous, binary and ordinal outcomes: a unifying approach</title>
      <link>https://arxiv.org/abs/2512.06621</link>
      <description>arXiv:2512.06621v2 Announce Type: replace 
Abstract: The monotone data augmentation (MDA) algorithm has been widely used to impute missing data for longitudinal continuous outcomes. Compared to a full data augmentation approach, the MDA scheme accelerates the mixing of the Markov chain, reduces computational costs per iteration, and aids in missing data imputation under nonignorable dropouts. We extend the MDA algorithm to the multivariate probit (MVP) model for longitudinal binary and ordinal outcomes. The MVP model assumes the categorical outcomes are discretized versions of underlying longitudinal latent Gaussian outcomes modeled by a mixed effects model for repeated measures. A parameter expansion strategy is employed to facilitate the posterior sampling, and expedite the convergence of the Markov chain in MVP. The method enables the sampling of the regression coefficients and covariance matrix for longitudinal continuous, binary and ordinal outcomes in a unified manner. This property aids in understanding the algorithm and developing computer codes for MVP. We also introduce independent Metropolis-Hasting samplers to handle complex priors, and evaluate how the choice between flat and diffuse normal priors for regression coefficients influences parameter estimation and missing data imputation. Numerical examples are used to illustrate the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06621v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongqiang Tang</dc:creator>
    </item>
    <item>
      <title>Bounds for causal mediation effects</title>
      <link>https://arxiv.org/abs/2512.11549</link>
      <description>arXiv:2512.11549v2 Announce Type: replace 
Abstract: Several frameworks have been proposed for studying causal mediation analysis. What these frameworks have in common is that they all make assumptions for point identifications that can be violated even when treatment is randomized. When a causal effect is not point-identified, one can sometimes derive bounds, i.e. a range of possible values that are consistent with the observed data. In this work, we study causal bounds for mediation effects under both the natural effects framework and the separable effects framework. In particular, we show that when there are unmeasured confounders for the intermediate variables(s) the sharp symbolic bounds on separable (in)direct effect coincide with existing bounds for natural (in)direct effects in the analogous setting. We compare these bounds to valid bounds for the natural direct effects when only the cross-world independence assumption does not hold. Furthermore, we demonstrate the use and compare the results of the bounds on data from a trial investigating the effect of peanut consumption on the development of peanut allergy in infants through specific pathways of measured immunological biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11549v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie S. Breum, Vanessa Didelez, Erin E. Gabriel, Michael C. Sachs</dc:creator>
    </item>
    <item>
      <title>On the bias of the Gini estimator: Poisson and geometric cases, a characterization of the gamma family, and unbiasedness under gamma distributions</title>
      <link>https://arxiv.org/abs/2512.14983</link>
      <description>arXiv:2512.14983v2 Announce Type: replace 
Abstract: In this paper, we derive a general representation for the expectation of the Gini coefficient estimator in terms of the Laplace transform of the underlying distribution, together with the mean and the Gini coefficient of its exponentially tilted version. This representation leads to a new characterization of the gamma family within the class of nonnegative scale families, based on a stability property under exponential tilting. As direct applications, we show that the Gini estimator is biased for both Poisson and geometric populations and provide an alternative, unified proof of its unbiasedness for gamma populations. By using the derived bias expressions, we propose plug-in bias-corrected estimators and assess their finite-sample performance through a Monte Carlo study, which demonstrates substantial improvements over the original estimator. Compared with existing approaches, our framework highlights the fundamental role of scale invariance and exponential tilting, rather than distribution-specific algebraic calculations, and complements recent results in Baydil et al. (2025) [Unbiased estimation of the gini coefficient. SPL, 222:110376] and Vila and Saulo (2025a,b) [Bias in Gini coefficient estimation for gamma mixture populations. STPA, 66:1-18; and The mth gini index estimator: Unbiasedness for gamma populations. J. Econ. Inequal].</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14983v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Bayesian Empirical Bayes: Simultaneous Inference from Probabilistic Symmetries</title>
      <link>https://arxiv.org/abs/2512.16239</link>
      <description>arXiv:2512.16239v2 Announce Type: replace 
Abstract: Empirical Bayes (EB) improves the accuracy of simultaneous inference "by learning from the experience of others" (Efron, 2012). Classical EB theory focuses on latent variables that are iid draws from a fitted prior (Efron, 2019). Modern applications, however, feature complex structure, like arrays, spatial processes, or covariates. How can we apply EB ideas to these settings? We propose a generalized approach to empirical Bayes based on the notion of probabilistic symmetry. Our method pairs a simultaneous inference problem-with an unknown prior-to a symmetry assumption on the joint distribution of the latent variables. Each symmetry implies an ergodic decomposition, which we use to derive a corresponding empirical Bayes method. We call this methodBayesian empirical Bayes (BEB). We show how BEB recovers the classical methods of empirical Bayes, which implicitly assume exchangeability. We then use it to extend EB to other probabilistic symmetries: (i) EB matrix recovery for arrays and graphs; (ii) covariate-assisted EB for conditional data; (iii) EB spatial regression under shift invariance. We develop scalable algorithms based on variational inference and neural networks. In simulations, BEB outperforms existing approaches to denoising arrays and spatial data. On real data, we demonstrate BEB by denoising a cancer gene-expression matrix and analyzing spatial air-quality data from New York City.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16239v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, Eli N. Weinstein, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions</title>
      <link>https://arxiv.org/abs/2407.01751</link>
      <description>arXiv:2407.01751v2 Announce Type: replace-cross 
Abstract: In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \in \mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01751v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Antonio Di Noia</dc:creator>
    </item>
    <item>
      <title>Estimating velocities of infectious disease spread through spatio-temporal log-Gaussian Cox point processes</title>
      <link>https://arxiv.org/abs/2409.05036</link>
      <description>arXiv:2409.05036v2 Announce Type: replace-cross 
Abstract: Understanding the spread of infectious diseases such as COVID-19 is crucial for informed decision-making and resource allocation. A critical component of disease behavior is the velocity with which disease spreads, defined as the rate of change between time and space. In this paper, we propose a spatio-temporal modeling approach to determine the velocities of infectious disease spread. Our approach assumes that the locations and times of people infected can be considered as a spatio-temporal point pattern that arises as a realization of a spatio-temporal log-Gaussian Cox process. The intensity of this process is estimated using fast Bayesian inference by employing the integrated nested Laplace approximation (INLA) and the Stochastic Partial Differential Equations (SPDE) approaches. The velocity is then calculated using finite differences that approximate the derivatives of the intensity function. Finally, the directions and magnitudes of the velocities can be mapped at specific times to examine better the spread of the disease throughout the region. We demonstrate our method by analyzing COVID-19 spread in Cali, Colombia, during the 2020-2021 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05036v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Rodriguez Avellaneda, Jorge Mateu, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Optimal sequencing depth for single-cell RNA-sequencing in Wasserstein space</title>
      <link>https://arxiv.org/abs/2409.14326</link>
      <description>arXiv:2409.14326v2 Announce Type: replace-cross 
Abstract: How many samples should one collect for an empirical distribution to be as close as possible to the true population? This question is not trivial in the context of single-cell RNA-sequencing. With limited sequencing depth, profiling more cells comes at the cost of fewer reads per cell. Therefore, one must strike a balance between the number of cells sampled and the accuracy of each measured gene expression profile. In this paper, we analyze an empirical distribution of cells and obtain upper and lower bounds on the Wasserstein distance to the true population. Our analysis holds for general, non-parametric distributions of cells, and is validated by simulation experiments on a real single-cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14326v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Sharvaj Kubal, Geoffrey Schiebinger</dc:creator>
    </item>
    <item>
      <title>Anytime Validity is Free: Inducing Sequential Tests</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v5 Announce Type: replace-cross 
Abstract: Anytime valid sequential tests permit us to stop testing based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe this must come at the cost of power when compared to a conventional test that waits until all $N$ observations have arrived. Our first contribution is to show that this is false: for any valid test based on $N$ observations, we show how to construct an anytime valid sequential test that matches it after $N$ observations. Our second contribution is that we may continue testing by using the outcome of a $[0, 1]$-valued test as a conditional significance level in subsequent testing, leading to an overall procedure that is valid at the original significance level. This shows that anytime validity and optional continuation are readily available in traditional testing, without requiring explicit use of e-values. We illustrate this by deriving the anytime valid sequentialized $z$-test and $t$-test, which at time $N$ coincide with the traditional $z$-test and $t$-test. Finally, we characterize the SPRT by invariance under test induction, and also show under an i.i.d. assumption that the SPRT is induced by the Neyman-Pearson test for a tiny significance level and huge $N$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Density estimation via mixture discrepancy and moments</title>
      <link>https://arxiv.org/abs/2504.01570</link>
      <description>arXiv:2504.01570v2 Announce Type: replace-cross 
Abstract: With the aim of generalizing histogram statistics to higher dimensional cases, density estimation via discrepancy based sequential partition (DSP) has been proposed to learn an adaptive piecewise constant approximation defined on a binary sequential partition of the underlying domain, where the star discrepancy is adopted to measure the uniformity of particle distribution. However, the calculation of the star discrepancy is NP-hard and it does not satisfy the reflection invariance and rotation invariance either. To this end, we use the mixture discrepancy and the comparison of moments as a replacement of the star discrepancy, leading to the density estimation via mixture discrepancy based sequential partition (DSP-mix) and density estimation via moment-based sequential partition (MSP), respectively. Both DSP-mix and MSP are computationally tractable and exhibit the reflection and rotation invariance. Numerical experiments in reconstructing Beta mixtures, Gaussian mixtures and heavy-tailed Cauchy mixtures up to 30 dimension are conducted, demonstrating that MSP can maintain the same accuracy compared with DSP, while gaining an increase in speed by a factor of two to twenty for large sample size, and DSP-mix can achieve satisfactory accuracy and boost the efficiency in low-dimensional tests ($d \le 6$), but might lose accuracy in high-dimensional problems due to a reduction in partition level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01570v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyang Lei, Lirong Qu, Sihong Shao, Yunfeng Xiong</dc:creator>
    </item>
    <item>
      <title>Linear Regression Using Principal Components from General Hilbert-Space-Valued Covariates</title>
      <link>https://arxiv.org/abs/2504.16780</link>
      <description>arXiv:2504.16780v3 Announce Type: replace-cross 
Abstract: We consider linear regression with covariates that are random elements in a general Hilbert space. We first develop a principal component analysis for Hilbert-space-valued covariates based on finite-dimensional projections of the covariance operator, and establish asymptotic linearity and joint Gaussian limits for the leading eigenvalues and eigenfunctions under mild moment conditions. We then propose a principal component regression framework that combines Euclidean and Hilbert-space-valued covariates, obtain root-n consistent and asymptotically normal estimators of the regression parameters, and establish the validity of nonparametric and wild bootstrap procedures for inference. Simulation studies with two- and three-dimensional imaging predictors demonstrate accurate recovery of eigenstructures, regression coefficients, and bootstrap coverage. The methodology is further illustrated with neuroimaging data, in both a standard regression setting and a precision-medicine formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16780v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Li, Margaret Hoch, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>NA-DiD: Extending Difference-in-Differences with Capabilities</title>
      <link>https://arxiv.org/abs/2507.12690</link>
      <description>arXiv:2507.12690v2 Announce Type: replace-cross 
Abstract: This paper introduces the Non-Additive Difference-in-Differences (NA-DiD) framework, which extends classical DiD by incorporating non-additive measures the Choquet integral for effect aggregation. It serves as a novel econometric tool for impact evaluation, particularly in settings with non-additive treatment effects. First, we introduce the integral representation of the classial DiD model, and then extend it to non-additive measures, therefore deriving the formulae for NA-DiD estimation. Then, we give its theoretical properties. Applying NA-DiD to a simulated hospital hygiene intervention, we find that classical DiD can overestimate treatment effects, f.e. failing to account for compliance erosion. In contrast, NA-DiD provides a more accurate estimate by incorporating non-linear aggregation. The Julia implementation of the techniques used and introduced in this article is provided in the appendices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12690v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanis{\l}aw M. S. Halkiewicz</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</title>
      <link>https://arxiv.org/abs/2511.14455</link>
      <description>arXiv:2511.14455v3 Announce Type: replace-cross 
Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14455v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola Rares Franco, Lorenzo Tedesco</dc:creator>
    </item>
  </channel>
</rss>
