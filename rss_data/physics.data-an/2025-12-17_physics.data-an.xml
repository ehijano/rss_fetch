<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Dec 2025 05:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Look everywhere effects in anomaly detection</title>
      <link>https://arxiv.org/abs/2512.13787</link>
      <description>arXiv:2512.13787v1 Announce Type: cross 
Abstract: Machine learning-based anomaly detection methods are able to search high-dimensional spaces for hints of new physics with much less theory bias than traditional searches. However, by searching in many directions all at once, the statistical power of these search strategies is diluted by a variant of the look elsewhere effect. We examine this challenge in detail, focusing on weakly supervised methods. We find that training and testing on the same data results in badly miscalibrated $p$-values due to the anomaly detector searching everywhere in the data and overfitting on statistical fluctuations. However, if these $p$-values can be calibrated, they may offer the best sensitivity to anomalies, since this approach uses all of the data. Conversely, training on half of the data and testing on the other half results in perfectly calibrated $p$-values, but at the cost of reduced sensitivity to anomalies. Similarly, regularization methods such as early stopping can help with $p$-value calibration but also possibly at the expense of sensitivity. Finally, we find that k-folding strikes an effective balance between calibration and sensitivity. Our findings are supported by numerical studies with Gaussian random variables as well as from collider physics using the LHC Olympics benchmark anomaly detection dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13787v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Hein, Benjamin Nachman, David Shih</dc:creator>
    </item>
    <item>
      <title>Renormalization group for spectral collapse in random matrices with power-law variance profiles</title>
      <link>https://arxiv.org/abs/2512.13883</link>
      <description>arXiv:2512.13883v1 Announce Type: cross 
Abstract: We propose a renormalization group (RG) approach to compare and collapse eigenvalue densities of random matrix models of complex systems across different system sizes. The approach is to fix a natural spectral scale by letting the model normalization run with size, turning raw spectra into comparable, collapsed density curves. We demonstrate this approach on generalizations of two classic random matrix ensembles--Wigner and Wishart--modified to have power-law variance profiles. We use random matrix theory methods to derive self-consistent fixed-point equations for the resolvent to compute their eigenvalue densities, we define an RG scheme based on matrix decimation, and compute the Beta function controlling the RG flow as a function of the variance profile power-law exponent. The running normalization leads to spectral collapse which we confirm in simulations and solutions of the fixed-point equations. We expect this RG approach to carry over to other ensembles, providing a method for data analysis of a broad range of complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13883v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Fleig</dc:creator>
    </item>
    <item>
      <title>WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields</title>
      <link>https://arxiv.org/abs/2512.14656</link>
      <description>arXiv:2512.14656v1 Announce Type: cross 
Abstract: We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14656v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.CV</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriele Accarino, Viviana Acquaviva, Sara Shamekh, Duncan Watson-Parris, David Lawrence</dc:creator>
    </item>
    <item>
      <title>Modeling Multistability and Hysteresis in Urban Congestion Spreading</title>
      <link>https://arxiv.org/abs/2507.06659</link>
      <description>arXiv:2507.06659v2 Announce Type: replace-cross 
Abstract: Growing evidence suggests that the macroscopic functional states of urban road networks exhibit multistability and hysteresis, but microscopic mechanisms underlying these phenomena remain elusive. Here, we demonstrate that in real-world road networks, the recovery process of congested roads is not spontaneous, as assumed in existing models, but is hindered by connected congested roads, and such hindered recovery can lead to the emergence of multistability and hysteresis in urban traffic dynamics. By analyzing real-world urban traffic data, we observed that congestion propagation between individual roads is well described by a simple contagion process like an epidemic, but the recovery rate of a congested road decreases drastically by the congestion of the adjacent roads unlike an epidemic. Based on this microscopic observation, we proposed a simple model of congestion propagation and dissipation, and found that our model shows a discontinuous phase transition between macroscopic functional states of road networks when the recovery hindrance is strong enough through a mean-field approach and numerical simulations. Our findings shed light on an overlooked role of recovery processes in the collective dynamics of failures in networked systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06659v2</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/hrfw-1g56</arxiv:DOI>
      <arxiv:journal_reference>Physical Review E (2025) 112 (6), 064308</arxiv:journal_reference>
      <dc:creator>Jung-Hoon Jung, Young-Ho Eom</dc:creator>
    </item>
    <item>
      <title>Reliable Statistical Guarantees for Conformal Predictors with Small Datasets</title>
      <link>https://arxiv.org/abs/2512.04566</link>
      <description>arXiv:2512.04566v2 Announce Type: replace-cross 
Abstract: Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the relevance of the uncertainty model's statistical guarantee, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers relevant information about the coverage of a conformal predictor for small data sizes. We validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04566v2</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel S\'anchez-Dom\'inguez, Lucas Lacasa, Javier de Vicente, Gonzalo Rubio, Eusebio Valero</dc:creator>
    </item>
    <item>
      <title>Learnability Window in Gated Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2512.05790</link>
      <description>arXiv:2512.05790v2 Announce Type: replace-cross 
Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $\mu_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-\alpha}$, where $f(\ell)=\|\mu_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory shows that the time-scale spectra induced by the effective learning rates are the dominant determinants of learnability. Broader or more heterogeneous spectra slow the decay of $f(\ell)$, enlarging the learnability window, while heavy-tailed noise compresses $\mathcal{H}_N$ by limiting statistical concentration. By integrating gate-induced time-scale geometry with gradient noise and sample complexity, the framework identifies the effective learning rates as the primary objects that determine whether, when, and over what horizons recurrent networks can learn long-range temporal dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05790v2</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Livi</dc:creator>
    </item>
  </channel>
</rss>
