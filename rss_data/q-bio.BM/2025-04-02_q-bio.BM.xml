<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 01:56:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>GP-MoLFormer: A Foundation Model For Molecular Generation</title>
      <link>https://arxiv.org/abs/2405.04912</link>
      <description>arXiv:2405.04912v2 Announce Type: replace 
Abstract: Transformer-based models trained on large and general purpose datasets consisting of molecular strings have recently emerged as a powerful tool for successfully modeling various structure-property relations. Inspired by this success, we extend the paradigm of training chemical language transformers on large-scale chemical datasets to generative tasks in this work. Specifically, we propose GP-MoLFormer, an autoregressive molecular string generator that is trained on more than 1.1B (billion) chemical SMILES. GP-MoLFormer uses a 46.8M parameter transformer decoder model with linear attention and rotary positional encodings as the base architecture. GP-MoLFormer's utility is evaluated and compared with that of existing baselines on three different tasks: de novo generation, scaffold-constrained molecular decoration, and unconstrained property-guided optimization. While the first two are handled with no additional training, we propose a parameter-efficient fine-tuning method for the last task, which uses property-ordered molecular pairs as input. We call this new approach pair-tuning. Our results show GP-MoLFormer performs better or comparable with baselines across all three tasks, demonstrating its general utility for a variety of molecular generation tasks. We further report strong memorization of training data in GP-MoLFormer generations, which has so far remained unexplored for chemical language models. Our analyses reveal that training data memorization and novelty in generations are impacted by the quality and scale of the training data; duplication bias in training data can enhance memorization at the cost of lowering novelty. We further establish a scaling law relating inference compute and novelty in generations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04912v2</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <category>physics.chem-ph</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jerret Ross, Brian Belgodere, Samuel C. Hoffman, Vijil Chenthamarakshan, Jiri Navratil, Youssef Mroueh, Payel Das</dc:creator>
    </item>
    <item>
      <title>PharMolixFM: All-Atom Foundation Models for Molecular Modeling and Generation</title>
      <link>https://arxiv.org/abs/2503.21788</link>
      <description>arXiv:2503.21788v3 Announce Type: replace 
Abstract: Structural biology relies on accurate three-dimensional biomolecular structures to advance our understanding of biological functions, disease mechanisms, and therapeutics. While recent advances in deep learning have enabled the development of all-atom foundation models for molecular modeling and generation, existing approaches face challenges in generalization due to the multi-modal nature of atomic data and the lack of comprehensive analysis of training and sampling strategies. To address these limitations, we propose PharMolixFM, a unified framework for constructing all-atom foundation models based on multi-modal generative techniques. Our framework includes three variants using state-of-the-art multi-modal generative models. By formulating molecular tasks as a generalized denoising process with task-specific priors, PharMolixFM achieves robust performance across various structural biology applications. Experimental results demonstrate that PharMolixFM-Diff achieves competitive prediction accuracy in protein-small-molecule docking (83.9% vs. 90.2% RMSD &lt; 2{\AA}, given pocket) with significantly improved inference speed. Moreover, we explore the empirical inference scaling law by introducing more sampling repeats or steps. Our code and model are available at https://github.com/PharMolix/OpenBioMed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21788v3</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhen Luo, Jiashuo Wang, Siqi Fan, Zaiqing Nie</dc:creator>
    </item>
    <item>
      <title>GPx4 is bound to peroxidized membranes by a hydrophobic anchor</title>
      <link>https://arxiv.org/abs/2503.23341</link>
      <description>arXiv:2503.23341v2 Announce Type: replace 
Abstract: Ferroptosis is a form of cell death discovered in recent years, induced by excessive peroxidation of phospholipids. Glutathione peroxidase 4 (GPx4) is an intracellular enzyme that can repair the peroxidized phospholipids on membranes, thus regulating ferroptosis. By combining multiscale molecular dynamics (MD) simulations and experimental assays, we investigate the binding mechanisms of GPx4 on membranes. Using coarse-grained MD simulations, we found that L130 and its adjacent residues on GPx4 can form a stable and unique binding interface with PE/PS-rich and peroxidized membranes. Subsequent all-atom MD simulations verified the stability of the binding interface. The critical residue on the interface, L130, was inserted deeply into the membrane as a hydrophobic anchor and guided the reaction center toward the membrane surface. Enzyme activity assays and in vitro cell experiments showed that mutations of L130 resulted in weaker activities of the enzyme, probably caused by non-functional binding modes of GPx4 on membranes, as revealed by in silico simulations. This study highlights the crucial role of the hydrophobic residue, L130, in the proper anchoring of GPx4 on membranes, the first step of its membrane-repairing function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23341v2</guid>
      <category>q-bio.BM</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qingyang Hu, Hantian You, Kenan Li, Luhua Lai, Chen Song</dc:creator>
    </item>
    <item>
      <title>Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions</title>
      <link>https://arxiv.org/abs/2408.16245</link>
      <description>arXiv:2408.16245v3 Announce Type: replace-cross 
Abstract: The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. Almost all research on large-scale biosequence transformers has focused on one domain at a time (single-omic), usually DNA/RNA or proteins. These models have seen incredible success in downstream tasks in each domain, and have achieved particularly noteworthy breakthroughs in sequence modeling and structural modeling. However, these single-omic models are naturally incapable of efficiently modeling multi-omic tasks, one of the most biologically critical being protein-nucleic acid interactions. We present our work training the largest open-source multi-omic foundation model to date. We show that these multi-omic models (MOMs) can learn joint representations between various single-omic distributions that are emergently consistent with the Central Dogma of molecular biology despite only being trained on unlabeled biosequences. We further demonstrate that MOMs can be fine-tuned to achieve state-of-the-art results on protein-nucleic acid interaction tasks, namely predicting the change in Gibbs free energy ($\Delta G$) of the binding interaction between a given nucleic acid and protein. Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any \textit{a priori} structural training, allowing us to predict which protein residues are most involved in the protein-nucleic acid binding interaction. Lastly, we provide evidence that multi-omic biosequence models are in many cases superior to foundation models trained on single-omics distributions, both in performance-per-FLOP and absolute performance, suggesting a more generalized or foundational approach to building these models for biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16245v3</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sully F. Chen, Robert J. Steele, Glen M. Hocky, Beakal Lemeneh, Shivanand P. Lad, Eric K. Oermann</dc:creator>
    </item>
  </channel>
</rss>
