<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 May 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Convergence Properties of PINNs for the Navier-Stokes-Cahn-Hilliard System</title>
      <link>https://arxiv.org/abs/2505.07964</link>
      <description>arXiv:2505.07964v1 Announce Type: new 
Abstract: Approximating solutions to differential equations using neural networks has become increasingly popular and shows significant promise. In this paper, we propose a simplified framework for analyzing the potential of neural networks to simulate differential equations based on the properties of the equations themselves. We apply this framework to the Cahn-Hilliard and Navier-Stokes-Cahn-Hilliard systems, presenting both theoretical analysis and practical implementations. We then conduct numerical experiments on toy problems to validate the framework's efficacy in accurately capturing the desired properties of these systems and numerically estimate relevant convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07964v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Buck, Roger Temam</dc:creator>
    </item>
    <item>
      <title>Dyadic Factorization and Efficient Inversion of Sparse Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2505.08144</link>
      <description>arXiv:2505.08144v1 Announce Type: new 
Abstract: In inverting large sparse matrices, the key difficulty lies in effectively exploiting sparsity during the inversion process. One well-established strategy is the nested dissection, which seeks the so-called sparse Cholesky factorization. We argue that the matrices for which such factors can be found are characterized by a hidden dyadic sparsity structure. This paper builds on that idea by proposing an efficient approach for inverting such matrices. The method consists of two independent steps: the first packs the matrix into a dyadic form, while the second performs a sparse (dyadic) Gram-Schmidt orthogonalization of the packed matrix.
  The novel packing procedure works by recovering block-tridiagonal structures, focusing on aggregating terms near the diagonal using the $l_1$-norm, which contrasts with traditional methods that prioritize minimizing bandwidth, i.e. the $l_\infty$-norm. The algorithm performs particularly well for matrices that can be packed into banded or dyadic forms which are moderately dense. Due to the properties of $l_1$-norm, the packing step can be applied iteratively to reconstruct the hidden dyadic structure, which corresponds to the detection of separators in the nested dissection method.
  We explore the algebraic properties of dyadic-structured matrices and present an algebraic framework that allows for a unified mathematical treatment of both sparse factorization and efficient inversion of factors. For matrices with a dyadic structure, we introduce an optimal inversion algorithm and evaluate its computational complexity.
  The proposed inversion algorithm and core algebraic operations for dyadic matrices are implemented in the R package DyadiCarma, utilizing Rcpp and RcppArmadillo for high-performance computing. An independent R-based matrix packing module, supported by C++ code, is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08144v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha{\l} Kos, Krzysztof Podg\'orski, Hanqing Wu</dc:creator>
    </item>
    <item>
      <title>Adaptive and hybrid reduced order models to mitigate Kolmogorov barrier in a multiscale kinetic transport equation</title>
      <link>https://arxiv.org/abs/2505.08214</link>
      <description>arXiv:2505.08214v1 Announce Type: new 
Abstract: In this work, we develop reduced order models (ROMs) to predict solutions to a multiscale kinetic transport equation with a diffusion limit under the parametric setting. When the underlying scattering effect is not sufficiently strong, the system governed by this equation exhibits transport-dominated behavior. Suffering from the Kolmogorov barrier for transport-dominant problems, classical linear ROMs may become inefficient in this regime. To address this issue, we first develop a piecewise linear ROM by introducing a novel goal-oriented adaptive time partitioning strategy. To avoid local over-refinement or under-refinement, we propose an adaptive coarsening and refinement strategy that remains robust with various initial empirical partitions. Additionally, for problems where a local linear approximation is not sufficiently efficient, we further develop a hybrid ROM, which combines autoencoder-based nonlinear ROMs and piecewise linear ROMs. Compared to previous autoencoder-based ROMs, this hybridized method reduces the offline autoencoder's training cost by only applying it to time intervals that are adaptively identified as the most challenging. Numerical experiments demonstrate that our proposed approaches successfully predict full-order solutions at unseen parameter values with both efficiency and accuracy. To the best of our knowledge, this is the first attempt to address the Kolmogorov barrier for multiscale kinetic transport problems with the coexistence of both transport- and diffusion-dominant behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08214v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Jin, Zhichao Peng, Yang Xiang</dc:creator>
    </item>
    <item>
      <title>Local Convergence Behavior of Extended LOBPCG for Computing Eigenvalues of Hermitian Matrices</title>
      <link>https://arxiv.org/abs/2505.08218</link>
      <description>arXiv:2505.08218v1 Announce Type: new 
Abstract: This paper provides a comprehensive and detailed analysis of the local convergence behavior of an extended variation of the locally optimal preconditioned conjugate gradient method (LOBPCG) for computing the extreme eigenvalue of a Hermitian matrix. The convergence rates derived in this work are either obtained for the first time or sharper than those previously established, including those in Ovtchinnikov's work ({\em SIAM J. Numer. Anal.}, 46(5):2567--2592, 2008). The study also extends to generalized problems, including Hermitian matrix polynomials that admit an extended form of the Rayleigh quotient. The new approach used to obtain these rates may also serve as a valuable tool for the convergence analysis of other gradient-type optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08218v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhechen Shen, Xin Liang</dc:creator>
    </item>
    <item>
      <title>The Lax--Wendroff theorem for Patankar-type methods applied to hyperbolic conservation laws</title>
      <link>https://arxiv.org/abs/2505.08387</link>
      <description>arXiv:2505.08387v1 Announce Type: new 
Abstract: For hyperbolic conservation laws, the famous Lax--Wendroff theorem delivers sufficient conditions for the limit of a convergent numerical method to be a weak (entropy) solution. This theorem is a fundamental result, and many investigations have been done to verify its validity for finite difference, finite volume, and finite element schemes, using either explicit or implicit linear time-integration methods. Recently, the use of modified Patankar (MP) schemes as time-integration methods for the discretization of hyperbolic conservation laws has gained increasing interest. These schemes are unconditionally conservative and positivity-preserving and only require the solution of a linear system. However, MP schemes are by construction nonlinear, which is why the theoretical investigation of these schemes is more involved. We prove an extension of the Lax--Wendroff theorem for the class of MP methods. This is the first extension of the Lax--Wendroff theorem to nonlinear time integration methods with just an additional hypothesis on the total time variation boundness of the numerical solutions. We provide some numerical simulations that validate the theoretical observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08387v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Janina Bender, Thomas Izgin, Philipp \"Offner, Davide Torlo</dc:creator>
    </item>
    <item>
      <title>Lower bounds for the reach and applications</title>
      <link>https://arxiv.org/abs/2505.08427</link>
      <description>arXiv:2505.08427v1 Announce Type: new 
Abstract: The reach of a submanifold of $\mathbb{R}^N$ is defined as the largest radius of a tubular neighbourhood around the submanifold that avoids self-intersections. While essential in geometric and topological applications, computing the reach explicitly is notoriously difficult. In this paper, we introduce a rigorous and practical method to compute a guaranteed lower bound for the reach of a submanifold described as the common zero-set of finitely many smooth functions, not necessarily polynomials. Our algorithm uses techniques from numerically verified proofs and is particularly suitable for high-performance parallel implementations.
  We illustrate the utility of this method through several applications. Of special note is a novel algorithm for computing the homology groups of planar curves, achieved by constructing a cubical complex that deformation retracts onto the curve--an approach potentially extendable to higher-dimensional manifolds. Additional applications include an improved comparison inequality between intrinsic and extrinsic distances for submanifolds of $\mathbb{R}^N$, lower bounds for the first eigenvalue of the Laplacian on algebraic varieties and explicit bounds on how much smooth varieties can be deformed without changing their diffeomorphism type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08427v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Platt, Ra\'ul S\'anchez Gal\'an</dc:creator>
    </item>
    <item>
      <title>An Optimal and Robust Nonconforming Finite Element Method for the Strain Gradient Elasticity</title>
      <link>https://arxiv.org/abs/2505.08461</link>
      <description>arXiv:2505.08461v1 Announce Type: new 
Abstract: An optimal and robust low-order nonconforming finite element method is developed for the strain gradient elasticity (SGE) model in arbitrary dimension. An $H^2$-nonconforming quadratic vector-valued finite element in arbitrary dimension is constructed, which together with an $H^1$-nonconforming scalar finite element and the Nitsche's technique, is applied for solving the SGE model. The resulting nonconforming finite element method is optimal and robust with respect to the Lam\'{e} coefficient $\lambda$ and the size parameter $\iota$, as confirmed by numerical results. Additionally, nonconforming finite element discretization of the smooth Stokes complex in two and three dimensions is devised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08461v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianguo Huang, Xuehai Huang, Zheqian Tang</dc:creator>
    </item>
    <item>
      <title>Numerical Solution of Mixed-Dimensional PDEs Using a Neural Preconditioner</title>
      <link>https://arxiv.org/abs/2505.08491</link>
      <description>arXiv:2505.08491v1 Announce Type: new 
Abstract: Mixed-dimensional partial differential equations (PDEs) are characterized by coupled operators defined on domains of varying dimensions and pose significant computational challenges due to their inherent ill-conditioning. Moreover, the computational workload increases considerably when attempting to accurately capture the behavior of the system under significant variations or uncertainties in the low-dimensional structures such as fractures, fibers, or vascular networks, due to the inevitable necessity of running multiple simulations. In this work, we present a novel preconditioning strategy that leverages neural networks and unsupervised operator learning to design an efficient preconditioner specifically tailored to a class of 3D-1D mixed-dimensional PDEs. The proposed approach is capable of generalizing to varying shapes of the 1D manifold without retraining, making it robust to changes in the 1D graph topology. Moreover, thanks to convolutional neural networks, the neural preconditioner can adapt over a range of increasing mesh resolutions of the discrete problem, enabling us to train it on low resolution problems and deploy it on higher resolutions. Numerical experiments validate the effectiveness of the preconditioner in accelerating convergence in iterative solvers, demonstrating its appeal and limitations over traditional methods. This study lays the groundwork for applying neural network-based preconditioning techniques to a broader range of coupled multi-physics systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08491v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nunzio Dimola, Nicola Rares Franco, Paolo Zunino</dc:creator>
    </item>
    <item>
      <title>Numerical Analysis of Stabilization for Random Hyperbolic Systems of Balance Laws</title>
      <link>https://arxiv.org/abs/2505.08511</link>
      <description>arXiv:2505.08511v1 Announce Type: new 
Abstract: This paper extends the deterministic Lyapunov-based stabilization framework to random hyperbolic systems of balance laws, where uncertainties arise in boundary controls and initial data. Building on the finite volume discretization method from [{\sc M. Banda and M. Herty}, Math. Control Relat. Fields., 3 (2013), pp. 121--142], we introduce a stochastic discrete Lyapunov function to prove the exponential decay of numerical solutions for systems with random perturbations. For linear systems, we derive explicit decay rates, which depend on boundary control parameters, grid resolutions, and the statistical properties of the random inputs. Theoretical decay rates are verified through numerical examples, including boundary stabilization of the linear wave equations and linearized shallow-water flows with random perturbations. We also demonstrate the decay rates for a nonlinear example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08511v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoshuai Chu, Michael Herty, Alexander Kurganov</dc:creator>
    </item>
    <item>
      <title>Improving Data Fidelity via Diffusion Model-based Correction and Super resolution</title>
      <link>https://arxiv.org/abs/2505.08526</link>
      <description>arXiv:2505.08526v1 Announce Type: new 
Abstract: We propose a unified diffusion model-based correction and super-resolution method to enhance the fidelity and resolution of diverse low-quality data through a two-step pipeline. First, the correction step employs a novel enhanced stochastic differential editing technique based on an imbalanced perturbation and denoising process, ensuring robust and effective bias correction at the low-resolution level. The robustness and effectiveness of this approach are validated theoretically and experimentally. Next, the super-resolution step leverages cascaded conditional diffusion models to iteratively refine the corrected data to high-resolution. Numerical experiments on three PDE problems and a climate dataset demonstrate that the proposed method effectively enhances low-fidelity, low-resolution data by correcting numerical errors and noise while simultaneously improving resolution to recover fine-scale structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08526v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wuzhe Xu, Yulong Lu, Sifan Wang, Tong-Rui Liu</dc:creator>
    </item>
    <item>
      <title>Entropy numbers of classes defined by integral operators</title>
      <link>https://arxiv.org/abs/2505.08572</link>
      <description>arXiv:2505.08572v1 Announce Type: new 
Abstract: In this paper we develop the following general approach. We study asymptotic behavior of the entropy numbers not for an individual smoothness class, how it is usually done, but for the collection of classes, which are defined by integral operators with kernels coming from a given class of functions. Earlier, such approach was realized for the Kolmogorov widths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08572v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V. Temlyakov</dc:creator>
    </item>
    <item>
      <title>Two-Level Sketching Alternating Anderson acceleration for Complex Physics Applications</title>
      <link>https://arxiv.org/abs/2505.08587</link>
      <description>arXiv:2505.08587v1 Announce Type: new 
Abstract: We present a novel two-level sketching extension of the Alternating Anderson-Picard (AAP) method for accelerating fixed-point iterations in challenging single- and multi-physics simulations governed by discretized partial differential equations. Our approach combines a static, physics-based projection that reduces the least-squares problem to the most informative field (e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage driven by a backward stability analysis under Lipschitz continuity. We introduce inexpensive estimators for stability thresholds and cache-aware randomized selection strategies to balance computational cost against memory-access overhead. The resulting algorithm solves reduced least-squares systems in place, minimizes memory footprints, and seamlessly alternates between low-cost Picard updates and Anderson mixing. Implemented in Julia, our two-level sketching AAP achieves up to 50% time-to-solution reductions compared to standard Anderson acceleration-without degrading convergence rates-on benchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes formulations at varying problem sizes. These results demonstrate the method's robustness, scalability, and potential for integration into high-performance scientific computing frameworks. Our implementation is available open-source in the AAP.jl library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08587v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicol\'as A. Barnafi, Massimiliano Lupo Pasini</dc:creator>
    </item>
    <item>
      <title>Learning cardiac activation and repolarization times with operator learning</title>
      <link>https://arxiv.org/abs/2505.08631</link>
      <description>arXiv:2505.08631v1 Announce Type: new 
Abstract: Solving partial or ordinary differential equation models in cardiac electrophysiology is a computationally demanding task, particularly when high-resolution meshes are required to capture the complex dynamics of the heart. Moreover, in clinical applications, it is essential to employ computational tools that provide only relevant information, ensuring clarity and ease of interpretation. In this work, we exploit two recently proposed operator learning approaches, namely Fourier Neural Operators (FNO) and Kernel Operator Learning (KOL), to learn the operator mapping the applied stimulus in the physical domain into the activation and repolarization time distributions. These data-driven methods are evaluated on synthetic 2D and 3D domains, as well as on a physiologically realistic left ventricle geometry. Notably, while the learned map between the applied current and activation time has its modelling counterpart in the Eikonal model, no equivalent partial differential equation (PDE) model is known for the map between the applied current and repolarization time. Our results demonstrate that both FNO and KOL approaches are robust to hyperparameter choices and computationally efficient compared to traditional PDE-based Monodomain models. These findings highlight the potential use of these surrogate operators to accelerate cardiac simulations and facilitate their clinical integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08631v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Centofanti, Giovanni Ziarelli, Nicola Parolini, Simone Scacchi, Marco Verani, Luca Franco Pavarino</dc:creator>
    </item>
    <item>
      <title>A Reynolds-semi-robust H(div)-conforming method for unsteady incompressible non-Newtonian flows</title>
      <link>https://arxiv.org/abs/2505.08708</link>
      <description>arXiv:2505.08708v1 Announce Type: new 
Abstract: In this work, we prove what appear to be the first Reynolds-semi-robust and pressure-robust velocity error estimates for an H(div)-conforming approximation of unsteady incompressible flows of power-law type fluids. The proposed methods hinges on a discontinuous Galerkin approximation of the viscous term and a reinforced upwind-type stabilization of the convective term. The derived velocity error estimates account for pre-asymptotic orders of convergence observed in convection-dominated flows through regime-dependent estimates of the error contributions. A complete set of numerical results validate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08708v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louren\c{c}o Beir\~ao da Veiga, Daniele A. Di Pietro, Kirubell B. Haile</dc:creator>
    </item>
    <item>
      <title>Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks</title>
      <link>https://arxiv.org/abs/2505.08022</link>
      <description>arXiv:2505.08022v1 Announce Type: cross 
Abstract: Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94% compression while recovering or improving adversarial accuracy relative to uncompressed baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08022v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steffen Schotth\"ofer, H. Lexie Yang, Stefan Schnake</dc:creator>
    </item>
    <item>
      <title>Identifying Memorization of Diffusion Models through p-Laplace Analysis</title>
      <link>https://arxiv.org/abs/2505.08246</link>
      <description>arXiv:2505.08246v1 Announce Type: cross 
Abstract: Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely p-Laplace operators. We show here these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. We analyze the structured case of Gaussian mixture models, and demonstrate the results carry-over to image generative models, where memorization identification based on the p-Laplace operator is performed for the first time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08246v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Brokman, Amit Giloni, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Guy Gilboa</dc:creator>
    </item>
    <item>
      <title>Nonlinear optical response in kagome lattice with inversion symmetry breaking</title>
      <link>https://arxiv.org/abs/2505.08289</link>
      <description>arXiv:2505.08289v1 Announce Type: cross 
Abstract: The kagome lattice is a fundamental model structure in condensed matter physics and materials science featuring symmetry-protected flat bands, saddle points, and Dirac points. This structure has emerged as an ideal platform for exploring various quantum physics. By combining effective model analysis and first-principles calculations, we propose that the synergy among inversion symmetry breaking, flat bands, and saddle point-related van Hove singularities within the kagome lattice holds significant potential for generating strong second-order nonlinear optical response. This property provides an inspiring insight into the practical application of the kagome-like materials, which is helpful for a comprehensive understanding of kagome lattice-related physics. Moreover, this work offers an alternative approach for designing materials with strong a second-order nonlinear optical response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08289v1</guid>
      <category>physics.optics</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1021/acsnano.4c17371</arxiv:DOI>
      <dc:creator>Xiangyang Liu, Junwen Lai, Jie Zhan, Tianye Yu, Peitao Liu, Seiji Yunoki, Xing-Qiu Chen, Yan Sun</dc:creator>
    </item>
    <item>
      <title>An incremental algorithm for non-convex AI-enhanced medical image processing</title>
      <link>https://arxiv.org/abs/2505.08324</link>
      <description>arXiv:2505.08324v1 Announce Type: cross 
Abstract: Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08324v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Morotti</dc:creator>
    </item>
    <item>
      <title>A Fourier finite volume approach for the optical inverse problem of quantitative photoacoustic tomography</title>
      <link>https://arxiv.org/abs/2505.08400</link>
      <description>arXiv:2505.08400v1 Announce Type: cross 
Abstract: A new approach for solving the optical inverse problem of quantitative photoacoustic tomography is introduced, which interpolates between the well-known diffusion approximation and a radiative transfer equation based model. The proposed formulation combines a spatial finite volume scheme with a truncated Fourier expansion in the direction variable for the radiative transfer equation. The finite volume scheme provides a natural and simple approach for representing piecewise constant image data modelled using transport equations. The truncated Fourier expansion in the direction variable facilitates the interpolation between the diffusion approximation at low order, and the full radiative transfer model as the truncation limit $N\rightarrow\infty$. It is therefore possible to tune the precision of the model to the demands of the imaging application, taking $N=1$ for cases when the diffusion approximation would suffice and increasing the number of terms otherwise. We will then utilise the non-linear optimisation functionality of Matlab to address the corresponding large-scale nonlinear inverse problem using gradient based quasi-Newton minimisation via the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Numerical experiments for two test-cases of increasing complexity and resolution will be presented, and the effect of logarithmically rescaling the problem data on the accuracy of the reconstructed solutions will be investigated. We will focus on cases where the diffusion approximation is not sufficient to demonstrate that our approach can provide significant accuracy gains with only a modest increase in the number of Fourier terms included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08400v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.med-ph</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David J. Chappell</dc:creator>
    </item>
    <item>
      <title>CodePDE: An Inference Framework for LLM-driven PDE Solver Generation</title>
      <link>https://arxiv.org/abs/2505.08783</link>
      <description>arXiv:2505.08783v1 Announce Type: cross 
Abstract: Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling -- all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at https://github.com/LithiumDA/CodePDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08783v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar</dc:creator>
    </item>
    <item>
      <title>Using generalized simplex methods to approximate derivatives</title>
      <link>https://arxiv.org/abs/2310.16997</link>
      <description>arXiv:2310.16997v2 Announce Type: replace 
Abstract: This paper presents two methods for approximating a proper subset of the entries of a Hessian using only function evaluations. These approximations are obtained using the techniques called \emph{generalized simplex Hessian} and \emph{generalized centered simplex Hessian}. We show how to choose the matrices of directions involved in the computation of these two techniques depending on the entries of the Hessian of interest. We discuss the number of function evaluations required in each case and develop a general formula to approximate all order-$P$ partial derivatives. Since only function evaluations are required to compute the methods discussed in this paper, they are suitable for use in derivative-free optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16997v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Jarry-Bolduc, Chayne Planiden</dc:creator>
    </item>
    <item>
      <title>Asymptotic convergence of restarted Anderson acceleration for certain normal linear systems</title>
      <link>https://arxiv.org/abs/2312.04776</link>
      <description>arXiv:2312.04776v4 Announce Type: replace 
Abstract: Anderson acceleration (AA) is widely used for accelerating the convergence of an underlying fixed-point iteration $\bm{x}_{k+1} = \bm{q}( \bm{x}_{k} )$, $k = 0, 1, \ldots$, with $\bm{x}_k \in \mathbb{R}^n$, $\bm{q} \colon \mathbb{R}^n \to \mathbb{R}^n$. Despite AA's widespread use, relatively little is understood theoretically about the extent to which it may accelerate the underlying fixed-point iteration. To this end, we analyze a restarted variant of AA with a restart size of one, a method closely related to GMRES(1). We consider the case of $\bm{q}( \bm{x} ) = M \bm{x} + \bm{b}$ with matrix $M \in \mathbb{R}^{n \times n}$ either symmetric or skew-symmetric. For both classes of $M$ we compute the worst-case root-average asymptotic convergence factor of the AA method, partially relying on conjecture in the symmetric setting, proving that it is strictly smaller than that of the underlying fixed-point iteration. For symmetric $M$, we show that the AA residual iteration corresponds to a fixed-point iteration for solving an eigenvector-dependent nonlinear eigenvalue problem (NEPv), and we show how this can result in the convergence factor strongly depending on the initial iterate, which we quantify exactly in certain special cases. Conversely, for skew-symmetric $M$ we show that the AA residual iteration is closely related to a power iteration for $M$, and how this results in the convergence factor being independent of the initial iterate. Supporting numerical results are given, which also indicate the theory is applicable to the more general setting of nonlinear $\bm{q}$ with Jacobian at the fixed point that is symmetric or skew symmetric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04776v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver A. Krzysik, Hans De Sterck, Adam Smith</dc:creator>
    </item>
    <item>
      <title>Efficient nonlocal linear image denoising: Bilevel optimization with Nonequispaced Fast Fourier Transform and matrix-free preconditioning</title>
      <link>https://arxiv.org/abs/2407.06834</link>
      <description>arXiv:2407.06834v2 Announce Type: replace 
Abstract: We present a new approach for nonlocal image denoising, based around the application of an unnormalized extended Gaussian ANOVA kernel within a bilevel optimization algorithm. A critical bottleneck when solving such problems for finely-resolved images is the solution of huge-scale, dense linear systems arising from the minimization of an energy term. We tackle this using a Krylov subspace approach, with a Nonequispaced Fast Fourier Transform utilized to approximate matrix-vector products in a matrix-free manner. We accelerate the algorithm using a novel change of basis approach to account for the (known) smallest eigenvalue-eigenvector pair of the matrices involved, coupled with a simple but frequently very effective diagonal preconditioning approach. We present a number of theoretical results concerning the eigenvalues and predicted convergence behavior, and a range of numerical experiments which validate our solvers and use them to tackle parameter learning problems. These demonstrate that very large problems may be effectively and rapidly denoised with very low storage requirements on a computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06834v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'es Miniguano-Trujillo, John W. Pearson, Benjamin D. Goddard</dc:creator>
    </item>
    <item>
      <title>A Canonical Gauge for Computing of Eigenpairs of the Magnetic Schr\"odinger Operator</title>
      <link>https://arxiv.org/abs/2409.06023</link>
      <description>arXiv:2409.06023v3 Announce Type: replace 
Abstract: We consider the eigenvalue problem for the magnetic Schr\"odinger operator and take advantage of a property called gauge invariance to transform the given problem into an equivalent problem that is more amenable to numerical approximation. More specifically, we propose a canonical magnetic gauge that can be computed by solving a Poisson problem, that yields a new operator having the same spectrum but eigenvectors that are less oscillatory. Extensive numerical tests demonstrate that accurate computation of eigenpairs can be done more efficiently and stably with the canonical magnetic gauge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06023v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey S. Ovall, Li Zhu</dc:creator>
    </item>
    <item>
      <title>Multiscale method for image denoising using nonlinear diffusion process: local denoising and spectral multiscale basis functions</title>
      <link>https://arxiv.org/abs/2409.15952</link>
      <description>arXiv:2409.15952v2 Announce Type: replace 
Abstract: We consider image denoising using a nonlinear diffusion process, where we solve unsteady partial differential equations with nonlinear coefficients. The noised image is given as an initial condition, and nonlinear coefficients are used to preserve the main image features. In this paper, we present a multiscale method for the resulting nonlinear parabolic equation in order to construct an efficient solver. To both filter out noise and preserve essential image features during the denoising process, we utilize a time-dependent nonlinear diffusion model. Here, the noised image is fed as an initial condition and the denoised image is stimulated with given parameters. We numerically implement this model by constructing a discrete system for a given image resolution using a finite volume method and employing an implicit time approximation scheme to avoid time-step restriction. However, the resulting discrete system size is proportional to the number of pixels which leads to computationally expensive numerical algorithms for high-resolution images. In order to reduce the size of the system and construct efficient computational algorithms, we construct a coarse-resolution representation of the system. We incorporate local noise reduction in the coarsening process to construct an efficient algorithm with fewer denoising iterations. We propose a computational approach with two main ingredients: (1) performing local image denoising in each local domain of basis support; and (2) constructing multiscale basis functions to construct a coarse resolution representation by a Galerkin coupling. We present numerical results for several classic and high-resolution image datasets to demonstrate the effectiveness of the proposed multiscale approach with local denoising and local multiscale representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15952v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Vasilyeva, Aleksei Krasnikov, Kelum Gajamannage, Mehrube Mehrubeoglu</dc:creator>
    </item>
    <item>
      <title>Deflation-based certified greedy algorithm and adaptivity for bifurcating nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2501.12361</link>
      <description>arXiv:2501.12361v3 Announce Type: replace 
Abstract: This work deals with tailored reduced order models for bifurcating nonlinear parametric partial differential equations, where multiple coexisting solutions arise for a given parametric instance. Approaches based on proper orthogonal decomposition have been widely investigated in the literature, but they usually rely on some \emph{a-priori} knowledge about the bifurcating model and lack any error estimation. On the other hand, standard certified reduced basis techniques fail to represent correctly the branching behavior, since the error estimator is no longer reliable. The main goal of the contribution is to overcome these limitations by introducing two novel algorithms: (i) the adaptive-greedy, detecting the bifurcation point starting from scarce information over the parametric space, and (ii) the deflated-greedy, certifying multiple coexisting branches simultaneously. The former approach takes advantage of the features of the reduced manifold to detect the bifurcation, while the latter exploits the deflation and continuation methods to discover the bifurcating solutions and enrich the reduced space. We test the two strategies for the Coanda effect held by the Navier-Stokes equations in a sudden-expansion channel. The accuracy of the approach and the error certification are compared with vanilla-greedy and proper orthogonal decomposition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12361v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico Pichi, Maria Strazzullo</dc:creator>
    </item>
    <item>
      <title>Tensor parametric Hamiltonian operator inference</title>
      <link>https://arxiv.org/abs/2502.10888</link>
      <description>arXiv:2502.10888v2 Announce Type: replace 
Abstract: This work presents a tensorial approach to constructing data-driven reduced-order models corresponding to semi-discrete partial differential equations with canonical Hamiltonian structure. By expressing parameter-varying operators with affine dependence as contractions of a generalized parameter vector against a constant tensor, this method leverages the operator inference framework to capture parametric dependence in the learned reduced-order model via the solution to a convex, least-squares optimization problem. This leads to a concise and straightforward implementation which compactifies previous parametric operator inference approaches and directly extends to learning parametric operators with symmetry constraints, a key feature required for constructing structure-preserving surrogates of Hamiltonian systems. The proposed approach is demonstrated on both a (non-Hamiltonian) heat equation with variable diffusion coefficient as well as a Hamiltonian wave equation with variable wave speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10888v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjun Vijaywargiya, Shane A. McQuarrie, Anthony Gruber</dc:creator>
    </item>
    <item>
      <title>A Wong--Zakai resonance-based integrator for nonlinear Schr\"odinger equation with white noise dispersion</title>
      <link>https://arxiv.org/abs/2503.19346</link>
      <description>arXiv:2503.19346v2 Announce Type: replace 
Abstract: We introduce a novel approach to numerical approximation of nonlinear Schr\"odinger equation with white noise dispersion in the regime of low-regularity solutions. Approximating such solutions in the stochastic setting is particularly challenging due to randomized frequency interactions and presents a compelling challenge for the construction of tailored schemes. In particular, we design the first resonance-based schemes for this equation, which achieve provable convergence for solutions of much lower regularity than previously required. A crucial ingredient in this construction is the Wong--Zakai approximation of stochastic dispersive system, which introduces piecewise linear phases that capture nonlinear frequency interactions and can subsequently be approximated to construct resonance-based schemes. We prove the well-posedness of the Wong--Zakai approximated equation and establish its proximity to the original full stochastic dispersive system. Based on this approximation, we demonstrate an improved strong convergence rate for our new scheme, which exploits the stochastic nature of the dispersive terms. Finally, we provide numerical experiments underlining the favourable performance of our novel method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19346v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianbo Cui, Georg Maierhofer</dc:creator>
    </item>
    <item>
      <title>Enhanced convergence rates of Adaptive Importance Sampling with recycling schemes via quasi-Monte Carlo methods</title>
      <link>https://arxiv.org/abs/2505.05037</link>
      <description>arXiv:2505.05037v2 Announce Type: replace 
Abstract: This article investigates the integration of quasi-Monte Carlo (QMC) methods using the Adaptive Multiple Importance Sampling (AMIS). Traditional Importance Sampling (IS) often suffers from poor performance since it heavily relies on the choice of the proposal distributions. The AMIS and the Modified version of AMIS (MAMIS) address this by iteratively refining proposal distributions and reusing all past samples through a recycling strategy. We introduce the RQMC methods into the MAMIS, achieving higher convergence rates compared to the Monte Carlo (MC) methods. Our main contributions include a detailed convergence analysis of the MAMIS estimator under randomized QMC (RQMC) sampling. Specifically, we establish the $L^q$ $(q \geq 2)$ error bound for the RQMC-based estimator using a smoothed projection method, which enables us to apply the H\"older's inequality in the error analysis of the RQMC-based MAMIS estimator. As a result, we prove that the root mean square error of the RQMC-based MAMIS estimator converges at a rate of $\mathcal{O}(\bar{N}_T^{-1+\epsilon})$, where $\bar{N}_T$ is the average number of samples used in each step over $T$ iterations, and $\epsilon &gt; 0$ is arbitrarily small. Numerical experiments validate the effectiveness of our method, including mixtures of Gaussians, a banana-shaped model, and Bayesian Logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05037v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianlong Chen, Jiarui Du, Xiaoqun Wang, Zhijian He</dc:creator>
    </item>
    <item>
      <title>On Discrete-Time Approximations to Infinite Horizon Differential Games</title>
      <link>https://arxiv.org/abs/2112.03153</link>
      <description>arXiv:2112.03153v2 Announce Type: replace-cross 
Abstract: In this paper we study a discrete-time semidiscretization and a fully discretization (discrete-time, discrete-state) of an infinite time horizon noncooperative $N$-player differential game. We prove that as either the discretization time step or both time step and mesh size parameters approach zero the discrete value function approximates the value function of the differential game. Furthermore, the discrete Nash equilibrium is an $\epsilon$-Nash equilibrium for the continuous-time differential game both in the discrete-time and fully discrete cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03153v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier de Frutos, V\'ictor Gat\'on, Julia Novo</dc:creator>
    </item>
    <item>
      <title>An explicit Euler method for Sobolev vector fields with applications to the continuity equation on non cartesian grids</title>
      <link>https://arxiv.org/abs/2402.04118</link>
      <description>arXiv:2402.04118v3 Announce Type: replace-cross 
Abstract: We prove a novel stability estimate in $L^\infty _t (L^p _x)$ between the regular Lagrangian flow of a Sobolev vector field and a piecewise affine approximation of such flow. This approximation of the flow is obtained by a (sort of) explicit Euler method, and it is the crucial tool to prove approximation results for the solution of the continuity equation by using the representation of the solution as the push-forward via the regular Lagrangian flow of the initial datum. We approximate the solution in two ways, one probabilistic and one deterministic, using different approximations for both the flow and the initial datum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04118v3</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matpur.2025.103722</arxiv:DOI>
      <arxiv:journal_reference>Journal de Mathematiques Pur\'es et Appliqu\'ees, 199, (2025)</arxiv:journal_reference>
      <dc:creator>Tommaso Cortopassi</dc:creator>
    </item>
    <item>
      <title>Stable Derivative Free Gaussian Mixture Variational Inference for Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2501.04259</link>
      <description>arXiv:2501.04259v2 Announce Type: replace-cross 
Abstract: This paper is concerned with the approximation of probability distributions known up to normalization constants, with a focus on Bayesian inference for large-scale inverse problems in scientific computing. In this context, key challenges include costly repeated evaluations of forward models, multimodality, and inaccessible gradients for the forward model. To address them, we develop a variational inference framework that combines Fisher-Rao natural gradient with specialized quadrature rules to enable derivative free updates of Gaussian mixture variational families. The resulting method, termed Derivative Free Gaussian Mixture Variational Inference (DF-GMVI), guarantees covariance positivity and affine invariance, offering a stable and efficient framework for approximating complex posterior distributions. The effectiveness of DF-GMVI is demonstrated through numerical experiments on challenging scenarios, including distributions with multiple modes, infinitely many modes, and curved modes in spaces with up to 100 dimensions. The method's practicality is further demonstrated in a large-scale application, where it successfully recovers the initial conditions of the Navier-Stokes equations from solution data at positive times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04259v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baojun Che, Yifan Chen, Zhenghao Huan, Daniel Zhengyu Huang, Weijie Wang</dc:creator>
    </item>
  </channel>
</rss>
