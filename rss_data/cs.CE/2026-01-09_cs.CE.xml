<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inference in the presence of model-form uncertainties: Leveraging a prediction-oriented approach to improve uncertainty characterization</title>
      <link>https://arxiv.org/abs/2601.04396</link>
      <description>arXiv:2601.04396v1 Announce Type: new 
Abstract: Bayesian inference is a popular approach to calibrating uncertainties, but it can underpredict such uncertainties when model misspecification is present, impacting its reliability to inform decision making. Recently, the statistics and machine learning communities have developed prediction-oriented inference approaches that provide better calibrated uncertainties and adapt to the level of misspecification present. However, these approaches have yet to be demonstrated in the context of complex scientific applications where phenomena of interest are governed by physics-based models. Such settings often involve single realizations of high-dimensional spatio-temporal data and nonlinear, computationally expensive parameter-to-observable maps. This work investigates variational prediction-oriented inference in problems exhibiting these relevant features; namely, we consider a polynomial model and a contaminant transport problem governed by advection-diffusion equations. The prediction-oriented loss is formulated as the log-predictive probability of the calibration data. We study the effects of increasing misspecification and noise, and we assess approximations of the predictive density using Monte Carlo sampling and component-wise kernel density estimation. A novel aspect of this work is applying prediction-oriented inference to the calibration of model-form uncertainty (MFU) representations, which are embedded physics-based modifications to the governing equations that aim to reduce (but rarely eliminate) model misspecification. The computational results demonstrate that prediction-oriented frameworks can provide better uncertainty characterizations in comparison to standard inference while also being amenable to the calibration of MFU representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04396v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebekah White, Rileigh Bandy, Teresa Portone</dc:creator>
    </item>
    <item>
      <title>A Semi-supervised Molecular Learning Framework for Activity Cliff Estimation</title>
      <link>https://arxiv.org/abs/2601.04507</link>
      <description>arXiv:2601.04507v1 Announce Type: new 
Abstract: Machine learning (ML) enables accurate and fast molecular property predictions, which are of interest in drug discovery and material design. Their success is based on the principle of similarity at its heart, assuming that similar molecules exhibit close properties. However, activity cliffs challenge this principle, and their presence leads to a sharp decline in the performance of existing ML algorithms, particularly graph-based methods. To overcome this obstacle under a low-data scenario, we propose a novel semi-supervised learning (SSL) method dubbed SemiMol, which employs predictions on numerous unannotated data as pseudo-signals for subsequent training. Specifically, we introduce an additional instructor model to evaluate the accuracy and trustworthiness of proxy labels because existing pseudo-labeling approaches require probabilistic outputs to reveal the model's confidence and fail to be applied in regression tasks. Moreover, we design a self-adaptive curriculum learning algorithm to progressively move the target model toward hard samples at a controllable pace. Extensive experiments on 30 activity cliff datasets demonstrate that SemiMol significantly enhances graph-based ML architectures and outpasses state-of-the-art pretraining and SSL baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04507v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence 2024</arxiv:journal_reference>
      <dc:creator>Fang Wu</dc:creator>
    </item>
    <item>
      <title>Towards Spatio-Temporal Extrapolation of Phase-Field Simulations with Convolution-Only Neural Networks</title>
      <link>https://arxiv.org/abs/2601.04510</link>
      <description>arXiv:2601.04510v1 Announce Type: new 
Abstract: Phase-field simulations of liquid metal dealloying (LMD) can capture complex microstructural evolutions but can be prohibitively expensive for large domains and long time horizons. In this paper, we introduce a fully convolutional, conditionally parameterized U-Net surrogate designed to extrapolate far beyond its training data in both space and time. The architecture integrates convolutional self-attention, physically informed padding, and a flood-fill corrector method to maintain accuracy under extreme extrapolation, while conditioning on simulation parameters allows for flexible time-step skipping and adaptation to varying alloy compositions. To remove the need for costly solver-based initialization, we couple the surrogate with a conditional diffusion model that generates synthetic, physically consistent initial conditions. We train our surrogate on simulations generated over small domain sizes and short time spans, but, by taking advantage of the convolutional nature of U-Nets, we are able to run and extrapolate surrogate simulations for longer time horizons than what would be achievable with classic numerical solvers. Across multiple alloy compositions, the framework is able to reproduce the LMD physics accurately. It predicts key quantities of interest and spatial statistics with relative errors typically below 5% in the training regime and under 15% during large-scale, long time-horizon extrapolations. Our framework can also deliver speed-ups of up to 36,000 times, bringing the time to run weeks-long simulations down to a few seconds. This work is a first stepping stone towards high-fidelity extrapolation in both space and time of phase-field simulation for LMD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04510v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bonneville, Nathan Bieberdorf, Pieterjan Robbe, Mark Asta, Habib Najm, Laurent Capolungo, Cosmin Safta</dc:creator>
    </item>
    <item>
      <title>Industrial Data-Service-Knowledge Governance: Toward Integrated and Trusted Intelligence for Industry 5.0</title>
      <link>https://arxiv.org/abs/2601.04569</link>
      <description>arXiv:2601.04569v1 Announce Type: new 
Abstract: The convergence of artificial intelligence, cyber-physical systems, and cross-enterprise data ecosystems has propelled industrial intelligence to unprecedented scales. Yet, the absence of a unified trust foundation across data, services, and knowledge layers undermines reliability, accountability, and regulatory compliance in real-world deployments. While existing surveys address isolated aspects, such as data governance, service orchestration, and knowledge representation, none provides a holistic, cross-layer perspective on trustworthiness tailored to industrial settings. To bridge this gap, we present \textsc{Trisk} (TRusted Industrial Data-Service-Knowledge governance), a novel conceptual and taxonomic framework for trustworthy industrial intelligence. Grounded in a five-dimensional trust model (quality, security, privacy, fairness, and explainability), \textsc{Trisk} unifies 120+ representative studies along three orthogonal axes: governance scope (data, service, and knowledge), architectural paradigm (centralized, federated, or edge-embedded), and enabling technology (knowledge graphs, zero-trust policies, causal inference, etc.). We systematically analyze how trust propagates across digital layers, identify critical gaps in semantic interoperability, runtime policy enforcement, and operational/information technologies alignment, and evaluate the maturity of current industrial implementations. Finally, we articulate a forward-looking research agenda for Industry 5.0, advocating for an integrated governance fabric that embeds verifiable trust semantics into every layer of the industrial intelligence stack. This survey serves as both a foundational reference for researchers and a practical roadmap for engineers to deploy trustworthy AI in complex and multi-stakeholder environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04569v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hailiang Zhao, Ziqi Wang, Daojiang Hu, Zhiwei Ling, Wenzhuo Qian, Jiahui Zhai, Yuhao Yang, Zhipeng Gao, Mingyi Liu, Kai Di, Xinkui Zhao, Zhongjie Wang, Jianwei Yin, MengChu Zhou, Shuiguang Deng</dc:creator>
    </item>
    <item>
      <title>MMFCTUB: Multi-Modal Financial Credit Table Understanding Benchmark</title>
      <link>https://arxiv.org/abs/2601.04643</link>
      <description>arXiv:2601.04643v1 Announce Type: new 
Abstract: The advent of multi-modal language models (MLLMs) has spurred research into their application across various table understanding tasks. However, their performance in credit table understanding (CTU) for financial credit review remains largely unexplored due to the following barriers: low data consistency, high annotation costs stemming from domain-specific knowledge and complex calculations, and evaluation paradigm gaps between benchmark and real-world scenarios. To address these challenges, we introduce MMFCTUB (Multi-Modal Financial Credit Table Understanding Benchmark), a practical benchmark, encompassing more than 7,600 high quality CTU samples across 5 table types. MMFCTUB employ a minimally supervised pipeline that adheres to inter-table constraints and maintains data distributions consistency. The benchmark leverages capacity-driven questions and mask-and-recovery strategy to evaluate models' cross-table structure perception, domain knowledge utilization, and numerical calculation capabilities. Utilizing MMFCTUB, we conduct comprehensive evaluations of both proprietary and open-source MLLMs, revealing their strengths and limitations in CTU tasks. MMFCTUB serves as a valuable resource for the research community, facilitating rigorous evaluation of MLLMs in the domain of CTU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04643v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cui Yakun, Yanting Zhang, Zhu Lei, Jian Xie, Zhizhuo Kou, Hang Du, Zhenghao Zhu, Sirui Han</dc:creator>
    </item>
    <item>
      <title>Zoomy: flexible modeling and simulation software for free-surface flows</title>
      <link>https://arxiv.org/abs/2601.04826</link>
      <description>arXiv:2601.04826v1 Announce Type: new 
Abstract: Free-surface flow is relevant to many researchers in water resources engineering, geohazard assessment, as well as coastal and river engineering. Many different free-surface models have been proposed, which span modeling complexity from the hydrostatic Saint-Venant equations to the Reynolds-averaged Navier-Stokes equations. Particularly efficient methods can be derived by depth-averaging, resulting in dimensionally reduced models. Typically, this yields hierarchies of models -- models with a variable system structure depending on the polynomial expansion of the flow variables -- that need to be analyzed and numerically solved.
  This description, analysis, and simulation are challenging, and existing software solutions only cover a specific subset of models generated by these hierarchies. We propose a new software framework to address this issue. Zoomy allows for an efficient description, symbolic analysis, and numerical solution of depth-averaged hierarchies of free-surface flow models. Zoomy handles a numerical discretization in one- and two-dimensional space on unstructured grids.
  With this framework, systematic evaluation of hierarchies of depth-averaged free-surface flows becomes feasible. Additionally, our open-source framework increases the accessibility of these depth-averaged systems to application engineers interested in efficient methods for free-surface flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04826v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ingo Steldermann, Julia Kowalski</dc:creator>
    </item>
    <item>
      <title>MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification</title>
      <link>https://arxiv.org/abs/2601.04264</link>
      <description>arXiv:2601.04264v1 Announce Type: cross 
Abstract: Deep learning models, particularly recurrent neural networks and their variants, such as long short-term memory, have significantly advanced time series data analysis. These models capture complex, sequential patterns in time series, enabling real-time assessments. However, their high computational complexity and large model sizes pose challenges for deployment in resource-constrained environments, such as wearable devices and edge computing platforms. Knowledge Distillation (KD) offers a solution by transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), thereby retaining high performance while reducing computational demands. Current KD methods, originally designed for computer vision tasks, neglect the unique temporal dependencies and memory retention characteristics of time series models. To this end, we propose a novel KD framework termed Memory-Discrepancy Knowledge Distillation (MemKD). MemKD leverages a specialized loss function to capture memory retention discrepancies between the teacher and student models across subsequences within time series data, ensuring that the student model effectively mimics the teacher model's behaviour. This approach facilitates the development of compact, high-performing recurrent neural networks suitable for real-time, time series analysis tasks. Our extensive experiments demonstrate that MemKD significantly outperforms state-of-the-art KD methods. It reduces parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04264v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICASSP49660.2025.10888308</arxiv:DOI>
      <dc:creator>Nilushika Udayangani, Kishor Nandakishor, Marimuthu Palaniswami</dc:creator>
    </item>
    <item>
      <title>Surface-based Molecular Design with Multi-modal Flow Matching</title>
      <link>https://arxiv.org/abs/2601.04506</link>
      <description>arXiv:2601.04506v1 Announce Type: cross 
Abstract: Therapeutic peptides show promise in targeting previously undruggable binding sites, with recent advancements in deep generative models enabling full-atom peptide co-design for specific protein receptors. However, the critical role of molecular surfaces in protein-protein interactions (PPIs) has been underexplored. To bridge this gap, we propose an omni-design peptides generation paradigm, called SurfFlow, a novel surface-based generative algorithm that enables comprehensive co-design of sequence, structure, and surface for peptides. SurfFlow employs a multi-modality conditional flow matching (CFM) architecture to learn distributions of surface geometries and biochemical properties, enhancing peptide binding accuracy. Evaluated on the comprehensive PepMerge benchmark, SurfFlow consistently outperforms full-atom baselines across all metrics. These results highlight the advantages of considering molecular surfaces in de novo peptide discovery and demonstrate the potential of integrating multiple protein modalities for more effective therapeutic peptide discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04506v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>KDD 2025</arxiv:journal_reference>
      <dc:creator>Fang Wu, Zhengyuan Zhou, Shuting Jin, Xiangxiang Zeng, Jure Leskovec, Jinbo Xu</dc:creator>
    </item>
    <item>
      <title>Global Inequalities in Clinical Trials Participation</title>
      <link>https://arxiv.org/abs/2601.04660</link>
      <description>arXiv:2601.04660v1 Announce Type: cross 
Abstract: Clinical trials shape medical evidence and determine who gains access to experimental therapies. Whether participation in these trials reflects the global burden of disease remains unclear. Here we analyze participation inequality across more than 62,000 randomized controlled trials spanning 16 major disease categories from 2000 to 2024. Linking 36.8 million trial participants to country-level disease burden, we show that global inequality in clinical trial participation is overwhelmingly structured by country rather than disease. Country-level factors explain over 90% of variation in participation, whereas disease-specific effects contribute only marginally. Removing entire disease categories, including those traditionally considered underfunded, has little effect on overall inequality. Instead, participation is highly concentrated geographically, with a small group of countries enrolling a disproportionate share of participants across nearly all diseases. These patterns have persisted despite decades of disease-targeted funding and increasing alignment between research attention and disease burden within diseases. Our findings indicate that disease-vertical strategies alone cannot correct participation inequality. Reducing global inequities in clinical research requires horizontal investments in research capacity, health infrastructure, and governance that operate across disease domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04660v1</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wen Lou, Adri\'an A. D\'iaz-Faes, Jiangen He, Zhihao Liu, Vincent Larivi\`ere</dc:creator>
    </item>
    <item>
      <title>Benchmarking Time Series Foundation Models for Short-Term Household Electricity Load Forecasting</title>
      <link>https://arxiv.org/abs/2410.09487</link>
      <description>arXiv:2410.09487v3 Announce Type: replace 
Abstract: Accurate household electricity short-term load forecasting (STLF) is key to future and sustainable energy systems. While various studies have analyzed statistical, machine learning, or deep learning approaches for household electricity STLF, recently proposed time series foundation models such as Chronos, TimesFM or Time-MoE promise a new approach for household electricity STLF. These models are trained on a vast amount of time series data and are able to forecast time series without explicit task-specific training (zero-shot learning). In this study, we benchmark the forecasting capabilities of time series foundation models compared to Trained-from-Scratch (TFS) Transformer-based approaches. Our results suggest that foundation models perform comparably to TFS Transformer models, while certain time series foundation models outperform all TFS models when the input size increases. At the same time, they require less effort, as they need no domain-specific training and only limited contextual data for inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09487v3</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2025.3648056</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, vol. 13, pp. 218141-218153, 2025</arxiv:journal_reference>
      <dc:creator>Marcel Meyer, David Zapata, Sascha Kaltenpoth, Oliver M\"uller</dc:creator>
    </item>
    <item>
      <title>A Paradigm Shift to Assembly-like Finite Element Model Updating</title>
      <link>https://arxiv.org/abs/2502.02592</link>
      <description>arXiv:2502.02592v3 Announce Type: replace 
Abstract: In general, there is a mismatch between a finite element model of a structure and its real behaviour. In aeronautics, this mismatch must be small because finite element models are a fundamental part of the development of an aircraft and of increasing importance with the trend to more flexible wings in modern designs. Finite element model updating can be computationally expensive for complex structures and surrogate models can be employed to reduce the computational burden. A novel approach for finite element model updating, namely assembly-like, is proposed and validated using real experimental data. The assembly-like model updating framework implies that the model is updated as parts are assembled. Benchmarking against the classical global, or one-shot, approach demonstrates that the proposed method is more computationally efficient since it takes 20% fewer iterations to obtain convergence, of which 95% comes from the subassembly models, which have fewer degrees of freedom and so are more computationally efficient. Despite requiring fewer model evaluations, the new approach retains the fidelity, within 1% of a joint natural frequencies and modal shapes index, of the global approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02592v3</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Dessena, Alessandro Pontillo, Dmitry I. Ignatyev, James F. Whidborne, Luca Zanotti Fragonara</dc:creator>
    </item>
    <item>
      <title>Aplicaci\'on Gr\'afica para el estudio de un Modelo de Celda Electrol\'itica usando T\'ecnicas de Visualizaci\'on de Campos Vectoriales</title>
      <link>https://arxiv.org/abs/1001.4002</link>
      <description>arXiv:1001.4002v3 Announce Type: replace-cross 
Abstract: The use of floating bipolar electrodes in copper electro-winning cells represents an emerging technology that promises economic and operational impacts. This thesis presents EWCellCAD, a computational tool designed for the simulation and analysis of these electrochemical systems. Based on the generalization and optimization of an existing 2D finite difference model for calculating electrical variables in rectangular cells, EWCellCAD implements a new 3D model capable of processing complex geometries, not necessarily rectangular, which also accelerates calculations by several orders of magnitude. At the same time, a new analytical method for estimating potentials in floating electrodes is introduced, overcoming the inaccuracies of previous heuristic approaches. The analysis of the results is supported by an interactive visualization technique of three-dimensional vector fields as flow lines.</description>
      <guid isPermaLink="false">oai:arXiv.org:1001.4002v3</guid>
      <category>cs.GR</category>
      <category>cs.CE</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.1.1291.4082</arxiv:DOI>
      <dc:creator>C\'esar Mena</dc:creator>
    </item>
    <item>
      <title>All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection</title>
      <link>https://arxiv.org/abs/2601.04160</link>
      <description>arXiv:2601.04160v2 Announce Type: replace-cross 
Abstract: We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04160v2</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuechen Jiang, Zhiwei Liu, Yupeng Cao, Yueru He, Chen Xu, Ziyang Xu, Zhiyang Deng, Prayag Tiwari, Xi Chen, Alejandro Lopez-Lira, Jimin Huang, Junichi Tsujii, Sophia Ananiadou</dc:creator>
    </item>
  </channel>
</rss>
