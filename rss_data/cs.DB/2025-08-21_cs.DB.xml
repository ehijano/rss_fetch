<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accelerating K-Core Computation in Temporal Graphs</title>
      <link>https://arxiv.org/abs/2508.14147</link>
      <description>arXiv:2508.14147v1 Announce Type: new 
Abstract: We address the problem of enumerating all temporal k-cores given a query time range and a temporal graph, which suffers from poor efficiency and scalability in the state-of-the-art solution. Motivated by an existing concept called core times, we propose a novel algorithm to compute all temporal $k$-cores based on core times and prove that the algorithmic running time is bounded by the size of all resulting temporal k-cores, which is optimal in this scenario. Meanwhile, we show that the cost of computing core times is much lower, which demonstrates the close relevance between our overall running time and the result size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14147v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuo Ma, Dong Wen, Hanchen Wang, Wentao Li, Wenjie Zhang, Xuemin Lin</dc:creator>
    </item>
    <item>
      <title>Efficient Size Constraint Community Search over Heterogeneous Information Networks</title>
      <link>https://arxiv.org/abs/2508.14356</link>
      <description>arXiv:2508.14356v1 Announce Type: new 
Abstract: The goal of community search in heterogeneous information networks (HINs) is to identify a set of closely related target nodes that includes a query target node. In practice, a size constraint is often imposed due to limited resources, which has been overlooked by most existing HIN community search works. In this paper, we introduce the size-bounded community search problem to HIN data. Specifically, we propose a refined (k, P)-truss model to measure community cohesiveness, aiming to identify the most cohesive community of size s that contains the query node. We prove that this problem is NP-hard. To solve this problem, we develop a novel B\&amp;B framework that efficiently generates target node sets of size s. We then tailor novel bounding, branching, total ordering, and candidate reduction optimisations, which enable the framework to efficiently lead to an optimum result. We also design a heuristic algorithm leveraging structural properties of HINs to efficiently obtain a high-quality initial solution, which serves as a global lower bound to further enhance the above optimisations. Building upon these, we propose two exact algorithms that enumerate combinations of edges and nodes, respectively. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14356v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinjian Zhang, Lu Chen, Chengfei Liu, Rui Zhou, Bo Ning</dc:creator>
    </item>
    <item>
      <title>A DBMS-independent approach for capturing provenance polynomials through query rewriting</title>
      <link>https://arxiv.org/abs/2508.14608</link>
      <description>arXiv:2508.14608v1 Announce Type: new 
Abstract: In today's data-driven ecosystems, ensuring data integrity, traceability and accountability is important. Provenance polynomials constitute a powerful formalism for tracing the origin and the derivations made to produce database query results. Despite their theoretical expressiveness, current implementations have limitations in handling aggregations and nested queries, and some of them and tightly coupled to a single Database Management System (DBMS), hindering interoperability and broader applicability.
  This paper presents a query rewriting-based approach for annotating Structured Query Language (SQL) queries with provenance polynomials. The proposed methods are DBMS-independent and support Select-Projection-Join-Union-Aggregation (SPJUA) operations and nested queries, through recursive propagation of provenance annotations. This constitutes the first full implementation of semiring-based theory for provenance polynomials extended with semimodule structures. It also presents an experimental evaluation to assess the validity of the proposed methods and compare the performance against state-of-the-art systems using benchmark data and queries. The results indicate that our solution delivers a comprehensive implementation of the theoretical formalisms proposed in the literature, and demonstrates improved performance and scalability, outperforming existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14608v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paulo Pintor, Rog\'erio Costa, Jos\'e Moreira</dc:creator>
    </item>
    <item>
      <title>Confidence Estimation for Text-to-SQL in Large Language Models</title>
      <link>https://arxiv.org/abs/2508.14056</link>
      <description>arXiv:2508.14056v1 Announce Type: cross 
Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of model-generated SQL queries without having access to gold answers. We study this problem in the context of large language models (LLMs), where access to model weights and gradients is often constrained. We explore both black-box and white-box confidence estimation strategies, evaluating their effectiveness on cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior performance of consistency-based methods among black-box models and the advantage of SQL-syntax-aware approaches for interpreting LLM logits in white-box settings. Furthermore, we show that execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14056v1</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepideh Entezari Maleki, Mohammadreza Pourreza, Davood Rafiei</dc:creator>
    </item>
    <item>
      <title>Auditable Shared Objects: From Registers to Synchronization Primitives</title>
      <link>https://arxiv.org/abs/2508.14506</link>
      <description>arXiv:2508.14506v1 Announce Type: cross 
Abstract: Auditability allows to track operations performed on a shared object, recording who accessed which information. This gives data owners more control on their data. Initially studied in the context of single-writer registers, this work extends the notion of auditability to other shared objects, and studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide an implementation of an auditable $n$-writer $m$-reader read / write register, with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding registers, which have consensus number $m+n$. We show that this consensus number is necessary. The implementation extends naturally to support an auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a primitive that supports efficient implementation of many shared objects. Finally, we relate auditable registers to other access control objects, by implementing an anti-flickering deny list from auditable registers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14506v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>DISC 2025</arxiv:journal_reference>
      <dc:creator>Hagit Attiya, Antonio Fern\'andez Anta, Alessia Milani, Alexandre Rapetti, Corentin Travers</dc:creator>
    </item>
    <item>
      <title>Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)</title>
      <link>https://arxiv.org/abs/2505.15820</link>
      <description>arXiv:2505.15820v4 Announce Type: replace 
Abstract: During football matches, a variety of different parties (e.g., companies) each collect (possibly overlapping) data about the match ranging from basic information (e.g., starting players) to detailed positional data. This data is provided to clubs, federations, and other organizations who are increasingly interested in leveraging this data to inform their decision making. Unfortunately, analyzing such data pose significant barriers because each provider may (1) collect different data, (2) use different specifications even within the same category of data, (3) represent the data differently, and (4) delivers the data in a different manner (e.g., file format, protocol). Consequently, working with these data requires a significant investment of time and money. The goal of this work is to propose a uniform and standardized format for football data called the Common Data Format (CDF). The CDF specifies a minimal schema for five types of match data: match sheet data, video footage, event data, tracking data, and match meta data. It aims to ensure that the provided data is clear, sufficiently contextualized (e.g., its provenance is clear), and complete such that it enables common downstream analysis tasks. Concretely, this paper will detail the technical specifications of the CDF, the representational choices that were made to help ensure the clarity of the provided data, and a concrete approach for delivering data in the CDF. This represents Version 1.0.0 of the CDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15820v4</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Anzer, Kilian Arnsmeyer, Pascal Bauer, Joris Bekkers, Ulf Brefeld, Jesse Davis, Nicolas Evans, Matthias Kempe, Samuel J Robertson, Joshua Wyatt Smith, Jan Van Haaren</dc:creator>
    </item>
    <item>
      <title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2508.02091</link>
      <description>arXiv:2508.02091v2 Announce Type: replace-cross 
Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at https://github.com/deepreinforce-ai/CRINN</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02091v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li</dc:creator>
    </item>
    <item>
      <title>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows</title>
      <link>https://arxiv.org/abs/2508.02866</link>
      <description>arXiv:2508.02866v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) and other foundation models are increasingly used as the core of AI agents. In agentic workflows, these agents plan tasks, interact with humans and peers, and influence scientific outcomes across federated and heterogeneous environments. However, agents can hallucinate or reason incorrectly, propagating errors when one agent's output becomes another's input. Thus, assuring that agents' actions are transparent, traceable, reproducible, and reliable is critical to assess hallucination risks and mitigate their workflow impacts. While provenance techniques have long supported these principles, existing methods fail to capture and relate agent-centric metadata such as prompts, responses, and decisions with the broader workflow context and downstream outcomes. In this paper, we introduce PROV-AGENT, a provenance model that extends W3C PROV and leverages the Model Context Protocol (MCP) and data observability to integrate agent interactions into end-to-end workflow provenance. Our contributions include: (1) a provenance model tailored for agentic workflows, (2) a near real-time, open-source system for capturing agentic provenance, and (3) a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02866v3</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva</dc:creator>
    </item>
  </channel>
</rss>
