<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Jan 2025 02:44:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sectorial Exclusion Criteria in the Marxist Analysis of the Average Rate of Profit: The United States Case (1960-2020)</title>
      <link>https://arxiv.org/abs/2501.06270</link>
      <description>arXiv:2501.06270v1 Announce Type: new 
Abstract: The long-term estimation of the Marxist average rate of profit does not adhere to a theoretically grounded standard regarding which economic activities should or should not be included for such purposes, which is relevant because methodological non-uniformity can be a significant source of overestimation or underestimation, generating a less accurate reflection of the capital accumulation dynamics. This research aims to provide a standard Marxist decision criterion regarding the inclusion and exclusion of economic activities for the calculation of the Marxist average profit rate for the case of United States economic sectors from 1960 to 2020, based on the Marxist definition of productive labor, its location in the circuit of capital, and its relationship with the production of surplus value. Using wavelet-transformed Daubechies filters with increased symmetry, empirical mode decomposition, Hodrick-Prescott filter embedded in unobserved components model, and a wide variety of unit root tests the internal theoretical consistency of the presented criteria is evaluated. Also, the objective consistency of the theory is evaluated by a dynamic factor auto-regressive model, Principal Component Analysis, Singular Value Decomposition and Backward Elimination with Linear and Generalized Linear Models. The results are consistent both theoretically and econometrically with the logic of Marx's political economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06270v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Mauricio Gomez Julian</dc:creator>
    </item>
    <item>
      <title>Endogenous Persistence at the Effective Lower Bound</title>
      <link>https://arxiv.org/abs/2501.06473</link>
      <description>arXiv:2501.06473v1 Announce Type: new 
Abstract: We develop a perfect foresight method to solve models with an interest rate lower bound constraint that nests OccBin/DynareOBC and \cite{Eggertsson2010}'s as well as \cite{Mertens2014}'s pen and paper solutions as special cases. Our method generalizes the pen-and-paper solutions by allowing for endogenous persistence while maintaining tractability and interpretability. We prove that our method necessarily gives stable multipliers. We use it to solve a New Keynesian model with habit formation and government spending, which we match to expectations data from the Great Recession. We find an output multiplier of government spending close to 1 for the US and Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06473v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cai Chunbing, Jordan Roulleau-Pasdeloup, Zheng Zhongxi</dc:creator>
    </item>
    <item>
      <title>Is the Monetary Transmission Mechanism Broken? Time for People's Quantitative Easing</title>
      <link>https://arxiv.org/abs/2501.06575</link>
      <description>arXiv:2501.06575v1 Announce Type: new 
Abstract: The monetary transmission channel is disrupted by many factors, especially securitization and liquidity traps. In our study we try to estimate the effect of securitization on the interest elasticity and to identify if a liquidity trap occurred during 1954Q3-2019Q3. The yield curve inversion mechanism shows us that economic cycles are very sensitive to decreasing profitability of banks. However there is no evidence that restoring their profits will ensure a strong recovery. In this regard, we research the low effect of Quantitative Easing (QE) upon economic growth and analyze whether securitization and liquidity traps posed challenges to QE or is it the mainstream theory flawed. In this regard we will examine the main weaknesses of QE, respectively the speculative behavior induced by artificial low rates and its unequal distribution. We propose a new form of QE that will relief households and not reward banks for their risky behavior before recession.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06575v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Revista Economic\u{a}, 72(3), 2020, 29-43</arxiv:journal_reference>
      <dc:creator>Sebastian Dragoe, Camelia Oprean-Stan</dc:creator>
    </item>
    <item>
      <title>A novel approach to assessing corporate sustainable economic value</title>
      <link>https://arxiv.org/abs/2501.06584</link>
      <description>arXiv:2501.06584v1 Announce Type: new 
Abstract: The goal of this study is to propose a new concept, Sustainable Economic Value, to define it logically, and to build a simplified model for its evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06584v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In: Kaya, M. V. (Editor). Social and Economic Studies within the Framework of Emerging Global Developments Volume 2, 2023, 79 - 90, Peter Lang</arxiv:journal_reference>
      <dc:creator>Camelia Oprean-Stan</dc:creator>
    </item>
    <item>
      <title>Causal Claims in Economics</title>
      <link>https://arxiv.org/abs/2501.06873</link>
      <description>arXiv:2501.06873v1 Announce Type: new 
Abstract: We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a custom language model to construct knowledge graphs that map economic concepts and their relationships. We distinguish between general claims and those documented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document a substantial rise in the share of causal claims-from roughly 4% in 1990 to nearly 28% in 2020-reflecting the growing influence of the "credibility revolution." We find that causal narrative complexity (e.g., the depth of causal chains) strongly predicts both publication in top-5 journals and higher citation counts, whereas non-causal complexity tends to be uncorrelated or negatively associated with these outcomes. Novelty is also pivotal for top-5 publication, but only when grounded in credible causal methods: introducing genuinely new causal edges or paths markedly increases both the likelihood of acceptance at leading outlets and long-run citations, while non-causal novelty exhibits weak or even negative effects. Papers engaging with central, widely recognized concepts tend to attract more citations, highlighting a divergence between factors driving publication success and long-term academic impact. Finally, bridging underexplored concept pairs is rewarded primarily when grounded in causal methods, yet such gap filling exhibits no consistent link with future citations. Overall, our findings suggest that methodological rigor and causal innovation are key drivers of academic recognition, but sustained impact may require balancing novel contributions with conceptual integration into established economic discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06873v1</guid>
      <category>econ.GN</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prashant Garg, Thiemo Fetzer</dc:creator>
    </item>
    <item>
      <title>The Spoils of Algorithmic Collusion: Profit Allocation Among Asymmetric Firms</title>
      <link>https://arxiv.org/abs/2501.07178</link>
      <description>arXiv:2501.07178v1 Announce Type: new 
Abstract: We study the propensity of independent algorithms to collude in repeated Cournot duopoly games. Specifically, we investigate the predictive power of different oligopoly and bargaining solutions regarding the effect of asymmetry between firms. We find that both consumers and firms can benefit from asymmetry. Algorithms produce more competitive outcomes when firms are symmetric, but less when they are very asymmetric. Although the static Nash equilibrium underestimates the effect on total quantity and overestimates the effect on profits, it delivers surprisingly accurate predictions in terms of total welfare. The best description of our results is provided by the equal relative gains solution. In particular, we find algorithms to agree on profits that are on or close to the Pareto frontier for all degrees of asymmetry. Our results suggest that the common belief that symmetric industries are more prone to collusion may no longer hold when algorithms increasingly drive managerial decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07178v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Martin, Hans-Theo Normann, Paul P\"uplichhuisen, Tobias Werner</dc:creator>
    </item>
    <item>
      <title>Anonymous Attention and Abuse</title>
      <link>https://arxiv.org/abs/2501.07410</link>
      <description>arXiv:2501.07410v1 Announce Type: new 
Abstract: We analyze the content of the anonymous online discussion forum Economics Job Market Rumors (EJMR) and document its evolving interactions with external information sources. We focus on three key aspects: the prevalence and impact of links to external domains, the surge in discussions driven by Twitter posts since 2018, and the categorization of individuals whose tweets are most frequently discussed on EJMR. Using data on linked domains, we show how these trends reflect broader changes in the economics profession's digital footprint. Our analysis sheds light on EJMR's informational role but also raises questions about inclusivity and professional ethics in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07410v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Ederer, Paul Goldsmith-Pinkham, Kyle Jensen</dc:creator>
    </item>
    <item>
      <title>Optimizing Supply Chain Networks with the Power of Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2501.06221</link>
      <description>arXiv:2501.06221v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as transformative tools for modeling complex relational data, offering unprecedented capabilities in tasks like forecasting and optimization. This study investigates the application of GNNs to demand forecasting within supply chain networks using the SupplyGraph dataset, a benchmark for graph-based supply chain analysis. By leveraging advanced GNN methodologies, we enhance the accuracy of forecasting models, uncover latent dependencies, and address temporal complexities inherent in supply chain operations. Comparative analyses demonstrate that GNN-based models significantly outperform traditional approaches, including Multilayer Perceptrons (MLPs) and Graph Convolutional Networks (GCNs), particularly in single-node demand forecasting tasks. The integration of graph representation learning with temporal data highlights GNNs' potential to revolutionize predictive capabilities for inventory management, production scheduling, and logistics optimization. This work underscores the pivotal role of forecasting in supply chain management and provides a robust framework for advancing research and applications in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06221v1</guid>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Sheng Chen, Ying-Jung Chen</dc:creator>
    </item>
    <item>
      <title>Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models</title>
      <link>https://arxiv.org/abs/2501.06248</link>
      <description>arXiv:2501.06248v1 Announce Type: cross 
Abstract: Current methods that train large language models (LLMs) with reinforcement learning feedback, often resort to averaging outputs of multiple rewards functions during training. This overlooks crucial aspects of individual reward dimensions and inter-reward dependencies that can lead to sub-optimal outcomes in generations. In this work, we show how linear aggregation of rewards exhibits some vulnerabilities that can lead to undesired properties of generated text. We then propose a transformation of reward functions inspired by economic theory of utility functions (specifically Inada conditions), that enhances sensitivity to low reward values while diminishing sensitivity to already high values. We compare our approach to the existing baseline methods that linearly aggregate rewards and show how the Inada-inspired reward feedback is superior to traditional weighted averaging. We quantitatively and qualitatively analyse the difference in the methods, and see that models trained with Inada-transformations score as more helpful while being less harmful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06248v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto-Rafael Maura-Rivero, Chirag Nagpal, Roma Patel, Francesco Visin</dc:creator>
    </item>
    <item>
      <title>LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural Agents</title>
      <link>https://arxiv.org/abs/2501.06834</link>
      <description>arXiv:2501.06834v1 Announce Type: cross 
Abstract: Despite its importance, studying economic behavior across diverse, non-WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations presents significant challenges. We address this issue by introducing a novel methodology that uses Large Language Models (LLMs) to create synthetic cultural agents (SCAs) representing these populations. We subject these SCAs to classic behavioral experiments, including the dictator and ultimatum games. Our results demonstrate substantial cross-cultural variability in experimental behavior. Notably, for populations with available data, SCAs' behaviors qualitatively resemble those of real human subjects. For unstudied populations, our method can generate novel, testable hypotheses about economic behavior. By integrating AI into experimental economics, this approach offers an effective and ethical method to pilot experiments and refine protocols for hard-to-reach populations. Our study provides a new tool for cross-cultural economic studies and demonstrates how LLMs can help experimental behavioral research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06834v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Augusto Gonzalez-Bonorino (Pomona College Economics Department), Monica Capra (Claremont Graduate University Economics Department, University of Arizona Center for the Philosophy of Freedom), Emilio Pantoja (Pitzer College Economics and Computer Science Department)</dc:creator>
    </item>
    <item>
      <title>Revisiting Group Differences in High-Dimensional Choices: Method and Application to Congressional Speech</title>
      <link>https://arxiv.org/abs/2206.10877</link>
      <description>arXiv:2206.10877v2 Announce Type: replace 
Abstract: Gentzkow, Shapiro and Taddy, Econometrica Vol 87, No 4, 2019 (henceforth GST) use a supervised text-based regression model to assess changes in partisanship in U.S. congressional speech over time. Their estimates imply that partisanship is far greater in recent years than in the past, and that it increased sharply in the early 1990s. The paper at hand provides a replication in the wide sense of GST by complementing their analysis in three ways. First, we propose an alternative unsupervised language model, which combines ideas of topic models and ideal point models, to analyze the change in partisanship over time. We apply this model to the Senate speech data used in GST ranging from 1981-2017. Using our model we replicate their results on the specific evolution of partisanship. Second, our model provides additional insights such as the data-driven estimation of evolvement of topical contents over time. Third, we identify key phrases of partisanship on topic level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.10877v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Hofmarcher, Jan V\'avra, Sourav Adhikari, Bettina Gr\"un</dc:creator>
    </item>
    <item>
      <title>Provincial allocation of China's commercial building operational carbon towards carbon neutrality</title>
      <link>https://arxiv.org/abs/2412.14523</link>
      <description>arXiv:2412.14523v2 Announce Type: replace 
Abstract: National carbon peak track and optimized provincial carbon allocations are crucial for mitigating regional inequality within the commercial building sector during China's transition to carbon neutrality. This study proposes a top-down model to evaluate carbon trajectories in operational commercial buildings up to 2060. Through Monte Carlo simulation, scenario analysis is conducted to assess carbon peak values and the corresponding peaking year, thereby optimizing carbon allocation schemes both nationwide and provincially. The results reveal that (1) the nationwide carbon peak for commercial building operations is projected to reach 890 (+- 50) megatons of carbon dioxide (MtCO2) by 2028 (+- 3.7 years) in the case of the business-as-usual scenario, with a 7.87% probability of achieving the carbon peak under the decarbonization scenario. (2) Significant disparities will exist among provinces, with Shandong's carbon peak projected at 69.6 (+- 4.0) MtCO2 by 2029, approximately 11 times higher than Ningxia's peak of 6.0 (+- 0.3) MtCO2 by 2027. (3) Guided by the principle of maximizing the emission reduction potential, the optimal provincial allocation scheme reveals the top three provinces requiring the most significant reductions in the commercial sector: Xinjiang (5.6 MtCO2), Shandong (4.8 MtCO2), and Henan (4.7 MtCO2). Overall, this study offers optimized provincial carbon allocation strategies within the commercial building sector in China via dynamic scenario simulations, with the goal of hitting the carbon peak target and progressing toward a low-carbon future for the building sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14523v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanqiao Deng, Minda Ma, Nan Zhou, Chenchen Zou, Zhili Ma, Ran Yan, Xin Ma</dc:creator>
    </item>
    <item>
      <title>Optimal Consumption--Investment Problems under Time-Varying Incomplete Preferences</title>
      <link>https://arxiv.org/abs/2312.00266</link>
      <description>arXiv:2312.00266v2 Announce Type: replace-cross 
Abstract: The main objective of this paper is to develop a martingale-type solution to optimal consumption--investment choice problems ([Merton, 1969] and [Merton, 1971]) under time-varying incomplete preferences driven by externalities such as patience, socialization effects, and market volatility. The market is composed of multiple risky assets and multiple consumption goods, while in addition there are multiple fluctuating preference parameters with inexact values connected to imprecise tastes. Utility maximization is a multi-criteria problem with possibly function-valued criteria. To come up with a complete characterization of the solutions, first we motivate and introduce a set-valued stochastic process for the dynamics of multi-utility indices and formulate the optimization problem in a topological vector space. Then, we modify a classical scalarization method allowing for infiniteness and randomness in dimensions and prove results of equivalence to the original problem. Illustrative examples are given to demonstrate practical interests and method applicability progressively. The link between the original problem and a dual problem is also discussed, relatively briefly. Finally, using Malliavin calculus with stochastic geometry, we find optimal investment policies to be generally set-valued, each of whose selectors admits a four-way decomposition involving an additional indecisiveness risk-hedging portfolio. Our results touch on new directions for optimal consumption--investment choices in the presence of incomparability and time inconsistency, also signaling potentially testable assumptions on the variability of asset prices. Simulation techniques for set-valued processes are studied for how solved optimal policies can be computed in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00266v2</guid>
      <category>q-fin.MF</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixuan Xia</dc:creator>
    </item>
    <item>
      <title>Can AI Help with Your Personal Finances?</title>
      <link>https://arxiv.org/abs/2412.19784</link>
      <description>arXiv:2412.19784v4 Announce Type: replace-cross 
Abstract: In recent years, Large Language Models (LLMs) have emerged as a transformative development in artificial intelligence (AI), drawing significant attention from industry and academia. Trained on vast datasets, these sophisticated AI systems exhibit impressive natural language processing and content generation capabilities. This paper explores the potential of LLMs to address key challenges in personal finance, focusing on the United States. We evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini, Anthropic's Claude, and Meta's Llama, to assess their effectiveness in providing accurate financial advice on topics such as mortgages, taxes, loans, and investments. Our findings show that while these models achieve an average accuracy rate of approximately 70%, they also display notable limitations in certain areas. Specifically, LLMs struggle to provide accurate responses for complex financial queries, with performance varying significantly across different topics. Despite these limitations, the analysis reveals notable improvements in newer versions of these models, highlighting their growing utility for individuals and financial advisors. As these AI systems continue to evolve, their potential for advancing AI-driven applications in personal finance becomes increasingly promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19784v4</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00036846.2025.2450384</arxiv:DOI>
      <dc:creator>Oudom Hean, Utsha Saha, Binita Saha</dc:creator>
    </item>
  </channel>
</rss>
