<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Embodied sensorimotor control: computational modeling of the neural control of movement</title>
      <link>https://arxiv.org/abs/2509.14360</link>
      <description>arXiv:2509.14360v1 Announce Type: new 
Abstract: We review how sensorimotor control is dictated by interacting neural populations, optimal feedback mechanisms, and the biomechanics of bodies. First, we outline the distributed anatomical loops that shuttle sensorimotor signals between cortex, subcortical regions, and spinal cord. We then summarize evidence that neural population activity occupies low-dimensional, dynamically evolving manifolds during planning and execution of movements. Next, we summarize literature explaining motor behavior through the lens of optimal control theory, which clarifies the role of internal models and feedback during motor control. Finally, recent studies on embodied sensorimotor control address gaps within each framework by aiming to elucidate neural population activity through the explicit control of musculoskeletal dynamics. We close by discussing open problems and opportunities: multi-tasking and cognitively rich behavior, multi-regional circuit models, and the level of anatomical detail needed in body and network models. Together, this review and recent advances point towards reaching an integrative account of the neural control of movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14360v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Noman Almani, John Lazzari, Jeff Walker, Shreya Saxena</dc:creator>
    </item>
    <item>
      <title>Theoretical Note: The Relation Between Structure and Dynamics in Psychological Networks of Attitudes</title>
      <link>https://arxiv.org/abs/2509.14418</link>
      <description>arXiv:2509.14418v1 Announce Type: new 
Abstract: Two claims of the Causal Attitude Network (CAN) model and the descendent Attitude Entropy framework (AE) are indicative of significant theoretical hurdles facing the psychological network modeling efforts of attitudes. The first claim is that the dynamics of change in an Ising like attitude network, under perturbation of any one single node, can be inferred from the static network attributes of said node. The second claim is that psychological network models of attitudes with Ising like dynamics will maximize both attitudinal consistency and accuracy when within the small world topological regime. The first claim, one with significant application potentials, has not been sufficiently tested; the second claim, one with high theoretical novelty, has never been addressed. Using a set of analytic results and simulations, we found little support for these claims; in short, the predictions are not logically consistent with the theory. Our results have implications beyond attitude models to the larger field of psychological networks (e.g., in clinical psychology) in reference to how we should explain and understand their dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14418v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark G. Orr, Emily S. Teti, Andrei Bura, Henning Mortveit</dc:creator>
    </item>
    <item>
      <title>Mouse vs. AI: A Neuroethological Benchmark for Visual Robustness and Neural Alignment</title>
      <link>https://arxiv.org/abs/2509.14446</link>
      <description>arXiv:2509.14446v1 Announce Type: new 
Abstract: Visual robustness under real-world conditions remains a critical bottleneck for modern reinforcement learning agents. In contrast, biological systems such as mice show remarkable resilience to environmental changes, maintaining stable performance even under degraded visual input with minimal exposure. Inspired by this gap, we propose the Mouse vs. AI: Robust Foraging Competition, a novel bioinspired visual robustness benchmark to test generalization in reinforcement learning (RL) agents trained to navigate a virtual environment toward a visually cued target. Participants train agents to perform a visually guided foraging task in a naturalistic 3D Unity environment and are evaluated on their ability to generalize to unseen, ecologically realistic visual perturbations. What sets this challenge apart is its biological grounding: real mice performed the same task, and participants receive both behavioral performance data and large-scale neural recordings (over 19,000 neurons across visual cortex) for benchmarking. The competition features two tracks: (1) Visual Robustness, assessing generalization across held-out visual perturbations; and (2) Neural Alignment, evaluating how well agents' internal representations predict mouse visual cortical activity via a linear readout. We provide the full Unity environment, a fog-perturbed training condition for validation, baseline proximal policy optimization (PPO) agents, and a rich multimodal dataset. By bridging reinforcement learning, computer vision, and neuroscience through a shared, behaviorally grounded task, this challenge advances the development of robust, generalizable, and biologically inspired AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14446v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marius Schneider, Joe Canzano, Jing Peng, Yuchen Hou, Spencer LaVere Smith, Michael Beyeler</dc:creator>
    </item>
    <item>
      <title>Charting trajectories of human thought using large language models</title>
      <link>https://arxiv.org/abs/2509.14455</link>
      <description>arXiv:2509.14455v1 Announce Type: new 
Abstract: Language provides the most revealing window into the ways humans structure conceptual knowledge within cognitive maps. Harnessing this information has been difficult, given the challenge of reliably mapping words to mental concepts. Artificial Intelligence large language models (LLMs) now offer unprecedented opportunities to revisit this challenge. LLMs represent words and phrases as high-dimensional numerical vectors that encode vast semantic knowledge. To harness this potential for cognitive science, we introduce VECTOR, a computational framework that aligns LLM representations with human cognitive map organisation. VECTOR casts a participant's verbal reports as a geometric trajectory through a cognitive map representation, revealing how thoughts flow from one idea to the next. Applying VECTOR to narratives generated by 1,100 participants, we show these trajectories have cognitively meaningful properties that predict paralinguistic behaviour (response times) and real-world communication patterns. We suggest our approach opens new avenues for understanding how humans dynamically organise and navigate conceptual knowledge in naturalistic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14455v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew M Nour, Daniel C McNamee, Isaac Fradkin, Raymond J Dolan</dc:creator>
    </item>
    <item>
      <title>Network representations reveal structured uncertainty in music</title>
      <link>https://arxiv.org/abs/2509.14053</link>
      <description>arXiv:2509.14053v1 Announce Type: cross 
Abstract: Music, as a structured yet perceptually rich experience, can be modeled as a network to uncover how humans encode and process auditory information. While network-based representations of music are increasingly common, the impact of feature selection on structural properties and cognitive alignment remains underexplored. In this study, we evaluated eight network models, each constructed from symbolic representations of piano compositions using distinct combinations of pitch, octave, duration, and interval, designed to be representative of existing approaches in the literature. By comparing these models through topological metrics, entropy analysis, and divergence with respect to inferred cognitive representations, we assessed both their structural and perceptual efficiency. Our findings reveal that simpler, feature-specific models better match human perception, whereas complex, multidimensional representations introduce cognitive inefficiencies. These results support the view that humans rely on modular, parallel cognitive networks--an architecture consistent with theories of predictive processing and free energy minimization. Moreover, we find that musical networks are structurally organized to guide attention toward transitions that are both uncertain and inferable. The resulting structure concentrates uncertainty in a few frequently visited nodes, creating local entropy gradients that alternate between stable and unpredictable regions, thereby enabling the expressive dynamics of tension and release that define the musical experience. These findings show that network structures make the organization of uncertainty in music observable, offering new insight into how patterned flows of expectation shape perception, and open new directions for studying how musical structures evolve across genres, cultures, and historical periods through the lens of network science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14053v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lluc Bono Rossell\'o, Robert Jankowski, Hugues Bersini, Mari\'an Bogu\~n\'a, M. \'Angeles Serrano</dc:creator>
    </item>
    <item>
      <title>Data coarse graining can improve model performance</title>
      <link>https://arxiv.org/abs/2509.14498</link>
      <description>arXiv:2509.14498v1 Announce Type: cross 
Abstract: Lossy data transformations by definition lose information. Yet, in modern machine learning, methods like data pruning and lossy data augmentation can help improve generalization performance. We study this paradox using a solvable model of high-dimensional, ridge-regularized linear regression under 'data coarse graining.' Inspired by the renormalization group in statistical physics, we analyze coarse-graining schemes that systematically discard features based on their relevance to the learning task. Our results reveal a nonmonotonic dependence of the prediction risk on the degree of coarse graining. A 'high-pass' scheme--which filters out less relevant, lower-signal features--can help models generalize better. By contrast, a 'low-pass' scheme that integrates out more relevant, higher-signal features is purely detrimental. Crucially, using optimal regularization, we demonstrate that this nonmonotonicity is a distinct effect of data coarse graining and not an artifact of double descent. Our framework offers a clear, analytical explanation for why careful data augmentation works: it strips away less relevant degrees of freedom and isolates more predictive signals. Our results highlight a complex, nonmonotonic risk landscape shaped by the structure of the data, and illustrate how ideas from statistical physics provide a principled lens for understanding modern machine learning phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14498v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Nguyen, David J. Schwab, Vudtiwat Ngampruetikorn</dc:creator>
    </item>
    <item>
      <title>Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation</title>
      <link>https://arxiv.org/abs/2509.14827</link>
      <description>arXiv:2509.14827v1 Announce Type: cross 
Abstract: Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14827v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Patrick Madlindl, Fabian Bongratz, Christian Wachinger</dc:creator>
    </item>
    <item>
      <title>Dynamic effects of electric field in hybrid coupling thermosensitive neuronal network</title>
      <link>https://arxiv.org/abs/2509.14910</link>
      <description>arXiv:2509.14910v1 Announce Type: cross 
Abstract: The dynamic of thermosensitive neuronal networks under the influence of external electric fields is explored, focusing on hybrid coupling models that incorporate both electrical and chemical synapses. Numerical simulations reveal a variety of complex behaviors, including coherent, incoherent, and chimera states, which are influenced by the frequency of the external field and the cell size. Specificaly chemical coupling is shown to enhances the appearance of a typical chimera states called traveling chimera states, while the external electric field modulates synchronization based on its frequency. Low-frequency fields induce localized synchronization, whereas high-frequency fields exert minimal influence. The study also investigates the role of intrinsic electric fields, represented by cell size, and their effect on both individual neuron activity and network dynamics. These findings enhance the understanding of how external fields control collective neuronal behavior and open up new possibilities for the modulation of neural activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14910v1</guid>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ediline L. F. Nguessap (Department of Physics, FFCLRP, Sao Paulo, Brazil), Antonio C. Roque (Department of Physics, FFCLRP, Sao Paulo, Brazil), Fernando F. Ferreira (Department of Physics, FFCLRP, Sao Paulo, Brazil)</dc:creator>
    </item>
    <item>
      <title>Synaptic Theory of Chunking in Working Memory</title>
      <link>https://arxiv.org/abs/2408.07637</link>
      <description>arXiv:2408.07637v2 Announce Type: replace 
Abstract: Working memory often appears to exceed its basic span by organizing items into compact representations called chunks. Chunking can be learned over time for familiar inputs; however, it can also arise spontaneously for novel stimuli. Such on-the-fly structuring is crucial for cognition, yet the underlying neural mechanism remains unclear. Here we introduce a synaptic theory of chunking, in which short-term synaptic plasticity enables the formation of chunk representations in working memory. We show that a specialized population of ``chunking neurons'' selectively controls groups of stimulus-responsive neurons, akin to gating. As a result, the network maintains and retrieves the stimuli in chunks, thereby exceeding the basic capacity. Moreover, we show that our model can dynamically construct hierarchical representations within working memory through hierarchical chunking. A consequence of this proposed mechanism is a new limit on the number of items that can be stored and subsequently retrieved from working memory, depending only on the basic working memory capacity when chunking is not invoked. Predictions from our model were confirmed by analyzing single-unit responses in epileptic patients and memory experiments with verbal material. Our work provides a novel conceptual and analytical framework for understanding how the brain organizes information in real time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07637v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CL</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weishun Zhong, Mikhail Katkov, Misha Tsodyks</dc:creator>
    </item>
    <item>
      <title>Controllable Surface Diffusion Generative Model for Neurodevelopmental Trajectories</title>
      <link>https://arxiv.org/abs/2508.03706</link>
      <description>arXiv:2508.03706v2 Announce Type: replace 
Abstract: Preterm birth disrupts the typical trajectory of cortical neurodevelopment, increasing the risk of cognitive and behavioral difficulties. However, outcomes vary widely, posing a significant challenge for early prediction. To address this, individualized simulation offers a promising solution by modeling subject-specific neurodevelopmental trajectories, enabling the identification of subtle deviations from normative patterns that might act as biomarkers of risk. While generative models have shown potential for simulating neurodevelopment, prior approaches often struggle to preserve subject-specific cortical folding patterns or to reproduce region-specific morphological variations. In this paper, we present a novel graph-diffusion network that supports controllable simulation of cortical maturation. Using cortical surface data from the developing Human Connectome Project (dHCP), we demonstrate that the model maintains subject-specific cortical morphology while modeling cortical maturation sufficiently well to fool an independently trained age regression network, achieving a prediction accuracy of $0.85 \pm 0.62$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03706v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenshan Xie, Levente Baljer, M. Jorge Cardoso, Emma Robinson</dc:creator>
    </item>
    <item>
      <title>Radial Basis Function Techniques for Neural Field Models on Surfaces</title>
      <link>https://arxiv.org/abs/2504.13379</link>
      <description>arXiv:2504.13379v2 Announce Type: replace-cross 
Abstract: We present a numerical framework for solving neural field equations on surfaces using Radial Basis Function (RBF) interpolation and quadrature. Neural field models describe the evolution of macroscopic brain activity, but modeling studies often overlook the complex geometry of curved cortical domains. Traditional numerical methods, such as finite element or spectral methods, can be computationally expensive and challenging to implement on irregular domains. In contrast, RBF-based methods provide a flexible alternative by offering interpolation and quadrature schemes that efficiently handle arbitrary geometries with high-order accuracy. We first develop an RBF-based interpolatory projection framework for neural field models on general surfaces. Quadrature for both flat and curved domains are derived in detail, ensuring high-order accuracy and stability as they depend on RBF hyperparameters (basis functions, augmenting polynomials, and stencil size). Through numerical experiments, we demonstrate the convergence of our method, highlighting its advantages over traditional approaches in terms of flexibility and accuracy. We conclude with an exposition of numerical simulations of spatiotemporal activity on complex surfaces, illustrating the method's ability to capture complex wave propagation patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13379v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>nlin.PS</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sage B Shaw, Zachary P Kilpatrick, Daniele Avitabile</dc:creator>
    </item>
  </channel>
</rss>
