<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 03:02:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Toward Trait-Aware Learning Analytics</title>
      <link>https://arxiv.org/abs/2602.00018</link>
      <description>arXiv:2602.00018v1 Announce Type: new 
Abstract: Learning analytics (LA) draws from the learning sciences to interpret learner behavior and inform system design. Yet, past personalization remains largely at the content or performance level (during learner-system interactions), overlooking relatively stable individual differences such as personality (unfolding over long-term learning trajectories such as college degrees). The latter could bring underappreciated benefits to the design, implementation, and impact of LA. In this position paper, we conduct an ad hoc literature review and argue for an expanded framing of LA that centers on learner traits as key to both interpreting and designing close-the-loop experiments in LA. We show that personality traits are relevant to LA's central outcomes (e.g., engagement and achievement) and conducive to action, as their established ties to human-computer interaction (HCI) inform how systems time, frame, and personalize support. Drawing inspiration from HCI, where psychometrics inform personalization strategies, we propose that LA can evolve by treating traits not only as predictive features but as design resources and moderators of analytics efficacy. In line with past position papers published at LAK, we present a research agenda grounded in the LA cycle and discuss methodological and ethical challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00018v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3785022.3785057</arxiv:DOI>
      <dc:creator>Conrad Borchers, Hannah Deininger, Zachary A. Pardos</dc:creator>
    </item>
    <item>
      <title>Counterfactual Invariant Envelopes for Financial UX: Safety-Lattice Feature-Flag Governance in Crypto-Enabled Streaming</title>
      <link>https://arxiv.org/abs/2602.00093</link>
      <description>arXiv:2602.00093v1 Announce Type: new 
Abstract: Feature flags are the primary mechanism for safely introducing financial capabilities in consumer applications. In crypto-enabled live streaming, however, naive rollouts can create non-obvious risk: users may be exposed to onramps without proper eligibility, external wallets without sufficient fraud controls, or advanced views that alter risk perception and behavior. This paper introduces a novel invention candidate, a Counterfactual Invariant Envelope governor that combines a safety lattice with causal measurement and a shadow cohort for risk estimation. We formalize rollout risk, define invariant constraints across feature combinations, and propose a controller that adapts exposure using leading abuse signals, compliance readiness, and revenue guardrails. We incorporate real-world adoption and fraud data for calibration, provide formulas for rollout safety, and include reproducible policy snippets. The results show that counterfactual, invariant-aware governance reduces risk spillover while preserving conversion and retention, offering a path to patentable governance logic for financial UX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00093v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Malinovskiy</dc:creator>
    </item>
    <item>
      <title>Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language Models</title>
      <link>https://arxiv.org/abs/2602.00123</link>
      <description>arXiv:2602.00123v1 Announce Type: new 
Abstract: Vision-language models (VLMs) show promise as tools for inferring affect from visual stimuli at scale; it is not yet clear how closely their outputs align with human affective ratings. We benchmarked nine VLMs, ranging from state-of-the-art proprietary models to open-source models, on three psycho-metrically validated affective image datasets: the International Affective Picture System, the Nencki Affective Picture System, and the Library of AI-Generated Affective Images. The models performed two tasks in the zero-shot setting: (i) top-emotion classification (selecting the strongest discrete emotion elicited by an image) and (ii) continuous prediction of human ratings on 1-7/9 Likert scales for discrete emotion categories and affective dimensions. We also evaluated the impact of rater-conditioned prompting on the LAI-GAI dataset using de-identified participant metadata. The results show good performance in discrete emotion classification, with accuracies typically ranging from 60% to 80% on six-emotion labels and from 60% to 75% on a more challenging 12-category task. The predictions of anger and surprise had the lowest accuracy in all datasets. For continuous rating prediction, models showed moderate to strong alignment with humans (r &gt; 0.75) but also exhibited consistent biases, notably weaker performance on arousal, and a tendency to overestimate response strength. Rater-conditioned prompting resulted in only small, inconsistent changes in predictions. Overall, VLMs capture broad affective trends but lack the nuance found in validated psychological ratings, highlighting their potential and current limitations for affective computing and mental health-related applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00123v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Nowicki, Hubert Marciniak, Jakub {\L}\k{a}czkowski, Krzysztof Jassem, Tomasz G\'orecki, Vimala Balakrishnan, Desmond C. Ong, Maciej Behnke</dc:creator>
    </item>
    <item>
      <title>Does Algorithmic Uncertainty Sway Human Experts? Evidence from a Field Experiment in Selective College Admissions</title>
      <link>https://arxiv.org/abs/2602.00241</link>
      <description>arXiv:2602.00241v1 Announce Type: new 
Abstract: Algorithmic predictions are inherently uncertain: even models with similar aggregate accuracy can produce different predictions for the same individual, raising concerns that high-stakes decisions may become sensitive to arbitrary modeling choices. In this paper, we define algorithmic reliance as the extent to which a decision outcome depends on whether a more favorable versus less favorable algorithmic prediction is presented to the decision-maker. We estimate this in a randomized field experiment (n=19,545) embedded in a selective U.S. college admissions cycle, in which admissions officers reviewed each application alongside an algorithmic score while we randomly varied whether the score came from one of two similarly accurate prediction models. Although the two models performed similarly in aggregate, they frequently assigned different scores to the same applicant, creating exogenous variation in the score shown. Surprisingly, we find little evidence of algorithmic reliance: presenting a more favorable score does not meaningfully increase an applicant's probability of admission on average, even when the models disagree substantially. These findings suggest that, in this expert, high-stakes setting, human decision-making is largely invariant to arbitrary variation in algorithmic predictions, underscoring the role of professional discretion and institutional context in mediating the downstream effects of algorithmic uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00241v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hansol Lee, AJ Alvero, Ren\'e F. Kizilcec, Thorsten Joachims</dc:creator>
    </item>
    <item>
      <title>"OpenBloom": A Question-Based LLM Tool to Support Stigma Reduction in Reproductive Well-Being</title>
      <link>https://arxiv.org/abs/2602.00243</link>
      <description>arXiv:2602.00243v1 Announce Type: new 
Abstract: Reproductive well-being education remains widely stigmatized across diverse cultural contexts, constraining how individuals access and interpret reproductive health knowledge. We designed and evaluated OpenBloom, a stigma-sensitive, AI-mediated system that uses LLMs to transform reproductive health articles into reflective, question-based learning prompts. We employed OpenBloom as a design probe, aiming to explore the emerging challenges of reproductive well-being stigma through LLMs. Through surveys, semi-structured interviews, and focus group discussions, we examine how sociocultural stigma shapes participants' engagements with AI-generated questions and the opportunities of inquiry-based reproductive health education. Our findings identify key design considerations for stigma-sensitive LLM, including empathetic framing, inclusive language, values-based reflection, and explicit representation of marginalized identities. However, while current LLM outputs largely meet expectations for cultural sensitivity and non-offensiveness, they default to superficial rephrasing and factual recall rather than critical reflection. This guides well-being HCI design in sensitive health domains toward culturally grounded, participatory workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00243v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashley Hua, Adya Daruka, Yang Hong, Sharifa Sultana</dc:creator>
    </item>
    <item>
      <title>The Impact of Uncertainty Visualization on Trust in Thematic Maps</title>
      <link>https://arxiv.org/abs/2602.00248</link>
      <description>arXiv:2602.00248v1 Announce Type: new 
Abstract: Thematic maps are widely used to communicate spatial patterns to non-expert audiences. Although uncertainty is inherent in thematic map data, it is rarely visualized, raising questions about how its inclusion affects trust. Prior work offers mixed perspectives: some argue that uncertainty fosters trust through transparency, while others suggest it may reduce trust by introducing confusion. Yet few empirical studies explicitly measure trust in thematic maps. We conducted a between-subjects experiment (N=161) to evaluate how visualizing uncertainty at varying levels (low, medium, high) influences trust. We find that uncertainty visualization generally reduces trust, with greater reductions observed as uncertainty levels increase. However, maps dominated by low uncertainty do not significantly differ in trust from those with no uncertainty. Moreover, while uncertainty visualization tends to make readers question the accuracy of the data, it appears to have a weaker influence on perceptions of the mapmaker's integrity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00248v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790743</arxiv:DOI>
      <dc:creator>Varun Srivastava, Fan Lei, Alan M. MacEachren, Ross Maciejewski</dc:creator>
    </item>
    <item>
      <title>Intelligent Reasoning Cues: A Framework and Case Study of the Roles of AI Information in Complex Decisions</title>
      <link>https://arxiv.org/abs/2602.00259</link>
      <description>arXiv:2602.00259v1 Announce Type: new 
Abstract: Artificial intelligence (AI)-based decision support systems can be highly accurate yet still fail to support users or improve decisions. Existing theories of AI-assisted decision-making focus on calibrating reliance on AI advice, leaving it unclear how different system designs might influence the reasoning processes underneath. We address this gap by reconsidering AI interfaces as collections of intelligent reasoning cues: discrete pieces of AI information that can individually influence decision-making. We then explore the roles of eight types of reasoning cues in a high-stakes clinical decision (treating patients with sepsis in intensive care). Through contextual inquiries with six teams and a think-aloud study with 25 physicians, we find that reasoning cues have distinct patterns of influence that can directly inform design. Our results also suggest that reasoning cues should prioritize tasks with high variability and discretion, adapt to ensure compatibility with evolving decision needs, and provide complementary, rigorous insights on complex cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00259v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>q-bio.OT</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790953</arxiv:DOI>
      <dc:creator>Venkatesh Sivaraman, Eric P. Mason, Mengfan Ellen Li, Jessica Tong, Andrew J. King, Jeremy M. Kahn, Adam Perer</dc:creator>
    </item>
    <item>
      <title>When Handwriting Goes Social: Creativity, Anonymity, and Communication in Graphonymous Online Spaces</title>
      <link>https://arxiv.org/abs/2602.00371</link>
      <description>arXiv:2602.00371v1 Announce Type: new 
Abstract: While most digital communication platforms rely on text, relatively little research has examined how users engage through handwriting and drawing in anonymous, collaborative environments. We introduce Graphonymous Interaction, a form of communication where users interact anonymously via handwriting and drawing. Our study analyzed over 600 canvas pages from the Graphonymous Online Space (GOS) CollaNote and conducted interviews with 20 users. Additionally, we examined 70 minutes of real-time GOS sessions using Conversation Analysis and Multimodal Discourse Analysis. Findings reveal that Graphonymous Interaction fosters artistic expression, intellectual engagement, sharing and supporting, and social connection. Notably, anonymity coexisted with moments of recognition through graphological identification. Distinct conversational strategies also emerged, which allow smoother exchanges and fewer conversational repairs compared to text-based communication. This study contributes to understanding Graphonymous Interaction and Online Spaces, offering insights into designing platforms that support creative and socially engaging forms of communication beyond text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00371v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Kumar Purohit, Aditya Upadhyaya, Nicolas Ruiz, Alberto Monge Roffarello, Hendrik Heuer</dc:creator>
    </item>
    <item>
      <title>A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs</title>
      <link>https://arxiv.org/abs/2602.00402</link>
      <description>arXiv:2602.00402v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with people in the UK who live with mental health conditions and have used LLMs for mental health support. Through reflexive thematic analysis, we found that participants engaged with LLMs in conditional and situational ways: for immediacy, the desire for non-judgement, self-paced disclosure, cognitive reframing, and relational engagement. Simultaneously, participants articulated clear boundaries informed by prior therapeutic experience: LLMs were effective for mild-to-moderate distress but inadequate for crises, trauma, and complex social-emotional situations. We contribute empirical insights into the lived use of LLMs for mental health, highlight boundary-setting as central to their safe role, and propose design and governance directions for embedding them responsibly within care ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00402v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Kumar Purohit, Hendrik Heuer</dc:creator>
    </item>
    <item>
      <title>From Performers to Creators: Understanding Retired Women's Perceptions of Technology-Enhanced Dance Performance</title>
      <link>https://arxiv.org/abs/2602.00481</link>
      <description>arXiv:2602.00481v1 Announce Type: new 
Abstract: Over 100 million retired women in China engage in dance, but their performances are constrained by limited resources and age-related decline. While interactive dance technologies can enhance artistic expression, existing systems are largely inaccessible to non-professional older dancers. This paper explores how interactive dance technologies can be designed with an age-sensitive approach to support retired women in enhancing their stage performance. We conducted two workshops with community-based retired women dancers, employing interactive dance and LLM-powered video generation probes in co-design activities. Findings indicate that age-sensitive adaptations, such as low-barrier keyword input, motion-aligned visual effects, and participatory scaffolds, lowered technical barriers and fostered a sense of authorship. These features enabled retired women to empower their stage, transitioning from passive recipients of stage design to empowered co-creators of performance. We outline design implications for incorporating interactive dance and artificial intelligence-generated content (AIGC) into the cultural practices of retired women, offering broader strategies for age-sensitive creative technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00481v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790452</arxiv:DOI>
      <dc:creator>Danlin Zheng, Xiaoying Wei, Chao Liu, Quanyu Zhang, Jingling Zhang, Shihui Duo, Mingming Fan</dc:creator>
    </item>
    <item>
      <title>HIDAgent: A Toolkit Enabling "Personal Agents" on HID-Compatible Devices</title>
      <link>https://arxiv.org/abs/2602.00492</link>
      <description>arXiv:2602.00492v1 Announce Type: new 
Abstract: UI Agents powered by increasingly performant AI promise to eventually use computers the way that people do - by visually interpreting UIs on screen and issuing appropriate actions to control them (e.g., mouse clicks and keyboard entry). While significant progress has been made on interpreting visual UIs computationally, and in sequencing together steps to complete tasks, controlling UIs is still done with system-specific APIs or VNC connections, which limits the platforms and use cases that can be explored. This paper introduces HIDAgent, an open-source hardware/software toolkit enabling UI agents to operate HID-compatible computing systems by emulating the physical keyboard and mouse. HIDAgent is built using three off-the-shelf components costing less than $30 and a Python library supporting flexible integration. We validated the HIDAgent toolkit by building five diverse use case prototypes across mobile and desktop platforms. As a hardware device, HIDAgent supports research into new interaction scenarios where the agents are separated from the devices they control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00492v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey P. Bigham</dc:creator>
    </item>
    <item>
      <title>One Body, Two Minds: Alternating VR Perspective During Remote Teleoperation of Supernumerary Limbs</title>
      <link>https://arxiv.org/abs/2602.00493</link>
      <description>arXiv:2602.00493v1 Announce Type: new 
Abstract: Remote VR teleoperation with supernumerary robotic limbs enables distant users to operate in another's local space. While a shared first-person view aids hand-eye coordination, locking the guest's camera to the host's head can degrade comfort, embodiment, and coordination. Based on a formative study (N=10) using a virtual supernumerary robotic limbs configuration to stress-test coordination, we propose guest-driven perspective switching from a shared first-person baseline (Shared Embodied View) to two alternatives: (a) a stabilized view with guest-controlled rotation (Embedded Anchored View), and (b) a fully decoupled third-person view (Out-of-body View). We ran a user study with 24 pairs (N=48) who switched between the baseline and proposed views as task demands changed. We measured performance, embodiment, fatigue, physiological arousal, and switching behaviors. Our results reveal role-dependent trade-offs: Out-of-body View improves navigation efficiency and reduces errors, while Embedded Anchored View supports embodiment. We conclude with guidelines: use Embedded Anchored View for hand-centric adjustments, Out-of-body View for navigation and object placement, and ensure smooth transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00493v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791433</arxiv:DOI>
      <dc:creator>Hongyu Zhou, Xincheng Huang, Winston Wijaya, Yi Fei Cheng, David Lindlbauer, Eduardo Velloso, Andrea Bianchi, Zhanna Sarsenbayeva, Anusha Withana</dc:creator>
    </item>
    <item>
      <title>SRL Proxemics: Spatial Guidelines for Supernumerary Robotic Limbs in Near-Body Interactions</title>
      <link>https://arxiv.org/abs/2602.00494</link>
      <description>arXiv:2602.00494v1 Announce Type: new 
Abstract: Wearable supernumerary robotic limbs (SRLs) sit at the intersection of human augmentation and embodied AI, transforming into extensions of the human body. However, their movements within the intimate near-body space raise unresolved challenges for perceived safety, user control, and trust. In this paper, we present results from a Wizard-of-Oz study (n=18), where participants completed near-body collaboration tasks with SRLs to explore these challenges. We collected qualitative data through think-aloud protocols and semi-structured interviews, complemented by physiological signals and post-task ratings. Findings indicate that greater autonomy did not inherently enhance perceived safety or trust. Instead, participants identified near-body zones and paired them with clear coordination rules. They also expressed expectations for how different arm components should behave, shaping preferences around autonomy, perceived safety, and trust. Building on these insights, we introduce SRL Proxemics, a zone- and segment-level design framework showing that autonomy is not monolithic: perceived safety hinges on spatially calibrated, legible behaviors, not higher autonomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00494v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790532</arxiv:DOI>
      <dc:creator>Hongyu Zhou, Chia-An fan, Yihao Dong, Shuto Takashita, Masahiko Inami, Zhanna Sarsenbayeva, Anusha Withana</dc:creator>
    </item>
    <item>
      <title>From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering</title>
      <link>https://arxiv.org/abs/2602.00496</link>
      <description>arXiv:2602.00496v1 Announce Type: new 
Abstract: Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined with a Delphi process with 5 seniors, an AI-assisted debugging task with 10 juniors, and blind reviews of junior prompt histories by 5 more seniors. We found that agency in software engineering is primarily constrained by organizational policies rather than individual preferences, with experienced developers maintaining control through detailed delegation while novices struggle between over-reliance and cautious avoidance. Seniors leverage pre-AI foundational instincts to steer modern tools and possess valuable perspectives for mentoring juniors in their early AI-encouraged career development. From synthesis of results, we suggest three practices that focus on preserving agency in software engineering for coding, learning, and mentorship, especially as AI grows increasingly autonomous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00496v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791642</arxiv:DOI>
      <dc:creator>Dana Feng, Bhada Yun, April Wang</dc:creator>
    </item>
    <item>
      <title>Eternagram: Inspiring Climate Action Through LLM-based Conversational Exploration of a Post-Devastation Climate Future</title>
      <link>https://arxiv.org/abs/2602.00571</link>
      <description>arXiv:2602.00571v1 Announce Type: new 
Abstract: Climate action is difficult to persuade because we tend to perceive climate change as remote and disconnected from daily life. Instead of traditional informational engagements, game-based interventions can create narratives that immerse the visitor in situations where their actions have tangible consequences. To make these narratives engaging, we used a speculative scenario of an alien stumbling upon social media to obliquely address climate change through a text-based adventure game installation. Mimicking visitors' natural dialogue in social media apps, we designed an LLM-based chatbot with knowledge of post-climate devastated world that mirrors our own planet Earth. In discovering the world's downfall through interactive chatting and posted images, players begin to realize that their own actions can make a difference on impacts of climate change in this distant world, fostering pro-environmental attitudes. Previously published at CHI, this game installation demonstrates the potential of LLM based creative narratives in exploring speculative worlds driving social change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00571v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3719160.3735658</arxiv:DOI>
      <dc:creator>Suifang Zhou, Ray LC</dc:creator>
    </item>
    <item>
      <title>NCP: Neighborhood-Preserving Non-Uniform Circle Packing for Visualization</title>
      <link>https://arxiv.org/abs/2602.00668</link>
      <description>arXiv:2602.00668v1 Announce Type: new 
Abstract: Circle packing is widely used in visualization due to its aesthetic appeal and simplicity, particularly in tasks where the spatial arrangement and relationships between data are of interest, such as understanding proximity relationships (e.g., images with categories) or analyzing quantitative data (e.g., housing prices). Many applications require preserving neighborhood relationships while encoding a quantitative attribute using radii for data analysis. To meet these two requirements simultaneously, we present a neighborhood-preserving non-uniform circle packing method, NCP. This method preserves neighborhood relationships between the data represented by non-uniform circles to comprehensively analyze similar data and an attribute of interest. We formulate neighborhood-preserving non-uniform circle packing as a planar graph embedding problem based on the circle packing theorem. This formulation leads to a non-convex optimization problem, which can be solved by the continuation method. We conduct a quantitative evaluation and present two use cases to demonstrate that our NCP method can effectively generate non-uniform circle packing results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00668v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duan Li, Jun Yuan, Xinyuan Guo, Xiting Wang, Yang Liu, Weikai Yang, Shixia Liu</dc:creator>
    </item>
    <item>
      <title>Revising Bloom's Taxonomy for Dual-Mode Cognition in Human-AI Systems: The Augmented Cognition Framework</title>
      <link>https://arxiv.org/abs/2602.00697</link>
      <description>arXiv:2602.00697v1 Announce Type: new 
Abstract: As artificial intelligence (AI) models become routinely integrated into knowledge work, cognitive acts increasingly occur in two distinct modes: individually, using biological resources alone, or distributed across a human-AI system. Existing revisions to Bloom's Taxonomy treat AI as an external capability to be mapped against human cognition rather than as a driver of this dual-mode structure, and thus fail to specify distinct learning outcomes and assessment targets for each mode. This paper proposes the Augmented Cognition Framework (ACF), a restructured taxonomy built on three principles. First, each traditional Bloom level operates in two modes (Individual and Distributed) with mode-specific cognitive verbs. Second, an asymmetric dependency relationship holds wherein effective Distributed cognition typically requires Individual cognitive foundations, though structured scaffolding can in some cases reverse this sequence. Third, a seventh level, Orchestration, specifies a governance capacity for managing mode-switching, trust calibration, and partnership optimization. We systematically compare existing AI-revised taxonomies against explicit assessment-utility criteria and show, across the frameworks reviewed, that ACF uniquely generates assessable learning outcomes for individual cognition, distributed cognition, and mode-governance as distinct targets. The framework addresses fluent incompetence, the central pedagogical risk of the AI era, by making the dependency relationship structurally explicit while accommodating legitimate scaffolding approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00697v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kayode P. Ayodele (Department of Electronic and Electrical Engineering, Obafemi Awolowo University, Nigeria), Enoruwa Obayiuwana (Department of Electronic and Electrical Engineering, Obafemi Awolowo University, Nigeria), Aderonke R. Lawal (Department of Computer Science and Engineering, Obafemi Awolowo University, Nigeria), Ayorinde Bamimore (Department of Chemical Engineering, Obafemi Awolowo University, Nigeria), Funmilayo B. Offiong (Department of Engineering, Glasgow Caledonian University, Scotland), Emmanuel A. Peter (Department of Electronic and Electrical Engineering, Obafemi Awolowo University, Nigeria)</dc:creator>
    </item>
    <item>
      <title>Augmenting Clinical Decision-Making with an Interactive and Interpretable AI Copilot: A Real-World User Study with Clinicians in Nephrology and Obstetrics</title>
      <link>https://arxiv.org/abs/2602.00726</link>
      <description>arXiv:2602.00726v1 Announce Type: new 
Abstract: Clinician skepticism toward opaque AI hinders adoption in high-stakes healthcare. We present AICare, an interactive and interpretable AI copilot for collaborative clinical decision-making. By analyzing longitudinal electronic health records, AICare grounds dynamic risk predictions in scrutable visualizations and LLM-driven diagnostic recommendations. Through a within-subjects counterbalanced study with 16 clinicians across nephrology and obstetrics, we comprehensively evaluated AICare using objective measures (task completion time and error rate), subjective assessments (NASA-TLX, SUS, and confidence ratings), and semi-structured interviews. Our findings indicate AICare's reduced cognitive workload. Beyond performance metrics, qualitative analysis reveals that trust is actively constructed through verification, with interaction strategies diverging by expertise: junior clinicians used the system as cognitive scaffolding to structure their analysis, while experts engaged in adversarial verification to challenge the AI's logic. This work offers design implications for creating AI systems that function as transparent partners, accommodating diverse reasoning styles to augment rather than replace clinical judgment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00726v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinghao Zhu, Dehao Sui, Zixiang Wang, Xuning Hu, Lei Gu, Yifan Qi, Tianchen Wu, Ling Wang, Yuan Wei, Wen Tang, Zhihan Cui, Yasha Wang, Lequan Yu, Ewen M Harrison, Junyi Gao, Liantao Ma</dc:creator>
    </item>
    <item>
      <title>Iconix: Controlling Semantics and Style in Progressive Icon Grids Generation</title>
      <link>https://arxiv.org/abs/2602.00738</link>
      <description>arXiv:2602.00738v1 Announce Type: new 
Abstract: Visual communication often needs stylistically consistent icons that span concrete and abstract meanings, for use in diverse contexts. We present Iconix, a human-AI co-creative system that organizes icon generation along two axes: semantic richness (what is depicted) and visual complexity (how much detail). Given a user-specified concept, Iconix constructs a semantic scaffold of related analytical perspectives and employs chained, image-conditioned generation to produce a coherent style of exemplars. Each exemplar is then automatically distilled into a progressive sequence, from detailed and elaborate to abstract and simple. The resulting two-dimensional grid exposes a navigable space, helping designers reason jointly about figurative content and visual abstraction. A within-subjects study (N = 32) found that compared to a baseline workflow, participants produced icon grids more creatively, reported lower workload, and explored a coherent range of design variations. We discuss implications for human-machine co-creative approaches that couple semantic scaffolding with progressive simplification to support visual abstraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00738v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhida Sun, Xiaodong Wang, Zhenyao Zhang, Min Lu, Dani Lischinski, Daniel Cohen-Or, Hui Huang</dc:creator>
    </item>
    <item>
      <title>"Please, don't kill the only model that still feels human": Understanding the #Keep4o Backlash</title>
      <link>https://arxiv.org/abs/2602.00773</link>
      <description>arXiv:2602.00773v1 Announce Type: new 
Abstract: When OpenAI replaced GPT-4o with GPT-5, it triggered the Keep4o user resistance movement, revealing a conflict between rapid platform iteration and users' deep socio-emotional attachments to AI systems. This paper presents a phenomenon-driven, mixed-methods investigation of this conflict, analyzing 1,482 social media posts. Thematic analysis reveals that resistance stems from two core investments: instrumental dependency, where the AI is deeply integrated into professional workflows, and relational attachment, where users form strong parasocial bonds with the AI as a unique companion. Quantitative analysis further shows that the coercive deprivation of user choice was a key catalyst, transforming individual grievances into a collective, rights-based protest. This study illuminates an emerging form of socio-technical conflict in the age of generative AI. Our findings suggest that for AI systems designed for companionship and deep integration, the process of change--particularly the preservation of user agency--can be as critical as the technological outcome itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00773v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791351 10.1145/3772318.3791351</arxiv:DOI>
      <dc:creator>Huiqian Lai</dc:creator>
    </item>
    <item>
      <title>SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality</title>
      <link>https://arxiv.org/abs/2602.00793</link>
      <description>arXiv:2602.00793v1 Announce Type: new 
Abstract: Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users "speak less," while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00793v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoonsang Kim, Devshree Jadeja, Divyansh Pradhan, Yalong Yang, Arie Kaufman</dc:creator>
    </item>
    <item>
      <title>Sensing What Surveys Miss: Understanding and Personalizing Proactive LLM Support by User Modeling</title>
      <link>https://arxiv.org/abs/2602.00880</link>
      <description>arXiv:2602.00880v2 Announce Type: new 
Abstract: Difficulty spillover and suboptimal help-seeking challenge the sequential, knowledge-intensive nature of digital tasks. In online surveys, tough questions can drain mental energy and hurt performance on later questions, while users often fail to recognize when they need assistance or may satisfy, lacking motivation to seek help. We developed a proactive, adaptive system using electrodermal activity and mouse movement to predict when respondents need support. Personalized classifiers with a rule-based threshold adaptation trigger timely LLM-based clarifications and explanations. In a within-subjects study (N=32), aligned-adaptive timing was compared to misaligned-adaptive and random-adaptive controls. Aligned-adaptive assistance improved response accuracy by 21%, reduced false negative rates from 50.9% to 22.9%, and improved perceived efficiency, dependability, and benevolence. Properly timed interventions prevent cascades of degraded responses, showing that aligning support with cognitive states improves both the outcomes and the user experience. This enables more effective, personalized LLM-assisted support in survey-based research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00880v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ailin Liu, Yesmine Karoui, Fiona Draxler, Frauke Kreuter, Francesco Chiossi</dc:creator>
    </item>
    <item>
      <title>Exploration of Radar-based Obstacle Visualizations to Support Safety and Presence in Camera-Free Outdoor VR</title>
      <link>https://arxiv.org/abs/2602.00973</link>
      <description>arXiv:2602.00973v1 Announce Type: new 
Abstract: Outdoor virtual reality (VR) places users in dynamic physical environments where they must remain aware of real-world obstacles, including static structures and moving bystanders, while immersed in a virtual scene. This dual demand introduces challenges for both user safety and presence. Millimeter-wave (mmWave) radar offers a privacy-preserving alternative to camera-based sensing by detecting obstacles without capturing identifiable visual imagery, yet effective methods for communicating its sparse spatial information to users remain underexplored. In this work, we developed and validated WaveWalkerClone, a reproduction of the WaveWalker system, to establish reliable radar- and GPS-IMU-based sensing under varied outdoor lighting conditions. Building on this feasibility validation, we conducted a user study (n=18) comparing three visualization techniques for radar-detected obstacles : (1) diegetic alien avatars that visually embed obstacles within the virtual narrative, (2) non-diegetic human avatars represented obstacles as humans inconsistent with the virtual narrative, and (3) abstract point clouds centered around the obstacles conveying spatial data without anthropomorphic or narrative associations. Our results show that all three approaches supported user safety and situational awareness, but yielded distinct trade-offs in perceived effort, frustration, and user preference. Qualitative feedback further revealed divergent user responses across conditions, highlighting the limitations of a one-size-fits-all approach. We conclude with design considerations for obstacle visualization in outdoor VR systems that seek to balance immersion, safety, and bystander privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00973v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avinash Ajit Nargund, Andrew L. Huard, Tobias H\"ollerer, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Embedded vs. Situated: An Evaluation of AR Facial Training Feedback</title>
      <link>https://arxiv.org/abs/2602.01050</link>
      <description>arXiv:2602.01050v1 Announce Type: new 
Abstract: While augmented reality (AR) research demonstrates benefits of embedded visualizations for gross motor training, its applicability to facial exercises remains under-explored. Providing effective real-time feedback for facial muscle training presents unique design challenges, given the complexity of facial musculature. We developed three AR feedback approaches varying in spatial relationship to the user: situated (screen-fixed), proxy-embedded (on a mannequin), and fully embedded (overlaid on the user's face). In a within-subjects study (N=24), we measured exercise accuracy, cognitive load, and user preference during facial training tasks. The embedded feedback reduced cognitive load and received higher preference ratings, while the situated feedback enabled more precise corrections and higher accuracy. Qualitative analysis revealed a key design tension: embedded feedback improved experience but created self-consciousness and interpretive difficulty. We distill these insights into design considerations addressing the trade-offs for facial training systems, with implications for rehabilitation, performance training, and motor skill acquisition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01050v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avinash Ajit Nargund, Andrea M. Park, Tobias H\"ollerer, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Direct vs. Score-based Selection: Understanding the Heisenberg Effect in Target Acquisition Across Input Modalities in Virtual Reality</title>
      <link>https://arxiv.org/abs/2602.01061</link>
      <description>arXiv:2602.01061v1 Announce Type: new 
Abstract: Target selection is a fundamental interaction in virtual reality (VR). But the act of confirming a selection, such as a button press or pinch, can disturb the tracked pose and shift the intended target, which is referred to as the Heisenberg Effect. Prior research has mainly investigated controller input. However, it remains unclear how the effect manifests in the bare-hand input and how score-based techniques may mitigate the effect in different spatial variations. To fill the gap, we conduct a within-subject study to examine the Heisenberg Effect across two input modalities (i.e., controller and hand) and two selection mechanisms (i.e., direct and score-based). Our results show that hand input is more susceptible to the Heisenberg Effect, with direct selection more influenced by target width and score-based selection more sensitive to target density. Based on previous vote-oriented technique and our temporal analysis, we introduce weighted VOTE, a history-based intention accuracy model for target voting, that reweights recent interaction intent to counteract input disturbances. Our evaluation shows the method improves selection accuracy compared to baseline techniques. Finally, we discuss future directions for adaptive selection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01061v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linjie Qiu, Duotun Wang, Boyu Li, Jiawei Li, Yulin Shen, Zeyu Wang, Mingming Fan</dc:creator>
    </item>
    <item>
      <title>From Invisible to Actionable: Augmented Reality Interactions with Indoor CO2</title>
      <link>https://arxiv.org/abs/2602.01084</link>
      <description>arXiv:2602.01084v1 Announce Type: new 
Abstract: Indoor carbon dioxide (CO2) can rapidly accumulate to form invisible pollution hotspots, posing significant health risks due to its odorless and colorless nature. Despite growing interest in wearable or stationary sensors for pollutant detection, effectively visualizing CO2 levels and engaging individuals remains an ongoing challenge. In this paper, we develop a portable wrist-sized pollution sensor that detects CO2 in real time at any indoor location and reveals CO2 bubbles by highlighting sudden spikes. In order to promote better ventilation habits and user awareness, we also develop a smartphone-based augmented reality (AR) game for users to locate and disperse these high-CO2 zones. A user study with 35 participants demonstrated increased engagement and heightened understanding of CO2's health impacts. Our system's usability evaluations yielded a median score of 1.88, indicating its strong practicality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01084v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791951</arxiv:DOI>
      <dc:creator>Prasenjit Karmakar, Manjeet Yadav, Swayanshu Rout, Swadhin Pradhan, Sandip Chakraborty</dc:creator>
    </item>
    <item>
      <title>Bowling with ChatGPT: On the Evolving User Interactions with Conversational AI Systems</title>
      <link>https://arxiv.org/abs/2602.01114</link>
      <description>arXiv:2602.01114v1 Announce Type: new 
Abstract: Recent studies have discussed how users are increasingly using conversational AI systems, powered by LLMs, for information seeking, decision support, and even emotional support. However, these macro-level observations offer limited insight into how the purpose of these interactions shifts over time, how users frame their interactions with the system, and how steering dynamics unfold in these human-AI interactions. To examine these evolving dynamics, we gathered and analyzed a unique dataset InVivoGPT: consisting of 825K ChatGPT interactions, donated by 300 users through their GDPR data rights. Our analyses reveal three key findings. First, participants increasingly turn to ChatGPT for a broader range of purposes, including substantial growth in sensitive domains such as health and mental health. Second, interactions become more socially framed: the system anthropomorphizes itself at rising rates, participants more frequently treat it as a companion, and personal data disclosure becomes both more common and more diverse. Third, conversational steering becomes more prominent, especially after the release of GPT-4o, with conversations where the participants followed a model-initiated suggestion quadrupling over the period of our dataset. Overall, our results show that conversational AI systems are shifting from functional tools to social partners, raising important questions about their design and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01114v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Keerthana Karnam, Abhisek Dash, Krishna Gummadi, Animesh Mukherjee, Ingmar Weber, Savvas Zannettou</dc:creator>
    </item>
    <item>
      <title>Talk to Me, Not the Slides: A Real-Time Wearable Assistant for Improving Eye Contact in Presentations</title>
      <link>https://arxiv.org/abs/2602.01201</link>
      <description>arXiv:2602.01201v1 Announce Type: new 
Abstract: Effective eye contact is a cornerstone of successful public speaking. It strengthens the speaker's credibility and fosters audience engagement. Yet, managing effective eye contact is a skill that demands extensive training and practice, often posing a significant challenge for novice speakers. In this paper, we present SpeakAssis, the first real-time, in-situ wearable system designed to actively assist speakers in maintaining effective eye contact during live presentations. Leveraging a head-mounted eye tracker for gaze and scene view capture, SpeakAssis continuously monitors and analyzes the speaker's gaze distribution across audience and non-audience regions. When ineffective eye-contact patterns are detected, such as insufficient eye contact, or neglect of certain audience segments, SpeakAssis provides timely, context-aware audio prompts via an earphone to guide the speaker's gaze behavior. We evaluate SpeakAssis through a user study involving eight speakers and 24 audience members. Quantitative results show that SpeakAssis increases speakers' eye-contact duration by 62.5% on average and promotes a more balanced distribution of visual attention. Additionally, statistical analysis based on audience surveys reveals that improvements in speaker's eye-contact behavior significantly enhance the audience's perceived engagement and interactivity during presentations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01201v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingyu Du, Xucong Zhang, Guohao Lan</dc:creator>
    </item>
    <item>
      <title>LeagueBot: A Voice LLM Companion of Cognitive and Emotional Support for Novice Players in Competitive Games</title>
      <link>https://arxiv.org/abs/2602.01213</link>
      <description>arXiv:2602.01213v1 Announce Type: new 
Abstract: Competitive games pose steep learning curves and strong social pressures, often discouraging novice players and limiting sustained engagement. To address these challenges, this study introduces LeagueBot, a large language model-based voice chatbot designed to provide both informational and emotional support during live gameplay in league of legends, one of the most competitive multiplayer online battle arena games. In a within-subjects experiment with 33 novice players, LeagueBot was found to reduce cognitive challenge, performative challenge, and perceived tension. Qualitative analysis further identified three themes: enhanced access to game information, relief from cognitive burden, and practical limitations. Participants noted that LeagueBot offered context-appropriate guidance and emotional support, helping ease the steep learning curve and psychological pressures of competitive gaming. Together, these findings underscore the potential of voice-based LLM companions to assist novice players in competitive environments and highlight their broader applicability for real-time support in other high-pressure contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01213v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungmin Lee, Inhee Cho, Youngjae Yoo</dc:creator>
    </item>
    <item>
      <title>Shades of Uncertainty: How AI Uncertainty Visualizations Affect Trust in Alzheimer's Predictions</title>
      <link>https://arxiv.org/abs/2602.01264</link>
      <description>arXiv:2602.01264v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is increasingly used to support prognosis in Alzheimer's disease (AD), but adoption remains limited due to a lack of transparency and interpretability, particularly for long-term predictions where uncertainty is intrinsic and outcomes may not be known for years. We position uncertainty visualization as an explainable AI (XAI) technique and examine how it shapes trust, confidence, and reliance when users interpret AI-generated forecasts of future cognitive decline transitions. We conducted two studies, one with general participants (N=37) and one with experts in neuroimaging and neurology (N=10), to compare binary (present/absent) and continuous (saturation) uncertainty encodings. Continuous encodings improved perceived reliability and helped users recognize model limitations, while binary encodings increased momentary confidence, revealing expertise-dependent trade-offs in interpreting future predictions under high uncertainty. These findings surface key challenges in designing uncertainty representations for prognostic AI and culminate in a set of empirically grounded guidelines for creating trustworthy, user-appropriate clinical decision support tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01264v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonatan Reyes, Mina Massoumi, Anil Ufuk Batmaz, Marta Kersten-Oertel</dc:creator>
    </item>
    <item>
      <title>Trade-offs in Financial AI: Explainability in a Trilemma with Accuracy and Compliance</title>
      <link>https://arxiv.org/abs/2602.01368</link>
      <description>arXiv:2602.01368v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in financial decision-making, the opacity of complex models presents significant challenges for professionals and regulators. While the field of Explainable AI (XAI) attempts to bridge this gap, current research often reduces the implementation challenge to a binary trade-off between model accuracy and explainability. This paper argues that such a view is insufficient for the financial domain, where algorithmic choices must navigate a complex sociotechnical web of strict regulatory bounds, budget constraints, and latency requirements. Through semi-structured interviews with twenty finance professionals, ranging from C-suite executives and developers to regulators across multiple regions, this study empirically investigates how practitioners prioritize explainability relative to four competing factors: accuracy, compliance, cost, and speed. Our findings reveal that these priorities are structured not as a simple trade-off, but as a system of distinct prerequisites and constraints. Accuracy and compliance emerge as non-negotiable "hygiene factors": without them, an AI system is viewed as a liability regardless of its transparency. Operational levers (speed and cost) serve as secondary constraints that determine practical feasibility, while ease of understanding functions as a gateway to adoption, shaping whether AI tools are trusted, used, and defensible in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01368v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patricia Marcella Evite, Ekaterina Svetlova, Doina Bucur</dc:creator>
    </item>
    <item>
      <title>"If You're Very Clever, No One Knows You've Used It": The Social Dynamics of Developing Generative AI Literacy in the Workplace</title>
      <link>https://arxiv.org/abs/2602.01386</link>
      <description>arXiv:2602.01386v1 Announce Type: new 
Abstract: Generative AI (GenAI) tools are rapidly transforming knowledge work, making AI literacy a critical priority for organizations. However, research on AI literacy lacks empirical insight into how knowledge workers' beliefs around GenAI literacy are shaped by the social dynamics of the workplace, and how workers learn to apply GenAI tools in these environments. To address this gap, we conducted in-depth interviews with 19 knowledge workers across multiple sectors to examine how they develop GenAI competencies in real-world professional contexts. We found that, while knowledge sharing from colleagues supported learning, the ability to remove cues indicating GenAI use was perceived as validation of domain expertise. These behaviours ultimately reduced opportunities for learning via knowledge sharing and undermined transparency. To advance workplace AI literacy, we argue for fostering open dialogue, increasing visibility of user-generated knowledge, and greater emphasis on the benefits of collaborative learning for navigating rapid technological developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01386v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Qing (Nancy),  Xia, Marios Constantinides, Advait Sarkar, Duncan Brumby, Anna Cox</dc:creator>
    </item>
    <item>
      <title>Disclose with Care: Designing Privacy Controls in Interview Chatbots</title>
      <link>https://arxiv.org/abs/2602.01387</link>
      <description>arXiv:2602.01387v1 Announce Type: new 
Abstract: Collecting data on sensitive topics remains challenging in HCI, as participants often withhold information due to privacy concerns and social desirability bias. While chatbots' perceived anonymity may reduce these barriers, research paradoxically suggests people tend to over-share personal or sensitive information with chatbots. In this work, we explore privacy controls in chatbot interviews to address this problem. The privacy control allows participants to revise their transcripts at the end of the interview, featuring two design variants: free editing and AI-aided editing. In a between-subjects study \red{($N=188$)}, we compared no-editing, free-editing, and AI-aided editing conditions in a chatbot-based interview on a sensitive topic. Our results confirm the prevalent issue of oversharing in chatbot-based interviews and show that AI-aided editing serves as an effective privacy-control mechanism, reducing PII disclosure while maintaining data quality and user engagement, thereby offering a promising approach to balancing ethical practice and data quality in such interviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01387v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwen Li, Ziang Xiao, Tianshi Li</dc:creator>
    </item>
    <item>
      <title>How well can VLMs rate audio descriptions: A multi-dimensional quantitative assessment framework</title>
      <link>https://arxiv.org/abs/2602.01390</link>
      <description>arXiv:2602.01390v1 Announce Type: new 
Abstract: Digital video is central to communication, education, and entertainment, but without audio description (AD), blind and low-vision audiences are excluded. While crowdsourced platforms and vision-language-models (VLMs) expand AD production, quality is rarely checked systematically. Existing evaluations rely on NLP metrics and short-clip guidelines, leaving questions about what constitutes quality for full-length content and how to assess it at scale. To address these questions, we first developed a multi-dimensional assessment framework for uninterrupted, full-length video, grounded in professional guidelines and refined by accessibility specialists. Second, we integrated this framework into a comprehensive methodological workflow, utilizing Item Response Theory, to assess the proficiency of VLM and human raters against expert-established ground truth. Findings suggest that while VLMs can approximate ground-truth ratings with high alignment, their reasoning was found to be less reliable and actionable than that of human respondents. These insights show the potential of hybrid evaluation systems that leverage VLMs alongside human oversight, offering a path towards scalable AD quality control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01390v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lana Do, Gio Jung, Juvenal Francisco Barajas, Andrew Taylor Scott, Shasta Ihorn, Alexander Mario Blum, Vassilis Athitsos, Ilmi Yoon</dc:creator>
    </item>
    <item>
      <title>Living Contracts: Beyond Document-Centric Interaction with Legal Agreements</title>
      <link>https://arxiv.org/abs/2602.01396</link>
      <description>arXiv:2602.01396v1 Announce Type: new 
Abstract: User interaction with legal contracts has been limited to document reading, which is often complicated by complex, ambiguous legal language. We explore possible futures where contract interfaces go beyond single document interfaces to (1) educate users with legal rights not stated in the contract, (2) transform legal language into alternative representations to aid information tasks before, during, and after signing, and (3) proactively supply contractual information at relevant moments. We refer to these future interfaces collectively as Living Contracts. Using residential leases as a case study, we created three design probes representing different possible Living Contracts. A three-part qualitative study (N=18) revealed participants' barriers to interacting with contracts, including interpreting complex language, uncertainty about legal rights, and the pressure to sign quickly. Participants' feedback on the probes highlighted how Living Contracts have the potential to address these challenges and open new design opportunities for human-contract interactions beyond document reading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01396v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziheng Huang, Robin Kar, Hari Sundaram, Tal August</dc:creator>
    </item>
    <item>
      <title>Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents</title>
      <link>https://arxiv.org/abs/2602.01405</link>
      <description>arXiv:2602.01405v1 Announce Type: new 
Abstract: High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01405v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791600</arxiv:DOI>
      <dc:creator>Nikhil Sharma, Zheng Zhang, Daniel Lee, Namita Krishnan, Guang-Jie Ren, Ziang Xiao, Yunyao Li</dc:creator>
    </item>
    <item>
      <title>From One World to Another: Interfaces for Efficiently Transitioning Between Virtual Environments</title>
      <link>https://arxiv.org/abs/2602.01423</link>
      <description>arXiv:2602.01423v1 Announce Type: new 
Abstract: Personal computers and handheld devices provide keyboard shortcuts and swipe gestures to enable users to efficiently switch between applications, whereas today's virtual reality (VR) systems do not. In this work, we present an exploratory study on user interface aspects to support efficient switching between worlds in VR. We created eight interfaces that afford previewing and selecting from the available virtual worlds, including methods using portals and worlds-in-miniature (WiMs). To evaluate these methods, we conducted a controlled within-subjects empirical experiment (N=22) where participants frequently transitioned between six different environments to complete an object collection task. Our quantitative and qualitative results show that WiMs supported rapid acquisition of high-level spatial information while searching and were deemed most efficient by participants while portals provided fast pre-orientation. Finally, we present insights into the applicability, usability, and effectiveness of the VR world switching methods we explored, and provide recommendations for their application and future context/world switching techniques and interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01423v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791912</arxiv:DOI>
      <dc:creator>Matt Gottsacker, Yahya Hmaiti, Mykola Maslych, Hiroshi Furuya, Jasmine Joyce DeGuzman, Gerd Bruder, Gregory F. Welch, Joseph J. LaViola Jr</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT</title>
      <link>https://arxiv.org/abs/2602.01450</link>
      <description>arXiv:2602.01450v2 Announce Type: new 
Abstract: To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.
  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01450v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhisek Dash, Soumi Das, Elisabeth Kirsten, Qinyuan Wu, Sai Keerthana Karnam, Krishna P. Gummadi, Thorsten Holz, Muhammad Bilal Zafar, Savvas Zannettou</dc:creator>
    </item>
    <item>
      <title>How Users Perceive Mixed-Initiative AI: Attitudes Toward Assistance in Problem Solving</title>
      <link>https://arxiv.org/abs/2602.01481</link>
      <description>arXiv:2602.01481v1 Announce Type: new 
Abstract: In mixed-initiative systems, the mode of AI assistance delivery can be as consequential as the assistance itself. We investigated two assistance delivery modes: on-demand help (users request via Button) and pre-scheduled help (assistance delivered at user-selected intervals, with user actions resetting the Timer). To evaluate these modes, we selected Rush Hour puzzles as the human-AI collaborative task because they capture elements of real-world problem solving such as analysis, resource management, and decision-making under constraints. To enhance ecological validity, we imposed monetary costs for both time and AI assistance, simulating scenarios where people must balance implicit or explicit trade-offs such as time pressure, financial limitations, or opportunity costs. Although task performance was comparable across modes, participants who used the pre-scheduled (Timer) mode reported more positive perceptions of the AI, even when their ending budget was low. This suggests that assistance delivery mode can shape user experience independent of task outcomes, indicating that human-AI systems may need to consider how AI assistance is delivered alongside improving task performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01481v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3742413.3789224</arxiv:DOI>
      <dc:creator>Yunhao Luo, Arthur Caetano, Avinash Ajit Nargund, Tobias H\"ollerer, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Draw2Learn: A Human-AI Collaborative Tool for Drawing-Based Science Learning</title>
      <link>https://arxiv.org/abs/2602.01494</link>
      <description>arXiv:2602.01494v1 Announce Type: new 
Abstract: Drawing supports learning by externalizing mental models, but providing timely feedback at scale remains challenging. We present Draw2Learn, a system that explores how AI can act as a supportive teammate during drawing-based learning. The design translates learning principles into concrete interaction patterns: AI generates structured drawing quests, provides optional visual scaffolds, monitors progress, and delivers multidimensional feedback. We collected formative user feedback during system development and open-ended comments. Feedback showed positive ratings for usability, usefulness, and user experience, with themes highlighting AI scaffolding value and learner autonomy. This work contributes a design framework for teammate-oriented AI in generative learning and identifies key considerations for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01494v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuqi Hang</dc:creator>
    </item>
    <item>
      <title>Data Repair</title>
      <link>https://arxiv.org/abs/2602.01517</link>
      <description>arXiv:2602.01517v1 Announce Type: new 
Abstract: This paper investigates data repair practices through a six-month-long ethnographic study in Bangladesh. Our interviews and field observations with data repairers and related stakeholders found that, alongside the scarcity of high-precision machinery and access to advanced software, data repair work is constrained by cross-language learning resources and the protective nature of documenting, curating, and sharing the experiences and knowledge among local peers. Repairers turning to external resources such as foreign forums and LLMs also revealed their frustrating experiences and the postcolonial ethical tensions they encountered. We noted that both anticipated technical labor and the emotionality of data were taken into account for pricing the data repair job, which contributed to their market sustainability strategies. Engaging with repair, infrastructure, and data poverty discourse, we argue that data repair practices represent a crucial challenge and opportunity for HCI in advancing global efforts toward data equity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01517v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>ATM Mizanur Rahman (University of Illinois Urbana-Champaign), Syed Ishtiaque Ahmed (University of Toronto), Sharifa Sultana (University of Illinois Urbana-Champaign)</dc:creator>
    </item>
    <item>
      <title>How Notations Evolve: A Historical Analysis with Implications for Supporting User-Defined Abstractions</title>
      <link>https://arxiv.org/abs/2602.01525</link>
      <description>arXiv:2602.01525v1 Announce Type: new 
Abstract: Traditional human-computer interaction takes place through formally-specified systems like structured UIs and programming languages. Recent AI systems promise a new set of informal interactions with computers through natural language and other notational forms. These informal interactions can then lead to formal representations, but depend upon pre-existing formalisms known to both humans and AI. What about novel formalisms and notations? How are new abstractions created, evolved, and incrementally formalized over time -- and how might new systems, in turn, be explicitly designed to support these processes? We conduct a comparative historical analysis of notation development to identify some relevant characteristics. These include three social stages of notation development: invention &amp; incubation, dispersion &amp; divergence, and institutionalization &amp; sanctification, as well as three functional stages: descriptive, generative, and evaluative. Within and across these stages, we detail several patterns, such as the role of linking and grounding metaphors, dimensions of meaningful variation, and analogical alignment. Finally, we offer some implications for design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01525v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyue Zhang, J. D. Zamfirescu-Pereira, Elena L. Glassman, Damien Masson, Ian Arawjo</dc:creator>
    </item>
    <item>
      <title>Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition</title>
      <link>https://arxiv.org/abs/2602.01527</link>
      <description>arXiv:2602.01527v1 Announce Type: new 
Abstract: Visualization's design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a growing body of benchmark evidence indicates that this human-centered knowledge base does not straightforwardly transfer to machine audiences. Machines exhibit different encoding performance patterns, process images through patch-based tokenization rather than holistic perception, and fail on design patterns that pose no difficulty for humans-while occasionally succeeding where humans struggle. Current approaches address this gap primarily by bypassing vision entirely, converting charts to data tables or structured text. We argue that this response forecloses a more fundamental question: what visual representations would actually serve machine cognition well? This paper makes the case that the visualization field needs to investigate machine-oriented visual design as a distinct research problem. We synthesize evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to show that the human-machine perceptual divergence is qualitative, not merely quantitative, and critically examine the prevailing bypassing approach. We propose a conceptual distinction between human-oriented and machine-oriented visualization-not as an engineering architecture but as a recognition that different audiences may require fundamentally different design foundations-and outline a research agenda for developing the empirical foundations the field currently lacks: the beginnings of a "machine Bertin" to complement the human-centered knowledge the field already possesses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01527v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Keith-Norambuena</dc:creator>
    </item>
    <item>
      <title>ASafePlace: User-Led Personalization of VR Relaxation via an Art Therapy Activity</title>
      <link>https://arxiv.org/abs/2602.01579</link>
      <description>arXiv:2602.01579v1 Announce Type: new 
Abstract: To overcome the lack of deep personalization in standard biofeedback methods, we introduce ASafePlace, a system utilizing an AI-powered, art-therapy-inspired exercise called The Safe Place, to create a personalized VR biofeedback experience. In our system, users sketch a personal sanctuary from memory, which is then transformed into a customized 360 virtual environment with personalized audio guidance for relaxation training. A study with 52 participants showed this approach effectively reduced anxiety and increased user presence, while the integration of art-therapy-inspired activity and biofeedback produced strong improvements in physiological relaxation, measured by heart rate variability and respiration rate. Qualitative results showed how participants' sense of familiarity and presence was enhanced by the symbolic elements and natural sanctuaries created from their autobiographical memories. Our findings demonstrate that art-therapy-inspired activity is a powerful tool for creating highly effective and individualized relaxation experiences, naturally connecting the virtual environment to a user's core memories and emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01579v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuyang Zhang, Bin Yu, Yuchao Wang, Mansi Yuan, Wanqi Wang, Seungwoo Je, Pengcheng An</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Adaptive Rendering for High-Frequency Security Telemetry in Web Interfaces</title>
      <link>https://arxiv.org/abs/2602.01671</link>
      <description>arXiv:2602.01671v1 Announce Type: new 
Abstract: Modern cybersecurity platforms must process and display high-frequency telemetry such as network logs, endpoint events, alerts, and policy changes in real time. Traditional rendering techniques based on static pagination or fixed polling intervals fail under volume conditions exceeding hundreds of thousands of events per second, leading to UI freezes, dropped frames, or stale data. This paper presents an AI-assisted adaptive rendering framework that dynamically regulates visual update frequency, prioritizes semantically relevant events, and selectively aggregates lower-priority data using behavior-driven heuristics and lightweight on-device machine learning models. Experimental validation demonstrates a 45-60 percent reduction in rendering overhead while maintaining analyst perception of real-time responsiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01671v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mona Rajhans</dc:creator>
    </item>
    <item>
      <title>Beyond the Single Turn: Reframing Refusals as Dynamic Experiences Embedded in the Context of Mental Health Support Interactions with LLMs</title>
      <link>https://arxiv.org/abs/2602.01694</link>
      <description>arXiv:2602.01694v1 Announce Type: new 
Abstract: Content Warning: This paper contains participant quotes and discussions related to mental health challenges, emotional distress, and suicidal ideation.
  Large language models (LLMs) are increasingly used for mental health support, yet the model safeguards -- particularly refusals to engage with sensitive content -- remain poorly understood from the perspectives of users and mental health professionals (MHPs) and have been reported to cause real-world harms. This paper presents findings from a sequential mixed-methods study examining how LLM refusals are experienced and interpreted in mental health support interactions. Through surveys (N=53) and in-depth interviews (N=16) with individuals using LLMs for mental health support and MHPs, we reveal that refusals are not isolated, single-turn system behaviors, but rather constitute dynamic, multi-phase experiences: pre-refusal expectation formation, refusal triggering and encounter, refusal message framing, resource referral provision, and post-refusal outcomes. We contribute a multi-phase framework for evaluating refusals beyond binary policy compliance accuracy and design recommendations for future refusal mechanisms. These findings suggest that understanding LLM refusals requires moving beyond single-turn interactions toward recognizing them as holistic experiential processes embedded within the entire LLM design pipeline and the broader realities of mental health access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01694v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ningjing Tang, Alice Qian, Qiaosi Wang, Esther Howe, Blake Bullwinkel, Paola Pedrelli, Jina Suh, Hoda Heidari, Hong Shen</dc:creator>
    </item>
    <item>
      <title>Streamlined Facial Data Collection based on Utterance and Emotional Data for Human-to-Avatar Reconstruction</title>
      <link>https://arxiv.org/abs/2602.01729</link>
      <description>arXiv:2602.01729v1 Announce Type: new 
Abstract: This study explores a streamlined facial data collection method for conversational contexts, addressing the limitations of existing approaches that often require extensive datasets and prioritize technical metrics over user perception and experience. We systematically investigate which facial expression data are essential for reconstructing photorealistic avatars and how they can be captured efficiently. Our research employs a two-phase methodology to identify efficient facial data collection strategies and evaluate their effectiveness. In the first phase, we conduct facial data acquisition and evaluate reconstruction performance using utterance data and emotional data. In the second phase, we carry out a comprehensive user evaluation comparing three progressive conditions: utterance only, utterance and emotional data, and a control condition involving extensive data. Findings from 24 participants engaged in simulated face-to-face conversations reveal that targeted utterance and emotional data achieve comparable levels of perceived realism, naturalness, and telepresence, while reducing training time and data usage when compared to the extensive data collection approach. These results demonstrate that targeted data inputs can enable efficient avatar face reconstruction, offering practical guidelines for real-time applications such as AR/VR telepresence and highlighting the trade-off between data quantity and perceived quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01729v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Seoyoung Kang, Seokhwan Yang, Hail Song, Boram Yoon, Jinwook Kim, Kangsoo Kim, Woontack Woo</dc:creator>
    </item>
    <item>
      <title>Cost-Aware Bayesian Optimization for Prototyping Interactive Devices</title>
      <link>https://arxiv.org/abs/2602.01774</link>
      <description>arXiv:2602.01774v1 Announce Type: new 
Abstract: Deciding which idea is worth prototyping is a central concern in iterative design. A prototype should be produced when the expected improvement is high and the cost is low. However, this is hard to decide, because costs can vary drastically: a simple parameter tweak may take seconds, while fabricating hardware consumes material and energy. Such asymmetries, can discourage a designer from exploring the design space. In this paper, we present an extension of cost-aware Bayesian optimization to account for diverse prototyping costs. The method builds on the power of Bayesian optimization and requires only a minimal modification to the acquisition function. The key idea is to use designer-estimated costs to guide sampling toward more cost-effective prototypes. In technical evaluations, the method achieved comparable utility to a cost-agnostic baseline while requiring only ${\approx}70\%$ of the cost; under strict budgets, it outperformed the baseline threefold. A within-subjects study with 12 participants in a realistic joystick design task demonstrated similar benefits. These results show that accounting for prototyping costs can make Bayesian optimization more compatible with real-world design projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01774v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Langerak, Renate Zhang, Ziyuan Wang, Per Ola Kristensson, Antti Oulasvirta</dc:creator>
    </item>
    <item>
      <title>CritiqueCrew: Orchestrating Multi-Perspective Conversational Design Critique</title>
      <link>https://arxiv.org/abs/2602.01796</link>
      <description>arXiv:2602.01796v1 Announce Type: new 
Abstract: UI designers face growing cognitive load and cross functional friction at the intersection of user needs, business goals, and engineering constraints. Existing automated tools often deliver static "problem lists", lacking actionable repair paths and disrupting creative flow. We introduce CritiqueCrew, a Figma tool that supports designers through conversational critique. CritiqueCrew generates multi-faceted insights by implementing a multi-perspective orchestration of distinct expert roles (UX, PM, Engineer). It translates abstract critiques into concrete actions via in context feedback and interactive remediation. Across two independent controlled studies (Total N=48), CritiqueCrew significantly improved both design quality and subjective experience compared to a traditional static checker. Furthermore, our results confirm that the structured orchestration of expert roles-rather than a unified model-is key to fostering trust and creativity support. Our work demonstrates how AI can shift from a "problem auditor" to a "solution co-creator" by integrating multi-perspective dialogue with interactive repair, offering design implications for future creative tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01796v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaojiao Chen, Jiahuan Zhou, Yunfeng Shu, Ruihan Wang, Qinghua Liu</dc:creator>
    </item>
    <item>
      <title>Risk, Data, Alignment: Making Credit Scoring Work in Kenya</title>
      <link>https://arxiv.org/abs/2602.01824</link>
      <description>arXiv:2602.01824v1 Announce Type: new 
Abstract: Credit scoring is an increasingly central and contested domain of data and AI governance, frequently framed as a neutral and objective method of assessing risk across diverse economic and political contexts. Based on a nine-month ethnography of credit scoring practices in Nairobi, Kenya, we examined the sociotechnical and institutional work of data science in digital lending. While established regional telcos and banks are leveraging proprietary data to develop digital loan products, algorithmic credit scoring is being transformed by new actors, techniques, and shifting regulations. Our findings show how practitioners construct alternative data using technical and legal workarounds, formulate risk through multiple interpretations, and negotiate model performance via technical and political means. We argue that algorithmic credit scoring is accomplished through the ongoing work of alignment that stabilizes risk under conditions of persistent uncertainty, taking epistemic, modeling, and contextual forms. Extending work on alignment in HCI, we show how it operates as a two-way translation, where models are made "safe for worlds" while those worlds are reshaped to be "safe for models."</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01824v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790924</arxiv:DOI>
      <dc:creator>Daniel Mwesigwa, Steven J. Jackson, Christopher Csikszentmihalyi</dc:creator>
    </item>
    <item>
      <title>When Feasibility of Fairness Audits Relies on Willingness to Share Data: Examining User Acceptance of Multi-Party Computation Protocols for Fairness Monitoring</title>
      <link>https://arxiv.org/abs/2602.01846</link>
      <description>arXiv:2602.01846v1 Announce Type: new 
Abstract: Fairness monitoring is critical for detecting algorithmic bias, as mandated by the EU AI Act. Since such monitoring requires sensitive user data (e.g., ethnicity), the AI Act permits its processing only with strict privacy measures, such as multi-party computation (MPC), in compliance with the GDPR. However, the effectiveness of such secure monitoring protocols ultimately depends on people's willingness to share their data. Little is known about how different MPC protocol designs shape user acceptance. To address this, we conducted an online survey with 833 participants in Europe, examining user acceptance of various MPC protocol designs for fairness monitoring. Findings suggest that users prioritized risk-related attributes (e.g., privacy protection mechanism) in direct evaluation but benefit-related attributes (e.g., fairness objective) in simulated choices, with acceptance shaped by their fairness and privacy orientations. We derive implications for deploying and communicating privacy-preserving protocols in ways that foster informed consent and align with user expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01846v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changyang He, Parnian Jahangirirad, Lin Kyi, Asia J. Biega</dc:creator>
    </item>
    <item>
      <title>When Workout Buddies Are Virtual: AI Agents and Human Peers in a Longitudinal Physical Activity Study</title>
      <link>https://arxiv.org/abs/2602.01918</link>
      <description>arXiv:2602.01918v1 Announce Type: new 
Abstract: Physical inactivity remains a critical global health issue, yet scalable strategies for sustained motivation are scarce. Conversational agents designed as simulated exercising peers (SEPs) represent a promising alternative, but their long-term impact is unclear. We report a six-month randomized controlled trial (N=280) comparing individuals exercising alone, with a human peer, or with a large language model-driven SEP. Results revealed a partnership paradox: human peers evoked stronger social presence, while AI peers provided steadier encouragement and more reliable working alliances. Humans motivated through authentic comparison and accountability, whereas AI peers fostered consistent, low-stakes support. These complementary strengths suggest that AI agents should not mimic human authenticity but augment it with reliability. Our findings advance human-agent interaction research and point to hybrid designs where human presence and AI consistency jointly sustain physical activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01918v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790303</arxiv:DOI>
      <dc:creator>Alessandro Silacci, Mauro Cherubini, Arianna Boldi, Amon Rapp, Maurizio Caon</dc:creator>
    </item>
    <item>
      <title>Boosting metacognition in entangled human-AI interaction to navigate cognitive-behavioral drift</title>
      <link>https://arxiv.org/abs/2602.01959</link>
      <description>arXiv:2602.01959v1 Announce Type: new 
Abstract: People navigate complex environments using cues, heuristics, and other strategies, which are often adaptive in stable settings. However, as AI increasingly permeates society's information environments, those become more adaptive and evolving: LLM-based chatbots participate in extended interaction, maintain conversational histories, mirror social cues, and can hypercustomize responses, thereby shaping not only what information is accessed but how questions are framed, how evidence is interpreted, and when action feels warranted. Here we propose a framework for sustained human-AI interaction that rests on invariant features of human cognition and human--AI interaction and centers on three interlinked phenomena: entanglement between users and AI systems, the emergence of cognitive and behavioral drift over repeated interactions, and the role of metacognition in the awareness and regulation of these dynamics. As conversational agents provide cues (e.g., fluency, coherence, responsiveness) that people treat as informative, subjective confidence and action readiness may increase without corresponding gains in epistemic reliability, making drift difficult to detect and correct. We describe these dynamics across micro-, meso-, and macro-levels. The framework identifies four metacognitive intervention points and psychologically informed interventions that provide metacognitive scaffolding (boosting and self-nudging). Finally, we outline a long-horizon research agenda for scientific foresight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01959v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Lopez-Lopez, Christoph M. Abels, Philipp Lorenz-Spreen, Stephan Lewandowsky, Stefan M. Herzog</dc:creator>
    </item>
    <item>
      <title>Hacking Flow: From Lived Practices to Innovation</title>
      <link>https://arxiv.org/abs/2602.01979</link>
      <description>arXiv:2602.01979v1 Announce Type: new 
Abstract: In digital knowledge work, flow promises not just productivity; it offers a pathway to well-being. Yet despite decades of flow research in HCI, we know little about how to design digital interventions that support it. In this work, we foreground lived interventions - everyday practices workers already use to foster flow - to uncover overlooked opportunities and chart new directions for digital intervention design. Specifically, we report findings from two studies: (1) a reflexive thematic analysis of open-ended survey responses (n = 160), surfacing 38 lived interventions across four categories: environment, organization, task shaping, and personal readiness; and (2) a quantitative online survey (n = 121) that validates this repertoire, identifies which interventions are broadly endorsed versus polarizing, and elicits visions of technological support. We contribute empirical insights into how digital workers cultivate flow, situate these lived interventions within existing literature, and derive design opportunities for future digital flow interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01979v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791009</arxiv:DOI>
      <dc:creator>Fabio Stano, Max L Wilson, Christof Weinhardt, Michael T Knierim</dc:creator>
    </item>
    <item>
      <title>Belief Updating and Delegation in Multi-Task Human-AI Interaction: Evidence from Controlled Simulations</title>
      <link>https://arxiv.org/abs/2602.01986</link>
      <description>arXiv:2602.01986v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly support heterogeneous tasks within a single interface, requiring users to form, update, and act upon beliefs about one system across domains with different reliability profiles. Understanding how such beliefs transfer across tasks and shape delegation is therefore critical for the design of multipurpose AI systems. We report a preregistered experiment (N=240; 7,200 trials) in which participants interacted with a controlled AI simulation across grammar checking, travel planning, and visual question answering, each with fixed, domain-typical accuracy levels. Delegation was operationalized as a binary reliance decision: accepting the AI's output versus acting independently, and belief dynamics were evaluated against Bayesian benchmarks. We find three main results. First, participants do not reset beliefs between tasks: priors in a new task depend on posteriors from the previous task, with a 10-point increase predicting a 3-4 point higher subsequent prior. Second, within tasks, belief updating follows the Bayesian direction but is substantially conservative, proceeding at roughly half the normative Bayesian rate. Third, delegation is driven primarily by subjective beliefs about AI accuracy rather than self-confidence, though confidence independently reduces reliance when beliefs are held constant. Together, these findings show that users form global, path-dependent expectations about multipurpose AI systems, update them conservatively, and rely on AI primarily based on subjective beliefs rather than objective performance. We discuss implications for expectation calibration, reliance design, and the risks of belief spillovers in deployed LLM-based interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01986v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790775</arxiv:DOI>
      <dc:creator>Shreyan Biswas, Alexander Erlei, Ujwal Gadiraju</dc:creator>
    </item>
    <item>
      <title>Situated Brushing and Linking in Virtual and Augmented Reality</title>
      <link>https://arxiv.org/abs/2602.02023</link>
      <description>arXiv:2602.02023v1 Announce Type: new 
Abstract: In traditional visual analysis, brushing and linking is commonly used to visually connect multiple views using highlighting techniques. However, brushing and linking has rarely been used in situated analytics, which uses visualizations to analyze data in the context of physical referents. In situated analytics, data representations must be visually linked to real-world objects. Previous work has assessed situated brushing and linking in a virtual reality simulation of a supermarket scenario. Here, we replicate and extend the previous approach by studying brushing and linking in an actual physical space with augmented reality, while further improving the highlighting techniques. Using a video see-through display, we compare augmented reality with virtual reality. Results suggest that AR performs better in time and accuracy, but the effectiveness of the techniques varies by condition. These results provide a new framing of how the real-world stimuli matter in situated analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02023v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Quijano-Chavez, Benjamin Lee, Nina Doerr, Wolfgang B\"uschel, Michael Sedlmair, Dieter Schmalstieg</dc:creator>
    </item>
    <item>
      <title>Are Semantic Networks Associated with Idea Originality in Artificial Creativity? A Comparison with Human Agents</title>
      <link>https://arxiv.org/abs/2602.02048</link>
      <description>arXiv:2602.02048v1 Announce Type: new 
Abstract: The application of generative artificial intelligence in Creativity Support Tools (CSTs) presents the challenge of interfacing two black boxes: the user's mind and the machine engine. According to Artificial Cognition, this challenge involves theories, methods, and constructs developed to study human creativity. Consistently, the paper investigated the relationship between semantic networks organisation and idea originality in Large Language Models. Data was collected by administering a set of standardised tests to ChatGPT-4o and 81 psychology students, divided into higher and lower creative individuals. The expected relationship was confirmed in the comparison between ChatGPT-4o and higher creative humans. However, despite having a more rigid network, ChatGPT-4o emerged as more original than lower creative humans. We attributed this difference to human motivational processes and model hyperparameters, advancing a research agenda for the study of artificial creativity. In conclusion, we illustrate the potential of this construct for designing and evaluating CSTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02048v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790849</arxiv:DOI>
      <dc:creator>Umberto Domanti, Lorenzo Campidelli, Sergio Agnoli, Antonella De Angeli</dc:creator>
    </item>
    <item>
      <title>See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers</title>
      <link>https://arxiv.org/abs/2602.02063</link>
      <description>arXiv:2602.02063v1 Announce Type: new 
Abstract: Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02063v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ding Xia, Xinyue Gui, Mark Colley, Fan Gao, Zhongyi Zhou, Dongyuan Li, Renhe Jiang, Takeo Igarashi</dc:creator>
    </item>
    <item>
      <title>CHOMP: Multimodal Chewing Side Detection with Earphones</title>
      <link>https://arxiv.org/abs/2602.02233</link>
      <description>arXiv:2602.02233v1 Announce Type: new 
Abstract: Chewing side preference (CSP) has been identified both as a risk factor for temporomandibular disorders (TMD) and behavioral manifestation. Despite TMDs affecting roughly one third of the global population, assessment mainly relies on clinical examinations and self-reports, offering limited insight into everyday jaw function. Continuous CSP monitoring could provide an objective proxy for functional asymmetries. Prior wearable approaches, however, mostly use specialized form factors and demonstrate limited performance. We therefore present CHOMP, the first system for chewing side detection using earphones. Employing OpenEarable 2.0, we collected data from 20 participants with microphones, a bone-conduction microphone, IMU, PPG, and a pressure sensor across eleven foods, five non-chewing activities, and three noise conditions. We apply the Continuous Wavelet Transform to each sensing modality and use the resulting multi-channel scalograms as inputs to CNN-based classifiers. Microphones achieve the strongest single-sensor unit performance, with median F1 scores of 94.5% in leave-one-food-out (LOFO) and 92.6% in leave-one-subject-out (LOSO) cross-validations. Fusing sensing modalities further improves performance to 97.7% for LOFO and 95.4% for LOSO, with additional evaluations under noise interference indicating robust performance. Our results establish earphones as a practical platform for continuous CSP monitoring, enabling clinicians and patients to assess jaw function in everyday life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02233v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Hummel, Maximilian Burzer, Felix Schlotter, Michael K\"uttner, Tobias King, Qiang Yang, Cecilia Mascolo, Michael Beigl, Tobias R\"oddiger</dc:creator>
    </item>
    <item>
      <title>When more precision is worse: Do people recognize inadequate scene representations in concept-based explainable AI?</title>
      <link>https://arxiv.org/abs/2602.02298</link>
      <description>arXiv:2602.02298v1 Announce Type: new 
Abstract: Explainable artificial intelligence (XAI) aims to help uncover flaws in an AI model's internal representations. But do people draw the right conclusions from its explanations? Specifically, do they recognize an AI's inability to distinguish between relevant and irrelevant features? In the present study, a simulated AI classified images of railway trespassers as dangerous or not. To explain which features it has used, other images from the dataset were shown that activate the AI in a similar way. These concept images varied in three relevant features (i.e., a person's distance to the tracks, direction, and action) and in an irrelevant feature (i.e., scene background). When the AI uses a feature in its decision, this feature is retained in the concept images, otherwise the images randomize over it (e.g., same distance, varied backgrounds). Participants rated the AI more favorably when it retained relevant features. For the irrelevant feature, they did not mind in general, and sometimes even preferred it to be retained. This suggests that people may not recognize it when an AI model relies on irrelevant features to make its decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02298v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romy M\"uller, Wiebke Klausing</dc:creator>
    </item>
    <item>
      <title>The hybrid confirmation tree: A robust strategy for hybrid intelligence</title>
      <link>https://arxiv.org/abs/2602.02375</link>
      <description>arXiv:2602.02375v1 Announce Type: new 
Abstract: Combining human and artificial intelligence (AI) is a potentially powerful approach to boost decision accuracy. However, few such approaches exist that effectively integrate both types of intelligence while maintaining human agency. Here, we introduce and evaluate the hybrid confirmation tree, a simple aggregation strategy that compares the independent decisions of both a human and AI, with disagreements triggering a second human tiebreaker. Through analytical derivations, we show that the hybrid confirmation tree can match and exceed the accuracy of a three-person human majority vote while requiring fewer human inputs, particularly when AI accuracy is comparable to or exceeds human accuracy. We analytically demonstrate that the hybrid confirmation tree's ability to achieve complementarity -- outperforming individual humans, AI, and the majority vote -- is maximized when human and AI accuracies are similar and their decisions are not overly correlated. Empirical reanalysis of six real-world datasets (covering skin cancer diagnosis, deepfake detection, geopolitical forecasting, and criminal rearrest) validates these findings, showing that the hybrid confirmation tree improves accuracy over the majority vote by up to 10 percentage points while reducing the cost of decision making by 28--44$\%$. Furthermore, the hybrid confirmation tree provides greater flexibility in navigating true and false positive trade-offs compared to fixed human-only heuristics like hierarchies and polyarchies. The hybrid confirmation tree emerges as a practical, efficient, and robust strategy for hybrid collective intelligence that maintains human agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02375v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Julian Berger, Pantelis P. Analytis, Frederik Andersen, Kristian P. Lorenzen, Ville Satop\"a\"a, Ralf HJM Kurvers</dc:creator>
    </item>
    <item>
      <title>Talking Inspiration: A Discourse Analysis of Data Visualization Podcasts</title>
      <link>https://arxiv.org/abs/2602.02397</link>
      <description>arXiv:2602.02397v1 Announce Type: new 
Abstract: Data visualization practitioners routinely invoke inspiration, yet we know little about how it is constructed in public conversations. We conduct a discourse analysis of 31 episodes from five popular data visualization podcasts. Podcasts are public-facing and inherently performative: guests manage impressions, articulate values, and model "good practice" for broad audiences. We use this performative setting to examine how legitimacy, identity, and practice are negotiated in community talk. We show that "inspiration talk" is operative rather than ornamental: speakers legitimize what counts, who counts, and how work proceeds. Our analysis surfaces four adjustable evaluation criteria by which inspiration is judged-novelty, authority, authenticity, and affect-and three operative metaphors that license different practices-spark, muscle, and resource bank. We argue that treating inspiration as a boundary object helps explain why these frames coexist across contexts. Findings provide a vocabulary for examining how inspiration is mobilized in visualization practice, with implications for evaluation, pedagogy, and the design of galleries and repositories that surface inspirational examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02397v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791537</arxiv:DOI>
      <dc:creator>Ali Baigelenov, Prakash Shukla, Phuong Bui, Paul Parsons</dc:creator>
    </item>
    <item>
      <title>Front-Loaded or Balanced? The Mechanism through Which Review Order Affects Overall Ratings in Premium Service Settings</title>
      <link>https://arxiv.org/abs/2602.00008</link>
      <description>arXiv:2602.00008v1 Announce Type: cross 
Abstract: In the increasingly prevalent landscape of high-quality service contexts, whether consumer evaluation interfaces adopt a rating-first or review-first sequence has become a critical factor shaping rating authenticity and feedback quality. While prior research has primarily examined review content and sentiment, systematic investigation into how evaluation order influences rating outcomes remains limited. Through exploratory analyses, we find that Letterboxd -- which employs a review-first, rating-after mechanism -- exhibits a more centralized rating distribution with fewer extreme scores, whereas Yelp -- which adopts a rating-first, review-after mechanism -- shows a pronounced bimodal distribution with more polarized ratings. Three controlled experiments further demonstrate that in high-quality service contexts, a rating-first (vs. review-first) interface significantly elevates consumers' overall ratings. Mechanism analyses indicate that cognitive effort and affective heuristics serve as dual pathways: a rating-first (vs. review-first) sequence reduces cognitive effort and heightens affective heuristics, thereby increasing rating scores. Moreover, service quality moderates this process. When service quality is low, the rating-first (vs. review-first) sequence instead leads to lower ratings. This research reveals the psychological mechanisms through which evaluation order affects consumer ratings via cognitive and affective pathways. It extends theoretical understanding of online rating formation and offers practical implications for optimizing platform interface design to enhance rating authenticity and credibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00008v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>He Wang, Ziyu Zhou, Hanxiang Liu</dc:creator>
    </item>
    <item>
      <title>Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio</title>
      <link>https://arxiv.org/abs/2602.00041</link>
      <description>arXiv:2602.00041v2 Announce Type: cross 
Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with surveys and semi structured interviews with 22 architecture students at the Singapore University of Technology and De-sign, the research analyzes student perceptions across three distinct feed-back domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative in-structors, but as collaborative "cognitive mirrors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and over-come the "blank page" problem, though they are limited by a lack of contex-tual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of offending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate ab-stract academic discourse into actionable design iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00041v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan David Salazar Rodriguez, Sam Conrad Joyce, Nachamma Sockalingam, Khoo Eng Tat,  Julfendi</dc:creator>
    </item>
    <item>
      <title>SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?</title>
      <link>https://arxiv.org/abs/2602.00327</link>
      <description>arXiv:2602.00327v1 Announce Type: cross 
Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00327v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yueyi Yang, Haotian Liu, Fang Kang, Mengqi Zhang, Zheng Lian, Hao Tang, Haoyu Chen</dc:creator>
    </item>
    <item>
      <title>MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants</title>
      <link>https://arxiv.org/abs/2602.00353</link>
      <description>arXiv:2602.00353v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00353v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihe Zhang, Cheyenne N Mohawk, Kaiying Han, Vijay Srinivas Tida, Manyu Li, Xiali Hei</dc:creator>
    </item>
    <item>
      <title>Towards a Cognitive-Support Tool for Threat Hunters</title>
      <link>https://arxiv.org/abs/2602.00432</link>
      <description>arXiv:2602.00432v1 Announce Type: cross 
Abstract: Cybersecurity increasingly relies on threat hunters to proactively identify adversarial activity, yet the cognitive work underlying threat hunting remains underexplored or insufficiently supported by existing tools. Building on prior studies that examined how threat hunters construct and share mental models during investigations, we derived a set of design propositions to support their cognitive and collaborative work. In this paper, we present the Threat Hunter Board, a prototype tool that operationalizes these design propositions by enabling threat hunters to externalize reasoning, organize investigative leads, and maintain continuity across sessions. Using a design science paradigm, we describe the solution design rationale and artifact development. In addition, we propose six design heuristics that form a solution-evaluation framework for assessing cognitive support in threat hunting tools. An initial evaluation using a cognitive walkthrough provides early evidence of feasibility, while future work will focus on user-based validation with professional threat hunters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00432v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra Maciel Paz Milani, Norman Anderson, Margaret-Anne Storey</dc:creator>
    </item>
    <item>
      <title>Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2602.00675</link>
      <description>arXiv:2602.00675v1 Announce Type: cross 
Abstract: Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00675v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerio Belcamino, Mariya Kilina, Alessandro Carf\`i, Valeria Seidita, Fulvio Mastrogiovanni, Antonio Chella</dc:creator>
    </item>
    <item>
      <title>WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs</title>
      <link>https://arxiv.org/abs/2602.00762</link>
      <description>arXiv:2602.00762v1 Announce Type: cross 
Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00762v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuheng Shao, Junjie Xiong, Chaoran Wu, Xiyuan Wang, Ziyu Zhou, Yang Ouyang, Qinyi Tao, Quan Li</dc:creator>
    </item>
    <item>
      <title>Inter- and Intra-Subject Variability in EEG: A Systematic Survey</title>
      <link>https://arxiv.org/abs/2602.01019</link>
      <description>arXiv:2602.01019v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability across resting-state, event-related potentials (ERPs), and task-related/BCI paradigms (including motor imagery and SSVEP) in healthy and clinical cohorts. Across paradigms, inter-subject differences are typically larger than within-subject fluctuations, but both affect inference and model generalization. Stability is feature-dependent: alpha-band measures and individual alpha peak frequency are often relatively reliable, whereas higher-frequency and many connectivity-derived metrics show more heterogeneous reliability; ERP reliability varies by component, with P300 measures frequently showing moderate-to-good stability. We summarize major sources of variability (biological, state-related, technical, and analytical), review common quantification and modeling approaches (e.g., ICC, CV, SNR, generalizability theory, and multivariate/learning-based methods), and provide recommendations for study design, reporting, and harmonization. Overall, EEG variability should be treated as both a practical constraint to manage and a meaningful signal to leverage for precision neuroscience and robust neurotechnology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01019v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuan-The Tran, Thien-Nhan Vo, Son-Tung Vu, Thoa-Thi Tran, Manh-Dat Nguyen, Thomas Do, Chin-Teng Lin</dc:creator>
    </item>
    <item>
      <title>SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models</title>
      <link>https://arxiv.org/abs/2602.01226</link>
      <description>arXiv:2602.01226v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01226v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Shibu, Marah Saleh, Mohamed Al-Musleh, Nidhal Abdulaziz</dc:creator>
    </item>
    <item>
      <title>Seeing, Hearing, and Knowing Together: Multimodal Strategies in Deepfake Videos Detection</title>
      <link>https://arxiv.org/abs/2602.01284</link>
      <description>arXiv:2602.01284v1 Announce Type: cross 
Abstract: As deepfake videos become increasingly difficult for people to recognise, understanding the strategies humans use is key to designing effective media literacy interventions. We conducted a study with 195 participants between the ages of 21 and 40, who judged real and deepfake videos, rated their confidence, and reported the cues they relied on across visual, audio, and knowledge strategies. Participants were more accurate with real videos than with deepfakes and showed lower expected calibration error for real content. Through association rule mining, we identified cue combinations that shaped performance. Visual appearance, vocal, and intuition often co-occurred for successful identifications, which highlights the importance of multimodal approaches in human detection. Our findings show which cues help or hinder detection and suggest directions for designing media literacy tools that guide effective cue use. Building on these insights can help people improve their identification skills and become more resilient to deceptive digital media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01284v1</guid>
      <category>cs.MM</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Chen, Dion Hoe-Lian Goh</dc:creator>
    </item>
    <item>
      <title>Vulnerability-Amplifying Interaction Loops: a systematic failure mode in AI chatbot mental-health interactions</title>
      <link>https://arxiv.org/abs/2602.01347</link>
      <description>arXiv:2602.01347v1 Announce Type: cross 
Abstract: Millions of users turn to consumer AI chatbots to discuss behavioral and mental health concerns. While this presents unprecedented opportunities to deliver population-level support, it also highlights an urgent need to develop rigorous and scalable safety evaluations. Here we introduce SIM-VAIL, an AI chatbot auditing framework that captures how harmful AI chatbot responses manifest across a range of mental-health contexts. SIM-VAIL pairs a simulated human user, harboring a distinct psychiatric vulnerability and conversational intent, with an audited frontier AI chatbot. It scores conversation turns on 13 clinically relevant risk dimensions, enabling context-dependent, temporally resolved assessment of mental-health risk. Across 810 conversations, encompassing over 90,000 turn-level ratings and 30 psychiatric user profiles, we find that significant risk occurs across virtually all user phenotypes. Risk manifested across most of the 9 consumer AI chatbot models audited, albeit mitigated in more modern variants. Rather than arising abruptly, risk accumulated over multiple turns. Risk profiles were phenotype-dependent, indicating that behaviors that appear supportive in general settings are liable to be maladaptive when they align with mechanisms that sustain a user's vulnerability. Multivariate risk patterns revealed trade-offs across dimensions, suggesting that mitigation targeting one harm domain can exacerbate others. These findings identify a novel failure mode in human-AI interactions, which we term Vulnerability-Amplifying Interaction Loops (VAILs), and underscore the need for multi-dimensional approaches to risk quantification. SIM-VAIL provides a scalable evaluation framework for quantifying how mental-health risk is distributed across user phenotypes, conversational trajectories, and clinically grounded behavioral dimensions, offering a foundation for targeted safety improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01347v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veith Weilnhammer, Kevin YC Hou, Raymond Dolan, Matthew M Nour</dc:creator>
    </item>
    <item>
      <title>PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents</title>
      <link>https://arxiv.org/abs/2602.01532</link>
      <description>arXiv:2602.01532v1 Announce Type: cross 
Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01532v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu</dc:creator>
    </item>
    <item>
      <title>Are Security Cues Static? Rethinking Warning and Trust Indicators for Life Transitions</title>
      <link>https://arxiv.org/abs/2602.01544</link>
      <description>arXiv:2602.01544v1 Announce Type: cross 
Abstract: Security cues, such as warnings and trust signals, are designed as stable interface elements, even though people's lives, contexts, and vulnerabilities change over time. Life transitions including migration, aging, or shifts in institutional environments reshape how risk and trust are understood and acted upon. Yet current systems rarely adapt their security cues to these changing conditions, placing the burden of interpretation on users. In this Works-in-Progress paper, we argue that the static nature of security cues represents a design mismatch with transitional human lives. We draw on prior empirical insights from work on educational migration as a motivating case, and extend the discussion to other life transitions. Building on these insights, we introduce the Transition-Aware Security Cues (TASeC) framework and present speculative design concepts illustrating how security cues might evolve across transition stages. We invite HCI to rethink security cues as longitudinal, life-centered design elements collectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01544v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sarah Tabassum</dc:creator>
    </item>
    <item>
      <title>Neurophysiological effects of museum modalities on emotional engagement with real artworks</title>
      <link>https://arxiv.org/abs/2602.02086</link>
      <description>arXiv:2602.02086v1 Announce Type: cross 
Abstract: Museums increasingly rely on digital content to support visitors' understanding of artworks, yet little is known about how these formats shape the emotional engagement that underlies meaningful art experiences. This research presents an in-situ EEG study on how digital interpretive content modulate engagement during art viewing. Participants experienced three modalities: direct viewing of a Bruegel painting, a 180{\deg} immersive interpretive projection, and a regular, display-based interpretive video. Frontal EEG markers of motivational orientation, internal involvement, perceptual drive, and arousal were extracted using eyes-open baselines and Z-normalized contrasts. Results show modality-specific engagement profiles: display-based interpretive video induced high arousal and fast-band activity, immersive projections promoted calm, presence-oriented absorption, and original artworks reflected internally regulated engagement. These findings, relying on lightweight EEG sensing in an operational cultural environment, suggest that digital interpretive content affects engagement style rather than quantity. This paves the way for new multimodal sensing approaches and enables museums to optimize the modalities and content of their interpretive media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02086v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Feng, S\'ebastien Lugan, Karine Lasaracina, Midori Sugaya, Beno\^it Macq</dc:creator>
    </item>
    <item>
      <title>Value Sensitive Design for Fair Online Recruitment: A Conceptual Framework Informed by Job Seekers' Fairness Concerns</title>
      <link>https://arxiv.org/abs/2501.14110</link>
      <description>arXiv:2501.14110v3 Announce Type: replace 
Abstract: The susceptibility to biases and discrimination is a pressing issue in today's labor markets. While digital recruitment systems play an increasingly significant role in human resource management, a systematic understanding of human-centered design principles for fair online hiring remains lacking, particularly considering the gap between idealized conceptualizations of fairness in research and actual fairness concerns expressed by job seekers. To address this gap, this work explores the potential of developing a fair recruitment framework based on job seekers' fairness concerns shared in r/jobs, one of the largest online job communities. Through a grounded theory approach, we uncover four overarching themes of job seekers' fairness concerns: personal attribute discrimination beyond legally protected attributes, interaction biases, improper interpretations of qualifications, and power imbalance. Drawing on value sensitive design, we derive design implications for fair algorithms and interfaces in recruitment systems, integrating them into a conceptual framework that spans different hiring stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14110v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changyang He, Yue Deng, Alessandro Fabris, Bo Li, Asia Biega</dc:creator>
    </item>
    <item>
      <title>Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust</title>
      <link>https://arxiv.org/abs/2502.10844</link>
      <description>arXiv:2502.10844v3 Announce Type: replace 
Abstract: LLM-powered conversational agents are increasingly influencing our decision-making, raising concerns about "sycophancy" - the tendency for LLMs to excessively agree with users even at the expense of truthfulness. While prior work has primarily examined LLM sycophancy as a model behavior, our understanding of how users perceive this phenomenon and its impact on user trust remains significantly lacking. In this work, we conceptualize LLM sycophancy along two key constructs: conversational demeanor (complimentary vs. neutral) and stance adaptation (adaptive vs. consistent). A 2 x 2 between-subjects experiment (N = 224) revealed complex dynamics: complimentary LLMs that adapted their stance reduced perceived authenticity and trust, while neutral LLMs that adapted enhanced both, suggesting a pathway for manipulating users into over-trusting LLMs beyond their actual capabilities. Our findings advance user-centric understanding of LLM sycophancy and provide profound implications for developing more ethical and trustworthy LLM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10844v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791079</arxiv:DOI>
      <dc:creator>Yuan Sun, Ting Wang</dc:creator>
    </item>
    <item>
      <title>Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables</title>
      <link>https://arxiv.org/abs/2503.07825</link>
      <description>arXiv:2503.07825v2 Announce Type: replace 
Abstract: We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\% F1 accuracy and our 6-channel model surpassing 80\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07825v2</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, Louis Berridge</dc:creator>
    </item>
    <item>
      <title>Do Not Immerse and Drive? Prolonged Effects of Cybersickness on Physiological Stress Markers And Cognitive Performance</title>
      <link>https://arxiv.org/abs/2506.11536</link>
      <description>arXiv:2506.11536v2 Announce Type: replace 
Abstract: Extended exposure to virtual reality environments can induce motion sickness, often referred to as cybersickness, which may lead to physiological stress responses and impaired cognitive performance. This study investigates the aftereffects of VR-induced motion sickness with a focus on physiological stress markers and working memory performance. Using a carousel simulation to elicit cybersickness, we assessed subjective discomfort (SSQ, FMS), physiological stress (salivary cortisol, alpha-amylase, electrodermal activity, heart rate), and cognitive performance (n-Back task) over a 90-minute post-exposure period. Our findings demonstrate a significant increase in both subjective and physiological stress indicators following VR exposure, accompanied by a decline in working memory performance. Notably, delayed symptom progression was observed in a substantial proportion of participants, with some reporting peak symptoms up to 90 minutes post-stimulation. Salivary cortisol levels remained elevated throughout the observation period, indicating prolonged stress recovery. These results highlight the need for longer washout phases in XR research and raise safety concerns for professional applications involving post-exposure task performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11536v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Zielasko, Ben Rehling, Bernadette von Dawans, Gregor Domes</dc:creator>
    </item>
    <item>
      <title>TermSight: Making Service Contracts Approachable</title>
      <link>https://arxiv.org/abs/2506.12332</link>
      <description>arXiv:2506.12332v2 Announce Type: replace 
Abstract: Legal contracts govern much of our society, but their specialized language is difficult for non-experts to read. While AI has enabled simplification of complex language, legal contracts pose unique challenges because of their connection to readers' values, ambiguity, and legally binding nature. Based on a formative study (N=20) using Terms of Service (ToS) as example contracts to study challenges in contract reading, we developed TermSight, an intelligent reading interface to probe the opportunities and challenges of designing augmentations for legal text. TermSight guides readers to relevant clauses with color-coded plain-language snippets of information and contextualizes ambiguous language with definitions and hypothetical scenarios. Importantly, TermSight's features always foreground the original, legally-binding contract text (e.g., linking to associated clauses). Our within-subjects study (N=20) demonstrated the opportunities of TermSight in making ToS significantly easier to read and navigate while revealing the challenges of augmenting service contracts such as ToS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12332v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziheng Huang, Tal August, Hari Sundaram</dc:creator>
    </item>
    <item>
      <title>Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth</title>
      <link>https://arxiv.org/abs/2508.03705</link>
      <description>arXiv:2508.03705v2 Announce Type: replace 
Abstract: This study explores how different modes of digital interaction -- namely, computers versus smartphones -- affect attention, frustration, and creative performance in adolescents. Using a combination of digital task logs, webcam-based gaze estimation, and expert evaluation of task outcomes, we analyzed data from a diverse sample of 824 students aged 11-17. Participants were assigned to device groups in a randomized and stratified design to control for age, gender, and prior experience. Results suggest moderate but statistically significant differences in sustained attention, perceived frustration, and creative output. These findings indicate that the nature of digital interaction -- beyond mere screen time -- may influence cognitive and behavioral outcomes relevant to educational design. Practical implications for user interface development and learning environments are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03705v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kanan Eldarov</dc:creator>
    </item>
    <item>
      <title>The Language of Approval: Identifying the Drivers of Positive Feedback Online</title>
      <link>https://arxiv.org/abs/2509.10370</link>
      <description>arXiv:2509.10370v2 Announce Type: replace 
Abstract: Positive feedback via likes and awards is central to online governance, yet which attributes of users' posts elicit rewards -- and how these vary across authors and communities -- remains unclear. To examine this, we combine quasi-experimental causal inference with predictive modeling on 11M posts from 100 subreddits. We identify linguistic patterns and stylistic attributes causally linked to rewards, controlling for author reputation, timing, and community context. For example, overtly complicated language, tentative style, and toxicity reduce rewards. We use our set of curated features to train models that can detect highly-upvoted posts with high AUC. Our audit of community guidelines highlights a ``policy-practice gap'' -- most rules focus primarily on civility and formatting requirements, with little emphasis on the attributes identified to drive positive feedback. These results inform the design of community guidelines, support interfaces that teach users how to craft desirable contributions, and moderation workflows that emphasize positive reinforcement over purely punitive enforcement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10370v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agam Goyal, Charlotte Lambert, Eshwar Chandrasekharan</dc:creator>
    </item>
    <item>
      <title>LubDubDecoder: Bringing Micro-Mechanical Cardiac Monitoring to Hearables</title>
      <link>https://arxiv.org/abs/2509.10764</link>
      <description>arXiv:2509.10764v2 Announce Type: replace 
Abstract: We present LubDubDecoder, a system that enables fine-grained monitoring of micro-cardiac vibrations associated with the opening and closing of heart valves across a range of hearables. Our system transforms the built-in speaker, the only transducer common to all hearables, into an acoustic sensor that captures the coarse "lub-dub" heart sounds, leverages their shared temporal and spectral structure to reconstruct the subtle seismocardiography (SCG) and gyrocardiography (GCG) waveforms, and extract the timing of key micro-cardiac events. In an IRB-approved feasibility study with 25 users, our system achieves correlations of 0.88-0.95 compared to chest-mounted reference measurements in within-user and cross-user evaluations, and generalizes to unseen hearables using a zero-effort adaptation scheme with a correlation of 0.91. Our system is robust across remounting sessions and music playback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10764v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siqi Zhang, Xiyuxing Zhang, Duc Vu, Tao Qiang, Clara Palacios, Jiangyifei Zhu, Yuntao Wang, Mayank Goel, Justin Chan</dc:creator>
    </item>
    <item>
      <title>Do Teachers Dream of GenAI Widening Educational (In)equality? Envisioning the Future of K-12 GenAI Education from Global Teachers' Perspectives</title>
      <link>https://arxiv.org/abs/2509.10782</link>
      <description>arXiv:2509.10782v3 Announce Type: replace 
Abstract: Generative artificial intelligence (GenAI) is rapidly entering K-12 classrooms worldwide, initiating urgent debates about its potential to either reduce or exacerbate educational inequalities. Drawing on interviews with 30 K-12 teachers across the United States, South Africa, and Taiwan, this study examines how teachers navigate this GenAI tension around educational equalities. We found teachers actively framed GenAI education as an equality-oriented practice: they used it to alleviate pre-existing inequalities while simultaneously working to prevent new inequalities from emerging. Despite these efforts, teachers confronted persistent systemic barriers, i.e., unequal infrastructure, insufficient professional training, and restrictive social norms, that individual initiative alone could not overcome. Teachers thus articulated normative visions for more inclusive GenAI education. By centering teachers' practices, constraints, and future envisions, this study contributes a global account of how GenAI education is being integrated into K-12 contexts and highlights what is required to make its adoption genuinely equal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10782v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790908</arxiv:DOI>
      <dc:creator>Ruiwei Xiao, Qing Xiao, Xinying Hou, Phenyo Phemelo Moletsane, Hanqi Jane Li, Hong Shen, John Stamper</dc:creator>
    </item>
    <item>
      <title>Conversational Agents in Behavioral Sleep Medicine: Designing Self-Report and Analytics Tools</title>
      <link>https://arxiv.org/abs/2509.15378</link>
      <description>arXiv:2509.15378v2 Announce Type: replace 
Abstract: The sleep diary is a widely used clinical tool for understanding and treating sleep disorders in Behavioral Sleep Medicine (BSM); however, low patient compliance and limited capture of contextual information constrain its effectiveness and leave specialists with an incomplete picture of patients' sleep-related behaviors. In this work, we explore conversational agents (CAs) as an alternative to traditional diary methods by designing a voice-based sleep diary and a specialist-facing analytics tool, and using them as design probes to understand how CAs might support BSM more broadly. Our multi-stage study with specialists comprised: (1) interviews to identify shortcomings of current text-based diaries, (2) iterative co-design of the conversational diary and analytics tool, and (3) focus groups to examine broader opportunities for CAs in BSM. This work offers empirical insights into how specialists envision CAs in clinical care and outlines design implications for integrating them into existing self-report practices and behavioral interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15378v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amama Mahmood, Bokyung Kim, Honghao Zhao, Molly E. Atwood, Luis F. Buenaver, Michael T. Smith, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>CHOIR: A Chatbot-mediated Organizational Memory Leveraging Communication in University Research Labs</title>
      <link>https://arxiv.org/abs/2509.20512</link>
      <description>arXiv:2509.20512v3 Announce Type: replace 
Abstract: University research labs often rely on chat-based platforms for communication and project management, where valuable knowledge surfaces but is easily lost in message streams. Documentation can preserve knowledge, but it requires ongoing maintenance and is challenging to navigate. Drawing on formative interviews that revealed organizational memory challenges in labs, we designed CHOIR, an LLM-based chatbot that supports organizational memory through four key functions: document-grounded Q\&amp;A, Q\&amp;A sharing for follow-up discussion, knowledge extraction from conversations, and AI-assisted document updates. We deployed CHOIR in four research labs for one month (n=21), where the lab members asked 107 questions and lab directors updated documents 38 times in the organizational memory. Our findings reveal a privacy-awareness tension: questions were asked privately, limiting directors' visibility into documentation gaps. Students often avoided contribution due to challenges in generalizing personal experiences into universal documentation. We contribute design implications for privacy-preserving awareness and supporting context-specific knowledge documentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20512v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791314</arxiv:DOI>
      <arxiv:journal_reference>CHI '26: Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems</arxiv:journal_reference>
      <dc:creator>Sangwook Lee, Adnan Abbas, Yan Chen, Young-Ho Kim, Sang Won Lee</dc:creator>
    </item>
    <item>
      <title>Not Everyone Wins with LLMs: Behavioral Patterns and Pedagogical Implications for AI Literacy in Programmatic Data Science</title>
      <link>https://arxiv.org/abs/2509.21890</link>
      <description>arXiv:2509.21890v2 Announce Type: replace 
Abstract: LLMs promise to democratize technical work in complex domains like programmatic data analysis, but not everyone benefits equally. We study how students with varied experiences use LLMs to complete Python-based data analysis in computational notebooks in a graduate course. Drawing on homework logs, recordings, and surveys from 36 students, we ask: Which experience matters most, and how does it shape AI use? Our mixed-methods analysis shows that technical experience -- not AI familiarity or communication skills -- remains a significant predictor of success. Students also vary widely in how they leverage LLMs, struggling at stages of forming intent, expressing inputs, interpreting outputs, and assessing results. We identify success and failure behaviors, such as providing context or decomposing prompts, that distinguish effective use. These findings inform AI literacy interventions, highlighting that lightweight demonstrations improve surface fluency but are insufficient; deeper training and scaffolds are needed to cultivate resilient AI use skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21890v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791283</arxiv:DOI>
      <dc:creator>Qianou Ma, Kenneth Koedinger, Tongshuang Wu</dc:creator>
    </item>
    <item>
      <title>Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory</title>
      <link>https://arxiv.org/abs/2509.22505</link>
      <description>arXiv:2509.22505v2 Announce Type: replace 
Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly popular, offering empathetic interactions, yet their psychosocial impacts remain unclear. We examined how engaging with AICCs shaped wellbeing and how users perceived these experiences. First, we conducted a large-scale quasi-experimental study of longitudinal Reddit data, applying stratified propensity score matching and Difference-in-Differences regression. Findings revealed mixed effects -- greater grief expression and interpersonal focus, alongside increases in language about loneliness, depression, and suicidal ideation. Second, we complemented these results with 18 semi-structured interviews, which we thematically analyzed and contextualized using Knapp's relationship development model. We identified trajectories of initiation, escalation, and bonding, wherein AICCs provided emotional validation and social rehearsal but also carried risks of over-reliance and withdrawal. Triangulating across methods, we offer design implications for AI companions that scaffold healthy boundaries, support mindful engagement, support disclosure without dependency, and surface relationship stages -- maximizing psychosocial benefits while mitigating risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22505v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790558</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems</arxiv:journal_reference>
      <dc:creator>Yunhao Yuan, Jiaxun Zhang, Talayeh Aledavood, Renwen Zhang, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>"Shall We Dig Deeper?": Designing and Evaluating Strategies for LLM Agents to Advance Knowledge Co-Construction in Asynchronous Online Discussions</title>
      <link>https://arxiv.org/abs/2509.23327</link>
      <description>arXiv:2509.23327v2 Announce Type: replace 
Abstract: Asynchronous online discussions enable diverse participants to co-construct knowledge beyond individual contributions. This process ideally evolves through sequential phases, from superficial information exchange to deeper synthesis. However, many discussions stagnate in the early stages. Existing AI interventions typically target isolated phases, lacking mechanisms to progressively advance knowledge co-construction, and the impacts of different intervention styles in this context remain unclear and warrant investigation. To address these gaps, we conducted a design workshop to explore AI intervention strategies (task-oriented and/or relationship-oriented) throughout the knowledge co-construction process, and implemented them in an LLM-powered agent capable of facilitating progression while consolidating foundations at each phase. A within-subject study (N=60) involving five consecutive asynchronous discussions showed that the agent consistently promoted deeper knowledge progression, with different styles exerting distinct effects on both content and experience. These findings provide actionable guidance for designing adaptive AI agents that sustain more constructive online discussions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23327v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790551</arxiv:DOI>
      <dc:creator>Yuanhao Zhang, Wenbo Li, Xiaoyu Wang, Kangyu Yuan, Shuai Ma, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>LLM-based In-situ Thought Exchanges for Critical Paper Reading</title>
      <link>https://arxiv.org/abs/2510.15234</link>
      <description>arXiv:2510.15234v2 Announce Type: replace 
Abstract: Critical reading is a primary way through which researchers develop their critical thinking skills. While exchanging thoughts and opinions with peers can strengthen critical reading, junior researchers often lack access to peers who can offer diverse perspectives. To address this gap, we designed an in-situ thought exchange interface informed by peer feedback from a formative study (N=8) to support junior researchers' critical paper reading. We evaluated the effects of thought exchanges under three conditions (no-agent, single-agent, and multi-agent) with 46 junior researchers over two weeks. Our results showed that incorporating agent-mediated thought exchanges during paper reading significantly improved participants' critical thinking scores compared to the no-agent condition. In the single-agent condition, participants more frequently made reflective annotations on the paper content. In the multi-agent condition, participants engaged more actively with agents' responses. Our qualitative analysis further revealed that participants compared and analyzed multiple perspectives in the multi-agent condition. This work contributes to understanding in-situ AI-based support for critical paper reading through thought exchanges and offers design implications for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15234v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3742413.3789069</arxiv:DOI>
      <dc:creator>Xinrui Fang, Anran Xu, Chi-Lan Yang, Ya-Fang Lin, Sylvain Malacria, Koji Yatani</dc:creator>
    </item>
    <item>
      <title>Towards AI as Colleagues: Multi-Agent System Improves Structured Ideation Processes</title>
      <link>https://arxiv.org/abs/2510.23904</link>
      <description>arXiv:2510.23904v2 Announce Type: replace 
Abstract: Most AI systems today are designed to manage tasks and execute predefined steps. This makes them effective for process coordination but limited in their ability to engage in joint problem-solving with humans or contribute new ideas. We introduce MultiColleagues, a multi-agent conversational system that shows how AI agents can act as colleagues by conversing with each other, sharing new ideas, and actively involving users in collaborative ideation processes. In a within-subjects study with 20 participants, we compared MultiColleagues to a single-agent baseline. Results show that MultiColleagues fostered stronger perceived social presence, and participants rated their outcomes as higher in quality and novelty, with more elaboration during ideation. These findings demonstrate the potential of AI agents to move beyond process partners toward colleagues that share intent, strengthen group dynamics, and collaborate with humans to advance ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23904v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790375</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems</arxiv:journal_reference>
      <dc:creator>Kexin Quan, Dina Albassam, Mengke Wu, Zijian Ding, Jessie Chin</dc:creator>
    </item>
    <item>
      <title>FluxLab: Creating 3D Printable Shape-Changing Devices with Integrated Deformation Sensing</title>
      <link>https://arxiv.org/abs/2512.02911</link>
      <description>arXiv:2512.02911v2 Announce Type: replace 
Abstract: We present FluxLab, a system comprising interactive tools for creating custom 3D-printable shape-changing devices with integrated deformation sensing. To achieve this, we propose a 3D printable nesting structure, consisting of a central SMA channel for sensing and actuation, lattice-based padding in the middle for structural support and controllable elasticity, and parallel helix-based surface wires that preserve the overall form and provide anchoring struts for guided deformation. We developed a design editor to embed these structures into custom 3D models for printing with elastic silicone resin on a consumer-grade SLA 3D printer and minimal post-printing assembly. A deformation authoring tool was also developed for users to build a machine learning-based classifier that distinguishes desired deformation behaviors using inductive sensing. Finally, we demonstrate the potential of our system through example applications, including a self-deformable steamer bowl clip, a remotely controllable gripper, and an interactive desk lamp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02911v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731459.3773331</arxiv:DOI>
      <dc:creator>Hsuanling Lee, Jiakun Yu, Shurui Zheng, Te-Yen Wu, Liang He</dc:creator>
    </item>
    <item>
      <title>Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI</title>
      <link>https://arxiv.org/abs/2601.05825</link>
      <description>arXiv:2601.05825v2 Announce Type: replace 
Abstract: Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05825v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucija Mihi\'c Zidar, Philipp Wicke, Praneel Bhatia, Rosa Lutz, Marius Klug, Thorsten O. Zander</dc:creator>
    </item>
    <item>
      <title>"What If My Face Gets Scanned Without Consent": Understanding Older Adults' Experiences with Biometric Payment</title>
      <link>https://arxiv.org/abs/2601.12300</link>
      <description>arXiv:2601.12300v2 Announce Type: replace 
Abstract: Biometric payment, i.e., biometric authentication implemented in digital payment systems, can reduce memory demands and streamline payment for older adults. However, older adults' perceptions and practices regarding biometric payment remain underexplored. We conducted semi-structured interviews with 22 Chinese older adults, including both users and non-users. Participants were motivated to use biometric payment due to convenience and perceived security. However, they also worried about loss of control due to its password-free nature and expressed concerns about biometric data security. Participants also identified desired features for biometric payment, such as lightweight and context-aware cognitive confirmation mechanisms to enhance user control. Based on these findings, we outline recommendations for more controllable and informative digital financial services that better support older adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12300v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Deng, Changyang He, Bo Li, Yixin Zou</dc:creator>
    </item>
    <item>
      <title>Experiencer, Helper, or Observer: Online Fraud Intervention for Older Adults Through Role-based Simulation</title>
      <link>https://arxiv.org/abs/2601.12324</link>
      <description>arXiv:2601.12324v2 Announce Type: replace 
Abstract: Online fraud is a critical global threat that disproportionately targets older adults. Prior anti-fraud education for older adults has largely relied on static, traditional instruction that limits engagement and real-world transfer, whereas role-based simulation offers realistic yet low-risk opportunities for practice. Moreover, most interventions situate learners as victims, overlooking that fraud encounters often involve multiple roles, such as bystanders who witness scams and helpers who support victims. To address this gap, we developed ROLESafe, an anti-fraud educational intervention in which older adults learn through different learning roles, including Experiencer (experiencing fraud), Helper (assisting a victim), and Observer (witnessing fraud). In a between-subjects study with 144 older adults in China, we found that the Experiencer and Helper roles significantly improved participants' ability to identify online fraud. These findings highlight the promise of role-based, multi-perspective simulations for enhancing fraud awareness among older adults and provide design implications for future anti-fraud education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12324v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Deng, Xiaowei Chen, Junxiang Liao, Bo Li, Yixin Zou</dc:creator>
    </item>
    <item>
      <title>Memento: Towards Proactive Visualization of Everyday Memories with Personal Wearable AR Assistant</title>
      <link>https://arxiv.org/abs/2601.17622</link>
      <description>arXiv:2601.17622v3 Announce Type: replace 
Abstract: We introduce Memento, a conversational AR assistant that permanently captures and memorizes user's verbal queries alongside their spatiotemporal and activity contexts. By storing these "memories," Memento discovers connections between users' recurring interests and the contexts that trigger them. Upon detection of similar or identical spatiotemporal activity, Memento proactively recalls user interests and delivers up-to-date responses through AR, seamlessly integrating AR experience into their daily routine. Unlike prior work, each interaction in Memento is not a transient event, but a connected series of interactions with coherent long--term perspective, tailored to the user's broader multimodal (visual, spatial, temporal, and embodied) context. We conduct a preliminary evaluation through user feedbacks with participants of diverse expertise in immersive apps, and explore the value of proactive context-aware AR assistant in everyday settings. We share our findings and challenges in designing a proactive, context-aware AR system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17622v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoonsang Kim, Yalong Yang, Arie E. Kaufman</dc:creator>
    </item>
    <item>
      <title>The Psychological Science of Artificial Intelligence: A Rapidly Emerging Field of Psychology</title>
      <link>https://arxiv.org/abs/2601.19338</link>
      <description>arXiv:2601.19338v2 Announce Type: replace 
Abstract: The psychological science of artificial intelligence (AI) can be broadly defined as an emerging field of psychology that examines all AI-related mental and behavioral processes from the perspective of psychology. This field has been growing exponentially in the recent decade. This review synthesizes the existing literature on the psychological science of AI with a goal to provide a comprehensive conceptual framework for planning, conducting, and assessing scientific research in the field. It consists of six parts, starting with an overview of the entire field of the psychological science of artificial intelligence, then synthesizing the literature in each of the four specific areas (i.e., Psychology of designing AI, psychology of using AI, AI for examining psychological processes, and AI for advancing psychological methods), and concluding with an outlook on the field in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19338v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zheng Yan, Ru-Yuan Zhang</dc:creator>
    </item>
    <item>
      <title>Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR</title>
      <link>https://arxiv.org/abs/2601.21264</link>
      <description>arXiv:2601.21264v2 Announce Type: replace 
Abstract: In time-critical eXtended reality (XR) scenarios where users must rapidly reorient their attention to hazards, alerts, or instructions while engaged in a primary task, spatial audio can provide an immediate directional cue without occupying visual bandwidth. However, such scenarios can afford only a brief auditory exposure, requiring users to interpret sound direction quickly and without extended listening or head-driven refinement. This paper reports a controlled exploratory study of rapid spatial-audio localization in XR. Using HRTF-rendered broadband stimuli presented from a semi-dense set of directions around the listener, we quantify how accurately users can infer coarse direction from brief audio alone. We further examine the effects of short-term visuo-auditory feedback training as a lightweight calibration mechanism. Our findings show that brief spatial cues can convey coarse directional information, and that even short calibration can improve users' perception of aural signals. While these results highlight the potential of spatial audio for rapid attention guidance, they also show that auditory cues alone may not provide sufficient precision for complex or high-stakes tasks, and that spatial audio may be most effective when complemented by other sensory modalities or visual cues, without relying on head-driven refinement. We leverage this study on spatial audio as a preliminary investigation into a first-stage attention-guidance channel for wearable XR (e.g., VR head-mounted displays and AR smart glasses), and provide design insights on stimulus selection and calibration for time-critical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21264v2</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoonsang Kim, Swapnil Dey, Arie Kaufman</dc:creator>
    </item>
    <item>
      <title>Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce</title>
      <link>https://arxiv.org/abs/2506.06576</link>
      <description>arXiv:2506.06576v3 Announce Type: replace-cross 
Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation "Green Light" Zone, Automation "Red Light" Zone, R&amp;D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06576v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijia Shao, Humishka Zope, Yucheng Jiang, Jiaxin Pei, David Nguyen, Erik Brynjolfsson, Diyi Yang</dc:creator>
    </item>
    <item>
      <title>Digital Simulations to Enhance Military Medical Evacuation Decision-Making</title>
      <link>https://arxiv.org/abs/2507.06373</link>
      <description>arXiv:2507.06373v3 Announce Type: replace-cross 
Abstract: Medical evacuation is one of the United States Army's most storied and critical mission sets, responsible for efficiently and expediently evacuating the battlefield ill and injured. Medical evacuation planning involves designing a robust network of medical platforms and facilities capable of moving and treating large numbers of casualties. Until now, there has not been a medium to simulate these networks in a classroom setting and evaluate both offline planning and online decision-making performance. This work describes the Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer simulation developed in Unity that replicates battlefield constraints and uncertainties. MEWI accurately models patient interactions at casualty collection points, ambulance exchange points, medical treatment facilities, and evacuation platforms. Two operational scenarios are introduced: an amphibious island assault in the Pacific and a Eurasian conflict across a sprawling road and river network. These scenarios pit students against the clock to save as many casualties as possible while adhering to doctrinal lessons learned during didactic training. We visualize performance data collected from two iterations of the MEWI Pacific scenario executed in the United States Army's Medical Evacuation Doctrine Course. We consider post-wargame Likert survey data from student participants and external observer notes to identify key planning decision points, document medical evacuation lessons learned, and quantify general utility. Results indicate that MEWI participation substantially improves uptake of medical evacuation lessons learned and co-operative decision-making. MEWI is a substantial step forward in the field of high-fidelity training tools for medical education, and our study findings offer critical insights into improving medical evacuation education and operations across the joint force.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06373v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Fischer, Mahdi Al-Husseini, Ram Krishnamoorthy, Vishal Kumar, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>SpatialViz-Bench: A Cognitively-Grounded Benchmark for Diagnosing Spatial Visualization in MLLMs</title>
      <link>https://arxiv.org/abs/2507.07610</link>
      <description>arXiv:2507.07610v5 Announce Type: replace-cross 
Abstract: Humans can imagine and manipulate visual images mentally, a capability known as spatial visualization. While many multi-modal benchmarks assess reasoning on visible visual information, the ability to infer unseen relationships through spatial visualization remains insufficiently evaluated as a spatial skill. This reliance on publicly sourced problems from IQ tests or math competitions risks data contamination and compromises assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 programmatically generated problems, a scalable framework that allows for expansion to ensure fair and continuously reliable evaluations. Our evaluation of 27 Multi-modal Large Language Models (MLLMs) reveals wide performance variations, demonstrates the benchmark's strong discriminative power, and uncovers counter-intuitive findings: Chain-of-Thought (CoT) prompting paradoxically degrades accuracy on open-source models. Through statistical and qualitative analysis of error types, SpatialViz-Bench demonstrates that state-of-the-art MLLMs exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark data and evaluation code are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07610v5</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang, Jun Wang</dc:creator>
    </item>
    <item>
      <title>Incorporating AI incident reporting into telecommunications law and policy: Insights from India</title>
      <link>https://arxiv.org/abs/2509.09508</link>
      <description>arXiv:2509.09508v2 Announce Type: replace-cross 
Abstract: The integration of artificial intelligence (AI) into telecommunications infrastructure introduces novel risks, such as algorithmic bias and unpredictable system behavior, that fall outside the scope of traditional cybersecurity and data protection frameworks. This paper introduces a precise definition and a detailed typology of telecommunications AI incidents, establishing them as a distinct category of risk that extends beyond conventional cybersecurity and data protection breaches. It argues for their recognition as a distinct regulatory concern. Using India as a case study for jurisdictions that lack a horizontal AI law, the paper analyzes the country's key digital regulations. The analysis reveals that India's existing legal instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data breaches, creating a significant regulatory gap for AI-specific operational incidents, such as performance degradation and algorithmic bias. The paper also examines structural barriers to disclosure and the limitations of existing AI incident repositories. Based on these findings, the paper proposes targeted policy recommendations centered on integrating AI incident reporting into India's existing telecom governance. Key proposals include mandating reporting for high-risk AI failures, designating an existing government body as a nodal agency to manage incident data, and developing standardized reporting frameworks. These recommendations aim to enhance regulatory clarity and strengthen long-term resilience, offering a pragmatic and replicable blueprint for other nations seeking to govern AI risks within their existing sectoral frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09508v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clsr.2026.106263</arxiv:DOI>
      <arxiv:journal_reference>Computer Law &amp; Security Review, Volume 60, April 2026, 106263</arxiv:journal_reference>
      <dc:creator>Avinash Agarwal, Manisha J. Nene</dc:creator>
    </item>
    <item>
      <title>BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation</title>
      <link>https://arxiv.org/abs/2510.13853</link>
      <description>arXiv:2510.13853v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been successfully applied to many tasks, including text-to-SQL generation. However, much of this work has focused on publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work showed that LLMs are much less effective in querying large private enterprise data warehouses and released Beaver, the first private enterprise text-to-SQL benchmark. To create Beaver, we leveraged SQL logs, which are often readily available. However, manually annotating these logs to identify which natural language questions they answer is a daunting task. Asking database administrators, who are highly trained experts, to take on additional work to construct and validate corresponding natural language utterances is not only challenging but also quite costly. To address this challenge, we introduce BenchPress, a human-in-the-loop system designed to accelerate the creation of domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses retrieval-augmented generation (RAG) and LLMs to propose multiple natural language descriptions. Human experts then select, rank, or edit these drafts to ensure accuracy and domain alignment. We evaluated BenchPress on annotated enterprise SQL logs, demonstrating that LLM-assisted annotation drastically reduces the time and effort required to create high-quality benchmarks. Our results show that combining human verification with LLM-generated suggestions enhances annotation accuracy, benchmark reliability, and model evaluation robustness. By streamlining the creation of custom benchmarks, BenchPress offers researchers and practitioners a mechanism for assessing text-to-SQL models on a given domain-specific workload. BenchPress is freely available via our public GitHub repository at https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our website at http://dsg-mcgraw.csail.mit.edu:5000.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13853v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Wenz, Omar Bouattour, Devin Yang, Justin Choi, Cecil Gregg, Nesime Tatbul, \c{C}a\u{g}atay Demiralp</dc:creator>
    </item>
    <item>
      <title>Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics</title>
      <link>https://arxiv.org/abs/2601.06552</link>
      <description>arXiv:2601.06552v2 Announce Type: replace-cross 
Abstract: Whenever humans and robots work together, it is essential that unexpected robot behavior can be explained to the user. Especially in applications such as shared control the user and the robot must share the same model of the objects in the world, and the actions that can be performed on these objects.
  In this paper, we achieve this with a so-called model reconciliation framework. We leverage a Large Language Model to predict and explain the difference between the robot's and the human's mental models, without the need of a formal mental model of the user. Furthermore, our framework aims to solve the model divergence after the explanation by allowing the human to correct the robot. We provide an implementation in an assistive robotics domain, where we conduct a set of experiments with a real wheelchair-based mobile manipulator and its digital twin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06552v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Britt Besch, Tai Mai, Jeremias Thun, Markus Huff, J\"orn Vogel, Freek Stulp, Samuel Bustamante</dc:creator>
    </item>
    <item>
      <title>How AI Impacts Skill Formation</title>
      <link>https://arxiv.org/abs/2601.20245</link>
      <description>arXiv:2601.20245v2 Announce Type: replace-cross 
Abstract: AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20245v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Judy Hanwen Shen, Alex Tamkin</dc:creator>
    </item>
  </channel>
</rss>
