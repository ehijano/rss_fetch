<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Oct 2024 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accurate Inference for Penalized Logistic Regression</title>
      <link>https://arxiv.org/abs/2410.20045</link>
      <description>arXiv:2410.20045v1 Announce Type: new 
Abstract: Inference for high-dimensional logistic regression models using penalized methods has been a challenging research problem. As an illustration, a major difficulty is the significant bias of the Lasso estimator, which limits its direct application in inference. Although various bias corrected Lasso estimators have been proposed, they often still exhibit substantial biases in finite samples, undermining their inference performance. These finite sample biases become particularly problematic in one-sided inference problems, such as one-sided hypothesis testing. This paper proposes a novel two-step procedure for accurate inference in high-dimensional logistic regression models. In the first step, we propose a Lasso-based variable selection method to select a suitable submodel of moderate size for subsequent inference. In the second step, we introduce a bias corrected estimator to fit the selected submodel. We demonstrate that the resulting estimator from this two-step procedure has a small bias order and enables accurate inference. Numerical studies and an analysis of alcohol consumption data are included, where our proposed method is compared to alternative approaches. Our results indicate that the proposed method exhibits significantly smaller biases than alternative methods in finite samples, thereby leading to improved inference performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20045v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuming Zhang, St\'ephane Guerrier, Runze Li</dc:creator>
    </item>
    <item>
      <title>Functional Mixture Regression Control Chart</title>
      <link>https://arxiv.org/abs/2410.20138</link>
      <description>arXiv:2410.20138v1 Announce Type: new 
Abstract: Industrial applications often exhibit multiple in-control patterns due to varying operating conditions, which makes a single functional linear model (FLM) inadequate to capture the complexity of the true relationship between a functional quality characteristic and covariates, which gives rise to the multimode profile monitoring problem. This issue is clearly illustrated in the resistance spot welding (RSW) process in the automotive industry, where different operating conditions lead to multiple in-control states. In these states, factors such as electrode tip wear and dressing may influence the functional quality characteristic differently, resulting in distinct FLMs across subpopulations. To address this problem, this article introduces the functional mixture regression control chart (FMRCC) to monitor functional quality characteristics with multiple in-control patterns and covariate information, modeled using a mixture of FLMs. A monitoring strategy based on the likelihood ratio test is proposed to monitor any deviation from the estimated in-control heterogeneous population. An extensive Monte Carlo simulation study is performed to compare the FMRCC with competing monitoring schemes that have already appeared in the literature, and a case study in the monitoring of an RSW process in the automotive industry, which motivated this research, illustrates its practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20138v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Capezza, Fabio Centofanti, Davide Forcina, Antonio Lepore, Biagio Palumbo</dc:creator>
    </item>
    <item>
      <title>Robust Bayes-assisted Confidence Regions</title>
      <link>https://arxiv.org/abs/2410.20169</link>
      <description>arXiv:2410.20169v1 Announce Type: new 
Abstract: The Frequentist, Assisted by Bayes (FAB) framework aims to construct confidence regions that leverage information about parameter values in the form of a prior distribution. FAB confidence regions (FAB-CRs) have smaller volume for values of the parameter that are likely under the prior, while maintaining exact frequentist coverage. This work introduces several methodological and theoretical contributions to the FAB framework. For Gaussian likelihoods, we show that the posterior mean of the parameter of interest is always contained in the FAB-CR. As such, the posterior mean constitutes a natural notion of FAB estimator to be reported alongside the FAB-CR. More generally, we show that for a likelihood in the natural exponential family, a transformation of the posterior mean of the natural parameter is always contained in the FAB-CR. For Gaussian likelihoods, we show that power law tails conditions on the marginal likelihood induce robust FAB-CRs, that are uniformly bounded and revert to the standard frequentist confidence intervals for extreme observations. We translate this result into practice by proposing a class of shrinkage priors for the FAB framework that satisfy this condition without sacrificing analytical tractability. The resulting FAB estimators are equal to prominent Bayesian shrinkage estimators, including the horseshoe estimator, thereby establishing insightful connections between robust FAB-CRs and Bayesian shrinkage methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20169v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Cortinovis, Fran\c{c}ois Caron</dc:creator>
    </item>
    <item>
      <title>scpQCA: Enhancing mvQCA Applications through Set-Covering-Based QCA Method</title>
      <link>https://arxiv.org/abs/2410.20208</link>
      <description>arXiv:2410.20208v1 Announce Type: new 
Abstract: In fields such as sociology, political science, public administration, and business management, particularly in the direction of international relations, Qualitative Comparative Analysis (QCA) has been widely adopted as a research method. This article addresses the limitations of the QCA method in its application, specifically in terms of low coverage, factor limitations, and value limitations. scpQCA enhances the coverage of results and expands the tolerance of the QCA method for multi-factor and multi-valued analyses by maintaining the consistency threshold. To validate these capabilities, we conducted experiments on both random data and specific case datasets, utilizing different approaches of CCM (Configurational Comparative Methods) such as scpQCA, CNA, and QCApro, and presented the different results. In addition, the robustness of scpQCA has been examined from the perspectives of internal and external across different case datasets, thereby demonstrating its extensive applicability and advantages over existing QCA algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20208v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manqing Fu</dc:creator>
    </item>
    <item>
      <title>High-dimensional partial linear model with trend filtering</title>
      <link>https://arxiv.org/abs/2410.20319</link>
      <description>arXiv:2410.20319v1 Announce Type: new 
Abstract: We study the high-dimensional partial linear model, where the linear part has a high-dimensional sparse regression coefficient and the nonparametric part includes a function whose derivatives are of bounded total variation. We expand upon the univariate trend filtering to develop partial linear trend filtering--a doubly penalized least square estimation approach based on $\ell_1$ penalty and total variation penalty. Analogous to the advantages of trend filtering in univariate nonparametric regression, partial linear trend filtering not only can be efficiently computed, but also achieves the optimal error rate for estimating the nonparametric function. This in turn leads to the oracle rate for the linear part as if the underlying nonparametric function were known. We compare the proposed approach with a standard smoothing spline based method, and show both empirically and theoretically that the former outperforms the latter when the underlying function possesses heterogeneous smoothness. We apply our approach to the IDATA study to investigate the relationship between metabolomic profiles and ultra-processed food (UPF) intake, efficiently identifying key metabolites associated with UPF consumption and demonstrating strong predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20319v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sang Kyu Lee, Hyokyoung G. Hong, Haolei Weng, Erikka Loftfield</dc:creator>
    </item>
    <item>
      <title>A Robust Topological Framework for Detecting Regime Changes in Multi-Trial Experiments with Application to Predictive Maintenance</title>
      <link>https://arxiv.org/abs/2410.20443</link>
      <description>arXiv:2410.20443v1 Announce Type: new 
Abstract: We present a general and flexible framework for detecting regime changes in complex, non-stationary data across multi-trial experiments. Traditional change point detection methods focus on identifying abrupt changes within a single time series (single trial), targeting shifts in statistical properties such as the mean, variance, and spectrum over time within that sole trial. In contrast, our approach considers changes occurring across trials, accommodating changes that may arise within individual trials due to experimental inconsistencies, such as varying delays or event duration. By leveraging diverse metrics to analyze time-frequency characteristics specifically topological changes in the spectrum and spectrograms, our approach offers a comprehensive framework for detecting such variations. Our approach can handle different scenarios with various statistical assumptions, including varying levels of stationarity within and across trials, making our framework highly adaptable. We validate our approach through simulations using time-varying autoregressive processes that exhibit different regime changes. Our results demonstrate the effectiveness of detecting changes across trials under diverse conditions. Furthermore, we illustrate the effectiveness of our method by applying it to predictive maintenance using the NASA bearing dataset. By analyzing the time-frequency characteristics of vibration signals recorded by accelerometers, our approach accurately identifies bearing failures, showcasing its strong potential for early fault detection in mechanical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20443v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anass B. El-Yaagoubi, Jean-Marc Freyermuth, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>The Curious Problem of the Normal Inverse Mean</title>
      <link>https://arxiv.org/abs/2410.20641</link>
      <description>arXiv:2410.20641v1 Announce Type: new 
Abstract: In astronomical observations, the estimation of distances from parallaxes is a challenging task due to the inherent measurement errors and the non-linear relationship between the parallax and the distance. This study leverages ideas from robust Bayesian inference to tackle these challenges, investigating a broad class of prior densities for estimating distances with a reduced bias and variance. Through theoretical analysis, simulation experiments, and the application to data from the Gaia Data Release 1 (GDR1), we demonstrate that heavy-tailed priors provide more reliable distance estimates, particularly in the presence of large fractional parallax errors. Theoretical results highlight the "curse of a single observation," where the likelihood dominates the posterior, limiting the impact of the prior. Nevertheless, heavy-tailed priors can delay the explosion of posterior risk, offering a more robust framework for distance estimation. The findings suggest that reciprocal invariant priors, with polynomial decay in their tails, such as the Half-Cauchy and Product Half-Cauchy, are particularly well-suited for this task, providing a balance between bias reduction and variance control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20641v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Uttaran Chatterjee, Jyotishka Datta</dc:creator>
    </item>
    <item>
      <title>Statistical Inference in High-dimensional Poisson Regression with Applications to Mediation Analysis</title>
      <link>https://arxiv.org/abs/2410.20671</link>
      <description>arXiv:2410.20671v1 Announce Type: new 
Abstract: Large-scale datasets with count outcome variables are widely present in various applications, and the Poisson regression model is among the most popular models for handling count outcomes. This paper considers the high-dimensional sparse Poisson regression model and proposes bias-corrected estimators for both linear and quadratic transformations of high-dimensional regression vectors. We establish the asymptotic normality of the estimators, construct asymptotically valid confidence intervals, and conduct related hypothesis testing. We apply the devised methodology to high-dimensional mediation analysis with count outcome, with particular application of testing for the existence of interaction between the treatment variable and high-dimensional mediators. We demonstrate the proposed methods through extensive simulation studies and application to real-world epigenetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20671v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prabrisha Rakshit, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Making all pairwise comparisons in multi-arm clinical trials without control treatment</title>
      <link>https://arxiv.org/abs/2410.20908</link>
      <description>arXiv:2410.20908v1 Announce Type: new 
Abstract: The standard paradigm for confirmatory clinical trials is to compare experimental treatments with a control, for example the standard of care or a placebo. However, it is not always the case that a suitable control exists. Efficient statistical methodology is well studied in the setting of randomised controlled trials. This is not the case if one wishes to compare several experimental with no control arm. We propose hypothesis testing methods suitable for use in such a setting. These methods are efficient, ensuring the error rate is controlled at exactly the desired rate with no conservatism. This in turn yields an improvement in power when compared with standard methods one might otherwise consider using, such as a Bonferroni adjustment. The proposed testing procedure is also highly flexible. We show how it may be extended for use in multi-stage adaptive trials, covering the majority of scenarios in which one might consider the use of such procedures in the clinical trials setting. With such a highly flexible nature, these methods may also be applied more broadly outside of a clinical trials setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20908v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Burnett, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>On Spatio-Temporal Stochastic Frontier Models</title>
      <link>https://arxiv.org/abs/2410.20915</link>
      <description>arXiv:2410.20915v1 Announce Type: new 
Abstract: In the literature on stochastic frontier models until the early 2000s, the joint consideration of spatial and temporal dimensions was often inadequately addressed, if not completely neglected. However, from an evolutionary economics perspective, the production process of the decision-making units constantly changes over both dimensions: it is not stable over time due to managerial enhancements and/or internal or external shocks, and is influenced by the nearest territorial neighbours. This paper proposes an extension of the Fusco and Vidoli [2013] SEM-like approach, which globally accounts for spatial and temporal effects in the term of inefficiency. In particular, coherently with the stochastic panel frontier literature, two different versions of the model are proposed: the time-invariant and the time-varying spatial stochastic frontier models. In order to evaluate the inferential properties of the proposed estimators, we first run Monte Carlo experiments and we then present the results of an application to a set of commonly referenced data, demonstrating robustness and stability of estimates across all scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20915v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elisa Fusco, Giuseppe Arbia, Francesco Vidoli, Vincenzo Nardelli</dc:creator>
    </item>
    <item>
      <title>Almost goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2410.20918</link>
      <description>arXiv:2410.20918v1 Announce Type: new 
Abstract: We introduce the almost goodness-of-fit test, a procedure to decide if a (parametric) model provides a good representation of the probability distribution generating the observed sample. We consider the approximate model determined by an M-estimator of the parameters as the best representative of the unknown distribution within the parametric class. The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value, the margin of error. The methodology also allows quantifying the percentage improvement of the proposed model compared to a non-informative (constant) one. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and the corresponding one of the estimated (parametric) model. The value of the parameter $p$ allows modulating the impact of the tails of the distribution in the validation of the model. By deriving the asymptotic distribution of the test statistic, as well as proving the consistency of its bootstrap approximation, we present an easy-to-implement and flexible method. The performance of the proposal is illustrated with a simulation study and the analysis of a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20918v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amparo Ba\'illo, Javier C\'arcamo</dc:creator>
    </item>
    <item>
      <title>Single CASANOVA? Not in multiple comparisons</title>
      <link>https://arxiv.org/abs/2410.21098</link>
      <description>arXiv:2410.21098v1 Announce Type: new 
Abstract: When comparing multiple groups in clinical trials, we are not only interested in whether there is a difference between any groups but rather the location. Such research questions lead to testing multiple individual hypotheses. To control the familywise error rate (FWER), we must apply some corrections or introduce tests that control the FWER by design. In the case of time-to-event data, a Bonferroni-corrected log-rank test is commonly used. This approach has two significant drawbacks: (i) it loses power when the proportional hazards assumption is violated [1] and (ii) the correction generally leads to a lower power, especially when the test statistics are not independent [2]. We propose two new tests based on combined weighted log-rank tests. One as a simple multiple contrast test of weighted log-rank tests and one as an extension of the so-called CASANOVA test [3]. The latter was introduced for factorial designs. We propose a new multiple contrast test based on the CASANOVA approach. Our test promises to be more powerful under crossing hazards and eliminates the need for additional p-value correction. We assess the performance of our tests through extensive Monte Carlo simulation studies covering both proportional and non-proportional hazard scenarios. Finally, we apply the new and reference methods to a real-world data example. The new approaches control the FWER and show reasonable power in all scenarios. They outperform the adjusted approaches in some non-proportional settings in terms of power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21098v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ina Dormuth, Carolin Herrmann, Frank Konietschke, Markus Pauly, Matthias Wirth, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>Topological Identification of Agent Status in Information Contagions: Application to Financial Markets</title>
      <link>https://arxiv.org/abs/2410.21104</link>
      <description>arXiv:2410.21104v1 Announce Type: new 
Abstract: Cascade models serve as effective tools for understanding the propagation of information and diseases within social networks. Nevertheless, their applicability becomes constrained when the states of the agents (nodes) are hidden and can only be inferred through indirect observations or symptoms. This study proposes a Mapper-based strategy to infer the status of agents within a hidden information cascade model using expert knowledge. To verify and demonstrate the method we identify agents who are likely to take advantage of information obtained from an inside information network. We do this using data on insider networks and stock market transactions. Recognizing the sensitive nature of allegations of insider trading, we design a conservative approach to minimize false positives, ensuring that innocent agents are not wrongfully implicated. The Mapper-based results systematically outperform other methods, such as clustering and unsupervised anomaly detection, on synthetic data. We also apply the method to empirical data and verify the results using a statistical validation method based on persistence homology. Our findings highlight that the proposed Mapper-based technique successfully identifies a subpopulation of opportunistic agents within the information cascades. The adaptability of this method to diverse data types and sizes is demonstrated, with potential for tailoring for specific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21104v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anubha Goel, Henri Hansen, Juho Kanniainen</dc:creator>
    </item>
    <item>
      <title>A Componentwise Estimation Procedure for Multivariate Location and Scatter: Robustness, Efficiency and Scalability</title>
      <link>https://arxiv.org/abs/2410.21166</link>
      <description>arXiv:2410.21166v1 Announce Type: new 
Abstract: Covariance matrix estimation is an important problem in multivariate data analysis, both from theoretical as well as applied points of view. Many simple and popular covariance matrix estimators are known to be severely affected by model misspecification and the presence of outliers in the data; on the other hand robust estimators with reasonably high efficiency are often computationally challenging for modern large and complex datasets. In this work, we propose a new, simple, robust and highly efficient method for estimation of the location vector and the scatter matrix for elliptically symmetric distributions. The proposed estimation procedure is designed in the spirit of the minimum density power divergence (DPD) estimation approach with appropriate modifications which makes our proposal (sequential minimum DPD estimation) computationally very economical and scalable to large as well as higher dimensional datasets. Consistency and asymptotic normality of the proposed sequential estimators of the multivariate location and scatter are established along with asymptotic positive definiteness of the estimated scatter matrix. Robustness of our estimators are studied by means of influence functions. All theoretical results are illustrated further under multivariate normality. A large-scale simulation study is presented to assess finite sample performances and scalability of our method in comparison to the usual maximum likelihood estimator (MLE), the ordinary minimum DPD estimator (MDPDE) and other popular non-parametric methods. The applicability of our method is further illustrated with a real dataset on credit card transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21166v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Chakraborty, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Spatial causal inference in the presence of preferential sampling to study the impacts of marine protected areas</title>
      <link>https://arxiv.org/abs/2410.21213</link>
      <description>arXiv:2410.21213v1 Announce Type: new 
Abstract: Marine Protected Areas (MPAs) have been established globally to conserve marine resources. Given their maintenance costs and impact on commercial fishing, it is critical to evaluate their effectiveness to support future conservation. In this paper, we use data collected from the Australian coast to estimate the effect of MPAs on biodiversity. Environmental studies such as these are often observational, and processes of interest exhibit spatial dependence, which presents challenges in estimating the causal effects. Spatial data can also be subject to preferential sampling, where the sampling locations are related to the response variable, further complicating inference and prediction. To address these challenges, we propose a spatial causal inference method that simultaneously accounts for unmeasured spatial confounders in both the sampling process and the treatment allocation. We prove the identifiability of key parameters in the model and the consistency of the posterior distributions of those parameters. We show via simulation studies that the causal effect of interest can be reliably estimated under the proposed model. The proposed method is applied to assess the effect of MPAs on fish biomass. We find evidence of preferential sampling and that properly accounting for this source of bias impacts the estimate of the causal effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21213v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongjae Son, Brian J. Reich, Erin M. Schliep, Shu Yang, David A. Gill</dc:creator>
    </item>
    <item>
      <title>Adaptive Transfer Clustering: A Unified Framework</title>
      <link>https://arxiv.org/abs/2410.21263</link>
      <description>arXiv:2410.21263v1 Announce Type: new 
Abstract: We propose a general transfer learning framework for clustering given a main dataset and an auxiliary one about the same subjects. The two datasets may reflect similar but different latent grouping structures of the subjects. We propose an adaptive transfer clustering (ATC) algorithm that automatically leverages the commonality in the presence of unknown discrepancy, by optimizing an estimated bias-variance decomposition. It applies to a broad class of statistical models including Gaussian mixture models, stochastic block models, and latent class models. A theoretical analysis proves the optimality of ATC under the Gaussian mixture model and explicitly quantifies the benefit of transfer. Extensive simulations and real data experiments confirm our method's effectiveness in various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21263v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqi Gu, Zhongyuan Lyu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2410.19780</link>
      <description>arXiv:2410.19780v1 Announce Type: cross 
Abstract: We propose a scalable kinetic Langevin dynamics algorithm for sampling parameter spaces of big data and AI applications. Our scheme combines a symmetric forward/backward sweep over minibatches with a symmetric discretization of Langevin dynamics. For a particular Langevin splitting method (UBU), we show that the resulting Symmetric Minibatch Splitting-UBU (SMS-UBU) integrator has bias $O(h^2 d^{1/2})$ in dimension $d&gt;0$ with stepsize $h&gt;0$, despite only using one minibatch per iteration, thus providing excellent control of the sampling bias as a function of the stepsize. We apply the algorithm to explore local modes of the posterior distribution of Bayesian neural networks (BNNs) and evaluate the calibration performance of the posterior predictive probabilities for neural networks with convolutional neural network architectures for classification problems on three different datasets (Fashion-MNIST, Celeb-A and chest X-ray). Our results indicate that BNNs sampled with SMS-UBU can offer significantly better calibration performance compared to standard methods of training and stochastic weight averaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19780v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Paulin, Peter A. Whalley, Neil K. Chada, Benedict Leimkuhler</dc:creator>
    </item>
    <item>
      <title>Language Agents Meet Causality -- Bridging LLMs and Causal World Models</title>
      <link>https://arxiv.org/abs/2410.19923</link>
      <description>arXiv:2410.19923v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLs with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19923v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Gkountouras, Matthias Lindemann, Phillip Lippe, Efstratios Gavves, Ivan Titov</dc:creator>
    </item>
    <item>
      <title>L\'evy graphical models</title>
      <link>https://arxiv.org/abs/2410.19952</link>
      <description>arXiv:2410.19952v1 Announce Type: cross 
Abstract: Conditional independence and graphical models are crucial concepts for sparsity and statistical modeling in higher dimensions. For L\'evy processes, a widely applied class of stochastic processes, these notions have not been studied. By the L\'evy-It\^o decomposition, a multivariate L\'evy process can be decomposed into the sum of a Brownian motion part and an independent jump process. We show that conditional independence statements between the marginal processes can be studied separately for these two parts. While the Brownian part is well-understood, we derive a novel characterization of conditional independence between the sample paths of the jump process in terms of the L\'evy measure. We define L\'evy graphical models as L\'evy processes that satisfy undirected or directed Markov properties. We prove that the graph structure is invariant under changes of the univariate marginal processes. L\'evy graphical models allow the construction of flexible, sparse dependence models for L\'evy processes in large dimensions, which are interpretable thanks to the underlying graph. For trees, we develop statistical methodology to learn the underlying structure from low- or high-frequency observations of the L\'evy process and show consistent graph recovery. We apply our method to model stock returns from U.S. companies to illustrate the advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19952v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Jevgenijs Ivanovs, Jakob D. Th{\o}stesen</dc:creator>
    </item>
    <item>
      <title>Sample Efficient Bayesian Learning of Causal Graphs from Interventions</title>
      <link>https://arxiv.org/abs/2410.20089</link>
      <description>arXiv:2410.20089v1 Announce Type: cross 
Abstract: Causal discovery is a fundamental problem with applications spanning various areas in science and engineering. It is well understood that solely using observational data, one can only orient the causal graph up to its Markov equivalence class, necessitating interventional data to learn the complete causal graph. Most works in the literature design causal discovery policies with perfect interventions, i.e., they have access to infinite interventional samples. This study considers a Bayesian approach for learning causal graphs with limited interventional samples, mirroring real-world scenarios where such samples are usually costly to obtain. By leveraging the recent result of Wien\"obst et al. (2023) on uniform DAG sampling in polynomial time, we can efficiently enumerate all the cut configurations and their corresponding interventional distributions of a target set, and further track their posteriors. Given any number of interventional samples, our proposed algorithm randomly intervenes on a set of target vertices that cut all the edges in the graph and returns a causal graph according to the posterior of each target set. When the number of interventional samples is large enough, we show theoretically that our proposed algorithm will return the true causal graph with high probability. We compare our algorithm against various baseline methods on simulated datasets, demonstrating its superior accuracy measured by the structural Hamming distance between the learned DAG and the ground truth. Additionally, we present a case study showing how this algorithm could be modified to answer more general causal questions without learning the whole graph. As an example, we illustrate that our method can be used to estimate the causal effect of a variable that cannot be intervened.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20089v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Zhou, Muhammad Qasim Elahi, Murat Kocaoglu</dc:creator>
    </item>
    <item>
      <title>Low-rank Bayesian matrix completion via geodesic Hamiltonian Monte Carlo on Stiefel manifolds</title>
      <link>https://arxiv.org/abs/2410.20318</link>
      <description>arXiv:2410.20318v1 Announce Type: cross 
Abstract: We present a new sampling-based approach for enabling efficient computation of low-rank Bayesian matrix completion and quantifying the associated uncertainty. Firstly, we design a new prior model based on the singular-value-decomposition (SVD) parametrization of low-rank matrices. Our prior is analogous to the seminal nuclear-norm regularization used in non-Bayesian setting and enforces orthogonality in the factor matrices by constraining them to Stiefel manifolds. Then, we design a geodesic Hamiltonian Monte Carlo (-within-Gibbs) algorithm for generating posterior samples of the SVD factor matrices. We demonstrate that our approach resolves the sampling difficulties encountered by standard Gibbs samplers for the common two-matrix factorization used in matrix completion. More importantly, the geodesic Hamiltonian sampler allows for sampling in cases with more general likelihoods than the typical Gaussian likelihood and Gaussian prior assumptions adopted in most of the existing Bayesian matrix completion literature. We demonstrate an applications of our approach to fit the categorical data of a mice protein dataset and the MovieLens recommendation problem. Numerical examples demonstrate superior sampling performance, including better mixing and faster convergence to a stationary distribution. Moreover, they demonstrate improved accuracy on the two real-world benchmark problems we considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20318v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiangang Cui, Alex Gorodetsky</dc:creator>
    </item>
    <item>
      <title>A Distributed Lag Approach to the Generalised Dynamic Factor Model (GDFM)</title>
      <link>https://arxiv.org/abs/2410.20885</link>
      <description>arXiv:2410.20885v1 Announce Type: cross 
Abstract: We provide estimation and inference for the Generalised Dynamic Factor Model (GDFM) under the assumption that the dynamic common component can be expressed in terms of a finite number of lags of contemporaneously pervasive factors. The proposed estimator is simply an OLS regression of the observed variables on factors extracted via static principal components and therefore avoids frequency domain techniques entirely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20885v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Gersing</dc:creator>
    </item>
    <item>
      <title>BSD: a Bayesian framework for parametric models of neural spectra</title>
      <link>https://arxiv.org/abs/2410.20896</link>
      <description>arXiv:2410.20896v1 Announce Type: cross 
Abstract: The analysis of neural power spectra plays a crucial role in understanding brain function and dysfunction. While recent efforts have led to the development of methods for decomposing spectral data, challenges remain in performing statistical analysis and group-level comparisons. Here, we introduce Bayesian Spectral Decomposition (BSD), a Bayesian framework for analysing neural spectral power. BSD allows for the specification, inversion, comparison, and analysis of parametric models of neural spectra, addressing limitations of existing methods. We first establish the face validity of BSD on simulated data and show how it outperforms an established method (\fooof{}) for peak detection on artificial spectral data. We then demonstrate the efficacy of BSD on a group-level study of EEG spectra in 204 healthy subjects from the LEMON dataset. Our results not only highlight the effectiveness of BSD in model selection and parameter estimation, but also illustrate how BSD enables straightforward group-level regression of the effect of continuous covariates such as age. By using Bayesian inference techniques, BSD provides a robust framework for studying neural spectral data and their relationship to brain function and dysfunction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20896v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan Medrano, Nicholas A. Alexander, Robert A. Seymour, Peter Zeidman</dc:creator>
    </item>
    <item>
      <title>A novel decomposition to explain heterogeneity in observational and randomized studies of causality</title>
      <link>https://arxiv.org/abs/2208.05543</link>
      <description>arXiv:2208.05543v2 Announce Type: replace 
Abstract: This paper introduces a novel decomposition framework to explain heterogeneity in causal effects observed across different studies, considering both observational and randomized settings. We present a formal decomposition of between-study heterogeneity, identifying sources of variability in treatment effects across studies. The proposed methodology allows for robust estimation of causal parameters under various assumptions, addressing differences in pre-treatment covariate distributions, mediating variables, and the outcome mechanism. Our approach is validated through a simulation study and applied to data from the Moving to Opportunity (MTO) study, demonstrating its practical relevance. This work contributes to the broader understanding of causal inference in multi-study environments, with potential applications in evidence synthesis and policy-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.05543v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brian Gilbert, Ivan D{\i}az, Kara E. Rudolph, Tat-Thang Vo</dc:creator>
    </item>
    <item>
      <title>A General Framework for Cutting Feedback within Modularised Bayesian Inference</title>
      <link>https://arxiv.org/abs/2211.03274</link>
      <description>arXiv:2211.03274v3 Announce Type: replace 
Abstract: Standard Bayesian inference can build models that combine information from various sources, but this inference may not be reliable if components of a model are misspecified. Cut inference, as a particular type of modularized Bayesian inference, is an alternative which splits a model into modules and cuts the feedback from the suspect module. Previous studies have focused on a two-module case, but a more general definition of a "module" remains unclear. We present a formal definition of a "module" and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation satisfying this condition to the joint distribution in the Kullback-Leibler divergence. We also extend cut inference for the two-module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.03274v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Robert J. B. Goudie</dc:creator>
    </item>
    <item>
      <title>Conformal prediction with local weights: randomization enables local guarantees</title>
      <link>https://arxiv.org/abs/2310.07850</link>
      <description>arXiv:2310.07850v5 Announce Type: replace 
Abstract: In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building such intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally we would want to have local coverage guarantees that hold for each possible value of the test point's features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favourable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We propose randomly localized conformal prediction (RLCP), a method that builds on localized CP and weighted CP techniques to return prediction intervals that are not only marginally valid but also offer relaxed local coverage guarantees and validity under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07850v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Hore, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Double Debiased Covariate Shift Adaptation Robust to Density-Ratio Estimation</title>
      <link>https://arxiv.org/abs/2310.16638</link>
      <description>arXiv:2310.16638v3 Announce Type: replace 
Abstract: Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's parameters of interest. To mitigate these challenges, we introduce a doubly robust estimator for covariate shift adaptation via importance weighting, which incorporates an additional estimator for the regression function. Leveraging double machine learning techniques, our estimator reduces the bias arising from the density ratio estimation errors. We demonstrate the asymptotic distribution of the regression parameter estimator. Notably, our estimator remains consistent if either the density ratio estimator or the regression function is consistent, showcasing its robustness against potential errors in density ratio estimation. Finally, we confirm the soundness of our proposed method via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16638v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Kota Matsui, Ryo Inokuchi</dc:creator>
    </item>
    <item>
      <title>FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings</title>
      <link>https://arxiv.org/abs/2311.16984</link>
      <description>arXiv:2311.16984v5 Announce Type: replace 
Abstract: External control arms (ECA) can inform the early clinical development of experimental drugs and provide efficacy evidence for regulatory approval. However, the main challenge in implementing ECA lies in accessing real-world or historical clinical trials data. Indeed, regulations protecting patients' rights by strictly controlling data processing make pooling data from multiple sources in a central server often difficult. To address these limitations, we develop a new method, 'FedECA' that leverages federated learning (FL) to enable inverse probability of treatment weighting (IPTW) for time-to-event outcomes on separate cohorts without needing to pool data. To showcase the potential of FedECA, we apply it in different settings of increasing complexity culminating with a real-world use-case in which FedECA provides evidence for a differential effect between two drugs that would have otherwise gone unnoticed. By sharing our code, we hope FedECA will foster the creation of federated research networks and thus accelerate drug development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16984v5</guid>
      <category>stat.ME</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Ta\"ieb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, J\'erome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, F\'elix Balazard, Mathieu Andreux</dc:creator>
    </item>
    <item>
      <title>Random Interval Distillation for Detection of Change-Points in Markov Chain Bernoulli Networks</title>
      <link>https://arxiv.org/abs/2403.00600</link>
      <description>arXiv:2403.00600v2 Announce Type: replace 
Abstract: We propose a new and generic approach for detecting multiple change-points in dynamic networks with Markov formation, termed random interval distillation (RID). By collecting random intervals with sufficient strength of signals and reassembling them into a sequence of informative short intervals, together with sparse universal singular value thresholding, our new approach can achieve nearly minimax optimality as their independent counterparts for both detection and localization bounds in low-rank networks without any prior knowledge about minimal spacing, which is unlike many previous methods. In particular, motivated by a recent nonasymptotic bound, our method uses the operator norm of CUSUMs of the adjacency matrices, and achieves the aforementioned optimality without sample splitting as required by the previous method. For practical applications, we introduce a clustering-based and data-driven procedure to determine the optimal threshold for signal strength, utilizing the connection between RID and clustering. We examine the effectiveness and usefulness of our methodology via simulations and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00600v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyuan Fan, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Two-phase rejective sampling and its asymptotic properties</title>
      <link>https://arxiv.org/abs/2403.01477</link>
      <description>arXiv:2403.01477v2 Announce Type: replace 
Abstract: Rejective sampling improves design and estimation efficiency of single-phase sampling when auxiliary information in a finite population is available. When such auxiliary information is unavailable, we propose to use two-phase rejective sampling (TPRS), which involves measuring auxiliary variables for the sample of units in the first phase, followed by the implementation of rejective sampling for the outcome in the second phase. We explore the asymptotic design properties of double expansion and regression estimators under TPRS. We show that TPRS enhances the efficiency of the double expansion estimator, rendering it comparable to a regression estimator. We further refine the design to accommodate varying importance of covariates and extend it to multi-phase sampling. We start with the theory for the population mean and then extend the theory to parameters defined by general estimating equations. Our asymptotic results for TPRS immediately cover the existing single-phase rejective sampling, under which the asymptotic theory has not been fully established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01477v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Yang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>An Adaptive Multivariate Functional EWMA Control Chart</title>
      <link>https://arxiv.org/abs/2403.03837</link>
      <description>arXiv:2403.03837v2 Announce Type: replace 
Abstract: In many modern industrial scenarios, the measurements of the quality characteristics of interest are often required to be represented as functional data or profiles. This motivates the growing interest in extending traditional univariate statistical process monitoring (SPM) schemes to the functional data setting. This article proposes a new SPM scheme, which is referred to as adaptive multivariate functional EWMA (AMFEWMA), to extend the well-known exponentially weighted moving average (EWMA) control chart from the univariate scalar to the multivariate functional setting. The favorable performance of the AMFEWMA control chart over existing methods is assessed via an extensive Monte Carlo simulation. Its practical applicability is demonstrated through a case study in the monitoring of the quality of a resistance spot welding process in the automotive industry through the online observations of dynamic resistance curves, which are associated with multiple spot welds on the same car body and recognized as the full technological signature of the process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03837v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Capezza, Giovanna Capizzi, Fabio Centofanti, Antonio Lepore, Biagio Palumbo</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v3 Announce Type: replace 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to standardized average treatment effects and quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Understanding Reliability from a Regression Perspective</title>
      <link>https://arxiv.org/abs/2404.16709</link>
      <description>arXiv:2404.16709v2 Announce Type: replace 
Abstract: Reliability is an essential property of scores, which quantifies measurement precision. We expand on the work of McDonald (2011) and present a regression framework of reliability. From this framework, reliability reflects the explained variance in either observed or latent scores assuming an underlying latent variable measurement model. This framework unifies two extant perspectives of reliability: (a) classical test theory (measurement decomposition), and (b) optimal prediction of latent scores (prediction decomposition). The framework also highlights that reliability should be treated as a property of the outcome variable in the regression. As such, reliability is attached to the observed scores under a measurement decomposition but to the latent scores under a prediction decomposition. We introduce a Monte Carlo (MC) approach that calculates values of reliability, which is a direct application of the regression framework. The MC procedure has the advantage of easily obtaining reliability coefficients for complex measurement models, which we illustrate with an empirical example on measuring susceptibility and severity of depressive symptoms using a two-dimensional item response theory model. We conclude with a discussion on computing reliability coefficients and outline future avenues of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16709v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Jolynn Pek, Alberto Maydeu-Olivares</dc:creator>
    </item>
    <item>
      <title>Selective Randomization Inference for Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2405.07026</link>
      <description>arXiv:2405.07026v2 Announce Type: replace 
Abstract: Adaptive experiments use preliminary analyses of the data to inform further course of action and are commonly used in many disciplines including medical and social sciences. Because the null hypothesis and experimental design are not pre-specified, it has long been recognized that statistical inference for adaptive experiments is not straightforward. Most existing methods only apply to specific adaptive designs and rely on strong assumptions. In this work, we propose selective randomization inference as a general framework for analysing adaptive experiments. In a nutshell, our approach applies conditional post-selection inference to randomization tests. By using directed acyclic graphs to describe the data generating process, we derive a selective randomization p-value that controls the selective type-I error without requiring independent and identically distributed data or any other modelling assumptions. We show how rejection sampling and Markov Chain Monte Carlo can be used to compute the selective randomization p-values and construct confidence intervals for a homogeneous treatment effect. To mitigate the risk of disconnected confidence intervals, we propose the use of hold-out units. Lastly, we demonstrate our method and compare it with other randomization tests using synthetic and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07026v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Freidling, Qingyuan Zhao, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows</title>
      <link>https://arxiv.org/abs/2405.14392</link>
      <description>arXiv:2405.14392v2 Announce Type: replace 
Abstract: Continuous normalizing flows (CNFs) learn the probability path between a reference distribution and a target distribution by modeling the vector field generating said path using neural networks. Recently, Lipman et al. (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM). In this paper, we repurpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective, and using the learned CNF to improve Monte Carlo sampling. Specifically, we propose an adaptive Markov chain Monte Carlo (MCMC) algorithm, which combines a local Markov transition kernel with a non-local, flow-informed transition kernel, defined using a CNF. This CNF is adapted on-the-fly using samples from the Markov chain, which are used to specify the probability path for the FM objective. Our method also includes an adaptive tempering mechanism that allows the discovery of multiple modes in the target distribution. Under mild assumptions, we establish convergence of our method to a local optimum of the FM objective. We then benchmark our approach on several synthetic and real-world examples, achieving similar performance to other state-of-the-art methods, but often at a significantly lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14392v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Cabezas, Louis Sharrock, Christopher Nemeth</dc:creator>
    </item>
    <item>
      <title>A Bayesian Generalized Bridge Regression Approach to Covariance Estimation in the Presence of Covariates</title>
      <link>https://arxiv.org/abs/2406.00906</link>
      <description>arXiv:2406.00906v2 Announce Type: replace 
Abstract: A hierarchical Bayesian approach that permits simultaneous inference for the regression coefficient matrix and the error precision (inverse covariance) matrix in the multivariate linear model is proposed. Assuming a natural ordering of the elements of the response, the precision matrix is reparameterized so it can be estimated with univariate-response linear regression techniques. A novel generalized bridge regression prior that accommodates both sparse and dense settings and is competitive with alternative methods for univariate-response regression is proposed and used in this framework. Two component-wise Markov chain Monte Carlo algorithms are developed for sampling, including a data augmentation algorithm based on a scale mixture of normals representation. Numerical examples demonstrate that the proposed method is competitive with comparable joint mean-covariance models, particularly in estimation of the precision matrix. The method is also used to estimate the 253 by 253 precision matrix of 90,670 spectra extracted from images taken by the Hubble Space Telescope, demonstrating its computational feasibility for problems with large n and q.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00906v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christina Zhao (School of Statistics, University of Minnesota), Ding Xiang (Liberty Mutual Insurance), Galin L. Jones (School of Statistics, University of Minnesota), Adam J. Rothman (School of Statistics, University of Minnesota)</dc:creator>
    </item>
    <item>
      <title>Non-stationary Spatio-Temporal Modeling Using the Stochastic Advection-Diffusion Equation</title>
      <link>https://arxiv.org/abs/2406.03400</link>
      <description>arXiv:2406.03400v2 Announce Type: replace 
Abstract: We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying. Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method. The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting, and evaluated in terms of root mean square errors and continuous rank probability scores. A simulation study demonstrates that the non-separable model performs better when the data is simulated from a non-separable model with diffusion and advection. Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonomous underwater vehicles. The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03400v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Outzen Berild, Geir-Arne Fuglstad</dc:creator>
    </item>
    <item>
      <title>Nonparametric Density Estimation for Data Scattered on Irregular Spatial Domains: A Likelihood-Based Approach Using Bivariate Penalized Spline Smoothing</title>
      <link>https://arxiv.org/abs/2408.16963</link>
      <description>arXiv:2408.16963v2 Announce Type: replace 
Abstract: Accurately estimating data density is crucial for making informed decisions and modeling in various fields. This paper presents a novel nonparametric density estimation procedure that utilizes bivariate penalized spline smoothing over triangulation for data scattered over irregular spatial domains. The approach is likelihood-based with a regularization term that addresses the roughness of the logarithm of density based on a second-order differential operator. The proposed method offers greater efficiency and flexibility in estimating density over complex domains and has been theoretically supported by establishing the asymptotic convergence rate under mild natural conditions. Through extensive simulation studies and a real-world application that analyzes motor vehicle theft data from Portland City, Oregon, we demonstrate the advantages of the proposed method over existing techniques detailed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16963v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kunal Das, Shan Yu, Guannan Wang, Li Wang</dc:creator>
    </item>
    <item>
      <title>Directional data analysis using the spherical Cauchy and the Poisson kernel-based distribution</title>
      <link>https://arxiv.org/abs/2409.03292</link>
      <description>arXiv:2409.03292v3 Announce Type: replace 
Abstract: In 2020, two novel distributions for the analysis of directional data were introduced: the spherical Cauchy distribution and the Poisson kernel-based distribution. This paper provides a detailed exploration of both distributions within various analytical frameworks. To enhance the practical utility of these distributions, alternative parametrizations that offer advantages in numerical stability and parameter estimation are presented, such as implementation of the Newton-Raphson algorithm for parameter estimation, while facilitating a more efficient and simplified approach in the regression framework. Additionally, a two-sample location test based on the log-likelihood ratio test is introduced. This test is designed to assess whether the location parameters of two populations can be assumed equal. The maximum likelihood discriminant analysis framework is developed for classification purposes, and finally, the problem of clustering directional data is addressed, by fitting finite mixtures of Spherical Cauchy or Poisson kernel-based distributions. Empirical validation is conducted through comprehensive simulation studies and real data applications, wherein the performance of the spherical Cauchy and Poisson kernel-based distributions is systematically compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03292v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Perturbation-Robust Predictive Modeling of Social Effects by Network Subspace Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2410.01163</link>
      <description>arXiv:2410.01163v3 Announce Type: replace 
Abstract: Network-linked data, where multivariate observations are interconnected by a network, are becoming increasingly prevalent in fields such as sociology and biology. These data often exhibit inherent noise and complex relational structures, complicating conventional modeling and statistical inference. Motivated by empirical challenges in analyzing such data sets, this paper introduces a family of network subspace generalized linear models designed for analyzing noisy, network-linked data. We propose a model inference method based on subspace-constrained maximum likelihood, which emphasizes flexibility in capturing network effects and provides a robust inference framework against network perturbations. We establish the asymptotic distributions of the estimators under network perturbations, demonstrating the method's accuracy through extensive simulations involving random network models and deep-learning-based embedding algorithms. The proposed methodology is applied to a comprehensive analysis of a large-scale study on school conflicts, where it identifies significant social effects, offering meaningful and interpretable insights into student behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01163v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianxiang Wang, Can M. Le, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v3 Announce Type: replace 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
    <item>
      <title>Median Based Unit Weibull (MBUW): a new unit distribution Properties</title>
      <link>https://arxiv.org/abs/2410.19019</link>
      <description>arXiv:2410.19019v2 Announce Type: replace 
Abstract: A new 2 parameter unit Weibull distribution is defined on the unit interval (0,1). The methodology of deducing its PDF, some of its properties and related functions are discussed. The paper is supplied by many figures illustrating the new distribution and how this can make it illegible to fit a wide range of skewed data. The new distribution holds a name (Attia) as a nickname.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19019v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>A model and method for analyzing the precision of binary measurement methods based on beta-binomial distributions, and related statistical tests</title>
      <link>https://arxiv.org/abs/2008.13619</link>
      <description>arXiv:2008.13619v3 Announce Type: replace-cross 
Abstract: This study developed a new statistical model and method for analyzing the precision of binary measurement methods from collaborative studies. The model is based on beta-binomial distributions. In other words, it assumes that the sensitivity of each laboratory obeys a beta distribution, and the binary measured values under a given sensitivity follow a binomial distribution. We propose the key precision measures of repeatability and reproducibility for the model, and provide their unbiased estimates. Further, through consideration of a number of statistical test methods for homogeneity of proportions, we propose appropriate methods for determining laboratory effects in the new model. Finally, we apply the results to real-world examples in the fields of food safety and chemical risk assessment and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.13619v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun-ichi Takeshita, Tomomichi Suzuki</dc:creator>
    </item>
    <item>
      <title>Scalable couplings for the random walk Metropolis algorithm</title>
      <link>https://arxiv.org/abs/2211.12585</link>
      <description>arXiv:2211.12585v3 Announce Type: replace-cross 
Abstract: There has been a recent surge of interest in coupling methods for Markov chain Monte Carlo algorithms: they facilitate convergence quantification and unbiased estimation, while exploiting embarrassingly parallel computing capabilities. Motivated by these, we consider the design and analysis of couplings of the random walk Metropolis algorithm which scale well with the dimension of the target measure. Methodologically, we introduce a low-rank modification of the synchronous coupling that is provably optimally contractive in standard high-dimensional asymptotic regimes. We expose a shortcoming of the reflection coupling, the state of the art at the time of writing, and we propose a modification which mitigates the issue. Our analysis bridges the gap to the optimal scaling literature and builds a framework of asymptotic optimality which may be of independent interest. We illustrate the applicability of our proposed couplings, and the potential for extending our ideas, with various numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12585v3</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tam\'as P. Papp, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees for neural control variates in MCMC</title>
      <link>https://arxiv.org/abs/2304.01111</link>
      <description>arXiv:2304.01111v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a variance reduction approach for Markov chains based on additive control variates and the minimization of an appropriate estimate for the asymptotic variance. We focus on the particular case when control variates are represented as deep neural networks. We derive the optimal convergence rate of the asymptotic variance under various ergodicity assumptions on the underlying Markov chain. The proposed approach relies upon recent results on the stochastic errors of variance reduction algorithms and function approximation theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01111v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Belomestny, Artur Goldman, Alexey Naumov, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>Fused Extended Two-Way Fixed Effects for Difference-in-Differences With Staggered Adoptions</title>
      <link>https://arxiv.org/abs/2312.05985</link>
      <description>arXiv:2312.05985v3 Announce Type: replace-cross 
Abstract: To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We demonstrate FETWFE in simulation studies and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05985v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Faletto</dc:creator>
    </item>
    <item>
      <title>Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference</title>
      <link>https://arxiv.org/abs/2401.01426</link>
      <description>arXiv:2401.01426v2 Announce Type: replace-cross 
Abstract: Sound and complete algorithms have been proposed to compute identifiable causal queries using the causal structure and data. However, most of these algorithms assume accurate estimation of the data distribution, which is impractical for high-dimensional variables such as images. On the other hand, modern deep generative architectures can be trained to sample from high-dimensional distributions. However, training these networks are typically very costly. Thus, it is desirable to leverage pre-trained models to answer causal queries using such high-dimensional data. To address this, we propose modular training of deep causal generative models that not only makes learning more efficient, but also allows us to utilize large, pre-trained conditional generative models. To the best of our knowledge, our algorithm, Modular-DCM is the first algorithm that, given the causal structure, uses adversarial training to learn the network weights, and can make use of pre-trained models to provably sample from any identifiable causal query in the presence of latent confounders. With extensive experiments on the Colored-MNIST dataset, we demonstrate that our algorithm outperforms the baselines. We also show our algorithm's convergence on the COVIDx dataset and its utility with a causal invariant prediction problem on CelebA-HQ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01426v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Musfiqur Rahman, Murat Kocaoglu</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Acceleration of SN Ia Photometric Distance Estimation with BayeSN</title>
      <link>https://arxiv.org/abs/2405.06013</link>
      <description>arXiv:2405.06013v2 Announce Type: replace-cross 
Abstract: Type Ia supernovae (SNe Ia) are standarizable candles whose observed light curves can be used to infer their distances, which can in turn be used in cosmological analyses. As the quantity of observed SNe Ia grows with current and upcoming surveys, increasingly scalable analyses are necessary to take full advantage of these new datasets for precise estimation of cosmological parameters. Bayesian inference methods enable fitting SN Ia light curves with robust uncertainty quantification, but traditional posterior sampling using Markov Chain Monte Carlo (MCMC) is computationally expensive. We present an implementation of variational inference (VI) to accelerate the fitting of SN Ia light curves using the BayeSN hierarchical Bayesian model for time-varying SN Ia spectral energy distributions (SEDs). We demonstrate and evaluate its performance on both simulated light curves and data from the Foundation Supernova Survey with two different forms of surrogate posterior -- a multivariate normal and a custom multivariate zero-lower-truncated normal distribution -- and compare them with the Laplace Approximation and full MCMC analysis. To validate of our variational approximation, we calculate the pareto-smoothed importance sampling (PSIS) diagnostic, and perform variational simulation-based calibration (VSBC). The VI approximation achieves similar results to MCMC but with an order-of-magnitude speedup for the inference of the photometric distance moduli. Overall, we show that VI is a promising method for scalable parameter inference that enables analysis of larger datasets for precision cosmology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06013v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Sof\'ia M. Uzsoy, Stephen Thorp, Matthew Grayling, Kaisey S. Mandel</dc:creator>
    </item>
    <item>
      <title>IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark</title>
      <link>https://arxiv.org/abs/2405.16069</link>
      <description>arXiv:2405.16069v3 Announce Type: replace-cross 
Abstract: Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16069v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fredrik D. Johansson</dc:creator>
    </item>
    <item>
      <title>Task-Agnostic Machine-Learning-Assisted Inference</title>
      <link>https://arxiv.org/abs/2405.20039</link>
      <description>arXiv:2405.20039v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) is playing an increasingly important role in scientific research. In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings. This has also opened a whole field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges. One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples, and then uses predicted outcomes in downstream statistical inference. However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis. This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for the same scientific problem given observed data. This severely constrains the scope of application for post-prediction inference. To address this challenge, we introduce a novel statistical framework named PSPS for task-agnostic ML-assisted inference. It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routines. It delivers valid and efficient inference that is robust to arbitrary choice of ML model, allowing nearly all existing statistical frameworks to be incorporated into the analysis of ML-predicted data. Through extensive experiments, we showcase our method's validity, versatility, and superiority compared to existing approaches. Our software is available at https://github.com/qlu-lab/psps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20039v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacheng Miao, Qiongshi Lu</dc:creator>
    </item>
    <item>
      <title>End-To-End Causal Effect Estimation from Unstructured Natural Language Data</title>
      <link>https://arxiv.org/abs/2407.07018</link>
      <description>arXiv:2407.07018v3 Announce Type: replace-cross 
Abstract: Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce NATURAL, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07018v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Dhawan, Leonardo Cotta, Karen Ullrich, Rahul G. Krishnan, Chris J. Maddison</dc:creator>
    </item>
    <item>
      <title>Unconditional Randomization Tests for Interference</title>
      <link>https://arxiv.org/abs/2409.09243</link>
      <description>arXiv:2409.09243v2 Announce Type: replace-cross 
Abstract: When conducting causal inference or designing policy, researchers are often concerned with the existence and extent of interference between units, which may be influenced by factors such as distance, proximity, and connection strength. However, complex correlations across units pose significant challenges for inference. This paper introduces partial null randomization tests (PNRTs), a novel framework for testing interference in experimental settings. PNRTs adopt a design-based approach, combining unconditional randomization testing with pairwise comparisons to enable straightforward implementation and ensure finite-sample validity under minimal assumptions about network structure. To illustrate the method's broad applicability, this paper applies it to a large-scale experiment by Blattman et al. (2021) in Bogota, Colombia, which evaluates the impact of hotspot policing on crime using street segments as units of analysis. The findings indicate that increasing police patrolling time in hotspots has a significant displacement effect on violent crime but not on property crime. A simulation study calibrated to this dataset further demonstrates the strong power properties of PNRTs and their suitability for general interference scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09243v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Zhong</dc:creator>
    </item>
  </channel>
</rss>
