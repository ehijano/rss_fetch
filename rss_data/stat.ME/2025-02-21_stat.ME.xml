<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Online detection of forecast model inadequacies using forecast errors</title>
      <link>https://arxiv.org/abs/2502.14173</link>
      <description>arXiv:2502.14173v1 Announce Type: new 
Abstract: In many organisations, accurate forecasts are essential for making informed decisions for a variety of applications from inventory management to staffing optimization. Whatever forecasting model is used, changes in the underlying process can lead to inaccurate forecasts, which will be damaging to decision-making. At the same time, models are becoming increasingly complex and identifying change through direct modelling is problematic. We present a novel framework for online monitoring of forecasts to ensure they remain accurate. By utilizing sequential changepoint techniques on the forecast errors, our framework allows for the real-time identification of potential changes in the process caused by various external factors. We show theoretically that some common changes in the underlying process will manifest in the forecast errors and can be identified faster by identifying shifts in the forecast errors than within the original modelling framework. Moreover, we demonstrate the effectiveness of this framework on numerous forecasting approaches through simulations and show its effectiveness over alternative approaches. Finally, we present two concrete examples, one from Royal Mail parcel delivery volumes and one from NHS A\&amp;E admissions relating to gallstones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Grundy, Rebecca Killick, Ivan Svetunkov</dc:creator>
    </item>
    <item>
      <title>Addressing Positivity Violations in Continuous Interventions through Data-Adaptive Strategies</title>
      <link>https://arxiv.org/abs/2502.14566</link>
      <description>arXiv:2502.14566v1 Announce Type: new 
Abstract: Positivity violations pose a key challenge in the estimation of causal effects, particularly for continuous interventions. Current approaches for addressing this issue include the use of projection functions or modified treatment policies. While effective in many contexts, these methods can result in estimands that potentially do not align well with the original research question, thereby leading to compromises in interpretability. In this paper, we introduce a novel diagnostic tool, the non-overlap ratio, to detect positivity violations. To address these violations while maintaining interpretability, we propose a data-adaptive solution, specially a "most feasible" intervention strategy. Our strategy operates on a unit-specific basis. For a given intervention of interest, we first assess whether the intervention value is feasible for each unit. For units with sufficient support, conditional on confounders, we adhere to the intervention of interest. However, for units lacking sufficient support, as identified through the assessment of the non-overlap ratio, we do not assign the actual intervention value of interest. Instead, we assign the closest feasible value within the support region. We propose an estimator using g-computation coupled with flexible conditional density estimation to estimate high- and low support regions to estimate this new estimand. Through simulations, we demonstrate that our method effectively reduces bias across various scenarios by addressing positivity violations. Moreover, when positivity violations are absent, the method successfully recovers the standard estimand. We further validate its practical utility using real-world data from the CHAPAS-3 trial, which enrolled HIV-positive children in Zambia and Uganda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14566v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Bao, Michael Schomaker</dc:creator>
    </item>
    <item>
      <title>Spectral decomposition-assisted multi-study factor analysis</title>
      <link>https://arxiv.org/abs/2502.14600</link>
      <description>arXiv:2502.14600v1 Announce Type: new 
Abstract: This article focuses on covariance estimation for multi-study data. Popular approaches employ factor-analytic terms with shared and study-specific loadings that decompose the variance into (i) a shared low-rank component, (ii) study-specific low-rank components, and (iii) a diagonal term capturing idiosyncratic variability. Our proposed methodology estimates the latent factors via spectral decompositions and infers the factor loadings via surrogate regression tasks, avoiding identifiability and computational issues of existing alternatives. Reliably inferring shared vs study-specific components requires novel developments that are of independent interest. The approximation error decreases as the sample size and the data dimension diverge, formalizing a blessing of dimensionality. Conditionally on the factors, loadings and residual error variances are inferred via conjugate normal-inverse gamma priors. The conditional posterior distribution of factor loadings has a simple product form across outcomes, facilitating parallelization. We show favorable asymptotic properties, including central limit theorems for point estimators and posterior contraction, and excellent empirical performance in simulations. The methods are applied to integrate three studies on gene associations among immune cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14600v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mauri, Niccol\`o Anceschi, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Asymptotic Analysis and Practical Evaluation of Jump Rate Estimators in Piecewise-Deterministic Markov Processes</title>
      <link>https://arxiv.org/abs/2502.14621</link>
      <description>arXiv:2502.14621v1 Announce Type: new 
Abstract: Piecewise-deterministic Markov processes (PDMPs) offer a powerful stochastic modeling framework that combines deterministic trajectories with random perturbations at random times. Estimating their local characteristics (particularly the jump rate) is an important yet challenging task. In recent years, non-parametric methods for jump rate inference have been developed, but these approaches often rely on distinct theoretical frameworks, complicating direct comparisons. In this paper, we propose a unified framework to standardize and consolidate state-of-the-art approaches. We establish new results on consistency and asymptotic normality within this framework, enabling rigorous theoretical comparisons of convergence rates and asymptotic variances. Notably, we demonstrate that no single method uniformly outperforms the others, even within the same model. These theoretical insights are validated through numerical simulations using a representative PDMP application: the TCP model. Furthermore, we extend the comparison to real-world data, focusing on cell growth and division dynamics in Escherichia coli. This work enhances the theoretical understanding of PDMP inference while offering practical insights into the relative strengths and limitations of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14621v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Aza\"is, Solune Denis</dc:creator>
    </item>
    <item>
      <title>Outlier Detection in Mendelian Randomisation</title>
      <link>https://arxiv.org/abs/2502.14716</link>
      <description>arXiv:2502.14716v1 Announce Type: new 
Abstract: Mendelian Randomisation (MR) uses genetic variants as instrumental variables to infer causal effects of exposures on an outcome. One key assumption of MR is that the genetic variants used as instrumental variables are independent of the outcome conditional on the risk factor and unobserved confounders. Violations of this assumption, i.e. the effect of the instrumental variables on the outcome through a path other than the risk factor included in the model (which can be caused by pleiotropy), are common phenomena in human genetics. Genetic variants, which deviate from this assumption, appear as outliers to the MR model fit and can be detected by the general heterogeneity statistics proposed in the literature, which are known to suffer from overdispersion, i.e. too many genetic variants are declared as false outliers. We propose a method that corrects for overdispersion of the heterogeneity statistics in uni- and multivariable MR analysis by making use of the estimated inflation factor to correctly remove outlying instruments and therefore account for pleiotropic effects. Our method is applicable to summary-level data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14716v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian M Mandl, Anne-Laure Boulesteix, Stephen Burgess, Verena Zuber</dc:creator>
    </item>
    <item>
      <title>Cross Validation for Correlated Data in Regression and Classification Models, with Applications to Deep Learning</title>
      <link>https://arxiv.org/abs/2502.14808</link>
      <description>arXiv:2502.14808v1 Announce Type: new 
Abstract: We present a methodology for model evaluation and selection where the sampling mechanism violates the i.i.d. assumption. Our methodology involves a formulation of the bias between the standard Cross-Validation (CV) estimator and the mean generalization error, denoted by $w_{cv}$, and practical data-based procedures to estimate this term. This concept was introduced in the literature only in the context of a linear model with squared error loss as the criterion for prediction performance. Our proposed bias-corrected CV estimator, $\text{CV}_c=\text{CV}+w_{cv}$, can be applied to any learning model, including deep neural networks, and to a wide class of criteria for prediction performance in regression and classification tasks. We demonstrate the applicability of the proposed methodology in various scenarios where the data contains complex correlation structures (such as clustered and spatial relationships) with synthetic data and real-world datasets, providing evidence that the estimator $\text{CV}_c$ is better than the standard CV estimator. This paper is an expanded version of our published conference paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14808v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oren Yuval, Saharon Rosset</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction under L\'evy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations</title>
      <link>https://arxiv.org/abs/2502.14105</link>
      <description>arXiv:2502.14105v1 Announce Type: cross 
Abstract: Conformal prediction provides a powerful framework for constructing prediction intervals with finite-sample guarantees, yet its robustness under distribution shifts remains a significant challenge. This paper addresses this limitation by modeling distribution shifts using L\'evy-Prokhorov (LP) ambiguity sets, which capture both local and global perturbations. We provide a self-contained overview of LP ambiguity sets and their connections to popular metrics such as Wasserstein and Total Variation. We show that the link between conformal prediction and LP ambiguity sets is a natural one: by propagating the LP ambiguity set through the scoring function, we reduce complex high-dimensional distribution shifts to manageable one-dimensional distribution shifts, enabling exact quantification of worst-case quantiles and coverage. Building on this analysis, we construct robust conformal prediction intervals that remain valid under distribution shifts, explicitly linking LP parameters to interval width and confidence levels. Experimental results on real-world datasets demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14105v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan, Youssef Marzouk, Zheyu Oliver Wang, Julie Zhu</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Adaptive Shrinkage Estimation</title>
      <link>https://arxiv.org/abs/2502.14166</link>
      <description>arXiv:2502.14166v1 Announce Type: cross 
Abstract: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical tasks, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14166v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sida Li, Nikolaos Ignatiadis</dc:creator>
    </item>
    <item>
      <title>Distribution Matching for Self-Supervised Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.14424</link>
      <description>arXiv:2502.14424v1 Announce Type: cross 
Abstract: In this paper, we propose a novel self-supervised transfer learning method called Distribution Matching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance. The design of DM results in a learned representation space that is intuitively structured and offers easily interpretable hyperparameters. Experimental results across multiple real-world datasets and evaluation metrics demonstrate that DM performs competitively on target classification tasks compared to existing self-supervised transfer learning methods. Additionally, we provide robust theoretical guarantees for DM, including a population theorem and an end-to-end sample theorem. The population theorem bridges the gap between the self-supervised learning task and target classification accuracy, while the sample theorem shows that, even with a limited number of samples from the target domain, DM can deliver exceptional classification performance, provided the unlabeled sample size is sufficiently large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14424v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuling Jiao, Wensen Ma, Defeng Sun, Hansheng Wang, Yang Wang</dc:creator>
    </item>
    <item>
      <title>A projected nonlinear state-space model for forecasting time series signals</title>
      <link>https://arxiv.org/abs/2311.13247</link>
      <description>arXiv:2311.13247v2 Announce Type: replace 
Abstract: Learning and forecasting stochastic time series is essential in various scientific fields. However, despite the proposals of nonlinear filters and deep-learning methods, it remains challenging to capture nonlinear dynamics from a few noisy samples and predict future trajectories with uncertainty estimates while maintaining computational efficiency. Here, we propose a fast algorithm to learn and forecast nonlinear dynamics from noisy time series data. A key feature of the proposed model is kernel functions applied to projected lines, enabling fast and efficient capture of nonlinearities in the latent dynamics. Through empirical case studies and benchmarking, the model demonstrates its effectiveness in learning and forecasting complex nonlinear dynamics, offering a valuable tool for researchers and practitioners in time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13247v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijforecast.2025.01.002</arxiv:DOI>
      <dc:creator>Christian Donner, Anuj Mishra, Hideaki Shimazaki</dc:creator>
    </item>
    <item>
      <title>Bayesian multilevel compositional data analysis: introduction, evaluation, and application</title>
      <link>https://arxiv.org/abs/2405.03985</link>
      <description>arXiv:2405.03985v3 Announce Type: replace 
Abstract: Multilevel compositional data are data that are repeatedly measured or clustered within groups and are non-negative and sum to a constant value. These data arise in various settings, such as intensive, longitudinal studies using ecological momentary assessments and wearable devices. Examples include 24h sleep-wake behaviours, sleep architecture, and macronutrients. This article presents a novel method for analysing multilevel compositional data using Bayesian inference. We describe the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all conditions investigated in the simulation study, the fitted models had minimal convergence issues (convergence rate &gt; 99%) and achieved excellent quality parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian multilevel compositional data analysis. We hope to promote wider application of this method to gain novel and robust answers to scientific questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03985v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1037/met0000750</arxiv:DOI>
      <dc:creator>Flora Le, Tyman E. Stanford, Dorothea Dumuid, Joshua F. Wiley</dc:creator>
    </item>
    <item>
      <title>Inference in semiparametric formation models for directed networks</title>
      <link>https://arxiv.org/abs/2405.19637</link>
      <description>arXiv:2405.19637v2 Announce Type: replace 
Abstract: We propose a semiparametric model for dyadic link formations in directed networks. The model contains a set of degree parameters that measure different effects of popularity or outgoingness across nodes, a regression parameter vector that reflects the homophily effect resulting from the nodal attributes or pairwise covariates associated with edges, and a set of latent random noises with unknown distributions. Our interest lies in inferring the unknown degree parameters and homophily parameters. The dimension of the degree parameters increases with the number of nodes. Under the high-dimensional regime, we develop a kernel-based least squares approach to estimate the unknown parameters. The major advantage of our estimator is that it does not encounter the incidental parameter problem for the homophily parameters. We prove consistency of all the resulting estimators of the degree parameters and homophily parameters. We establish high-dimensional central limit theorems for the proposed estimators and provide several applications of our general theory, including testing the existence of degree heterogeneity, testing sparse signals and recovering the support. Simulation studies and a real data application are conducted to illustrate the finite sample performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19637v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lianqiang Qu, Lu Chen, Ting Yan, Yuguo Chen</dc:creator>
    </item>
    <item>
      <title>Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title>
      <link>https://arxiv.org/abs/2406.08390</link>
      <description>arXiv:2406.08390v4 Announce Type: replace 
Abstract: Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08390v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model of multiple longitudinal and categorical outcomes with application to multiple myeloma using permutation-based variable importance</title>
      <link>https://arxiv.org/abs/2407.14311</link>
      <description>arXiv:2407.14311v2 Announce Type: replace 
Abstract: Joint models have proven to be an effective approach for uncovering potentially hidden connections between various types of outcomes, mainly continuous, time-to-event, and binary. Typically, longitudinal continuous outcomes are characterized by linear mixed-effects models, survival outcomes are described by proportional hazards models, and the link between outcomes are captured by shared random effects. Other modeling variations include generalized linear mixed-effects models for longitudinal data and logistic regression when a binary outcome is present, rather than time until an event of interest. However, in a clinical research setting, one might be interested in modeling the physician's chosen treatment based on the patient's medical history to identify prognostic factors. In this situation, there are often multiple treatment options, requiring the use of a multiclass classification approach. Inspired by this context, we develop a Bayesian joint model for longitudinal and categorical data. In particular, our motivation comes from a multiple myeloma study, in which biomarkers display nonlinear trajectories that are well captured through bi-exponential submodels, where patient-level information is shared with the categorical submodel. We also present a variable importance strategy to rank prognostic factors. We apply our proposal and a competing model to the multiple myeloma data, compare the variable importance and inferential results for both models, and illustrate patient-level interpretations using our joint model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14311v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Alvares, Jessica K. Barrett, Fran\c{c}ois Mercier, Jochen Schulze, Sean Yiu, Felipe Castro, Spyros Roumpanis, Yajing Zhu</dc:creator>
    </item>
    <item>
      <title>A Novel Multiscale Framework for Testing Independence: Efficient Detection of Explicit or Implicit Functional Relationships</title>
      <link>https://arxiv.org/abs/2410.11192</link>
      <description>arXiv:2410.11192v2 Announce Type: replace 
Abstract: In this article, we consider the problem of testing the independence between two random variables. Our primary objective is to develop tests that are highly effective at detecting associations arising from explicit or implicit functional relationship between two variables. We adopt a multiscale approach by analyzing neighborhoods of varying sizes within the dataset and aggregating the results. We introduce a general testing framework designed to enhance the power of existing independence tests to achieve our objective. Additionally, we propose a novel test method that is powerful as well as computationally efficient. The performance of these tests is compared with existing methods using various simulated datasets. Additionally, a visualization method has been proposed for exploring the localization of dependence within datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11192v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seetharaman P, Sagnik Das, Angshuman Roy</dc:creator>
    </item>
    <item>
      <title>Partial Information Rate Decomposition</title>
      <link>https://arxiv.org/abs/2502.04550</link>
      <description>arXiv:2502.04550v2 Announce Type: replace 
Abstract: Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04550v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luca Faes, Laura Sparacino, Gorana Mijatovic, Yuri Antonacci, Leonardo Ricci, Daniele Marinazzo, Sebastiano Stramaglia</dc:creator>
    </item>
    <item>
      <title>Decomposing Multivariate Information Rates in Networks of Random Processes</title>
      <link>https://arxiv.org/abs/2502.04555</link>
      <description>arXiv:2502.04555v2 Announce Type: replace 
Abstract: The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interactions that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04555v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Laura Sparacino, Gorana Mijatovic, Yuri Antonacci, Leonardo Ricci, Daniele Marinazzo, Sebastiano Stramaglia, Luca Faes</dc:creator>
    </item>
    <item>
      <title>Consider or Choose? The Role and Power of Consideration Sets</title>
      <link>https://arxiv.org/abs/2302.04354</link>
      <description>arXiv:2302.04354v4 Announce Type: replace-cross 
Abstract: Consideration sets play a crucial role in discrete choice modeling, where customers often form consideration sets in the first stage and then use a second-stage choice mechanism to select the product with the highest utility. While many recent studies aim to improve choice models by incorporating more sophisticated second-stage choice mechanisms, this paper takes a step back and goes into the opposite extreme. We simplify the second-stage choice mechanism to its most basic form and instead focus on modeling customer choice by emphasizing the role and power of the first-stage consideration set formation. To this end, we study a model that is parameterized solely by a distribution over consideration sets with a bounded rationality interpretation. Intriguingly, we show that this model is characterized by the axiom of symmetric demand cannibalization, enabling complete statistical identification. The latter finding highlights the critical role of consideration sets in the identifiability of two-stage choice models. We also examine the model's implications for assortment planning, proving that the optimal assortment is revenue-ordered within each partition block created by consideration sets. Despite this compelling structure, we establish that the assortment problem under this model is NP-hard even to approximate, highlighting how consideration sets contribute to nontractability, even under the simplest uniform second-stage choice mechanism. Finally, using real-world data, we show that the model achieves prediction performance comparable to other advanced choice models. Given the simplicity of the model's second-stage phase, this result showcases the enormous power of first-stage consideration set formation in capturing customers' decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04354v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Chun Akchen, Dmitry Mitrofanov</dc:creator>
    </item>
    <item>
      <title>Conditioning diffusion models by explicit forward-backward bridging</title>
      <link>https://arxiv.org/abs/2405.13794</link>
      <description>arXiv:2405.13794v2 Announce Type: replace-cross 
Abstract: Given an unconditional diffusion model targeting a joint model $\pi(x, y)$, using it to perform conditional simulation $\pi(x \mid y)$ is still largely an open question and is typically achieved by learning conditional drifts to the denoising SDE after the fact. In this work, we express \emph{exact} conditional simulation within the \emph{approximate} diffusion model as an inference problem on an augmented space corresponding to a partial SDE bridge. This perspective allows us to implement efficient and principled particle Gibbs and pseudo-marginal samplers marginally targeting the conditional distribution $\pi(x \mid y)$. Contrary to existing methodology, our methods do not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error. We showcase the benefits and drawbacks of our approach on a series of synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13794v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Zheng Zhao, Simo S\"arkk\"a, Jens Sj\"olund, Thomas B. Sch\"on</dc:creator>
    </item>
    <item>
      <title>Totally Concave Regression</title>
      <link>https://arxiv.org/abs/2501.04360</link>
      <description>arXiv:2501.04360v2 Announce Type: replace-cross 
Abstract: Shape constraints in nonparametric regression provide a powerful framework for estimating regression functions under realistic assumptions without tuning parameters. However, most existing methods$\unicode{x2013}$except additive models$\unicode{x2013}$impose too weak restrictions, often leading to overfitting in high dimensions. Conversely, additive models can be too rigid, failing to capture covariate interactions. This paper introduces a novel multivariate shape-constrained regression approach based on total concavity, originally studied by T. Popoviciu. Our method allows interactions while mitigating the curse of dimensionality, with convergence rates that depend only logarithmically on the number of covariates. We characterize and compute the least squares estimator over totally concave functions, derive theoretical guarantees, and demonstrate its practical effectiveness through empirical studies on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04360v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyeong Ki, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>WENDy for Nonlinear-in-Parameters ODEs</title>
      <link>https://arxiv.org/abs/2502.08881</link>
      <description>arXiv:2502.08881v2 Announce Type: replace-cross 
Abstract: The Weak-form Estimation of Non-linear Dynamics (WENDy) algorithm is extended to accommodate systems of ordinary differential equations that are nonlinear-in-parameters. The extension rests on derived analytic expressions for a likelihood function, its gradient and its Hessian matrix. WENDy makes use of these to approximate a maximum likelihood estimator based on optimization routines suited for non-convex optimization problems. The resulting parameter estimation algorithm has better accuracy, a substantially larger domain of convergence, and is often orders of magnitude faster than the conventional output error least squares method (based on forward solvers).
  The algorithm is efficiently implemented in Julia. We demonstrate the algorithm's ability to accommodate the weak form optimization for both additive normal and multiplicative log-normal noise, and present results on a suite of benchmark systems of ordinary differential equations. In order to demonstrate the practical benefits of our approach, we present extensive comparisons between our method and output error methods in terms of accuracy, precision, bias, and coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08881v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nic Rummel, Daniel A. Messenger, Stephen Becker, Vanja Dukic, David M. Bortz</dc:creator>
    </item>
    <item>
      <title>Potato Potahto in the FAO-GAEZ Productivity Measures? Nonclassical Measurement Error with Multiple Proxies</title>
      <link>https://arxiv.org/abs/2502.12141</link>
      <description>arXiv:2502.12141v2 Announce Type: replace-cross 
Abstract: The FAO-GAEZ crop productivity data are widely used in Economics. However, the existence of measurement error is rarely recognized in the empirical literature. We propose a novel method to partially identify the effect of agricultural productivity, deriving bounds that allow for nonclassical measurement error by leveraging two proxies. These bounds exhaust all the information contained in the first two moments of the data. We reevaluate three influential studies, documenting that measurement error matters and that the impact of agricultural productivity on economic outcomes may be smaller than previously reported. Our methodology has broad applications in empirical research involving mismeasured variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12141v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Araujo, Vitor Possebom</dc:creator>
    </item>
  </channel>
</rss>
