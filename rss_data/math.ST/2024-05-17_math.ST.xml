<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 May 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The radius of statistical efficiency</title>
      <link>https://arxiv.org/abs/2405.09676</link>
      <description>arXiv:2405.09676v1 Announce Type: new 
Abstract: Classical results in asymptotic statistics show that the Fisher information matrix controls the difficulty of estimating a statistical model from observed data. In this work, we introduce a companion measure of robustness of an estimation problem: the radius of statistical efficiency (RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular. We compute RSE up to numerical constants for a variety of test bed problems, including principal component analysis, generalized linear models, phase retrieval, bilinear sensing, and matrix completion. In all cases, the RSE quantifies the compatibility between the covariance of the population data and the latent model parameter. Interestingly, we observe a precise reciprocal relationship between RSE and the intrinsic complexity/sensitivity of the problem instance, paralleling the classical Eckart-Young theorem in numerical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09676v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Cutler, Mateo D\'iaz, Dmitriy Drusvyatskiy</dc:creator>
    </item>
    <item>
      <title>Robust Statistics meets elicitability: When fair model validation breaks down</title>
      <link>https://arxiv.org/abs/2405.09943</link>
      <description>arXiv:2405.09943v1 Announce Type: new 
Abstract: A crucial part of data analysis is the validation of the resulting estimators, in particular, if several competing estimators need to be compared. Whether an estimator can be objectively validated is not a trivial property. If there exists a loss function such that the theoretical risk is minimized by the quantity of interest, this quantity is called elicitable, allowing estimators for this quantity to be objectively validated and compared by evaluating such a loss function. Elicitability requires assumptions on the underlying distributions, often in the form of regularity conditions. Robust Statistics is a discipline that provides estimators in the presence of contaminated data. In this paper, we, introducing the elicitability breakdown point, formally pin down why the problems that contaminated data cause for estimation spill over to validation, letting elicitability fail. Furthermore, as the goal is usually to estimate the quantity of interest w.r.t. the non-contaminated distribution, even modified notions of elicitability may be doomed to fail. The performance of a trimming procedure that filters out instances from non-ideal distributions, which would be theoretically sound, is illustrated in several numerical experiments. Even in simple settings, elicitability however often fails, indicating the necessity to find validation procedures with non-zero elicitability breakdown point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09943v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tino Werner</dc:creator>
    </item>
    <item>
      <title>Distribution of Test Statistic for Euclidean Distance Matrices</title>
      <link>https://arxiv.org/abs/2405.10049</link>
      <description>arXiv:2405.10049v1 Announce Type: cross 
Abstract: Methods for global navigation satellite system fault detection using Euclidean Distance Matrices have been presented recently in the literature. Published methods define a test statistic in terms of eigenvalues of a certain matrix, but the distribution of the test statistic was not known, which presented a barrier to practical implementation. This document was a personal correspondence from Beatty to Derek Knowles. It includes a brief derivation of the distribution of the test statistic and a representative case showing that the theoretical distribution closely matches a simulated empirical distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10049v1</guid>
      <category>cs.RO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawson Beatty</dc:creator>
    </item>
    <item>
      <title>Estimating a Function and Its Derivatives Under a Smoothness Condition</title>
      <link>https://arxiv.org/abs/2405.10126</link>
      <description>arXiv:2405.10126v1 Announce Type: cross 
Abstract: We consider the problem of estimating an unknown function f* and its partial derivatives from a noisy data set of n observations, where we make no assumptions about f* except that it is smooth in the sense that it has square integrable partial derivatives of order m. A natural candidate for the estimator of f* in such a case is the best fit to the data set that satisfies a certain smoothness condition. This estimator can be seen as a least squares estimator subject to an upper bound on some measure of smoothness. Another useful estimator is the one that minimizes the degree of smoothness subject to an upper bound on the average of squared errors. We prove that these two estimators are computable as solutions to quadratic programs, establish the consistency of these estimators and their partial derivatives, and study the convergence rate as n increases to infinity. The effectiveness of the estimators is illustrated numerically in a setting where the value of a stock option and its second derivative are estimated as functions of the underlying stock price.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10126v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1287/moor.2020.0161</arxiv:DOI>
      <dc:creator>Eunji Lim</dc:creator>
    </item>
    <item>
      <title>Multivariate strong invariance principle and uncertainty assessment for time in-homogeneous cyclic MCMC samplers</title>
      <link>https://arxiv.org/abs/2405.10194</link>
      <description>arXiv:2405.10194v1 Announce Type: cross 
Abstract: Time in-homogeneous cyclic Markov chain Monte Carlo (MCMC) samplers, including deterministic scan Gibbs samplers and Metropolis within Gibbs samplers, are extensively used for sampling from multi-dimensional distributions. We establish a multivariate strong invariance principle (SIP) for Markov chains associated with these samplers. The rate of this SIP essentially aligns with the tightest rate available for time homogeneous Markov chains. The SIP implies the strong law of large numbers (SLLN) and the central limit theorem (CLT), and plays an essential role in uncertainty assessments. Using the SIP, we give conditions under which the multivariate batch means estimator for estimating the covariance matrix in the multivariate CLT is strongly consistent. Additionally, we provide conditions for a multivariate fixed volume sequential termination rule, which is associated with the concept of effective sample size (ESS), to be asymptotically valid. Our uncertainty assessment tools are demonstrated through various numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10194v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxiang Li, Qian Qin</dc:creator>
    </item>
    <item>
      <title>Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees</title>
      <link>https://arxiv.org/abs/2405.10289</link>
      <description>arXiv:2405.10289v1 Announce Type: cross 
Abstract: In nonsmooth, nonconvex stochastic optimization, understanding the uniform convergence of subdifferential mappings is crucial for analyzing stationary points of sample average approximations of risk as they approach the population risk. Yet, characterizing this convergence remains a fundamental challenge.
  This work introduces a novel perspective by connecting the uniform convergence of subdifferential mappings to that of subgradient mappings as empirical risk converges to the population risk. We prove that, for stochastic weakly-convex objectives, and within any open set, a uniform bound on the convergence of subgradients -- chosen arbitrarily from the corresponding subdifferential sets -- translates to a uniform bound on the convergence of the subdifferential sets itself, measured by the Hausdorff metric.
  Using this technique, we derive uniform convergence rates for subdifferential sets of stochastic convex-composite objectives. Our results do not rely on key distributional assumptions in the literature, which require the population and finite sample subdifferentials to be continuous in the Hausdorff metric, yet still provide tight convergence rates. These guarantees lead to new insights into the nonsmooth landscapes of such objectives within finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10289v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ruan</dc:creator>
    </item>
    <item>
      <title>Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift</title>
      <link>https://arxiv.org/abs/2405.10302</link>
      <description>arXiv:2405.10302v1 Announce Type: cross 
Abstract: As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10302v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Ge, Debarghya Mukherjee, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Continuous Time Locally Stationary Wavelet Processes</title>
      <link>https://arxiv.org/abs/2310.12788</link>
      <description>arXiv:2310.12788v3 Announce Type: replace 
Abstract: This article introduces the class of continuous time locally stationary wavelet processes. Continuous time models enable us to properly provide scale-based time series models for irregularly-spaced observations for the first time, while also permitting a spectral representation of the process over a continuous range of scales. We derive results for both the theoretical setting, where we assume access to the entire process sample path, and a more practical one, which develops methods for estimating the quantities of interest from sampled time series. The latter estimates are accurately computable in reasonable time by solving the relevant linear integral equation using the iterative thresholding method due to Daubechies, Defrise and De Mol. Appropriate smoothing techniques are also developed and applied in this new setting. We exemplify our new methods by computing spectral and autocovariance estimates on irregularly-spaced heart rate data obtained from a recent sleep-state study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12788v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Antonio Palasciano, Marina I. Knight, Guy P. Nason</dc:creator>
    </item>
    <item>
      <title>Optimal linear prediction with functional observations: Why you can use a simple post-dimension reduction estimator</title>
      <link>https://arxiv.org/abs/2401.06326</link>
      <description>arXiv:2401.06326v3 Announce Type: replace 
Abstract: This paper investigates optimal linear prediction for a random function in an infinite-dimensional Hilbert space. We analyze the mean square prediction error (MSPE) associated with a linear predictor, revealing that non-unique solutions that minimize the MSPE generally exist, and consistent estimation is often impossible even if a unique solution exists. However, this paper shows that it is still feasible to construct an asymptotically optimal linear operator, for which the empirical MSPE approaches the minimal achievable level. Remarkably, standard post-dimension reduction estimators, widely employed in the literature, serve as such estimators under minimal conditions. This finding affirms the use of standard post-dimension reduction estimators as a way to achieve the minimum MSPE without requiring a careful examination of various technical conditions commonly required in functional linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06326v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Won-Ki Seo</dc:creator>
    </item>
    <item>
      <title>A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability</title>
      <link>https://arxiv.org/abs/2405.07910</link>
      <description>arXiv:2405.07910v3 Announce Type: replace 
Abstract: Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the non-differential error assumption (NDEA) could be overly stringent in practice. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe2) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R2) in the regression of Xep against X may sometimes be sufficient to measure Pe2. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE requires controlling for EPC, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment. This paper provides maximum insight into when AEE(Xep) is an appropriate surrogate of AEE(X) and how to measure the difference between these two. Differential errors could be addressed and may not compromise causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07910v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Honghyok Kim</dc:creator>
    </item>
    <item>
      <title>A Consistent ICM-based $\chi^2$ Specification Test</title>
      <link>https://arxiv.org/abs/2208.13370</link>
      <description>arXiv:2208.13370v2 Announce Type: replace-cross 
Abstract: In spite of the omnibus property of Integrated Conditional Moment (ICM) specification tests, they are not commonly used in empirical practice owing to, e.g., the non-pivotality of the test and the high computational cost of available bootstrap schemes especially in large samples. This paper proposes specification and mean independence tests based on a class of ICM metrics termed the generalized martingale difference divergence (GMDD). The proposed tests exhibit consistency, asymptotic $\chi^2$-distribution under the null hypothesis, and computational efficiency. Moreover, they demonstrate robustness to heteroskedasticity of unknown form and can be adapted to enhance power towards specific alternatives. A power comparison with classical bootstrap-based ICM tests using Bahadur slopes is also provided. Monte Carlo simulations are conducted to showcase the proposed tests' excellent size control and competitive power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13370v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feiyu Jiang, Emmanuel Selorm Tsyawo</dc:creator>
    </item>
    <item>
      <title>Exploring the Complexity of Deep Neural Networks through Functional Equivalence</title>
      <link>https://arxiv.org/abs/2305.11417</link>
      <description>arXiv:2305.11417v3 Announce Type: replace-cross 
Abstract: We investigate the complexity of deep neural networks through the lens of functional equivalence, which posits that different parameterizations can yield the same network function. Leveraging the equivalence property, we present a novel bound on the covering number for deep neural networks, which reveals that the complexity of neural networks can be reduced. Additionally, we demonstrate that functional equivalence benefits optimization, as overparameterized networks tend to be easier to train since increasing network width leads to a diminishing volume of the effective parameter space. These findings can offer valuable insights into the phenomenon of overparameterization and have implications for understanding generalization and optimization in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11417v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guohao Shen</dc:creator>
    </item>
    <item>
      <title>Robust Point Matching with Distance Profiles</title>
      <link>https://arxiv.org/abs/2312.12641</link>
      <description>arXiv:2312.12641v2 Announce Type: replace-cross 
Abstract: While matching procedures based on pairwise distances are conceptually appealing and thus favored in practice, theoretical guarantees for such procedures are rarely found in the literature. We propose and analyze matching procedures based on distance profiles that are easily implementable in practice, showing these procedures are robust to outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12641v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Yuehaw Khoo</dc:creator>
    </item>
    <item>
      <title>An invitation to the sample complexity of quantum hypothesis testing</title>
      <link>https://arxiv.org/abs/2403.17868</link>
      <description>arXiv:2403.17868v3 Announce Type: replace-cross 
Abstract: Quantum hypothesis testing (QHT) has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of QHT, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on QHT, we characterize the sample complexity of binary QHT in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple QHT. In more detail, we prove that the sample complexity of symmetric binary QHT depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary QHT depends logarithmically on the inverse type II error probability and inversely on the quantum relative entropy, provided that the type II error probability is sufficiently small. We then provide lower and upper bounds on the sample complexity of multiple QHT, with it remaining an intriguing open question to improve these bounds. The final part of our paper outlines and reviews how sample complexity of QHT is relevant to a broad swathe of research areas and can enhance understanding of many fundamental concepts, including quantum algorithms for simulation and search, quantum learning and classification, and foundations of quantum mechanics. As such, we view our paper as an invitation to researchers coming from different communities to study and contribute to the problem of sample complexity of QHT, and we outline a number of open directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17868v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde</dc:creator>
    </item>
  </channel>
</rss>
