<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Dec 2024 05:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Measuring Partial Reachability in the Public Internet</title>
      <link>https://arxiv.org/abs/2412.09711</link>
      <description>arXiv:2412.09711v1 Announce Type: new 
Abstract: The Internet provides global connectivity by virtue of a public core -- the routable public IP addresses that host services and to which cloud, enterprise, and home networks connect. Today the public core faces many challenges to uniform, global reachability: firewalls and access control lists, commercial disputes that stretch for days or years, and government-mandated sanctions. We define two algorithms to detect partial connectivity: Taitao detects peninsulas of persistent, partial connectivity, and Chiloe detects islands, when one or more computers are partitioned from the public core. These new algorithms apply to existing data collected by multiple long-lived measurement studies. We evaluate these algorithms with rigorous measurements from two platforms: Trinocular, where 6 locations observe 5M networks frequently, RIPE Atlas, where 10k locations scan the DNS root frequently, and validate adding a third: CAIDA Ark, where 171 locations traceroute to millions of networks daily. Root causes suggest that most peninsula events (45%) are routing transients, but most peninsula-time (90%) is due to long-lived events (7%). We show that the concept of peninsulas and islands can improve existing measurement systems. They identify measurement error and persistent problems in RIPE's DNSmon that are $5\times$ to $9.7\times$ larger than the operationally important changes of interest. They explain previously contradictory results in several outage detection systems. Peninsulas are at least as common as Internet outages, posing new research direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09711v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guillermo Baltra, Tarang Saluja, Yuri Pradkin, John Heidemann</dc:creator>
    </item>
    <item>
      <title>TelApart: Differentiating Network Faults from Customer-Premise Faults in Cable Broadband Networks</title>
      <link>https://arxiv.org/abs/2412.09740</link>
      <description>arXiv:2412.09740v1 Announce Type: new 
Abstract: Two types of radio frequency (RF) impairments frequently occur in a cable broadband network: impairments that occur inside a cable network and impairments occur at the edge of the broadband network, i.e., in a subscriber's premise. Differentiating these two types of faults is important, as different faults require different types of technical personnel to repair them. Presently, the cable industry lacks publicly available tools to automatically diagnose the type of fault. In this work, we present TelApart, a fault diagnosis system for cable broadband networks. TelApart uses telemetry data collected by the Proactive Network Maintenance (PNM) infrastructure in cable networks to effectively differentiate the type of fault. Integral to TelApart's design is an unsupervised machine learning model that groups cable devices sharing similar anomalous patterns together. We use metrics derived from an ISP's customer trouble tickets to programmatically tune the model's hyper-parameters so that an ISP can deploy TelApart in various conditions without hand-tuning its hyper-parameters. We also address the data challenge that the telemetry data collected by the PNM system contain numerous missing, duplicated, and unaligned data points. Using real-world data contributed by a cable ISP, we show that TelApart can effectively identify different types of faults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09740v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiyao Hu, Zhenyu Zhou, Xiaowei Yang</dc:creator>
    </item>
    <item>
      <title>Spectrum and RAN Sharing: How to Avoid Cross-Subsidization While Taking Full Advantage of Massive MU-MIMO?</title>
      <link>https://arxiv.org/abs/2412.09747</link>
      <description>arXiv:2412.09747v1 Announce Type: new 
Abstract: Motivated by the need to use spectrum more efficiently, this paper investigates fine grained spectrum sharing (FGSS) in Multi-User massive MIMO (MU-mMIMO) systems where a neutral host enables users from different operators to share the same resource blocks. To be accepted by operators, FGSS must i) guarantee isolation so that the load of one operator does not impact the performance of another, and ii) avoid cross-subsidization whereby one operator gains more from sharing than another.
  We first formulate and solve an offline problem to assess the potential performance gains of FGSS with respect to the static spectrum sharing case, where operators have fixed separate sub-bands, and find that the gains can be significant, motivating the development for online solutions for FGSS. Transitioning from an offline to an online study presents unique challenges, including the lack of apriori knowledge regarding the performance of the fixed sharing case that is required to ensure isolation and cross-subsidization avoidance. We overcome these challenges and propose an online algorithm that is fast and significantly outperforms the static case.
  The main finding is that FGSS for a MU-mMIMO downlink system is doable in a way that is ``safe" to operators and brings large gains in spectrum efficiency (e.g., for 4 operators, a gain above 60\% is seen in many cases).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09747v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdalla Hussein, Patrick Mitran, Catherine Rosenberg</dc:creator>
    </item>
    <item>
      <title>Dual-Zone Hard-Core Model for RTS/CTS Handshake Analysis in WLANs</title>
      <link>https://arxiv.org/abs/2412.09953</link>
      <description>arXiv:2412.09953v1 Announce Type: new 
Abstract: This paper introduces a new stochastic geometry-based model to analyze the Request-to-Send/Clear-to-Send (RTS/CTS) handshake mechanism in wireless local area networks (WLANs). We develop an advanced hard-core point process model, termed the dual-zone hard-core process (DZHCP), which extends traditional hard-core models to capture the spatial interactions and exclusion effects introduced by the RTS/CTS mechanism. This model integrates key parameters accounting for the thinning effects imposed by RTS/CTS, enabling a refined characterization of active transmitters in the network. Analytical expressions are derived for the intensity of the DZHCP, the mean interference, and an approximation of the success probability, providing insight into how network performance depends on critical design parameters. Our results provide better estimates of interference levels and success probability, which could inform strategies for better interference management and improved performance in future WLAN designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09953v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhong, Zhuoling Chen, Wenyi Zhang, Martin Haenggi</dc:creator>
    </item>
    <item>
      <title>NetOrchLLM: Mastering Wireless Network Orchestration with Large Language Models</title>
      <link>https://arxiv.org/abs/2412.10107</link>
      <description>arXiv:2412.10107v1 Announce Type: new 
Abstract: The transition to 6G networks promises unprecedented advancements in wireless communication, with increased data rates, ultra-low latency, and enhanced capacity. However, the complexity of managing and optimizing these next-generation networks presents significant challenges. The advent of large language models (LLMs) has revolutionized various domains by leveraging their sophisticated natural language understanding capabilities. However, the practical application of LLMs in wireless network orchestration and management remains largely unexplored. Existing literature predominantly offers visionary perspectives without concrete implementations, leaving a significant gap in the field. To address this gap, this paper presents NETORCHLLM, a wireless NETwork ORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse wireless-specific models from wireless communication communities using their language understanding and generation capabilities. A comprehensive framework is introduced, demonstrating the practical viability of our approach and showcasing how LLMs can be effectively harnessed to optimize dense network operations, manage dynamic environments, and improve overall network performance. NETORCHLLM bridges the theoretical aspirations of prior research with practical, actionable solutions, paving the way for future advancements in integrating generative AI technologies within the wireless communications sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10107v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Abdallah, Abdullatif Albaseer, Abdulkadir Celik, Mohamed Abdallah, Ahmed M. Eltawil</dc:creator>
    </item>
    <item>
      <title>Design a Reliable Communication Network for handling a Smart Factory Applications using Time Sensitive Networks and Emerging Technologies</title>
      <link>https://arxiv.org/abs/2412.10243</link>
      <description>arXiv:2412.10243v1 Announce Type: new 
Abstract: This paper presents a comprehensive approach to designing a reliable communication network for the main Smart Factory applications, leveraging Time Sensitive Networks (TSN) and emerging technologies. As the manufacturing sector evolves under Industry 4.0, the integration of digital technologies and the Industrial Internet of Things (IIoT) necessitate robust communication frameworks capable of addressing diverse industrial applications requirements in terms of latency, bandwidth, and reliability. However, the traditional networks do not meet the requirements of the main smart factory applications together, such as Remote control and safety applications which considered as strict real time applications, So TSN mechanisms, including Strict Priority (SP), Credit-Based Shaping (CBS), Time-Aware Shaping (TAS) and Frame Preemption (FP), have been explored to enhance data flow and support real-time functionalities of such applications. Moreover, the work employs H.265 compression technology based on edge computing concept to mitigate the impact consuming more bandwidth (such as Augmented Reality (AR) application) on overall network performance. Through several scenarios, enhanced network reliability and reduced end-to-end latency have been demonstrated, thereby addressing the challenges posed by the diverse requirements of Smart Factory applications facilitating the seamless integration of time-sensitive and non-time-sensitive applications within a unified communication network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10243v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yazen S. Sheet, Mohammed Younis Thanoun, Firas S. Alsharbaty</dc:creator>
    </item>
    <item>
      <title>Online Adaptive Real-Time Beamforming Design for Dynamic Environments in Cell-Free Systems</title>
      <link>https://arxiv.org/abs/2412.09629</link>
      <description>arXiv:2412.09629v1 Announce Type: cross 
Abstract: In this paper, we consider real-time beamforming design for dynamic wireless environments with varying channels and different numbers of access points (APs) and users in cell-free systems. Specifically, a sum-rate maximization optimization problem is formulated for the beamforming design in dynamic wireless environments of cell-free systems. To efficiently solve it, a high-generalization network (HGNet) is proposed to adapt to the changing numbers of APs and users. Then, a high-generalization beamforming module is also designed in HGNet to extract the valuable features for the varying channels, and we theoretically prove that such a high-generalization beamforming module is able to reduce the upper bound of the generalization error. Subsequently, by online adaptively updating about 3% of the parameters of HGNet, an online adaptive updating (OAU) algorithm is proposed to enable the online adaptive real-time beamforming design for improving the sum rate. Numerical results demonstrate that the proposed HGNet with OAU algorithm achieves a higher sum rate with a lower computational cost on the order of milliseconds, thus realizing the real-time beamforming design for dynamic wireless environments in cell-free systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09629v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanghui Chen, Zheng Wang, Hongxin Lin, Pengguang Du, Yongming Huang</dc:creator>
    </item>
    <item>
      <title>The PET Paradox: How Amazon Instrumentalises PETs in Sidewalk to Entrench Its Infrastructural Power</title>
      <link>https://arxiv.org/abs/2412.09994</link>
      <description>arXiv:2412.09994v1 Announce Type: cross 
Abstract: Recent applications of Privacy Enhancing Technologies (PETs) reveal a paradox. PETs aim to alleviate power asymmetries, but can actually entrench the infrastructural power of companies implementing them vis-\`a-vis other public and private organisations. We investigate whether and how this contradiction manifests with an empirical study of Amazon's cloud connectivity service called Sidewalk. In 2021, Amazon remotely updated Echo and Ring devices in consumers' homes, to transform them into Sidewalk "gateways". Compatible Internet of Things (IoT) devices, called "endpoints", can connect to an associated "Application Server" in Amazon Web Services (AWS) through these gateways. We find that Sidewalk is not just a connectivity service, but an extension of Amazon's cloud infrastructure as a software production environment for IoT manufacturers. PETs play a prominent role in this pursuit: we observe a two-faceted PET paradox. First, suppressing some information flows allows Amazon to promise narrow privacy guarantees to owners of Echo and Ring devices when "flipping" them into gateways. Once flipped, these gateways constitute a crowdsourced connectivity infrastructure that covers 90% of the US population and expands their AWS offerings. We show how novel information flows, enabled by Sidewalk connectivity, raise greater surveillance and competition concerns. Second, Amazon governs the implementation of these PETs, requiring manufacturers to adjust their device hardware, operating system and software; cloud use; factory lines; and organisational processes. Together, these changes turn manufacturers' endpoints into accessories of Amazon's computational infrastructure; further entrenching Amazon's infrastructural power. We argue that power analyses undergirding PET design should go beyond analysing information flows. We propose future steps for policy and tech research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09994v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thijmen van Gend, Donald Jay Bertulfo, Seda G\"urses</dc:creator>
    </item>
    <item>
      <title>Adversarial Robustness of Bottleneck Injected Deep Neural Networks for Task-Oriented Communication</title>
      <link>https://arxiv.org/abs/2412.10265</link>
      <description>arXiv:2412.10265v1 Announce Type: cross 
Abstract: This paper investigates the adversarial robustness of Deep Neural Networks (DNNs) using Information Bottleneck (IB) objectives for task-oriented communication systems. We empirically demonstrate that while IB-based approaches provide baseline resilience against attacks targeting downstream tasks, the reliance on generative models for task-oriented communication introduces new vulnerabilities. Through extensive experiments on several datasets, we analyze how bottleneck depth and task complexity influence adversarial robustness. Our key findings show that Shallow Variational Bottleneck Injection (SVBI) provides less adversarial robustness compared to Deep Variational Information Bottleneck (DVIB) approaches, with the gap widening for more complex tasks. Additionally, we reveal that IB-based objectives exhibit stronger robustness against attacks focusing on salient pixels with high intensity compared to those perturbing many pixels with lower intensity. Lastly, we demonstrate that task-oriented communication systems that rely on generative models to extract and recover salient information have an increased attack surface. The results highlight important security considerations for next-generation communication systems that leverage neural networks for goal-oriented compression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10265v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Furutanpey, Pantelis A. Frangoudis, Patrik Szabo, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Distributed Computation Offloading for Energy Provision Minimization in WP-MEC Networks with Multiple HAPs</title>
      <link>https://arxiv.org/abs/2411.00397</link>
      <description>arXiv:2411.00397v2 Announce Type: replace 
Abstract: This paper investigates a wireless powered mobile edge computing (WP-MEC) network with multiple hybrid access points (HAPs) in a dynamic environment, where wireless devices (WDs) harvest energy from radio frequency (RF) signals of HAPs, and then compute their computation data locally (i.e., local computing mode) or offload it to the chosen HAPs (i.e., edge computing mode). In order to pursue a green computing design, we formulate an optimization problem that minimizes the long-term energy provision of the WP-MEC network subject to the energy, computing delay and computation data demand constraints. The transmit power of HAPs, the duration of the wireless power transfer (WPT) phase, the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing are jointly optimized adapting to the time-varying generated computation data and wireless channels of WDs. To efficiently address the formulated non-convex mixed integer programming (MIP) problem in a distributed manner, we propose a Two-stage Multi-Agent deep reinforcement learning-based Distributed computation Offloading (TMADO) framework, which consists of a high-level agent and multiple low-level agents. The high-level agent residing in all HAPs optimizes the transmit power of HAPs and the duration of the WPT phase, while each low-level agent residing in each WD optimizes its offloading decision, time allocation for offloading and CPU frequency for local computing. Simulation results show the superiority of the proposed TMADO framework in terms of the energy provision minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00397v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoying Liu, Anping Chen, Kechen Zheng, Kaikai Chi, Bin Yang, Tarik Taleb</dc:creator>
    </item>
    <item>
      <title>To Adopt or Not to Adopt L4S-Compatible Congestion Control? Understanding Performance in a Partial L4S Deployment</title>
      <link>https://arxiv.org/abs/2411.10952</link>
      <description>arXiv:2411.10952v2 Announce Type: replace 
Abstract: With few exceptions, the path to deployment for any Internet technology requires that there be some benefit to unilateral adoption of the new technology. In an Internet where the technology is not fully deployed, is an individual better off sticking to the status quo, or adopting the new technology? This question is especially relevant in the context of the Low Latency, Low Loss, Scalable Throughput (L4S) architecture, where the full benefit is realized only when compatible protocols (scalable congestion control, accurate ECN, and flow isolation at queues) are adopted at both endpoints of a connection and also at the bottleneck router. In this paper, we consider the perspective of the sender of an L4S flow using scalable congestion control, without knowing whether the bottleneck router uses an L4S queue, or whether other flows sharing the bottleneck queue are also using scalable congestion control. We show that whether the sender uses TCP Prague or BBRv2 as the scalable congestion control, it cannot be assured that it will not harm or be harmed by another flow sharing the bottleneck link. We further show that the harm is not necessarily mitigated when a scalable flow shares a bottleneck with multiple classic flows. Finally, we evaluate the approach of BBRv3, where scalable congestion control is used only when the path delay is small, with ECN feedback ignored otherwise, and show that it does not solve the coexistence problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10952v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatih Berkay Sarpkaya, Fraida Fund, Shivendra Panwar</dc:creator>
    </item>
    <item>
      <title>Key Focus Areas and Enabling Technologies for 6G</title>
      <link>https://arxiv.org/abs/2412.07029</link>
      <description>arXiv:2412.07029v2 Announce Type: replace 
Abstract: We provide a taxonomy of a dozen enabling network architectures, protocols, and technologies that will define the evolution from 5G to 6G. These technologies span the network protocol stack, different target deployment environments, and various perceived levels of technical maturity. We outline four areas of societal focus that will be impacted by these technologies, and overview several research directions that hold the potential to address the problems in these important focus areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07029v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher G. Brinton, Mung Chiang, Kwang Taik Kim, David J. Love, Michael Beesley, Morris Repeta, John Roese, Per Beming, Erik Ekudden, Clara Li, Geng Wu, Nishant Batra, Amitava Ghosh, Volker Ziegler, Tingfang Ji, Rajat Prakash, John Smee</dc:creator>
    </item>
    <item>
      <title>A Framework for testing Federated Learning algorithms using an edge-like environment</title>
      <link>https://arxiv.org/abs/2407.12980</link>
      <description>arXiv:2407.12980v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) is a machine learning paradigm in which many clients cooperatively train a single centralized model while keeping their data private and decentralized. FL is commonly used in edge computing, which involves placing computer workloads (both hardware and software) as close as possible to the edge, where the data is being created and where actions are occurring, enabling faster response times, greater data privacy, and reduced data transfer costs. However, due to the heterogeneous data distributions/contents of clients, it is non-trivial to accurately evaluate the contributions of local models in global centralized model aggregation. This is an example of a major challenge in FL, commonly known as data imbalance or class imbalance. In general, testing and assessing FL algorithms can be a very difficult and complex task due to the distributed nature of the systems. In this work, a framework is proposed and implemented to assess FL algorithms in a more easy and scalable way. This framework is evaluated over a distributed edge-like environment managed by a container orchestration platform (i.e. Kubernetes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12980v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2024.107626</arxiv:DOI>
      <arxiv:journal_reference>Future Generation Computer Systems (Elsevier), Volume 166, May 2025, 107626</arxiv:journal_reference>
      <dc:creator>Felipe Machado Schwanck, Marcos Tomazzoli Leipnitz, Joel Lu\'is Carbonera, Juliano Araujo Wickboldt</dc:creator>
    </item>
    <item>
      <title>Low-Latency Scalable Streaming for Event-Based Vision</title>
      <link>https://arxiv.org/abs/2412.07889</link>
      <description>arXiv:2412.07889v2 Announce Type: replace-cross 
Abstract: Recently, we have witnessed the rise of novel ``event-based'' camera sensors for high-speed, low-power video capture. Rather than recording discrete image frames, these sensors output asynchronous ``event'' tuples with microsecond precision, only when the brightness change of a given pixel exceeds a certain threshold. Although these sensors have enabled compelling new computer vision applications, these applications often require expensive, power-hungry GPU systems, rendering them incompatible for deployment on the low-power devices for which event cameras are optimized. Whereas receiver-driven rate adaptation is a crucial feature of modern video streaming solutions, this topic is underexplored in the realm of event-based vision systems. On a real-world event camera dataset, we first demonstrate that a state-of-the-art object detection application is resilient to dramatic data loss, and that this loss may be weighted towards the end of each temporal window. We then propose a scalable streaming method for event-based data based on Media Over QUIC, prioritizing object detection performance and low latency. The application server can receive complementary event data across several streams simultaneously, and drop streams as needed to maintain a certain latency. With a latency target of 5 ms for end-to-end transmission across a small network, we observe an average reduction in detection mAP as low as 0.36. With a more relaxed latency target of 50 ms, we observe an average mAP reduction as low as 0.19.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07889v2</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Hamara, Benjamin Kilpatrick, Alex Baratta, Brendon Kofink, Andrew C. Freeman</dc:creator>
    </item>
  </channel>
</rss>
