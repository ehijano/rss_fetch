<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 May 2024 04:01:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>BILTS: A novel bi-invariant local trajectory-shape descriptor for rigid-body motion</title>
      <link>https://arxiv.org/abs/2405.04392</link>
      <description>arXiv:2405.04392v1 Announce Type: cross 
Abstract: Measuring the similarity between motions and established motion models is crucial for motion analysis, recognition, generation, and adaptation. To enhance similarity measurement across diverse contexts, invariant motion descriptors have been proposed. However, for rigid-body motion, few invariant descriptors exist that are bi-invariant, meaning invariant to both the body and world reference frames used to describe the motion. Moreover, their robustness to singularities is limited. This paper introduces a novel Bi-Invariant Local Trajectory-Shape descriptor (BILTS) and a corresponding dissimilarity measure. Mathematical relationships between BILTS and existing descriptors are derived, providing new insights into their properties. The paper also includes an algorithm to reproduce the motion from the BILTS descriptor, demonstrating its bidirectionality and usefulness for trajectory generation. Experimental validation using datasets of daily-life activities shows the higher robustness of the BILTS descriptor compared to the bi-invariant ISA descriptor. This higher robustness supports the further application of bi-invariant descriptors for motion recognition and generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04392v1</guid>
      <category>cs.RO</category>
      <category>cs.CG</category>
      <category>cs.CV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arno Verduyn, Erwin Aertbeli\"en, Glenn Maes, Joris De Schutter, Maxim Vochten</dc:creator>
    </item>
    <item>
      <title>On polynomials associated to Voronoi diagrams of point sets and crossing numbers</title>
      <link>https://arxiv.org/abs/2304.12238</link>
      <description>arXiv:2304.12238v2 Announce Type: replace-cross 
Abstract: Three polynomials are defined for given sets $S$ of $n$ points in general position in the plane: The Voronoi polynomial with coefficients the numbers of vertices of the order-$k$ Voronoi diagrams of $S$, the circle polynomial with coefficients the numbers of circles through three points of $S$ enclosing $k$ points of $S$, and the $E_{\leq k}$ polynomial with coefficients the numbers of (at most $k$)-edges of $S$. We present several formulas for the rectilinear crossing number of $S$ in terms of these polynomials and their roots. We also prove that the roots of the Voronoi polynomial lie on the unit circle if, and only if, $S$ is in convex position. Further, we present bounds on the location of the roots of these polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12238v2</guid>
      <category>math.CO</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Merc\`e Claverol, Andrea de las Heras-Parrilla, David Flores-Pe\~naloza, Clemens Huemer, David Orden</dc:creator>
    </item>
    <item>
      <title>Local certification of geometric graph classes</title>
      <link>https://arxiv.org/abs/2311.16953</link>
      <description>arXiv:2311.16953v3 Announce Type: replace-cross 
Abstract: The goal of local certification is to locally convince the vertices of a graph $G$ that $G$ satisfies a given property. A prover assigns short certificates to the vertices of the graph, then the vertices are allowed to check their certificates and the certificates of their neighbors, and based only on this local view, they must decide whether $G$ satisfies the given property. If the graph indeed satisfies the property, all vertices must accept the instance, and otherwise at least one vertex must reject the instance (for any possible assignment of certificates). The goal is to minimize the size of the certificates.
  In this paper we study the local certification of geometric and topological graph classes. While it is known that in $n$-vertex graphs, planarity can be certified locally with certificates of size $O(\log n)$, we show that several closely related graph classes require certificates of size $\Omega(n)$. This includes penny graphs, unit-distance graphs, (induced) subgraphs of the square grid, 1-planar graphs, and unit-square graphs. These bounds are tight up to a constant factor and give the first known examples of hereditary (and even monotone) graph classes for which the certificates must have linear size. For unit-disk graphs we obtain a lower bound of $\Omega(n^{1-\delta})$ for any $\delta&gt;0$ on the size of the certificates, and an upper bound of $O(n \log n)$. The lower bounds are obtained by proving rigidity properties of the considered graphs, which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16953v3</guid>
      <category>cs.DM</category>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Defrain, Louis Esperet, Aur\'elie Lagoutte, Pat Morin, Jean-Florent Raymond</dc:creator>
    </item>
    <item>
      <title>Accelerating Material Property Prediction using Generically Complete Isometry Invariants</title>
      <link>https://arxiv.org/abs/2401.15089</link>
      <description>arXiv:2401.15089v2 Announce Type: replace-cross 
Abstract: Periodic material or crystal property prediction using machine learning has grown popular in recent years as it provides a computationally efficient replacement for classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a finite number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous and generically complete isometry invariant for periodic point sets, as a representation for our learning algorithm. The PDD distinguished all (more than 660 thousand) periodic crystals in the Cambridge Structural Database as purely periodic sets of points without atomic types. We develop a transformer model with a modified self-attention mechanism that combines PDD with compositional information via a spatial encoding method. This model is tested on the crystals of the Materials Project and Jarvis-DFT databases and shown to produce accuracy on par with state-of-the-art methods while being several times faster in both training and prediction time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15089v2</guid>
      <category>cs.LG</category>
      <category>cs.CG</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-024-59938-z</arxiv:DOI>
      <arxiv:journal_reference>Sci Rep 14, 10132 (2024)</arxiv:journal_reference>
      <dc:creator>Jonathan Balasingham, Viktor Zamaraev, Vitaliy Kurlin</dc:creator>
    </item>
    <item>
      <title>A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)</title>
      <link>https://arxiv.org/abs/2404.01568</link>
      <description>arXiv:2404.01568v3 Announce Type: replace-cross 
Abstract: We propose VecKM, a local point cloud geometry encoder that is descriptive and efficient to compute. VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point cloud. Such representation's descriptiveness is supported by two theorems that validate its ability to reconstruct and preserve the similarity of the local shape. Unlike existing encoders downsampling the local point cloud, VecKM constructs the local geometry encoding using all neighboring points, producing a more descriptive encoding. Moreover, VecKM is efficient to compute and scalable to large point cloud inputs: VecKM reduces the memory cost from $(n^2+nKd)$ to $(nd+np)$; and reduces the major runtime cost from computing $nK$ MLPs to $n$ MLPs, where $n$ is the size of the point cloud, $K$ is the neighborhood size, $d$ is the encoding dimension, and $p$ is a marginal factor. The efficiency is due to VecKM's unique factorizable property that eliminates the need of explicitly grouping points into neighbors. In the normal estimation task, VecKM demonstrates not only 100x faster inference speed but also highest accuracy and strongest robustness. In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point transformer baselines, and runs consistently faster by up to 10 times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01568v3</guid>
      <category>cs.CV</category>
      <category>cs.CG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dehao Yuan, Cornelia Ferm\"uller, Tahseen Rabbani, Furong Huang, Yiannis Aloimonos</dc:creator>
    </item>
  </channel>
</rss>
