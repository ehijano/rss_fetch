<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Feb 2026 02:33:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simulation-Based Study of AI-Assisted Channel Adaptation in UAV-Enabled Cellular Networks</title>
      <link>https://arxiv.org/abs/2602.13199</link>
      <description>arXiv:2602.13199v1 Announce Type: new 
Abstract: This paper presents a simulation based study of Artificial Intelligence assisted communication channel adaptation in Unmanned Aerial Vehicle enabled cellular networks. The considered system model includes communication channel Ground Base Station Aerial Repeater UAV Base Station Cluster of Cellular Network Users. The primary objective of the study is to investigate the impact of adaptive channel parameter control on communication performance under dynamically changing interference conditions. A lightweight supervised machine learning approach based on linear regression is employed to implement cognitive channel adaptation. The AI model operates on packet level performance indicators and enables real time adjustment of Transaction Size in response to variations in Bit Error Rate and effective Data Rate. A custom simulation environment is developed to generate training and testing datasets and to evaluate system behavior under both static and adaptive channel configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13199v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrii Grekhov, Volodymyr Kharchenko, Vasyl Kondratiuk</dc:creator>
    </item>
    <item>
      <title>Traffic Simulation in Ad Hoc Network of Flying UAVs with Generative AI Adaptation</title>
      <link>https://arxiv.org/abs/2602.13200</link>
      <description>arXiv:2602.13200v1 Announce Type: new 
Abstract: The purpose of this paper is to model traffic in Ad Hoc network of Unmanned Aerial Vehicles and demonstrate a way for adapting communication channel using Artificial Intelligence. The modeling was based on the original model of Ad Hoc network including 20 Unmanned Aerial Vehicles. The dependences of packet loss on the packet size for different transmission powers, on the packet size for different frequencies, on Unmanned Aerial Vehicles flight area and on the number of Unmanned Aerial Vehicles were obtained and analyzed. The implementation of adaptive data transmission is presented in the program code. The dependences of packet loss, power and transaction size on time during Artificial Intelligence adaptation are shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13200v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrii Grekhov, Volodymyr Kharchenko, Vasyl Kondratiuk</dc:creator>
    </item>
    <item>
      <title>CLF-ULP: Cross-Layer Fusion-Based Link Prediction in Dynamic Multiplex UAV Networks</title>
      <link>https://arxiv.org/abs/2602.13201</link>
      <description>arXiv:2602.13201v1 Announce Type: new 
Abstract: In complex Unmanned Aerial Vehicle (UAV) networks, UAVs can establish dynamic and heterogeneous links with one another for various purposes, such as communication coverage, collective sensing, and task collaboration. These interactions give rise to dynamic multiplex UAV networks, where each layer represents a distinct type of interaction among UAVs. Understanding how such links form and evolve is both of theoretical interest and of practical importance for the control and maintenance of networked UAV systems. In this paper, we first develop a dynamic multiplex network model for UAV networks to characterize their dynamic and heterogeneous link properties. We then propose a cross-layer fusion-based deep learning model, termed CLF-ULP, to predict future inter-UAV links based on historical topology data. CLF-ULP incorporates graph attention networks to extract topological features within each layer and perform a cross-layer attention fusion to capture inter-layer dependencies. Furthermore, a shared-parameter long short-term memory network is employed to model the temporal evolution of each layer. To improve embedding quality and link prediction performance, we develop a joint loss function that considers both intra-layer and inter-layer UAV adjacency. Extensive experiments on simulated UAV datasets under diverse mobility patterns demonstrate that CLF-ULP achieves state-of-the-art performance in predicting links within dynamic multiplex UAV networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13201v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cunlai Pu, Fangrui Wu, Zhe Wang, Xiangbo Shu</dc:creator>
    </item>
    <item>
      <title>Enhancing NOMA Handover Performance Using Hybrid AI-Driven Modulated Deterministic Sequences</title>
      <link>https://arxiv.org/abs/2602.13202</link>
      <description>arXiv:2602.13202v1 Announce Type: new 
Abstract: Non-Orthogonal Multiple Access (NOMA) is an information-theoretical approach used in 5G networks to improve spectral efficiency, but it is prone to interference during handovers. In this work, we propose a hybrid method that combines Gold-Walsh modulated sequences with Deep Q-Networks (DQN) to intelligently manage interference during NOMA handovers. This method optimizes sequence selection and power allocation dynamically. As a result, it achieves a 95.2\% handover success rate, which is an improvement of up to 23.1 percentage points. It also delivers up to 28\% throughput gain and reduces interference by up to 41\% in various mobility scenarios. All improvements are statistically significant (\(p &lt; 0.001\)). The DQN trains in \(4{,}200 \pm 400\) episodes with a complexity of \(O(N \log N + d \cdot h + \log B)\) and can be deployed in real-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13202v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumita Majhi, G Vasantha Reddy, Pinaki Mitra</dc:creator>
    </item>
    <item>
      <title>Adversarial Network Imagination: Causal LLMs and Digital Twins for Proactive Telecom Mitigation</title>
      <link>https://arxiv.org/abs/2602.13203</link>
      <description>arXiv:2602.13203v1 Announce Type: new 
Abstract: Telecommunication networks experience complex failures such as fiber cuts, traffic overloads, and cascading outages. Existing monitoring and digital twin systems are largely reactive, detecting failures only after service degradation occurs. We propose Adversarial Network Imagination, a closed-loop framework that integrates a Causal Large Language Model (LLM), a Knowledge Graph, and a Digital Twin to proactively generate, simulate, and evaluate adversarial network failures. The Causal LLM produces structured failure scenarios grounded in network dependencies encoded in the Knowledge Graph. These scenarios are executed within a Digital Twin to measure performance degradation and evaluate mitigation strategies. By iteratively refining scenarios based on simulation feedback, the framework shifts network operations from reactive troubleshooting toward anticipatory resilience analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13203v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vignesh Sriram, Yuqiao Meng, Luoxi Tang, Zhaohan Xi</dc:creator>
    </item>
    <item>
      <title>Hybrid Secure Routing in Mobile Ad-hoc Networks (MANETSs)</title>
      <link>https://arxiv.org/abs/2602.13204</link>
      <description>arXiv:2602.13204v1 Announce Type: new 
Abstract: Because wireless communication is dynamic and has inherent defects, routing algorithms are crucial in the quickly evolving field of mobile ad hoc networks, or MANETs This study looks at the many security problems that MANETs encounter. These problems, which pose major risks to network performance, include flooding, sinkholes, and black hole assaults to address these challenges. We introduce the Hybrid Secure Routing Protocol (HSRP), which enhances the security and robustness of routing operations by fusing trust-based tactics with cryptographic approaches. HSRP combines the strengths of both proactive and reactive routing strategies, enabling it to adapt dynamically to evolving network conditions while protecting against malicious activities. We use extensive simulations with Network Simulator (NS-2) and a thorough review of the literature to assess HSRP's performance under different attack scenarios. The results show that, in comparison to traditional protocols, HSRP increases throughput and decreases latency, hence improving routing efficiency while simultaneously bolstering data transfer security. With uses in vital domains including military operations and disaster response, this study provides a scalable and workable approach for safe routing in MANETs. The findings highlight how crucial it is to include cutting-edge security features in routing protocol design to guarantee the dependability and integrity of MANETs in practical situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13204v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soundes Oumaima Boufaida, Abdemadjid Benmachiche, Majda Maatallah, Chaouki Chemam</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning-Enabled Dynamic Code Assignment for Ultra-Dense IoT Networks: A NOMA-Based Approach to Massive Device Connectivity</title>
      <link>https://arxiv.org/abs/2602.13205</link>
      <description>arXiv:2602.13205v1 Announce Type: new 
Abstract: Ultra-dense IoT networks require an effective non-orthogonal multiple access (NOMA) scheme, yet they experience intense interference because of fixed code assignment. We suggest a reinforcement learning (RL) model of dynamic Gold code assignment in IoT-NOMA networks. Our Markov Decision Process which is IoT aware is a joint optimization of throughput, energy efficiency, and fairness. Two RL algorithms are created, including Natural Policy Gradient (NPG) to learn stable discrete actions and Deep Deterministic Policy Gradient (DDPG) with continuous code embedding. Under smart city conditions, NPG can attain throughput of 11.6% and energy efficiency of 15.8 likewise superior to its performance with a static allocation. Nonetheless, the performance is worse in organized industrial settings, and the reliability is minimal (0-2%), which points to the fact that dynamic code assignment is not a sufficient measure of ultra-reliable IoT and needs to be supplemented by power control or retransmission schemes. The work offers a basis to the RL-based resource allocation in massive IoT network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13205v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumita Majhi, Kishan Thakkar, Pinaki Mitra</dc:creator>
    </item>
    <item>
      <title>Toward Resource-Efficient Collaboration of Large AI Models in Mobile Edge Networks</title>
      <link>https://arxiv.org/abs/2602.13206</link>
      <description>arXiv:2602.13206v1 Announce Type: new 
Abstract: The collaboration of large artificial intelligence (AI) models in mobile edge networks has emerged as a promising paradigm to meet the growing demand for intelligent services at the network edge. By enabling multiple devices to cooperatively execute submodels or subtasks, collaborative AI enhances inference efficiency and service quality with constrained resources. However, deploying large AI models in such environments remains challenging due to the intrinsic mismatch between model complexity and the limited computation, memory, and communication resources in edge networks. This article provides a comprehensive overview of the system architecture for collaborative AI in mobile edge networks, along with representative application scenarios in transportation and healthcare. We further present recent advances in resource-efficient collaboration techniques, categorized into spatial and temporal approaches. The major spatial approaches include federated tuning, mixture of experts, patch-based diffusion, and hierarchical diffusion. Meanwhile, the important temporal approaches encompass split learning, cascading inference, speculative decoding, and routing inference. Building upon these foundations, we propose a multi-stage diffusion framework that enables elastic distribution of large generative models across heterogeneous edge resources. Experimental results demonstrate that our framework achieves performance improvement in both efficiency and adaptability for data generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13206v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MNET.2025.3650049</arxiv:DOI>
      <dc:creator>Peichun Li, Liping Qian, Dusit Niyato, Shiwen Mao, Yuan Wu</dc:creator>
    </item>
    <item>
      <title>A Safety-Constrained Reinforcement Learning Framework for Reliable Wireless Autonomy</title>
      <link>https://arxiv.org/abs/2602.13207</link>
      <description>arXiv:2602.13207v1 Announce Type: new 
Abstract: Artificial intelligence (AI) and reinforcement learning (RL) have shown significant promise in wireless systems, enabling dynamic spectrum allocation, traffic management, and large-scale Internet of Things (IoT) coordination. However, their deployment in mission-critical applications introduces the risk of unsafe emergent behaviors, such as UAV collisions, denial-of-service events, or instability in vehicular networks. Existing safety mechanisms are predominantly reactive, relying on anomaly detection or fallback controllers that intervene only after unsafe actions occur, which cannot guarantee reliability in ultra-reliable low-latency communication (URLLC) settings. In this work, we propose a proactive safety-constrained RL framework that integrates proof-carrying control (PCC) with empowerment-budgeted (EB) enforcement. Each agent action is verified through lightweight mathematical certificates to ensure compliance with interference constraints, while empowerment budgets regulate the frequency of safety overrides to balance safety and autonomy. We implement this framework on a wireless uplink scheduling task using Proximal Policy Optimization (PPO). Simulation results demonstrate that the proposed PCC+EB controller eliminates unsafe transmissions while preserving system throughput and predictable autonomy. Compared with unconstrained and reactive baselines, our method achieves provable safety guarantees with minimal performance degradation. These results highlight the potential of proactive safety constrained RL to enable trustworthy wireless autonomy in future 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13207v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdikarim Mohamed Ibrahim, Rosdiadee Nordin</dc:creator>
    </item>
    <item>
      <title>Large Language Model (LLM)-enabled Reinforcement Learning for Wireless Network Optimization</title>
      <link>https://arxiv.org/abs/2602.13210</link>
      <description>arXiv:2602.13210v1 Announce Type: new 
Abstract: Enhancing future wireless networks presents a significant challenge for networking systems due to diverse user demands and the emergence of 6G technology. While reinforcement learning (RL) is a powerful framework, it often encounters difficulties with high-dimensional state spaces and complex environments, leading to substantial computational demands, distributed intelligence, and potentially inconsistent outcomes. Large language models (LLMs), with their extensive pretrained knowledge and advanced reasoning capabilities, offer promising tools to enhance RL in optimizing 6G wireless networks. We explore RL models augmented by LLMs, emphasizing their roles and the potential benefits of their synergy in wireless network optimization. We then examine LLM-enabled RL across various protocol layers: physical, data link, network, transport, and application layers. Additionally, we propose an LLM-assisted state representation and semantic extraction to enhance the multi-agent reinforcement learning (MARL) framework. This approach is applied to service migration and request routing, as well as topology graph generation in unmanned aerial vehicle (UAV)-satellite networks. Through case studies, we demonstrate that our framework effectively performs optimization of wireless network. Finally, we outline prospective research directions for LLM-enabled RL in wireless network optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13210v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Zheng, Ruichen Zhang, Dusit Niyato, Haijun Zhang, Jiacheng Wang, Hongyang Du, Jiawen Kang, Zehui Xiong</dc:creator>
    </item>
    <item>
      <title>An Overlay Multicast Routing Method Based on Network Situational Aware-ness and Hierarchical Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2602.13211</link>
      <description>arXiv:2602.13211v1 Announce Type: new 
Abstract: Compared with IP multicast, Overlay Multicast (OM) offers better compatibility and flexible deployment in heterogeneous, cross-domain networks. However, traditional OM struggles to adapt to dynamic traffic due to unawareness of physical resource states, and existing reinforcement learning methods fail to decouple OM's tightly coupled multi-objective nature, leading to high complexity, slow convergence, and instability. To address this, we propose MA-DHRL-OM, a multi-agent deep hierarchical reinforcement learning approach. Using SDN's global view, it builds a traffic-aware model for OM path planning. The method decomposes OM tree construction into two stages via hierarchical agents, reducing action space and improving convergence stability. Multi-agent collaboration balances multi-objective optimization while enhancing scalability and adaptability. Experiments show MA-DHRL-OM outperforms existing methods in delay, bandwidth utilization, and packet loss, with more stable convergence and flexible routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13211v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miao Ye, Yanye Chen, Yong Wang, Cheng Zhu, Qiuxiang Jiang, Gai Huang, Feng Ding</dc:creator>
    </item>
    <item>
      <title>Network-Adaptive Cloud Preprocessing for Visual Neuroprostheses</title>
      <link>https://arxiv.org/abs/2602.13216</link>
      <description>arXiv:2602.13216v1 Announce Type: new 
Abstract: Cloud-based machine learning is increasingly explored as a preprocessing strategy for next-generation visual neuroprostheses, where advanced scene understanding may exceed the computational and energy constraints of battery-powered visual processing units (VPUs). Offloading computation to remote servers enables the use of state-of-the-art vision models, but also introduces sensitivity to network latency, jitter, and packet loss, which can disrupt the temporal consistency of the delivered neural stimulus. In this work, we examine the feasibility of cloud-assisted visual preprocessing for artificial vision by framing remote inference as a perceptually constrained systems problem. We present a network-adaptive cloud-assisted pipeline in which real-time round-trip-time (RTT) feedback is used to dynamically modulate image resolution, compression, and transmission rate, explicitly prioritizing temporal continuity under adverse network conditions. Using a Raspberry Pi 4 as a simulated VPU and a client-server architecture, we evaluate system performance across a range of realistic wireless network regimes. Results show that adaptive visual encoding substantially reduces end-to-end latency during network congestion, with only modest degradation of global scene structure, while boundary precision degrades more sharply. Together, these findings delineate operating regimes in which cloud-assisted preprocessing may remain viable for future visual neuroprostheses and underscore the importance of network-aware adaptation for maintaining perceptual stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13216v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayi Liu, Yilin Wang, Michael Beyeler</dc:creator>
    </item>
    <item>
      <title>An Agentic AI Control Plane for 6G Network Slice Orchestration, Monitoring, and Trading</title>
      <link>https://arxiv.org/abs/2602.13227</link>
      <description>arXiv:2602.13227v1 Announce Type: new 
Abstract: 6G networks are expected to be AI-native, intent-driven, and economically programmable, requiring fundamentally new approaches to network slice orchestration. Existing slicing frameworks, largely designed for 5G, rely on static policies and manual workflows and are ill-suited for the dynamic, multi-domain, and service-centric nature of emerging 6G environments. In this paper, we propose an agentic AI control plane architecture for 6G network slice orchestration, monitoring, and trading that treats orchestration as a holistic control function encompassing slice planning, deployment, continuous monitoring, and economically informed decision-making. The proposed control plane is realized as a layered architecture in which multiple cooperating AI agents. To support flexible and on-demand slice utilization, the control plane incorporates market-aware orchestration capabilities, allowing slice requirements, pricing, and availability to be jointly considered during orchestration decisions. A natural language interface, implemented using the Model Context Protocol (MCP), enables users and applications to interact with control-plane functions through intent-based queries while enforcing safety and policy constraints. To ensure responsible and explainable autonomy, the control plane integrates fine-tuned large language models organized as a multi-model consortium, governed by a dedicated reasoning model. The proposed approach is evaluated using a real-world testbed with multiple mobile core instances (e.g Open5GS) integrated with Ericsson's RAN infrastructure. The results demonstrate that combining agentic autonomy, closed-loop SLA assurance, market-aware orchestration, and natural language control enables a scalable and adaptive 6G-native control plane for network slice management, highlighting the potential of agentic AI as a foundational mechanism for future 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13227v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eranga Bandara, Ross Gore, Sachin Shetty, Ravi Mukkamala, Tharaka Hewa, Abdul Rahman, Xueping Liang, Safdar H. Bouk, Amin Hass, Peter Foytik, Ng Wee Keong, Kasun De Zoysa</dc:creator>
    </item>
    <item>
      <title>Pocket RAG: On-Device RAG for First Aid Guidance in Offline Mobile Environment</title>
      <link>https://arxiv.org/abs/2602.13229</link>
      <description>arXiv:2602.13229v1 Announce Type: new 
Abstract: In disaster scenarios or remote areas, first responders often lose network connectivity when providing first aid. In such situations, server-based AI systems fail to provide critical guidance. To address this issue, we present a lightweight, mobile-based retrieval-augmented generation system for small language models (SLMs) that can run directly on Android devices. Our system integrates a mobile-friendly optimized pipeline featuring Hybrid RAG, selective compression, batched prompt decoding, and quantization caching. Despite the model's small size, our RAG-based system achieves 94.5\% accuracy for physical first aid and 97.0\% for psychological first aid. Additionally, we reduce response time from 14.2s to 3.7s, achieving a nearly 4x speedup. These results prove that our system is practical and can deliver reliable first aid guidance even without internet connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13229v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Ho Kang, Hyunjoon Lee, Hyeonjeong Cha, Minkyu Choi, Sungsoo Lim</dc:creator>
    </item>
    <item>
      <title>An Explainable Failure Prediction Framework for Neural Networks in Radio Access Networks</title>
      <link>https://arxiv.org/abs/2602.13231</link>
      <description>arXiv:2602.13231v1 Announce Type: new 
Abstract: As 5G networks continue to evolve to deliver high speed, low latency, and reliable communications, ensuring uninterrupted service has become increasingly critical. While millimeter wave (mmWave) frequencies enable gigabit data rates, they are highly susceptible to environmental factors, often leading to radio link failures (RLF). Predictive models leveraging radio and weather data have been proposed to address this issue; however, many operate as black boxes, offering limited transparency for operational deployment. This work bridges that gap by introducing a framework that combines explainability based feature pruning with model refinement. Our framework can be integrated into state of the art predictors such as GNN Transformer and LSTM based architectures for RLF prediction, enabling the development of accurate and explainability guided models in 5G networks. It provides insights into the contribution of input features and the decision making logic of neural networks, leading to lighter and more scalable models. When applied to RLF prediction, our framework unveils that weather data contributes minimally to the forecast in extensive real world datasets, which informs the design of a leaner model with 50 percent fewer parameters and improved F1 scores with respect to the state of the art solution. Ultimately, this work empowers network providers to evaluate and refine their neural network based prediction models for better interpretability, scalability, and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13231v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khaleda Papry, Francesco Spinnato, Marco Fiore, Mirco Nanni, Israat Haque</dc:creator>
    </item>
    <item>
      <title>Securing SIM-Assisted Wireless Networks via Quantum Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2602.13238</link>
      <description>arXiv:2602.13238v1 Announce Type: new 
Abstract: Stacked intelligent metasurfaces (SIMs) have recently emerged as a powerful wave-domain technology that enables multi-stage manipulation of electromagnetic signals through multilayer programmable architectures. While SIMs offer unprecedented degrees of freedom for enhancing physical-layer security, their extremely large number of meta-atoms leads to a high-dimensional and strongly coupled optimization space, making conventional design approaches inefficient and difficult to scale. Moreover, existing deep reinforcement learning (DRL) techniques suffer from slow convergence and performance degradation in dynamic wireless environments with imperfect knowledge of passive eavesdroppers. To overcome these challenges, we propose a hybrid quantum proximal policy optimization (Q-PPO) framework for SIM-assisted secure communications, which jointly optimizes transmit power allocation and SIM phase shifts to maximize the average secrecy rate under power and quality-of-service constraints. Specifically, a parameterized quantum circuit is embedded into the actor network, forming a hybrid classical-quantum policy architecture that enhances policy representation capability and exploration efficiency in high-dimensional continuous action spaces. Extensive simulations demonstrate that the proposed Q-PPO scheme consistently outperforms DRL baselines, achieving approximately 15% higher secrecy rates and 30% faster convergence under imperfect eavesdropper channel state information. These results establish Q-PPO as a powerful optimization paradigm for SIM-enabled secure wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13238v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Le-Hung Hoang, Quang-Trung Luu, Dinh Thai Hoang, Diep N. Nguyen, Van-Dinh Nguyen</dc:creator>
    </item>
    <item>
      <title>Embodied Intelligent Spectrum Management: A New Paradigm for Dynamic Spectrum Access</title>
      <link>https://arxiv.org/abs/2602.13245</link>
      <description>arXiv:2602.13245v1 Announce Type: new 
Abstract: Wireless communication is evolving into an agent era, where numerous intelligent agents equipped with perception, reasoning, and interaction capabilities will operate in highly dynamic wireless environments. To complete diverse complex tasks, agent communication will play a critical role, which enables autonomous information exchange with external tools, services, and other agents. This trendy movement will dramatically increase spectrum demand and result in unprecedented challenges for spectrum management. However, current spectrum management paradigms, including static spectrum allocation and intelligent management, lack the flexibility and generalization to accommodate the dynamic and heterogeneous demands of agent communication. The recent advancements in embodied intelligence (EI) bring a promising solution, and this article will provide our vision of an emerging embodied intelligent spectrum management (EISM) paradigm. We start with an architecture for EISM, elaborating its key enabling technologies. Then, a prototype platform is presented to demonstrate the advantages of EISM. Finally, key challenges and open issues are outlined to facilitate future research in this emerging field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13245v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihe Diao, Yuhang Wu, Hongtao Liang, Ming Xu, Rui Ding, Fuhui Zhou, Qihui Wu, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Modality-Tailored Age of Information for Multimodal Data in Edge Computing Systems</title>
      <link>https://arxiv.org/abs/2602.13269</link>
      <description>arXiv:2602.13269v1 Announce Type: new 
Abstract: As Internet of Things (IoT) systems scale and device heterogeneity grows, multimodal data have become ubiquitous. Meanwhile, evaluating the freshness of multimodal data is essential, as stale updates would delay task execution, degrade decision accuracy, and undermine safety in latency-sensitive services. However, existing freshness metrics such as Age of Information (AoI) are not suitable for multimodal data, as they do not capture modality-specific characteristics. In this paper, we propose a metric, namely, Modality-Tailored Age of Information (MAoI), to provide a unified and decision-relevant evaluation of freshness for resource management and policy optimization for multimodal data. This metric integrates modality-specific semantic and temporal characteristics, reflecting both age evolution and content importance for multimodal data in multi-access edge computing (MEC) systems. Then, the closed-form expression of the average MAoI is derived, and an MAoI minimization problem is formulated, where sampling intervals and offloading decisions are optimized with practical energy constraints. To effectively solve this problem, a Joint Sampling Offloading Optimization (JSO) algorithm is proposed to jointly optimize the sampling intervals and offloading decisions. It is a block coordinate descent-based algorithm where an optimal sampling-interval subalgorithm is used to update the sampling intervals, and an interference-aware best-response offloading subalgorithm is proposed to update the offloading decisions alternately. Finally, a comprehensive simulation is performed, confirming that the MAoI metric effectively quantifies multimodal freshness compared to traditional AoI, and the JSO algorithm significantly minimizes the average MAoI compared to state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13269v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Liu, Yifan Zhang, Xinyu Wang, Chao Yang, Kandaraj Piamrat, Stephan Sigg, Zheng Changr, Yusheng Ji</dc:creator>
    </item>
    <item>
      <title>Intent-driven Diffusion-based Path for Mobile Data Collector in IoT-enabled Dense WSNs</title>
      <link>https://arxiv.org/abs/2602.13277</link>
      <description>arXiv:2602.13277v1 Announce Type: new 
Abstract: Mobile data collection using controllable sinks is an effective approach to improve energy efficiency and data freshness in densely deployed wireless sensor networks (WSNs). However, existing path-planning methods are often heuristic-driven and lack the flexibility to adapt to high-level operational objectives under dynamic network conditions. In this paper, we propose ID2P2, a intent-driven diffusion-based path planning framework for jointly addresses rendezvous point selection and mobile data collector (MDC) tour construction in IoT-enabled dense WSNs. High-level intents, such as latency minimization, energy balancing, or coverage prioritization, are explicitly modeled and incorporated into a generative diffusion planning process that produces feasible and adaptive data collection trajectories. The proposed approach learns a trajectory prior that captures spatial node distribution and network characteristics, enabling the MDC to generate paths that align with specified intents while maintaining collision-free and energy-aware operation. Extensive simulations are conducted to evaluate the effectiveness of the proposed framework against conventional path-planning baselines. The results demonstrate that ID2P2 consistently outperforms representative baselines, achieving up to 25-30% reduction in tour completion time and travel overhead, approximately 10-30% improvement in data freshness, and 15-30% gains in energy efficiency and packet delivery performance, while maintaining higher throughput and fairness as network density increases, confirming its robustness and scalability for WSNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13277v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uma Mahesh Boda, Mallikharjuna Rao Nuka</dc:creator>
    </item>
    <item>
      <title>GraFSTNet: Graph-based Frequency SpatioTemporal Network for Cellular Traffic Prediction</title>
      <link>https://arxiv.org/abs/2602.13282</link>
      <description>arXiv:2602.13282v1 Announce Type: new 
Abstract: With rapid expansion of cellular networks and the proliferation of mobile devices, cellular traffic data exhibits complex temporal dynamics and spatial correlations, posing challenges to accurate traffic prediction. Previous methods often focus predominantly on temporal modeling or depend on predefined spatial topologies, which limits their ability to jointly model spatio-temporal dependencies and effectively capture periodic patterns in cellular traffic. To address these issues, we propose a cellular traffic prediction framework that integrates spatio-temporal modeling with time-frequency analysis. First, we construct a spatial modeling branch to capture inter-cell dependencies through an attention mechanism, minimizing the reliance on predefined topological structures. Second, we build a time-frequency modeling branch to enhance the representation of periodic patterns. Furthermore, we introduce an adaptive-scale LogCosh loss function, which adjusts the error penalty based on traffic magnitude, preventing large errors from dominating the training process and helping the model maintain relatively stable prediction accuracy across different traffic intensities. Experiments on three open-sourced datasets demonstrate that the proposed method achieves prediction performance superior to state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13282v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyi Li, Hui Ma, Fei Xing, Chunjiong Zhang, Ming Yan</dc:creator>
    </item>
    <item>
      <title>Benchmarking Anomaly Detection Across Heterogeneous Cloud Telemetry Datasets</title>
      <link>https://arxiv.org/abs/2602.13288</link>
      <description>arXiv:2602.13288v1 Announce Type: new 
Abstract: Anomaly detection is important for keeping cloud systems reliable and stable. Deep learning has improved time-series anomaly detection, but most models are evaluated on one dataset at a time. This raises questions about whether these models can handle different types of telemetry, especially in large-scale and high-dimensional environments.
  In this study, we evaluate four deep learning models, GRU, TCN, Transformer, and TSMixer. We also include Isolation Forest as a classical baseline. The models are tested across four telemetry datasets: the Numenta Anomaly Benchmark, Microsoft Cloud Monitoring dataset, Exathlon dataset, and IBM Console dataset. These datasets differ in structure, dimensionality, and labelling strategy. They include univariate time series, synthetic multivariate workloads, and real-world production telemetry with over 100,000 features.
  We use a unified training and evaluation pipeline across all datasets. The evaluation includes NAB-style metrics to capture early detection behaviour for datasets where anomalies persist over contiguous time intervals. This enables window-based scoring in settings where anomalies occur over contiguous time intervals, even when labels are recorded at the point level. The unified setup enables consistent analysis of model behaviour under shared scoring and calibration assumptions.
  Our results demonstrate that anomaly detection performance in cloud systems is governed not only by model architecture, but critically by calibration stability and feature-space geometry. By releasing our preprocessing pipelines, benchmark configuration, and evaluation artifacts, we aim to support reproducible and deployment-aware evaluation of anomaly detection systems for cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13288v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Saiful Islam, Andriy Miranskyy</dc:creator>
    </item>
    <item>
      <title>AGORA: Agentic Green Orchestration Architecture for Beyond 5G Networks</title>
      <link>https://arxiv.org/abs/2602.13290</link>
      <description>arXiv:2602.13290v1 Announce Type: new 
Abstract: Effective management and operational decision-making for complex mobile network systems present significant challenges, particularly when addressing conflicting requirements such as efficiency, user satisfaction, and energy-efficient traffic steering. The literature presents various approaches aimed at enhancing network management, including the Zero-Touch Network (ZTN) and Self-Organizing Network (SON); however, these approaches often lack a practical and scalable mechanism to consider human sustainability goals as input, translate them into energy-aware operational policies, and enforce them at runtime. In this study, we address this gap by proposing the AGORA: Agentic Green Orchestration Architecture for Beyond 5G Networks. AGORA embeds a local tool-augmented Large Language Model (LLM) agent in the mobile network control loop to translate natural-language sustainability goals into telemetry-grounded actions, actuating the User Plane Function (UPF) to perform energy-aware traffic steering. The findings indicate a strong latency-energy coupling in tool-driven control loops and demonstrate that compact models can achieve a low energy footprint while still facilitating correct policy execution, including non-zero migration behavior under stressed Multi-access Edge Computing (MEC) conditions. Our approach paves the way for sustainability-first, intent-driven network operations that align human objectives with executable orchestration in Beyond-5G infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13290v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, Maycon Peixoto, Flavio De Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>Cooperative Edge Caching with Large Language Model in Wireless Networks</title>
      <link>https://arxiv.org/abs/2602.13307</link>
      <description>arXiv:2602.13307v1 Announce Type: new 
Abstract: Cooperative edge caching in overlapping zones creates intricate coupling among Base Station (BS) decisions, making content replacement highly sensitive to topology and temporal reuse. While heuristics are often myopic and Deep Reinforcement Learning lacks robustness under dynamics, this paper proposes a Large Language Model (LLM)-based multi-BS orchestrator. The LLM acts as the sole autonomous engine, interacting with the environment via a validated text-to-action interface. Each time slot, the system renders environmental states -- including cache inventories and frequency statistics -- into prompts, parsing LLM-generated decisions against strict feasibility constraints. We align the model through a two-stage paradigm: Supervised Fine-Tuning on oracle trajectories for syntax and initialization, followed by Group Relative Policy Optimization. The latter employs an ``opportunity-aware'' reward that prioritizes multi-step cooperative gains relative to a No-Operation baseline. Evaluated on identical request traces, the orchestrator approaches exhaustive-search performance (0.610 vs.\ 0.617 in a 5-BS scenario), outperforms classical baselines (e.g., +4.1\% over least-frequently used), and demonstrates robust zero-shot transfer across varying cache capacities, library sizes, and user densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13307v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Yang, Wentao Wang, Lingtao Ouyang, Haijun Zhang</dc:creator>
    </item>
    <item>
      <title>Resilient and Freshness-Aware Scheduling for Industrial Multi-Hop IAB Networks: A Packet Duplication Approach</title>
      <link>https://arxiv.org/abs/2602.13311</link>
      <description>arXiv:2602.13311v1 Announce Type: new 
Abstract: In industrial millimeter-wave (mmWave) multi-hop Integrated Access and Backhaul (IAB) networks, dynamic blockages caused by moving obstacles pose a severe threat to robust and continuous networks. While Packet Duplication (PD) enhances reliability by path diversity, it inevitably doubles the traffic load, leading to severe congestion and degraded Age of Information (AoI). To navigate this reliability-congestion trade-off, we formulated an optimization problem in a multi-hop IAB scenario that minimizes the average AOI while satisfying strict queue stability constraints. We utilize Lyapunov optimization to transform the long-term stochastic optimization problem into tractable deterministic sub-problems. To solve these sub-problems efficiently, we propose a Resilient and Freshness-Aware Scheduling (RFAS) algorithm. Simulation results show that in blockage-prone environments, RFAS significantly outperforms baselines by maintaining a Packet Delivery Ratio (PDR) above 95\%. Crucially, it strictly guarantees queue stability under hard buffer constraints, whereas baselines suffer from buffer overflows. Furthermore, RFAS reduces the network load imbalance by 19\% compared to the baseline in high-frequency traffic scenarios. This confirms RFAS as a robust and sustainable solution for real-time industrial control loops.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13311v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Zhu, Siyu Lin, Zijing Wang, Qiao Ren, Xiaoheng Deng, Bo Ai</dc:creator>
    </item>
    <item>
      <title>Semantic Waveforms for AI-Native 6G Networks</title>
      <link>https://arxiv.org/abs/2602.13316</link>
      <description>arXiv:2602.13316v1 Announce Type: new 
Abstract: In this paper, we propose a semantic-aware waveform design framework for AI-native 6G networks that jointly optimizes physical layer resource usage and semantic communication efficiency and robustness, while explicitly accounting for the hardware constraints of RF chains. Our approach, called Orthogonal Semantic Sequency Division Multiplexing (OSSDM), introduces a parametrizable, orthogonal-base waveform design that enables controlled degradation of the wireless transmitted signal to preserve semantically significant content while minimizing resource consumption. We demonstrate that OSSDM not only reinforces semantic robustness against channel impairments but also improves semantic spectral efficiency by encoding meaningful information directly at the waveform level. Extensive numerical evaluations show that OSSDM outperforms conventional OFDM waveforms in spectral efficiency and semantic fidelity. The proposed semantic waveform co-design opens new research frontiers for AI-native, intelligent communication systems by enabling meaning-aware physical signal construction through the direct encoding of semantics at the waveform level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13316v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Hello, Mohamed Amine Hamoura, Francois Rivet, Emilio Calvanese Strinati</dc:creator>
    </item>
    <item>
      <title>3D Wi-Fi Signal Measurement in Realistic Digital Twin Testbed Environments Using Ray Tracing</title>
      <link>https://arxiv.org/abs/2602.13340</link>
      <description>arXiv:2602.13340v1 Announce Type: new 
Abstract: Accurate and efficient modeling of indoor wireless signal propagation is crucial for the deployment of next-generation Wi-Fi. This paper presents a digital twin-based measurement system that integrates real-world 3D environment reconstruction with deterministic ray tracing for physically grounded electromagnetic modeling. Building geometry is obtained through LiDAR scanning, followed by object segmentation and assignment of ITU-R standard material parameters. The propagation process is simulated with a GPU-accelerated ray-tracing engine that generates path-level channel attributes, including delay, power, angular dispersion, and Ricean K-factor. Under identical runtime constraints, the proposed system is evaluated against a commercial measurement simulator, demonstrating up to 21 dB higher path gain and consistently improved signal-to-interference-plus-noise ratio in line-of-sight conditions. Additionally, experiments against onsite RSSI measurements confirm a high spatial correlation of 0.98 after calibration, proving the system's fidelity in real-world settings. Furthermore, coverage analysis across 2.4 GHz, 5 GHz, and 6 GHz bands demonstrates the capability of system to model frequency-dependent material attenuation for Wi-Fi 6E/7 networks. Finally, the system offers interactive 3D visualization and on-demand data extraction, highlighting its potential for digital twin-driven wireless system design and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13340v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIM.2026.3660422</arxiv:DOI>
      <dc:creator>Mengyuan Wang, Haopeng Wang, Haiwei Dong, Abdulmotaleb El Saddik</dc:creator>
    </item>
    <item>
      <title>Location as a service with a MEC architecture</title>
      <link>https://arxiv.org/abs/2602.13358</link>
      <description>arXiv:2602.13358v1 Announce Type: new 
Abstract: In recent years, automated driving has become viable, and advanced driver assistance systems (ADAS) are now part of modern cars. These systems require highly precise positioning. In this paper, a cooperative approach to localization is presented. The GPS information from several road users is collected in a Mobile Edge Computing cloud, and the characteristics of GNSS positioning are used to provide lane-precise positioning for all participants by applying probabilistic filters and HD maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13358v1</guid>
      <category>cs.NI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICOIN59985.2024.10572033</arxiv:DOI>
      <dc:creator>Christopher Schahn, Jorin Kouril, Bernd Schaeufele, Ilja Radusch</dc:creator>
    </item>
    <item>
      <title>Parametric Traversal for Multi-Dimensional Cost-Aware Graph Reasoning</title>
      <link>https://arxiv.org/abs/2602.13369</link>
      <description>arXiv:2602.13369v1 Announce Type: new 
Abstract: Classical path search assumes complete graphs and scalar optimization metrics, yet real infrastructure networks are incomplete and require multi-dimensional evaluation. We introduce the concept of traversal: a generalization of paths that combines existing edges with gap transitions, missing but acceptable connections representing links that can be built. This abstraction captures how engineers actually reason about infrastructure: not just what exists, but what can be realized.
  We present a parametric framework that treats planned connections as first-class transitions, scales to large graphs through efficient candidate filtering, and uses multi-dimensional criteria to decide whether a traversal should continue to be explored or be abandoned. We evaluate the framework through representative scenarios in datacenter circuit design and optical route construction in telecommunication networks, demonstrating conditional feasibility, non-scalarizable trade-offs, and policy calibration capabilities beyond the reach of classical formulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13369v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Tacheny</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Feature Alignment and Weighted Fusion in Collaborative Perception Enabled by Network Synchronization and Age of Information</title>
      <link>https://arxiv.org/abs/2602.13439</link>
      <description>arXiv:2602.13439v1 Announce Type: new 
Abstract: Collaborative perception in Internet of Vehicles (IoV) aggregates multi-vehicle observations for broader scene coverage and improved decision-making. However, fusion quality degrades under spatiotemporal heterogeneity from unsynchronized clocks, communication delays, and motion variations across vehicles. Prior work mitigates these through spatial transformations or fixed time-offset corrections, overlooking time-varying clock drifts and delays that cause persistent feature misalignment. To overcome these, we propose a spatiotemporal feature alignment and weighted fusion framework. Specifically, network synchronization is designed to continuously compensate for clock state differences between vehicles and establish a common time reference, onto which all feature timestamps can be mapped. After synchronization, to align the freshness of received features since their generation, their Age of Information (AoI) is determined by estimating network delay with given feature size and link quality. Our spatiotemporal feature alignment then projects vehicles' features into one spatial coordinate and corrects them to a synchronized fusion instant using AoIs, enabling all features to describe the scene coherently. Furthermore, due to varying synchronization and alignment quality, we estimate their uncertainties and integrate with AoI to generate feature weights for efficient fusion, prioritizing fresh, reliable feature regions. Simulations show consistent perception accuracy improvements over strong baselines under clock drifts and link delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13439v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiaomei Han, Xianbin Wang, Minghui Liwang, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>SIDSense: Database-Free TV White Space Sensing for Disaster-Resilient Connectivity</title>
      <link>https://arxiv.org/abs/2602.13542</link>
      <description>arXiv:2602.13542v1 Announce Type: new 
Abstract: Small Island Developing States (SIDS) are disproportionately exposed to climate-driven disasters, yet often rely on fragile terrestrial networks that fail when they are most needed. TV White Space (TVWS) links offer long-range, low-power coverage; however, current deployments depend on Protocol to Access White Spaces (PAWS) database connectivity for channel authorization, creating a single point of failure during outages.
  We present SIDSense, an edge AI framework for database-free TVWS operation that preserves regulatory intent through a compliance-gated controller, audit logging, and graceful degradation. SIDSense couples CNN-based spectrum classification with a hybrid sensing-first, authorization-as-soon-as-possible workflow and co-locates sensing and video enhancement with a private 5G stack on a maritime vessel to sustain situational-awareness video backhaul.
  Field experiments in Barbados demonstrate sustained connectivity during simulated PAWS outages, achieving 94.2% sensing accuracy over 470-698 MHz with 23 ms mean decision latency, while maintaining zero missed 5G Layer-1 (L1) deadlines under GPU-aware scheduling. We release an empirical Caribbean TVWS propagation and occupancy dataset and look to contribute some of the components of the SIDSense pipeline to the open source community to accelerate resilient connectivity deployments in climate-vulnerable regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13542v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George M. Gichuru, Zoe Aiyanna M. Cayetano</dc:creator>
    </item>
    <item>
      <title>Multi-Modal Sensing and Fusion in mmWave Beamforming for Connected Vehicles: A Transformer Based Framework</title>
      <link>https://arxiv.org/abs/2602.13606</link>
      <description>arXiv:2602.13606v1 Announce Type: new 
Abstract: Millimeter wave (mmWave) communication, utilizing beamforming techniques to address the inherent path loss limitation, is considered as one of the key technologies to support ever increasing high throughput and low latency demands of connected vehicles. However, adopting standard defined beamforming approach in highly dynamic vehicular environments often incurs high beam training overheads and reduction in the available airtime for communications, which is mainly due to exchanging pilot signals and exhaustive beam measurements. To this end, we present a multi-modal sensing and fusion learning framework as a potential alternative solution to reduce such overheads. In this framework, we first extract the representative features from the sensing modalities by modality specific encoders, then, utilize multi-head cross-modal attention to learn dependencies and correlations between different modalities, and subsequently fuse the multimodal features to obtain predicted top-k beams so that the best line-of-sight links can be proactively established. To show the generalizability of the proposed framework, we perform a comprehensive experiment in four different vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) scenarios from real world multimodal and 60 GHz mmWave wireless sensing data. The experiment reveals that the proposed framework (i) achieves up to 96.72% accuracy on predicting top-15 beams correctly, (ii) incurs roughly 0.77 dB average power loss, and (iii) improves the overall latency and beam searching space overheads by 86.81% and 76.56% respectively for top-15 beams compared to standard defined approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13606v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVT.2026.3665294</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Vehicular Technology, 2026</arxiv:journal_reference>
      <dc:creator>Muhammad Baqer Mollah, Honggang Wang, Mohammad Ataul Karim, Hua Fang</dc:creator>
    </item>
    <item>
      <title>Parametric-Sensitivity Aware Retransmission for Efficient AI Downloading</title>
      <link>https://arxiv.org/abs/2602.13607</link>
      <description>arXiv:2602.13607v1 Announce Type: new 
Abstract: The edge artificial intelligence (AI) applications in next-generation mobile networks demand efficient AI-model downloading techniques to support real-time, on-device inference. However, transmitting high-dimensional AI models over wireless channels remains challenging due to limited communication resources. To address this issue, we propose a parametric-sensitivity-aware retransmission (PASAR) framework that manages radio-resource usage of different parameter packets according to their importance on model inference accuracy, known as parametric sensitivity. Empirical analysis reveals a highly right-skewed sensitivity distribution, indicating that only a small fraction of parameters significantly affect model performance. Leveraging this insight, we design a novel online retransmission protocol, i.e., the PASAR protocol, that adaptively terminates packet transmission based on real-time bit error rate (BER) measurements and the associated parametric sensitivity. The protocol employs an adaptive, round-wise stopping criterion, enabling heterogeneous, packet-level retransmissions that preserve overall model functionality but reduce overall latency. Extensive experiments across diverse deep neural network architectures and real-world datasets demonstrate that PASAR substantially outperforms classical hybrid automatic repeat request (HARQ) schemes in terms of communication efficiency and latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13607v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>You Zhou, Qunsong Zeng, Kaibin Huang</dc:creator>
    </item>
    <item>
      <title>Compact LLM Deployment and World Model Assisted Offloading in Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2602.13628</link>
      <description>arXiv:2602.13628v1 Announce Type: new 
Abstract: This paper investigates compact large language model (LLM) deployment and world-model-assisted inference offloading in mobile edge computing (MEC) networks. We first propose an edge compact LLM deployment (ECLD) framework that jointly applies structured pruning, low-bit quantization, and knowledge distillation to construct edge-deployable LLM variants, and we evaluate these models using four complementary metrics: accessibility, energy consumption, hallucination rate, and generalization accuracy. Building on the resulting compact models, we formulate an MEC offloading optimization problem that minimizes the long-term average inference latency subject to per-device energy budgets and LLM-specific quality-of-service constraints on effective accuracy and hallucination. To solve this problem under unknown and time-varying network dynamics, we develop a world model-proximal policy optimization (PPO) algorithm, which augments an on-policy PPO algorithm with a learned recurrent world model that provides improved value targets and short imagination rollouts. Extensive experiments on Llama-3.1-8B, Qwen3-8B, and Mistral-12B show that ECLD compresses base models by about 70-80% in storage (i.e., from 15.3 GB to 3.3 GB for Llama-3.1-8B) and reduces per-query energy consumption by up to 50%, while largely preserving accuracy and often lowering hallucination compared with quantization-only or pruning-only baselines. Moreover, they also show that world model-PPO speeds up convergence by about 50%, improves the final reward by 15.8% over vanilla PPO, and reduces average inference latency by 12-30% across different user populations, while satisfying the accuracy and hallucination constraints and approaching the generation quality of always-offloading with much of the efficiency of local execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13628v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruichen Zhang, Xiaofeng Luo, Jiayi He, Dusit Niyato, Jiawen Kang, Zehui Xiong, Yonghui Li</dc:creator>
    </item>
    <item>
      <title>LEAD-Drift: Real-time and Explainable Intent Drift Detection by Learning a Data-Driven Risk Score</title>
      <link>https://arxiv.org/abs/2602.13672</link>
      <description>arXiv:2602.13672v1 Announce Type: new 
Abstract: Intent-Based Networking (IBN) simplifies network management, but its reliability is challenged by "intent drift", where the network's state gradually deviates from its intended goal, often leading to silent failures. Conventional approaches struggle to detect the subtle, early stages of intent drift, raising alarms only when degradation is significant and failure is imminent, which limits their effectiveness for proactive assurance. To address this, we propose LEAD-Drift, a framework that detects intent drift in real time to enable proactive failure prevention. LEAD-Drift's core contribution is reformulating intent failure detection as a supervised learning problem by training a lightweight neural network on fixed-horizon labels to predict a future risk score. The model's raw output is then smoothed with an Exponential Moving Average (EMA) and passed through a statistically tuned threshold to generate robust, real-time alerts. Furthermore, we enhance the framework with two key features for operational intelligence: a multi-horizon modeling technique for dynamic time-to-failure estimation, and per-alert explainability using SHAP to identify root-cause KPIs. Our evaluation on a time-series dataset shows LEAD-Drift provides significantly earlier warnings, improving the average lead time by 7.3 minutes (+17.8\%) compared to a distance-based baseline. It also reduces alert noise by 80.2\% compared to a weighted-KPI heuristic, with only a minor trade-off in lead time. These results demonstrate that LEAD-Drift as a highly effective, interpretable, and operationally efficient solution for proactive network assurance in IBN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13672v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Kamrul Hossain, Walid Aljoby</dc:creator>
    </item>
    <item>
      <title>Agent-OSI: A Layered Protocol Stack Toward a Decentralized Internet of Agents</title>
      <link>https://arxiv.org/abs/2602.13795</link>
      <description>arXiv:2602.13795v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are accelerating the shift from an Internet of information to an Internet of Agents (IoA), where autonomous entities discover services, negotiate, execute tasks, and exchange value. Yet today's agents are still confined to platform silos and proprietary interfaces, lacking a common stack for interoperability, trust, and pay-per-use settlement. This article proposes \textit{Agent-OSI}, a six-layer reference stack for decentralized agent networking built on top of the existing Internet. Agent-OSI combines secure connectivity and A2A messaging, decentralized identity and authorization, settlement and metering, verifiable execution and provenance, and semantic interoperability for orchestration. In particular, we treat HTTP 402 (Payment Required) as an application-level payment challenge (analogous to HTTP 401 for authentication) that triggers escrow-based settlement and verifiable receipts (instantiated via a blockchain escrow in our prototype), rather than introducing a new network-layer protocol. We implement a prototype and evaluate cost and latency. Results show that keeping negotiation and delivery off-chain while preserving verifiable settlement reduces on-chain session costs by approximately 51\% compared with a standard Web3 baseline in our prototype setting, and that blockchain confirmation latency is often not the dominant factor for generative workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13795v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Xu, Taotao Wang, Yihan Xia, Shengli Zhang, Soung Chang Liew</dc:creator>
    </item>
    <item>
      <title>Agentic Assistant for 6G: Turn-based Conversations for AI-RAN Hierarchical Co-Management</title>
      <link>https://arxiv.org/abs/2602.13868</link>
      <description>arXiv:2602.13868v1 Announce Type: new 
Abstract: New generations of radio access networks (RAN), especially with native AI services are increasingly difficult for human engineers to manage in real-time. Enterprise networks are often managed locally, where expertise is scarce. Existing research has focused on creating Retrieval-Augmented Generation (RAG) LLMs that can help to plan and configure RAN and core aspects only. Co-management of RAN and edge AI is the gap, which creates hierarchical and dynamic problems that require turn-based human interactions. Here, we create an agentic network manager and turn-based conversation assistant that can understand human intent-based queries that match hierarchical problems in AI-RAN. The framework constructed consists of: (a) a user interface and evaluation dashboard, (b) an intelligence layer that interfaces with the AI-RAN, and (c) a knowledge layer for providing the basis for evaluations and recommendations. These form 3 layers of capability with the following validation performances (average response time 13s): (1) design and planning a service (78\% accuracy), (2) operating specific AI-RAN tools (89\% accuracy), and (3) tuning AI-RAN performance (67\%). These initial results indicate the universal challenges of hallucination but also fast response performance success that can really reduce OPEX costs for small scale enterprise users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13868v1</guid>
      <category>cs.NI</category>
      <category>cs.IR</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Udhaya Srinivasan, Weisi Guo</dc:creator>
    </item>
    <item>
      <title>Toward Autonomous O-RAN: A Multi-Scale Agentic AI Framework for Real-Time Network Control and Management</title>
      <link>https://arxiv.org/abs/2602.14117</link>
      <description>arXiv:2602.14117v1 Announce Type: new 
Abstract: Open Radio Access Networks (O-RAN) promise flexible 6G network access through disaggregated, software-driven components and open interfaces, but this programmability also increases operational complexity. Multiple control loops coexist across the service management layer and RAN Intelligent Controller (RIC), while independently developed control applications can interact in unintended ways. In parallel, recent advances in generative Artificial Intelligence (AI) are enabling a shift from isolated AI models toward agentic AI systems that can interpret goals, coordinate multiple models and control functions, and adapt their behavior over time. This article proposes a multi-scale agentic AI framework for O-RAN that organizes RAN intelligence as a coordinated hierarchy across the Non-Real-Time (Non-RT), Near-Real-Time (Near-RT), and Real-Time (RT) control loops: (i) A Large Language Model (LLM) agent in the Non-RT RIC translates operator intent into policies and governs model lifecycles. (ii) Small Language Model (SLM) agents in the Near-RT RIC execute low-latency optimization and can activate, tune, or disable existing control applications; and (iii) Wireless Physical-layer Foundation Model (WPFM) agents near the distributed unit provide fast inference close to the air interface. We describe how these agents cooperate through standardized O-RAN interfaces and telemetry. Using a proof-of-concept implementation built on open-source models, software, and datasets, we demonstrate the proposed agentic approach in two representative scenarios: robust operation under non-stationary conditions and intent-driven slice resource control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14117v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hojjat Navidan, Mohammad Cheraghinia, Jaron Fontaine, Mohamed Seif, Eli De Poorter, H. Vincent Poor, Ingrid Moerman, Adnan Shahid</dc:creator>
    </item>
    <item>
      <title>MILD: Multi-Intent Learning and Disambiguation for Proactive Failure Prediction in Intent-based Networking</title>
      <link>https://arxiv.org/abs/2602.14283</link>
      <description>arXiv:2602.14283v1 Announce Type: new 
Abstract: In multi-intent intent-based networks, a single fault can trigger co-drift where multiple intents exhibit symptomatic KPI degradation, creating ambiguity about the true root-cause intent. We present MILD, a proactive framework that reformulates intent assurance from reactive drift detection to fixed-horizon failure prediction with intent-level disambiguation. MILD uses a teacher-augmented Mixture-of-Experts where a gated disambiguation module identifies the root-cause intent while per-intent heads output calibrated risk scores. On a benchmark with non-linear failures and co-drifts, MILD provides 3.8\%--92.5\% longer remediation lead time and improves intent-level root-cause disambiguation accuracy by 9.4\%--45.8\% over baselines. MILD also provides per-alert KPI explanations, enabling actionable diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14283v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Kamrul Hossain, Walid Aljoby</dc:creator>
    </item>
    <item>
      <title>LiSFC-Search: Lifelong Search for Network SFC Optimization under Non-stationary Drifts</title>
      <link>https://arxiv.org/abs/2602.14360</link>
      <description>arXiv:2602.14360v1 Announce Type: new 
Abstract: Edge-cloud convergence is reshaping service provisioning across 5G/6G and computing power networks (CPNs). Service function chaining (SFC) requires continuously placing and scheduling virtual network functions (VNFs) chains under compute/bandwidth and end-to-end QoS constraints. Most SFC optimizers assume static or stationary networks, and degrade under long-term topology/resource changes (failures, upgrades, expansions) that induce non-stationary graph drifts. We propose LiSFC, a Lipschitz lifelong planner that transfers MCTS statistics across drifting network configurations using an MDP-distance bound. More precisely, we formulate the problem as a sequence of MDPs indexed by the underlying network graph and constraints, and we define a \emph{graph drift} metric that upper-bounds the LiZero MDP distance. This allows LiSFC to import theoretical guarantees on bias and sample efficiency from the LiZero framework while being tailored to cloud-network convergence. We then design \emph{LiSFC-Search}, an SFC-aware unified MCTS (UMCTS) procedure that uses transferable adaptive UCT (aUCT) bonuses to reuse search statistics from prior CPN configurations. Preliminary results on synthetic CPN topologies and SFC workloads show that LiSFC consistently reduces SFC blocking probability and improves tail delay compared to non-transfer MCTS and purely learning-based baselines, highlighting its potential as an AI/ML building block for cloud-network convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14360v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zuyuan Zhang, Vaneet Aggarwal, Tian Lan</dc:creator>
    </item>
    <item>
      <title>Bitcoin Under Stress: Measuring Infrastructure Resilience 2014-2025</title>
      <link>https://arxiv.org/abs/2602.14372</link>
      <description>arXiv:2602.14372v1 Announce Type: new 
Abstract: Bitcoin's design promises resilience through decentralization, yet physical infrastructure creates hidden dependencies. We present the first longitudinal study of Bitcoin's resilience to infrastructure failures using 11 years of P2P network data (2014-2025), 658 submarine cables, and 68 verified cable fault events. Using a Buldyrev-style cascade model with a country-level physical layer (225 countries, 354 submarine cable edges, 325 land border edges), we find that Bitcoin's clearnet percolation threshold $p_c \approx 0.72$-$0.92$ for random cable failures, declining 22% from $p_c \approx 0.92$ (2014-2017) to a minimum of $p_c = 0.72$ in 2021 during peak mining concentration. Targeted attacks reduce $p_c$ to 0.05-0.20. To address the 64% of nodes using Tor with unobservable locations, we develop a 4-layer multiplex model incorporating Tor relay infrastructure. Tor relay bandwidth concentrates in well-connected European countries, increasing resilience by $\Delta p_c \approx +0.02$-$+0.10$ rather than introducing fragility. Empirical validation shows 87% of cable faults caused less than 5% node impact. We contribute: (1) a multiplex percolation framework for overlay-underlay coupling with a 4-layer Tor relay model; (2) the first empirical measurement of Bitcoin's physical-layer resilience over a decade; and (3) evidence that Tor adoption amplifies resilience with distributional bounds under partial observability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14372v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Wu, Alexander Neumueller</dc:creator>
    </item>
    <item>
      <title>A Q-Learning Approach for Dynamic Resource Management in Three-Tier Vehicular Fog Computing</title>
      <link>https://arxiv.org/abs/2602.14390</link>
      <description>arXiv:2602.14390v1 Announce Type: new 
Abstract: In this paper, a method for predicting the resources required for an intelligent vehicle client using a three-layer vehicular computing architecture is proposed. This method leverages Q-Learning to optimize resource allocation and enhance overall system performance. This approach employs reinforcement learning capabilities to provide a dynamic and adaptive strategy for resource management in a fog computing environment. The key findings of this study indicate that Q-learning can effectively predict the appropriate allocation of resources by learning from past experiences and making informed decisions. Through continuous training and updating of the Q-learning agent, the system can adapt to changing conditions and make resource allocation decisions based on real-time information. The experimental results demonstrate the effectiveness of the proposed method in optimizing resource allocation. The Q-learning agent predicts the optimal values for memory, bandwidth, and processor. These predictions not only minimize resource consumption but also meet the performance requirements of the fog system. Implementations show that this method improves the average task processing time in compared to other methods evaluated in this study</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14390v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bahar Mojtabaei Ranani, Mahmood Ahmadi, Sajad Ahmadian</dc:creator>
    </item>
    <item>
      <title>ASA: Adaptive Smart Agent Federated Learning via Device-Aware Clustering for Heterogeneous IoT</title>
      <link>https://arxiv.org/abs/2602.14391</link>
      <description>arXiv:2602.14391v1 Announce Type: new 
Abstract: Federated learning (FL) has become a promising answer to facilitating privacy-preserving collaborative learning in distributed IoT devices. However, device heterogeneity is a key challenge because IoT networks include devices with very different computational powers, memory availability, and network environments. To this end, we introduce ASA (Adaptive Smart Agent). This new framework clusters devices adaptively based on real-time resource profiles and adapts customized models suited to every cluster's capability. ASA capitalizes on an intelligent agent layer that examines CPU power, available memory, and network environment to categorize devices into three levels: high-performance, mid-tier, and low-capability. Each level is provided with a model tuned to its computational power to ensure inclusive engagement across the network. Experimental evaluation on two benchmark datasets, MNIST and CIFAR-10, proves that ASA decreases communication burden by 43% to 50%, improves resource utilization by 43%, and achieves final model accuracies of 98.89% on MNIST and 85.30% on CIFAR-10. These results highlight ASA's efficacy in enhancing efficiency, scalability, and fairness in heterogeneous FL environments, rendering it a suitable answer for real-world IoT apps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14391v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Salimi, Saadat Izadi, Mahmood Ahmadi, Hadi Tabatabaee Malazi</dc:creator>
    </item>
    <item>
      <title>DORA: Dataflow Oriented Robotic Architecture</title>
      <link>https://arxiv.org/abs/2602.13252</link>
      <description>arXiv:2602.13252v1 Announce Type: cross 
Abstract: Robotic middleware serves as the foundational infrastructure, enabling complex robotic systems to operate in a coordinated and modular manner. In data-intensive robotic applications, especially in industrial scenarios, communication efficiency directly impact system responsiveness, stability, and overall productivity. However, existing robotic middleware exhibit several limitations: (1) they rely heavily on (de)serialization mechanisms, introducing significant overhead for large-sized data; (2) they lack efficient and flexible support for heterogeneous data sizes, particularly in intra-robot communication and Python-based execution environments. To address these challenges, we propose Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. We implement the proposed framework as an open-source system and evaluate it through extensive experiments in both simulation and real-world robotic environments. Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13252v1</guid>
      <category>cs.RO</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaodong Zhang, Baorui Lv, Xavier Tao, Xiong Wang, Jie Bao, Yong He, Yue Chen, Zijiang Yang</dc:creator>
    </item>
    <item>
      <title>COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception</title>
      <link>https://arxiv.org/abs/2602.13287</link>
      <description>arXiv:2602.13287v1 Announce Type: cross 
Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13287v1</guid>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shilpa Mukhopadhyay, Amit Roy-Chowdhury, Hang Qiu</dc:creator>
    </item>
    <item>
      <title>Instruction-Set Architecture for Programmable NV-Center Quantum Repeater Nodes</title>
      <link>https://arxiv.org/abs/2602.14995</link>
      <description>arXiv:2602.14995v1 Announce Type: cross 
Abstract: Programmability is increasingly central in emerging quantum network software stacks, yet the node-internal controller-to-hardware interface for quantum repeater devices remains under-specified. We introduce the idea of an instruction-set architecture (ISA) for controller-driven programmability of nitrogen-vacancy (NV) center quantum repeater nodes. Each node consists of an optically interfaced electron spin acting as a data qubit and a long-lived nuclear-spin register acting as a control program. We formalize two modes of programmability: (i) deterministic register control, where the nuclear register is initialized in a basis state to select a specific operation on the data qubit; and (ii) coherent register control, where the register is prepared in superposition, enabling coherent combinations of operations beyond classical programmability. Network protocols are expressed as controller-issued instruction vectors, which we illustrate through a compact realization of the BBPSSW purification protocol. We further show that coherent register control enables interferometric diagnostics such as fidelity witnessing and calibration, providing tools unavailable in classical programmability. Finally, we discuss scalability to multi-electron and multi-nuclear spin architectures and connection to Linear combination of unitaries (LCU) and Kraus formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14995v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinay Kumar, Claudio Cicconetti, Riccardo Bassoli, Marco Conti, Andrea Passarella</dc:creator>
    </item>
    <item>
      <title>Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses</title>
      <link>https://arxiv.org/abs/2504.14326</link>
      <description>arXiv:2504.14326v2 Announce Type: replace 
Abstract: Mobile metaverses are envisioned as a transformative digital ecosystem that delivers immersive, intelligent, and ubiquitous services through mobile devices. Driven by Large Language Models (LLMs) and Vision-Language Models (VLMs), Artificial Intelligence (AI) agents hold the potential to empower the creation, maintenance, and evolution of mobile metaverses, enabling seamless human-machine interaction and dynamic service adaptation. Currently, AI agents are primarily built upon cloud-based LLMs and VLMs. However, several challenges hinder their efficient deployment, including high service latency and a risk of sensitive data leakage during perception and processing. In this paper, we develop an edge-cloud collaboration-based federated AI agent construction framework in mobile metaverses. Specifically, Edge Servers (ESs), as agent infrastructures, first create agent modules in a distributed manner. The cloud server then integrates these modules into AI agents and deploys them at the edge, thereby enabling low-latency AI agent services for users. Considering that ESs may exhibit dynamic levels of willingness to participate in federated AI agent construction, we design a two-period dynamic contract model to continuously incentivize ESs to participate in agent module creation, effectively addressing the dynamic information asymmetry between the cloud server and ESs. Furthermore, we propose an Enhanced Diffusion Model-based Soft Actor-Critic (EDMSAC) algorithm to effectively generate optimal dynamic contracts. In the algorithm, we apply dynamic structured pruning to DM-based actor networks to enhance denoising efficiency and policy learning performance. Simulation results demonstrate that the EDMSAC algorithm outperforms the DMSAC algorithm by up to $23\%$ in optimal dynamic contract generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14326v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Jiawen Kang, Yang Zhang, Yue Zhong, Dusit Niyato, Jie Xu, Jianhang Tang, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction</title>
      <link>https://arxiv.org/abs/2507.14186</link>
      <description>arXiv:2507.14186v2 Announce Type: replace 
Abstract: The expansion of the low-altitude economy has underscored the significance of Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors. While accurate LANC forecasting hinges on the antenna beam patterns of Base Stations (BSs), these patterns are typically proprietary and not readily accessible. Operational parameters of BSs, which inherently contain beam information, offer an opportunity for data-driven low-altitude coverage prediction. However, collecting extensive low-altitude road test data is cost-prohibitive, often yielding only sparse samples per BS. This scarcity results in two primary challenges: imbalanced feature sampling due to limited variability in high-dimensional operational parameters against the backdrop of substantial changes in low-dimensional sampling locations, and diminished generalizability stemming from insufficient data samples. To overcome these obstacles, we introduce a dual strategy comprising expert knowledge-based feature compression and disentangled representation learning. The former reduces feature space complexity by leveraging communications expertise, while the latter enhances model generalizability through the integration of propagation models and distinct subnetworks that capture and aggregate the semantic representations of latent features. Experimental evaluation confirms the efficacy of our framework, yielding a 7% reduction in error compared to the best baseline algorithm. Real-network validations further attest to its reliability, achieving practical prediction accuracy with MAE errors at the 5dB level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14186v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2025.3637091</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Mobile Computing, early access, 2025</arxiv:journal_reference>
      <dc:creator>Xiaojie Li, Zhijie Cai, Nan Qi, Chao Dong, Guangxu Zhu, Haixia Ma, Qihui Wu, Shi Jin</dc:creator>
    </item>
    <item>
      <title>Virne: A Comprehensive Benchmark for RL-based Network Resource Allocation in NFV</title>
      <link>https://arxiv.org/abs/2507.19234</link>
      <description>arXiv:2507.19234v2 Announce Type: replace 
Abstract: Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19234v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Zhengyu Hu, Wei Wu, Leilei Ding, Qilin Fan, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models for Automated Reproduction of Networking Research Results</title>
      <link>https://arxiv.org/abs/2509.21074</link>
      <description>arXiv:2509.21074v3 Announce Type: replace 
Abstract: Code reproduction is a cornerstone of scientific validity, yet it remains a formidable challenge in computer networking research due to the scarcity of open-source implementations and the complexity of heterogeneous system architectures. While Large Language Models have demonstrated potential in code generation, existing code generation frameworks often fail to address the long-context constraints and intricate logical dependencies required to reproduce network systems from academic papers. To facilitate result reproduction, we introduce \emph{RepLLM}, an end-to-end multi-agent framework designed to automate the transformation of network research into executable code. RepLLM features a novel collaborative architecture comprising four specialized agents -- Content Parsing, Architecture Design, Code Generation, and Audit \&amp; Repair -- coordinated through an explicit \textit{Shared Memory} mechanism to ensure global context consistency. With the enhancement of Chain-of-Thought LLM reasoning and a sandbox-isolated static-dynamic debugging methodology, our framework effectively resolves semantic discrepancies and runtime errors. Extensive evaluations on representative papers from SIGCOMM and NSDI demonstrate that RepLLM significantly outperforms state-of-the-art baselines in generating compile-ready and logically correct systems. Results further demonstrate that RepLLM facilitates the reproduction of 80\% of the original benchmarks with only four hours of human intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21074v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yining Jiang, Yunxin Xu, Wenyun Xu, Yufan Zhu, Tangtang He, Haiying Huang, Letian Zhu, Qingyu Song, Qiang Su, Lizhao You, Lu Tang, Wanjin Feng, Yuchao Zhang, Linghe Kong, Qiao Xiang, Jiwu Shu</dc:creator>
    </item>
    <item>
      <title>TriCloudEdge: A multi-layer Cloud Continuum</title>
      <link>https://arxiv.org/abs/2602.02121</link>
      <description>arXiv:2602.02121v2 Announce Type: replace 
Abstract: TriCloudEdge is a scalable three-tier cloud continuum that integrates far-edge devices, intermediate edge nodes, and central cloud services, working in parallel as a unified solution. At the far edge, ultra-low-cost microcontrollers can handle lightweight AI tasks, while intermediate edge devices provide local intelligence, and the cloud tier offers large-scale analytics, federated learning, model adaptation, and global identity management. The proposed architecture enables multi-protocols and technologies (WebSocket, MQTT, HTTP) compared to a versatile protocol (Zenoh) to transfer diverse bidirectional data across the tiers, offering a balance between computational challenges and latency requirements. Comparative implementations between these two architectures demonstrate the trade-offs between resource utilization and communication efficiency. The results show that TriCloudEdge can distribute computational challenges to address latency and privacy concerns. The work also presents tests of AI model adaptation on the far edge and the computational effort challenges under the prism of parallelism. This work offers a perspective on the practical continuum challenges of implementation aligned with recent research advances addressing challenges across the different cloud levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02121v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Violettas, Lefteris Mamatas</dc:creator>
    </item>
    <item>
      <title>Resource-Efficient Personal Large Language Models Fine-Tuning with Collaborative Edge Computing</title>
      <link>https://arxiv.org/abs/2408.10746</link>
      <description>arXiv:2408.10746v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10746v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyuan Ye, Bei Ouyang, Tianyi Qian, Liekang Zeng, Jingyi Li, Jiangsu Du, Xiaowen Chu, Guoliang Xing, Xu Chen</dc:creator>
    </item>
    <item>
      <title>A Generalized Hierarchical Federated Learning Framework with Theoretical Guarantees</title>
      <link>https://arxiv.org/abs/2505.08145</link>
      <description>arXiv:2505.08145v2 Announce Type: replace-cross 
Abstract: Almost all existing hierarchical federated learning (FL) models are limited to two aggregation layers, restricting scalability and flexibility in complex, large-scale networks. In this work, we propose a Multi-Layer Hierarchical Federated Learning framework (QMLHFL), which appears to be the first study that generalizes hierarchical FL to arbitrary numbers of layers and network architectures through nested aggregation, while employing a layer-specific quantization scheme to meet communication constraints. We develop a comprehensive convergence analysis for QMLHFL and derive a general convergence condition and rate that reveal the effects of key factors, including quantization parameters, hierarchical architecture, and intra-layer iteration counts. Furthermore, we determine the optimal number of intra-layer iterations to maximize the convergence rate while meeting a deadline constraint that accounts for both communication and computation times. Our results show that QMLHFL consistently achieves high learning accuracy, even under high data heterogeneity, and delivers notably improved performance when optimized, compared to using randomly selected values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08145v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Seyed Mohammad Azimi-Abarghouyi, Carlo Fischione</dc:creator>
    </item>
    <item>
      <title>Age of Job Completion Minimization with Stable Queues</title>
      <link>https://arxiv.org/abs/2511.04630</link>
      <description>arXiv:2511.04630v2 Announce Type: replace-cross 
Abstract: We consider a time-slotted job-assignment system with a central server, N users and a machine which changes its state according to a Markov chain (hence called a Markov machine). The users submit their jobs to the central server according to a stochastic job arrival process. For each user, the server has a dedicated job queue. Upon receiving a job from a user, the server stores that job in the corresponding queue. When the machine is not working on a job assigned by the server, the machine can be either in internally busy or in free state, and the dynamics of these states follow a binary symmetric Markov chain. Upon sampling the state information of the machine, if the server identifies that the machine is in the free state, it schedules a user and submits a job to the machine from the job queue of the scheduled user. To maximize the number of jobs completed per unit time, we introduce a new metric, referred to as the age of job completion. To minimize the age of job completion and the sampling cost, we propose two policies and numerically evaluate their performance. For both of these policies, we find sufficient conditions under which the job queues will remain stable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04630v2</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stavros Mitrolaris, Subhankar Banerjee, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction</title>
      <link>https://arxiv.org/abs/2512.20885</link>
      <description>arXiv:2512.20885v2 Announce Type: replace-cross 
Abstract: Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20885v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sami Marouani, Kamal Singh, Baptiste Jeudy, Amaury Habrard</dc:creator>
    </item>
    <item>
      <title>Physics-Aware RIS Codebook Compilation for Near-Field Beam Focusing under Mutual Coupling and Specular Reflections</title>
      <link>https://arxiv.org/abs/2601.12982</link>
      <description>arXiv:2601.12982v2 Announce Type: replace-cross 
Abstract: Next-generation wireless networks are envisioned to achieve reliable, low-latency connectivity within environments characterized by strong multipath and severe channel variability. Programmable wireless environments (PWEs) address this challenge by enabling deterministic control of electromagnetic (EM) propagation through software-defined reconfigurable intelligent surfaces (RISs). However, effectively configuring RISs in real time remains a major bottleneck, particularly under near-field conditions where mutual coupling and specular reflections alter the intended response. To overcome this limitation, this paper introduces MATCH, a physics-based codebook compilation algorithm that explicitly accounts for the EM coupling among RIS unit cells and the reflective interactions with surrounding structures, ensuring that the resulting codebooks remain consistent with the physical characteristics of the environment. Finally, MATCH is evaluated under a full-wave simulation framework incorporating mutual coupling and secondary reflections, demonstrating its ability to concentrate scattered energy within the focal region, confirming that physics-consistent, codebook-based optimization constitutes an effective approach for practical and efficient RIS configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12982v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandros I. Papadopoulos, Maria Anna Pistela, Dimitrios Tyrovolas, Antonios Lalas, Konstantinos Votis, Sotiris Ioannidis, George K. Karagiannidis, Christos Liaskos</dc:creator>
    </item>
    <item>
      <title>AI-Driven Fuzzing for Vulnerability Assessment of 5G Traffic Steering Algorithms</title>
      <link>https://arxiv.org/abs/2601.18690</link>
      <description>arXiv:2601.18690v2 Announce Type: replace-cross 
Abstract: Traffic Steering (TS) dynamically allocates user traffic across cells to enhance Quality of Experience (QoE), load balance, and spectrum efficiency in 5G networks. However, TS algorithms remain vulnerable to adversarial conditions such as interference spikes, handover storms, and localized outages. To address this, an AI-driven fuzz testing framework based on the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) is proposed to systematically expose hidden vulnerabilities. Using NVIDIA Sionna, five TS algorithms are evaluated across six scenarios. Results show that AI-driven fuzzing detects 34.2% more total vulnerabilities and 5.8% more critical failures than traditional testing, achieving superior diversity and edge-case discovery. The observed variance in critical failure detection underscores the stochastic nature of rare vulnerabilities. These findings demonstrate that AI-driven fuzzing offers an effective and scalable validation approach for improving TS algorithm robustness and ensuring resilient 6G-ready networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18690v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyed Bagher Hashemi Natanzi, Hossein Mohammadi, Bo Tang, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>C-POD: An AWS Cloud Framework for Edge Pod Automation and Remote Wireless Testbed Sharing</title>
      <link>https://arxiv.org/abs/2602.11585</link>
      <description>arXiv:2602.11585v2 Announce Type: replace-cross 
Abstract: This paper presents C-POD, a cloud-native framework that automates the deployment and management of edge pods for seamless remote access and sharing of wireless testbeds. C-POD leverages public cloud resources and edge pods to lower the barrier to over-the-air (OTA) experimentation, enabling researchers to share and access distributed testbeds without extensive local infrastructure. A supporting toolkit has been developed for C-POD to enable flexible and scalable experimental workflows, including containerized edge environments, persistent Secure Shell (SSH) tunnels, and stable graphical interfaces. We prototype and deploy C-POD on the Amazon Web Services (AWS) public cloud to demonstrate its key features, including cloud-assisted edge pod deployment automation, elastic computing resource management, and experiment observability, by integrating two wireless testbeds that focus on RF signal generation and 5G(B) communications, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11585v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annoy Dey, Vineet Sreeram, Gokkul Eraivan Arutkani Aiyanathan, Maxwell McManus, Yuqing Cui, Guanying Sun, Elizabeth Serena Bentley, Nicholas Mastronarde, Zhangyu Guan</dc:creator>
    </item>
  </channel>
</rss>
