<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Apr 2025 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Length-Constrained Directed Expander Decomposition and Length-Constrained Vertex-Capacitated Flow Shortcuts</title>
      <link>https://arxiv.org/abs/2503.23217</link>
      <description>arXiv:2503.23217v1 Announce Type: new 
Abstract: We show the existence of length-constrained expander decomposition in directed graphs and undirected vertex-capacitated graphs. Previously, its existence was shown only in undirected edge-capacitated graphs [Haeupler-R\"acke-Ghaffari, STOC 2022; Haeupler-Hershkowitz-Tan, FOCS 2024]. Along the way, we prove the multi-commodity maxflow-mincut theorems for length-constrained expansion in both directed and undirected vertex-capacitated graphs. Based on our decomposition, we build a length-constrained flow shortcut for undirected vertex-capacitated graphs, which roughly speaking is a set of edges and vertices added to the graph so that every multi-commodity flow demand can be routed with approximately the same vertex-congestion and length, but all flow paths only contain few edges. This generalizes the shortcut for undirected edge-capacitated graphs from [Haeupler-Hershkowitz-Li-Roeyskoe-Saranurak, STOC 2024]. Length-constrained expander decomposition and flow shortcuts have been crucial in the recent algorithms in undirected edge-capacitated graphs [Haeupler-Hershkowitz-Li-Roeyskoe-Saranurak, STOC 2024; Haeupler-Long-Saranurak, FOCS 2024]. Our work thus serves as a foundation to generalize these concepts to directed and vertex-capacitated graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23217v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Yaowei Long, Thatchaphol Saranurak, Shengzhe Wang</dc:creator>
    </item>
    <item>
      <title>Improved algorithms for single machine serial-batch scheduling to minimize makespan and maximum cost</title>
      <link>https://arxiv.org/abs/2503.23273</link>
      <description>arXiv:2503.23273v1 Announce Type: new 
Abstract: This paper studies the bicriteria problem of scheduling $n$ jobs on a serial-batch machine to minimize makespan and maximum cost simultaneously. A serial-batch machine can process up to $b$ jobs as a batch, where $b$ is known as the batch capacity. When a new batch starts, a constant setup time is required for the machine. Within each batch, the jobs are processed sequentially, and thus the processing time of a batch equals the sum of the processing times of its jobs. All the jobs in a batch have the same completion time, namely, the completion time of the batch. The main result is an $O(n^3)$-time algorithm which can generate all Pareto optimal points for the bounded model ($b&lt;n$) without precedence relation. The algorithm can be modified to solve the unbounded model ($b\ge n$) with strict precedence relation in $O(n^3)$ time as well. The results improve the previously best known running time of $O(n^4)$ for both the bounded and unbounded models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23273v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuguang Li, Zhenxin Wen, Jing Wei</dc:creator>
    </item>
    <item>
      <title>Generalized Capacity Planning for the Hospital-Residents Problem</title>
      <link>https://arxiv.org/abs/2503.23328</link>
      <description>arXiv:2503.23328v1 Announce Type: new 
Abstract: The Hospital Residents setting models important problems like school choice, assignment of undergraduate students to degree programs, among many others. In this setting, fixed quotas are associated with the programs that limit the number of agents that can be assigned to them. Motivated by scenarios where all agents must be matched, we propose and study a generalized capacity planning problem, which allows cost-controlled flexibility with respect to quotas.
  Our setting is an extension of the Hospital Resident setting where programs have the usual quota as well as an associated cost, indicating the cost of matching an agent beyond the initial quotas. We seek to compute a matching that matches all agents and is optimal with respect to preferences, and minimizes either a local or a global objective on cost.
  We show that there is a sharp contrast -- minimizing the local objective is polynomial-time solvable, whereas minimizing the global objective is NP-hard. On the positive side, we present approximation algorithms for the global objective in the general case and a particular hard case. We achieve the approximation guarantee for the special hard case via a linear programming based algorithm. We strengthen the NP-hardness by showing a matching lower bound to our algorithmic result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23328v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haricharan Balasundaram, Girija Limaye, Meghana Nasre, Abhinav Raja</dc:creator>
    </item>
    <item>
      <title>Multi-Pass Streaming Lower Bounds for Approximating Max-Cut</title>
      <link>https://arxiv.org/abs/2503.23404</link>
      <description>arXiv:2503.23404v1 Announce Type: new 
Abstract: In the Max-Cut problem in the streaming model, an algorithm is given the edges of an unknown graph $G = (V,E)$ in some fixed order, and its goal is to approximate the size of the largest cut in $G$. Improving upon an earlier result of Kapralov, Khanna and Sudan, it was shown by Kapralov and Krachun that for all $\varepsilon&gt;0$, no $o(n)$ memory streaming algorithm can achieve a $(1/2+\varepsilon)$-approximation for Max-Cut. Their result holds for single-pass streams, i.e.~the setting in which the algorithm only views the stream once, and it was open whether multi-pass access may help. The state-of-the-art result along these lines, due to Assadi and N, rules out arbitrarily good approximation algorithms with constantly many passes and $n^{1-\delta}$ space for any $\delta&gt;0$.
  We improve upon this state-of-the-art result, showing that any non-trivial approximation algorithm for Max-Cut requires either polynomially many passes or polynomially large space. More specifically, we show that for all $\varepsilon&gt;0$, a $k$-pass streaming $(1/2+\varepsilon)$-approximation algorithm for Max-Cut requires $\Omega_{\varepsilon}\left(n^{1/3}/k\right)$ space. This result leads to a similar lower bound for the Maximum Directed Cut problem, showing the near optimality of the algorithm of [Saxena, Singer, Sudan, Velusamy, SODA 2025].
  Our lower bounds proceed by showing a communication complexity lower bound for the Distributional Implicit Hidden Partition (DIHP) Problem, introduced by Kapralov and Krachun. While a naive application of the discrepancy method fails, we identify a property of protocols called ``globalness'', and show that (1) any protocol for DIHP can be turned into a global protocol, (2) the discrepancy of a global protocol must be small. The second step is the more technically involved step in the argument, and therein we use global hypercontractive inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23404v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumou Fei, Dor Minzer, Shuo Wang</dc:creator>
    </item>
    <item>
      <title>Network Unreliability in Almost-Linear Time</title>
      <link>https://arxiv.org/abs/2503.23526</link>
      <description>arXiv:2503.23526v1 Announce Type: new 
Abstract: The network unreliability problem asks for the probability that a given undirected graph gets disconnected when every edge independently fails with a given probability $p$. Valiant (1979) showed that this problem is \#P-hard; therefore, the best we can hope for are approximation algorithms. In a classic result, Karger (1995) obtained the first FPTAS for this problem by leveraging the fact that when a graph disconnects, it almost always does so at a near-minimum cut, and there are only a small (polynomial) number of near-minimum cuts. Since then, a series of results have obtained progressively faster algorithms to the current bound of $m^{1+o(1)} + \tilde{O}(n^{3/2})$ (Cen, He, Li, and Panigrahi, 2024). In this paper, we obtain an $m^{1+o(1)}$-time algorithm for the network unreliability problem. This is essentially optimal, since we need $O(m)$ time to read the input graph. Our main new ingredient is relating network unreliability to an {\em ideal} tree packing of spanning trees (Thorup, 2001).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23526v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruoxu Cen, Jason Li, Debmalya Panigrahi</dc:creator>
    </item>
    <item>
      <title>Space of Data through the Lens of Multilevel Graph</title>
      <link>https://arxiv.org/abs/2503.23602</link>
      <description>arXiv:2503.23602v1 Announce Type: new 
Abstract: This work seeks to tackle the inherent complexity of dataspaces by introducing a novel data structure that can represent datasets across multiple levels of abstraction, ranging from local to global. We propose the concept of a multilevel graph, which is equipped with two fundamental operations: contraction and expansion of its topology. This multilevel graph is specifically designed to fulfil the requirements for incremental abstraction and flexibility, as outlined in existing definitions of dataspaces. Furthermore, we provide a comprehensive suite of methods for manipulating this graph structure, establishing a robust framework for data analysis. While its effectiveness has been empirically validated for unstructured data, its application to structured data is also inherently viable. Preliminary results are presented through a real-world scenario based on a collection of dream reports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23602v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Caputo, Michele Russo, Emanuela Merelli</dc:creator>
    </item>
    <item>
      <title>Word Break on SLP-Compressed Texts</title>
      <link>https://arxiv.org/abs/2503.23759</link>
      <description>arXiv:2503.23759v1 Announce Type: new 
Abstract: Word Break is a prototypical factorization problem in string processing: Given a word $w$ of length $N$ and a dictionary $\mathcal{D} = \{d_1, d_2, \ldots, d_{K}\}$ of $K$ strings, determine whether we can partition $w$ into words from $\mathcal{D}$. We propose the first algorithm that solves the Word Break problem over the SLP-compressed input text $w$. Specifically, we show that, given the string $w$ represented using an SLP of size $g$, we can solve the Word Break problem in $\mathcal{O}(g \cdot m^{\omega} + M)$ time, where $m = \max_{i=1}^{K} |d_i|$, $M = \sum_{i=1}^{K} |d_i|$, and $\omega \geq 2$ is the matrix multiplication exponent. We obtain our algorithm as a simple corollary of a more general result: We show that in $\mathcal{O}(g \cdot m^{\omega} + M)$ time, we can index the input text $w$ so that solving the Word Break problem for any of its substrings takes $\mathcal{O}(m^2 \log N)$ time (independent of the substring length). Our second contribution is a lower bound: We prove that, unless the Combinatorial $k$-Clique Conjecture fails, there is no combinatorial algorithm for Word Break on SLP-compressed strings running in $\mathcal{O}(g \cdot m^{2-\epsilon} + M)$ time for any $\epsilon &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23759v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajat De, Dominik Kempa</dc:creator>
    </item>
    <item>
      <title>Sample-Optimal Private Regression in Polynomial Time</title>
      <link>https://arxiv.org/abs/2503.24321</link>
      <description>arXiv:2503.24321v1 Announce Type: new 
Abstract: We consider the task of privately obtaining prediction error guarantees in ordinary least-squares regression problems with Gaussian covariates (with unknown covariance structure). We provide the first sample-optimal polynomial time algorithm for this task under both pure and approximate differential privacy. We show that any improvement to the sample complexity of our algorithm would violate either statistical-query or information-theoretic lower bounds. Additionally, our algorithm is robust to a small fraction of arbitrary outliers and achieves optimal error rates as a function of the fraction of outliers. In contrast, all prior efficient algorithms either incurred sample complexities with sub-optimal dimension dependence, scaling with the condition number of the covariates, or obtained a polynomially worse dependence on the privacy parameters.
  Our technical contributions are two-fold: first, we leverage resilience guarantees of Gaussians within the sum-of-squares framework. As a consequence, we obtain efficient sum-of-squares algorithms for regression with optimal robustness rates and sample complexity. Second, we generalize the recent robustness-to-privacy framework [HKMN23, (arXiv:2212.05015)] to account for the geometry induced by the covariance of the input samples. This framework crucially relies on the robust estimators to be sum-of-squares algorithms, and combining the two steps yields a sample-optimal private regression algorithm. We believe our techniques are of independent interest, and we demonstrate this by obtaining an efficient algorithm for covariance-aware mean estimation, with an optimal dependence on the privacy parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24321v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prashanti Anderson, Ainesh Bakshi, Mahbod Majid, Stefan Tiegel</dc:creator>
    </item>
    <item>
      <title>Accelerated Approximate Optimization of Multi-Commodity Flows on Directed Graphs</title>
      <link>https://arxiv.org/abs/2503.24373</link>
      <description>arXiv:2503.24373v1 Announce Type: new 
Abstract: We provide $m^{1+o(1)}k\epsilon^{-1}$-time algorithms for computing multiplicative $(1 - \epsilon)$-approximate solutions to multi-commodity flow problems with $k$-commodities on $m$-edge directed graphs, including concurrent multi-commodity flow and maximum multi-commodity flow.
  To obtain our results, we provide new optimization tools of potential independent interest. First, we provide an improved optimization method for solving $\ell_{q, p}$-regression problems to high accuracy. This method makes $\tilde{O}_{q, p}(k)$ queries to a high accuracy convex minimization oracle for an individual block, where $\tilde{O}_{q, p}(\cdot)$ hides factors depending only on $q$, $p$, or $\mathrm{poly}(\log m)$, improving upon the $\tilde{O}_{q, p}(k^2)$ bound of [Chen-Ye, ICALP 2024]. As a result, we obtain the first almost-linear time algorithm that solves $\ell_{q, p}$ flows on directed graphs to high accuracy. Second, we present optimization tools to reduce approximately solving composite $\ell_{1, \infty}$-regression problems to solving $m^{o(1)}\epsilon^{-1}$ instances of composite $\ell_{q, p}$-regression problem. The method builds upon recent advances in solving box-simplex games [Jambulapati-Tian, NeurIPS 2023] and the area convex regularizer introduced in [Sherman, STOC 2017] to obtain faster rates for constrained versions of the problem. Carefully combining these techniques yields our directed multi-commodity flow algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24373v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Chen, Andrei Graur, Aaron Sidford</dc:creator>
    </item>
    <item>
      <title>A quantum algorithm for solving 0-1 Knapsack problems</title>
      <link>https://arxiv.org/abs/2310.06623</link>
      <description>arXiv:2310.06623v2 Announce Type: cross 
Abstract: Here we present two novel contributions for achieving quantum advantage in solving difficult optimisation problems, both in theory and foreseeable practice. (1) We introduce the "Quantum Tree Generator", an approach to generate in superposition all feasible solutions of a given instance, yielding together with amplitude amplification the optimal solutions for 0-1 knapsack problems. The QTG offers massive memory savings and enables competitive runtimes compared to the classical state-of-the-art knapsack solvers (such as COMBO, Gurobi, CP-SAT, Greedy) already for instances involving as few as 100 variables. (2) By introducing a new runtime calculation technique that exploits logging data from the classical solver COMBO, we can predict the runtime of our method way beyond the range of existing quantum platforms and simulators, for various benchmark instances with up to 600 variables. Combining both of these innovations, we demonstrate the QTG's potential practical quantum advantage for large-scale problems, indicating an effective approach for combinatorial optimisation problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06623v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\"oren Wilkening, Andreea-Iulia Lefterovici, Lennart Binkowski, Michael Perk, S\'andor Fekete, Tobias J. Osborne</dc:creator>
    </item>
    <item>
      <title>Wagner's Algorithm Provably Runs in Subexponential Time for SIS$^\infty$</title>
      <link>https://arxiv.org/abs/2503.23238</link>
      <description>arXiv:2503.23238v1 Announce Type: cross 
Abstract: At CRYPTO 2015, Kirchner and Fouque claimed that a carefully tuned variant of the Blum-Kalai-Wasserman (BKW) algorithm (JACM 2003) should solve the Learning with Errors problem (LWE) in slightly subexponential time for modulus $q=\mathrm{poly}(n)$ and narrow error distribution, when given enough LWE samples. Taking a modular view, one may regard BKW as a combination of Wagner's algorithm (CRYPTO 2002), run over the corresponding dual problem, and the Aharonov-Regev distinguisher (JACM 2005). Hence the subexponential Wagner step alone should be of interest for solving this dual problem - namely, the Short Integer Solution problem (SIS) - but this appears to be undocumented so far.
  We re-interpret this Wagner step as walking backward through a chain of projected lattices, zigzagging through some auxiliary superlattices. We further randomize the bucketing step using Gaussian randomized rounding to exploit the powerful discrete Gaussian machinery. This approach avoids sample amplification and turns Wagner's algorithm into an approximate discrete Gaussian sampler for $q$-ary lattices.
  For an SIS lattice with $n$ equations modulo $q$, this algorithm runs in subexponential time $\exp(O(n/\log \log n))$ to reach a Gaussian width parameter $s = q/\mathrm{polylog}(n)$ only requiring $m = n + \omega(n/\log \log n)$ many SIS variables. This directly provides a provable algorithm for solving the Short Integer Solution problem in the infinity norm ($\mathrm{SIS}^\infty$) for norm bounds $\beta = q/\mathrm{polylog}(n)$. This variant of SIS underlies the security of the NIST post-quantum cryptography standard Dilithium. Despite its subexponential complexity, Wagner's algorithm does not appear to threaten Dilithium's concrete security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23238v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'eo Ducas, Lynn Engelberts, Johanna Loyer</dc:creator>
    </item>
    <item>
      <title>On Speedups for Convex Optimization via Quantum Dynamics</title>
      <link>https://arxiv.org/abs/2503.24332</link>
      <description>arXiv:2503.24332v1 Announce Type: cross 
Abstract: We explore the potential for quantum speedups in convex optimization using discrete simulations of the Quantum Hamiltonian Descent (QHD) framework, as proposed by Leng et al., and establish the first rigorous query complexity bounds. We develop enhanced analyses for quantum simulation of Schr\"odinger operators with black-box potential via the pseudo-spectral method, providing explicit resource estimates independent of wavefunction assumptions. These bounds are applied to assess the complexity of optimization through QHD. Our findings pertain to unconstrained convex optimization in $d$ dimensions. In continuous time, we demonstrate that QHD, with suitable parameters, can achieve arbitrarily fast convergence rates. The optimization speed limit arises solely from the discretization of the dynamics, mirroring a property of the classical dynamics underlying QHD. Considering this cost, we show that a $G$-Lipschitz convex function can be optimized to an error of $\epsilon$ with $\widetilde{\mathcal{O}}(d^{1.5}G^2 R^2/\epsilon^2)$ queries. Moreover, under reasonable assumptions on the complexity of Hamiltonian simulation, $\widetilde{\Omega}(d/\epsilon^2)$ queries are necessary. Thus, QHD does not offer a speedup over classical zeroth order methods with exact oracles. However, we demonstrate that the QHD algorithm tolerates $\widetilde{\mathcal{O}}(\epsilon^3/d^{1.5}G^2 R^2)$ noise in function evaluation. We show that QHD offers a super-quadratic query advantage over all known classical algorithms tolerating this level of evaluation noise in the high-dimension regime. Additionally, we design a quantum algorithm for stochastic convex optimization that provides a super-quadratic speedup over all known classical algorithms in the high-dimension regime. To our knowledge, these results represent the first rigorous quantum speedups for convex optimization achieved through a dynamical algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24332v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shouvanik Chakrabarti, Dylan Herman, Jacob Watkins, Enrico Fontana, Brandon Augustino, Junhyung Lyle Kim, Marco Pistoia</dc:creator>
    </item>
    <item>
      <title>Testing Support Size More Efficiently Than Learning Histograms</title>
      <link>https://arxiv.org/abs/2410.18915</link>
      <description>arXiv:2410.18915v2 Announce Type: replace 
Abstract: Consider two problems about an unknown probability distribution $p$:
  1. How many samples from $p$ are required to test if $p$ is supported on $n$ elements or not? Specifically, given samples from $p$, determine whether it is supported on at most $n$ elements, or it is "$\epsilon$-far" (in total variation distance) from being supported on $n$ elements.
  2. Given $m$ samples from $p$, what is the largest lower bound on its support size that we can produce?
  The best known upper bound for problem (1) uses a general algorithm for learning the histogram of the distribution $p$, which requires $\Theta(\tfrac{n}{\epsilon^2 \log n})$ samples. We show that testing can be done more efficiently than learning the histogram, using only $O(\tfrac{n}{\epsilon \log n} \log(1/\epsilon))$ samples, nearly matching the best known lower bound of $\Omega(\tfrac{n}{\epsilon \log n})$. This algorithm also provides a better solution to problem (2), producing larger lower bounds on support size than what follows from previous work. The proof relies on an analysis of Chebyshev polynomial approximations outside the range where they are designed to be good approximations, and the paper is intended as an accessible self-contained exposition of the Chebyshev polynomial method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18915v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Ferreira Pinto Jr., Nathaniel Harms</dc:creator>
    </item>
    <item>
      <title>A Faster Algorithm for Maximum Weight Matching on Unrestricted Bipartite Graphs</title>
      <link>https://arxiv.org/abs/2502.20889</link>
      <description>arXiv:2502.20889v2 Announce Type: replace 
Abstract: Given a weighted bipartite graph $G = (L, R, E, w)$, the maximum weight matching (MWM) problem seeks to find a matching $M \subseteq E$ that maximizes the total weight $\sum_{e \in M} w(e)$.
  This paper presents a novel algorithm with a time complexity of $O(\min(X^3 + E, XE + X^2\log X))$, where $X = \min(|L|, |R|)$. Unlike many existing algorithms, our approach supports real-valued weights without additional constraints. Under this condition, our result improves upon the previous best-known bound of $O(VE + V^2\log V)$, or more specifically $O(XE + XV\log V)$, where $V = L \cup R$.
  The simplified suggested implementation is publicly available at \url{https://github.com/ShawxingKwok/Kwok-algorithm}, with the average-case time complexity of $\tilde{O}(E + LR)$ roughly evaluated from theory and experimental results on random graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20889v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shawxing Kwok</dc:creator>
    </item>
    <item>
      <title>Optimal mass estimation in the conditional sampling model</title>
      <link>https://arxiv.org/abs/2503.12518</link>
      <description>arXiv:2503.12518v3 Announce Type: replace 
Abstract: The conditional sampling model, introduced by Cannone, Ron and Servedio (SODA 2014, SIAM J. Comput. 2015) and independently by Chakraborty, Fischer, Goldhirsh and Matsliah (ITCS 2013, SIAM J. Comput. 2016), is a common framework for a number of studies concerning strengthened models of distribution testing. A core task in these investigations is that of estimating the mass of individual elements. The above mentioned works, and the improvement of Kumar, Meel and Pote (AISTATS 2025), provided polylogarithmic algorithms for this task.
  In this work we shatter the polylogarithmic barrier, and provide an estimator for the mass of individual elements that uses only $O(\log \log N) + O(\mathrm{poly}(1/\varepsilon))$ conditional samples. We complement this result with an $\Omega(\log\log N)$ lower bound.
  We then show that our mass estimator provides an improvement (and in some cases a unifying framework) for a number of related tasks, such as testing by learning of any label-invariant property, and distance estimation between two (unknown) distribution. By considering some known lower bounds, this also shows that the full power of the conditional model is indeed required for the doubly-logarithmic upper bound.
  Finally, we exponentially improve the previous lower bound on testing by learning of label-invariant properties from double-logarithmic to $\Omega(\log N)$ conditional samples, whereas our testing by learning algorithm provides an upper bound of $O(\mathrm{poly}(1/\varepsilon)\cdot\log N \log \log N)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12518v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomer Adar, Eldar Fischer, Amit Levi</dc:creator>
    </item>
    <item>
      <title>An Efficient Frequency-Based Approach for Maximal Square Detection in Binary Matrices</title>
      <link>https://arxiv.org/abs/2503.18974</link>
      <description>arXiv:2503.18974v2 Announce Type: replace 
Abstract: This paper presents a novel frequency-based algorithm which solves the maximal square problem with improved practical speed performance while maintaining optimal asymptotic complexity. My approach tracks the columnar continuity of ones through an adaptive frequency vector and dynamic thresholding mechanism that eliminates the need for nested minimum operations commonly found in standard dynamic programming solutions. Theoretical analysis confirms a time complexity of O(mn) and a space complexity of O(n).Formal loop-invariant proofs verify correctness, while comprehensive benchmarking demonstrates speed improvements of 1.3-5x over standard methods in various matrix densities and sizes. This method improves algorithm design and simultaneously creates opportunities for faster spatial pattern recognition in fields like urban planning, environmental science, and medical imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18974v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swastik Bhandari</dc:creator>
    </item>
    <item>
      <title>Solving the Correlation Cluster LP in Sublinear Time</title>
      <link>https://arxiv.org/abs/2503.20883</link>
      <description>arXiv:2503.20883v2 Announce Type: replace 
Abstract: Correlation Clustering is a fundamental and widely-studied problem in unsupervised learning and data mining. The input is a graph and the goal is to construct a clustering minimizing the number of inter-cluster edges plus the number of missing intra-cluster edges.
  CCL+24 introduced the cluster LP for Correlation Clustering, which they argued captures the problem much more succinctly than previous linear programming formulations. However, the cluster LP has exponential size, with a variable for every possible set of vertices in the input graph. Nevertheless, CCL+24 showed how to find a feasible solution for the cluster LP in time $O(n^{\text{poly}(1/\eps)})$ with objective value at most $(1+\epsilon)$ times the value of an optimal solution for the respective Correlation Clustering instance. Furthermore, they showed how to round a solution to the cluster LP, yielding a $(1.437+\eps)$-approximation algorithm for the Correlation Clustering problem.
  The main technical result of this paper is a new approach to find a feasible solution for the cluster LP with objective value at most $(1+\epsilon)$ of the optimum in time $\widetilde O(2^{\text{poly}(1/\eps)} n)$, where $n$ is the number of vertices in the graph. We also show how to implement the rounding within the same time bounds, thus achieving a fast $(1.437+\eps)$-approximation algorithm for the Correlation Clustering problem. This bridges the gap between state-of-the-art methods for approximating Correlation Clustering and the recent focus on fast algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20883v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3717823.3718181</arxiv:DOI>
      <dc:creator>Nairen Cao, Vincent Cohen-Addad, Shi Li, Euiwoong Lee, David Rasmussen Lolck, Alantha Newman, Mikkel Thorup, Lukas Vogl, Shuyi Yan, Hanwen Zhang</dc:creator>
    </item>
    <item>
      <title>Fast Approximation Algorithms for Euclidean Minimum Weight Perfect Matching</title>
      <link>https://arxiv.org/abs/2407.07749</link>
      <description>arXiv:2407.07749v2 Announce Type: replace-cross 
Abstract: We study the problem of finding a Euclidean minimum weight perfect matching for $n$ points in the plane. It is known that a deterministic approximation algorithm for this problems must have at least $\Omega(n \log n)$ runtime. We propose such an algorithm for the Euclidean minimum weight perfect matching problem with runtime $O(n\log n)$ and show that it has approximation ratio $O(n^{0.206})$. This improves the so far best known approximation ratio of $n/2$. We also develop an $O(n \log n)$ algorithm for the Euclidean minimum weight perfect matching problem in higher dimensions and show it has approximation ratio $O(n^{0.412})$ in all fixed dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07749v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefan Hougardy, Karolina Tammemaa</dc:creator>
    </item>
    <item>
      <title>Asymptotically Optimal Inapproximability of Maxmin $k$-Cut Reconfiguration</title>
      <link>https://arxiv.org/abs/2410.03416</link>
      <description>arXiv:2410.03416v2 Announce Type: replace-cross 
Abstract: $k$-Coloring Reconfiguration is one of the most well-studied reconfiguration problems, which asks to transform a given proper $k$-coloring of a graph to another by repeatedly recoloring a single vertex. Its approximate version, Maxmin $k$-Cut Reconfiguration, is defined as an optimization problem of maximizing the minimum fraction of bichromatic edges during the transformation between (not necessarily proper) $k$-colorings. In this paper, we prove that the optimal approximation factor of this problem is $1 - \Theta\left(\frac{1}{k}\right)$ for every $k \ge 2$. Specifically, we show the $\mathsf{PSPACE}$-hardness of approximating the objective value within a factor of $1 - \frac{\varepsilon}{k}$ for some universal constant $\varepsilon &gt; 0$, whereas we present a deterministic polynomial-time algorithm that achieves the approximation factor of $1 - \frac{2}{k}$.
  To prove the hardness result, we develop a new probabilistic verifier that tests a ``striped'' pattern. Our polynomial-time algorithm is based on ``a random reconfiguration via a random solution,'' i.e., the transformation that goes through one random $k$-coloring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03416v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuichi Hirahara, Naoto Ohsaka</dc:creator>
    </item>
    <item>
      <title>Tight Bounds on the Number of Closest Pairs in Vertical Slabs</title>
      <link>https://arxiv.org/abs/2502.17600</link>
      <description>arXiv:2502.17600v2 Announce Type: replace-cross 
Abstract: Let $S$ be a set of $n$ points in $\mathbb{R}^d$, where $d \geq 2$ is a constant, and let $H_1,H_2,\ldots,H_{m+1}$ be a sequence of vertical hyperplanes that are sorted by their first coordinates, such that exactly $n/m$ points of $S$ are between any two successive hyperplanes. Let $|A(S,m)|$ be the number of different closest pairs in the ${{m+1} \choose 2}$ vertical slabs that are bounded by $H_i$ and $H_j$, over all $1 \leq i &lt; j \leq m+1$. We prove tight bounds for the largest possible value of $|A(S,m)|$, over all point sets of size $n$, and for all values of $1 \leq m \leq n$.
  As a result of these bounds, we obtain, for any constant $\epsilon&gt;0$, a data structure of size $O(n)$, such that for any vertical query slab $Q$, the closest pair in the set $Q \cap S$ can be reported in $O(n^{1/2+\epsilon})$ time. Prior to this work, no linear space data structure with sublinear query time was known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17600v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad Biniaz, Prosenjit Bose, Chaeyoon Chung, Jean-Lou De Carufel, John Iacono, Anil Maheshwari, Saeed Odak, Michiel Smid, Csaba D. T\'oth</dc:creator>
    </item>
    <item>
      <title>Fast Quantum Amplitude Encoding of Typical Classical Data</title>
      <link>https://arxiv.org/abs/2503.17113</link>
      <description>arXiv:2503.17113v2 Announce Type: replace-cross 
Abstract: We present an improved version of a quantum amplitude encoding scheme that encodes the $N$ entries of a unit classical vector $\vec{v}=(v_1,..,v_N)$ into the amplitudes of a quantum state. Our approach has a quadratic speed-up with respect to the original one. We also describe several generalizations, including to complex entries of the input vector and a parameter $M$ that determines the parallelization. The number of qubits required for the state preparation scales as $\mathcal{O}(M\log N)$. The runtime, which depends on the data density $\rho$ and on the parallelization paramater $M$, scales as $\mathcal{O}(\frac{1}{\sqrt{\rho}}\frac{N}{M}\log (M+1))$, which in the most parallel version ($M=N$) is always less than $\mathcal{O}(\sqrt{N}\log N)$. By analysing the data density, we prove that the average runtime is $\mathcal{O}(\log^{1.5} N)$ for uniformly random inputs. We present numerical evidence that this favourable runtime behaviour also holds for real-world data, such as radar satellite images. This is promising as it allows for an input-to-output advantage of the quantum Fourier transform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17113v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vittorio Pagni, Sigurd Huber, Michael Epping, Michael Felderer</dc:creator>
    </item>
    <item>
      <title>Graph neural networks extrapolate out-of-distribution for shortest paths</title>
      <link>https://arxiv.org/abs/2503.19173</link>
      <description>arXiv:2503.19173v2 Announce Type: replace-cross 
Abstract: Neural networks (NNs), despite their success and wide adoption, still struggle to extrapolate out-of-distribution (OOD), i.e., to inputs that are not well-represented by their training dataset. Addressing the OOD generalization gap is crucial when models are deployed in environments significantly different from the training set, such as applying Graph Neural Networks (GNNs) trained on small graphs to large, real-world graphs. One promising approach for achieving robust OOD generalization is the framework of neural algorithmic alignment, which incorporates ideas from classical algorithms by designing neural architectures that resemble specific algorithmic paradigms (e.g. dynamic programming). The hope is that trained models of this form would have superior OOD capabilities, in much the same way that classical algorithms work for all instances. We rigorously analyze the role of algorithmic alignment in achieving OOD generalization, focusing on graph neural networks (GNNs) applied to the canonical shortest path problem. We prove that GNNs, trained to minimize a sparsity-regularized loss over a small set of shortest path instances, exactly implement the Bellman-Ford (BF) algorithm for shortest paths. In fact, if a GNN minimizes this loss within an error of $\epsilon$, it implements the BF algorithm with an error of $O(\epsilon)$. Consequently, despite limited training data, these GNNs are guaranteed to extrapolate to arbitrary shortest-path problems, including instances of any size. Our empirical results support our theory by showing that NNs trained by gradient descent are able to minimize this loss and extrapolate in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19173v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert R. Nerem, Samantha Chen, Sanjoy Dasgupta, Yusu Wang</dc:creator>
    </item>
  </channel>
</rss>
