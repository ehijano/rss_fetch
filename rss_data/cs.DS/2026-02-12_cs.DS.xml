<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 06:02:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Complexity of Bayesian Network Learning: Revisiting the Superstructure</title>
      <link>https://arxiv.org/abs/2602.10253</link>
      <description>arXiv:2602.10253v1 Announce Type: new 
Abstract: We investigate the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. We follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. While known results imply that BNSL is unlikely to be fixed-parameter tractable even when parameterized by the size of a vertex cover in the superstructure, here we show that a different kind of parameterization - notably by the size of a feedback edge set - yields fixed-parameter tractability. We proceed by showing that this result can be strengthened to a localized version of the feedback edge set, and provide corresponding lower bounds that complement previous results to provide a complexity classification of BNSL w.r.t. virtually all well-studied graph parameters.
  We then analyze how the complexity of BNSL depends on the representation of the input. In particular, while the bulk of past theoretical work on the topic assumed the use of the so-called non-zero representation, here we prove that if an additive representation can be used instead then BNSL becomes fixed-parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone. Last but not least, we show how our results can be extended to the closely related problem of Polytree Learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10253v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Ganian, Viktoriia Korchemna</dc:creator>
    </item>
    <item>
      <title>Online Bisection with Ring Demands</title>
      <link>https://arxiv.org/abs/2602.10337</link>
      <description>arXiv:2602.10337v1 Announce Type: new 
Abstract: The online bisection problem requires maintaining a dynamic partition of $n$ nodes into two equal-sized clusters. Requests arrive sequentially as node pairs. If the nodes lie in different clusters, the algorithm pays unit cost. After each request, the algorithm may migrate nodes between clusters at unit cost per node. This problem models datacenter resource allocation where virtual machines must be assigned to servers, balancing communication costs against migration overhead.
  We study the variant where requests are restricted to edges of a ring network, an abstraction of ring-allreduce patterns in distributed machine learning. Despite this restriction, the problem remains challenging with an $\Omega(n)$ deterministic lower bound. We present a randomized algorithm achieving $O(\varepsilon^{-3} \cdot \log^2 n)$ competitive ratio using resource augmentation that allows clusters of size at most $(3/4 + \varepsilon) \cdot n$.
  Our approach formulates the problem as a metrical task system with a restricted state space. By limiting the number of cut-edges (i.e., ring edges between clusters) to at most $2k$, where $k = \Theta(1/\varepsilon)$, we reduce the state space from exponential to polynomial (i.e., $n^{O(k)}$). The key technical contribution is proving that this restriction increases cost by only a factor of $O(k)$. Our algorithm follows by applying the randomized MTS solution of Bubeck et al. [SODA 2019].
  The best result to date for bisection with ring demands is the $O(n \cdot \log n)$-competitive deterministic online algorithm of Rajaraman and Wasim [ESA 2024] for the general setting. While prior work for ring-demands by R\"acke et al. [SPAA 2023] achieved $O(\log^3 n)$ for multiple clusters, their approach employs a resource augmentation factor of $2+\varepsilon$, making it inapplicable to bisection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10337v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Basiak, Marcin Bienkowski, Guy Even, Agnieszka Tatarczuk</dc:creator>
    </item>
    <item>
      <title>Skirting Additive Error Barriers for Private Turnstile Streams</title>
      <link>https://arxiv.org/abs/2602.10360</link>
      <description>arXiv:2602.10360v1 Announce Type: new 
Abstract: We study differentially private continual release of the number of distinct items in a turnstile stream, where items may be both inserted and deleted. A recent work of Jain, Kalemaj, Raskhodnikova, Sivakumar, and Smith (NeurIPS '23) shows that for streams of length $T$, polynomial additive error of $\Omega(T^{1/4})$ is necessary, even without any space restrictions. We show that this additive error lower bound can be circumvented if the algorithm is allowed to output estimates with both additive \emph{and multiplicative} error. We give an algorithm for the continual release of the number of distinct elements with $\text{polylog} (T)$ multiplicative and $\text{polylog}(T)$ additive error. We also show a qualitatively similar phenomenon for estimating the $F_2$ moment of a turnstile stream, where we can obtain $1+o(1)$ multiplicative and $\text{polylog} (T)$ additive error. Both results can be achieved using polylogarithmic space whereas prior approaches use polynomial space. In the sublinear space regime, some multiplicative error is necessary even if privacy is not a consideration. We raise several open questions aimed at better understanding trade-offs between multiplicative and additive error in private continual release.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10360v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Aamand, Justin Y. Chen, Sandeep Silwal</dc:creator>
    </item>
    <item>
      <title>New Algorithms and Hardness Results for Robust Satisfiability of (Promise) CSPs</title>
      <link>https://arxiv.org/abs/2602.10368</link>
      <description>arXiv:2602.10368v1 Announce Type: new 
Abstract: In this paper, we continue the study of robust satisfiability of promise CSPs (PCSPs), initiated in (Brakensiek, Guruswami, Sandeep, STOC 2023 / Discrete Analysis 2025), and obtain the following results:
  For the PCSP 1-in-3-SAT vs NAE-SAT with negations, we prove that it is hard, under the Unique Games conjecture (UGC), to satisfy $1-\Omega(1/\log (1/\epsilon))$ constraints in a $(1-\epsilon)$-satisfiable instance. This shows that the exponential loss incurred by the BGS algorithm for the case of Alternating-Threshold polymorphisms is necessary, in contrast to the polynomial loss achievable for Majority polymorphisms.
  For any Boolean PCSP that admits Majority polymorphisms, we give an algorithm satisfying $1-O(\sqrt{\epsilon})$ fraction of the weaker constraints when promised the existence of an assignment satisfying $1-\epsilon$ fraction of the stronger constraints. This significantly generalizes the Charikar--Makarychev--Makarychev algorithm for 2-SAT, and matches the optimal trade-off possible under the UGC. The algorithm also extends, with the loss of an extra $\log (1/\epsilon)$ factor, to PCSPs on larger domains with a certain structural condition, which is implied by, e.g., a family of Plurality polymorphisms.
  We prove that assuming the UGC, robust satisfiability is preserved under the addition of equality constraints. As a consequence, we can extend the rich algebraic techniques for decision/search PCSPs to robust PCSPs. The methods involve the development of a correlated and robust version of the general SDP rounding algorithm for CSPs due to (Brown-Cohen, Raghavendra, ICALP 2016), which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10368v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Joshua Brakensiek, Lorenzo Ciardo, Venkatesan Guruswami, Aaron Potechin, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>Better Diameter Bounds for Efficient Shortcuts and a Structural Criterion for Constructiveness</title>
      <link>https://arxiv.org/abs/2602.10747</link>
      <description>arXiv:2602.10747v1 Announce Type: new 
Abstract: All parallel algorithms for directed connectivity and shortest paths crucially rely on efficient shortcut constructions that add a linear number of transitive closure edges to a given DAG to reduce its diameter. A long sequence of works has studied both (efficient) shortcut constructions and impossibility results on the best diameter and therefore the best parallelism that can be achieved with this approach.
  This paper introduces a new conceptual and technical tool, called certified shortcuts, for this line of research in the form of a simple and natural structural criterion that holds for any shortcut constructed by an efficient (combinatorial) algorithm. It allows us to drastically simplify and strengthen existing impossibility results by proving that any near-linear-time shortcut-based algorithm cannot reduce a graph's diameter below $n^{1/4-o(1)}$. This greatly improves over the $n^{2/9-o(1)}$ lower bound of [HXX25] and seems to be the best bound one can hope for with current techniques.
  Our structural criterion also precisely captures the constructiveness of all known shortcut constructions: we show that existing constructions satisfy the criterion if and only if they have known efficient algorithms. We believe our new criterion and perspective of looking for certified shortcuts can provide crucial guidance for designing efficient shortcut constructions in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10747v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Antti Roeyskoe, Zhijun Zhang</dc:creator>
    </item>
    <item>
      <title>Data Reductions for the Strong Maximum Independent Set Problem in Hypergraphs</title>
      <link>https://arxiv.org/abs/2602.10781</link>
      <description>arXiv:2602.10781v1 Announce Type: new 
Abstract: This work addresses the well-known Maximum Independent Set problem in the context of hypergraphs. While this problem has been extensively studied on graphs, we focus on its strong extension to hypergraphs, where edges may connect any number of vertices. A set of vertices in a hypergraph is strongly independent if there is at most one vertex per edge in the set. One application for this problem is to find perfect minimal hash functions. We propose nine new data reduction rules specifically designed for this problem. Our reduction routine can serve as a preprocessing step for any solver. We analyze the impact on the size of the reduced instances and the performance of several subsequent solvers when combined with this preprocessing. Our results demonstrate a significant reduction in instance size and improvements in running time for subsequent solvers. The preprocessing routine reduces instances, on average, to 22% of their original size in 6.76 seconds. When combining our reduction preprocessing with the best-performing exact solver, we observe an average speedup of 3.84x over not using the reduction rules. In some cases, we can achieve speedups of up to 53x. Additionally, one more instance becomes solvable by a method when combined with our preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10781v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ernestine Gro{\ss}mann, Christian Schulz, Darren Strash, Antonie Wagner</dc:creator>
    </item>
    <item>
      <title>Personalized PageRank Estimation in Undirected Graphs</title>
      <link>https://arxiv.org/abs/2602.10843</link>
      <description>arXiv:2602.10843v1 Announce Type: new 
Abstract: Given an undirected graph $G=(V, E)$, the Personalized PageRank (PPR) of $t\in V$ with respect to $s\in V$, denoted $\pi(s,t)$, is the probability that an $\alpha$-discounted random walk starting at $s$ terminates at $t$. We study the time complexity of estimating $\pi(s,t)$ with constant relative error and constant failure probability, whenever $\pi(s,t)$ is above a given threshold parameter $\delta\in(0,1)$. We consider common graph-access models and furthermore study the single source, single target, and single node (PageRank centrality) variants of the problem.
  We provide a complete characterization of PPR estimation in undirected graphs by giving tight bounds (up to logarithmic factors) for all problems and model variants in both the worst-case and average-case setting. This includes both new upper and lower bounds.
  Tight bounds were recently obtained by Bertram, Jensen, Thorup, Wang, and Yan for directed graphs. However, their lower bound constructions rely on asymmetry and therefore do not carry over to undirected graphs. At the same time, undirected graphs exhibit additional structure that can be exploited algorithmically. Our results resolve the undirected case by developing new techniques that capture both aspects, yielding tight bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10843v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Bertram, Mads Vestergaard Jensen</dc:creator>
    </item>
    <item>
      <title>Random Access in Grammar-Compressed Strings: Optimal Trade-Offs in Almost All Parameter Regimes</title>
      <link>https://arxiv.org/abs/2602.10864</link>
      <description>arXiv:2602.10864v1 Announce Type: new 
Abstract: A Random Access query to a string $T\in [0..\sigma)^n$ asks for the character $T[i]$ at a given position $i\in [0..n)$. In $O(n\log\sigma)$ bits of space, this fundamental task admits constant-time queries. While this is optimal in the worst case, much research has focused on compressible strings, hoping for smaller data structures that still admit efficient queries.
  We investigate the grammar-compressed setting, where $T$ is represented by a straight-line grammar. Our main result is a general trade-off that optimizes Random Access time as a function of string length $n$, grammar size (the total length of productions) $g$, alphabet size $\sigma$, data structure size $M$, and word size $w=\Omega(\log n)$ of the word RAM model. For any $M$ with $g\log n&lt;Mw&lt;n\log\sigma$, we show an $O(M)$-size data structure with query time $O(\frac{\log(n\log\sigma\,/\,Mw)}{\log(Mw\,/\,g\log n)})$. Remarkably, we also prove a matching unconditional lower bound that holds for all parameter regimes except very small grammars and relatively small data structures.
  Previous work focused on query time as a function of $n$ only, achieving $O(\log n)$ time using $O(g)$ space [Bille et al.; SIAM J. Comput. 2015] and $O(\frac{\log n}{\log \log n})$ time using $O(g\log^{\epsilon} n)$ space for any constant $\epsilon &gt; 0$ [Belazzougui et al.; ESA'15], [Ganardi, Je\.z, Lohrey; J. ACM 2021]. The only tight lower bound [Verbin and Yu; CPM'13] was $\Omega(\frac{\log n}{\log\log n})$ for $w=\Theta(\log n)$, $n^{\Omega(1)}\le g\le n^{1-\Omega(1)}$, and $M=g\log^{\Theta(1)}n$. In contrast, our result yields tight bounds in all relevant parameters and almost all regimes.
  Our data structure admits efficient deterministic construction. It relies on novel grammar transformations that generalize contracting grammars [Ganardi; ESA'21]. Beyond Random Access, its variants support substring extraction, rank, and select.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10864v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anouk Duyster, Tomasz Kociumaka</dc:creator>
    </item>
    <item>
      <title>Bounding the Average Move Structure Query for Faster and Smaller RLBWT Permutations</title>
      <link>https://arxiv.org/abs/2602.11029</link>
      <description>arXiv:2602.11029v1 Announce Type: new 
Abstract: The move structure represents permutations with long contiguously permuted intervals in compressed space with optimal query time. They have become an important feature of compressed text indexes using space proportional to the number of Burrows-Wheeler Transform (BWT) runs, often applied in genomics. This is in thanks not only to theoretical improvements over past approaches, but great cache efficiency and average case query time in practice. This is true even without using the worst case guarantees provided by the interval splitting balancing of the original result. In this paper, we show that an even simpler type of splitting, length capping by truncating long intervals, bounds the average move structure query time to optimal whilst obtaining a superior construction time than the traditional approach. This also proves constant query time when amortized over a full traversal of a single cycle permutation from an arbitrary starting position.
  Such a scheme has surprising benefits both in theory and practice. We leverage the approach to improve the representation of any move structure with $r$ runs over a domain $n$ to $O(r \log r + r \log \frac{n}{r})$-bits of space. The worst case query time is also improved to $O(\log \frac{n}{r})$ without balancing. An $O(r)$-time and $O(r)$-space construction lets us apply the method to run-length encoded BWT (RLBWT) permutations such as LF and $\phi$ to obtain optimal-time algorithms for BWT inversion and suffix array (SA) enumeration in $O(r)$ additional working space. Finally, we provide the RunPerm library, providing flexible plug and play move structure support, and use it to evaluate our splitting approach. Experiments find length capping results in faster move structures, but also a space reduction: at least $\sim 40\%$ for LF across large repetitive genomic collections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11029v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathaniel K. Brown, Ben Langmead</dc:creator>
    </item>
    <item>
      <title>Quadratic Speedup for Computing Contraction Fixed Points</title>
      <link>https://arxiv.org/abs/2602.10296</link>
      <description>arXiv:2602.10296v1 Announce Type: cross 
Abstract: We study the problem of finding an $\epsilon$-fixed point of a contraction map $f:[0,1]^k\mapsto[0,1]^k$ under both the $\ell_\infty$-norm and the $\ell_1$-norm. For both norms, we give an algorithm with running time $O(\log^{\lceil k/2\rceil}(1/\epsilon))$, for any constant $k$. These improve upon the previous best $O(\log^k(1/\epsilon))$-time algorithm for the $\ell_{\infty}$-norm by Shellman and Sikorski [SS03], and the previous best $O(\log^k (1/\epsilon ))$-time algorithm for the $\ell_{1}$-norm by Fearnley, Gordon, Mehta and Savani [FGMS20].</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10296v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Yuhao Li, Mihalis Yannakakis</dc:creator>
    </item>
    <item>
      <title>Chamfer-Linkage for Hierarchical Agglomerative Clustering</title>
      <link>https://arxiv.org/abs/2602.10444</link>
      <description>arXiv:2602.10444v1 Announce Type: cross 
Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10444v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kishen N Gowda, Willem Fletcher, MohammadHossein Bateni, Laxman Dhulipala, D Ellis Hershkowitz, Rajesh Jayaram, Jakub {\L}\k{a}cki</dc:creator>
    </item>
    <item>
      <title>Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems</title>
      <link>https://arxiv.org/abs/2602.10486</link>
      <description>arXiv:2602.10486v1 Announce Type: cross 
Abstract: We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).
  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10486v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijay K. Garg, Rohan Garg</dc:creator>
    </item>
    <item>
      <title>Near-Feasible Stable Matchings: Incentives and Optimality</title>
      <link>https://arxiv.org/abs/2602.10851</link>
      <description>arXiv:2602.10851v1 Announce Type: cross 
Abstract: Stable matching is a fundamental area with many practical applications, such as centralised clearinghouses for school choice or job markets. Recent work has introduced the paradigm of near-feasibility in capacitated matching settings, where agent capacities are slightly modified to ensure the existence of desirable outcomes. While useful when no stable matching exists, or some agents are left unmatched, it has not previously been investigated whether near-feasible stable matchings satisfy desirable properties with regard to their stability in the original instance. Furthermore, prior works often leave open deviation incentive issues that arise when the centralised authority modifies agents' capacities.
  We consider these issues in the Stable Fixtures problem model, which generalises many classical models through non-bipartite preferences and capacitated agents. We develop a formal framework to analyse and quantify agent incentives to adhere to computed matchings. Then, we embed near-feasible stable matchings in this framework and study the trade-offs between instability, capacity modifications, and computational complexity. We prove that capacity modifications can be simultaneously optimal at individual and aggregate levels, and provide efficient algorithms to compute them. We show that different modification strategies significantly affect stability, and establish that minimal modifications and minimal deviation incentives are compatible and efficiently computable under general conditions. Finally, we provide exact algorithms and experimental results for tractable and intractable versions of these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10851v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Glitzner</dc:creator>
    </item>
    <item>
      <title>Implicit representations via the polynomial method</title>
      <link>https://arxiv.org/abs/2602.10922</link>
      <description>arXiv:2602.10922v1 Announce Type: cross 
Abstract: Semialgebraic graphs are graphs whose vertices are points in $\mathbb{R}^d$, and adjacency between two vertices is determined by the truth value of a semialgebraic predicate of constant complexity. We show how to harness polynomial partitioning methods to construct compact adjacency labeling schemes for families of semialgebraic graphs.
  That is, we show that for any family of semialgebraic graphs, given a graph on $n$ vertices in this family, we can assign a label consisting of $O(n^{1-2/(d+1) + \varepsilon})$ bits to each vertex (where $\varepsilon &gt; 0$ can be made arbitrarily small and the constant of proportionality depends on $\varepsilon$ and on the complexity of the adjacency-defining predicate), such that adjacency between two vertices can be determined solely from their two labels, without any additional information. We obtain for instance that unit disk graphs and segment intersection graphs have such labelings with labels of $O(n^{1/3 + \varepsilon})$ bits. This is in contrast to their natural implicit representation consisting of the coordinates of the disk centers or segment endpoints, which sometimes require exponentially many bits. It also improves on the best known bound of $O(n^{1-1/d}\log n)$ for $d$-dimensional semialgebraic families due to Alon (Discrete Comput. Geom., 2024), a bound that holds more generally for graphs with shattering functions bounded by a degree-$d$ polynomial.
  We also give new bounds on the size of adjacency labels for other families of graphs. In particular, we consider semilinear graphs, which are semialgebraic graphs in which the predicate only involves linear polynomials. We show that semilinear graphs have adjacency labels of size $O(\log n)$. We also prove that polygon visibility graphs, which are not semialgebraic in the above sense, have adjacency labels of size $O(\log^3 n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10922v1</guid>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean Cardinal, Micha Sharir</dc:creator>
    </item>
    <item>
      <title>Parameterized Complexity of Finding a Maximum Common Vertex Subgraph Without Isolated Vertices</title>
      <link>https://arxiv.org/abs/2602.10948</link>
      <description>arXiv:2602.10948v1 Announce Type: cross 
Abstract: In this paper, we study the Maximum Common Vertex Subgraph problem: Given two input graphs $G_1,G_2$ and a non-negative integer $h$, is there a common subgraph $H$ on at least $h$ vertices such that there is no isolated vertex in $H$. In other words, each connected component of $H$ has at least $2$ vertices. This problem naturally arises in graph theory along with other variants of the well-studied Maximum Common Subgraph problem and also has applications in computational social choice. We show that this problem is NP-hard and provide an FPT algorithm when parameterized by $h$. Next, we conduct a study of the problem on common structural parameters like vertex cover number, maximum degree, treedepth, pathwidth and treewidth of one or both input graphs. We derive a complete dichotomy of parameterized results for our problem with respect to individual parameterizations as well as combinations of parameterizations from the above structural parameters. This provides us with a deep insight into the complexity theoretic and parameterized landscape of this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10948v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Palash Dey, Anubhav Dhar, Ashlesha Hota, Sudeshna Kolay, Aritra Mitra</dc:creator>
    </item>
    <item>
      <title>Polytope Scheduling with Groups: Unified Models and Optimal Guarantees</title>
      <link>https://arxiv.org/abs/2501.17682</link>
      <description>arXiv:2501.17682v2 Announce Type: replace 
Abstract: We propose new abstract and unified perspectives on a range of scheduling and graph coloring problems with general min-sum objectives. Specifically, we consider various problems where the objective function is the weighted sum of completion times over groups of entities (jobs, vertices, or edges), thereby generalizing two important objectives in scheduling: makespan and the sum of weighted completion times.
  As one of our main results, we present a best-possible $\mathcal O(\log g)$-competitive algorithm in the non-clairvoyant online setting, where $g$ denotes the size of the largest group. This is the first non-trivial competitive bound for several problems with group completion time objective, and it is an exponential improvement over previous results for non-clairvoyant coflow scheduling. For offline scheduling, we provide elegant yet powerful meta-frameworks that, in a unifying way, yield new or stronger approximation algorithms for our new abstract problems as well as for previously well-studied special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17682v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Lindermayr, Zhenwei Liu, Nicole Megow</dc:creator>
    </item>
    <item>
      <title>Algorithmically Establishing Trust in Evaluators</title>
      <link>https://arxiv.org/abs/2506.03083</link>
      <description>arXiv:2506.03083v4 Announce Type: replace 
Abstract: An evaluator, such as an LLM-as-a-judge, is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. Traditional approaches either rely on testing the evaluator against references or assume that it `knows' somehow the correct labelling. Both approaches fail when references are unavailable: the former requires data, and the latter is an assumption, not evidence. To address this, we introduce the `No-Data Algorithm', which provably establishes trust in an evaluator without requiring any labelled data. Our algorithm works by successively posing challenges to said evaluator. We prove that after $r$ challenge rounds, it accepts an evaluator which knows the correct labels with probability $ \geq 1 - (1/4)^r$, and reliably flags untrustworthy ones. We present formal proofs of correctness, empirical tests, and applications to assessing trust in LLMs-as-judges for low-resource language labelling. Our work enables scientifically-grounded evaluator trust in low-data domains, addressing a critical bottleneck for scalable, trustworthy LLM deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03083v4</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adrian de Wynter</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Sparsifying Random CSPs</title>
      <link>https://arxiv.org/abs/2508.13345</link>
      <description>arXiv:2508.13345v2 Announce Type: replace 
Abstract: The problem of CSP sparsification asks: for a given CSP instance, what is the sparsest possible reweighting such that for every possible assignment to the instance, the number of satisfied constraints is preserved up to a factor of $1 \pm \epsilon$? We initiate the study of the sparsification of random CSPs. In particular, we consider two natural random models: the $r$-partite model and the uniform model. In the $r$-partite model, CSPs are formed by partitioning the variables into $r$ parts, with constraints selected by randomly picking one vertex out of each part. In the uniform model, $r$ distinct vertices are chosen at random from the pool of variables to form each constraint.
  In the $r$-partite model, we exhibit a sharp threshold phenomenon. For every predicate $P$, there is an integer $k$ such that a random instance on $n$ vertices and $m$ edges cannot (essentially) be sparsified if $m \le n^k$ and can be sparsified to size $\approx n^k$ if $m \ge n^k$. Here, $k$ corresponds to the largest copy of the AND which can be found within $P$. Furthermore, these sparsifiers are simple, as they can be constructed by i.i.d. sampling of the edges.
  In the uniform model, the situation is a bit more complex. For every predicate $P$, there is an integer $k$ such that a random instance on $n$ vertices and $m$ edges cannot (essentially) be sparsified if $m \le n^k$ and can sparsified to size $\approx n^k$ if $m \ge n^{k+1}$. However, for some predicates $P$, if $m \in [n^k, n^{k+1}]$, there may or may not be a nontrivial sparsifier. In fact, we show that there are predicates where the sparsifiability of random instances is non-monotone, i.e., as we add more random constraints, the instances become more sparsifiable. We give a precise (efficiently computable) procedure for determining which situation a specific predicate $P$ falls into.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13345v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Joshua Brakensiek, Venkatesan Guruswami, Aaron Putterman</dc:creator>
    </item>
    <item>
      <title>A Finer View of the Parameterized Landscape of Labeled Graph Contractions</title>
      <link>https://arxiv.org/abs/2510.06102</link>
      <description>arXiv:2510.06102v2 Announce Type: replace 
Abstract: We study the \textsc{Labeled Contractibility} problem, where the input consists of two vertex-labeled graphs $G$ and $H$, and the goal is to determine whether $H$ can be obtained from $G$ via a sequence of edge contractions.
  Lafond and Marchand~[WADS 2025] initiated the parameterized complexity study of this problem, showing it to be \(\W[1]\)-hard when parameterized by the number \(k\) of allowed contractions. They also proved that the problem is fixed-parameter tractable when parameterized by the tree-width \(\tw\) of \(G\), via an application of Courcelle's theorem resulting in a non-constructive algorithm.
  In this work, we present a constructive fixed-parameter algorithm for \textsc{Labeled Contractibility} with running time \(2^{\mathcal{O}(\tw^2)} \cdot |V(G)|^{\mathcal{O}(1)}\). We also prove that unless the Exponential Time Hypothesis (\ETH) fails, it does not admit an algorithm running in time \(2^{o(\tw^2)} \cdot |V(G)|^{\mathcal{O}(1)}\). This result adds \textsc{Labeled Contractibility} to a small list of problems that admit such a lower bound and matching algorithm.
  We further strengthen existing hardness results by showing that the problem remains \NP-complete even when both input graphs have bounded maximum degree. We also investigate parameterizations by \((k + \delta(G))\) where \(\delta(G)\) denotes the degeneracy of \(G\), and rule out the existence of subexponential-time algorithms. This answers question raised in Lafond and Marchand~[WADS 2025]. We additionally provide an improved \FPT\ algorithm with better dependence on \((k + \delta(G))\) than previously known. Finally, we analyze a brute-force algorithm for \textsc{Labeled Contractibility} with running time \(|V(H)|^{\mathcal{O}(|V(G)|)}\), and show that this running time is optimal under \ETH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06102v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.FSTTCS.2025.43</arxiv:DOI>
      <arxiv:journal_reference>LIPIcs, Vol. 360, FSTTCS 2025, pp. 43:1-43:19</arxiv:journal_reference>
      <dc:creator>Yashaswini Mathur, Prafullkumar Tale</dc:creator>
    </item>
    <item>
      <title>Planar Length-Constrained Minimum Spanning Trees</title>
      <link>https://arxiv.org/abs/2510.09002</link>
      <description>arXiv:2510.09002v2 Announce Type: replace 
Abstract: In length-constrained minimum spanning tree (MST) we are given an $n$-node graph $G = (V,E)$ with edge weights $w : E \to \mathbb{Z}_{\geq 0}$ and edge lengths $l: E \to \mathbb{Z}_{\geq 0}$ along with a root node $r \in V$ and a length-constraint $h \in \mathbb{Z}_{\geq 0}$. Our goal is to output a spanning tree of minimum weight according to $w$ in which every node is at distance at most $h$ from $r$ according to $l$.
  We give a polynomial-time algorithm for planar graphs which, for any constant $\epsilon &gt; 0$, outputs an $O\left(\log^{1+\epsilon} n\right)$-approximate solution with every node at distance at most $(1+\epsilon)h$ from $r$ for any constant $\epsilon &gt; 0$. Our algorithm is based on new length-constrained versions of classic planar separators which may be of independent interest. Additionally, our algorithm works for length-constrained Steiner tree. Complementing this, we show that any algorithm on general graphs for length-constrained MST in which nodes are at most $2h$ from $r$ cannot achieve an approximation of $O\left(\log ^{2-\epsilon} n\right)$ for any constant $\epsilon &gt; 0$ under standard complexity assumptions; as such, our results separate the approximability of length-constrained MST in planar and general graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09002v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D Ellis Hershkowitz, Richard Z Huang</dc:creator>
    </item>
    <item>
      <title>Covering and packing mixed-integer linear programs with a fixed number of constraints: Approximation and convex hull</title>
      <link>https://arxiv.org/abs/2512.02571</link>
      <description>arXiv:2512.02571v2 Announce Type: replace 
Abstract: This paper presents an algorithmic study of a class of covering mixed-integer linear programming problems which encompasses classic cover problems, including multidimensional knapsack, facility location and supplier selection problems. We first show some properties of optimal solutions, which are then used to decompose the problem into instances of the multidimensional knapsack cover problem with a single continuous variable per dimension. The proposed decomposition is used to design a polynomial-time approximation scheme for the problem with a fixed number of constraints. To the best of our knowledge, this is the first approximation scheme for such a general class of covering mixed-integer linear programs. Moreover, we design a fully polynomial-time approximation scheme and an approximate linear programming formulation for the case with a single constraint. These results improve upon the previously best-known 2-approximation algorithm for the knapsack cover problem with a single continuous variable. Finally, we show a perfect compact formulation for the case where all variables have the same lower and upper bounds. Analogous results are derived for the packing and more general variants of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02571v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kobe Grobben, Phablo F. S. Moura, Hande Yaman</dc:creator>
    </item>
    <item>
      <title>Two Complexity Results on Spanning-Tree Congestion Problems</title>
      <link>https://arxiv.org/abs/2601.10881</link>
      <description>arXiv:2601.10881v2 Announce Type: replace 
Abstract: In the spanning-tree congestion problem ($\mathsf{STC}$), we are given a graph $G$, and the objective is to compute a spanning tree of $G$ that minimizes the maximum edge congestion. While $\mathsf{STC}$ is known to be $\mathbb{NP}$-hard, even for some restricted graph classes, several key questions regarding its computational complexity remain open, and we address some of these in our paper. (i) For graphs of degree at most $\Delta$, it is known that $\mathsf{STC}$ is $\mathbb{NP}$-hard when $\Delta\ge 8$. We provide a complete resolution of this variant, by showing that $\mathsf{STC}$ remains $\mathbb{NP}$-hard for each degree bound $\Delta\ge 3$. (ii) In the decision version of $\mathsf{STC}$, given an integer $K$, the goal is to determine whether the congestion of $G$ is at most $K$. We prove that this variant is polynomial-time solvable for $K$-edge-connected graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10881v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunny Atalig, Marek Chrobak, Christoph D\"urr, Petr Kolman, Huong Luu, Ji\v{r}\'i Sgall, Gregory Zhu</dc:creator>
    </item>
    <item>
      <title>Explainable Information Design</title>
      <link>https://arxiv.org/abs/2508.14196</link>
      <description>arXiv:2508.14196v3 Announce Type: replace-cross 
Abstract: Optimal signaling schemes in information design (Bayesian persuasion) often involve randomization or disconnected partitions of state space, which might be too intricate to be audited or communicated. We propose explainable information design in the context of linear information design with a continuous state space. In the case of single-dimensional state, we restrict the information designer to use $K$-partitional signaling schemes defined by deterministic and monotone partitions of the state space, where a unique signal is sent for all states in each part. We prove that the price of explainability (PoE) -- the ratio between the performances of the optimal explainable signaling scheme and unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning that partitional signaling schemes are never worse than arbitrary signaling schemes by a factor of $2$. For a uniform prior, this PoE can be improved to a tight $2/3$. We then extend the analysis to multi-dimensional state spaces by studying two natural explainability notions: convex-partitional policies and axis-aligned rectangular policies. For convex-partitional policies, we prove a tight PoE of $1/(m+1)$, while for rectangular policies we establish a PoE guarantee under uniform prior that is independent of $K$ but unavoidably exponential in $m$. On the computational side, we prove that the exact optimization of explainable policy is NP-hard in general, but provide efficient approximation methods, including an FPTAS for Lipschitz utility functions and a polynomial-time algorithm that achieves the worst-case $1/2$ benchmark for the broad class of discontinuous, piecewise Lipschitz, utility functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14196v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Chen, Tao Lin, Wei Tang, Jamie Tucker-Foltz</dc:creator>
    </item>
    <item>
      <title>An Allele-Centric Pan-Graph-Matrix Representation for Scalable Pangenome Analysis</title>
      <link>https://arxiv.org/abs/2512.21320</link>
      <description>arXiv:2512.21320v2 Announce Type: replace-cross 
Abstract: Population-scale pangenome analysis increasingly requires representations that unify single-nucleotide and structural variation while remaining scalable across large cohorts. Existing formats are typically sequence-centric, path-centric, or sample-centric, and often obscure population structure or fail to exploit carrier sparsity. We introduce the H1 pan-graph-matrix, an allele-centric representation that encodes exact haplotype membership using adaptive per-allele compression. By treating alleles as first-class objects and selecting optimal encodings based on carrier distribution, H1 achieves near-optimal storage across both common and rare variants. We further introduce H2, a path-centric dual representation derived from the same underlying allele-haplotype incidence information that restores explicit haplotype ordering while remaining exactly equivalent in information content. Using real human genome data, we show that this representation yields substantial compression gains, particularly for structural variants, while remaining equivalent in information content to pangenome graphs. H1 provides a unified, population-aware foundation for scalable pangenome analysis and downstream applications such as rare-variant interpretation and drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21320v2</guid>
      <category>q-bio.GN</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Garrone</dc:creator>
    </item>
    <item>
      <title>Circuit Diameter of Polyhedra is Strongly Polynomial</title>
      <link>https://arxiv.org/abs/2602.06958</link>
      <description>arXiv:2602.06958v2 Announce Type: replace-cross 
Abstract: We prove a strongly polynomial bound on the circuit diameter of polyhedra, resolving the circuit analogue of the polynomial Hirsch conjecture. Specifically, we show that the circuit diameter of a polyhedron $P = \{x\in \mathbb{R}^n:\, A x = b, \, x \ge 0\}$ with $A\in\mathbb{R}^{m\times n}$ is $O(m^2 \log m)$. Our construction yields monotone circuit walks, giving the same bound for the monotone circuit diameter.
  The circuit diameter, introduced by Borgwardt, Finhold, and Hemmecke (SIDMA 2015), is a natural relaxation of the combinatorial diameter that allows steps along circuit directions rather than only along edges. All prior upper bounds on the circuit diameter were only weakly polynomial. Finding a circuit augmentation algorithm that matches this bound would yield a strongly polynomial time algorithm for linear programming, resolving Smale's 9th problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06958v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bento Natura</dc:creator>
    </item>
  </channel>
</rss>
