<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Nov 2024 02:44:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploring the Reductions Between SSP-NP-complete Problems and Developing a Compendium Website Displaying the Results</title>
      <link>https://arxiv.org/abs/2411.05796</link>
      <description>arXiv:2411.05796v1 Announce Type: new 
Abstract: SSP reductions are a type of polynomial reductions that also preserve the solutions of the instances. This means there is a mapping from each solution in the original instance to one in the reduced instance, allowing direct deduction of an original solution from a solution in the reduced instance. SSP reductions can be used to show SSP-NP completeness of a problem, which is interesting because it has been proven that two min-max variants of SSP-NP complete problems are $\Sigma_2^p$-complete. Min-max optimization problems are optimization problems with the objective of minimizing the maximum outcome. An example is a power network prone to attack by terrorists. Lets say the government wants to minimize the damage that can be achieved by the terrorists, e.g. by protecting the electricity poles which were found to be most important for distributing power. Most theoretical computer scientists assume min-max optimization problems to generally be $\Sigma_2^p$-complete, and with the help of the SSP-framework, this was shown for network interdiction and min-max regret robust optimization. This paper is devoted to the exploration of SSP reductions and the collection of SSP-NP-complete problems as well as SSP reductions. Additionally, a compendium website is developed, in which the SSP-NP-complete problems and SSP reductions are displayed in a graph, so that it becomes easy to understand the relations between the problems. The website is also extendable to include problems from other complexity classes and reductions that are not SSP reductions, such as gap-preserving reductions, fixed parameter tractable reductions or fine-grained reductions. Overall, 19 new SSP reductions are found, resulting in eight new SSP-NP-completeness proofs, and the compendium website is developed to enable theoretical computer scientists to easily access the results and provide a knowledge base for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05796v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Femke Pfaue</dc:creator>
    </item>
    <item>
      <title>Randomized Black-Box PIT for Small Depth +-Regular Non-commutative Circuits</title>
      <link>https://arxiv.org/abs/2411.06569</link>
      <description>arXiv:2411.06569v1 Announce Type: new 
Abstract: In this paper, we address the black-box polynomial identity testing (PIT) problem for non-commutative polynomials computed by $+$-regular circuits, a class of homogeneous circuits introduced by Arvind, Joglekar, Mukhopadhyay, and Raja (STOC 2017, Theory of Computing 2019). These circuits can compute polynomials with a number of monomials that are doubly exponential in the circuit size. They gave an efficient randomized PIT algorithm for +-regular circuits of depth 3 and posed the problem of developing an efficient black-box PIT for higher depths as an open problem.
  We present a randomized black-box polynomial-time algorithm for +-regular circuits of any constant depth. Specifically, our algorithm runs in $s^{O(d^2)}$ time, where $s$ and $d$ represent the size and the depth of the $+$-regular circuit, respectively. Our approach combines several key techniques in a novel way. We employ a nondeterministic substitution automaton that transforms the polynomial into a structured form and utilizes polynomial sparsification along with commutative transformations to maintain non-zeroness. Additionally, we introduce matrix composition, coefficient modification via the automaton, and multi-entry outputs--methods that have not previously been applied in the context of black-box PIT. Together, these techniques enable us to effectively handle exponential degrees and doubly exponential sparsity in non-commutative settings, enabling polynomial identity testing for higher-depth circuits. Our work resolves an open problem from \cite{AJMR19}.
  In particular, we show that if $f$ is a non-zero non-commutative polynomial in $n$ variables over the field $F$, computed by a depth-$d$ $+$-regular circuit of size $s$, then $f$ cannot be a polynomial identity for the matrix algebra $\mathbb{M}_{N}(F)$, where $N= s^{O(d^2)}$ and the size of the field $F$ depending on the degree of $f$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06569v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G V Sumukha Bharadwaj, S Raja</dc:creator>
    </item>
    <item>
      <title>Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra</title>
      <link>https://arxiv.org/abs/2411.07030</link>
      <description>arXiv:2411.07030v1 Announce Type: new 
Abstract: In our work, we consider the problem of computing a vector $x \in Z^n$ of minimum $\|\cdot\|_p$-norm such that $a^\top x \not= a_0$, for any vector $(a,a_0)$ from a given subset of $Z^n$ of size $m$. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the $\textit{Hyperplanes Avoiding Problem}$. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. More precisely, it appears when one needs to evaluate certain rational generating functions in an avoidable critical point. We show that:
  1) With respect to $\|\cdot\|_1$, the problem admits a feasible solution $x$ with $\|x\|_1 \leq (m+n)/2$, and show that such solution can be constructed by a deterministic polynomial-time algorithm with $O(n \cdot m)$ operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes $x$ with a guaranty $\|x\|_{1} \leq n \cdot m$. The original approach of A.~Barvinok can guarantee only $\|x\|_1 = O\bigl((n \cdot m)^n\bigr)$;
  2) The problem is NP-hard with respect to any norm $\|\cdot\|_p$, for $p \in \bigl(R_{\geq 1} \cup \{\infty\}\bigr)$.
  3) As an application, we show that the problem to count integer points in a polytope $P = \{x \in R^n \colon A x \leq b\}$, for given $A \in Z^{m \times n}$ and $b \in Q^m$, can be solved by an algorithm with $O\bigl(\nu^2 \cdot n^3 \cdot \Delta^3 \bigr)$ operations, where $\nu$ is the maximum size of a normal fan triangulation of $P$, and $\Delta$ is the maximum value of rank-order subdeterminants of $A$. It refines the previous state-of-the-art $O\bigl(\nu^2 \cdot n^4 \cdot \Delta^3\bigr)$-time algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07030v1</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorii Dakhno, Dmitry Gribanov, Nikita Kasianov, Anastasiia Kats, Andrey Kupavskii, Nikita Kuz'min</dc:creator>
    </item>
    <item>
      <title>Simple approximation algorithms for Polyamorous Scheduling</title>
      <link>https://arxiv.org/abs/2411.06292</link>
      <description>arXiv:2411.06292v1 Announce Type: cross 
Abstract: In Polyamorous Scheduling, we are given an edge-weighted graph and must find a periodic schedule of matchings in this graph which minimizes the maximal weighted waiting time between consecutive occurrences of the same edge. This NP-hard problem generalises Bamboo Garden Trimming and is motivated by the need to find schedules of pairwise meetings in a complex social group. We present two different analyses of an approximation algorithm based on the Reduce-Fastest heuristic, from which we obtain first a 6-approximation and then a 5.24-approximation for Polyamorous Scheduling. We also strengthen the extant proof that there is no polynomial-time $(1+\delta)$-approximation algorithm for the Optimisation Polyamorous Scheduling problem for any $\delta &lt; \frac1{12}$ unless P = NP to the bipartite case. The decision version of Polyamorous Scheduling has a notion of density, similar to that of Pinwheel Scheduling, where problems with density below the threshold are guaranteed to admit a schedule (cf. the recently proven 5/6 conjecture, Kawamura, STOC 2024). We establish the existence of a similar threshold for Polyamorous Scheduling and give the first non-trivial bounds on the poly density threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06292v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuriy Biktairov, Leszek G\k{a}sieniec, Wanchote Po Jiamjitrak,  Namrata, Benjamin Smith, Sebastian Wild</dc:creator>
    </item>
    <item>
      <title>Program Analysis via Multiple Context Free Language Reachability</title>
      <link>https://arxiv.org/abs/2411.06383</link>
      <description>arXiv:2411.06383v1 Announce Type: cross 
Abstract: Context-free language (CFL) reachability is a standard approach in static analyses, where the analysis question is phrased as a language reachability problem on a graph $G$ wrt a CFL L. While CFLs lack the expressiveness needed for high precision, common formalisms for context-sensitive languages are such that the corresponding reachability problem is undecidable. Are there useful context-sensitive language-reachability models for static analysis?
  In this paper, we introduce Multiple Context-Free Language (MCFL) reachability as an expressive yet tractable model for static program analysis. MCFLs form an infinite hierarchy of mildly context sensitive languages parameterized by a dimension $d$ and a rank $r$. We show the utility of MCFL reachability by developing a family of MCFLs that approximate interleaved Dyck reachability, a common but undecidable static analysis problem.
  We show that MCFL reachability be computed in $O(n^{2d+1})$ time on a graph of $n$ nodes when $r=1$, and $O(n^{d(r+1)})$ time when $r&gt;1$. Moreover, we show that when $r=1$, the membership problem has a lower bound of $n^{2d}$ based on the Strong Exponential Time Hypothesis, while reachability for $d=1$ has a lower bound of $n^{3}$ based on the combinatorial Boolean Matrix Multiplication Hypothesis. Thus, for $r=1$, our algorithm is optimal within a factor $n$ for all levels of the hierarchy based on $d$.
  We implement our MCFL reachability algorithm and evaluate it by underapproximating interleaved Dyck reachability for a standard taint analysis for Android. Used alongside existing overapproximate methods, MCFL reachability discovers all tainted information on 8 out of 11 benchmarks, and confirms $94.3\%$ of the reachable pairs reported by the overapproximation on the remaining 3. To our knowledge, this is the first report of high and provable coverage for this challenging benchmark set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06383v1</guid>
      <category>cs.PL</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanna Kobus Conrado, Adam Husted Kjelstr{\o}m, Andreas Pavlogianni, Jaco van de Pol</dc:creator>
    </item>
    <item>
      <title>The Equivalence Problem of E-Pattern Languages with Length Constraints is Undecidable</title>
      <link>https://arxiv.org/abs/2411.06904</link>
      <description>arXiv:2411.06904v1 Announce Type: cross 
Abstract: Patterns are words with terminals and variables. The language of a pattern is the set of words obtained by uniformly substituting all variables with words that contain only terminals. Length constraints restrict valid substitutions of variables by associating the variables of a pattern with a system (or disjunction of systems) of linear diophantine inequalities. Pattern languages with length constraints contain only words in which all variables are substituted to words with lengths that fulfill such a given set of length constraints. We consider membership, inclusion, and equivalence problems for erasing and non-erasing pattern languages with length constraints. Our main result shows that the erasing equivalence problem, one of the most prominent open problems in the realm of patterns-becomes undecidable if length constraints are allowed in addition to variable equality. Additionally, it is shown that the terminal-free inclusion problem-another prominent open problem in the realm of patterns-is also undecidable in this setting. It is also shown that considering regular constraints, i.e., associating variables also with regular languages as additional restrictions together with length constraints for valid substitutions, results in undecidability of the non-erasing equivalence problem. This sets a first upper bound on constraints to obtain undecidability in this case, as this problem is trivially decidable in the case of no constraints and as it has unknown decidability if only regular- or only length-constraints are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06904v1</guid>
      <category>cs.FL</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dirk Nowotka, Max Wiedenh\"oft</dc:creator>
    </item>
    <item>
      <title>Finite Variable Counting Logics with Restricted Requantification</title>
      <link>https://arxiv.org/abs/2411.06944</link>
      <description>arXiv:2411.06944v1 Announce Type: cross 
Abstract: Counting logics with a bounded number of variables form one of the central concepts in descriptive complexity theory. Although they restrict the number of variables that a formula can contain, the variables can be nested within scopes of quantified occurrences of themselves. In other words, the variables can be requantified. We study the fragments obtained from counting logics by restricting requantification for some but not necessarily all the variables. Similar to the logics without limitation on requantification, we develop tools to investigate the restricted variants. Specifically, we introduce a bijective pebble game in which certain pebbles can only be placed once and for all, and a corresponding two-parametric family of Weisfeiler-Leman algorithms. We show close correspondences between the three concepts. By using a suitable cops-and-robber game and adaptations of the Cai-F\"urer-Immerman construction, we completely clarify the relative expressive power of the new logics. We show that the restriction of requantification has beneficial algorithmic implications in terms of graph identification. Indeed, we argue that with regard to space complexity, non-requantifiable variables only incur an additive polynomial factor when testing for equivalence. In contrast, for all we know, requantifiable variables incur a multiplicative linear factor. Finally, we observe that graphs of bounded tree-depth and 3-connected planar graphs can be identified using no, respectively, only a very limited number of requantifiable variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06944v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ra{\ss}mann, Georg Schindling, Pascal Schweitzer</dc:creator>
    </item>
    <item>
      <title>Matrix hypercontractivity, streaming algorithms and LDCs: the large alphabet case</title>
      <link>https://arxiv.org/abs/2109.02600</link>
      <description>arXiv:2109.02600v3 Announce Type: replace 
Abstract: We prove a hypercontractive inequality for matrix-valued functions defined over large alphabets. In order to do so, we prove a generalization of the powerful $2$-uniform convexity inequality for trace norms of Ball, Carlen, Lieb (Inventiones Mathematicae'94). Using our hypercontractive~inequality, we present upper and lower bounds for the communication complexity of the Hidden Hypermatching problem defined over large alphabets. We then consider streaming algorithms for approximating the value of Unique Games on a hypergraph with $t$-size hyperedges. By using our communication lower bound, we show that every streaming algorithm in the adversarial model achieving an $(r-\varepsilon)$-approximation of this value requires $\Omega(n^{1-2/t})$ quantum space, where $r$ is the alphabet size. We next present a lower bound for locally decodable codes (LDC) $\mathbb{Z}_r^n\to \mathbb{Z}_r^N$ over large alphabets with recoverability probability at least $1/r + \varepsilon$. Using hypercontractivity, we give an exponential lower bound $N = 2^{\Omega(\varepsilon^4 n/r^4)}$ for $2$-query (possibly non-linear) LDCs over $\mathbb{Z}_r$ and using the non-commutative Khintchine inequality we prove an improved lower bound of $N = 2^{\Omega(\varepsilon^2 n/r^2)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.02600v3</guid>
      <category>cs.CC</category>
      <category>math.FA</category>
      <category>quant-ph</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3688824</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Comput. Theory 16, 4, Article 21 (November 2024)</arxiv:journal_reference>
      <dc:creator>Srinivasan Arunachalam, Joao F. Doriguello</dc:creator>
    </item>
    <item>
      <title>Low-Degree Testing Over Grids</title>
      <link>https://arxiv.org/abs/2305.04983</link>
      <description>arXiv:2305.04983v2 Announce Type: replace 
Abstract: We study the question of local testability of low (constant) degree functions from a product domain $S_1 \times \dots \times {S}_n$ to a field $\mathbb{F}$, where ${S_i} \subseteq \mathbb{F}$ can be arbitrary constant sized sets. We show that this family is locally testable when the grid is "symmetric". That is, if ${S_i} = {S}$ for all i, there is a probabilistic algorithm using constantly many queries that distinguishes whether $f$ has a polynomial representation of degree at most $d$ or is $\Omega(1)$-far from having this property. In contrast, we show that there exist asymmetric grids with $|{S}_1| =\dots= |{S}_n| = 3$ for which testing requires $\omega_n(1)$ queries, thereby establishing that even in the context of polynomials, local testing depends on the structure of the domain and not just the distance of the underlying code.
  The low-degree testing problem has been studied extensively over the years and a wide variety of tools have been applied to propose and analyze tests. Our work introduces yet another new connection in this rich field, by building low-degree tests out of tests for "junta-degrees". A function $f : {S}_1 \times \dots \times {S}_n \to {G}$, for an abelian group ${G}$ is said to be a junta-degree-$d$ function if it is a sum of $d$-juntas. We derive our low-degree test by giving a new local test for junta-degree-$d$ functions. For the analysis of our tests, we deduce a small-set expansion theorem for spherical noise over large grids, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04983v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prashanth Amireddy, Srikanth Srinivasan, Madhu Sudan</dc:creator>
    </item>
    <item>
      <title>Constructibility, computational complexity and P versus NP</title>
      <link>https://arxiv.org/abs/2406.16843</link>
      <description>arXiv:2406.16843v3 Announce Type: replace 
Abstract: If an algorithm is to be counted as a practically working solution to a decision problem, then the algorithm must must verifiable in some constructed and ``trusted'' theory such as PA or ZF. In this paper, a class of decision problems related to inconsistency proofs for a general class of formal theories is used to demonstrate that under this constructibility restriction, an unformalizable yet arguably convincing argument can be given for the existence of decision problems for which there exists an explicitly constructible algorithm recognizing correct solutions in polynomial time, but for which there exists no explicitly constructible solution algorithm. While these results do not solve the P versus NP problem in the classical sense of supplying a proof one way or the other in a ``trusted'' formal theory, arguably they resolve the natural constructive version of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16843v3</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arne Hole</dc:creator>
    </item>
    <item>
      <title>Verifiable Quantum Advantage without Structure</title>
      <link>https://arxiv.org/abs/2204.02063</link>
      <description>arXiv:2204.02063v3 Announce Type: replace-cross 
Abstract: We show the following hold, unconditionally unless otherwise stated, relative to a random oracle:
  - There are NP search problems solvable by quantum polynomial-time machines but not classical probabilistic polynomial-time machines.
  - There exist functions that are one-way, and even collision resistant, against classical adversaries but are easily inverted quantumly. Similar separations hold for digital signatures and CPA-secure public key encryption (the latter requiring the assumption of a classically CPA-secure encryption scheme). Interestingly, the separation does not necessarily extend to the case of other cryptographic objects such as PRGs.
  - There are unconditional publicly verifiable proofs of quantumness with the minimal rounds of interaction: for uniform adversaries, the proofs are non-interactive, whereas for non-uniform adversaries the proofs are two message public coin.
  - Our results do not appear to contradict the Aaronson-Ambanis conjecture. Assuming this conjecture, there exist publicly verifiable certifiable randomness, again with the minimal rounds of interaction.
  By replacing the random oracle with a concrete cryptographic hash function such as SHA2, we obtain plausible Minicrypt instantiations of the above results. Previous analogous results all required substantial structure, either in terms of highly structured oracles and/or algebraic assumptions in Cryptomania and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.02063v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3658665</arxiv:DOI>
      <arxiv:journal_reference>J. ACM 71(3): 20 (2024)</arxiv:journal_reference>
      <dc:creator>Takashi Yamakawa, Mark Zhandry</dc:creator>
    </item>
    <item>
      <title>Stochastic Optimization under Hidden Convexity</title>
      <link>https://arxiv.org/abs/2401.00108</link>
      <description>arXiv:2401.00108v2 Announce Type: replace-cross 
Abstract: In this work, we consider constrained stochastic optimization problems under hidden convexity, i.e., those that admit a convex reformulation via non-linear (but invertible) map $c(\cdot)$. A number of non-convex problems ranging from optimal control, revenue and inventory management, to convex reinforcement learning all admit such a hidden convex structure. Unfortunately, in the majority of applications considered, the map $c(\cdot)$ is unavailable or implicit; therefore, directly solving the convex reformulation is not possible. On the other hand, the stochastic gradients with respect to the original variable are often easy to obtain. Motivated by these observations, we examine the basic projected stochastic (sub-) gradient methods for solving such problems under hidden convexity. We provide the first sample complexity guarantees for global convergence in smooth and non-smooth settings. Additionally, in the smooth setting, we improve our results to the last iterate convergence in terms of function value gap using the momentum variant of projected stochastic gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00108v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilyas Fatkhullin, Niao He, Yifan Hu</dc:creator>
    </item>
  </channel>
</rss>
