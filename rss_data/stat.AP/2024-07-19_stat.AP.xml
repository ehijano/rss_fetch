<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Partially Pooled NSUM Model: Detailed estimation of CSEM trafficking prevalence in Philippine municipalities</title>
      <link>https://arxiv.org/abs/2407.13267</link>
      <description>arXiv:2407.13267v1 Announce Type: new 
Abstract: Effective policy and intervention strategies to combat human trafficking for child sexual exploitation material (CSEM) production require accurate prevalence estimates. Traditional Network Scale Up Method (NSUM) models often necessitate standalone surveys for each geographic region, escalating costs and complexity. This study introduces a partially pooled NSUM model, using a hierarchical Bayesian framework that efficiently aggregates and utilizes data across multiple regions without increasing sample sizes. We developed this model for a novel national survey dataset from the Philippines and we demonstrate its ability to produce detailed municipal-level prevalence estimates of trafficking for CSEM production. Our results not only underscore the model's precision in estimating hidden populations but also highlight its potential for broader application in other areas of social science and public health research, offering significant implications for resource allocation and intervention planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13267v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Nyarko-Agyei, Scott Moser, Rowland G Seymour, Ben Brewster, Sabrina Li, Esther Weir, Todd Landman, Emily Wyman, Christine Belle Torres, Imogen Fell, Doreen Boyd</dc:creator>
    </item>
    <item>
      <title>Prediction intervals for overdispersed binomial endpoints and their application to historical control data</title>
      <link>https://arxiv.org/abs/2407.13296</link>
      <description>arXiv:2407.13296v1 Announce Type: new 
Abstract: In toxicology, the validation of the concurrent control by historical control data (HCD) has become requirements. This validation is usually done by historical control limits (HCL) which in practice are often graphically displayed in a Sheward control chart like manner. In many applications, HCL are applied to dichotomous data, e.g. the number of rats with a tumor vs. the number of rats without a tumor (carcinogenicity studies) or the number of cells with a micronucleus out of a total number of cells. Dichotomous HCD may be overdispersed and can be heavily right- (or left-) skewed, which is usually not taken into account in the practical applications of HCL. To overcome this problem, four different prediction intervals (two frequentist, two Bayesian), that can be applied to such data, are proposed. Comprehensive Monte-Carlo simulations assessing the coverage probabilities of seven different methods for HCL calculation reveal, that frequentist bootstrap calibrated prediction intervals control the type-1-error best. Heuristics traditionally used in control charts (e.g. the limits in Sheward np-charts or the mean plus minus 2 SD) as well a the historical range fail to control a pre-specified coverage probability. The application of HCL is demonstrated based on a real life data set containing historical controls from long-term carcinogenicity studies run on behalf of the U.S. National Toxicology Program. The proposed frequentist prediction intervals are publicly available from the R package predint, whereas R code for the computation of the Bayesian prediction intervals is provided via GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13296v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max Menssen, Jonathan Ratjens</dc:creator>
    </item>
    <item>
      <title>Movement-based models for abundance data</title>
      <link>https://arxiv.org/abs/2407.13384</link>
      <description>arXiv:2407.13384v1 Announce Type: new 
Abstract: We develop two statistical models for space-time abundance data based on the modelling of an underlying continuous movement. Different from other models for abundance in the current statistical ecology literature, our models focus especially on an explicit connection between the movement of the individuals and the count, and on the space-time auto-correlation thus induced. Our first model (Snapshot) describes the count of free individuals with a false-negative detection error. Our second model (Capture) describes the capture and retention in traps of moving individuals, and it is constructed using an axiomatic approach establishing three simple principles, from which it is deduced that the density of the capture time is the solution of a Volterra integral equation of the second kind. We make explicit the space-time mean and covariance structures of the abundance fields thus generated, and we develop simulation methods for both models. The joint distribution of the space-time counts is an instance of a new multivariate distribution, here baptised the Evolving-Categories Multinomial distribution, for which we establish some key properties. Since a general expression of the likelihood remains intractable, we propose an approximated MLE fitting method by replacing it by a multivariate Gaussian one, which is justified by central limit theorem and respects mean and covariance structures. We apply this method to experimental data of fruit flies released in a meadow and repeatedly captured and counted in an array of traps. We estimate spread and advection parameters, compare our models to an Ecological Diffusion model, and conduct simulation studies to validate our analysis. Asymptotic consistency is experimentally verified. We conclude that we can estimate movement parameters using only abundance data, but must be aware of the necessary conditions to avoid underestimation of spread parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13384v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ricardo Carrizo Vergara, Marc K\'ery, Trevor Hefley</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty in area and regression coefficient estimation from remote sensing maps</title>
      <link>https://arxiv.org/abs/2407.13659</link>
      <description>arXiv:2407.13659v1 Announce Type: new 
Abstract: Remote sensing map products are used to obtain estimates of environmental quantities, such as deforested area or the effect of conservation zones on deforestation. However, the quality of map products varies, and - because maps are outputs of complex machine learning algorithms that take in a variety of remotely sensed variables as inputs - errors are difficult to characterize. Without capturing the biases that may be present, naive calculations of population-level estimates from such maps are statistically invalid. In this paper, we compare several uncertainty quantification methods - stratification, Olofsson area estimation method, and prediction-powered inference - that combine a small amount of randomly sampled ground truth data with large-scale remote sensing map products to generate statistically valid estimates. Applying these methods across four remote sensing use cases in area and regression coefficient estimation, we find that they result in estimates that are more reliable than naively using the map product as if it were 100% accurate and have lower uncertainty than using only the ground truth and ignoring the map product. Prediction-powered inference uses ground truth data to correct for bias in the map product estimate and (unlike stratification) does not require us to choose a map product before sampling. This is the first work to (1) apply prediction-powered inference to remote sensing estimation tasks, and (2) perform uncertainty quantification on remote sensing regression coefficients without assumptions on the structure of map product errors. To improve the utility of machine learning-generated remote sensing maps for downstream applications, we recommend that map producers provide a holdout ground truth dataset to be used for calibration in uncertainty quantification alongside their maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13659v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>eess.SP</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerri Lu, Stephen Bates, Sherrie Wang</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian Processes</title>
      <link>https://arxiv.org/abs/2407.13283</link>
      <description>arXiv:2407.13283v1 Announce Type: cross 
Abstract: We make use of Kronecker structure for scaling Gaussian Process models to large-scale, heterogeneous, clinical data sets. Repeated measures, commonly performed in clinical research, facilitate computational acceleration for nonlinear Bayesian nonparametric models and enable exact sampling for non-conjugate inference, when combinations of continuous and discrete endpoints are observed. Model inference is performed in Stan, and comparisons are made with brms on simulated data and two real clinical data sets, following a radiological image quality theme. Scalable Gaussian Process models compare favourably with parametric models on real data sets with 17,460 observations. Different GP model specifications are explored, with components analogous to random effects, and their theoretical properties are described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13283v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Owen Thomas, Leiv R{\o}nneberg</dc:creator>
    </item>
    <item>
      <title>NIRVAR: Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2407.13314</link>
      <description>arXiv:2407.13314v1 Announce Type: cross 
Abstract: High-dimensional panels of time series arise in many scientific disciplines such as neuroscience, finance, and macroeconomics. Often, co-movements within groups of the panel components occur. Extracting these groupings from the data provides a course-grained description of the complex system in question and can inform subsequent prediction tasks. We develop a novel methodology to model such a panel as a restricted vector autoregressive process, where the coefficient matrix is the weighted adjacency matrix of a stochastic block model. This network time series model, which we call the Network Informed Restricted Vector Autoregression (NIRVAR) model, yields a coefficient matrix that has a sparse block-diagonal structure. We propose an estimation procedure that embeds each panel component in a low-dimensional latent space and clusters the embedded points to recover the blocks of the coefficient matrix. Crucially, the method allows for network-based time series modelling when the underlying network is unobserved. We derive the bias, consistency and asymptotic normality of the NIRVAR estimator. Simulation studies suggest that the NIRVAR estimated embedded points are Gaussian distributed around the ground truth latent positions. On three applications to finance, macroeconomics, and transportation systems, NIRVAR outperforms competing factor and network time series models in terms of out-of-sample prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13314v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Francesco Sanna Passino, Mihai Cucuringu, Alessandra Luati</dc:creator>
    </item>
    <item>
      <title>A unifying modelling approach for hierarchical distributed lag models</title>
      <link>https://arxiv.org/abs/2407.13374</link>
      <description>arXiv:2407.13374v1 Announce Type: cross 
Abstract: We present a statistical modelling framework for implementing Distributed Lag Models (DLMs), encompassing several extensions of the approach to capture the temporally distributed effect from covariates via regression. We place DLMs in the context of penalised Generalized Additive Models (GAMs) and illustrate that implementation via the R package \texttt{mgcv}, which allows for flexible and interpretable inference in addition to thorough model assessment. We show how the interpretation of penalised splines as random quantities enables approximate Bayesian inference and hierarchical structures in the same practical setting. We focus on epidemiological studies and demonstrate the approach with application to mortality data from Cyprus and Greece. For the Cyprus case study, we investigate for the first time, the joint lagged effects from both temperature and humidity on mortality risk with the unexpected result that humidity severely increases risk during cold rather than hot conditions. Another novel application is the use of the proposed framework for hierarchical pooling, to estimate district-specific covariate-lag risk on morality and the use of posterior simulation to compare risk across districts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13374v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theo Economou, Daphne Parliari, Aurelio Tobias, Laura Dawkins, Oliver Stoner, Hamish Steptoe, Rachel Lowe, Maria Athanasiadou, Christophe Sarran, Jos Lelieveld</dc:creator>
    </item>
    <item>
      <title>With or Without Replacement? Improving Confidence in Fourier Imaging</title>
      <link>https://arxiv.org/abs/2407.13575</link>
      <description>arXiv:2407.13575v1 Announce Type: cross 
Abstract: Over the last few years, debiased estimators have been proposed in order to establish rigorous confidence intervals for high-dimensional problems in machine learning and data science. The core argument is that the error of these estimators with respect to the ground truth can be expressed as a Gaussian variable plus a remainder term that vanishes as long as the dimension of the problem is sufficiently high. Thus, uncertainty quantification (UQ) can be performed exploiting the Gaussian model. Empirically, however, the remainder term cannot be neglected in many realistic situations of moderately-sized dimensions, in particular in certain structured measurement scenarios such as Magnetic Resonance Imaging (MRI). This, in turn, can downgrade the advantage of the UQ methods as compared to non-UQ approaches such as the standard LASSO. In this paper, we present a method to improve the debiased estimator by sampling without replacement. Our approach leverages recent results of ours on the structure of the random nature of certain sampling schemes showing how a transition between sampling with and without replacement can lead to a weighted reconstruction scheme with improved performance for the standard LASSO. In this paper, we illustrate how this reweighted sampling idea can also improve the debiased estimator and, consequently, provide a better method for UQ in Fourier imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13575v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik Hoppe, Claudio Mayrink Verdun, Felix Krahmer, Marion I. Menzel, Holger Rauhut</dc:creator>
    </item>
    <item>
      <title>Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning</title>
      <link>https://arxiv.org/abs/2407.13666</link>
      <description>arXiv:2407.13666v1 Announce Type: cross 
Abstract: Uncertainty quantification (UQ) is a crucial but challenging task in many high-dimensional regression or learning problems to increase the confidence of a given predictor. We develop a new data-driven approach for UQ in regression that applies both to classical regression approaches such as the LASSO as well as to neural networks. One of the most notable UQ techniques is the debiased LASSO, which modifies the LASSO to allow for the construction of asymptotic confidence intervals by decomposing the estimation error into a Gaussian and an asymptotically vanishing bias component. However, in real-world problems with finite-dimensional data, the bias term is often too significant to be neglected, resulting in overly narrow confidence intervals. Our work rigorously addresses this issue and derives a data-driven adjustment that corrects the confidence intervals for a large class of predictors by estimating the means and variances of the bias terms from training data, exploiting high-dimensional concentration phenomena. This gives rise to non-asymptotic confidence intervals, which can help avoid overestimating uncertainty in critical applications such as MRI diagnosis. Importantly, our analysis extends beyond sparse regression to data-driven predictors like neural networks, enhancing the reliability of model-based deep learning. Our findings bridge the gap between established theory and the practical applicability of such debiased methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13666v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Frederik Hoppe, Claudio Mayrink Verdun, Hannah Laus, Felix Krahmer, Holger Rauhut</dc:creator>
    </item>
    <item>
      <title>Learning Risk Preferences in Markov Decision Processes: an Application to the Fourth Down Decision in the National Football League</title>
      <link>https://arxiv.org/abs/2309.00756</link>
      <description>arXiv:2309.00756v2 Announce Type: replace 
Abstract: For decades, National Football League (NFL) coaches' observed fourth down decisions have been largely inconsistent with prescriptions based on statistical models. In this paper, we develop a framework to explain this discrepancy using an inverse optimization approach. We model the fourth down decision and the subsequent sequence of plays in a game as a Markov decision process (MDP), the dynamics of which we estimate from NFL play-by-play data from the 2014 through 2022 seasons. We assume that coaches' observed decisions are optimal but that the risk preferences governing their decisions are unknown. This yields an inverse decision problem for which the optimality criterion, or risk measure, of the MDP is the estimand. Using the quantile function to parameterize risk, we estimate which quantile-optimal policy yields the coaches' observed decisions as minimally suboptimal. In general, we find that coaches' fourth-down behavior is consistent with optimizing low quantiles of the next-state value distribution, which corresponds to conservative risk preferences. We also find that coaches exhibit higher risk tolerances when making decisions in the opponent's half of the field as opposed to their own half, and that league average fourth down risk tolerances have increased over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00756v2</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Sandholtz, Lucas Wu, Martin Puterman, Timothy C. Y. Chan</dc:creator>
    </item>
    <item>
      <title>Hidden Markov models with an unknown number of states and a repulsive prior on the state parameters</title>
      <link>https://arxiv.org/abs/2407.10869</link>
      <description>arXiv:2407.10869v2 Announce Type: replace 
Abstract: Hidden Markov models (HMMs) offer a robust and efficient framework for analyzing time series data, modelling both the underlying latent state progression over time and the observation process, conditional on the latent state. However, a critical challenge lies in determining the appropriate number of underlying states, often unknown in practice. In this paper, we employ a Bayesian framework, treating the number of states as a random variable and employing reversible jump Markov chain Monte Carlo to sample from the posterior distributions of all parameters, including the number of states. Additionally, we introduce repulsive priors for the state parameters in HMMs, and hence avoid overfitting issues and promote parsimonious models with dissimilar state components. We perform an extensive simulation study comparing performance of models with independent and repulsive prior distributions on the state parameters, and demonstrate our proposed framework on two ecological case studies: GPS tracking data on muskox in Antarctica and acoustic data on Cape gannets in South Africa. Our results highlight how our framework effectively explores the model space, defined by models with different latent state dimensions, while leading to latent states that are distinguished better and hence are more interpretable, enabling better understanding of complex dynamic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10869v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Rotous, Alex Diana, Alessio Farcomeni, Eleni Matechou, Andr\'ea Thiebault</dc:creator>
    </item>
    <item>
      <title>Bayesian sequential design of computer experiments for quantile set inversion</title>
      <link>https://arxiv.org/abs/2211.01008</link>
      <description>arXiv:2211.01008v5 Announce Type: replace-cross 
Abstract: We consider an unknown multivariate function representing a system-such as a complex numerical simulator-taking both deterministic and uncertain inputs. Our objective is to estimate the set of deterministic inputs leading to outputs whose probability (with respect to the distribution of the uncertain inputs) of belonging to a given set is less than a given threshold. This problem, which we call Quantile Set Inversion (QSI), occurs for instance in the context of robust (reliability-based) optimization problems, when looking for the set of solutions that satisfy the constraints with sufficiently large probability.   To solve the QSI problem we propose a Bayesian strategy, based on Gaussian process modeling and the Stepwise Uncertainty Reduction (SUR) principle, to sequentially choose the points at which the function should be evaluated to efficiently approximate the set of interest. We illustrate the performance and interest of the proposed SUR strategy through several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01008v5</guid>
      <category>stat.ML</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Ait Abdelmalek-Lomenech (L2S, RT-UQ), Julien Bect (L2S, RT-UQ), Vincent Chabridon (EDF R\&amp;D PRISME, RT-UQ), Emmanuel Vazquez (L2S, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Dynamic survival analysis: modelling the hazard function via ordinary differential equations</title>
      <link>https://arxiv.org/abs/2308.05205</link>
      <description>arXiv:2308.05205v5 Announce Type: replace-cross 
Abstract: The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for parametrically modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over time. Our proposal capitalises on the extensive literature of ODEs which, in particular, allow for establishing basic rules or laws on the dynamics of the hazard function via the use of autonomous ODEs. We show how to implement the proposed modelling framework in cases where there is an analytic solution to the system of ODEs or where an ODE solver is required to obtain a numerical solution. We focus on the use of a Bayesian modelling approach, but the proposed methodology can also be coupled with maximum likelihood estimation. A simulation study is presented to illustrate the performance of these models and the interplay of sample size and censoring. Two case studies using real data are presented to illustrate the use of the proposed approach and to highlight the interpretability of the corresponding models. We conclude with a discussion on potential extensions of our work and strategies to include covariates into our framework. Although we focus on examples on Medical Statistics, the proposed framework is applicable in any context where the interest lies on estimating and interpreting the dynamics hazard function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05205v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. A. Christen, F. J. Rubio</dc:creator>
    </item>
    <item>
      <title>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</title>
      <link>https://arxiv.org/abs/2403.07657</link>
      <description>arXiv:2403.07657v2 Announce Type: replace-cross 
Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07657v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\"oster, Rif A. Saurous, Matthew Hoffman</dc:creator>
    </item>
    <item>
      <title>A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation</title>
      <link>https://arxiv.org/abs/2406.07320</link>
      <description>arXiv:2406.07320v2 Announce Type: replace-cross 
Abstract: Model performance evaluation is a critical and expensive task in machine learning and computer vision. Without clear guidelines, practitioners often estimate model accuracy using a one-time completely random selection of the data. However, by employing tailored sampling and estimation strategies, one can obtain more precise estimates and reduce annotation costs. In this paper, we propose a statistical framework for model evaluation that includes stratification, sampling, and estimation components. We examine the statistical properties of each component and evaluate their efficiency (precision). One key result of our work is that stratification via k-means clustering based on accurate predictions of model performance yields efficient estimators. Our experiments on computer vision datasets show that this method consistently provides more precise accuracy estimates than the traditional simple random sampling, even with substantial efficiency gains of 10x. We also find that model-assisted estimators, which leverage predictions of model accuracy on the unlabeled portion of the dataset, are generally more efficient than the traditional estimates based solely on the labeled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07320v2</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Fogliato, Pratik Patil, Mathew Monfort, Pietro Perona</dc:creator>
    </item>
  </channel>
</rss>
