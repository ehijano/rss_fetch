<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spatial Models for Crowdsourced Internet Access Network Performance Measurements</title>
      <link>https://arxiv.org/abs/2405.11138</link>
      <description>arXiv:2405.11138v1 Announce Type: new 
Abstract: Despite significant investments in access network infrastructure, universal access to high-quality Internet connectivity remains a challenge. Policymakers often rely on large-scale, crowdsourced measurement datasets to assess the distribution of access network performance across geographic areas. These decisions typically rest on the assumption that Internet performance is uniformly distributed within predefined social boundaries, such as zip codes, census tracts, or community areas. However, this assumption may not be valid for two reasons: (1) crowdsourced measurements often exhibit non-uniform sampling densities within geographic areas; and (2) predefined social boundaries may not align with the actual boundaries of Internet infrastructure.
  In this paper, we model Internet performance as a spatial process. We apply and evaluate a series of statistical techniques to: (1) aggregate Internet performance over a geographic region; (2) overlay interpolated maps with various sampling boundary choices; and (3) spatially cluster boundary units to identify areas with similar performance characteristics. We evaluated the effectiveness of these using a 17-month-long crowdsourced dataset from Ookla Speedtest. We evaluate several leading interpolation methods at varying spatial scales. Further, we examine the similarity between the resulting boundaries for smaller realizations of the dataset. Our findings suggest that our combination of techniques achieves a 56% gain in similarity score over traditional methods that rely on aggregates over raw measurement values for performance summarization. Our work highlights an urgent need for more sophisticated strategies in understanding and addressing Internet access disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11138v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Taveesh Sharma, Paul Schmitt, Francesco Bronzino, Nick Feamster, Nicole Marwell</dc:creator>
    </item>
    <item>
      <title>Hierarchical Reinforcement Learning Empowered Task Offloading in V2I Networks</title>
      <link>https://arxiv.org/abs/2405.11352</link>
      <description>arXiv:2405.11352v1 Announce Type: new 
Abstract: Edge computing plays an essential role in the vehicle-to-infrastructure (V2I) networks, where vehicles offload their intensive computation tasks to the road-side units for saving energy and reduce the latency. This paper designs the optimal task offloading policy to address the concerns involving processing delay, energy consumption and edge computing cost. Each computation task consisting of some interdependent sub-tasks is characterized as a directed acyclic graph (DAG). In such dynamic networks, a novel hierarchical Offloading scheme is proposed by leveraging deep reinforcement learning (DRL). The inter-dependencies among the DAGs of the computation tasks are extracted using a graph neural network with attention mechanism. A parameterized DRL algorithm is developed to deal with the hierarchical action space containing both discrete and continuous actions. Simulation results with a real-world car speed dataset demonstrate that the proposed scheme can effectively reduce the system overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11352v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu You, Haojie Yan, Yuedong Xu, Lifeng Wang, Liangui Dai</dc:creator>
    </item>
    <item>
      <title>Optimizing Layerwise Microservice Management in Heterogeneous Wireless Networks</title>
      <link>https://arxiv.org/abs/2405.11359</link>
      <description>arXiv:2405.11359v1 Announce Type: new 
Abstract: Small cells with edge computing are densely deployed in 5G mobile networks to provide high throughput communication and low-latency computation. The flexibility of edge computation is empowered by the deployment of lightweight container-based microservices. In this paper, we take the first step toward optimizing the microservice management in small-cell networks. The prominent feature is that each microservice consists of multiple image layers and different microservices may share some basic layers, thus bringing deep coupling in their placement and service provision. Our objective is to minimize the expected total latency of microservice requests under the storage, communication and computing constraints of the sparsely interconnected small cell nodes. We formulate a binary quadratic program (BQP) with the multi-dimensional strategy of the image layer placement, the access selection and the task assignment. The BQP problem is then transformed into an ILP problem, and is solved by use of a novel sphere-box alternating direction multipliers method (ADMM) with reasonable complexity $O(q^{4})$, where $q$ is the number of variables in the transformed problem. Trace-driven experiments show that the gap between our proposed algorithm and the optimal is reduced by 35$\%$ compared with benchmark algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11359v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haojie Yan, Yuedong Xu, Lianggui Dai</dc:creator>
    </item>
    <item>
      <title>Workload Prediction in P4 Programmable Switches: Smart Resource Scheduling</title>
      <link>https://arxiv.org/abs/2405.11408</link>
      <description>arXiv:2405.11408v1 Announce Type: new 
Abstract: The rapid expansion of cloud services and their unpredictable workload demands present significant challenges in resource management. Traditional resource management approaches, primarily based on static rules and thresholds, often fail to ensure cost-effectiveness and optimal resource utilization. This research introduces a predictive model designed to forecast traffic demand, aiming to shift from a reactive to a proactive resource management approach. By integrating advanced predictive analytics with the capabilities of P4 programmable switches, this study seeks to enhance the efficiency of resource utilization and improve system robustness. The goal is to equip organizations with the agility and economic efficiency required to navigate the complexities of dynamic cloud environments effectively. This approach not only promises to refine microservice resource allocation but also supports the broader objective of fostering more resilient and efficient cloud infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11408v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boyang Yan</dc:creator>
    </item>
    <item>
      <title>Machine Learning &amp; Wi-Fi: Unveiling the Path Towards AI/ML-Native IEEE 802.11 Networks</title>
      <link>https://arxiv.org/abs/2405.11504</link>
      <description>arXiv:2405.11504v1 Announce Type: new 
Abstract: Artificial intelligence (AI) and machine learning (ML) are nowadays mature technologies considered essential for driving the evolution of future communications systems. Simultaneously, Wi-Fi technology has constantly evolved over the past three decades and incorporated new features generation after generation, thus gaining in complexity. As such, researchers have observed that AI/ML functionalities may be required to address the upcoming Wi-Fi challenges that will be otherwise difficult to solve with traditional approaches. This paper discusses the role of AI/ML in current and future Wi-Fi networks and depicts the ways forward. A roadmap towards AI/ML-native Wi-Fi, key challenges, standardization efforts, and major enablers are also discussed. An exemplary use case is provided to showcase the potential of AI/ML in Wi-Fi at different adoption stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11504v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesc Wilhelmi, Szymon Szott, Katarzyna Kosek-Szott, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>Optimizing Underwater IoT Routing with Multi-Criteria Decision Making and Uncertainty Weights</title>
      <link>https://arxiv.org/abs/2405.11513</link>
      <description>arXiv:2405.11513v1 Announce Type: new 
Abstract: Effective data routing is vital in the Internet of Things (IoT) paradigm, especially in underwater mobile sensor networks where inefficiency can lead to significant resource consumption. This article presents an innovative method designed to enhance network performance and reduce resource usage, while also accurately determining component weights in these networks, ensuring quality service. Building upon previous research on multi-criteria decision-making systems in coastal RPL networks, our method involves key adaptations for underwater environments. It integrates comprehensive network features to identify the optimal parent node for each sensor, employing the fuzzy SWARA decision-making approach under uncertain conditions. This method takes into account various factors including hops, energy, ARSSI rate, delay, ETX, link delivery rate, and depth to determine the most effective parent node assignment. Through simulation, our approach demonstrates marked improvements in network performance compared to existing solutions. These advancements are significant, offering a new direction in enhancing underwater IoT communications and suggesting wider applications for IoT systems facing similar challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11513v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ali Karkehabadi, Mitra Bakhshi, Seyed Behnam Razavian</dc:creator>
    </item>
    <item>
      <title>PET: Multi-agent Independent PPO-based Automatic ECN Tuning for High-Speed Data Center Networks</title>
      <link>https://arxiv.org/abs/2405.11956</link>
      <description>arXiv:2405.11956v1 Announce Type: new 
Abstract: Explicit Congestion Notification (ECN)-based congestion control schemes have been widely adopted in high-speed data center networks (DCNs), where the ECN marking threshold plays a determinant role in guaranteeing a packet lossless DCN. However, existing approaches either employ static settings with immutable thresholds that cannot be dynamically self-adjusted to adapt to network dynamics, or fail to take into account many-to-one traffic patterns and different requirements of different types of traffic, resulting in relatively poor performance. To address these problems, this paper proposes a novel learning-based automatic ECN tuning scheme, named PET, based on the multi-agent Independent Proximal Policy Optimization (IPPO) algorithm. PET dynamically adjusts ECN thresholds by fully considering pivotal congestion-contributing factors, including queue length, output data rate, output rate of ECN-marked packets, current ECN threshold, the extent of incast, and the ratio of mice and elephant flows. PET adopts the Decentralized Training and Decentralized Execution (DTDE) paradigm and combines offline and online training to accommodate network dynamics. PET is also fair and readily deployable with commodity hardware. Comprehensive experimental results demonstrate that, compared with state-of-the-art static schemes and the learning-based automatic scheme, our PET achieves better performance in terms of flow completion time, convergence rate, queue length variance, and system robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11956v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Cheng, Ting Wang, Xiao Du, Shuyi Du, Haibin Cai</dc:creator>
    </item>
    <item>
      <title>DarkDNS: Revisiting the Value of Rapid Zone Update</title>
      <link>https://arxiv.org/abs/2405.12010</link>
      <description>arXiv:2405.12010v1 Announce Type: new 
Abstract: Malicious actors exploit the DNS namespace to launch spam campaigns, phishing attacks, malware, and other harmful activities. Combating these threats requires visibility into domain existence, ownership and nameservice activity that the DNS protocol does not itself provide. To facilitate visibility and security-related study of the expanding gTLD namespace, ICANN introduced the Centralized Zone Data Service (CZDS) that shares daily zone file snapshots of new gTLD zones. However, a remarkably high concentration of malicious activity is associated with domains that do not live long enough make it into these daily snapshots. Using public and private sources of newly observed domains to identify this activity, we discover that even with the best available data there is a considerable visibility gap. We find that the daily snapshots miss at least 1% of newly registered and short-lived domains, which are almost always registered with malicious intent. In reducing this critical visibility gap using public sources of data, we demonstrate how more timely access to TLD zone changes can help better prevent abuse. We hope that this work sparks a discussion in the community on how to effectively and safely revive the concept of sharing Rapid Zone Updates for security research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12010v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raffaele Sommese, Gautam Akiwate, Antonia Affinito, Moritz Muller, Mattijs Jonker, KC Claffy</dc:creator>
    </item>
    <item>
      <title>Sustainable business decision modelling with blockchain and digital twins: A survey</title>
      <link>https://arxiv.org/abs/2405.12101</link>
      <description>arXiv:2405.12101v1 Announce Type: new 
Abstract: Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions. BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios. Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability. Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure. These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts. To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research. Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads. Several research questions are put forward to motivate further research that significantly impacts BDM. Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12101v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyan Wickremasinghe, Siofra Frost, Karen Rafferty, Vishal Sharma</dc:creator>
    </item>
    <item>
      <title>FeMLoc: Federated Meta-learning for Adaptive Wireless Indoor Localization Tasks in IoT Networks</title>
      <link>https://arxiv.org/abs/2405.11079</link>
      <description>arXiv:2405.11079v1 Announce Type: cross 
Abstract: The rapid growth of the Internet of Things fosters collaboration among connected devices for tasks like indoor localization. However, existing indoor localization solutions struggle with dynamic and harsh conditions, requiring extensive data collection and environment-specific calibration. These factors impede cooperation, scalability, and the utilization of prior research efforts. To address these challenges, we propose FeMLoc, a federated meta-learning framework for localization. FeMLoc operates in two stages: (i) collaborative meta-training where a global meta-model is created by training on diverse localization datasets from edge devices. (ii) Rapid adaptation for new environments, where the pre-trained global meta-model initializes the localization model, requiring only minimal fine-tuning with a small amount of new data. In this paper, we provide a detailed technical overview of FeMLoc, highlighting its unique approach to privacy-preserving meta-learning in the context of indoor localization. Our performance evaluation demonstrates the superiority of FeMLoc over state-of-the-art methods, enabling swift adaptation to new indoor environments with reduced calibration effort. Specifically, FeMLoc achieves up to 80.95% improvement in localization accuracy compared to the conventional baseline neural network (NN) approach after only 100 gradient steps. Alternatively, for a target accuracy of around 5m, FeMLoc achieves the same level of accuracy up to 82.21% faster than the baseline NN approach. This translates to FeMLoc requiring fewer training iterations, thereby significantly reducing fingerprint data collection and calibration efforts. Moreover, FeMLoc exhibits enhanced scalability, making it well-suited for location-aware massive connectivity driven by emerging wireless communication technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11079v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaya Etiabi, Wafa Njima, El Mehdi Amhoud</dc:creator>
    </item>
    <item>
      <title>Generalized Multi-Objective Reinforcement Learning with Envelope Updates in URLLC-enabled Vehicular Networks</title>
      <link>https://arxiv.org/abs/2405.11331</link>
      <description>arXiv:2405.11331v1 Announce Type: cross 
Abstract: We develop a novel multi-objective reinforcement learning (MORL) framework to jointly optimize wireless network selection and autonomous driving policies in a multi-band vehicular network operating on conventional sub-6GHz spectrum and Terahertz frequencies. The proposed framework is designed to 1. maximize the traffic flow and 2. minimize collisions by controlling the vehicle's motion dynamics (i.e., speed and acceleration), and enhance the ultra-reliable low-latency communication (URLLC) while minimizing handoffs (HOs). We cast this problem as a multi-objective Markov Decision Process (MOMDP) and develop solutions for both predefined and unknown preferences of the conflicting objectives. Specifically, deep-Q-network and double deep-Q-network-based solutions are developed first that consider scalarizing the transportation and telecommunication rewards using predefined preferences. We then develop a novel envelope MORL solution which develop policies that address multiple objectives with unknown preferences to the agent. While this approach reduces reliance on scalar rewards, policy effectiveness varying with different preferences is a challenge. To address this, we apply a generalized version of the Bellman equation and optimize the convex envelope of multi-objective Q values to learn a unified parametric representation capable of generating optimal policies across all possible preference configurations. Following an initial learning phase, our agent can execute optimal policies under any specified preference or infer preferences from minimal data samples.Numerical results validate the efficacy of the envelope-based MORL solution and demonstrate interesting insights related to the inter-dependency of vehicle motion dynamics, HOs, and the communication data rate. The proposed policies enable autonomous vehicles to adopt safe driving behaviors with improved connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11331v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zijiang Yan, Hina Tabassum</dc:creator>
    </item>
    <item>
      <title>Quantum Network Tomography</title>
      <link>https://arxiv.org/abs/2405.11396</link>
      <description>arXiv:2405.11396v1 Announce Type: cross 
Abstract: Errors are the fundamental barrier to the development of quantum systems. Quantum networks are complex systems formed by the interconnection of multiple components and suffer from error accumulation. Characterizing errors introduced by quantum network components becomes a fundamental task to overcome their depleting effects in quantum communication. Quantum Network Tomography (QNT) addresses end-to-end characterization of link errors in quantum networks. It is a tool for building error-aware applications, network management, and system validation. We provide an overview of QNT and its initial results for characterizing quantum star networks. We apply a previously defined QNT protocol for estimating bit-flip channels to estimate depolarizing channels. We analyze the performance of our estimators numerically by assessing the Quantum Cram\`er-Rao Bound (QCRB) and the Mean Square Error (MSE) in the finite sample regime. Finally, we provide a discussion on current challenges in the field of QNT and elicit exciting research directions for future investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11396v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matheus Guedes de Andrade, Jake Navas, Saikat Guha, In\`es Monta\~no, Michael Raymer, Brian Smith, Don Towsley</dc:creator>
    </item>
    <item>
      <title>A GAN-Based Data Poisoning Attack Against Federated Learning Systems and Its Countermeasure</title>
      <link>https://arxiv.org/abs/2405.11440</link>
      <description>arXiv:2405.11440v1 Announce Type: cross 
Abstract: As a distributed machine learning paradigm, federated learning (FL) is collaboratively carried out on privately owned datasets but without direct data access. Although the original intention is to allay data privacy concerns, "available but not visible" data in FL potentially brings new security threats, particularly poisoning attacks that target such "not visible" local data. Initial attempts have been made to conduct data poisoning attacks against FL systems, but cannot be fully successful due to their high chance of causing statistical anomalies. To unleash the potential for truly "invisible" attacks and build a more deterrent threat model, in this paper, a new data poisoning attack model named VagueGAN is proposed, which can generate seemingly legitimate but noisy poisoned data by untraditionally taking advantage of generative adversarial network (GAN) variants. Capable of manipulating the quality of poisoned data on demand, VagueGAN enables to trade-off attack effectiveness and stealthiness. Furthermore, a cost-effective countermeasure named Model Consistency-Based Defense (MCD) is proposed to identify GAN-poisoned data or models after finding out the consistency of GAN outputs. Extensive experiments on multiple datasets indicate that our attack method is generally much more stealthy as well as more effective in degrading FL performance with low complexity. Our defense method is also shown to be more competent in identifying GAN-poisoned data or models. The source codes are publicly available at \href{https://github.com/SSssWEIssSS/VagueGAN-Data-Poisoning-Attack-and-Its-Countermeasure}{https://github.com/SSssWEIssSS/VagueGAN-Data-Poisoning-Attack-and-Its-Countermeasure}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11440v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Sun, Bo Gao, Ke Xiong, Yuwei Wang, Pingyi Fan, Khaled Ben Letaief</dc:creator>
    </item>
    <item>
      <title>NetMamba: Efficient Network Traffic Classification via Pre-training Unidirectional Mamba</title>
      <link>https://arxiv.org/abs/2405.11449</link>
      <description>arXiv:2405.11449v1 Announce Type: cross 
Abstract: Network traffic classification is a crucial research area aiming to enhance service quality, streamline network management, and bolster cybersecurity. To address the growing complexity of transmission encryption techniques, various machine learning and deep learning methods have been proposed. However, existing approaches encounter two main challenges. Firstly, they struggle with model inefficiency due to the quadratic complexity of the widely used Transformer architecture. Secondly, they suffer from unreliable traffic representation because of discarding important byte information while retaining unwanted biases. To address these challenges, we propose NetMamba, an efficient linear-time state space model equipped with a comprehensive traffic representation scheme. We replace the Transformer with our specially selected and improved Mamba architecture for the networking field to address efficiency issues. In addition, we design a scheme for traffic representation, which is used to extract valid information from massive traffic while removing biased information. Evaluation experiments on six public datasets encompassing three main classification tasks showcase NetMamba's superior classification performance compared to state-of-the-art baselines. It achieves up to 4.83\% higher accuracy and 4.64\% higher f1 score on encrypted traffic classification tasks. Additionally, NetMamba demonstrates excellent efficiency, improving inference speed by 2.24 times while maintaining comparably low memory usage. Furthermore, NetMamba exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. To the best of our knowledge, NetMamba is the first model to tailor the Mamba architecture for networking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11449v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongze Wang, Xiaohui Xie, Wenduo Wang, Chuyi Wang, Youjian Zhao, Yong Cui</dc:creator>
    </item>
    <item>
      <title>EdgeLoc: A Communication-Adaptive Parallel System for Real-Time Localization in Infrastructure-Assisted Autonomous Driving</title>
      <link>https://arxiv.org/abs/2405.12120</link>
      <description>arXiv:2405.12120v1 Announce Type: cross 
Abstract: This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches. The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches. The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry. EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution. It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection. Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making. Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\% reduction in the localization error for real-time local visual odometry, a 29.95\% reduction for non-real-time collaborative inference, and a 30.26\% reduction compared to Kalman filtering. Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network. The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12120v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Boyi Liu, Jingwen Tong, Yufan Zhuang, Jiawei Shao, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Teal: Learning-Accelerated Optimization of WAN Traffic Engineering</title>
      <link>https://arxiv.org/abs/2210.13763</link>
      <description>arXiv:2210.13763v4 Announce Type: replace 
Abstract: The rapid expansion of global cloud wide-area networks (WANs) has posed a challenge for commercial optimization engines to efficiently solve network traffic engineering (TE) problems at scale. Existing acceleration strategies decompose TE optimization into concurrent subproblems but realize limited parallelism due to an inherent tradeoff between run time and allocation performance.
  We present Teal, a learning-based TE algorithm that leverages the parallel processing power of GPUs to accelerate TE control. First, Teal designs a flow-centric graph neural network (GNN) to capture WAN connectivity and network flows, learning flow features as inputs to downstream allocation. Second, to reduce the problem scale and make learning tractable, Teal employs a multi-agent reinforcement learning (RL) algorithm to independently allocate each traffic demand while optimizing a central TE objective. Finally, Teal fine-tunes allocations with ADMM (Alternating Direction Method of Multipliers), a highly parallelizable optimization algorithm for reducing constraint violations such as overutilized links.
  We evaluate Teal using traffic matrices from Microsoft's WAN. On a large WAN topology with &gt;1,700 nodes, Teal generates near-optimal flow allocations while running several orders of magnitude faster than the production optimization engine. Compared with other TE acceleration schemes, Teal satisfies 6--32% more traffic demand and yields 197--625x speedups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.13763v4</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiying Xu, Francis Y. Yan, Rachee Singh, Justin T. Chiu, Alexander M. Rush, Minlan Yu</dc:creator>
    </item>
    <item>
      <title>TrimCaching: Parameter-sharing AI Model Caching in Wireless Edge Networks</title>
      <link>https://arxiv.org/abs/2405.03990</link>
      <description>arXiv:2405.03990v2 Announce Type: replace 
Abstract: Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching. In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with $\left(1-\epsilon\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03990v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanqiao Qu, Zheng Lin, Fangming Liu, Xianhao Chen, Kaibin Huang</dc:creator>
    </item>
    <item>
      <title>On the Bipartite Entanglement Capacity of Quantum Networks</title>
      <link>https://arxiv.org/abs/2307.04477</link>
      <description>arXiv:2307.04477v2 Announce Type: replace-cross 
Abstract: We consider the problem of multi-path entanglement distribution to a pair of nodes in a quantum network consisting of devices with non-deterministic entanglement swapping capabilities. Multi-path entanglement distribution enables a network to establish end-to-end entangled links across any number of available paths with pre-established link-level entanglement. Probabilistic entanglement swapping, on the other hand, limits the amount of entanglement that is shared between the nodes; this is especially the case when, due to architectural and other practical constraints, swaps must be performed in temporal proximity to each other. Limiting our focus to the case where only bipartite entangled states are generated across the network, we cast the problem as an instance of generalized flow maximization between two quantum end nodes wishing to communicate. We propose a mixed-integer quadratically constrained program (MIQCP) to solve this flow problem for networks with arbitrary topology. We then compute the overall network capacity, defined as the maximum number of EPR states distributed to users per time unit, by solving the flow problem for all possible network states generated by probabilistic entangled link presence and absence, and subsequently by averaging over all network state capacities. The MIQCP can also be applied to networks with multiplexed links. While our approach for computing the overall network capacity has the undesirable property that the total number of states grows exponentially with link multiplexing capability, it nevertheless yields an exact solution that serves as an upper bound comparison basis for the throughput performance of easily-implementable yet non-optimal entanglement routing algorithms. We apply our capacity computation method to several networks, including a topology based on SURFnet -- a backbone network used for research purposes in the Netherlands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04477v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/tqe.2024.3366696</arxiv:DOI>
      <dc:creator>Gayane Vardoyan, Emily van Milligen, Saikat Guha, Stephanie Wehner, Don Towsley</dc:creator>
    </item>
    <item>
      <title>Multipartite Entanglement Distribution in Quantum Networks using Subgraph Complementations</title>
      <link>https://arxiv.org/abs/2308.13700</link>
      <description>arXiv:2308.13700v4 Announce Type: replace-cross 
Abstract: Quantum networks are important for quantum communication and allow for several tasks such as quantum teleportation, quantum key distribution, quantum sensing, and quantum error correction. Graph states are a specific class of multipartite entangled states that can be represented by graphs. We propose a novel approach for distributing graph states across a quantum network. We show that the distribution of graph states can be characterized by a system of subgraph complementations, which we also relate to the minimum rank of the underlying graph and the degree of entanglement quantified by the Schmidt-rank of the quantum state. We analyze resource usage for our algorithm and show that it improves on the number of qubits, bits for classical communication, and EPR pairs utilized, as compared to prior work. In fact, the number of local operations and resource consumption for our approach scales linearly in the number of vertices. This produces a quadratic improvement in completion time for several classes of graph states represented by dense graphs, which translates into an exponential improvement by allowing parallelization of gate operations. This leads to improved fidelities in the presence of noisy operations, as we show through simulation in the presence of noisy operations. Common classes of graph states are classified along with their optimal distribution time using subgraph complementations. We find a close to optimal sequence of subgraph complementation operations to distribute an arbitrary graph state, and establish upper bounds on distribution time along with providing approximate greedy algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13700v4</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aniruddha Sen, Kenneth Goodenough, Don Towsley</dc:creator>
    </item>
    <item>
      <title>When to Preempt in a Status Update System?</title>
      <link>https://arxiv.org/abs/2402.00845</link>
      <description>arXiv:2402.00845v2 Announce Type: replace-cross 
Abstract: We consider a time-slotted status update system with an error-free preemptive queue. The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor. The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server. We formulate this problem as a Markov decision process (MDP) and find the optimal sampling policy. We find a sufficient, and also separately a necessary, condition for the always preemption policy to be an optimal policy. We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor. We propose a double-threshold sampling policy which we show to be an optimal policy under some assumptions on the queue statistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00845v2</guid>
      <category>cs.IT</category>
      <category>cs.GT</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhankar Banerjee, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>MADRL-Based Rate Adaptation for 360{\deg} Video Streaming with Multi-Viewpoint Prediction</title>
      <link>https://arxiv.org/abs/2405.07759</link>
      <description>arXiv:2405.07759v2 Announce Type: replace-cross 
Abstract: Over the last few years, 360{\deg} video traffic on the network has grown significantly. A key challenge of 360{\deg} video playback is ensuring a high quality of experience (QoE) with limited network bandwidth. Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption. However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well. This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory. The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction. After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360{\deg} video streaming is proposed for maximizing different QoE objectives under various network conditions. We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem. The experimental results show that our proposed method improves the defined QoE metric by up to 85.5% compared to existing ABR methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07759v2</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2024.3398548</arxiv:DOI>
      <dc:creator>Haopeng Wang, Zijian Long, Haiwei Dong, Abdulmotaleb El Saddik</dc:creator>
    </item>
  </channel>
</rss>
