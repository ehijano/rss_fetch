<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Nov 2025 03:51:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>BayesChange: an R package for Bayesian Change Point Analysis</title>
      <link>https://arxiv.org/abs/2511.04785</link>
      <description>arXiv:2511.04785v1 Announce Type: new 
Abstract: We introduce BayesChange, a computationally efficient R package, built on C++, for Bayesian change point detection and clustering of observations sharing common change points. While many R packages exist for change point analysis, BayesChange offers methods not currently available elsewhere. The core functions are implemented in C++ to ensures computational efficiency, while an R user interface simplifies the package usage. The BayesChange package includes two R wrappers that integrate the C++ backend functions, along with S3 methods for summarizing the results. We present the theory beyond each method, the algorithms for posterior simulation and we illustrate the package's usage through synthetic examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04785v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Danese, Riccardo Corradin, Andrea Ongaro</dc:creator>
    </item>
    <item>
      <title>Do we Need Dozens of Methods for Real World Missing Value Imputation?</title>
      <link>https://arxiv.org/abs/2511.04833</link>
      <description>arXiv:2511.04833v1 Announce Type: new 
Abstract: Missing values pose a persistent challenge in modern data science. Consequently, there is an ever-growing number of publications introducing new imputation methods in various fields. While many studies compare imputation approaches, they often focus on a limited subset of algorithms and evaluate performance primarily through pointwise metrics such as RMSE, which are not suitable to measure the preservation of the true data distribution. In this work, we provide a systematic benchmarking method based on the idea of treating imputation as a distributional prediction task. We consider a large number of algorithms and, for the first time, evaluate them not only on synthetic missing mechanisms, but also on real-world missingness scenarios, using the concept of Imputation Scores. Finally, while the focus of previous benchmark has often been on numerical data, we also consider mixed data sets in our study. The analysis overwhelmingly confirms the superiority of iterative imputation algorithms, especially the methods implemented in the mice R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04833v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krystyna Grzesiak, Christophe Muller, Julie Josse, Jeffrey N\"af</dc:creator>
    </item>
    <item>
      <title>Sequential Markov chain Monte Carlo for Filtering of State-Space Models with Low or Degenerate Observation Noise</title>
      <link>https://arxiv.org/abs/2511.04975</link>
      <description>arXiv:2511.04975v1 Announce Type: new 
Abstract: We consider the discrete-time filtering problem in scenarios where the observation noise is degenerate or low. More precisely, one is given access to a discrete time observation sequence which at any time $k$ depends only on the state of an unobserved Markov chain. We specifically assume that the functional relationship between observations and hidden Markov chain has either degenerate or low noise. In this article, under suitable assumptions, we derive the filtering density and its recursions for this class of problems on a specific sequence of manifolds defined through the observation function. We then design sequential Markov chain Monte Carlo methods to approximate the filter serially in time. For a certain linear observation model, we show that using sequential Markov chain Monte Carlo for low noise will converge as the noise disappears to that of using sequential Markov chain Monte Carlo for degenerate noise. We illustrate the performance of our methodology on several challenging stochastic models deriving from Statistics and Applied Mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04975v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abylay Zhumekenov (School of Data Science, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China), Alexandros Beskos (Department of Statistical Science, University College London, London, United Kingdom), Dan Crisan (Department of Mathematics, Imperial College London, London, United Kingdom), Matthew Graham (Advanced Research Computing Centre, University College London, London, United Kingdom), Ajay Jasra (School of Data Science, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China), Nikolas Kantas (Department of Mathematics, Imperial College London, London, United Kingdom)</dc:creator>
    </item>
    <item>
      <title>Inference for the Extended Functional Cox Model: A UK Biobank Case Study</title>
      <link>https://arxiv.org/abs/2511.04852</link>
      <description>arXiv:2511.04852v1 Announce Type: cross 
Abstract: Multiple studies have shown that scalar summaries of objectively measured physical activity (PA) using accelerometers are the strongest predictors of mortality, outperforming all traditional risk factors, including age, sex, body mass index (BMI), and smoking. Here we show that diurnal patterns of PA and their day-to-day variability provide additional information about mortality. To do that, we introduce a class of extended functional Cox models and corresponding inferential tools designed to quantify the association between multiple functional and scalar predictors with time-to-event outcomes in large-scale (large $n$) high-dimensional (large $p$) datasets. Methods are applied to the UK Biobank study, which collected PA at every minute of the day for up to seven days, as well as time to mortality ($93{,}370$ participants with good quality accelerometry data and $931$ events). Simulation studies show that methods perform well in realistic scenarios and scale up to studies an order of magnitude larger than the UK Biobank accelerometry study. Establishing the feasibility and scalability of these methods for such complex and large data sets is a major milestone in applied Functional Data Analysis (FDA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04852v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erjia Cui, Angela Zhao, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Clustering in Networks with Time-varying Nodal Attributes</title>
      <link>https://arxiv.org/abs/2511.04859</link>
      <description>arXiv:2511.04859v1 Announce Type: cross 
Abstract: This manuscript studies nodal clustering in graphs having a time series at each node. The framework includes priors for low-dimensional representations and a decoder that bridges the latent representations and time series. The structural and temporal patterns are fused into representations that facilitate clustering, addressing the limitation that the evolution of nodal attributes is often overlooked. Parameters are learned via maximum approximate likelihood, with a graph-fused LASSO regularization imposed on prior parameters. The optimization problem is solved via alternating direction method of multipliers; Langevin dynamics are employed for posterior inference. Simulation studies on block and grid graphs with autoregressive dynamics, and applications to California county temperatures and a book word co-occurrence network demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04859v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Oscar Hernan Madrid Padilla, Rebecca Killick, James Wilson, Xi Chen, Robert Lund</dc:creator>
    </item>
    <item>
      <title>Scaling Up ROC-Optimizing Support Vector Machines</title>
      <link>https://arxiv.org/abs/2511.04979</link>
      <description>arXiv:2511.04979v1 Announce Type: cross 
Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the area under the ROC curve (AUC) and has become an attractive alternative of the conventional binary classification under the presence of class imbalance. However, its practical use is limited by high computational cost, as training involves evaluating all $O(n^2)$. To overcome this limitation, we develop a scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby substantially reducing computational complexity. We further extend the framework to nonlinear classification through a low-rank kernel approximation, enabling efficient training in reproducing kernel Hilbert spaces. Theoretical analysis establishes an error bound that justifies the proposed approximation, and empirical results on both synthetic and real datasets demonstrate that the proposed method achieves comparable AUC performance to the original ROC-SVM with drastically reduced training time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04979v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gimun Bae (Department of Statistics, Korea University, Seoul, Republic of Korea), Seung Jun Shin (Department of Statistics, Korea University, Seoul, Republic of Korea)</dc:creator>
    </item>
    <item>
      <title>Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles</title>
      <link>https://arxiv.org/abs/2507.01542</link>
      <description>arXiv:2507.01542v2 Announce Type: replace-cross 
Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning, particularly for unsupervised problems. While full GMMs suffer from the overparameterization of their covariance matrices in high-dimensional spaces, spherical GMMs (with isotropic covariance matrices) certainly lack flexibility to fit certain anisotropic distributions. Connecting these two extremes, we introduce a new family of parsimonious GMMs with piecewise-constant covariance eigenvalue profiles. These extend several low-rank models like the celebrated mixtures of probabilistic principal component analyzers (MPPCA), by enabling any possible sequence of eigenvalue multiplicities. If the latter are prespecified, then we can naturally derive an expectation-maximization (EM) algorithm to learn the mixture parameters. Otherwise, to address the notoriously-challenging issue of jointly learning the mixture parameters and hyperparameters, we propose a componentwise penalized EM algorithm, whose monotonicity is proven. We show the superior likelihood-parsimony tradeoffs achieved by our models on a variety of unsupervised experiments: density fitting, clustering and single-image denoising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01542v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Pierre-Alexandre Mattei, Charles Bouveyron, Xavier Pennec</dc:creator>
    </item>
  </channel>
</rss>
