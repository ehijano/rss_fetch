<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2024 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Student t-L\'evy regression model in YUIMA</title>
      <link>https://arxiv.org/abs/2403.12078</link>
      <description>arXiv:2403.12078v1 Announce Type: new 
Abstract: The aim of this paper is to discuss an estimation and a simulation method in the \textsf{R} package YUIMA for a linear regression model driven by a Student-$t$ L\'evy process with constant scale and arbitrary degrees of freedom. This process finds applications in several fields, for example finance, physic, biology, etc. The model presents two main issues. The first is related to the simulation of a sample path at high-frequency level. Indeed, only the $t$-L\'evy increments defined on an unitary time interval are Student-$t$ distributed. In YUIMA, we solve this problem by means of the inverse Fourier transform for simulating the increments of a Student-$t$ L\'{e}vy defined on a interval with any length. A second problem is due to the fact that joint estimation of trend, scale, and degrees of freedom does not seem to have been investigated as yet. In YUIMA, we develop a two-step estimation procedure that efficiently deals with this issue. Numerical examples are given in order to explain methods and classes used in the YUIMA package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12078v1</guid>
      <category>stat.CO</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Masuda, Lorenzo Mercuri, Yuma Uehara</dc:creator>
    </item>
    <item>
      <title>An Alternative Graphical Lasso Algorithm for Precision Matrices</title>
      <link>https://arxiv.org/abs/2403.12357</link>
      <description>arXiv:2403.12357v1 Announce Type: new 
Abstract: The Graphical Lasso (GLasso) algorithm is fast and widely used for estimating sparse precision matrices (Friedman et al., 2008). Its central role in the literature of high-dimensional covariance estimation rivals that of Lasso regression for sparse estimation of the mean vector. Some mysteries regarding its optimization target, convergence, positive-definiteness and performance have been unearthed, resolved and presented in Mazumder and Hastie (2011), leading to a new/improved (dual-primal) DP-GLasso. Using a new and slightly different reparametriztion of the last column of a precision matrix we show that the regularized normal log-likelihood naturally decouples into a sum of two easy to minimize convex functions one of which is a Lasso regression problem. This decomposition is the key in developing a transparent, simple iterative block coordinate descent algorithm for computing the GLasso updates with performance comparable to DP-GLasso. In particular, our algorithm has the precision matrix as its optimization target right at the outset, and retains all the favorable properties of the DP-GLasso algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12357v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aramayis Dallakyan, Mohsen Pourahmadi</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v1 Announce Type: cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
    <item>
      <title>Useful Compact Representations for Data-Fitting</title>
      <link>https://arxiv.org/abs/2403.12206</link>
      <description>arXiv:2403.12206v1 Announce Type: cross 
Abstract: For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12206v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes J. Brust</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization Sequential Surrogate (BOSS) Algorithm: Fast Bayesian Inference for a Broad Class of Bayesian Hierarchical Models</title>
      <link>https://arxiv.org/abs/2403.12250</link>
      <description>arXiv:2403.12250v1 Announce Type: cross 
Abstract: Approximate Bayesian inference based on Laplace approximation and quadrature methods have become increasingly popular for their efficiency at fitting latent Gaussian models (LGM), which encompass popular models such as Bayesian generalized linear models, survival models, and spatio-temporal models. However, many useful models fall under the LGM framework only if some conditioning parameters are fixed, as the design matrix would vary with these parameters otherwise. Such models are termed the conditional LGMs with examples in change-point detection, non-linear regression, etc. Existing methods for fitting conditional LGMs rely on grid search or Markov-chain Monte Carlo (MCMC); both require a large number of evaluations of the unnormalized posterior density of the conditioning parameters. As each evaluation of the density requires fitting a separate LGM, these methods become computationally prohibitive beyond simple scenarios. In this work, we introduce the Bayesian optimization sequential surrogate (BOSS) algorithm, which combines Bayesian optimization with approximate Bayesian inference methods to significantly reduce the computational resources required for fitting conditional LGMs. With orders of magnitude fewer evaluations compared to grid or MCMC methods, Bayesian optimization provides us with sequential design points that capture the majority of the posterior mass of the conditioning parameters, which subsequently yields an accurate surrogate posterior distribution that can be easily normalized. We illustrate the efficiency, accuracy, and practical utility of the proposed method through extensive simulation studies and real-world applications in epidemiology, environmental sciences, and astrophysics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12250v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dayi Li, Ziang Zhang</dc:creator>
    </item>
    <item>
      <title>On the use of the cumulant generating function for inference on time series</title>
      <link>https://arxiv.org/abs/2403.12714</link>
      <description>arXiv:2403.12714v1 Announce Type: cross 
Abstract: We introduce innovative inference procedures for analyzing time series data. Our methodology enables density approximation and composite hypothesis testing based on Whittle's estimator, a widely applied M-estimator in the frequency domain. Its core feature involves the (general Legendre transform of the) cumulant generating function of the Whittle likelihood score, as obtained using an approximated distribution of the periodogram ordinates. We present a testing algorithm that significantly expands the applicability of the state-of-the-art saddlepoint test, while maintaining the numerical accuracy of the saddlepoint approximation. Additionally, we demonstrate connections between our findings and three other prevalent frequency domain approaches: the bootstrap, empirical likelihood, and exponential tilting. Numerical examples using both simulated and real data illustrate the advantages and accuracy of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12714v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alban Moor, Davide La Vecchia, Elvezio Ronchetti</dc:creator>
    </item>
    <item>
      <title>Bayesian uncertainty evaluation applied to the tilted-wave interferometer</title>
      <link>https://arxiv.org/abs/2403.12715</link>
      <description>arXiv:2403.12715v1 Announce Type: cross 
Abstract: The tilted-wave interferometer is a promising technique for the development of a reference measurement system for the highly accurate form measurement of aspheres and freeform surfaces. The technique combines interferometric measurements, acquired with a special setup, and sophisticated mathematical evaluation procedures. To determine the form of the surface under test, a computational model is required that closely mimics the measurement process of the physical measurement instruments. The parameters of the computational model, comprising the surface under test sought, are then tuned by solving an inverse problem. Due to this embedded structure of the real experiment and computational model and the overall complexity, a thorough uncertainty evaluation is challenging. In this work, a Bayesian approach is proposed to tackle the inverse problem, based on a statistical model derived from the computational model of the tilted-wave interferometer. Such a procedure naturally allows for uncertainty quantification to be made. We present an approximate inference scheme to efficiently sample quantities of the posterior using Monte Carlo sampling involving the statistical model. In particular, the methodology derived is applied to the tilted-wave interferometer to obtain an estimate and corresponding uncertainty of the pixel-by-pixel form of the surface under test for two typical surfaces taking into account a number of key influencing factors. A statistical analysis using experimental design is employed to identify main influencing factors and a subsequent analysis confirms the efficacy of the method derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12715v1</guid>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Marschall, Ines Fortmeier, Manuel Stavridis, Finn Hughes, Clemens Elster</dc:creator>
    </item>
    <item>
      <title>Robust Numerical Methods for Nonlinear Regression</title>
      <link>https://arxiv.org/abs/2403.12759</link>
      <description>arXiv:2403.12759v1 Announce Type: cross 
Abstract: Many scientific and engineering applications require fitting regression models that are nonlinear in the parameters. Advances in computer hardware and software in recent decades have made it easier to fit such models. Relative to fitting regression models that are linear in the parameters, however, fitting nonlinear regression models is more complicated. In particular, software like the $\texttt{nls}$ R function requires care in how the model is parameterized and how initial values are chosen for the maximum likelihood iterations. Often special diagnostics are needed to detect and suggest approaches for dealing with identifiability problems that can arise with such model fitting. When using Bayesian inference, there is the added complication of having to specify (often noninformative or weakly informative) prior distributions. Generally, the details for these tasks must be determined for each new nonlinear regression model. This paper provides a step-by-step procedure for specifying these details for any appropriate nonlinear regression model. Following the procedure will result in a numerically robust algorithm for fitting the nonlinear regression model. We illustrate the methods with three different nonlinear models that are used in the analysis of experimental fatigue data and we include two detailed numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12759v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Liu, William Q. Meeker</dc:creator>
    </item>
    <item>
      <title>Regularizing nested Monte Carlo Sobol' index estimators to balance the trade-off between explorations and repetitions in global sensitivity analysis of stochastic models</title>
      <link>https://arxiv.org/abs/2210.08807</link>
      <description>arXiv:2210.08807v2 Announce Type: replace-cross 
Abstract: Sobol' sensitivity index estimators for stochastic models are  functions of nested Monte Carlo estimators, which are estimators  built from two nested Monte Carlo loops.  The outer loop explores  the input space and, for each of the explorations, the inner loop  repeats model runs to estimate conditional expectations.  Although  the optimal allocation between explorations and repetitions of one's  computational budget is well-known for nested Monte Carlo  estimators, it is less clear how to deal with functions of nested  Monte Carlo estimators, especially when those functions have  unbounded Hessian matrices, as it is the case for Sobol' index  estimators.  To address this problem, a regularization method is  introduced to bound the mean squared error of functions of nested  Monte Carlo estimators. Based on a heuristic, an  allocation strategy that seeks to minimize a bias-variance trade-off  is proposed. The method is applied to Sobol' index estimators for  stochastic models.  A practical algorithm that adapts to the level  of intrinsic randomness in the models is given and illustrated on  numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08807v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Mermoz Kouye (MaIAGE), Gildas Mazo (MaIAGE)</dc:creator>
    </item>
  </channel>
</rss>
