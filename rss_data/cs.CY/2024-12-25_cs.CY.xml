<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Dec 2024 05:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inspiring Women in Technology: Educational Pathways and Impact</title>
      <link>https://arxiv.org/abs/2412.17960</link>
      <description>arXiv:2412.17960v1 Announce Type: new 
Abstract: This paper presents initiatives aimed at fostering female involvement in the realm of computing and endeavoring to inspire more women to pursue careers in these fields. The Meninas++ Project coordinates activities at both the high school and higher education levels, facilitating dialogue between young women and computing professionals, and promoting female role models within the field. Our study demonstrated the significant impact of these activities on inspiring, empowering, and retaining female students in computing. Furthermore, higher education initiatives have fostered engagement among both women and men, promoting inclusivity, entrepreneurship, and collaboration to enhance women's representation in the computing field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17960v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5753/wit.2024.1910</arxiv:DOI>
      <arxiv:journal_reference>Women in Information Technology (WIT) 2024</arxiv:journal_reference>
      <dc:creator>Larissa F. Rodrigues Moreira, Liziane S. Soares, Adriana Z. Martinhago</dc:creator>
    </item>
    <item>
      <title>Collective sleep and activity patterns of college students from wearable devices</title>
      <link>https://arxiv.org/abs/2412.17969</link>
      <description>arXiv:2412.17969v1 Announce Type: new 
Abstract: To optimize interventions for improving wellness, it is essential to understand habits, which wearable devices can measure with greater precision. Using high temporal resolution biometric data taken from the Oura Gen3 ring, we examine daily and weekly sleep and activity patterns of a cohort of young adults (N=582) in their first semester of college. A high compliance rate is observed for both daily and nightly wear, with slight dips in wear compliance observed shortly after waking up and also in the evening. Most students have a late-night chronotype with a median midpoint of sleep at 5AM, with males and those with mental health impairment having more delayed sleep periods. Social jetlag, or the difference in sleep times between free days and school days, is prevalent in our sample. While sleep periods generally shift earlier on weekdays and later on weekends, sleep duration on both weekdays and weekends is shorter than during prolonged school breaks, suggesting chronic sleep debt when school is in session. Synchronized spikes in activity consistent with class schedules are also observed, suggesting that walking in between classes is a widespread behavior in our sample that substantially contributes to physical activity. Lower active calorie expenditure is associated with weekends and a delayed but longer sleep period the night before, suggesting that for our cohort, active calorie expenditure is affected less by deviations from natural circadian rhythms and more by the timing associated with activities. Our study shows that regular sleep and activity routines may be inferred from consumer wearable devices if high temporal resolution and long data collection periods are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17969v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikaela Irene Fudolig, Laura S. P. Bloomfield, Matthew Price, Yoshi M. Bird, Johanna E. Hidalgo, 5 Julia Kim, Jordan Llorin, Juniper Lovato, Ellen W. McGinnis, Ryan S. McGinnis, Taylor Ricketts, Kathryn Stanton, Peter Sheridan Dodds, Christopher M. Danforth</dc:creator>
    </item>
    <item>
      <title>Fundamental Limits in the Search for Less Discriminatory Algorithms -- and How to Avoid Them</title>
      <link>https://arxiv.org/abs/2412.18138</link>
      <description>arXiv:2412.18138v1 Announce Type: new 
Abstract: Disparate impact doctrine offers an important legal apparatus for targeting unfair data-driven algorithmic decisions. A recent body of work has focused on conceptualizing and operationalizing one particular construct from this doctrine -- the less discriminatory alternative, an alternative policy that reduces disparities while meeting the same business needs of a status quo or baseline policy. This paper puts forward four fundamental results, which each represent limits to searching for and using less discriminatory algorithms (LDAs). (1) Statistically, although LDAs are almost always identifiable in retrospect on fixed populations, making conclusions about how alternative classifiers perform on an unobserved distribution is more difficult. (2) Mathematically, a classifier can only exhibit certain combinations of accuracy and selection rate disparity between groups, given the size of each group and the base rate of the property or outcome of interest in each group. (3) Computationally, a search for a lower-disparity classifier at some baseline level of utility is NP-hard. (4) From a modeling and consumer welfare perspective, defining an LDA only in terms of business needs can lead to LDAs that leave consumers strictly worse off, including members of the disadvantaged group. These findings, which may seem on their face to give firms strong defenses against discrimination claims, only tell part of the story. For each of our negative results limiting what is attainable in this setting, we offer positive results demonstrating that there exist effective and low-cost strategies that are remarkably effective at identifying viable lower-disparity policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18138v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Laufer, Manisch Raghavan, Solon Barocas</dc:creator>
    </item>
    <item>
      <title>Elevating Information System Performance: A Deep Dive into Quality Metrics</title>
      <link>https://arxiv.org/abs/2412.18512</link>
      <description>arXiv:2412.18512v1 Announce Type: new 
Abstract: In today's digital age, information systems (IS) are indispensable tools for organizations of all sizes. The quality of these systems, encompassing system, information, and service dimensions, significantly impacts organizational performance. This study investigates the intricate relationships between these three quality dimensions and their collective influence on key performance indicators such as customer satisfaction and operational efficiency. By conducting a comparative analysis of various quality metrics, we aim to identify the most effective indicators for assessing IS quality. Our research contributes to the field by providing actionable insights for researchers or practitioners to develop the implementation, evaluation and design of information systems. Also, a quantitative study employing a structured questionnaire survey was conducted to achieve primary data from respondents across various sectors. Statistical analysis, including Cronbach's Alpha (0.953) and factor analysis (KMO = 0.965, Bartlett's Test p &lt; 0.000), revealed strong interdependencies among System Quality (SQ), Information Quality (IQ), and Service Quality (SerQ). The results demonstrate that high SQ leads to improved IQ, which in turn contributes to enhanced SerQ and user satisfaction. While all three qualities are crucial, SerQ emerges as the most relevant indicator of overall system performance due to its broader representation of quality dimensions</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18512v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dana A Abdullah, Hewir A. Khidir, Ismail Y. Maolood, Aso K. Ameen, Dana Rasul Hamad, Hakem Saed Beitolahi, Abdulhady Abas Abdullah, Tarik Ahmed Rashid, Mohammed Y. Shakor</dc:creator>
    </item>
    <item>
      <title>Bridging the Data Provenance Gap Across Text, Speech and Video</title>
      <link>https://arxiv.org/abs/2412.17847</link>
      <description>arXiv:2412.17847v1 Announce Type: cross 
Abstract: Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17847v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas, Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm, Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker, Jad Kabbara</dc:creator>
    </item>
    <item>
      <title>Active Geospatial Search for Efficient Tenant Eviction Outreach</title>
      <link>https://arxiv.org/abs/2412.17854</link>
      <description>arXiv:2412.17854v1 Announce Type: cross 
Abstract: Tenant evictions threaten housing stability and are a major concern for many cities. An open question concerns whether data-driven methods enhance outreach programs that target at-risk tenants to mitigate their risk of eviction. We propose a novel active geospatial search (AGS) modeling framework for this problem. AGS integrates property-level information in a search policy that identifies a sequence of rental units to canvas to both determine their eviction risk and provide support if needed. We propose a hierarchical reinforcement learning approach to learn a search policy for AGS that scales to large urban areas containing thousands of parcels, balancing exploration and exploitation and accounting for travel costs and a budget constraint. Crucially, the search policy adapts online to newly discovered information about evictions. Evaluation using eviction data for a large urban area demonstrates that the proposed framework and algorithmic approach are considerably more effective at sequentially identifying eviction cases than baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17854v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anindya Sarkar, Alex DiChristofano, Sanmay Das, Patrick J. Fowler, Nathan Jacobs, Yevgeniy Vorobeychik</dc:creator>
    </item>
    <item>
      <title>Emoji Retrieval from Gibberish or Garbled Social Media Text: A Novel Methodology and A Case Study</title>
      <link>https://arxiv.org/abs/2412.18046</link>
      <description>arXiv:2412.18046v1 Announce Type: cross 
Abstract: Emojis are widely used across social media platforms but are often lost in noisy or garbled text, posing challenges for data analysis and machine learning. Conventional preprocessing approaches recommend removing such text, risking the loss of emojis and their contextual meaning. This paper proposes a three-step reverse-engineering methodology to retrieve emojis from garbled text in social media posts. The methodology also identifies reasons for the generation of such text during social media data mining. To evaluate its effectiveness, the approach was applied to 509,248 Tweets about the Mpox outbreak, a dataset referenced in about 30 prior works that failed to retrieve emojis from garbled text. Our method retrieved 157,748 emojis from 76,914 Tweets. Improvements in text readability and coherence were demonstrated through metrics such as Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index, Automated Readability Index, Dale-Chall Readability Score, Text Standard, and Reading Time. Additionally, the frequency of individual emojis and their patterns of usage in these Tweets were analyzed, and the results are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18046v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuqi Cui, Nirmalya Thakur, Audrey Poon</dc:creator>
    </item>
    <item>
      <title>Fair Knowledge Tracing in Second Language Acquisition</title>
      <link>https://arxiv.org/abs/2412.18048</link>
      <description>arXiv:2412.18048v1 Announce Type: cross 
Abstract: In second-language acquisition, predictive modeling aids educators in implementing diverse teaching strategies, attracting significant research attention. However, while model accuracy is widely explored, model fairness remains under-examined. Model fairness ensures equitable treatment of groups, preventing unintentional biases based on attributes such as gender, ethnicity, or economic background. A fair model should produce impartial outcomes that do not systematically disadvantage any group.
  This study evaluates the fairness of two predictive models using the Duolingo dataset's en\_es (English learners speaking Spanish), es\_en (Spanish learners speaking English), and fr\_en (French learners speaking English) tracks. We analyze: 1. Algorithmic fairness across platforms (iOS, Android, Web). 2. Algorithmic fairness between developed and developing countries.
  Key findings include: 1. Deep learning outperforms machine learning in second-language knowledge tracing due to improved accuracy and fairness. 2. Both models favor mobile users over non-mobile users. 3. Machine learning exhibits stronger bias against developing countries compared to deep learning. 4. Deep learning strikes a better balance of fairness and accuracy in the en\_es and es\_en tracks, while machine learning is more suitable for fr\_en.
  This study highlights the importance of addressing fairness in predictive models to ensure equitable educational strategies across platforms and regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18048v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weitao Tang, Guanliang Chen, Shuaishuai Zu, Jiangyi Luo</dc:creator>
    </item>
    <item>
      <title>SoK: On the Offensive Potential of AI</title>
      <link>https://arxiv.org/abs/2412.18442</link>
      <description>arXiv:2412.18442v1 Announce Type: cross 
Abstract: Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence shows that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laymen -- all of which being valuable sources of information on offensive AI.
  To enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user study (N=549) entailing individuals with diverse backgrounds and expertise; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18442v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Saskia Laura Schr\"oer, Giovanni Apruzzese, Soheil Human, Pavel Laskov, Hyrum S. Anderson, Edward W. N. Bernroider, Aurore Fass, Ben Nassi, Vera Rimmer, Fabio Roli, Samer Salam, Ashley Shen, Ali Sunyaev, Tim Wadwha-Brown, Isabel Wagner, Gang Wang</dc:creator>
    </item>
    <item>
      <title>Persuasion and Phishing: Analysing the Interplay of Persuasion Tactics in Cyber Threats</title>
      <link>https://arxiv.org/abs/2412.18485</link>
      <description>arXiv:2412.18485v1 Announce Type: cross 
Abstract: This study extends the research of Ferreira and Teles (2019), who synthesized works by Cialdini (2007), Gragg (2003), and Stajano and Wilson (2011) to propose a unique list of persuasion principles in social engineering. While Ferreira and Teles focused on email subject lines, this research analyzed entire email contents to identify principles of human persuasion in phishing emails. This study also examined the goals and targets of phishing emails, providing a novel contribution to the field. Applying these findings to the ontological model by Mouton et al. (2014) reveals that when social engineers use email for phishing, individuals are the primary targets. The goals are typically unauthorized access, followed by financial gain and service disruption, with Distraction as the most commonly used compliance principle. This research highlights the importance of understanding human persuasion in technology-mediated interactions to develop methods for detecting and preventing phishing emails before they reach users. Despite previous identification of luring elements in phishing emails, empirical findings have been inconsistent. For example, Akbar (2014) found 'authority' and 'scarcity' most common, while Ferreira et al. (2015) identified 'liking' and 'similarity.' In this study, 'Distraction' was most frequently used, followed by 'Deception,' 'Integrity,' and 'Authority.' This paper offers additional insights into phishing email tactics and suggests future solutions should leverage socio-technical principles. Future work will apply this methodology to other social engineering techniques beyond phishing emails, using the ontological model to further inform the research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18485v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kalam Khadka</dc:creator>
    </item>
    <item>
      <title>A Survey on the Principles of Persuasion as a Social Engineering Strategy in Phishing</title>
      <link>https://arxiv.org/abs/2412.18488</link>
      <description>arXiv:2412.18488v1 Announce Type: cross 
Abstract: Research shows that phishing emails often utilize persuasion techniques, such as social proof, liking, consistency, authority, scarcity, and reciprocity to gain trust to obtain sensitive information or maliciously infect devices. The link between principles of persuasion and social engineering attacks, particularly in phishing email attacks, is an important topic in cyber security as they are the common and effective method used by cybercriminals to obtain sensitive information or access computer systems. This survey paper concluded that spear phishing, a targeted form of phishing, has been found to be specifically effective as attackers can tailor their messages to the specific characteristics, interests, and vulnerabilities of their targets. Understanding the uses of the principles of persuasion in spear phishing is key to the effective defence against it and eventually its elimination. This survey paper systematically summarizes and presents the current state of the art in understanding the use of principles of persuasion in phishing. Through a systematic review of the existing literature, this survey paper identifies a significant gap in the understanding of the impact of principles of persuasion as a social engineering strategy in phishing attacks and highlights the need for further research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18488v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TrustCom60117.2023.00222.</arxiv:DOI>
      <dc:creator>Kalam Khadka, Abu Barkat Ullah, Wanli Ma, Elisa Martinez Marroquin</dc:creator>
    </item>
    <item>
      <title>Resolving Ethics Trade-offs in Implementing Responsible AI</title>
      <link>https://arxiv.org/abs/2401.08103</link>
      <description>arXiv:2401.08103v5 Announce Type: replace 
Abstract: While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08103v5</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CAI59869.2024.00215</arxiv:DOI>
      <arxiv:journal_reference>IEEE Conference on Artificial Intelligence, 2024</arxiv:journal_reference>
      <dc:creator>Conrad Sanderson, Emma Schleiger, David Douglas, Petra Kuhnert, Qinghua Lu</dc:creator>
    </item>
    <item>
      <title>Revolutionizing Undergraduate Learning: CourseGPT and Its Generative AI Advancements</title>
      <link>https://arxiv.org/abs/2407.18310</link>
      <description>arXiv:2407.18310v2 Announce Type: replace 
Abstract: Integrating Generative AI (GenAI) into educational contexts presents a transformative potential for enhancing learning experiences. This paper introduces CourseGPT, a generative AI tool designed to support instructors and enhance the educational experiences of undergraduate students. Built on open-source Large Language Models (LLMs) from Mistral AI, CourseGPT offers continuous instructor support and regular updates to course materials, enriching the learning environment. By utilizing course-specific content, such as slide decks and supplementary readings and references, CourseGPT provides precise, dynamically generated responses to student inquiries. Unlike generic AI models, CourseGPT allows instructors to manage and control the responses, thus extending the course scope without overwhelming details. The paper demonstrates the application of CourseGPT using the CPR E 431 - Basics of Information System Security course as a pilot. This course, with its large enrollments and diverse curriculum, serves as an ideal testbed for CourseGPT. The tool aims to enhance the learning experience, accelerate feedback processes, and streamline administrative tasks. The study evaluates CourseGPT's impact on student outcomes, focusing on correctness scores, context recall, and faithfulness of responses. Results indicate that the Mixtral-8x7b model, with a higher parameter count, outperforms smaller models, achieving an 88.0% correctness score and a 66.6% faithfulness score. Additionally, feedback from former students and teaching assistants on CourseGPT's accuracy, helpfulness, and overall performance was collected. The outcomes revealed that a significant majority found CourseGPT to be highly accurate and beneficial in addressing their queries, with many praising its ability to provide timely and relevant information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18310v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad M. Nazar, Mohamed Y. Selim, Ashraf Gaffar, Shakil Ahmed</dc:creator>
    </item>
    <item>
      <title>Responsible AI Governance: A Response to UN Interim Report on Governing AI for Humanity</title>
      <link>https://arxiv.org/abs/2412.12108</link>
      <description>arXiv:2412.12108v2 Announce Type: replace 
Abstract: This report presents a comprehensive response to the United Nation's Interim Report on Governing Artificial Intelligence (AI) for Humanity. It emphasizes the transformative potential of AI in achieving the Sustainable Development Goals (SDGs) while acknowledging the need for robust governance to mitigate associated risks. The response highlights opportunities for promoting equitable, secure, and inclusive AI ecosystems, which should be supported by investments in infrastructure and multi-stakeholder collaborations across jurisdictions. It also underscores challenges, including societal inequalities exacerbated by AI, ethical concerns, and environmental impacts. Recommendations advocate for legally binding norms, transparency, and multi-layered data governance models, alongside fostering AI literacy and capacity-building initiatives. Internationally, the report calls for harmonising AI governance frameworks with established laws, human rights standards, and regulatory approaches. The report concludes with actionable principles for fostering responsible AI governance through collaboration among governments, industry, academia, and civil society, ensuring the development of AI aligns with universal human values and the public good.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12108v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Kiden, Bernd Stahl, Beverley Townsend, Carsten Maple, Charles Vincent, Fraser Sampson, Geoff Gilbert, Helen Smith, Jayati Deshmukh, Jen Ross, Jennifer Williams, Jesus Martinez del Rincon, Justyna Lisinska, Karen O'Shea, M\'arjory Da Costa Abreu, Nelly Bencomo, Oishi Deb, Peter Winter, Phoebe Li, Philip Torr, Pin Lean Lau, Raquel Iniesta, Gopal Ramchurn, Sebastian Stein, Vahid Yazdanpanah</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Guide to Item Recovery Using the Multidimensional Graded Response Model in R</title>
      <link>https://arxiv.org/abs/2412.16657</link>
      <description>arXiv:2412.16657v2 Announce Type: replace 
Abstract: The purpose of this study is to provide a step-by-step demonstration of item recovery for the Multidimensional Graded Response Model (MGRM) in R. Within this scope, a sample simulation design was constructed where the test lengths were set to 20 and 40, the interdimensional correlations were varied as 0.3 and 0.7, and the sample size was fixed at 2000. Parameter estimates were derived from the generated datasets for the 3-dimensional GRM, and bias and Root Mean Square Error (RMSE) values were calculated and visualized. In line with the aim of the study, R codes for all these steps were presented along with detailed explanations, enabling researchers to replicate and adapt the procedures for their own analyses. This study is expected to contribute to the literature by serving as a practical guide for implementing item recovery in the MGRM. In addition, the methods presented, including data generation, parameter estimation, and result visualization, are anticipated to benefit researchers even if they are not directly engaged in item recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16657v2</guid>
      <category>cs.CY</category>
      <category>stat.OT</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yesim Beril Soguksu, Ayse Bilicioglu Gunes, Hatice Gurdil</dc:creator>
    </item>
    <item>
      <title>Unpacking Political Bias in Large Language Models: Insights Across Topic Polarization</title>
      <link>https://arxiv.org/abs/2412.16746</link>
      <description>arXiv:2412.16746v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been widely used to generate responses on social topics due to their world knowledge and generative capabilities. Beyond reasoning and generation performance, political bias is an essential issue that warrants attention. Political bias, as a universal phenomenon in human society, may be transferred to LLMs and distort LLMs' behaviors of information acquisition and dissemination with humans, leading to unequal access among different groups of people. To prevent LLMs from reproducing and reinforcing political biases, and to encourage fairer LLM-human interactions, comprehensively examining political bias in popular LLMs becomes urgent and crucial.
  In this study, we systematically measure the political biases in a wide range of LLMs, using a curated set of questions addressing political bias in various contexts. Our findings reveal distinct patterns in how LLMs respond to political topics. For highly polarized topics, most LLMs exhibit a pronounced left-leaning bias. Conversely, less polarized topics elicit greater consensus, with similar response patterns across different LLMs. Additionally, we analyze how LLM characteristics, including release date, model scale, and region of origin affect political bias. The results indicate political biases evolve with model scale and release date, and are also influenced by regional factors of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16746v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiqi Yang, Hang Li, Yucheng Chu, Yuping Lin, Tai-Quan Peng, Hui Liu</dc:creator>
    </item>
    <item>
      <title>Fast and Interpretable Mortality Risk Scores for Critical Care Patients</title>
      <link>https://arxiv.org/abs/2311.13015</link>
      <description>arXiv:2311.13015v3 Announce Type: replace-cross 
Abstract: Prediction of mortality in intensive care unit (ICU) patients typically relies on black box models (that are unacceptable for use in hospitals) or hand-tuned interpretable models (that might lead to the loss in performance). We aim to bridge the gap between these two categories by building on modern interpretable ML techniques to design interpretable mortality risk scores that are as accurate as black boxes. We developed a new algorithm, GroupFasterRisk, which has several important benefits: it uses both hard and soft direct sparsity regularization, it incorporates group sparsity to allow more cohesive models, it allows for monotonicity constraint to include domain knowledge, and it produces many equally-good models, which allows domain experts to choose among them. For evaluation, we leveraged the largest existing public ICU monitoring datasets (MIMIC III and eICU). Models produced by GroupFasterRisk outperformed OASIS and SAPS II scores and performed similarly to APACHE IV/IVa while using at most a third of the parameters. For patients with sepsis/septicemia, acute myocardial infarction, heart failure, and acute kidney failure, GroupFasterRisk models outperformed OASIS and SOFA. Finally, different mortality prediction ML approaches performed better based on variables selected by GroupFasterRisk as compared to OASIS variables. GroupFasterRisk's models performed better than risk scores currently used in hospitals, and on par with black box ML models, while being orders of magnitude sparser. Because GroupFasterRisk produces a variety of risk scores, it allows design flexibility - the key enabler of practical model creation. GroupFasterRisk is a fast, accessible, and flexible procedure that allows learning a diverse set of sparse risk scores for mortality prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13015v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chloe Qinyu Zhu, Muhang Tian, Lesia Semenova, Jiachang Liu, Jack Xu, Joseph Scarpa, Cynthia Rudin</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?</title>
      <link>https://arxiv.org/abs/2408.08685</link>
      <description>arXiv:2408.08685v3 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph. The source code can be found in https://github.com/zhongjian-zhang/LLM4RGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08685v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3690624.3709256</arxiv:DOI>
      <dc:creator>Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi</dc:creator>
    </item>
    <item>
      <title>Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models</title>
      <link>https://arxiv.org/abs/2410.12880</link>
      <description>arXiv:2410.12880v2 Announce Type: replace-cross 
Abstract: As LLMs are increasingly deployed in global applications, the importance of cultural sensitivity becomes paramount, ensuring that users from diverse backgrounds feel respected and understood. Cultural harm can arise when these models fail to align with specific cultural norms, resulting in misrepresentations or violations of cultural values. This work addresses the challenges of ensuring cultural sensitivity in LLMs, especially in small-parameter models that often lack the extensive training data needed to capture global cultural nuances. We present two key contributions: (1) A cultural harm test dataset, created to assess model outputs across different cultural contexts through scenarios that expose potential cultural insensitivities, and (2) A culturally aligned preference dataset, aimed at restoring cultural sensitivity through fine-tuning based on feedback from diverse annotators. These datasets facilitate the evaluation and enhancement of LLMs, ensuring their ethical and safe deployment across different cultural landscapes. Our results show that integrating culturally aligned feedback leads to a marked improvement in model behavior, significantly reducing the likelihood of generating culturally insensitive or harmful content. Ultimately, this work paves the way for more inclusive and respectful AI systems, fostering a future where LLMs can safely and ethically navigate the complexities of diverse cultural landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12880v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Somnath Banerjee, Sayan Layek, Hari Shrawgi, Rajarshi Mandal, Avik Halder, Shanu Kumar, Sagnik Basu, Parag Agrawal, Rima Hazra, Animesh Mukherjee</dc:creator>
    </item>
  </channel>
</rss>
