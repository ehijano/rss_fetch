<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 02:50:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Predicting Depressive Symptoms through Emotion Pairs within Asian American Families</title>
      <link>https://arxiv.org/abs/2602.03943</link>
      <description>arXiv:2602.03943v1 Announce Type: new 
Abstract: Studies on intergenerational relationships between parents and children in Asian American families highlight their impact on mental health and well-being. This study investigates the role of ambivalent emotions in online narratives shared by Asian and Asian American children on the subreddit, r/Asianparentstories. By employing a BERT-based model to detect emotion at the sentence level and depressive symptoms at the post level, we analyze mixed feelings to better understand how they predict depressive symptoms. First, among 28 detectable, eight (realization, approval, sadness, anger, curiosity, annoyance, disappointment, disapproval) comprise over 50%, exhibiting significant co-occurrence among themselves and with other emotions. Second, we find the co-occurrence of multiple emotions, indicating that emotions in a single post are not limited to consistently positive or negative feelings. Finally, our findings indicate that while negative emotion pairs (e.g., confusion-grief, anger-grief) are associated with depressive symptoms, positive emotion pairs (e.g., admiration-realization, amusement-joy) negatively correlate with depressive symptoms, and combinations of ambivalent emotions indicate varied results in predicting depressive symptoms. These findings highlight the importance of automated emotion classification and the need to consider emotional ambivalence, which holds practical and clinical implications for understanding the dynamics of parent-child relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03943v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangpil Youm, Nari Yoo, Sou Hyun Jang</dc:creator>
    </item>
    <item>
      <title>The CitizenQuery Benchmark: A Novel Dataset and Evaluation Pipeline for Measuring LLM Performance in Citizen Query Tasks</title>
      <link>https://arxiv.org/abs/2602.04064</link>
      <description>arXiv:2602.04064v1 Announce Type: new 
Abstract: "Citizen queries" are questions asked by an individual about government policies, guidance, and services that are relevant to their circumstances, encompassing a range of topics including benefits, taxes, immigration, employment, public health, and more. This represents a compelling use case for Large Language Models (LLMs) that respond to citizen queries with information that is adapted to a user's context and communicated according to their needs. However, in this use case, any misinformation could have severe, negative, likely invisible ramifications for an individual placing their trust in a model's response.
  To this effect, we introduce CitizenQuery-UK, a benchmark dataset of 22 thousand pairs of citizen queries and responses that have been synthetically generated from the swathes of public information on $gov.uk$ about government in the UK. We present the curation methodology behind CitizenQuery-UK and an overview of its contents. We also introduce a methodology for the benchmarking of LLMs with the dataset, using an adaptation of FActScore to benchmark 11 models for factuality, abstention frequency, and verbosity. We document these results, and interpret them in the context of the public sector, finding that: (i) there are distinct performance profiles across model families, but each is competitive; (ii) high variance undermines utility; (iii) abstention is low and verbosity is high, with implications on reliability; and (iv) more trustworthy AI requires acknowledged "fallibility" in the way it interacts with users.
  The contribution of our research lies in assessing the trustworthiness of LLMs in citizen query tasks; as we see a world of increasing AI integration into day-to-day life, our benchmark, built entirely on open data, lays the foundations for better evidenced decision-making regarding AI and the public sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04064v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Majithia, Rajat Shinde, Zo Chapman, Prajun Trital, Jordan Decker, Manil Maskey, Elena Simperl, Nigel Shadbolt</dc:creator>
    </item>
    <item>
      <title>Quantifying Algorithmic Friction in Automated Resume Screening Systems</title>
      <link>https://arxiv.org/abs/2602.04087</link>
      <description>arXiv:2602.04087v1 Announce Type: new 
Abstract: Automated resume screening systems are now a central part of hiring at scale, yet there is growing evidence that rigid screening logic can exclude qualified candidates before human review. In prior work, we introduced the concept of Artificial Frictional Unemployment to describe labor market inefficiencies arising from automated recruitment systems. This paper extends that framework by focusing on measurement. We present a method for quantifying algorithmic friction in resume screening pipelines by modeling screening as a classification task and defining friction as excess false negative rejection caused by semantic misinterpretation. Using controlled simulations, we compare deterministic keyword-based screening with vector-space semantic matching under identical qualification conditions. The results show that keyword-based screening exhibits high levels of algorithmic friction, while semantic representations substantially reduce false negative rejection without compromising precision. By treating algorithmic friction as a system-level property, this study provides an empirical basis for evaluating how recruitment system design affects matching efficiency in modern labor markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04087v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Denis Fofanah</dc:creator>
    </item>
    <item>
      <title>They Call Her 'Miss' and Him 'Professor': Lived Experiences of Women Teaching Support Staff in IT/SE Education</title>
      <link>https://arxiv.org/abs/2602.04332</link>
      <description>arXiv:2602.04332v1 Announce Type: new 
Abstract: Despite their critical role in shaping student learning in computing education, the contributions of women teaching-support staff (TSS) often go unrecognised and undervalued. In this experience report, we synthesise lived experiences of 15 women TSS in IT/SE higher education to illuminate how authority is earned, resisted, and maintained in everyday teaching. Participants shared both their positive and negative lived experiences associated with finding and losing voice with teaching team colleagues on the one hand, and rewarding connections and gendered friction with students on the other. We map these dynamics onto an intersectional "wheel of privilege and power" tailored to TSS roles. The farther a TSS profile sits from the wheel's center (e.g., non-native English, non-white, younger-seeming, non-permanent, early-career), the more relational, emotional, and disciplinary labour is needed to reach parity. We provide actionable insights and recommendations for creating more inclusive education environments in technology dominant fields that are particularly timely as universities worldwide grapple with post-pandemic teaching models and seek to build more inclusive and resilient academic communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04332v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vasudha Malhotra, Rhea D'silva, Rashina Hoda</dc:creator>
    </item>
    <item>
      <title>Growth First, Care Second? Tracing the Landscape of LLM Value Preferences in Everyday Dilemmas</title>
      <link>https://arxiv.org/abs/2602.04456</link>
      <description>arXiv:2602.04456v1 Announce Type: new 
Abstract: People increasingly seek advice online from both human peers and large language model (LLM)-based chatbots. Such advice rarely involves identifying a single correct answer; instead, it typically requires navigating trade-offs among competing values. We aim to characterize how LLMs navigate value trade-offs across different advice-seeking contexts. First, we examine the value trade-off structure underlying advice seeking using a curated dataset from four advice-oriented subreddits. Using a bottom-up approach, we inductively construct a hierarchical value framework by aggregating fine-grained values extracted from individual advice options into higher-level value categories. We construct value co-occurrence networks to characterize how values co-occur within dilemmas and find substantial heterogeneity in value trade-off structures across advice-seeking contexts: a women-focused subreddit exhibits the highest network density, indicating more complex value conflicts; women's, men's, and friendship-related subreddits exhibit highly correlated value-conflict patterns centered on security-related tensions (security vs. respect/connection/commitment); by contrast, career advice forms a distinct structure where security frequently clashes with self-actualization and growth. We then evaluate LLM value preferences against these dilemmas and find that, across models and contexts, LLMs consistently prioritize values related to Exploration &amp; Growth over Benevolence &amp; Connection. This systemically skewed value orientation highlights a potential risk of value homogenization in AI-mediated advice, raising concerns about how such systems may shape decision-making and normative outcomes at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04456v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyi Chen, Eun Cheol Choi, Yingjia Luo, Xinyi Wang, Yulei Xiao, Aizi Yang, Luca Luceri</dc:creator>
    </item>
    <item>
      <title>Fine-grained Classification of A Million Life Trajectories from Wikipedia</title>
      <link>https://arxiv.org/abs/2602.04503</link>
      <description>arXiv:2602.04503v1 Announce Type: new 
Abstract: Life trajectories of notable people convey essential messages for human dynamics research. These trajectories consist of (\textit{person, time, location, activity type}) tuples recording when and where a person was born, went to school, started a job, or fought in a war. However, current studies only cover limited activity types such as births and deaths, lacking large-scale fine-grained trajectories. Using a tool that extracts (\textit{person, time, location}) triples from Wikipedia, we formulate the problem of classifying these triples into 24 carefully-defined types using textual context as complementary information. The challenge is that triple entities are often scattered in noisy contexts. We use syntactic graphs to bring triple entities and relevant information closer, fusing them with text embeddings to classify life trajectory activities. Since Wikipedia text quality varies, we use LLMs to refine the text for more standardized syntactic graphs. Our framework achieves 84.5\% accuracy, surpassing baselines. We construct the largest fine-grained life trajectory dataset with 3.8 million labeled activities for 589,193 individuals spanning 3 centuries. In the end, we showcase how these trajectories can support grand narratives of human dynamics across time and space. Code/data are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04503v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhaoyang Liu, Xiaocong Du, Yixi Zhou, Ye Shi, Haipeng Zhang</dc:creator>
    </item>
    <item>
      <title>Learning the Value Systems of Agents with Preference-based and Inverse Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2602.04518</link>
      <description>arXiv:2602.04518v1 Announce Type: new 
Abstract: Agreement Technologies refer to open computer systems in which autonomous software agents interact with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. With the advance of AI systems in recent years, it has become apparent that such agreements, in order to be acceptable to the involved parties, must remain aligned with ethical principles and moral values. However, this is notoriously difficult to ensure, especially as different human users (and their software agents) may hold different value systems, i.e. they may differently weigh the importance of individual moral values. Furthermore, it is often hard to specify the precise meaning of a value in a particular context in a computational manner. Methods to estimate value systems based on human-engineered specifications, e.g. based on value surveys, are limited in scale due to the need for intense human moderation. In this article, we propose a novel method to automatically \emph{learn} value systems from observations and human demonstrations. In particular, we propose a formal model of the \emph{value system learning} problem, its instantiation to sequential decision-making domains based on multi-objective Markov decision processes, as well as tailored preference-based and inverse reinforcement learning algorithms to infer value grounding functions and value systems. The approach is illustrated and evaluated by two simulated use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04518v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10458-026-09732-0</arxiv:DOI>
      <arxiv:journal_reference>Holgado-S\'anchez, A., Billhardt, H., Fern\'andez, A., Ossowski, S. Learning the value systems of agents with preference-based and inverse reinforcement learning. Autonomous Agents Multi-Agent Systems 40, 4 (2026)</arxiv:journal_reference>
      <dc:creator>Andr\'es Holgado-S\'anchez, Holger Billhardt, Alberto Fern\'andez, Sascha Ossowski</dc:creator>
    </item>
    <item>
      <title>Inference-Time Reasoning Selectively Reduces Implicit Social Bias in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.04742</link>
      <description>arXiv:2602.04742v1 Announce Type: new 
Abstract: Drawing on constructs from psychology, prior work has identified a distinction between explicit and implicit bias in large language models (LLMs). While many LLMs undergo post-training alignment and safety procedures to avoid expressions of explicit social bias, they still exhibit significant implicit biases on indirect tasks resembling the Implicit Association Test (IAT). Recent work has further shown that inference-time reasoning can impair LLM performance on tasks that rely on implicit statistical learning. Motivated by a theoretical link between implicit associations and statistical learning in human cognition, we examine how reasoning-enabled inference affects implicit bias in LLMs. We find that enabling reasoning significantly reduces measured implicit bias on an IAT-style evaluation for some model classes across fifteen stereotype topics. This effect appears specific to social bias domains, as we observe no corresponding reduction for non-social implicit associations. As reasoning is increasingly enabled by default in deployed LLMs, these findings suggest that it can meaningfully alter fairness evaluation outcomes in some systems, while also raising questions about how alignment procedures interact with inference-time reasoning to drive variation in bias reduction across model types. More broadly, this work highlights how theory from cognitive science and psychology can complement AI evaluation research by providing methodological and interpretive frameworks that reveal new insights into model behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04742v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Molly Apsel, Michael N. Jones</dc:creator>
    </item>
    <item>
      <title>How to Stop Playing Whack-a-Mole: Mapping the Ecosystem of Technologies Facilitating AI-Generated Non-Consensual Intimate Images</title>
      <link>https://arxiv.org/abs/2602.04759</link>
      <description>arXiv:2602.04759v1 Announce Type: new 
Abstract: The last decade has witnessed a rapid advancement of generative AI technology that significantly scaled the accessibility of AI-generated non-consensual intimate images (AIG-NCII), a form of image-based sexual abuse that disproportionately harms women and girls. There is a patchwork of commendable efforts across industry, policy, academia, and civil society to address AIG-NCII. However, these efforts lack a shared, consistent mental model that situates the technologies they target within the context of a large, interconnected, and ever-evolving technological ecosystem. As a result, interventions remain siloed and are difficult to evaluate and compare, leading to a reactive cycle of whack-a-mole. We contribute the first comprehensive AIG-NCII technological ecosystem that maps and taxonomizes 11 categories of technologies facilitating the creation, distribution, proliferation and discovery, infrastructural support, and monetization of AIG-NCII. First, we build and visualize the ecosystem through a synthesis of over a hundred primary sources from researchers, journalists, advocates, policymakers, and technologists. Next, we demonstrate how stakeholders can use the ecosystem as a tool to 1) understand new incidents of harm via a case study of Grok and 2) evaluate existing interventions via three more case studies. We conclude with three actionable recommendations, namely that stakeholders should 1) use the ecosystem to map out state, federal, and international laws to produce a clearer policy landscape, 2) collectively develop a database that dynamically tracks the 11 technologies in the ecosystem to better evaluate interventions, and 3) adopt a relational approach to researching AIG-NCII to better understand how the ecosystem technologies interact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04759v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle L. Ding, Harini Suresh, Suresh Venkatasubramanian</dc:creator>
    </item>
    <item>
      <title>Perceptions of AI-CBT: Trust and Barriers in Chinese Postgrads</title>
      <link>https://arxiv.org/abs/2602.03852</link>
      <description>arXiv:2602.03852v1 Announce Type: cross 
Abstract: The mental well-being of graduate students is an increasing concern, yet the adoption of scalable support remains uneven. Artificial intelligence-powered cognitive behavioral therapy chatbots (AI-CBT) offer low barrier help, but little is known about how Chinese postgraduates perceive and use them. This qualitative study explored perceptions and experiences of AI-CBT chatbots among ten Chinese graduate students recruited through social media. Semi-structured Zoom interviews were conducted and analyzed using reflexive thematic analysis, with the Health Belief Model (HBM) and the Theory of Planned Behavior (TPB) as sensitizing frameworks. The findings indicate a cautious openness to AI-CBT chatbots: perceived usefulness and 24/7 access supported favorable attitudes, while data privacy, emotional safety, and uncertainty about `fit' for complex problems restricted the intention to use. Social norms (e.g., stigma and peer views) and perceived control (digital literacy, language quality) further shaped adoption. The study offers context-specific information to guide the culturally sensitive design, communication, and deployment of AI mental well-being tools for student populations in China and outlines the design implications around transparency, safeguards, and graduated care pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03852v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chan-in Sio, Alex Mann, Lingxi Fan, Andrew Cheung, Lik-hang Lee</dc:creator>
    </item>
    <item>
      <title>From Expectation To Experience: A Before And After Survey Of Public Opinion On Autonomous Cars In Saudi Arabia</title>
      <link>https://arxiv.org/abs/2602.03854</link>
      <description>arXiv:2602.03854v1 Announce Type: cross 
Abstract: Autonomous vehicles (AVs) are emerging as a transformative innovation in transportation, offering potential benefits in safety, sustainability, and efficiency. Saudi Arabian adoption of AVs aligns with Vision 2030, emphasizing smart mobility through initiatives such as the Riyadh Autonomous Metro and self-driving cars. This study explores Saudi citizens perceptions of AVs before and after exposure to these technologies and examines whether demographic factors age, gender, education level, and driving habits affect acceptance. Using quantitative methods, the findings provide insights into the broader influences shaping AV adoption, highlighting the importance of trust, perceived safety, and convenience. These results can inform policymakers and industry stakeholders on strategies to facilitate successful integration of AVs into Saudi Arabian transportation ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03854v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mona Alfayez, Ohoud Alharbi</dc:creator>
    </item>
    <item>
      <title>Child Mortality Prediction in Bangladesh: A Decade-Long Validation Study</title>
      <link>https://arxiv.org/abs/2602.03957</link>
      <description>arXiv:2602.03957v1 Announce Type: cross 
Abstract: The predictive machine learning models for child mortality tend to be inaccurate when applied to future populations, since they suffer from look-ahead bias due to the randomization used in cross-validation. The Demographic and Health Surveys (DHS) data from Bangladesh for 2011-2022, with n = 33,962, are used in this paper. We trained the model on (2011-2014) data, validated it on 2017 data, and tested it on 2022 data. Eight years after the initial test of the model, a genetic algorithm-based Neural Architecture Search found a single-layer neural architecture (with 64 units) to be superior to XGBoost (AUROC = 0.76 vs. 0.73; p &lt; 0.01). Additionally, through a detailed fairness audit, we identified an overall "Socioeconomic Predictive Gradient," with a positive correlation between regional poverty level (r = -0.62) and the algorithm's AUC. In addition, we found that the model performed at its highest levels in the least affluent divisions (AUC 0.74) and decreased dramatically in the wealthiest divisions (AUC 0.66). These findings suggest that the model is identifying areas with the greatest need for intervention. Our model would identify approximately 1300 additional at-risk children annually than a Gradient Boosting model when screened at the 10% level and validated using SHAP values and Platt Calibration, and therefore provide a robust, production-ready computational phenotype for targeted maternal and child health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03957v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Muhtasim Munif Fahim, Md Rezaul Karim</dc:creator>
    </item>
    <item>
      <title>Dynamic Matching Under Patience Imbalance</title>
      <link>https://arxiv.org/abs/2602.03995</link>
      <description>arXiv:2602.03995v1 Announce Type: cross 
Abstract: We study a dynamic matching problem on a two-sided platform with unbalanced patience, in which long-lived supply accumulates over time with a unit waiting cost per period, while short-lived demand departs if not matched promptly. High- or low-quality agents arrive sequentially with one supply agent and one demand agent arriving in each period, and matching payoffs are supermodular. In the centralized benchmark, the optimal policy follows a threshold-based rule that rations high-quality supply, preserving it for future high-quality demand. In the decentralized system, where self-interested agents decide whether to match under an exogenously specified payoff allocation proportion, we characterize a welfare-maximizing Markov perfect equilibrium. Unlike outcomes in the centralized benchmark or in full-backlog markets, the equilibrium exhibits distinct matching patterns in which low-type demand may match with high-type supply even when low-type supply is available. Unlike settings in which both sides have long-lived agents and perfect coordination is impossible, the decentralized system can always be perfectly aligned with the centralized optimum by appropriately adjusting the allocation of matching payoffs across agents on both sides. Finally, when the arrival probabilities for H- and L-type arrivals are identical on both sides, we compare social welfare across systems with different patience levels: full backlog on both sides, one-sided backlog, and no backlog. In the centralized setting, social welfare is weakly ordered across systems. However, in the decentralized setting, the social welfare ranking across the three systems depends on the matching payoff allocation rule and the unit waiting cost, and enabling patience can either increase or decrease social welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03995v1</guid>
      <category>econ.TH</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiyuan Chen (David),  Rui (David),  Chen, Ming Hu, Yun Zhou</dc:creator>
    </item>
    <item>
      <title>On the Credibility of Evaluating LLMs using Survey Questions</title>
      <link>https://arxiv.org/abs/2602.04033</link>
      <description>arXiv:2602.04033v1 Announce Type: cross 
Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04033v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jind\v{r}ich Libovick\'y</dc:creator>
    </item>
    <item>
      <title>Counterfactual Explanations for Hypergraph Neural Networks</title>
      <link>https://arxiv.org/abs/2602.04360</link>
      <description>arXiv:2602.04360v1 Announce Type: cross 
Abstract: Hypergraph neural networks (HGNNs) effectively model higher-order interactions in many real-world systems but remain difficult to interpret, limiting their deployment in high-stakes settings.
  We introduce CF-HyperGNNExplainer, a counterfactual explanation method for HGNNs that identifies the minimal structural changes required to alter a model's prediction. The method generates counterfactual hypergraphs using actionable edits limited to removing node-hyperedge incidences or deleting hyperedges, producing concise and structurally meaningful explanations. Experiments on three benchmark datasets show that CF-HyperGNNExplainer generates valid and concise counterfactuals, highlighting the higher-order relations most critical to HGNN decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04360v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabiano Veglianti, Lorenzo Antonelli, Gabriele Tolomei</dc:creator>
    </item>
    <item>
      <title>Convivial Fabrication: Towards Relational Computational Tools For and From Craft Practices</title>
      <link>https://arxiv.org/abs/2602.04393</link>
      <description>arXiv:2602.04393v1 Announce Type: cross 
Abstract: Computational tools for fabrication often treat materials as passive rather than active participants in design, abstracting away relationships between craftspeople and materials. For craft communities that value relational practices, abstractions limit the adoption and creative uptake of computational tools which might otherwise be beneficial. To understand how better tool design could support richer relations between individuals, tools, and materials, we interviewed expert woodworkers, fiber artists, and metalworkers. We identify three orders of convivial relations central to craft: immediate relations between individuals, tools, and materials; mid-range relations between communities, platforms, and shared materials; and extended relations between institutions, infrastructures, and ecologies. Our analysis shows how craftspeople engage and struggle with convivial relations across all three orders, creating workflows that learn from materials while supporting autonomy. We conclude with design principles for computational tools and infrastructures to better support material dialogue, collective knowledge, and accountability, along with richer and more convivial relations between craftspeople, tools, and the material worlds around them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04393v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritik Batra, Roy Zunder, Amy Cheatle, Amritansh Kwatra, Ilan Mandel, Thijs Roumen, Steven J. Jackson</dc:creator>
    </item>
    <item>
      <title>Performative Learning Theory</title>
      <link>https://arxiv.org/abs/2602.04402</link>
      <description>arXiv:2602.04402v1 Announce Type: cross 
Abstract: Performative predictions influence the very outcomes they aim to forecast. We study performative predictions that affect a sample (e.g., only existing users of an app) and/or the whole population (e.g., all potential app users). This raises the question of how well models generalize under performativity. For example, how well can we draw insights about new app users based on existing users when both of them react to the app's predictions? We address this question by embedding performative predictions into statistical learning theory. We prove generalization bounds under performative effects on the sample, on the population, and on both. A key intuition behind our proofs is that in the worst case, the population negates predictions, while the sample deceptively fulfills them. We cast such self-negating and self-fulfilling predictions as min-max and min-min risk functionals in Wasserstein space, respectively. Our analysis reveals a fundamental trade-off between performatively changing the world and learning from it: the more a model affects data, the less it can learn from it. Moreover, our analysis results in a surprising insight on how to improve generalization guarantees by retraining on performatively distorted samples. We illustrate our bounds in a case study on prediction-informed assignments of unemployed German residents to job trainings, drawing upon administrative labor market records from 1975 to 2017 in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04402v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Rodemann, Unai Fischer-Abaigar, James Bailie, Krikamol Muandet</dc:creator>
    </item>
    <item>
      <title>Unmasking Superspreaders: Data-Driven Approaches for Identifying and Comparing Key Influencers of Conspiracy Theories on X.com</title>
      <link>https://arxiv.org/abs/2602.04546</link>
      <description>arXiv:2602.04546v1 Announce Type: cross 
Abstract: Conspiracy theories can threaten society by spreading misinformation, deepening polarization, and eroding trust in democratic institutions. Social media often fuels the spread of conspiracies, primarily driven by two key actors: Superspreaders -- influential individuals disseminating conspiracy content at disproportionately high rates, and Bots -- automated accounts designed to amplify conspiracies strategically. To counter the spread of conspiracy theories, it is critical to both identify these actors and to better understand their behavior. However, a systematic analysis of these actors as well as real-world-applicable identification methods are still lacking. In this study, we leverage over seven million tweets from the COVID-19 pandemic to analyze key differences between Human Superspreaders and Bots across dimensions such as linguistic complexity, toxicity, and hashtag usage. Our analysis reveals distinct communication strategies: Superspreaders tend to use more complex language and substantive content while relying less on structural elements like hashtags and emojis, likely to enhance credibility and authority. By contrast, Bots favor simpler language and strategic cross-usage of hashtags, likely to increase accessibility, facilitate infiltration into trending discussions, and amplify reach. To counter both Human Superspreaders and Bots, we propose and evaluate 27 novel metrics for quantifying the severity of conspiracy theory spread. Our findings highlight the effectiveness of an adapted H-Index for computationally feasible identification of Human Superspreaders. By identifying behavioral patterns unique to Human Superspreaders and Bots as well as providing suitable identification methods, this study provides a foundation for mitigation strategies, including platform moderation policies, temporary and permanent account suspensions, and public awareness campaigns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04546v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kramer, Henrich R. Greve, Moritz von Zahn, Hayagreeva Rao</dc:creator>
    </item>
    <item>
      <title>VK-LSVD: A Large-Scale Industrial Dataset for Short-Video Recommendation</title>
      <link>https://arxiv.org/abs/2602.04567</link>
      <description>arXiv:2602.04567v1 Announce Type: cross 
Abstract: Short-video recommendation presents unique challenges, such as modeling rapid user interest shifts from implicit feedback, but progress is constrained by a lack of large-scale open datasets that reflect real-world platform dynamics. To bridge this gap, we introduce the VK Large Short-Video Dataset (VK-LSVD), the largest publicly available industrial dataset of its kind. VK-LSVD offers an unprecedented scale of over 40 billion interactions from 10 million users and almost 20 million videos over six months, alongside rich features including content embeddings, diverse feedback signals, and contextual metadata. Our analysis supports the dataset's quality and diversity. The dataset's immediate impact is confirmed by its central role in the live VK RecSys Challenge 2025. VK-LSVD provides a vital, open dataset to use in building realistic benchmarks to accelerate research in sequential recommendation, cold-start scenarios, and next-generation recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04567v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandr Poslavsky, Alexander D'yakonov, Yuriy Dorn, Andrey Zimovnov</dc:creator>
    </item>
    <item>
      <title>VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2602.04587</link>
      <description>arXiv:2602.04587v1 Announce Type: cross 
Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04587v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaeyoon Jung, Yejun Yoon, Seunghyun Yoon, Kunwoo Park</dc:creator>
    </item>
    <item>
      <title>LID Framework: A new method for geospatial and exploratory data analysis of potential innovation deter-minants at the neighborhood level</title>
      <link>https://arxiv.org/abs/2602.04679</link>
      <description>arXiv:2602.04679v1 Announce Type: cross 
Abstract: The geography of innovation offers a framework to understand how territorial characteristics shape innovation, often via spatial and cognitive proximity. Empirical research has focused largely on national and regional scales, while urban and sub-regional geographies receive less attention. Local studies typically rely on limited indicators (e.g., firm-level data, patents, basic socioeconomic measures), with few offering a systematic framework integrating urban form, mobility, amenities, and human-capital proxies at the neighborhood scale. Our study investigates innovation at a finer spatial resolution, going beyond proprietary or static indicators. We develop the Local Innovation Determinants (LID) database and framework to identify key enabling factors across regions, combining traditional government data with publicly available data via APIs for a more granular understanding of spatial dynamics shaping innovation capacity. Using exploratory big and geospatial data analytics and random forest models, we examine neighborhoods in New York and Massachusetts across four dimensions: social factors, economic characteristics, land use and mobility, morphology, and environment. Results show that alternative data sources offer significant yet underexplored potential to enhance insights into innovation dynamics. City policymakers should consider neighborhood-specific determinants and characteristics when designing and implementing local innovation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04679v1</guid>
      <category>stat.CO</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eleni Oikonomaki, Belivanis Dimitris, Kakderi Christina</dc:creator>
    </item>
    <item>
      <title>Investigating Disability Representations in Text-to-Image Models</title>
      <link>https://arxiv.org/abs/2602.04687</link>
      <description>arXiv:2602.04687v1 Announce Type: cross 
Abstract: Text-to-image generative models have made remarkable progress in producing high-quality visual content from textual descriptions, yet concerns remain about how they represent social groups. While characteristics like gender and race have received increasing attention, disability representations remain underexplored. This study investigates how people with disabilities are represented in AI-generated images by analyzing outputs from Stable Diffusion XL and DALL-E 3 using a structured prompt design. We analyze disability representations by comparing image similarities between generic disability prompts and prompts referring to specific disability categories. Moreover, we evaluate how mitigation strategies influence disability portrayals, with a focus on assessing affective framing through sentiment polarity analysis, combining both automatic and human evaluation. Our findings reveal persistent representational imbalances and highlight the need for continuous evaluation and refinement of generative models to foster more diverse and inclusive portrayals of disability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04687v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Yian, Yu Fan, Liudmila Zavolokina, Sarah Ebling</dc:creator>
    </item>
    <item>
      <title>LinGO: A Linguistic Graph Optimization Framework with LLMs for Interpreting Intents of Online Uncivil Discourse</title>
      <link>https://arxiv.org/abs/2602.04693</link>
      <description>arXiv:2602.04693v1 Announce Type: cross 
Abstract: Detecting uncivil language is crucial for maintaining safe, inclusive, and democratic online spaces. Yet existing classifiers often misinterpret posts containing uncivil cues but expressing civil intents, leading to inflated estimates of harmful incivility online. We introduce LinGO, a linguistic graph optimization framework for large language models (LLMs) that leverages linguistic structures and optimization techniques to classify multi-class intents of incivility that use various direct and indirect expressions. LinGO decomposes language into multi-step linguistic components, identifies targeted steps that cause the most errors, and iteratively optimizes prompt and/or example components for targeted steps. We evaluate it using a dataset collected during the 2022 Brazilian presidential election, encompassing four forms of political incivility: Impoliteness (IMP), Hate Speech and Stereotyping (HSST), Physical Harm and Violent Political Rhetoric (PHAVPR), and Threats to Democratic Institutions and Values (THREAT). Each instance is annotated with six types of civil/uncivil intent. We benchmark LinGO using three cost-efficient LLMs: GPT-5-mini, Gemini 2.5 Flash-Lite, and Claude 3 Haiku, and four optimization techniques: TextGrad, AdalFlow, DSPy, and Retrieval-Augmented Generation (RAG). The results show that, across all models, LinGO consistently improves accuracy and weighted F1 compared with zero-shot, chain-of-thought, direct optimization, and fine-tuning baselines. RAG is the strongest optimization technique and, when paired with Gemini model, achieves the best overall performance. These findings demonstrate that incorporating multi-step linguistic components into LLM instructions and optimize targeted components can help the models explain complex semantic meanings, which can be extended to other complex semantic explanation tasks in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04693v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuan Zhang, Thales Bertaglia</dc:creator>
    </item>
    <item>
      <title>From Data to Behavior: Predicting Unintended Model Behaviors Before Training</title>
      <link>https://arxiv.org/abs/2602.04735</link>
      <description>arXiv:2602.04735v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04735v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang</dc:creator>
    </item>
    <item>
      <title>Agentic AI in Healthcare &amp; Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents</title>
      <link>https://arxiv.org/abs/2602.04813</link>
      <description>arXiv:2602.04813v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation &amp; Learning, Safety &amp; Ethics, Framework Typology and Core Tasks &amp; Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection &amp; Mitigation sub-dimension under Adaptation &amp; Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks &amp; Subtasks, information centric capabilities lead e.g., Medical Question Answering &amp; Decision Support and Benchmarking &amp; Simulation, while action and discovery oriented areas such as Treatment Planning &amp; Prescription still show substantial gaps (~59% Not Implemented).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04813v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2026.3651218</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, vol. 14, pp. 4840-4863, 2026</arxiv:journal_reference>
      <dc:creator>Shubham Vatsal, Harsh Dubey, Aditi Singh</dc:creator>
    </item>
    <item>
      <title>Evaluating Prediction-based Interventions with Human Decision Makers In Mind</title>
      <link>https://arxiv.org/abs/2503.05704</link>
      <description>arXiv:2503.05704v3 Announce Type: replace 
Abstract: Automated decision systems (ADS) are broadly deployed to inform and support human decision-making across a wide range of consequential settings. However, various context-specific details complicate the goal of establishing meaningful experimental evaluations for prediction-based interventions. Notably, current experiment designs rely on simplifying assumptions about human decision making in order to derive causal estimates. In reality, specific experimental design decisions may induce cognitive biases in human decision makers, which could then significantly alter the observed effect sizes of the prediction intervention. In this paper, we formalize and investigate various models of human decision-making in the presence of a predictive model aid. We show that each of these behavioural models produces dependencies across decision subjects and results in the violation of existing assumptions, with consequences for treatment effect estimation. This work aims to further advance the scientific validity of intervention-based evaluation schemes for the assessment of ADS deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05704v3</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Inioluwa Deborah Raji, Lydia Liu</dc:creator>
    </item>
    <item>
      <title>LLM Agents for Education: Advances and Applications</title>
      <link>https://arxiv.org/abs/2503.11733</link>
      <description>arXiv:2503.11733v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents are transforming education by automating complex pedagogical tasks and enhancing both teaching and learning processes. In this survey, we present a systematic review of recent advances in applying LLM agents to address key challenges in educational settings, such as feedback comment generation, curriculum design, etc. We analyze the technologies enabling these agents, including representative datasets, benchmarks, and algorithmic frameworks. Additionally, we highlight key challenges in deploying LLM agents in educational settings, including ethical issues, hallucination and overreliance, and integration with existing educational ecosystems. Beyond the core technical focus, we include in Appendix A a comprehensive overview of domain-specific educational agents, covering areas such as science learning, language learning, and professional development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11733v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S. Yu, Qingsong Wen</dc:creator>
    </item>
    <item>
      <title>Mutually Assured Deregulation</title>
      <link>https://arxiv.org/abs/2508.12300</link>
      <description>arXiv:2508.12300v3 Announce Type: replace 
Abstract: We have convinced ourselves that the way to make AI safe is to make it unsafe. Since 2022, policymakers worldwide have embraced the Regulation Sacrifice - the belief that dismantling safety oversight will deliver security through AI dominance. Fearing China or USA will gain advantage, nations rush to eliminate safeguards that might slow progress. This Essay reveals the fatal flaw: though AI poses national security challenges, the solution demands stronger regulatory frameworks, not weaker ones. A race without guardrails breeds shared danger, not competitive strength. The Regulation Sacrifice makes three false promises. First, it promises durable technological leads. But AI capabilities spread rapidly - performance gaps between U.S. and Chinese systems collapsed from 9 percent to 2 percent in thirteen months. When advantages evaporate in months, sacrificing permanent safety for temporary speed makes no sense. Second, it promises deregulation accelerates innovation. The opposite often proves true. Companies report well-designed governance streamlines development. Investment flows toward regulated markets. Clear rules reduce uncertainty; uncertain liability creates paralysis. Environmental standards did not kill the auto industry; they created Tesla and BYD. Third, enhanced national security through deregulation actually undermines security across all timeframes. Near term: it hands adversaries information warfare tools. Medium term: it democratizes bioweapon capabilities. Long term: it guarantees deployment of uncontrollable AGI systems. The Regulation Sacrifice persists because it serves powerful interests, not security. Tech companies prefer freedom to accountability. Politicians prefer simple stories to complex truths. This creates mutually assured deregulation, where each nation's sprint for advantage guarantees collective vulnerability. The only way to win is not to play.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12300v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilad Abiri</dc:creator>
    </item>
    <item>
      <title>Toward Substantive Intersectional Algorithmic Fairness: Desiderata for a Feminist Approach</title>
      <link>https://arxiv.org/abs/2508.17944</link>
      <description>arXiv:2508.17944v2 Announce Type: replace 
Abstract: People's experiences of discrimination are often shaped by multiple intersecting factors, yet algorithmic fairness research rarely reflects this complexity. While intersectionality offers tools for understanding how forms of oppression interact, current approaches to intersectional algorithmic fairness tend to focus on narrowly defined demographic subgroups. These methods contribute important insights but risk oversimplifying social reality and neglecting structural inequalities. In this paper, we outline how a substantive approach to intersectional algorithmic fairness can reorient this research and practice. In particular, we propose Substantive Intersectional Algorithmic Fairness, extending Green's (2022) notion of substantive algorithmic fairness with insights from intersectional feminist theory. Aiming to provide as actionable guidance as possible, our approach is articulated as ten desiderata to guide the design, assessment, and deployment of algorithmic systems that address systemic inequities while mitigating harms to intersectionally marginalized communities. Rather than prescribing fixed operationalizations, these desiderata invite AI practitioners and experts to reflect on assumptions of neutrality, the use of protected attributes, the inclusion of multiply marginalized groups, and the transformative potential of algorithmic systems. By bridging computational and social science perspectives, the approach emphasizes that fairness cannot be separated from social context, and that in some cases, principled non-deployment may be necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17944v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s43681-025-00926-y</arxiv:DOI>
      <arxiv:journal_reference>AI Ethics 6, 90 (2026)</arxiv:journal_reference>
      <dc:creator>Marie Mirsch (RWTH Aachen University, Germany), Laila Wegner (Eindhoven University of Technology, The Netherlands), Jonas Strube (RWTH Aachen University, Germany), Carmen Leicht-Scholten (RWTH Aachen University, Germany)</dc:creator>
    </item>
    <item>
      <title>Reshaping Perception Through Technology: From Ancient Script to Large Language Models</title>
      <link>https://arxiv.org/abs/2602.02794</link>
      <description>arXiv:2602.02794v2 Announce Type: replace 
Abstract: As large language models reshape how we create and access information, questions arise about how to frame their role in human creative and cognitive life. We argue that AI is best understood not as artificial intelligence but as a new medium -- one that, like writing before it, reshapes perception and enables novel forms of creativity. Drawing on Marshall McLuhan's insight that "the medium is the massage," we trace a lineage of technologies -- from DNA and the nervous system to symbols, writing, and now LLMs -- that mold cognition through a shared logic of flexible unfolding and co-creation. We observe that as technologies become more externalized and decoupled from physiology, they introduce both greater creative potential and greater risk of inauthenticity and manipulation. This tension is acute with LLMs, but not unprecedented: ancient responses to writing reveal a recurring human tendency to project intelligence onto powerful new media. Rather than viewing AI as a competitor, we propose framing it as a medium that foregrounds artistic skills: aesthetic judgment, curation, and the articulation of vision. We discuss implications for education, creative practice, and how society might adapt to this new medium as it did to writing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02794v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Parham Pourdavood, Michael Jacob</dc:creator>
    </item>
    <item>
      <title>Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive</title>
      <link>https://arxiv.org/abs/2301.12534</link>
      <description>arXiv:2301.12534v5 Announce Type: replace-cross 
Abstract: Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voiced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.12534v5</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.18653/v1/2023.emnlp-main.713</arxiv:DOI>
      <dc:creator>Tharindu Cyril Weerasooriya, Sujan Dutta, Tharindu Ranasinghe, Marcos Zampieri, Christopher M. Homan, Ashiqur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>AI-Powered CPS-Enabled Vulnerable-User-Aware Urban Transportation Digital Twin: Methods and Applications</title>
      <link>https://arxiv.org/abs/2501.10396</link>
      <description>arXiv:2501.10396v3 Announce Type: replace-cross 
Abstract: We present methods and applications for the development of digital twins (DT) for urban traffic management. While the majority of studies on the DT focus on its ``eyes," which is the emerging sensing and perception like object detection and tracking, what really distinguishes the DT from a traditional simulator lies in its ``brain," the prediction and decision making capabilities of extracting patterns and making informed decisions from what has been seen and perceived. In order to add value to urban transportation management, DTs need to be powered by artificial intelligence and complement with low-latency high-bandwidth sensing and networking technologies, in other words, cyberphysical systems. This paper can be a pointer to help researchers and practitioners identify challenges and opportunities for the development of DTs; a bridge to initiate conversations across disciplines; and a road map to exploiting potentials of DTs for diverse urban transportation applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10396v3</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongjie Fu, Mehmet K. Turkcan, Mahshid Ghasemi, Zhaobin Mo, Chengbo Zang, Abhishek Adhikari, Zoran Kostic, Gil Zussman, Xuan Di</dc:creator>
    </item>
    <item>
      <title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.06969</link>
      <description>arXiv:2507.06969v4 Announce Type: replace-cross 
Abstract: Differentially private (DP) mechanisms are difficult to interpret and calibrate because existing methods for mapping standard privacy parameters to concrete privacy risks -- re-identification, attribute inference, and data reconstruction -- are both overly pessimistic and inconsistent. In this work, we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that bounds on attack success can take the same unified form across re-identification, attribute inference, and data reconstruction risks. Our unified bounds are (1) consistent across a multitude of attack settings, and (2) tunable, enabling practitioners to evaluate risk with respect to arbitrary, including worst-case, levels of baseline risk. Empirically, our results are tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated DP. As a result, calibrating noise using our bounds can reduce the required noise by 20% at the same risk level, which yields, e.g., an accuracy increase from 52% to 70% in a text classification task. Overall, this unifying perspective provides a principled framework for interpreting and calibrating the degree of protection in DP against specific levels of re-identification, attribute inference, or data reconstruction risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06969v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Jamie Hayes, Borja Balle, Flavio P. Calmon, Jean Louis Raisaro</dc:creator>
    </item>
    <item>
      <title>When Algorithms Meet Artists: Semantic Compression of Artists' Concerns in the Public AI-Art Debate</title>
      <link>https://arxiv.org/abs/2508.03037</link>
      <description>arXiv:2508.03037v3 Announce Type: replace-cross 
Abstract: Artists occupy a paradoxical position in generative AI: their work trains the models reshaping creative labor. We tested whether their concerns achieve proportional representation in public discourse shaping AI governance. Analyzing public AI-art discourse (news, podcasts, legal filings, research; 2013--2025) and projecting 1,259 survey-derived artist statements into this semantic space, we find stark compression: 95% of artist concerns cluster in 4 of 22 discourse topics, while 14 topics (62% of discourse) contain no artist perspective. This compression is selective - governance concerns (ownership, transparency) are 7x underrepresented; affective themes (threat, utility) show only 1.4x underrepresentation after style controls. The pattern indicates semantic, not stylistic, marginalization. These findings demonstrate a measurable representational gap: decision-makers relying on public discourse as a proxy for stakeholder priorities will systematically underweight those most affected. We introduce a consensus-based semantic projection methodology that is currently being validated across domains and generalizes to other stakeholder-technology contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03037v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariya Mukherjee-Gandhi, Oliver Muellerklein</dc:creator>
    </item>
    <item>
      <title>Building Interpretable Models for Moral Decision-Making</title>
      <link>https://arxiv.org/abs/2602.03351</link>
      <description>arXiv:2602.03351v2 Announce Type: replace-cross 
Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03351v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Goel, Aritra Das, Paras Chopra</dc:creator>
    </item>
  </channel>
</rss>
