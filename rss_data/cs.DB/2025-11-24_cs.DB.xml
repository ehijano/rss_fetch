<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Nov 2025 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>RAG-Driven Data Quality Governance for Enterprise ERP Systems</title>
      <link>https://arxiv.org/abs/2511.16700</link>
      <description>arXiv:2511.16700v1 Announce Type: new 
Abstract: Enterprise ERP systems managing hundreds of thousands of employee records face critical data quality challenges when human resources departments perform decentralized manual entry across multiple languages. We present an end-to-end pipeline combining automated data cleaning with LLM-driven SQL query generation, deployed on a production system managing 240,000 employee records over six months.
  The system operates in two integrated stages: a multi-stage cleaning pipeline that performs translation normalization, spelling correction, and entity deduplication during periodic synchronization from Microsoft SQL Server to PostgreSQL; and a retrieval-augmented generation framework powered by GPT-4o that translates natural-language questions in Turkish, Russian, and English into validated SQL queries. The query engine employs LangChain orchestration, FAISS vector similarity search, and few-shot learning with 500+ validated examples.
  Our evaluation demonstrates 92.5% query validity, 95.1% schema compliance, and 90.7\% semantic accuracy on 2,847 production queries. The system reduces query turnaround time from 2.3 days to under 5 seconds while maintaining 99.2% uptime, with GPT-4o achieving 46% lower latency and 68% cost reduction versus GPT-3.5. This modular architecture provides a reproducible framework for AI-native enterprise data governance, demonstrating real-world viability at enterprise scale with 4.3/5.0 user satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16700v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sedat Bin Vedat, Enes Kutay Yarkan, Meftun Akarsu, Recep Kaan Karaman, Arda Sar, \c{C}a\u{g}r{\i} \c{C}elikbilek, Sava\c{s} Sayg{\i}l{\i}</dc:creator>
    </item>
    <item>
      <title>LinkML: An Open Data Modeling Framework</title>
      <link>https://arxiv.org/abs/2511.16935</link>
      <description>arXiv:2511.16935v1 Announce Type: new 
Abstract: Scientific research relies on well-structured, standardized data; however, much of it is stored in formats such as free-text lab notebooks, non-standardized spreadsheets, or data repositories. This lack of structure challenges interoperability, making data integration, validation, and reuse difficult. LinkML (Linked Data Modeling Language) is an open framework that simplifies the process of authoring, validating, and sharing data. LinkML can describe a range of data structures, from flat, list-based models to complex, interrelated, and normalized models that utilize polymorphism and compound inheritance. It offers an approachable syntax that is not tied to any one technical architecture and can be integrated seamlessly with many existing frameworks. The LinkML syntax provides a standard way to describe schemas, classes, and relationships, allowing modelers to build well-defined, stable, and optionally ontology-aligned data structures. Once defined, LinkML schemas may be imported into other LinkML schemas. These key features make LinkML an accessible platform for interdisciplinary collaboration and a reliable way to define and share data semantics.
  LinkML helps reduce heterogeneity, complexity, and the proliferation of single-use data models while simultaneously enabling compliance with FAIR data standards. LinkML has seen increasing adoption in various fields, including biology, chemistry, biomedicine, microbiome research, finance, electrical engineering, transportation, and commercial software development. In short, LinkML makes implicit models explicitly computable and allows data to be standardized at its origin. LinkML documentation and code are available at linkml.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16935v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sierra A. T. Moxon (Lawrence Berkeley National Laboratory), Harold Solbrig (Johns Hopkins University), Nomi L. Harris (Lawrence Berkeley National Laboratory), Patrick Kalita (Lawrence Berkeley National Laboratory), Mark A. Miller (Lawrence Berkeley National Laboratory), Sujay Patil (Lawrence Berkeley National Laboratory), Kevin Schaper (University of North Carolina at Chapel Hill), Chris Bizon (University of North Carolina at Chapel Hill), J. Harry Caufield (Lawrence Berkeley National Laboratory), Silvano Cirujano Cuesta (Siemens AG), Corey Cox (University of North Carolina at Chapel Hill), Frank Dekervel (Kapernikov), Damion M. Dooley (Simon Fraser University), William D. Duncan (University of Florida), Tim Fliss (Allen Institute), Sarah Gehrke (University of North Carolina at Chapel Hill), Adam S. L. Graefe (Berlin Institute of Health), Harshad Hegde (GSK), AJ Ireland (Lawrence Berkeley National Laboratory), Julius O. B. Jacobsen (Lawrence Berkeley National Laboratory), Madan Krishnamurthy (University of North Carolina at Chapel Hill), Carlo Kroll (University of North Carolina at Chapel Hill), David Linke (Leibniz Institute for Catalysis), Ryan Ly (Lawrence Berkeley National Laboratory), Nicolas Matentzoglu (Semanticly), James A. Overton (Knocean Inc.), Jonny L. Saunders (UCLA), Deepak R. Unni (SIB Swiss Institute of Bioinformatics), Gaurav Vaidya (University of North Carolina at Chapel Hill), Wouter-Michiel A. M. Vierdag (European Molecular Biology Laboratory), LinkML Community Contributors (Lawrence Berkeley National Laboratory), Oliver Ruebel (Lawrence Berkeley National Laboratory), Christopher G. Chute (Johns Hopkins University), Matthew H. Brush (University of North Carolina at Chapel Hill), Melissa A. Haendel (University of North Carolina at Chapel Hill), Christopher J. Mungall (Lawrence Berkeley National Laboratory)</dc:creator>
    </item>
    <item>
      <title>Anomaly Pattern-guided Transaction Bug Testing in Relational Databases</title>
      <link>https://arxiv.org/abs/2511.17377</link>
      <description>arXiv:2511.17377v1 Announce Type: new 
Abstract: Concurrent transaction processing is a fundamental capability of Relational Database Management Systems (RDBMSs), widely utilized in applications requiring high levels of parallel user interaction, such as banking systems, e-commerce platforms, and telecommunications infrastructure. Isolation levels offer a configurable mechanism to manage the interaction between concurrent transactions, enabling varying degrees of consistency and performance trade-offs. These isolation guarantees are supported by all major RDBMSs. However, testing transaction behavior under different isolation levels remains a significant challenge due to two primary reasons. First, automatically generating test transactions that can effectively expose bugs in transaction handling logic is non-trivial, as such bugs are typically triggered under specific transactional constraints. Second, detecting logic anomalies in transaction outcomes is difficult because the correct execution results are often unknown for randomly generated transactions. To address these challenges, we propose an anomaly pattern-guided testing approach for uncovering transaction bugs in RDBMSs. Our solution tackles the first challenge by introducing a test case generation technique guided by predefined anomaly patterns, which increases the likelihood of exposing transactional bugs. For the second challenge, we present a two-phase detection process, involving explicit error detection and implicit error detection, to identify bugs in transaction execution. We have implemented our approach in a tool, APTrans, and evaluated it on three widely-used RDBMSs: MySQL, MariaDB, and OceanBase. APTrans successfully identified 13 previously unknown transaction-related bugs, 11 of which have been confirmed by the respective development teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17377v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huicong Xu, Shuang Liu, Xianyu Zhu, Qiyu Zhuang, Wei Lu, Xiaoyong Du</dc:creator>
    </item>
    <item>
      <title>CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection</title>
      <link>https://arxiv.org/abs/2511.16929</link>
      <description>arXiv:2511.16929v1 Announce Type: cross 
Abstract: Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16929v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Xue, Dan He, Fengmei Jin, Chen Zhang, Xiaofang Zhou</dc:creator>
    </item>
    <item>
      <title>AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale</title>
      <link>https://arxiv.org/abs/2511.17190</link>
      <description>arXiv:2511.17190v1 Announce Type: cross 
Abstract: For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \textbf{97.4\%} on Bird-Dev and \textbf{91.2\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \textbf{68.7\%} EX on Bird-Dev (better than CHESS) and \textbf{34.9\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \textbf{exceptional scalability}, \textbf{maintaining high recall}, \textbf{efficient token consumption}, and \textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17190v1</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Wang, Yuanlei Zheng, Zhenbiao Cao, Xiaojin Zhang, Zhongyu Wei, Pei Fu, Zhenbo Luo, Wei Chen, Xiang Bai</dc:creator>
    </item>
    <item>
      <title>Enumeration and updates for conjunctive linear algebra queries through expressibility</title>
      <link>https://arxiv.org/abs/2310.04118</link>
      <description>arXiv:2310.04118v4 Announce Type: replace-cross 
Abstract: Due to the importance of linear algebra and matrix operations in data analytics, there is significant interest in using relational query optimization and processing techniques for evaluating (sparse) linear algebra programs. In particular, in recent years close connections have been established between linear algebra programs and relational algebra that allow transferring optimization techniques of the latter to the former. In this paper, we ask ourselves which linear algebra programs in MATLANG correspond to the free-connex and q-hierarchical fragments of conjunctive first-order logic. Both fragments have desirable query processing properties: free-connex conjunctive queries support constant-delay enumeration after a linear-time preprocessing phase, and q-hierarchical conjunctive queries further allow constant-time updates. By characterizing the corresponding fragments of MATLANG, we hence identify the fragments of linear algebra programs that one can evaluate with constant-delay enumeration after linear-time preprocessing and with constant-time updates. To derive our results, we improve and generalize previous correspondences between MATLANG and relational algebra evaluated over semiring-annotated relations. In addition, we identify properties on semirings that allow to generalize the complexity bounds for free-connex and q-hierarchical conjunctive queries from Boolean annotations to general semirings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04118v4</guid>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mu\~noz, Cristian Riveros, Stijn Vansummeren</dc:creator>
    </item>
    <item>
      <title>Multi-variable Quantification of BDDs in External Memory using Nested Sweeping (Extended Paper)</title>
      <link>https://arxiv.org/abs/2408.14216</link>
      <description>arXiv:2408.14216v5 Announce Type: replace-cross 
Abstract: Previous research on the Adiar BDD package has been successful at designing algorithms capable of handling large Binary Decision Diagrams (BDDs) stored in external memory. To do so, it uses consecutive sweeps through the BDDs to resolve computations. Yet, this approach has kept algorithms for multi-variable quantification, the relational product, and variable reordering out of its scope.
  In this work, we address this by introducing the nested sweeping framework. Here, multiple concurrent sweeps pass information between eachother to compute the result. We have implemented the framework in Adiar and used it to create a new external memory multi-variable quantification algorithm. Compared to conventional depth-first implementations, Adiar with nested sweeping is able to solve more instances of our benchmarks and/or solve them faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14216v5</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffan Christ S{\o}lvsten, Jaco van de Pol</dc:creator>
    </item>
  </channel>
</rss>
