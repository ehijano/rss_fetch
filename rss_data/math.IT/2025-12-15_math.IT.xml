<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Dec 2025 03:47:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sphere Decoding Revisited</title>
      <link>https://arxiv.org/abs/2512.11195</link>
      <description>arXiv:2512.11195v1 Announce Type: new 
Abstract: In this paper, the paradigm of sphere decoding (SD) for solving the integer least square problem (ILS) is revisited, where extra degrees of freedom are introduced to exploit the decoding potential. Firstly, the equivalent sphere decoding (ESD) is proposed, which is essentially the same with the classic Fincke-Pohst sphere decoding but characterizes the sphere radius $D&gt;0$ with two new parameters named as initial searching size $K&gt;1$ and deviation factor $\sigma&gt;0$. By fixing $\sigma$ properly, we show that given the sphere radius $D\triangleq\sigma\sqrt{2\ln K}$, the complexity of ESD in terms of the number of visited nodes is upper bounded by $|S|&lt;nK$, thus resulting in an explicit and tractable decoding trade-off solely controlled by $K$. To the best of our knowledge, this is the first time that the complexity of sphere decoding is exactly specified, where considerable decoding potential can be explored from it. After that, two enhancement mechanisms named as normalized weighting and candidate protection are proposed to further upgrade the ESD algorithm. On one hand, given the same setups of $K$ and $\sigma$, a larger sphere radius is achieved, indicating a better decoding trade-off. On the other hand, the proposed ESD algorithm is generalized, which bridges suboptimal and optimal decoding performance through the flexible choice of $K$. Finally, further performance optimization and complexity reduction with respect to ESD are also derived, and the introduced tractable and flexible decoding trade-off is verified through large-scale MIMO detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11195v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCOMM.2023.3278732</arxiv:DOI>
      <arxiv:journal_reference>in IEEE Transactions on Communications, vol. 72, no. 1, pp. 85-100, Jan. 2024</arxiv:journal_reference>
      <dc:creator>Zheng Wang, Cong Ling, Shi Jin, Yongming Huang, Feifei Gao</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Equivalences Across Rate-Distortion, Quantization, and Decoding</title>
      <link>https://arxiv.org/abs/2512.11279</link>
      <description>arXiv:2512.11279v1 Announce Type: new 
Abstract: We propose a unified mathematical framework for rate-distortion theory, lattice quantization, and modern error-correcting codes by emphasizing their variational and convex-analytic structure. First, we establish a Gibbs-type variational formulation of the rate-distortion function and show that optimal test channels form an exponential family, with Fullback-Leibler divergence acting as a Bregman divergence. This yields a generalized Pythagorean theorem for projections and a Legendre duality that couples distortion constraints with inverse temperature parameters. Second, the reverse water-filling metaphor is extended to distributed lattice quantization, deriving distortion allocation bounds across eigenmodes of conditional covariance matrices. Third, inference is formalized as decoding by showing that belief propagation in LDPC ensembles and polarization in polar codes can be interpreted as recursive variational inference procedures. These results unify compression, quantization, and decoding as convex projections of continuous information onto discrete manifolds. Extensions to neural compression and quantum information are sketched as corollaries, illustrating the universality of the framework. Illustrative connections to other scientific fields are also presented. Finally, complementary numerical examples and scripts are located in the appendix</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11279v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno Macchiavello</dc:creator>
    </item>
    <item>
      <title>Refinements and Generalizations of the Shannon Lower Bound via Extensions of the Kraft Inequality</title>
      <link>https://arxiv.org/abs/2512.11322</link>
      <description>arXiv:2512.11322v1 Announce Type: new 
Abstract: We derive a few extended versions of the Kraft inequality for lossy compression, which pave the way to the derivation of several refinements and extensions of the well known Shannon lower bound in a variety of instances of rate-distortion coding. These refinements and extensions include sharper bounds for one-to-one codes and $D$-semifaithful codes, a Shannon lower bound for distortion measures based on sliding-window functions, and an individual-sequence counterpart of the Shannon lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11322v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neri Merhav</dc:creator>
    </item>
    <item>
      <title>AMBER: An Adaptive Multimodal Mask Transformer for Beam Prediction with Missing Modalities</title>
      <link>https://arxiv.org/abs/2512.11331</link>
      <description>arXiv:2512.11331v1 Announce Type: new 
Abstract: With the widespread adoption of millimeter-wave (mmWave) massive multi-input-multi-output (MIMO) in vehicular networks, accurate beam prediction and alignment have become critical for high-speed data transmission and reliable access. While traditional beam prediction approaches primarily rely on in-band beam training, recent advances have started to explore multimodal sensing to extract environmental semantics for enhanced prediction. However, the performance of existing multimodal fusion methods degrades significantly in real-world settings because they are vulnerable to missing data caused by sensor blockage, poor lighting, or GPS dropouts. To address this challenge, we propose AMBER ({A}daptive multimodal {M}ask transformer for {BE}am p{R}ediction), a novel end-to-end framework that processes temporal sequences of image, LiDAR, radar, and GPS data, while adaptively handling arbitrary missing-modality cases. AMBER introduces learnable modality tokens and a missing-modality-aware mask to prevent cross-modal noise propagation, along with a learnable fusion token and multihead attention to achieve robust modality-specific information distillation and feature-level fusion. Furthermore, a class-former-aided modality alignment (CMA) module and temporal-aware positional embedding are incorporated to preserve temporal coherence and ensure semantic alignment across modalities, facilitating the learning of modality-invariant and temporally consistent representations for beam prediction. Extensive experiments on the real-world DeepSense6G dataset demonstrate that AMBER significantly outperforms existing multimodal learning baselines. In particular, it maintains high beam prediction accuracy and robustness even under severe missing-modality scenarios, validating its effectiveness and practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11331v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyiming Wen, Binpu Shi, Min Li, Ming-Min Zhao, Min-Jian Zhao, Jiangzhou Wang</dc:creator>
    </item>
    <item>
      <title>Capacity-Achieving Codes with Inverse-Ackermann-Depth Encoders</title>
      <link>https://arxiv.org/abs/2512.11443</link>
      <description>arXiv:2512.11443v1 Announce Type: new 
Abstract: For any symmetric discrete memoryless channel with input and output alphabet of size $q$, where $q$ is a prime power, we prove that there exist error-correcting codes approaching channel capacity encodable by arithmetic circuits (with weighted addition gates) over $\mathbb{F}_q$ of size $O(n)$ and depth $\alpha(n)$, where $\alpha(n)$ is a version of the inverse Ackermann function. Our results suggest that certain capacity-achieving codes admit highly efficient encoding circuits that are both in linear size and of inverse-Ackermann depth. Our construction composes a linear code with constant rate and relative distance, based on the constructions of G\'{a}l, Hansen, Kouck\'{y}, Pudl\'{a}k, and Viola [IEEE Trans. Inform. Theory 59(10), 2013] and Drucker and Li [COCOON 2023], with an additional layer formed by a disperser graph whose edge weights are chosen uniformly at random.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11443v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Li</dc:creator>
    </item>
    <item>
      <title>Stable low-rank matrix recovery from 3-designs</title>
      <link>https://arxiv.org/abs/2512.11642</link>
      <description>arXiv:2512.11642v1 Announce Type: new 
Abstract: We study the recovery of low-rank Hermitian matrices from rank-one measurements obtained by uniform sampling from complex projective 3-designs, using nuclear-norm minimization. This framework includes phase retrieval as a special case via the PhaseLift method. In general, complex projective $t$-designs provide a practical means of partially derandomizing Gaussian measurement models. While near-optimal recovery guarantees are known for $4$-designs, and it is known that $2$-designs do not permit recovery with a subquadratic number of measurements, the case of $3$-designs has remained open. In this work, we close this gap by establishing recovery guarantees for (exact and approximate) $3$-designs that parallel the best-known results for $4$-designs. In particular, we derive bounds on the number of measurements sufficient for stable and robust low-rank recovery via nuclear-norm minimization. Our results are especially relevant in practice, as explicit constructions of $4$-designs are significantly more challenging than those of $3$-designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11642v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timm Gilles</dc:creator>
    </item>
    <item>
      <title>Uplink Rate Maximization for Pinching Antenna- Assisted Covert Backscatter Communication</title>
      <link>https://arxiv.org/abs/2512.10970</link>
      <description>arXiv:2512.10970v1 Announce Type: cross 
Abstract: The emerging pinching antenna (PA) technology enables flexible antenna positioning for creating line-of-sight (LoS) links, thus offering substantial potential to facilitate ambient signal-based backscatter communication (BSC). This paper investigates PA-assisted BSC for enhanced communication and covertness in the presence of a randomly distributed eavesdropper. An optimization problem is formulated to maximize the uplink covert transmission rate by jointly optimizing the transmit power and antenna positions while satisfying both communication reliability and covertness constraints. An alternative optimization (AO)-based framework is proposed to solve this problem. Numerical results demonstrate that the proposed PA-BSC effectively mitigates the double near-far problem, where energy harvesting and backscatter transmission degrade simultaneously due to distance disparities, thereby improving downlink energy harvesting and uplink data transmission while maintaining covertness performance under practical deployment scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10970v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yulei Wang, Yalin Liu, Yaru Fu, Yuanwei Liu</dc:creator>
    </item>
    <item>
      <title>A new group of transformations related to the Kullback-Leibler and R\'enyi divergences and universal classes of monotone measures of statistical complexity</title>
      <link>https://arxiv.org/abs/2512.11594</link>
      <description>arXiv:2512.11594v1 Announce Type: cross 
Abstract: In this work we introduce a family of transformations, named \textit{divergence transformations}, interpolating between any pair of probability density functions sharing the same support. We prove the remarkable property that the whole family of Kullback-Leibler and R\'enyi divergences evolves in a monotone way with respect to the transformation parameter. Moreover, fixing the reference density, we show that the divergence transformations enjoy a group structure and can be derived through the algebraic conjugation of the recently introduced differential-escort transformations and their relative counterparts.
  This algebraic structure allows us to deform any density function in such a way its divergence with respect a fixed reference density might also increase as much as possible. We also establish the monotonicity of composed measures involving the proper Kullback-Leibler and R\'enyi divergences as well as other recently introduced relative measures of moment and Fisher types.
  As applications, an approximation scheme of general density functions by simple functions is provided. In addition, we give a number of analytical and numerical examples of interest in both regimes of increasing and decreasing divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11594v1</guid>
      <category>math-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Razvan Gabriel Iagar, David Puertas-Centeno, Elio V. Toranzo</dc:creator>
    </item>
    <item>
      <title>A slightly improved upper bound for quantum statistical zero-knowledge</title>
      <link>https://arxiv.org/abs/2512.11597</link>
      <description>arXiv:2512.11597v1 Announce Type: cross 
Abstract: The complexity class Quantum Statistical Zero-Knowledge ($\mathsf{QSZK}$), introduced by Watrous (FOCS 2002) and later refined in Watrous (SICOMP, 2009), has the best known upper bound $\mathsf{QIP(2)} \cap \text{co-}\mathsf{QIP(2)}$, which was simplified following the inclusion $\mathsf{QIP(2)} \subseteq \mathsf{PSPACE}$ established in Jain, Upadhyay, and Watrous (FOCS 2009). Here, $\mathsf{QIP(2)}$ denotes the class of promise problems that admit two-message quantum interactive proof systems in which the honest prover is typically \textit{computationally unbounded}, and $\text{co-}\mathsf{QIP(2)}$ denotes the complement of $\mathsf{QIP(2)}$.
  We slightly improve this upper bound to $\mathsf{QIP(2)} \cap \text{co-}\mathsf{QIP(2)}$ with a quantum linear-space honest prover. A similar improvement also applies to the upper bound for the non-interactive variant $\mathsf{NIQSZK}$. Our main techniques are an algorithmic version of the Holevo-Helstrom measurement and the Uhlmann transform, both implementable in quantum linear space, implying polynomial-time complexity in the state dimension, using the recent space-efficient quantum singular value transformation of Le Gall, Liu, and Wang (CC, to appear).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11597v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Le Gall, Yupan Liu, Qisheng Wang</dc:creator>
    </item>
    <item>
      <title>From Quasi-Isometric Embeddings to Finite-Volume Property: A Theoretical Framework for Quantized Matrix Completion</title>
      <link>https://arxiv.org/abs/2311.05052</link>
      <description>arXiv:2311.05052v3 Announce Type: replace 
Abstract: We delve into the impact of memoryless scalar quantization on matrix completion. Our primary motivation for this research is to evaluate the recovery performance of nuclear norm minimization in handling quantized matrix problems without the use of any regularization terms such as those stemming from maximum likelihood estimation. We broaden our theoretical discussion to encompass the coarse quantization scenario with a dithering scheme, where the only available information for low-rank matrix recovery is a few-bit low-resolution data. We furnish theoretical guarantees for both scenarios: when access to dithers is available during the reconstruction process, and when we have access solely to the statistical properties of the dithers. Additionally, we conduct a comprehensive analysis of the effects of sign flips and prequantization noise on the recovery performance, particularly when the impact of sign flips is quantified using the well-known Hamming distance in the upper bound of recovery error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05052v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arian Eamaz, Farhang Yeganegi, Mojtaba Soltanalian</dc:creator>
    </item>
    <item>
      <title>DoDo-Code: an Efficient Levenshtein Distance Embedding-based Code for 4-ary IDS Channel</title>
      <link>https://arxiv.org/abs/2312.12717</link>
      <description>arXiv:2312.12717v3 Announce Type: replace 
Abstract: With the emergence of new storage and communication methods, the insertion, deletion, and substitution (IDS) channel has attracted considerable attention. However, many topics on the IDS channel and the associated Levenshtein distance remain open, making the invention of a novel IDS-correcting code a hard task. Furthermore, current studies on single-IDS-correcting code misalign with the requirements of applications which necessitates the correcting of multiple errors. Compromise solutions have involved shortening codewords to reduce the chance of multiple errors. However, the code rates of existing codes are poor at short lengths, diminishing the overall storage density. In this study, a novel method is introduced for designing high-code-rate single-IDS-correcting codewords through deep Levenshtein distance embedding. A deep learning model is utilized to project the sequences into embedding vectors that preserve the Levenshtein distances between the original sequences. This embedding space serves as a proxy for the complex Levenshtein domain, within which algorithms for codeword search and segment correcting is developed. While the concept underpinning this approach is straightforward, it bypasses the mathematical challenges typically encountered in code design. The proposed method results in a code rate that outperforms existing combinatorial solutions, particularly for designing short-length codewords.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12717v3</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alan J. X. Guo, Sihan Sun, Xiang Wei, Mengyi Wei, Xin Chen</dc:creator>
    </item>
    <item>
      <title>A Family of LZ78-based Universal Sequential Probability Assignments</title>
      <link>https://arxiv.org/abs/2410.06589</link>
      <description>arXiv:2410.06589v2 Announce Type: replace 
Abstract: We propose and study a family of universal sequential probability assignments on individual sequences, based on the incremental parsing procedure of the Lempel-Ziv (LZ78) compression algorithm. We show that the normalized log loss under any of these models converges to the normalized LZ78 codelength, uniformly over all individual sequences. To establish the universality of these models, we consolidate a set of results from the literature relating finite-state compressibility to optimal log-loss under Markovian and finite-state models. We also consider some theoretical and computational properties of these models when viewed as probabilistic sources. Finally, we present experimental results showcasing the potential benefit of using this family -- as models and as sources -- for compression, generation, and classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06589v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naomi Sagan, Tsachy Weissman</dc:creator>
    </item>
    <item>
      <title>On the hull-variation problem of equivalent vector rank metric codes</title>
      <link>https://arxiv.org/abs/2505.08506</link>
      <description>arXiv:2505.08506v2 Announce Type: replace 
Abstract: The intersection of a linear code with its dual is called the hull of the code. It is known that, for classical linear codes under the Hamming-metric, the dimension of the hull can be reduced up to equivalence. This phenomenon leads to the so-called hull-variation problem formulated by Hao Chen in 2023. In this paper, we consider the analogous problem for vector rank-metric codes, along with their associated matrix codes and extended block codes. Our results include the fact that every vector rank-metric code over any finite field $\mathbb{F}_q$, in particular when $q=2$ or $q=3$, is equivalent to an LCD code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08506v2</guid>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duy Ho, Trygve Johnsen</dc:creator>
    </item>
    <item>
      <title>E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion</title>
      <link>https://arxiv.org/abs/2509.19312</link>
      <description>arXiv:2509.19312v2 Announce Type: replace-cross 
Abstract: This paper investigates multimodal semantic non-orthogonal transmission and fusion in hybrid analog-digital massive multiple-input multiple-output (MIMO). A Transformer-based cross-modal source-channel semantic-aware network (CSC-SA-Net) framework is conceived, where channel state information (CSI) reference signal (RS), feedback, analog-beamforming/combining, and baseband semantic processing are data-driven end-to-end (E2E) optimized at the base station (BS) and user equipments (UEs). CSC-SA-Net comprises five sub-networks: BS-side CSI-RS network (BS-CSIRS-Net), UE-side channel semantic-aware network (UE-CSANet), BS-CSANet, UE-side multimodal semantic fusion network (UE-MSFNet), and BS-MSFNet. Specifically, we firstly E2E train BS-CSIRS-Net, UE-CSANet, and BS-CSANet to jointly design CSI-RS, feedback, analog-beamforming/combining with maximum {\emph{physical-layer's}} spectral-efficiency. Meanwhile, we E2E train UE-MSFNet and BS-MSFNet for optimizing {\emph{application-layer's}} source semantic downstream tasks. On these pre-trained models, we further integrate application-layer semantic processing with physical-layer tasks to E2E train five subnetworks. Extensive simulations show that the proposed CSC-SA-Net outperforms traditional separated designs, revealing the advantage of cross-modal channel-source semantic fusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19312v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghui Wu, Zhen Gao</dc:creator>
    </item>
    <item>
      <title>Statistical physics of deep learning: Optimal learning of a multi-layer perceptron near interpolation</title>
      <link>https://arxiv.org/abs/2510.24616</link>
      <description>arXiv:2510.24616v3 Announce Type: replace-cross 
Abstract: For four decades statistical physics has been providing a framework to analyse neural networks. A long-standing question remained on its capacity to tackle deep learning models capturing rich feature learning effects, thus going beyond the narrow networks or kernel methods analysed until now. We positively answer through the study of the supervised learning of a multi-layer perceptron. Importantly, (i) its width scales as the input dimension, making it more prone to feature learning than ultra wide networks, and more expressive than narrow ones or ones with fixed embedding layers; and (ii) we focus on the challenging interpolation regime where the number of trainable parameters and data are comparable, which forces the model to adapt to the task. We consider the matched teacher-student setting. Therefore, we provide the fundamental limits of learning random deep neural network targets and identify the sufficient statistics describing what is learnt by an optimally trained network as the data budget increases. A rich phenomenology emerges with various learning transitions. With enough data, optimal performance is attained through the model's "specialisation" towards the target, but it can be hard to reach for training algorithms which get attracted by sub-optimal solutions predicted by the theory. Specialisation occurs inhomogeneously across layers, propagating from shallow towards deep ones, but also across neurons in each layer. Furthermore, deeper targets are harder to learn. Despite its simplicity, the Bayes-optimal setting provides insights on how the depth, non-linearity and finite (proportional) width influence neural networks in the feature learning regime that are potentially relevant in much more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24616v3</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk</dc:creator>
    </item>
  </channel>
</rss>
