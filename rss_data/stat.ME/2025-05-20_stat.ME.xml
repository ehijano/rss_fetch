<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 May 2025 01:50:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Theory: Multidimensional Space of Events</title>
      <link>https://arxiv.org/abs/2505.11566</link>
      <description>arXiv:2505.11566v1 Announce Type: new 
Abstract: This paper extends Bayesian probability theory by developing a multidimensional space of events (MDSE) theory that accounts for mutual influences between events and hypotheses sets. While traditional Bayesian approaches assume conditional independence between certain variables, real-world systems often exhibit complex interdependencies that limit classical model applicability. Building on established probabilistic foundations, our approach introduces a mathematical formalism for modeling these complex relationships. We developed the MDSE theory through rigorous mathematical derivation and validated it using three complementary methodologies: analytical proofs, computational simulations, and case studies drawn from diverse domains. Results demonstrate that MDSE successfully models complex dependencies with 15-20% improved prediction accuracy compared to standard Bayesian methods when applied to datasets with high interdimensionality. This theory particularly excels in scenarios with over 50 interrelated variables, where traditional methods show exponential computational complexity growth while MDSE maintains polynomial scaling. Our findings indicate that MDSE provides a viable mathematical foundation for extending Bayesian reasoning to complex systems while maintaining computational tractability. This approach offers practical applications in engineering challenges including risk assessment, resource optimization, and forecasting problems where multiple interdependent factors must be simultaneously considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11566v1</guid>
      <category>stat.ME</category>
      <category>math.LO</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sergii Kavun</dc:creator>
    </item>
    <item>
      <title>Covariate-moderated Empirical Bayes Matrix Factorization</title>
      <link>https://arxiv.org/abs/2505.11639</link>
      <description>arXiv:2505.11639v1 Announce Type: new 
Abstract: Matrix factorization is a fundamental method in statistics and machine learning for inferring and summarizing structure in multivariate data. Modern data sets often come with ``side information'' of various forms (images, text, graphs) that can be leveraged to improve estimation of the underlying structure. However, existing methods that leverage side information are limited in the types of data they can incorporate, and they assume specific parametric models. Here, we introduce a novel method for this problem, covariate-moderated
  empirical Bayes matrix factorization (cEBMF). cEBMF is a modular framework that accepts any type of side information that is processable by a probabilistic model or neural network. The cEBMF framework can accommodate different assumptions and constraints on the factors through the use of different priors, and it adapts these priors to the data. We demonstrate the benefits of cEBMF in simulations and in analyses of spatial transcriptomics and MovieLens data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11639v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William R. P. Denault, Karl Tayeb, Peter Carbonetto, Jason Willwerscheid, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>Model-Based Clustering with Sequential Outlier Identification using the Distribution of Mahalanobis Distances</title>
      <link>https://arxiv.org/abs/2505.11668</link>
      <description>arXiv:2505.11668v1 Announce Type: new 
Abstract: The presence of outliers can prevent clustering algorithms from accurately determining an appropriate group structure within a data set. We present outlierMBC, a model-based approach for sequentially removing outliers and clustering the remaining observations. Our method identifies outliers one at a time while fitting a multivariate Gaussian mixture model to data. Since it can be difficult to classify observations as outliers without knowing what the correct cluster structure is a priori, and the presence of outliers interferes with the process of modelling clusters correctly, we use an iterative method to identify outliers one by one. At each iteration, outlierMBC removes the observation with the lowest density and fits a Gaussian mixture model to the remaining data. The method continues to remove potential outliers until a pre-set maximum number of outliers is reached, then retrospectively identifies the optimal number of outliers. To decide how many outliers to remove, it uses the fact that the squared sample Mahalanobis distances of Gaussian distributed observations are Beta distributed when scaled appropriately. outlierMBC chooses the number of outliers which minimises a dissimilarity between this theoretical Beta distribution and the observed distribution of the scaled squared sample Mahalanobis distances. This means that our method both clusters the data using a Gaussian mixture model and implements a model-based procedure to identify the optimal outliers to remove without requiring the number of outliers to be pre-specified. Unlike leading methods in the literature, outlierMBC does not assume that the outliers follow a known distribution or that the number of outliers can be pre-specified. Moreover, outlierMBC performs strongly compared to these algorithms when applied to a range of simulated and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11668v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ult\'an P. Doherty, Paul D. McNicholas, Arthur White</dc:creator>
    </item>
    <item>
      <title>BLOG: Bayesian Longitudinal Omics with Group Constraints</title>
      <link>https://arxiv.org/abs/2505.11673</link>
      <description>arXiv:2505.11673v1 Announce Type: new 
Abstract: Clinical investigators are increasingly interested in discovering computational biomarkers from short-term longitudinal omics data sets. This work focuses on Bayesian regression and variable selection for longitudinal omics datasets, which can quantify uncertainty and control false discovery. In our univariate approach, Zellner's $g$ prior is used with two different options of the tuning parameter $g$: $g=\sqrt{n}$ and a $g$ that minimizes Stein's unbiased risk estimate (SURE). Bayes Factors were used to quantify uncertainty and control for false discovery. In the multivariate approach, we use Bayesian Group LASSO with a spike and slab prior for group variable selection. In both approaches, we use the first difference ($\Delta$) scale of longitudinal predictor and the response. These methods work together to enhance our understanding of biomarker identification, improving inference and prediction. We compare our method against commonly used linear mixed effect models on simulated data and real data from a Tuberculosis (TB) study on metabolite biomarker selection. With an automated selection of hyperparameters, the Zellner's $g$ prior approach correctly identifies target metabolites with high specificity and sensitivity across various simulation and real data scenarios. The Multivariate Bayesian Group Lasso spike and slab approach also correctly selects target metabolites across various simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11673v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Livia Popa, Sumanta Basu, Myung Hee Lee, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Robust outlier detection for heterogeneous distributions applicable to censoring in functional MRI</title>
      <link>https://arxiv.org/abs/2505.11806</link>
      <description>arXiv:2505.11806v1 Announce Type: new 
Abstract: Functional magnetic resonance imaging (fMRI) data are prone to intense "burst" noise artifacts due to head movements and other sources. Such volumes can be considered as high-dimensional outliers that can be identified using statistical outlier detection techniques, which allows for controlling the false positive rate. Previous work has used dimension reduction and multivariate outlier detection techniques, including the use of robust minimum covariance determinant (MCD) distances. Under Gaussianity, the distribution of these robust distances can be approximated, and an upper quantile of that distribution can be used to identify outlying volumes. Unfortunately, the Gaussian assumption is unrealistic for fMRI data in this context. One way to address this is to transform the data to Normality. A limitation of existing robust methods for this purpose, such as robust Box-Cox and Yeo-Johnson transformations, is that they can deal with skew but not heavy or light tails. Here, we develop a novel robust method for transformation to central Normality based on the highly flexible sinh-arcsinh (SHASH) family of distributions. To avoid the influence of outliers, it is crucial to initialize the outlier labels with a high degree of sensitivity. For this purpose, we consider a commonplace robust z-score approach, and a modified isolation forest (iForest) approach, a popular technique for anomaly detection in machine learning. Through extensive simulation studies, we find that our proposed SHASH transformation initialized using iForest clearly outperforms benchmark methods in a variety of settings, including skewed and heavy tailed distributions, and light to heavy outlier contamination. We also apply the proposed techniques to several example datasets and find this combination to have consistently strong performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11806v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saranjeet Singh Saluja, Fatma Parlak, Damon Pham, Amanda Mejia</dc:creator>
    </item>
    <item>
      <title>Model-X Change-Point Detection of Conditional Distribution</title>
      <link>https://arxiv.org/abs/2505.12023</link>
      <description>arXiv:2505.12023v2 Announce Type: new 
Abstract: The dynamic nature of many real-world systems can lead to temporal outcome model shifts, causing a deterioration in model accuracy and reliability over time. This requires change-point detection on the outcome models to guide model retraining and adjustments. However, inferring the change point of conditional models is more prone to loss of validity or power than classic detection problems for marginal distributions. This is due to both the temporal covariate shift and the complexity of the outcome model. To address these challenges, we propose a novel model-X Conditional Random Testing (CRT) method computationally enhanced with latent mixture model (LMM) distillation for simultaneous change-point detection and localization of the conditional outcome model. Built upon the model-X framework, our approach can effectively adjust for the potential bias caused by the temporal covariate shift and allow the flexible use of general machine learning methods for outcome modeling. It preserves good validity against complex or erroneous outcome models, even with imperfect knowledge of the temporal covariate shift learned from some auxiliary unlabeled data. Moreover, the incorporation of LMM distillation significantly reduces the computational burden of the CRT by eliminating the need for repeated complex model refitting in its resampling procedure and preserves the statistical validity and power well. Theoretical validity of the proposed method is justified. Extensive simulation studies and a real-world example demonstrate the statistical effectiveness and computational scalability of our method as well as its significant improvements over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12023v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwen Huang, Yan Dong, Mengying Yan, Ziye Tian, Chuan Hong, Doudou Zhou, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Cyclic-Shift Sparse Kronecker Tensor Classifier for Signal-Region Detection in Neuroimaging</title>
      <link>https://arxiv.org/abs/2505.12113</link>
      <description>arXiv:2505.12113v1 Announce Type: new 
Abstract: This study proposes a cyclic-shift logistic sparse Kronecker product decomposition (SKPD) model for high-dimensional tensor data, enhancing the SKPD framework with a cyclic-shift mechanism for binary classification. The method enables interpretable and scalable analysis of brain MRI data, detecting disease-relevant regions through a structured low-rank factorization. By incorporating a second spatially shifted view of the data, the cyclic-shift logistic SKPD improves robustness to misalignment across subjects, a common challenge in neuroimaging. We provide asymptotic consistency guarantees under a restricted isometry condition adapted to logistic loss. Simulations confirm the model's ability to recover spatial signals under noise and identify optimal patch sizes for factor decomposition. Application to OASIS-1 and ADNI-1 datasets demonstrates that the model achieves strong classification accuracy and localizes estimated coefficients in clinically relevant brain regions, such as the hippocampus. A data-driven slice selection strategy further improves interpretability in 2D projections. The proposed framework offers a principled, interpretable, and computationally efficient tool for neuroimaging-based disease diagnosis, with potential extensions to multi-class settings and more complex transformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12113v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hsin-Hsiung Huang, Yuh-Haur Chen, Teng Zhang</dc:creator>
    </item>
    <item>
      <title>Counterfactual Q Learning via the Linear Buckley James Method for Longitudinal Survival Data</title>
      <link>https://arxiv.org/abs/2505.12159</link>
      <description>arXiv:2505.12159v1 Announce Type: new 
Abstract: Treatment strategies are critical in healthcare, particularly when outcomes are subject to censoring. This study introduces the Counterfactual Buckley-James Q-Learning framework, which integrates the Buckley-James method with reinforcement learning to address challenges posed by censored survival data. The Buckley-James method imputes censored survival times via conditional expectations based on observed data, offering a robust mechanism for handling incomplete outcomes. By incorporating these imputed values into a counterfactual Q-learning framework, the proposed method enables the estimation and comparison of potential outcomes under different treatment strategies. This facilitates the identification of optimal dynamic treatment regimes that maximize expected survival time. Through extensive simulation studies, the method demonstrates robust performance across various sample sizes and censoring scenarios, including right censoring and missing at random (MAR). Application to real-world clinical trial data further highlights the utility of this approach in informing personalized treatment decisions, providing an interpretable and reliable tool for optimizing survival outcomes in complex clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12159v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Lee, Jong-Min Kim</dc:creator>
    </item>
    <item>
      <title>Reliable fairness auditing with semi-supervised inference</title>
      <link>https://arxiv.org/abs/2505.12181</link>
      <description>arXiv:2505.12181v1 Announce Type: new 
Abstract: Machine learning (ML) models often exhibit bias that can exacerbate inequities in biomedical applications. Fairness auditing, the process of evaluating a model's performance across subpopulations, is critical for identifying and mitigating these biases. However, such audits typically rely on large volumes of labeled data, which are costly and labor-intensive to obtain. To address this challenge, we introduce $\textit{Infairness}$, a unified framework for auditing a wide range of fairness criteria using semi-supervised inference. Our approach combines a small labeled dataset with a large unlabeled dataset by imputing missing outcomes via regression with carefully selected nonlinear basis functions. We show that our proposed estimator is (i) consistent regardless of whether the ML or imputation models are correctly specified and (ii) more efficient than standard supervised estimation with the labeled data when the imputation model is correctly specified. Through extensive simulations, we also demonstrate that Infairness consistently achieves higher precision than supervised estimation. In a real-world application of phenotyping depression from electronic health records data, Infairness reduces variance by up to 64% compared to supervised estimation, underscoring its value for reliable fairness auditing with limited labeled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12181v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianhui Gao, Jessica Gronsbell</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Harm Rate via Partitioning</title>
      <link>https://arxiv.org/abs/2505.12209</link>
      <description>arXiv:2505.12209v1 Announce Type: new 
Abstract: In causal inference with binary outcomes, there is a growing interest in estimation of treatment harm rate (THR), which is a measure of treatment risk and reveals treatment effect heterogeneity in a subpopulation. The THR is generally non-identifiable even for randomized controlled trials (RCTs), and existing works focus primarily on the estimation of the THR under either untestable identification or ambiguous model assumptions. We develop a class of partitioning-based bounds for the THR based on data from RCTs with two distinct features: Our proposed bounds effectively use available auxiliary covariates information and the bounds can be consistently estimated without relying on any untestable or ambiguous model assumptions. Finite sample performances of our proposed interval estimators along with a conservatively extended confidence interval for the THR are evaluated through Monte Carlo simulation studies. An application of the proposed methods to the ACTG 175 data is presented. A Python package named partbte for the partitioning-based algorithm has been developed and is available on https://github.com/w62liang/partition-te.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12209v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Liang, Changbao Wu</dc:creator>
    </item>
    <item>
      <title>A Hybrid Prior Bayesian Method for Combining Domestic Real-World Data and Overseas Data in Global Drug Development</title>
      <link>https://arxiv.org/abs/2505.12308</link>
      <description>arXiv:2505.12308v1 Announce Type: new 
Abstract: Background Hybrid clinical trial design integrates randomized controlled trials (RCTs) with real-world data (RWD) to enhance efficiency through dynamic incorporation of external data. Existing methods like the Meta-Analytic Predictive Prior (MAP) inadequately control data heterogeneity, adjust baseline discrepancies, or optimize dynamic borrowing proportions, introducing bias and limiting applications in bridging trials and multi-regional clinical trials (MRCTs). Objective This study proposes a novel hybrid Bayesian framework (EQPS-rMAP) to address heterogeneity and bias in multi-source data integration, validated through simulations and retrospective case analyses of risankizumab's efficacy in moderate-to-severe plaque psoriasis. Design and Methods EQPS-rMAP eliminates baseline covariate discrepancies via propensity score stratification, constructs stratum-specific MAP priors to dynamically adjust external data weights, and introduces equivalence probability weights to quantify data conflict risks. Performance was evaluated across six simulated scenarios (heterogeneity differences, baseline shifts) and real-world case analyses, comparing it with traditional methods (MAP, PSMAP, EBMAP) on estimation bias, type I error control, and sample size requirements. Results Simulations show EQPS-rMAP maintains estimation robustness under significant heterogeneity while reducing sample size demands and enhancing trial efficiency. Case analyses confirm superior external bias control and accuracy compared to conventional approaches. Conclusion and Significance EQPS-rMAP provides empirical evidence for hybrid clinical designs. By resolving baseline-heterogeneity conflicts through adaptive mechanisms, it enables reliable integration of external and real-world data in bridging trials, MRCTs, and post-marketing studies, broadening applicability without compromising rigor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12308v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keer Chen, Zengyue Zheng, Pengfei Zhu, Shuping Jiang, Nan Li, Jumin Deng, Pingyan Chen, Zhenyu Wu, Ying Wu</dc:creator>
    </item>
    <item>
      <title>Truncated Gaussian copula principal component analysis with application to pediatric acute lymphoblastic leukemia patients' gut microbiome</title>
      <link>https://arxiv.org/abs/2505.12483</link>
      <description>arXiv:2505.12483v1 Announce Type: new 
Abstract: Increasing epidemiologic evidence suggests that the diversity and composition of the gut microbiome can predict infection risk in cancer patients. Infections remain a major cause of morbidity and mortality during chemotherapy. Analyzing microbiome data to identify associations with infection pathogenesis for proactive treatment has become a critical research focus. However, the high-dimensional nature of the data necessitates the use of dimension-reduction methods to facilitate inference and interpretation. Traditional dimension reduction methods, which assume Gaussianity, perform poorly with skewed and zero-inflated microbiome data. To address these challenges, we propose a semiparametric principal component analysis (PCA) method based on a truncated latent Gaussian copula model that accommodates both skewness and zero inflation. Simulation studies demonstrate that the proposed method outperforms existing approaches by providing more accurate estimates of scores and loadings across various copula transformation settings. We apply our method, along with competing approaches, to gut microbiome data from pediatric patients with acute lymphoblastic leukemia. The principal scores derived from the proposed method reveal the strongest associations between pre-chemotherapy microbiome composition and adverse events during subsequent chemotherapy, offering valuable insights for improving patient outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12483v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Wang, Yang Ni, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Modeling Nonstationary Extremal Dependence via Deep Spatial Deformations</title>
      <link>https://arxiv.org/abs/2505.12548</link>
      <description>arXiv:2505.12548v1 Announce Type: new 
Abstract: Modeling nonstationarity that often prevails in extremal dependence of spatial data can be challenging, and typically requires bespoke or complex spatial models that are difficult to estimate. Inference for stationary and isotropic models is considerably easier, but the assumptions that underpin these models are rarely met by data observed over large or topographically complex domains. A possible approach for accommodating nonstationarity in a spatial model is to warp the spatial domain to a latent space where stationarity and isotropy can be reasonably assumed. Although this approach is very flexible, estimating the warping function can be computationally expensive, and the transformation is not always guaranteed to be bijective, which may lead to physically unrealistic transformations when the domain folds onto itself. We overcome these challenges by developing deep compositional spatial models to capture nonstationarity in extremal dependence. Specifically, we focus on modeling high threshold exceedances of process functionals by leveraging efficient inference methods for limiting $r$-Pareto processes. A detailed high-dimensional simulation study demonstrates the superior performance of our model in estimating the warped space. We illustrate our method by modeling UK precipitation extremes and show that we can efficiently estimate the extremal dependence structure of data observed at thousands of locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12548v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanjie Shao, Jordan Richards, Raphael Huser</dc:creator>
    </item>
    <item>
      <title>Extended semi-Latin squares for use in field and glasshouse trials</title>
      <link>https://arxiv.org/abs/2505.12569</link>
      <description>arXiv:2505.12569v1 Announce Type: new 
Abstract: Semi-Latin squares have been extensively studied. They can be interpreted as a special case of latinized block designs where the number of columns is equal to the number of replicates in the design. Latinized row-column designs are frequently used in field and glasshouse trials when replicates are contiguous. These designs allow for the efficient adjustment of row and column effects within replicates. Here we define extended semi-Latin squares as a special case of latinized row-column designs and investigate optimality using the average efficiency factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12569v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. R. Williams</dc:creator>
    </item>
    <item>
      <title>Double machine learning to estimate the effects of multiple treatments and their interactions</title>
      <link>https://arxiv.org/abs/2505.12617</link>
      <description>arXiv:2505.12617v1 Announce Type: new 
Abstract: Causal inference literature has extensively focused on binary treatments, with relatively fewer methods developed for multi-valued treatments. In particular, methods for multiple simultaneously assigned treatments remain understudied despite their practical importance. This paper introduces two settings: (1) estimating the effects of multiple treatments of different types (binary, categorical, and continuous) and the effects of treatment interactions, and (2) estimating the average treatment effect across categories of multi-valued regimens. To obtain robust estimates for both settings, we propose a class of methods based on the Double Machine Learning (DML) framework. Our methods are well-suited for complex settings of multiple treatments/regimens, using machine learning to model confounding relationships while overcoming regularization and overfitting biases through Neyman orthogonality and cross-fitting. To our knowledge, this work is the first to apply machine learning for robust estimation of interaction effects in the presence of multiple treatments. We further establish the asymptotic distribution of our estimators and derive variance estimators for statistical inference. Extensive simulations demonstrate the performance of our methods. Finally, we apply the methods to study the effect of three treatments on HIV-associated kidney disease in an adult HIV cohort of 2455 participants in Nigeria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12617v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingyan Xiang, Yubai Yuan, Dongyuan Song, Usman J. Wudil, Muktar H. Aliyu, C. William Wester, Bryan E. Shepherd</dc:creator>
    </item>
    <item>
      <title>Pseudo-Likelihood Ratio Screening based on Network Data with Applications</title>
      <link>https://arxiv.org/abs/2505.12695</link>
      <description>arXiv:2505.12695v1 Announce Type: new 
Abstract: Social network platforms today generate vast amounts of data, including network structures and a large number of user-defined tags, which reflect users' interests. The dimensionality of these personalized tags can be ultra-high, posing challenges for model analysis in targeted preference analysis. Traditional categorical feature screening methods overlook the network structure, which can lead to incorrect feature set and suboptimal prediction accuracy. This study focuses on feature screening for network-involved preference analysis based on ultra-high-dimensional categorical tags. We introduce the concepts of self-related features and network-related features, defined as those directly related to the response and those related to the network structure, respectively. We then propose a pseudo-likelihood ratio feature screening procedure that identifies both types of features. Theoretical properties of this procedure under different scenarios are thoroughly investigated. Extensive simulations and real data analysis on Sina Weibo validate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12695v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Hu, Danyang Huang, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Deep Generative Modeling with Spatial and Network Images: An Explainable AI (XAI) Approach</title>
      <link>https://arxiv.org/abs/2505.12743</link>
      <description>arXiv:2505.12743v1 Announce Type: new 
Abstract: This article addresses the challenge of modeling the amplitude of spatially indexed low frequency fluctuations (ALFF) in resting state functional MRI as a function of cortical structural features and a multi-task coactivation network in the Adolescent Brain Cognitive Development (ABCD) Study. It proposes a generative model that integrates effects of spatially-varying inputs and a network-valued input using deep neural networks to capture complex non-linear and spatial associations with the output. The method models spatial smoothness, accounts for subject heterogeneity and complex associations between network and spatial images at different scales, enables accurate inference of each images effect on the output image, and allows prediction with uncertainty quantification via Monte Carlo dropout, contributing to one of the first Explainable AI (XAI) frameworks for heterogeneous imaging data. The model is highly scalable to high-resolution data without the heavy pre-processing or summarization often required by Bayesian methods. Empirical results demonstrate its strong performance compared to existing statistical and deep learning methods. We applied the XAI model to the ABCD data which revealed associations between cortical features and ALFF throughout the entire brain. Our model performed comparably to existing methods in predictive accuracy but provided superior uncertainty quantification and faster computation, demonstrating its effectiveness for large-scale neuroimaging analysis. Open-source software in Python for XAI is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12743v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler</dc:creator>
    </item>
    <item>
      <title>Adaptive Inference through Bayesian and Inverse Bayesian Inference with Symmetry-Bias in Nonstationary Environments</title>
      <link>https://arxiv.org/abs/2505.12796</link>
      <description>arXiv:2505.12796v2 Announce Type: new 
Abstract: This study introduces a novel inference framework, designated as Bayesian and inverse Bayesian (BIB) inference, which concurrently performs both conventional and inverse Bayesian updates by integrating symmetry bias into Bayesian inference. The effectiveness of the model was evaluated through a sequential estimation task involving observations sampled from a Gaussian distribution with a stochastically time-varying mean. Conventional Bayesian inference entails a fundamental trade-off between adaptability to abrupt environmental shifts and estimation accuracy during stable intervals. The BIB framework addresses this limitation by dynamically modulating the learning rate through inverse Bayesian updates, thereby enhancing adaptive flexibility. The BIB model generated spontaneous bursts in the learning rate during sudden environmental transitions, transiently entering a high-sensitivity state to accommodate incoming data. This intermittent burst-relaxation pattern functions as a dynamic mechanism that balances adaptability and accuracy. Further analysis of burst interval distributions demonstrated that the BIB model consistently produced power-law distributions under diverse conditions. Such robust scaling behavior, absent in conventional Bayesian inference, appears to emerge from a self-regulatory mechanism driven by inverse Bayesian updates. These results present a novel computational perspective on scale-free phenomena in natural systems and offer implications for designing adaptive inference systems in nonstationary environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12796v2</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuji Shinohara, Daiki Morita, Hayato Hirai, Ryosuke Kuribayashi, Nobuhito Manome, Toru Moriyama, Yoshihiro Nakajima, Yukio-Pegio Gunji, Ung-il Chung</dc:creator>
    </item>
    <item>
      <title>Discrete-time Markov chains with random observation times</title>
      <link>https://arxiv.org/abs/2505.12971</link>
      <description>arXiv:2505.12971v1 Announce Type: new 
Abstract: We propose a new approach for estimating the finite dimensional transition matrix of a Markov chain using a large number of independent sample paths observed at random times. The sample paths may be observed as few as two times, and the transitions are allowed to depend on covariates. Simple and easy to update kernel estimates are proposed, and their uniform convergence rates are derived. Simulation experiments show that our estimation approach performs well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12971v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daphne Aurouet, Valentin Patilea</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for the Transportability of Population-Level Causal Measures</title>
      <link>https://arxiv.org/abs/2505.13104</link>
      <description>arXiv:2505.13104v1 Announce Type: new 
Abstract: Generalization methods offer a powerful solution to one of the key drawbacks of randomized controlled trials (RCTs): their limited representativeness. By enabling the transport of treatment effect estimates to target populations subject to distributional shifts, these methods are increasingly recognized as the future of meta-analysis, the current gold standard in evidence-based medicine. Yet most existing approaches focus on the risk difference, overlooking the diverse range of causal measures routinely reported in clinical research. Reporting multiple effect measures-both absolute (e.g., risk difference, number needed to treat) and relative (e.g., risk ratio, odds ratio)-is essential to ensure clinical relevance, policy utility, and interpretability across contexts. To address this gap, we propose a unified framework for transporting a broad class of first-moment population causal effect measures under covariate shift. We provide identification results under two conditional exchangeability assumptions, derive both classical and semiparametric estimators, and evaluate their performance through theoretical analysis, simulations, and real-world applications. Our analysis shows the specificity of different causal measures and thus the interest of studying them all: for instance, two common approaches (one-step, estimating equation) lead to similar estimators for the risk difference but to two distinct estimators for the odds ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13104v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Boughdiri (PREMEDICAL), Cl\'ement Berenfeld (PREMEDICAL), Julie Josse (PREMEDICAL), Erwan Scornet</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit tests for spatial point processes: A power study</title>
      <link>https://arxiv.org/abs/2505.13127</link>
      <description>arXiv:2505.13127v1 Announce Type: new 
Abstract: Spatial point processes are used as models in many different fields ranging from ecology and forestry to cosmology and materials science. In recent years, model validation, and in particular goodness-of-fit testing of a proposed point process model have seen many advances. Most of the proposed tests are based on a functional summary statistic of the observed pattern. In this paper, the empirical powers of many possible goodness-of-fit tests that can be constructed from such a summary statistic are compared in an extensive simulation study. Recently introduced functional summary statistics derived from topological data analysis and new constructions for the test statistic such as the continuous ranked probability score are included in the comparison. We discuss the performance of specific combinations of functional summary statistic and test statistic and their robustness with respect to other tuning parameters. Finally, tests using more than one individual functional summary statistic are also investigated. The results allow us to provide guidelines on how to choose powerful tests in a particular test stetting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13127v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Fend, Claudia Redenbach</dc:creator>
    </item>
    <item>
      <title>Testing for sufficient follow-up in cure models with categorical covariates</title>
      <link>https://arxiv.org/abs/2505.13128</link>
      <description>arXiv:2505.13128v1 Announce Type: new 
Abstract: In survival analysis, estimating the fraction of 'immune' or 'cured' subjects who will never experience the event of interest, requires a sufficiently long follow-up period. A few statistical tests have been proposed to test the assumption of sufficient follow-up, i.e. whether the right extreme of the censoring distribution exceeds that of the survival time of the uncured subjects. However, in practice the problem remains challenging. To address this, a relaxed notion of 'practically' sufficient follow-up has been introduced recently, suggesting that the follow-up would be considered sufficiently long if the probability for the event occurring after the end of the study is very small. All these existing tests do not incorporate covariate information, which might affect the cure rate and the survival times. We extend the test for 'practically' sufficient follow-up to settings with categorical covariates. While a straightforward intersection-union type test could reject the null hypothesis of insufficient follow-up only if such hypothesis is rejected for all covariate values, in practice this approach is overly conservative and lacks power. To improve upon this, we propose a novel test procedure that relies on the test decision for one properly chosen covariate value. Our approach relies on the assumption that the conditional density of the uncured survival time is a non-increasing function of time in the tail region. We show that both methods yield tests of asymptotically level $\alpha$ and investigate their finite sample performance through simulations. The practical application of the methods is illustrated using a leukemia dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13128v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsz Pang Yuen, Eni Musta, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>A Kolmogorov-Arnold Neural Model for Cascading Extremes</title>
      <link>https://arxiv.org/abs/2505.13370</link>
      <description>arXiv:2505.13370v1 Announce Type: new 
Abstract: This paper addresses the growing concern of cascading extreme events, such as an extreme earthquake followed by a tsunami, by presenting a novel method for risk assessment focused on these domino effects. The proposed approach develops an extreme value theory framework within a Kolmogorov-Arnold network (KAN) to estimate the probability of one extreme event triggering another, conditionally on a feature vector. An extra layer is added to the KAN's architecture to enforce the definition of the parameter of interest within the unit interval, and we refer to the resulting neural model as KANE (KAN with Natural Enforcement). The proposed method is backed by exhaustive numerical studies and further illustrated with real-world applications to seismology and climatology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13370v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miguel de Carvalho, Clemente Ferrer, Ronny Vallejos</dc:creator>
    </item>
    <item>
      <title>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</title>
      <link>https://arxiv.org/abs/2505.11725</link>
      <description>arXiv:2505.11725v1 Announce Type: cross 
Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11725v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imon Banerjee, Sayak Chakrabarty</dc:creator>
    </item>
    <item>
      <title>(Visualizing) Plausible Treatment Effect Paths</title>
      <link>https://arxiv.org/abs/2505.12014</link>
      <description>arXiv:2505.12014v1 Announce Type: cross 
Abstract: We consider point estimation and inference for the treatment effect path of a policy. Examples include dynamic treatment effects in microeconomics, impulse response functions in macroeconomics, and event study paths in finance. We present two sets of plausible bounds to quantify and visualize the uncertainty associated with this object. Both plausible bounds are often substantially tighter than traditional confidence intervals, and can provide useful insights even when traditional (uniform) confidence bands appear uninformative. Our bounds can also lead to markedly different conclusions when there is significant correlation in the estimates, reflecting the fact that traditional confidence bands can be ineffective at visualizing the impact of such correlation. Our first set of bounds covers the average (or overall) effect rather than the entire treatment path. Our second set of bounds imposes data-driven smoothness restrictions on the treatment path. Post-selection Inference (Berk et al. [2013]) provides formal coverage guarantees for these bounds. The chosen restrictions also imply novel point estimates that perform well across our simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12014v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Freyaldenhoven, Christian Hansen</dc:creator>
    </item>
    <item>
      <title>Proximal optimal transport divergences</title>
      <link>https://arxiv.org/abs/2505.12097</link>
      <description>arXiv:2505.12097v1 Announce Type: cross 
Abstract: We introduce proximal optimal transport divergence, a novel discrepancy measure that interpolates between information divergences and optimal transport distances via an infimal convolution formulation. This divergence provides a principled foundation for optimal transport proximals and proximal optimization methods frequently used in generative modeling. We explore its mathematical properties, including smoothness, boundedness, and computational tractability, and establish connections to primal-dual formulation and adversarial learning. Building on the Benamou-Brenier dynamic formulation of optimal transport cost, we also establish a dynamic formulation for proximal OT divergences. The resulting dynamic formulation is a first order mean-field game whose optimality conditions are governed by a pair of nonlinear partial differential equations, a backward Hamilton-Jacobi and a forward continuity partial differential equations. Our framework generalizes existing approaches while offering new insights and computational tools for generative modeling, distributional optimization, and gradient-based learning in probability spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12097v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Panagiota Birmpa, Markos A. Katsoulakis, Luc Rey-Bellet, Benjamin J. Zhang</dc:creator>
    </item>
    <item>
      <title>Metric Graph Kernels via the Tropical Torelli Map</title>
      <link>https://arxiv.org/abs/2505.12129</link>
      <description>arXiv:2505.12129v1 Announce Type: cross 
Abstract: We propose new graph kernels grounded in the study of metric graphs via tropical algebraic geometry. In contrast to conventional graph kernels that are based on graph combinatorics such as nodes, edges, and subgraphs, our graph kernels are purely based on the geometry and topology of the underlying metric space. A key characterizing property of our construction is its invariance under edge subdivision, making the kernels intrinsically well-suited for comparing graphs that represent different underlying spaces. We develop efficient algorithms for computing these kernels and analyze their complexity, showing that it depends primarily on the genus of the input graphs. Empirically, our kernels outperform existing methods in label-free settings, as demonstrated on both synthetic and real-world benchmark datasets. We further highlight their practical utility through an urban road network classification task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12129v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Cao, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Stereographic Multi-Try Metropolis Algorithms for Heavy-tailed Sampling</title>
      <link>https://arxiv.org/abs/2505.12487</link>
      <description>arXiv:2505.12487v1 Announce Type: cross 
Abstract: Markov chain Monte Carlo (MCMC) methods for sampling from heavy-tailed distributions present unique challenges, particularly in high dimensions. Multi-proposal MCMC algorithms have recently gained attention for their potential to improve performance, especially through parallel implementation on modern hardware. This paper introduces a novel family of gradient-free MCMC algorithms that combine the multi-try Metropolis (MTM) with stereographic MCMC framework, specifically designed for efficient sampling from heavy-tailed targets. The proposed stereographic multi-try Metropolis (SMTM) algorithm not only outperforms traditional Euclidean MTM and existing stereographic random-walk Metropolis methods, but also avoids the pathological convergence behavior often observed in MTM and demonstrates strong robustness to tuning. These properties are supported by scaling analysis and extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12487v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Wang, Jun Yang</dc:creator>
    </item>
    <item>
      <title>Causality-Inspired Robustness for Nonlinear Models via Representation Learning</title>
      <link>https://arxiv.org/abs/2505.12868</link>
      <description>arXiv:2505.12868v1 Announce Type: cross 
Abstract: Distributional robustness is a central goal of prediction algorithms due to the prevalent distribution shifts in real-world data. The prediction model aims to minimize the worst-case risk among a class of distributions, a.k.a., an uncertainty set. Causality provides a modeling framework with a rigorous robustness guarantee in the above sense, where the uncertainty set is data-driven rather than pre-specified as in traditional distributional robustness optimization. However, current causality-inspired robustness methods possess finite-radius robustness guarantees only in the linear settings, where the causal relationships among the covariates and the response are linear. In this work, we propose a nonlinear method under a causal framework by incorporating recent developments in identifiable representation learning and establish a distributional robustness guarantee. To our best knowledge, this is the first causality-inspired robustness method with such a finite-radius robustness guarantee in nonlinear settings. Empirical validation of the theoretical findings is conducted on both synthetic data and real-world single-cell data, also illustrating that finite-radius robustness is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12868v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marin \v{S}ola, Peter B\"uhlmann, Xinwei Shen</dc:creator>
    </item>
    <item>
      <title>Model Selection for Gaussian-gated Gaussian Mixture of Experts Using Dendrograms of Mixing Measures</title>
      <link>https://arxiv.org/abs/2505.13052</link>
      <description>arXiv:2505.13052v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) models constitute a widely utilized class of ensemble learning approaches in statistics and machine learning, known for their flexibility and computational efficiency. They have become integral components in numerous state-of-the-art deep neural network architectures, particularly for analyzing heterogeneous data across diverse domains. Despite their practical success, the theoretical understanding of model selection, especially concerning the optimal number of mixture components or experts, remains limited and poses significant challenges. These challenges primarily stem from the inclusion of covariates in both the Gaussian gating functions and expert networks, which introduces intrinsic interactions governed by partial differential equations with respect to their parameters. In this paper, we revisit the concept of dendrograms of mixing measures and introduce a novel extension to Gaussian-gated Gaussian MoE models that enables consistent estimation of the true number of mixture components and achieves the pointwise optimal convergence rate for parameter estimation in overfitted scenarios. Notably, this approach circumvents the need to train and compare a range of models with varying numbers of components, thereby alleviating the computational burden, particularly in high-dimensional or deep neural network settings. Experimental results on synthetic data demonstrate the effectiveness of the proposed method in accurately recovering the number of experts. It outperforms common criteria such as the Akaike information criterion, the Bayesian information criterion, and the integrated completed likelihood, while achieving optimal convergence rates for parameter estimation and accurately approximating the regression function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13052v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuan Thai, TrungTin Nguyen, Dat Do, Nhat Ho, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>SIMBA -- A Bayesian Decision Framework for the Identification of Optimal Biomarker Subgroups for Cancer Basket Clinical Trials</title>
      <link>https://arxiv.org/abs/2505.13202</link>
      <description>arXiv:2505.13202v1 Announce Type: cross 
Abstract: We consider basket trials in which a biomarker-targeting drug may be efficacious for patients across different disease indications. Patients are enrolled if their cells exhibit some levels of biomarker expression. The threshold level is allowed to vary by indication. The proposed SIMBA method uses a decision framework to identify optimal biomarker subgroups (OBS) defined by an optimal biomarker threshold for each indication. The optimality is achieved through minimizing a posterior expected loss that balances estimation accuracy and investigator preference for broadly effective therapeutics. A Bayesian hierarchical model is proposed to adaptively borrow information across indications and enhance the accuracy in the estimation of the OBS. The operating characteristics of SIMBA are assessed via simulations and compared against a simplified version and an existing alternative method, both of which do not borrow information. SIMBA is expected to improve the identification of patient sub-populations that may benefit from a biomarker-driven therapeutics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13202v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Yuan, Jiaxin Liu, Zhihua Gong, Xia Qin, Crystal Qin, Yuan Ji, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI</title>
      <link>https://arxiv.org/abs/2505.13324</link>
      <description>arXiv:2505.13324v1 Announce Type: cross 
Abstract: Counterfactuals play a pivotal role in the two distinct data science fields of causal inference (CI) and explainable artificial intelligence (XAI). While the core idea behind counterfactuals remains the same in both fields--the examination of what would have happened under different circumstances--there are key differences in how they are used and interpreted. We introduce a formal definition that encompasses the multi-faceted concept of the counterfactual in CI and XAI. We then discuss how counterfactuals are used, evaluated, generated, and operationalized in CI vs. XAI, highlighting conceptual and practical differences. By comparing and contrasting the two, we hope to identify opportunities for cross-fertilization across CI and XAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13324v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Galit Shmueli, David Martens, Jaewon Yoo, Travis Greene</dc:creator>
    </item>
    <item>
      <title>Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising</title>
      <link>https://arxiv.org/abs/2505.13325</link>
      <description>arXiv:2505.13325v1 Announce Type: cross 
Abstract: In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly "expertly targeted" to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing that advisors incorporate diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Finally, we explore the broader implications of human discretion for long-term outcomes and equity, using heterogeneous treatment effect estimation. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13325v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofiia Druchyna, Kara Schechtman, Benjamin Brandon, Jenise Stafford, Hannah Li, Lydia T. Liu</dc:creator>
    </item>
    <item>
      <title>Causal Effect of Functional Treatment</title>
      <link>https://arxiv.org/abs/2210.00242</link>
      <description>arXiv:2210.00242v2 Announce Type: replace 
Abstract: We study the causal effect with a functional treatment variable, where practical applications often arise in neuroscience, biomedical sciences, etc. Previous research concerning the effect of a functional variable on an outcome is typically restricted to exploring correlation rather than causality. The generalized propensity score, which is often used to calibrate the selection bias, is not directly applicable to a functional treatment variable due to a lack of definition of probability density function for functional data. We propose three estimators for the average dose-response functional based on the functional linear model, namely, the functional stabilized weight estimator, the outcome regression estimator and the doubly robust estimator, each of which has its own merits. We study their theoretical properties, which are corroborated through extensive numerical experiments. A real data application on electroencephalography data and disease severity demonstrates the practical value of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00242v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoxu Tan, Wei Huang, Zheng Zhang, Guosheng Yin</dc:creator>
    </item>
    <item>
      <title>Identifiability of causal graphs under nonadditive conditionally parametric causal models</title>
      <link>https://arxiv.org/abs/2303.15376</link>
      <description>arXiv:2303.15376v5 Announce Type: replace 
Abstract: Causal discovery from observational data typically requires strong assumptions about the data-generating process. Previous research has established the identifiability of causal graphs under various models, including linear non-Gaussian, post-nonlinear, and location-scale models. However, these models may have limited applicability in real-world situations that involve a mixture of discrete and continuous variables or where the cause affects the variance or tail behavior of the effect. In this study, we introduce a new class of models, called Conditionally Parametric Causal Models (CPCM), which assume that the distribution of the effect, given the cause, belongs to well-known families such as Gaussian, Poisson, Gamma, or heavy-tailed Pareto distributions. These models are adaptable to a wide range of practical situations where the cause can influence the variance or tail behavior of the effect. We demonstrate the identifiability of CPCM by leveraging the concept of sufficient statistics. Furthermore, we propose an algorithm for estimating the causal structure from random samples drawn from CPCM. We evaluate the empirical properties of our methodology on various datasets, demonstrating state-of-the-art performance across multiple benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15376v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>Structural restrictions in local causal discovery: identifying direct causes of a target variable</title>
      <link>https://arxiv.org/abs/2307.16048</link>
      <description>arXiv:2307.16048v3 Announce Type: replace 
Abstract: We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Here, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. This allows us to relax the identifiability assumptions and develop possibly faster and more robust algorithms. In contrast to the Invariance Causal Prediction framework, we only assume that we observe one environment without any interventions. We discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16048v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>Principal stratification with continuous treatments and continuous post-treatment variables</title>
      <link>https://arxiv.org/abs/2309.14486</link>
      <description>arXiv:2309.14486v2 Announce Type: replace 
Abstract: Principal stratification (PS) is a commonly used approach for understanding the mechanisms through which a treatment affects an outcome. The goal of this work is to extend the PS framework to studies with continuous treatments, which introduces a number of both challenges and opportunities in terms of defining causal effects and performing inference. This manuscript provides multiple key methodological contributions: 1) we introduce principal causal estimands for continuous treatments that provide insights into different causal mechanisms, 2) we show that nonparametric identification is possible under a principal ignorability assumption, but only under a restrictive assumption on the joint distribution of potential mediators, which can be dropped under mild parametric assumptions, 3) we utilize nonparametric Bayesian models for the joint distribution of the potential mediating variables to ensure our approach is robust to model misspecification, and 4) we provide theoretical justification for utilizing an outcome model to identify the joint distribution of the potential mediating variables, and show that this is only possible if a principal ignorability assumption is violated. Lastly, we apply our methodology to a novel study of the relationship between the economy and arrest rates, and how this is potentially mediated by police capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14486v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Antonelli, Minxuan Wu, Fabrizia Mealli, Brenden Beck, Alessandra Mattei</dc:creator>
    </item>
    <item>
      <title>Not all distributional shifts are equal: Fine-grained robust conformal inference</title>
      <link>https://arxiv.org/abs/2402.13042</link>
      <description>arXiv:2402.13042v3 Announce Type: replace 
Abstract: We introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts. This framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome ($Y$) and the covariates ($X$). We propose to reweight the training samples to adjust for an identifiable covariate shift while protecting against worst-case conditional distribution shift bounded in an $f$-divergence ball. Based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts. As a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding. The proposed methods are evaluated in simulation studies and four real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13042v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Ai, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Towards identifiability of micro total effects in summary causal graphs with latent confounding: extension of the front-door criterion</title>
      <link>https://arxiv.org/abs/2406.05805</link>
      <description>arXiv:2406.05805v2 Announce Type: replace 
Abstract: Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine whether these effects can be identified from observational data. Identifying total effects in fully specified causal graphs has received considerable attention, with Pearl's front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of cycles and latent confounding, and when no variable set is sufficient for adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05805v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>New flexible versions of extended generalized Pareto model for count data</title>
      <link>https://arxiv.org/abs/2409.18719</link>
      <description>arXiv:2409.18719v3 Announce Type: replace 
Abstract: Accurate modeling is essential in integer-valued real phenomena, including the distribution of entire data, zero-inflated (ZI) data, and discrete exceedances. The Poisson and Negative Binomial distributions, along with their ZI variants, are considered suitable for modeling the entire data distribution, but they fail to capture the heavy tail behavior effectively alongside the bulk of the distribution. In contrast, the discrete generalized Pareto distribution (DGPD) is preferred for high threshold exceedances, but it becomes less effective for low threshold exceedances. However, in some applications, the selection of a suitable high threshold is challenging, and the asymptotic conditions required for using DGPD are not always met. To address these limitations, extended versions of DGPD are proposed. These extensions are designed to model one of three scenarios: first, the entire distribution of the data, including both bulk and tail and bypassing the threshold selection step; second, the entire distribution along with ZI; and third, the tail of the distribution for low threshold exceedances. The proposed extensions offer improved estimates across all three scenarios compared to existing models, providing more accurate and reliable results in simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18719v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Irshad Ahmad Arshad</dc:creator>
    </item>
    <item>
      <title>Testing procedures based on maximum likelihood estimation for Marked Hawkes processes</title>
      <link>https://arxiv.org/abs/2410.05008</link>
      <description>arXiv:2410.05008v2 Announce Type: replace 
Abstract: The Hawkes model is a past-dependent point process, widely used in various fields for modeling temporal clustering of events. Extending this framework, the multidimensional marked Hawkes process incorporates multiple interacting event types and additional marks, enhancing its capability to model complex dependencies in multivariate time series data. However, increasing the complexity of the model also increases the computational cost of the associated estimation methods and may induce an overfitting of the model. Therefore, it is essential to find a trade-off between accuracy and artificial complexity of the model. In order to find the appropriate version of Hawkes processes, we address, in this paper, the tasks of model fit evaluation and parameter testing for marked Hawkes processes. This article focuses on parametric Hawkes processes with exponential memory kernels, a popular variant for its theoretical and practical advantages. Our work introduces robust testing methodologies for assessing model parameters and complexity, building upon and extending previous theoretical frameworks. We then validate the practical robustness of these tests through comprehensive numerical studies, especially in scenarios where theoretical guarantees remains incomplete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05008v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bonnet, Charlotte Dion-Blanc, Maya Sadeler-Perrin</dc:creator>
    </item>
    <item>
      <title>panelPomp: Analysis of Panel Data via Partially Observed Markov Processes in R</title>
      <link>https://arxiv.org/abs/2410.07934</link>
      <description>arXiv:2410.07934v2 Announce Type: replace 
Abstract: Panel data arise when time series measurements are collected from multiple, dynamically independent but structurally related systems. Each system's time series can be modeled as a partially observed Markov process (POMP), and the ensemble of these models is called a PanelPOMP. If the time series are relatively short, statistical inference for each time series must draw information from across the entire panel. The component systems in the panel are called units; model parameters may be shared between units or may be unit-specific. Differences between units may be of direct inferential interest or may be a nuisance for studying the commonalities. The R package panelPomp supports analysis of panel data via a general class of PanelPOMP models. This includes a suite of tools for manipulation of models and data that take advantage of the panel structure. The panelPomp package currently highlights recent advances enabling likelihood based inference via simulation based algorithms. However, the general framework provided by panelPomp supports development of additional, new inference methodology for panel data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07934v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Bret\'o, Jesse Wheeler, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>AI-Powered Bayesian Inference</title>
      <link>https://arxiv.org/abs/2502.19231</link>
      <description>arXiv:2502.19231v3 Announce Type: replace 
Abstract: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19231v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean O'Hagan, Veronika Ro\v{c}kov\'a</dc:creator>
    </item>
    <item>
      <title>Change of some cropping systems in a long-term trial comparing different systems: rationale and implications for statistical analysis</title>
      <link>https://arxiv.org/abs/2503.16571</link>
      <description>arXiv:2503.16571v2 Announce Type: replace 
Abstract: The project Agriculture 4.0 without chemical synthetical plant protection (NOcsPS) tests a number of cropping systems that avoid the use of chemical synthetical pesticides while at the same time using mineral fertilizers. The experiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some of the cropping systems were modified. Analysis of this experiment may be done using linear mixed models. In order to include the data from 2020-2023 in joint analyses with the data collected for the modified systems from 2024 onwards, the mixed modelling approach needs to be reconsidered. In this paper, we develop models for this purpose. A key feature is the use of network meta-analytic concepts that allow a combination of direct and indirect comparisons among systems from the different years. The approach is first illustrated using a toy example. This is followed by detailed analyses of data from two the two trials sites Dahnsdorf and Hohenheim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16571v2</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans-Peter Piepho, Ingrid Cla{\ss}-Mahler, Beate Zimmermann, Wilfried Hermann, J\"urgen Schwarz, Enno Bahrs</dc:creator>
    </item>
    <item>
      <title>Interpretable Deep Neural Network for Modeling Functional Surrogates</title>
      <link>https://arxiv.org/abs/2503.20528</link>
      <description>arXiv:2503.20528v3 Announce Type: replace 
Abstract: Developing surrogates for computer models has become increasingly important for addressing complex problems in science and engineering. This article introduces an artificial intelligent (AI) surrogate, referred to as the DeepSurrogate, for analyzing functional outputs with vector-valued inputs. The relationship between the functional output and vector-valued input is modeled as an infinite sequence of unknown functions, each representing the relationship at a specific location within the functional domain. These spatially indexed functions are expressed through a combination of basis functions and their corresponding coefficient functions, both of which are modeled using deep neural networks (DNN). The proposed framework accounts for spatial dependencies across locations, while capturing the relationship between the functional output and scalar predictors. It also integrates a Monte Carlo (MC) dropout strategy to quantify prediction uncertainty, enhancing explainability in the deep neural network architecture. The proposed method enables efficient inference on datasets with approximately 50,000 spatial locations and 20 simulations, achieving results in under 10 minutes using standard hardware. The approach is validated on extensive synthetic datasets and a large-scale simulation from the Sea Lake and Overland Surge from Hurricanes (SLOSH) simulator. An open-source Python package implementing the method is made available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20528v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler, Devin Francom, Donatella Pasqualini</dc:creator>
    </item>
    <item>
      <title>Statistical Modeling of Combinatorial Response Data</title>
      <link>https://arxiv.org/abs/2504.11630</link>
      <description>arXiv:2504.11630v2 Announce Type: replace 
Abstract: There is a rich literature for modeling binary and polychotomous responses. However, existing methods are inadequate for handling combinatorial responses, where each response is an integer array under additional constraints. Such data are increasingly common in modern applications, such as surveys collected under skip logic, event propagation on a network, and observed matching in ecology. Ignoring the combinatorial structure leads to biased estimation and prediction. The fundamental challenge is the lack of a link function that connects a linear or functional predictor with a probability respecting the combinatorial constraints. In this article, we propose a novel augmented likelihood that views combinatorial response as a deterministic transform of a continuous latent variable. We specify the transform as the maximizer of integer linear program, and characterize useful properties such as dual thresholding representation. When taking a Bayesian approach and considering a multivariate normal distribution for the latent variable, our method becomes a direct generalization to the celebrated probit data augmentation, and enjoys straightforward computation via Gibbs sampler. We provide theoretical justification, including consistency and applicability, at an interesting intersection between duality and probability. We demonstrate the effectiveness of our method through simulations and a data application on the seasonal matching between waterfowl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11630v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yu Zheng, Malay Ghosh, Leo Duan</dc:creator>
    </item>
    <item>
      <title>Robust Parameter Estimation in Dynamical Systems by Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2505.00491</link>
      <description>arXiv:2505.00491v2 Announce Type: replace 
Abstract: Ordinary and stochastic differential equations (ODEs and SDEs) are widely used to model continuous-time processes across various scientific fields. While ODEs offer interpretability and simplicity, SDEs incorporate randomness, providing robustness to noise and model misspecifications. Recent research highlights the statistical advantages of SDEs, such as improved parameter identifiability and stability under perturbations. This paper investigates the robustness of parameter estimation in SDEs versus ODEs under three types of model misspecifications: unrecognized noise sources, external perturbations, and simplified models. Furthermore, the effect of missing data is explored. Through simulations and an analysis of Danish COVID-19 data, we demonstrate that SDEs yield more stable and reliable parameter estimates, making them a strong alternative to traditional ODE modeling in the presence of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00491v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingchuan Sun, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Estimating Covariate-adjusted Survival Curve in Distributed Data Environment using Data Collaboration Quasi-Experiment</title>
      <link>https://arxiv.org/abs/2505.06035</link>
      <description>arXiv:2505.06035v2 Announce Type: replace 
Abstract: In recent years, there has been an increasing demand for privacy-preserving survival analysis using integrated observational data from multiple institutions and data sources. In particular, estimating survival curves adjusted for covariates that account for confounding factors is essential for evaluating the effectiveness of medical treatments. While high-precision estimation of survival curves requires the collection of large amounts of individual-level data, sharing such data is challenging due to privacy concerns and, even if sharing were possible, the communication costs between institutions would be enormous. To address these challenges, this study proposes and evaluates a novel method that leverages an extended data collaboration quasi-experiment, to estimate covariate-adjusted survival curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06035v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akihiro Toyoda, Yuji Kawamata, Tomoru Nakayama, Akira Imakura, Tetsuya Sakurai, Yukihiko Okada</dc:creator>
    </item>
    <item>
      <title>Inverse Probability Weighting-based Mediation Analysis for Microbiome Data</title>
      <link>https://arxiv.org/abs/2110.02440</link>
      <description>arXiv:2110.02440v4 Announce Type: replace-cross 
Abstract: Mediation analysis is an important tool for studying causal associations in biomedical and other scientific areas and has recently gained attention in microbiome studies. Using a microbiome study of acute myeloid leukemia (AML) patients, we investigate whether the effect of induction chemotherapy intensity levels on infection status is mediated by microbial taxa abundance. The unique characteristics of the microbial mediators -- high-dimensionality, zero-inflation, and dependence -- call for new methodological developments in mediation analysis. The presence of an exposure-induced mediator-outcome confounder, antibiotic use, further requires a delicate treatment in the analysis. To address these unique challenges in our motivating AML microbiome study, we propose a novel nonparametric identification formula for the interventional indirect effect (IIE), a recently developed measure for assessing mediation effects. We develop a corresponding estimation algorithm using the inverse probability weighting method. We also test the presence of mediation effects via constructing the standard normal bootstrap confidence intervals. Simulation studies demonstrate that the proposed method has good finite-sample performance in terms of IIE estimation accuracy and the type-I error rate and power of the corresponding tests. In the AML microbiome study, our findings suggest that the effect of induction chemotherapy intensity levels on infection is mainly mediated by patients' gut microbiome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.02440v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuexia Zhang, Jian Wang, Jiayi Shen, Jessica Galloway-Pena, Samuel Shelburne, Linbo Wang, Jianhua Hu</dc:creator>
    </item>
    <item>
      <title>Sharp variance estimator and causal bootstrap in stratified randomized experiments</title>
      <link>https://arxiv.org/abs/2401.16667</link>
      <description>arXiv:2401.16667v3 Announce Type: replace-cross 
Abstract: Randomized experiments are the gold standard for estimating treatment effects, and randomization serves as a reasoned basis for inference. In widely used stratified randomized experiments, randomization-based finite-population asymptotic theory enables valid inference for the average treatment effect, relying on normal approximation and a Neyman-type conservative variance estimator. However, when the sample size is small or the outcomes are skewed, the Neyman-type variance estimator may become overly conservative, and the normal approximation can fail. To address these issues, we propose a sharp variance estimator and two causal bootstrap methods to more accurately approximate the sampling distribution of the weighted difference-in-means estimator in stratified randomized experiments. The first causal bootstrap procedure is based on rank-preserving imputation and we prove its second-order refinement over normal approximation. The second causal bootstrap procedure is based on constant-treatment-effect imputation and is further applicable in paired experiments. In contrast to traditional bootstrap methods, where randomness originates from hypothetical super-population sampling, our analysis for the proposed causal bootstrap is randomization-based, relying solely on the randomness of treatment assignment in randomized experiments. Numerical studies and two real data applications demonstrate advantages of our proposed methods in finite samples. The \texttt{R} package \texttt{CausalBootstrap} implementing our method is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16667v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Yu, Ke Zhu, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayesian Estimation and Inference with Control Functions</title>
      <link>https://arxiv.org/abs/2402.17374</link>
      <description>arXiv:2402.17374v2 Announce Type: replace-cross 
Abstract: This paper introduces a quasi-Bayesian method that integrates frequentist nonparametric estimation with Bayesian inference in a two-stage process. Applied to an endogenous discrete choice model, the approach first uses kernel or sieve estimators to estimate the control function nonparametrically, followed by Bayesian methods to estimate the structural parameters. This combination leverages the advantages of both frequentist tractability for nonparametric estimation and Bayesian computational efficiency for complicated structural models. We analyze the asymptotic properties of the resulting quasi-posterior distribution, finding that its mean provides a consistent estimator for the parameters of interest, although its quantiles do not yield valid confidence intervals. However, bootstrapping the quasi-posterior mean accounts for the estimation uncertainty from the first stage, thereby producing asymptotically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17374v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Quantifying the Internal Validity of Weighted Estimands</title>
      <link>https://arxiv.org/abs/2404.14603</link>
      <description>arXiv:2404.14603v3 Announce Type: replace-cross 
Abstract: In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects. The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework. Our focus is on answering two questions concerning weighted estimands. First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation? Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest? We argue that this upper bound provides a valuable diagnostic for empirical research. When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low. Our paper develops practical tools to quantify the internal validity of weighted estimands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14603v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Poirier, Tymon S{\l}oczy\'nski</dc:creator>
    </item>
    <item>
      <title>Modeling and Discovering Direct Causes for Predictive Models</title>
      <link>https://arxiv.org/abs/2412.02878</link>
      <description>arXiv:2412.02878v2 Announce Type: replace-cross 
Abstract: We introduce a causal modeling framework that captures the input-output behavior of predictive models (e.g., machine learning models). The framework enables us to identify features that directly cause the predictions, which has broad implications for data collection and model evaluation. We then present sound and complete algorithms for discovering direct causes (from data) under some assumptions. Furthermore, we propose a novel independence rule that can be integrated with the algorithms to accelerate the discovery process, as we demonstrate both theoretically and empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02878v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizuo Chen, Amit Bhatia</dc:creator>
    </item>
    <item>
      <title>Efficient Randomized Experiments Using Foundation Models</title>
      <link>https://arxiv.org/abs/2502.04262</link>
      <description>arXiv:2502.04262v2 Announce Type: replace-cross 
Abstract: Randomized experiments are the preferred approach for evaluating the effects of interventions, but they are costly and often yield estimates with substantial uncertainty. On the other hand, in silico experiments leveraging foundation models offer a cost-effective alternative that can potentially attain higher statistical precision. However, the benefits of in silico experiments come with a significant risk: statistical inferences are not valid if the models fail to accurately predict experimental responses to interventions. In this paper, we propose a novel approach that integrates the predictions from multiple foundation models with experimental data while preserving valid statistical inference. Our estimator is consistent and asymptotically normal, with asymptotic variance no larger than the standard estimator based on experimental data alone. Importantly, these statistical properties hold even when model predictions are arbitrarily biased. Empirical results across several randomized experiments show that our estimator offers substantial precision gains, equivalent to a reduction of up to 20% in the sample size needed to match the same precision as the standard estimator based on experimental data alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04262v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piersilvio De Bartolomeis, Javier Abad, Guanbo Wang, Konstantin Donhauser, Raymond M. Duch, Fanny Yang, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction under Levy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations</title>
      <link>https://arxiv.org/abs/2502.14105</link>
      <description>arXiv:2502.14105v2 Announce Type: replace-cross 
Abstract: Conformal prediction provides a powerful framework for constructing prediction intervals with finite-sample guarantees, yet its robustness under distribution shifts remains a significant challenge. This paper addresses this limitation by modeling distribution shifts using Levy-Prokhorov (LP) ambiguity sets, which capture both local and global perturbations. We provide a self-contained overview of LP ambiguity sets and their connections to popular metrics such as Wasserstein and Total Variation. We show that the link between conformal prediction and LP ambiguity sets is a natural one: by propagating the LP ambiguity set through the scoring function, we reduce complex high-dimensional distribution shifts to manageable one-dimensional distribution shifts, enabling exact quantification of worst-case quantiles and coverage. Building on this analysis, we construct robust conformal prediction intervals that remain valid under distribution shifts, explicitly linking LP parameters to interval width and confidence levels. Experimental results on real-world datasets demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14105v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Zheyu Oliver Wang, Julie Zhu, Michael I. Jordan, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score-Based Estimators</title>
      <link>https://arxiv.org/abs/2503.17290</link>
      <description>arXiv:2503.17290v3 Announce Type: replace-cross 
Abstract: The partitioning of data for estimation and calibration critically impacts the performance of propensity score based estimators like inverse probability weighting (IPW) and double/debiased machine learning (DML) frameworks. We extend recent advances in calibration techniques for propensity score estimation, improving the robustness of propensity scores in challenging settings such as limited overlap, small sample sizes, or unbalanced data. Our contributions are twofold: First, we provide a theoretical analysis of the properties of calibrated estimators in the context of DML. To this end, we refine existing calibration frameworks for propensity score models, with a particular emphasis on the role of sample-splitting schemes in ensuring valid causal inference. Second, through extensive simulations, we show that calibration reduces variance of inverse-based propensity score estimators while also mitigating bias in IPW, even in small-sample regimes. Notably, calibration improves stability for flexible learners (e.g., gradient boosting) while preserving the doubly robust properties of DML. A key insight is that, even when methods perform well without calibration, incorporating a calibration step does not degrade performance, provided that an appropriate sample-splitting approach is chosen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17290v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Klaassen, Jan Rabenseifner, Jannis Kueck, Philipp Bach</dc:creator>
    </item>
  </channel>
</rss>
