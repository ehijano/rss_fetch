<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Jul 2024 04:01:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comparative Effectiveness Research with Average Hazard for Censored Time-to-Event Outcomes: A Numerical Study</title>
      <link>https://arxiv.org/abs/2407.00709</link>
      <description>arXiv:2407.00709v1 Announce Type: new 
Abstract: The average hazard (AH), recently introduced by Uno and Horiguchi, represents a novel summary metric of event time distributions, conceptualized as the general censoring-free average person-time incidence rate on a given time window, $[0,\tau].$ This metric is calculated as the ratio of the cumulative incidence probability at $\tau$ to the restricted mean survival time at $\tau$ and can be estimated through non-parametric methods. The AH's difference and ratio present viable alternatives to the traditional Cox's hazard ratio for quantifying the treatment effect on time-to-event outcomes in comparative clinical studies. While the methodology for evaluating the difference and ratio of AH in randomized clinical trials has been previously proposed, the application of the AH-based approach in general comparative effectiveness research (CER), where interventions are not randomly allocated, remains underdiscussed. This paper aims to introduce several approaches for applying the AH in general CER, thereby extending its utility beyond randomized trial settings to observational studies where treatment assignment is non-random.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00709v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Xiong, Jean Connors, Deb Schrag, Hajime Uno</dc:creator>
    </item>
    <item>
      <title>Enlarging of the sample to address multicollinearity</title>
      <link>https://arxiv.org/abs/2407.01172</link>
      <description>arXiv:2407.01172v1 Announce Type: new 
Abstract: The paper analyzes how the enlarging of the sample affects to the mitigation of collinearity concluding that it may mitigate the consequences of collinearity related to statistical analysis but not necessarily the numerical instability. The problem that is addressed is of importance in the teaching of social sciences since it discusses one of the solutions proposed almost unanimously to solve the problem of multicollinearity. For a better understanding and illustration of the contribution of this paper, two empirical examples are presented and not highly technical developments are used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01172v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rom\'an Salmer\'on G\'omez, Catalina Garc\'ia Garc\'ia, Ainara Rodr\'iguez S\'anchez</dc:creator>
    </item>
    <item>
      <title>Harnessing XGBoost for Robust Biomarker Selection of Obsessive-Compulsive Disorder (OCD) from Adolescent Brain Cognitive Development (ABCD) data</title>
      <link>https://arxiv.org/abs/2407.00028</link>
      <description>arXiv:2407.00028v1 Announce Type: cross 
Abstract: This study evaluates the performance of various supervised machine learning models in analyzing highly correlated neural signaling data from the Adolescent Brain Cognitive Development (ABCD) Study, with a focus on predicting obsessive-compulsive disorder scales. We simulated a dataset to mimic the correlation structures commonly found in imaging data and evaluated logistic regression, elastic networks, random forests, and XGBoost on their ability to handle multicollinearity and accurately identify predictive features. Our study aims to guide the selection of appropriate machine learning methods for processing neuroimaging data, highlighting models that best capture underlying signals in high feature correlations and prioritize clinically relevant features associated with Obsessive-Compulsive Disorder (OCD).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00028v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Shen, Qimin Zhang, Huili Zheng, Weiwei Qi</dc:creator>
    </item>
    <item>
      <title>Optimal Transport for Latent Integration with An Application to Heterogeneous Neuronal Activity Data</title>
      <link>https://arxiv.org/abs/2407.00099</link>
      <description>arXiv:2407.00099v1 Announce Type: cross 
Abstract: Detecting dynamic patterns of task-specific responses shared across heterogeneous datasets is an essential and challenging problem in many scientific applications in medical science and neuroscience. In our motivating example of rodent electrophysiological data, identifying the dynamical patterns in neuronal activity associated with ongoing cognitive demands and behavior is key to uncovering the neural mechanisms of memory. One of the greatest challenges in investigating a cross-subject biological process is that the systematic heterogeneity across individuals could significantly undermine the power of existing machine learning methods to identify the underlying biological dynamics. In addition, many technically challenging neurobiological experiments are conducted on only a handful of subjects where rich longitudinal data are available for each subject. The low sample sizes of such experiments could further reduce the power to detect common dynamic patterns among subjects. In this paper, we propose a novel heterogeneous data integration framework based on optimal transport to extract shared patterns in complex biological processes. The key advantages of the proposed method are that it can increase discriminating power in identifying common patterns by reducing heterogeneity unrelated to the signal by aligning the extracted latent spatiotemporal information across subjects. Our approach is effective even with a small number of subjects, and does not require auxiliary matching information for the alignment. In particular, our method can align longitudinal data across heterogeneous subjects in a common latent space to capture the dynamics of shared patterns while utilizing temporal dependency within subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00099v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yubai Yuan, Babak Shahbaba, Norbert Fortin, Keiland Cooper, Qing Nie, Annie Qu</dc:creator>
    </item>
    <item>
      <title>A Calibrated Sensitivity Analysis for Weighted Causal Decompositions</title>
      <link>https://arxiv.org/abs/2407.00139</link>
      <description>arXiv:2407.00139v1 Announce Type: cross 
Abstract: Disparities in health or well-being experienced by minority groups can be difficult to study using the traditional exposure-outcome paradigm in causal inference, since potential outcomes in variables such as race or sexual minority status are challenging to interpret. Causal decomposition analysis addresses this gap by positing causal effects on disparities under interventions to other, intervenable exposures that may play a mediating role in the disparity. While invoking weaker assumptions than causal mediation approaches, decomposition analyses are often conducted in observational settings and require uncheckable assumptions that eliminate unmeasured confounders. Leveraging the marginal sensitivity model, we develop a sensitivity analysis for weighted causal decomposition estimators and use the percentile bootstrap to construct valid confidence intervals for causal effects on disparities. We also propose a two-parameter amplification that enhances interpretability and facilitates an intuitive understanding of the plausibility of unmeasured confounders and their effects. We illustrate our framework on a study examining the effect of parental acceptance on disparities in suicidal ideation among sexual minority youth. We find that the effect is small and sensitive to unmeasured confounding, suggesting that further screening studies are needed to identify mitigating interventions in this vulnerable population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00139v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy Shen, Elina Visoki, Ran Barzilay, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Permutation invariant multi-output Gaussian Processes for drug combination prediction in cancer</title>
      <link>https://arxiv.org/abs/2407.00175</link>
      <description>arXiv:2407.00175v1 Announce Type: cross 
Abstract: Dose-response prediction in cancer is an active application field in machine learning. Using large libraries of \textit{in-vitro} drug sensitivity screens, the goal is to develop accurate predictive models that can be used to guide experimental design or inform treatment decisions. Building on previous work that makes use of permutation invariant multi-output Gaussian Processes in the context of dose-response prediction for drug combinations, we develop a variational approximation to these models. The variational approximation enables a more scalable model that provides uncertainty quantification and naturally handles missing data. Furthermore, we propose using a deep generative model to encode the chemical space in a continuous manner, enabling prediction for new drugs and new combinations. We demonstrate the performance of our model in a simple setting using a high-throughput dataset and show that the model is able to efficiently borrow information across outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00175v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leiv R{\o}nneberg, Vidhi Lalchand, Paul D. W. Kirk</dc:creator>
    </item>
    <item>
      <title>Multimodal Prototyping for cancer survival prediction</title>
      <link>https://arxiv.org/abs/2407.00224</link>
      <description>arXiv:2407.00224v1 Announce Type: cross 
Abstract: Multimodal survival methods combining gigapixel histology whole-slide images (WSIs) and transcriptomic profiles are particularly promising for patient prognostication and stratification. Current approaches involve tokenizing the WSIs into smaller patches (&gt;10,000 patches) and transcriptomics into gene groups, which are then integrated using a Transformer for predicting outcomes. However, this process generates many tokens, which leads to high memory requirements for computing attention and complicates post-hoc interpretability analyses. Instead, we hypothesize that we can: (1) effectively summarize the morphological content of a WSI by condensing its constituting tokens using morphological prototypes, achieving more than 300x compression; and (2) accurately characterize cellular functions by encoding the transcriptomic profile with biological pathway prototypes, all in an unsupervised fashion. The resulting multimodal tokens are then processed by a fusion network, either with a Transformer or an optimal transport cross-alignment, which now operates with a small and fixed number of tokens without approximations. Extensive evaluation on six cancer types shows that our framework outperforms state-of-the-art methods with much less computation while unlocking new interpretability analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00224v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew H. Song, Richard J. Chen, Guillaume Jaume, Anurag J. Vaidya, Alexander S. Baras, Faisal Mahmood</dc:creator>
    </item>
    <item>
      <title>Interpret the estimand framework from a causal inference perspective</title>
      <link>https://arxiv.org/abs/2407.00292</link>
      <description>arXiv:2407.00292v1 Announce Type: cross 
Abstract: The estimand framework proposed by ICH in 2017 has brought fundamental changes in the pharmaceutical industry. It clearly describes how a treatment effect in a clinical question should be precisely defined and estimated, through attributes including treatments, endpoints and intercurrent events. However, ideas around the estimand framework are commonly in text, and different interpretations on this framework may exist. This article aims to interpret the estimand framework through its underlying theories, the causal inference framework based on potential outcomes. The statistical origin and formula of an estimand is given through the causal inference framework, with all attributes translated into statistical terms. How five strategies proposed by ICH to analyze intercurrent events are incorporated in the statistical formula of an estimand is described, and a new strategy to analyze intercurrent events is also suggested. The roles of target populations and analysis sets in the estimand framework are compared and discussed based on the statistical formula of an estimand. This article recommends continuing study of causal inference theories behind the estimand framework and improving the estimand framework with greater methodological comprehensibility and availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00292v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinghong Zeng</dc:creator>
    </item>
    <item>
      <title>Towards Statistically Significant Taxonomy Aware Co-location Pattern Detection</title>
      <link>https://arxiv.org/abs/2407.00317</link>
      <description>arXiv:2407.00317v1 Announce Type: cross 
Abstract: Given a collection of Boolean spatial feature types, their instances, a neighborhood relation (e.g., proximity), and a hierarchical taxonomy of the feature types, the goal is to find the subsets of feature types or their parents whose spatial interaction is statistically significant. This problem is for taxonomy-reliant applications such as ecology (e.g., finding new symbiotic relationships across the food chain), spatial pathology (e.g., immunotherapy for cancer), retail, etc. The problem is computationally challenging due to the exponential number of candidate co-location patterns generated by the taxonomy. Most approaches for co-location pattern detection overlook the hierarchical relationships among spatial features, and the statistical significance of the detected patterns is not always considered, leading to potential false discoveries. This paper introduces two methods for incorporating taxonomies and assessing the statistical significance of co-location patterns. The baseline approach iteratively checks the significance of co-locations between leaf nodes or their ancestors in the taxonomy. Using the Benjamini-Hochberg procedure, an advanced approach is proposed to control the false discovery rate. This approach effectively reduces the risk of false discoveries while maintaining the power to detect true co-location patterns. Experimental evaluation and case study results show the effectiveness of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00317v1</guid>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Ghosh, Arun Sharma, Jayant Gupta, Shashi Shekhar</dc:creator>
    </item>
    <item>
      <title>Advancing Information Integration through Empirical Likelihood: Selective Reviews and a New Idea</title>
      <link>https://arxiv.org/abs/2407.00561</link>
      <description>arXiv:2407.00561v1 Announce Type: cross 
Abstract: Information integration plays a pivotal role in biomedical studies by facilitating the combination and analysis of independent datasets from multiple studies, thereby uncovering valuable insights that might otherwise remain obscured due to the limited sample size in individual studies. However, sharing raw data from independent studies presents significant challenges, primarily due to the need to safeguard sensitive participant information and the cumbersome paperwork involved in data sharing. In this article, we first provide a selective review of recent methodological developments in information integration via empirical likelihood, wherein only summary information is required, rather than the raw data. Following this, we introduce a new insight and a potentially promising framework that could broaden the application of information integration across a wider spectrum. Furthermore, this new framework offers computational convenience compared to classic empirical likelihood-based methods. We provide numerical evaluations to assess its performance and discuss various extensions in the end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00561v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chixiang Chen, Jia Liang, Elynn Chen, Ming Wang</dc:creator>
    </item>
    <item>
      <title>Proper Scoring Rules for Multivariate Probabilistic Forecasts based on Aggregation and Transformation</title>
      <link>https://arxiv.org/abs/2407.00650</link>
      <description>arXiv:2407.00650v1 Announce Type: cross 
Abstract: Proper scoring rules are an essential tool to assess the predictive performance of probabilistic forecasts. However, propriety alone does not ensure an informative characterization of predictive performance and it is recommended to compare forecasts using multiple scoring rules. With that in mind, interpretable scoring rules providing complementary information are necessary. We formalize a framework based on aggregation and transformation to build interpretable multivariate proper scoring rules. Aggregation-and-transformation-based scoring rules are able to target specific features of the probabilistic forecasts; which improves the characterization of the predictive performance. This framework is illustrated through examples taken from the literature and studied using numerical experiments showcasing its benefits. In particular, it is shown that it can help bridge the gap between proper scoring rules and spatial verification tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00650v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Pic, Cl\'ement Dombry, Philippe Naveau, Maxime Taillardat</dc:creator>
    </item>
    <item>
      <title>Estimating the cognitive effects of statins from observational data using the survival-incorporated median: a summary measure for clinical outcomes in the presence of death</title>
      <link>https://arxiv.org/abs/2407.00846</link>
      <description>arXiv:2407.00846v1 Announce Type: cross 
Abstract: The issue of "truncation by death" commonly arises in clinical research: subjects may die before their follow-up assessment, resulting in undefined clinical outcomes. This article addresses truncation by death by analyzing the Long Life Family Study (LLFS), a multicenter observational study involving over 4000 older adults with familial longevity. We are interested in the cognitive effects of statins in LLFS participants, as the impact of statins on cognition remains unclear despite their widespread use. In this application, rather than treating death as a mechanism through which clinical outcomes are missing, we advocate treating death as part of the outcome measure. We focus on the survival-incorporated median, the median of a composite outcome combining death and cognitive scores, to summarize the effect of statins. We propose an estimator for the survival-incorporated median from observational data, applicable in both point-treatment settings and time-varying treatment settings. Simulations demonstrate the survival-incorporated median as a simple and useful summary measure. We apply this method to estimate the effect of statins on the change in cognitive function (measured by the Digit Symbol Substitution Test), incorporating death. Our results indicate no significant difference in cognitive decline between participants with a similar age distribution on and off statins from baseline. Through this application, we aim to not only contribute to this clinical question but also offer insights into analyzing clinical outcomes in the presence of death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00846v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingyan Xiang, Paola Sebastiani, Thomas Perls, Stacy L. Andersen, Svetlana Ukraintseva, Mikael Thinggaard, Judith J. Lok</dc:creator>
    </item>
    <item>
      <title>Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests</title>
      <link>https://arxiv.org/abs/2407.01036</link>
      <description>arXiv:2407.01036v1 Announce Type: cross 
Abstract: A/B testers conducting large-scale tests prioritize lifts and want to be able to control false rejections of the null. This work develops a decision-theoretic framework for maximizing profits subject to false discovery rate (FDR) control. We build an empirical Bayes solution for the problem via the greedy knapsack approach. We derive an oracle rule based on ranking the ratio of expected lifts and the cost of wrong rejections using the local false discovery rate (lfdr) statistic. Our oracle decision rule is valid and optimal for large-scale tests. Further, we establish asymptotic validity for the data-driven procedure and demonstrate finite-sample validity in experimental studies. We also demonstrate the merit of the proposed method over other FDR control methods. Finally, we discuss an application to actual Optimizely experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01036v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pallavi Basu, Ron Berman</dc:creator>
    </item>
    <item>
      <title>A General Purpose Approximation to the Ferguson-Klass Algorithm for Sampling from L\'evy Processes Without Gaussian Components</title>
      <link>https://arxiv.org/abs/2407.01483</link>
      <description>arXiv:2407.01483v1 Announce Type: cross 
Abstract: We propose a general-purpose approximation to the Ferguson-Klass algorithm for generating samples from L\'evy processes without Gaussian components. We show that the proposed method is more than 1000 times faster than the standard Ferguson-Klass algorithm without a significant loss of precision. This method can open an avenue for computationally efficient and scalable Bayesian nonparametric models which go beyond conjugacy assumptions, as demonstrated in the examples section.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01483v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawid Bernaciak, Jim E. Griffin</dc:creator>
    </item>
    <item>
      <title>Analyzing the Reporting Error of Public Transport Trips in the Danish National Travel Survey Using Smart Card Data</title>
      <link>https://arxiv.org/abs/2308.01198</link>
      <description>arXiv:2308.01198v3 Announce Type: replace 
Abstract: Household travel surveys have been used for decades to collect individuals and households' travel behavior. However, self-reported surveys are subject to recall bias, as respondents might struggle to recall and report their activities accurately. This study examines the time reporting error of public transit users in a nationwide household travel survey by matching, at the individual level, five consecutive years of data from two sources, namely the Danish National Travel Survey (TU) and the Danish Smart Card system (Rejsekort). Survey respondents are matched with travel cards from the Rejsekort data solely based on the respondents' declared spatiotemporal travel behavior. Approximately, 70% of the respondents were successfully matched with Rejsekort travel cards. The findings reveal a median time reporting error of 11.34 minutes, with an Interquartile Range of 28.14 minutes. Furthermore, a statistical analysis was performed to explore the relationships between the survey respondents' reporting error and their socio-economic and demographic characteristics. The results indicate that females and respondents with a fixed schedule are in general more accurate than males and respondents with a flexible schedule in reporting their times of travel. Moreover, trips reported during weekdays or via the internet displayed higher accuracies compared to trips reported during weekends and holidays or via telephone interviews. This disaggregated analysis provides valuable insights that could help in improving the design and analysis of travel surveys, as well accounting for reporting errors/biases in travel survey-based applications. Furthermore, it offers valuable insights underlying the psychology of travel recall by survey respondents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01198v3</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georges Sfeir, Filipe Rodrigues, Maya Abou Zeid, Francisco Camara Pereira</dc:creator>
    </item>
    <item>
      <title>A Regression-Based Approach to the CO2 Airborne Fraction: Enhancing Statistical Precision and Tackling Zero Emissions</title>
      <link>https://arxiv.org/abs/2311.01053</link>
      <description>arXiv:2311.01053v3 Announce Type: replace 
Abstract: The global fraction of anthropogenically emitted carbon dioxide (CO$_2$) that stays in the atmosphere, the CO$_2$ airborne fraction, has been fluctuating around a constant value over the period 1959 to 2022. The consensus estimate of the airborne fraction is around $44\%$; the remaining $56\%$ is absorbed by the oceanic and terrestrials biospheres. In this study, we show that the conventional estimator of the airborne fraction, based on a ratio of changes in atmospheric CO$_2$ concentrations and CO$_2$ emissions, suffers from a number of statistical deficiencies, such as non-existence of moments and a non-Gaussian limiting distribution. We propose an alternative regression-based estimator of the airborne fraction that does not suffer from these deficiencies. We show that the regression-based estimator has a Gaussian limiting distribution and reduces estimation uncertainty substantially. Our empirical analysis leads to an estimate of the airborne fraction over 1959--2022 of $47.0\%$ ($\pm 1.1\%$; $1 \sigma$), implying a higher, and better constrained, estimate than the current consensus. Using climate model output, we show that a regression-based approach provides sensible estimates of the airborne fraction, also in future scenarios where emissions are at or near zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01053v3</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman</dc:creator>
    </item>
    <item>
      <title>Binary response model with many weak instruments</title>
      <link>https://arxiv.org/abs/2201.04811</link>
      <description>arXiv:2201.04811v4 Announce Type: replace-cross 
Abstract: This paper considers an endogenous binary response model with many weak instruments. We employ a control function approach and a regularization scheme to obtain better estimation results for the endogenous binary response model in the presence of many weak instruments. Two consistent and asymptotically normally distributed estimators are provided, each of which is called a regularized conditional maximum likelihood estimator (RCMLE) and a regularized nonlinear least squares estimator (RNLSE). Monte Carlo simulations show that the proposed estimators outperform the existing ones when there are many weak instruments. We use the proposed estimation method to examine the effect of family income on college completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.04811v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dakyung Seong</dc:creator>
    </item>
    <item>
      <title>Towards Improving Unit Commitment Economics: An Add-On Tailor for Renewable Energy and Reserve Predictions</title>
      <link>https://arxiv.org/abs/2208.13065</link>
      <description>arXiv:2208.13065v3 Announce Type: replace-cross 
Abstract: Generally, day-ahead unit commitment (UC) is conducted in a predict-then-optimize process: it starts by predicting the renewable energy source (RES) availability and system reserve requirements; given the predictions, the UC model is then optimized to determine the economic operation plans. In fact, predictions within the process are raw. In other words, if the predictions are further tailored to assist UC in making the economic operation plans against realizations of the RES and reserve requirements, UC economics will benefit significantly. To this end, this paper presents a cost-oriented tailor of RES-and-reserve predictions for UC, deployed as an add-on to the predict-then-optimize process. The RES-and-reserve tailor is trained by solving a bi-level mixed-integer programming model: the upper level trains the tailor based on its induced operating cost; the lower level, given tailored predictions, mimics the system operation process and feeds the induced operating cost back to the upper level; finally, the upper level evaluates the training quality according to the fed-back cost. Through this training, the tailor learns to customize the raw predictions into cost-oriented predictions. Moreover, the tailor can be embedded into the existing predict-then-optimize process as an add-on, improving the UC economics. Lastly, the presented method is compared to traditional, binary-relaxation, neural network-based, stochastic, and robust methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13065v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xianbang Chen, Yikui Liu, Lei Wu</dc:creator>
    </item>
    <item>
      <title>Bayesian Modal Regression based on Mixture Distributions</title>
      <link>https://arxiv.org/abs/2211.10776</link>
      <description>arXiv:2211.10776v5 Announce Type: replace-cross 
Abstract: Compared to mean regression and quantile regression, the literature on modal regression is very sparse. A unifying framework for Bayesian modal regression is proposed, based on a family of unimodal distributions indexed by the mode, along with other parameters that allow for flexible shapes and tail behaviors. Sufficient conditions for posterior propriety under an improper prior on the mode parameter are derived. Following prior elicitation, regression analysis of simulated data and datasets from several real-life applications are conducted. Besides drawing inference for covariate effects that are easy to interpret, prediction and model selection under the proposed Bayesian modal regression framework are also considered. Evidence from these analyses suggest that the proposed inference procedures are very robust to outliers, enabling one to discover interesting covariate effects missed by mean or median regression, and to construct much tighter prediction intervals than those from mean or median regression. Computer programs for implementing the proposed Bayesian modal regression are available at https://github.com/rh8liuqy/Bayesian_modal_regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.10776v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.csda.2024.108012</arxiv:DOI>
      <dc:creator>Qingyang Liu, Xianzheng Huang, Rai Bai</dc:creator>
    </item>
    <item>
      <title>Bayesian Safety Validation for Failure Probability Estimation of Black-Box Systems</title>
      <link>https://arxiv.org/abs/2305.02449</link>
      <description>arXiv:2305.02449v2 Announce Type: replace-cross 
Abstract: Estimating the probability of failure is an important step in the certification of safety-critical systems. Efficient estimation methods are often needed due to the challenges posed by high-dimensional input spaces, risky test scenarios, and computationally expensive simulators. This work frames the problem of black-box safety validation as a Bayesian optimization problem and introduces a method that iteratively fits a probabilistic surrogate model to efficiently predict failures. The algorithm is designed to search for failures, compute the most-likely failure, and estimate the failure probability over an operating domain using importance sampling. We introduce three acquisition functions that aim to reduce uncertainty by covering the design space, optimize the analytically derived failure boundaries, and sample the predicted failure regions. Results show this Bayesian safety validation approach provides a more accurate estimate of failure probability with orders of magnitude fewer samples and performs well across various safety validation metrics. We demonstrate this approach on three test problems, a stochastic decision making system, and a neural network-based runway detection system. This work is open sourced (https://github.com/sisl/BayesianSafetyValidation.jl) and currently being used to supplement the FAA certification process of the machine learning components for an autonomous cargo aircraft.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02449v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2514/1.I011395</arxiv:DOI>
      <arxiv:journal_reference>AIAA Journal of Aerospace Information Systems (JAIS) 21.7 (2024): 533-546</arxiv:journal_reference>
      <dc:creator>Robert J. Moss, Mykel J. Kochenderfer, Maxime Gariel, Arthur Dubois</dc:creator>
    </item>
    <item>
      <title>Priming bias versus post-treatment bias in experimental designs</title>
      <link>https://arxiv.org/abs/2306.01211</link>
      <description>arXiv:2306.01211v3 Announce Type: replace-cross 
Abstract: Conditioning on variables affected by treatment can induce post-treatment bias when estimating causal effects. Although this suggests that researchers should measure potential moderators before administering the treatment in an experiment, doing so may also bias causal effect estimation if the covariate measurement primes respondents to react differently to the treatment. This paper formally analyzes this trade-off between post-treatment and priming biases in three experimental designs that vary when moderators are measured: pre-treatment, post-treatment, or a randomized choice between the two. We derive nonparametric bounds for interactions between the treatment and the moderator under each design and show how to use substantive assumptions to narrow these bounds. These bounds allow researchers to assess the sensitivity of their empirical findings to either source of bias. We then apply the proposed methodology to a survey experiment on electoral messaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01211v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Jacob R. Brown, Sophie Hill, Kosuke Imai, Teppei Yamamoto</dc:creator>
    </item>
    <item>
      <title>A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values</title>
      <link>https://arxiv.org/abs/2307.11465</link>
      <description>arXiv:2307.11465v5 Announce Type: replace-cross 
Abstract: In the field of lung cancer research, particularly in the analysis of overall survival (OS), artificial intelligence (AI) serves crucial roles with specific aims. Given the prevalent issue of missing data in the medical domain, our primary objective is to develop an AI model capable of dynamically handling this missing data. Additionally, we aim to leverage all accessible data, effectively analyzing both uncensored patients who have experienced the event of interest and censored patients who have not, by embedding a specialized technique within our AI model, not commonly utilized in other AI tasks. Through the realization of these objectives, our model aims to provide precise OS predictions for non-small cell lung cancer (NSCLC) patients, thus overcoming these significant challenges. We present a novel approach to survival analysis with missing values in the context of NSCLC, which exploits the strengths of the transformer architecture to account only for available features without requiring any imputation strategy. More specifically, this model tailors the transformer architecture to tabular data by adapting its feature embedding and masked self-attention to mask missing data and fully exploit the available ones. By making use of ad-hoc designed losses for OS, it is able to account for both censored and uncensored patients, as well as changes in risks over time. We compared our method with state-of-the-art models for survival analysis coupled with different imputation strategies. We evaluated the results obtained over a period of 6 years using different time granularities obtaining a Ct-index, a time-dependent variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11465v5</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Camillo Maria Caruso, Valerio Guarrasi, Sara Ramella, Paolo Soda</dc:creator>
    </item>
    <item>
      <title>Correcting Selection Bias in Standardized Test Scores Comparisons</title>
      <link>https://arxiv.org/abs/2309.10642</link>
      <description>arXiv:2309.10642v4 Announce Type: replace-cross 
Abstract: This paper addresses the issue of sample selection bias when comparing countries using International assessments like PISA (Program for International Student Assessment). Despite its widespread use, PISA rankings may be biased due to different attrition patterns in different countries, leading to inaccurate comparisons. This study proposes a methodology to correct for sample selection bias using a quantile selection model. Applying the method to PISA 2018 data, I find that correcting for selection bias significantly changes the rankings (based on the mean) of countries' educational performances. My results highlight the importance of accounting for sample selection bias in international educational comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10642v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onil Boussim</dc:creator>
    </item>
    <item>
      <title>Derivation of outcome-dependent dietary patterns for low-income women obtained from survey data using a Supervised Weighted Overfitted Latent Class Analysis</title>
      <link>https://arxiv.org/abs/2310.01575</link>
      <description>arXiv:2310.01575v2 Announce Type: replace-cross 
Abstract: Poor diet quality is a key modifiable risk factor for hypertension and disproportionately impacts low-income women. \sw{Analyzing diet-driven hypertensive outcomes in this demographic is challenging due to the complexity of dietary data and selection bias when the data come from surveys, a main data source for understanding diet-disease relationships in understudied populations. Supervised Bayesian model-based clustering methods summarize dietary data into latent patterns that holistically capture relationships among foods and a known health outcome but do not sufficiently account for complex survey design. This leads to biased estimation and inference and lack of generalizability of the patterns}. To address this, we propose a supervised weighted overfitted latent class analysis (SWOLCA) based on a Bayesian pseudo-likelihood approach that integrates sampling weights into an exposure-outcome model for discrete data. Our model adjusts for stratification, clustering, and informative sampling, and handles modifying effects via interaction terms within a Markov chain Monte Carlo Gibbs sampling algorithm. Simulation studies confirm that the SWOLCA model exhibits good performance in terms of bias, precision, and coverage. Using data from the National Health and Nutrition Examination Survey (2015-2018), we demonstrate the utility of our model by characterizing dietary patterns associated with hypertensive outcomes among low-income women in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01575v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie M. Wu, Matthew R. Williams, Terrance D. Savitsky, Briana J. K. Stephenson</dc:creator>
    </item>
    <item>
      <title>Bayesian inversion of GPR waveforms for sub-surface material characterization: an uncertainty-aware retrieval of soil moisture and overlaying biomass properties</title>
      <link>https://arxiv.org/abs/2312.07928</link>
      <description>arXiv:2312.07928v2 Announce Type: replace-cross 
Abstract: Accurate estimation of sub-surface properties such as moisture content and depth of soil and vegetation layers is crucial for applications spanning sub-surface condition monitoring, precision agriculture, and effective wildfire risk assessment. Soil in nature is often covered by overlaying vegetation and surface organic material, making its characterization challenging. In addition, the estimation of the properties of the overlaying layer is crucial for applications like wildfire risk assessment. This study thus proposes a Bayesian model-updating-based approach for ground penetrating radar (GPR) waveform inversion to predict moisture contents and depths of soil and overlaying material layer. Due to its high correlation with moisture contents, the dielectric permittivity of both layers were predicted with the proposed method, along with other parameters, including depth and electrical conductivity of layers. The proposed Bayesian model updating approach yields probabilistic estimates of these parameters that can provide information about the confidence and uncertainty related to the estimates. The methodology was evaluated for a diverse range of experimental data collected through laboratory and field investigations. Laboratory investigations included variations in soil moisture values, depth of the overlaying surface layer, and coarseness of its material. The field investigation included measurement of field soil moisture for sixteen days. The results demonstrated predictions consistent with time-domain reflectometry (TDR) measurements and conventional gravimetric tests. The depth of the surface layer could also be predicted with reasonable accuracy. The proposed method provides a promising approach for uncertainty-aware sub-surface parameter estimation that can enable decision-making for risk assessment across a wide range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07928v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishfaq Aziz, Elahe Soltanaghai, Adam Watts, Mohamad Alipour</dc:creator>
    </item>
    <item>
      <title>Corrected Correlation Estimates for Meta-Analysis</title>
      <link>https://arxiv.org/abs/2404.11678</link>
      <description>arXiv:2404.11678v2 Announce Type: replace-cross 
Abstract: Meta-analysis allows rigorous aggregation of estimates and uncertainty across multiple studies. When a given study reports multiple estimates, such as log odds ratios (ORs) or log relative risks (RRs) across exposure groups, accounting for within-study correlations improves accuracy and efficiency of meta-analytic results. Canonical approaches of Greenland-Longnecker and Hamling estimate pseudo cases and non-cases for exposure groups to obtain within-study correlations. However, currently available implementations for both methods fail on simple examples.
  We review both GL and Hamling methods through the lens of optimization. For ORs, we provide modifications of each approach that ensure convergence for any feasible inputs. For GL, this is achieved through a new connection to entropic minimization. For Hamling, a modification leads to a provably solvable equivalent set of equations given a specific initialization. For each, we provide implementations a guaranteed to work for any feasible input.
  For RRs, we show the new GL approach is always guaranteed to succeed, but any Hamling approach may fail: we give counter-examples where no solutions exist. We derive a sufficient condition on reported RRs that guarantees success when reported variances are all equal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11678v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Johnson-V\'azquez, Alexander W. Hsu, Peng Zheng, Aleksandr Aravkin</dc:creator>
    </item>
    <item>
      <title>Estimating Metocean Environments Associated with Extreme Structural Response to Demonstrate the Dangers of Environmental Contour Methods</title>
      <link>https://arxiv.org/abs/2404.16775</link>
      <description>arXiv:2404.16775v2 Announce Type: replace-cross 
Abstract: Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Alternatively, environmental contours provide an approximate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We demonstrate a methodology for efficient fully probabilistic analysis of structural failure. From this, we estimate the joint conditional probability density of the environment (CDE), given the occurrence of an extreme structural response. We use CDE as a diagnostic to highlight the deficiencies of environmental contour methods for design; none of the IFORM environmental contours considered characterise CDE well for three example structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16775v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, David Randell, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Analysis of Linked Files: A Missing Data Perspective</title>
      <link>https://arxiv.org/abs/2406.14717</link>
      <description>arXiv:2406.14717v2 Announce Type: replace-cross 
Abstract: In many applications, researchers seek to identify overlapping entities across multiple data files. Record linkage algorithms facilitate this task, in the absence of unique identifiers. As these algorithms rely on semi-identifying information, they may miss records that represent the same entity, or incorrectly link records that do not represent the same entity. Analysis of linked files commonly ignores such linkage errors, resulting in biased, or overly precise estimates of the associations of interest. We view record linkage as a missing data problem, and delineate the linkage mechanisms that underpin analysis methods with linked files. Following the missing data literature, we group these methods under three categories: likelihood and Bayesian methods, imputation methods, and weighting methods. We summarize the assumptions and limitations of the methods, and evaluate their performance in a wide range of simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14717v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauri Kamat, Roee Gutman</dc:creator>
    </item>
  </channel>
</rss>
