<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mistake-bounded online learning with operation caps</title>
      <link>https://arxiv.org/abs/2509.03892</link>
      <description>arXiv:2509.03892v1 Announce Type: cross 
Abstract: We investigate the mistake-bound model of online learning with caps on the number of arithmetic operations per round. We prove general bounds on the minimum number of arithmetic operations per round that are necessary to learn an arbitrary family of functions with finitely many mistakes. We solve a problem on agnostic mistake-bounded online learning with bandit feedback from (Filmus et al, 2024) and (Geneson \&amp; Tang, 2024). We also extend this result to the setting of operation caps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03892v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Geneson, Meien Li, Linus Tang</dc:creator>
    </item>
    <item>
      <title>Generalized and Unified Equivalences between Hardness and Pseudoentropy</title>
      <link>https://arxiv.org/abs/2507.05972</link>
      <description>arXiv:2507.05972v2 Announce Type: replace 
Abstract: Pseudoentropy characterizations provide a quantitatively precise demonstration of the close relationship between computational hardness and computational randomness. We prove a unified pseudoentropy characterization that generalizes and strengthens previous results for both uniform and non-uniform models of computation. Our characterization holds for a general family of entropy notions that encompasses the common notions of Shannon entropy and min entropy as special cases. Moreover, we show that the characterizations for different entropy notions can be simultaneously achieved by a single, universal function that simultaneously witnesses computational hardness and computational randomness. A key technical insight of our work is that the notion of weight-restricted calibration from the recent literature on algorithm fairness, along with standard computational indistinguishability (known as multiaccuracy in the fairness literature), suffices for proving pseudoentropy characterizations for general entropy notions. This demonstrates the power of weight-restricted calibration to enhance the classic Complexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009) and Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to achieve an exponential improvement in the complexity dependency on the alphabet size compared to the pseudoentropy characterizations by Casacuberta, Dwork, and Vadhan (2024) based on the much stronger notion of multicalibration. We show that the exponential dependency on the alphabet size is inevitable for multicalibration as well as for the weaker notion of calibrated multiaccuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05972v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lunjia Hu, Salil Vadhan</dc:creator>
    </item>
    <item>
      <title>StoqMA vs. MA: the power of error reduction</title>
      <link>https://arxiv.org/abs/2010.02835</link>
      <description>arXiv:2010.02835v4 Announce Type: replace-cross 
Abstract: StoqMA characterizes the computational hardness of stoquastic local Hamiltonians, which is a family of Hamiltonians that does not suffer from the sign problem. Although error reduction is commonplace for many complexity classes, such as BPP, BQP, MA, QMA, etc.,this property remains open for StoqMA since Bravyi, Bessen and Terhal defined this class in 2006. In this note, we show that error reduction forStoqMA will imply that StoqMA = MA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.02835v4</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dorit Aharonov, Alex B. Grilo, Yupan Liu</dc:creator>
    </item>
    <item>
      <title>Expected Complexity of Persistence Barcode Computation via Matrix Reduction</title>
      <link>https://arxiv.org/abs/2111.02125</link>
      <description>arXiv:2111.02125v5 Announce Type: replace-cross 
Abstract: We study the algorithmic complexity of computing the persistence barcode of a randomly generated filtration. We provide a general technique to bound the expected complexity of reducing the boundary matrix in terms of the density of its reduced form. We apply this technique finding upper bounds for the average fill-in (number of non-zero entries) of the boundary matrix on \v{C}ech, Vietoris--Rips and Erd\H{o}s--R\'enyi filtrations after matrix reduction, thus obtaining bounds on the expected complexity of the barcode computation. Our method is based on previous results on the expected Betti numbers of the corresponding complexes. Our fill-in bounds for \v{C}ech and Vietoris--Rips complexes are asymptotically tight up to a logarithmic factor. In particular, both our fill-in and computation bounds are better than the worst-case estimates. We also provide an Erd\H{o}s--R\'enyi filtration realising the worst-case fill-in and computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.02125v5</guid>
      <category>math.AT</category>
      <category>cs.CC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barbara Giunti, Guillaume Houry, Michael Kerber, Matthias S\"ols</dc:creator>
    </item>
    <item>
      <title>Exponential improvements to the average-case hardness of BosonSampling</title>
      <link>https://arxiv.org/abs/2411.04566</link>
      <description>arXiv:2411.04566v2 Announce Type: replace-cross 
Abstract: BosonSampling and Random Circuit Sampling are important both as a theoretical tool for separating quantum and classical computation, and as an experimental means of demonstrating quantum speedups. Prior works have shown that average-case hardness of sampling follows from certain unproven conjectures about the hardness of computing output probabilities, such as the Permanent-of-Gaussians Conjecture (PGC), which states that $e^{-n\log{n}-n-O(\log n)}$ additive-error estimates to the output probability of most random BosonSampling experiments are $\#P$-hard. Prior works have only shown weaker average-case hardness results that do not imply sampling hardness. Proving these conjectures has become a central question in quantum complexity.
  In this work, we show that $e^{-n\log n-n-O(n^\delta)}$ additive-error estimates to output probabilities of most random BosonSampling experiments are $\#P$-hard for any $\delta&gt;0$, exponentially improving on prior work. In the process, we circumvent all known barrier results for proving PGC. The remaining hurdle to prove PGC is now "merely" to show that the $O(n^\delta)$ in the exponent can be improved to $O(\log n).$ We also obtain an analogous result for Random Circuit Sampling.
  We then show, for the first time, a hardness of average-case classical sampling result for BosonSampling, under an anticoncentration conjecture. Specifically, we prove the impossibility of multiplicative-error sampling from random BosonSampling experiments with probability $1-2^{-\tilde{\mathstrut O}(N^{1/3})}$ for input size $N$, unless the Polynomial Hierarchy collapses. This exponentially improves upon the state-of-the-art. To do this, we introduce new proof techniques which tolerate exponential loss in the worst-to-average-case reduction. This opens the possibility to show the hardness of average-case sampling without ever proving PGC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04566v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bouland, Ishaun Datta, Bill Fefferman, Felipe Hernandez</dc:creator>
    </item>
    <item>
      <title>Bounding the computational power of bosonic systems</title>
      <link>https://arxiv.org/abs/2503.03600</link>
      <description>arXiv:2503.03600v3 Announce Type: replace-cross 
Abstract: Bosonic quantum systems operate in an infinite-dimensional Hilbert space, unlike discrete-variable quantum systems. This distinct mathematical structure leads to fundamental differences in quantum information processing, such as an exponentially greater complexity of state tomography [MMB+24] or a factoring algorithm in constant space [BCCRK24]. Yet, it remains unclear whether this structural difference of bosonic systems may also translate to a practical computational advantage over finite-dimensional quantum computers. Here we take a step towards answering this question by showing that universal bosonic quantum computations can be simulated in polynomial space (and exponential time) on a classical computer, significantly improving the best previous upper bound requiring exponential memory [CJMM24]. In complexity-theoretic terms, we improve the best upper bound on $\textsf{CVBQP}$ from $ \textsf{EXPSPACE}$ to $\textsf{PSPACE}$. This result is achieved using a simulation strategy based on finite energy cutoffs and approximate coherent state decompositions. While we propose ways to potentially refine this bound, we also present arguments supporting the plausibility of an exponential computational advantage of bosonic quantum computers over their discrete-variable counterparts. Furthermore, we emphasize the role of circuit energy as a resource and discuss why it may act as the fundamental bottleneck in realizing this advantage in practical implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03600v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Varun Upreti, Dorian Rudolph, Ulysse Chabaud</dc:creator>
    </item>
  </channel>
</rss>
