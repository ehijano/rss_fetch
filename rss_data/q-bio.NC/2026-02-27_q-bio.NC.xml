<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatiotemporal bursting in simulated cultures of cortical neurons</title>
      <link>https://arxiv.org/abs/2602.22364</link>
      <description>arXiv:2602.22364v1 Announce Type: new 
Abstract: Cultures of neurons grown on multi-electrode arrays have become a common experimental preparation for investigating developing neural networks. Experiment and simulation have shown that these developing networks eventually exhibit bursting behavior in which the entire culture participates for short periods of time, with inter-burst intervals in which the network is comparatively quiescent. This paper extends previous simulation results by examining the spatiotemporal patterns of such bursting. We show that these bursts originate at a small number of network locations and propagate as waves of activity. We demonstrate that this type of activity does not require fine tuning of neuron or network parameters. We also examine how this activity changes during development and the dependence of such activity and its triggering on both local and global network properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22364v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Stiber, Natalie Gonzales, Jewel YunHsuan Lee</dc:creator>
    </item>
    <item>
      <title>SPD Learn: A Geometric Deep Learning Python Library for Neural Decoding Through Trivialization</title>
      <link>https://arxiv.org/abs/2602.22895</link>
      <description>arXiv:2602.22895v1 Announce Type: new 
Abstract: Implementations of symmetric positive definite (SPD) matrix-based neural networks for neural decoding remain fragmented across research codebases and Python packages. Existing implementations often employ ad hoc handling of manifold constraints and non-unified training setups, which hinders reproducibility and integration into modern deep-learning workflows. To address this gap, we introduce SPD Learn, a unified and modular Python package for geometric deep learning with SPD matrices. SPD Learn provides core SPD operators and neural-network layers, including numerically stable spectral operators, and enforces Stiefel/SPD constraints via trivialization-based parameterizations. This design enables standard backpropagation and optimization in unconstrained Euclidean spaces while producing manifold-constrained parameters by construction. The package also offers reference implementations of representative SPDNet-based models and interfaces with widely used brain computer interface/neuroimaging toolkits and modern machine-learning libraries (e.g., MOABB, Braindecode, Nilearn, and SKADA), facilitating reproducible benchmarking and practical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22895v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bruno Aristimunha, Ce Ju, Antoine Collas, Florent Bouchard, Ammar Mian, Bertrand Thirion, Sylvain Chevallier, Reinmar Kobler</dc:creator>
    </item>
    <item>
      <title>Collective Dynamics in Spiking Neural Networks Beyond Dale's Principle</title>
      <link>https://arxiv.org/abs/2602.23202</link>
      <description>arXiv:2602.23202v1 Announce Type: new 
Abstract: Dale's Principle has historically guided neuroscience research as a valuable rule of thumb, namely that all synapses on each neuron release the same set of neurotransmitters. Most existing Spiking Neuron Network models share this dichotomous assumption that neurons are either excitatory or inhibitory; however, recent experimental evidence points towards co-release mechanisms that violate this assumption. Here, we introduce a minimal model of "Bilingual" neurons violating Dale's principle that can exert both excitatory and inhibitory effects. We identify parameter regimes in which this architecture exhibits transitions between synchronous and asynchronous dynamics that differ quantitatively from those observed in a matched monolingual control architecture. We report distinct information-processing signatures both at the level of neurons and higher-order interactions between them near the phase transitions. These results suggest that the population of neurons violating Dales principle may provide an alternative mechanism for regulating large-scale oscillatory activity in neural circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23202v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ross Ah-Weng, Hardik Rajpal</dc:creator>
    </item>
    <item>
      <title>Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus</title>
      <link>https://arxiv.org/abs/2602.22408</link>
      <description>arXiv:2602.22408v1 Announce Type: cross 
Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22408v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caroline Ahn, Quan Do, Leah Bakst, Michael P. Pascale, Joseph T. McGuire, Michael E. Hasselmo, Chantal E. Stern</dc:creator>
    </item>
    <item>
      <title>Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents</title>
      <link>https://arxiv.org/abs/2602.22523</link>
      <description>arXiv:2602.22523v1 Announce Type: cross 
Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22523v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Liu, Dilip Arumugam, Cedegao E. Zhang, Sean Escola, Xaq Pitkow, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Exploiting network topology in brain-scale simulations of spiking neural networks</title>
      <link>https://arxiv.org/abs/2602.23274</link>
      <description>arXiv:2602.23274v1 Announce Type: cross 
Abstract: Simulation code for conventional supercomputers serves as a reference for neuromorphic computing systems. The present bottleneck of distributed large-scale spiking neuronal network simulations is the communication between compute nodes. Communication speed seems limited by the interconnect between the nodes and the software library orchestrating the data transfer. Profiling reveals, however, that the variability of the time required by the compute nodes between communication calls is large. The bottleneck is in fact the waiting time for the slowest node. A statistical model explains total simulation time on the basis of the distribution of computation times between communication calls. A fundamental cure is to avoid communication calls because this requires fewer synchronizations and reduces the variability of computation times across compute nodes. The organization of the mammalian brain into areas lends itself to such an optimization strategy. Connections between neurons within an area have short delays, but the delays of the long-range connections across areas are an order of magnitude longer. This suggests a structure-aware mapping of areas to compute nodes allowing for a partition into more frequent communication between nodes simulating a particular area and less frequent global communication. We demonstrate a substantial performance gain on a real-world example. This work proposes a local-global hybrid communication architecture for large-scale neuronal network simulations as a first step in mapping the structure of the brain to the structure of a supercomputer. It challenges the long-standing belief that the bottleneck of simulation is synchronization inherent in the collective calls of standard communication libraries. We provide guidelines for the energy efficient simulation of neuronal networks on conventional computing systems and raise the bar for neuromorphic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23274v1</guid>
      <category>cs.DC</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Melissa Lober, Markus Diesmann, Susanne Kunkel</dc:creator>
    </item>
    <item>
      <title>Representational drift changes the encoding of fast and slow-varying natural scene features differently</title>
      <link>https://arxiv.org/abs/2305.11953</link>
      <description>arXiv:2305.11953v3 Announce Type: replace 
Abstract: Representational drift refers to an unstable mapping between neural activity and input sensory or output behavioral variables. While much work has focused on the effect of representational drift on single, simple external variables, we investigate the differences in representational drift across spatiotemporal features in a moving visual stimulus. The neural responses across animals to the same movie reflect both common, encoded stimulus features and idiosyncratic individual variation. To extract the shared neural encoding of stimulus features only, we learn a latent space embedding using weakly supervised contrastive learning. This approach pulls neural activity together in the embedding space if they are responses to the same stimulus segment and push them apart if not. This approach enables us to probe how stimulus features fluctuating as fast as 33 ms (the movie frame rate) are encoded by variable neural codes across animals. It also allows us to investigate how representational drift changes the encoding in individuals across sessions. We observe that our learned embedding is near-optimal for decoding natural features (background scenery, local motion, complex spatio-temporal features, and time) and neural activity from novel animals. This suggests that our embedding retains the encoding of multiple features at higher temporal granularity compared to previous methods. To quantify representational drift, we apply the trained decoder (which achieves near-optimal performance in one session) to a subsequent session recorded 90 minutes later. We then use the decrease in decoding performance as a proxy for the magnitude of drift. We show that the drift changes the encoding of fast-varying local motion features at a rate 5-6 times higher than slower-varying scenery features. Drift also perturbs the local geometry in the embedding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11953v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siwei Wang, Elizabeth A de Laittre, Jason MacLean, Stephanie E Palmer</dc:creator>
    </item>
    <item>
      <title>Atlas-free Brain Network Transformer</title>
      <link>https://arxiv.org/abs/2510.03306</link>
      <description>arXiv:2510.03306v2 Announce Type: replace 
Abstract: Current atlas-based approaches to brain network analysis rely heavily on standardized anatomical or connectivity-driven brain atlases. However, these fixed atlases often introduce significant limitations, such as spatial misalignment across individuals, functional heterogeneity within predefined regions, and atlas-selection biases, collectively undermining the reliability and interpretability of the derived brain networks. To address these challenges, we propose a novel atlas-free brain network transformer (atlas-free BNT) that leverages individualized brain parcellations derived directly from subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel connectivity features in a standardized voxel-based feature space, which are subsequently processed using the BNT architecture to produce comparable subject-level embeddings. Experimental evaluations on sex classification and brain-connectome age prediction tasks demonstrate that our atlas-free BNT consistently outperforms state-of-the-art atlas-based methods, including elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach significantly improves the precision, robustness, and generalizability of brain network analyses. This advancement holds great potential to enhance neuroimaging biomarkers and clinical diagnostic tools for personalized precision medicine. Reproducible code is available at https://github.com/shuai-huang/atlas_free_bnt</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03306v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Huang, Xuan Kan, James J. Lah, Deqiang Qiu</dc:creator>
    </item>
    <item>
      <title>Joint encoding of "what" and "when" predictions through error-modulated plasticity in biologically-plausible spiking networks</title>
      <link>https://arxiv.org/abs/2510.14382</link>
      <description>arXiv:2510.14382v3 Announce Type: replace 
Abstract: The brain anticipates future events using internal models that specify not only what will occur, but also when it will occur and with what probability. We refer to this joint specification of identity, timing, and likelihood as a complete prediction object. Existing computational models typically capture identity and timing separately, omit probability as an explicit representational dimension, or rely on biologically implausible global learning rules. Here we show that a single population of spiking neurons can acquire and flexibly maintain a complete prediction object through biologically grounded learning. We implemented a heterogeneous Izhikevich spiking reservoir with multiplexed readouts trained by an error-modulated, attention-gated three-factor Hebbian rule, and tested it on a task that independently manipulates event identity, latency, and probability. The network develops time-locked anticipatory activity whose amplitude scales with outcome probability and rapidly adapts when timing or probability statistics change. Identity and timing components self-organize into near-orthogonal readout subspaces within a shared neural population, demonstrating that multidimensional predictive structure can emerge without anatomical modularization or global error broadcast. Compared with least-squares-based approaches, local gated plasticity enables stable recalibration under nonstationary conditions. These results suggest that cortical mixed-selective populations, coupled with neuromodulator-gated synaptic plasticity, may be sufficient to jointly encode and update identity, timing, and probability within a single recurrent circuit. Flexible predictive cognition may therefore arise from generic population dynamics shaped by local learning rules rather than from specialized predictive modules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14382v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yohei Yamada, Zenas C. Chao</dc:creator>
    </item>
    <item>
      <title>Learning Task-Agnostic Motifs to Capture the Continuous Nature of Animal Behavior</title>
      <link>https://arxiv.org/abs/2506.15190</link>
      <description>arXiv:2506.15190v3 Announce Type: replace-cross 
Abstract: Animals flexibly recombine a finite set of core motor motifs to meet diverse task demands, but existing behavior segmentation methods oversimplify this process by imposing discrete syllables under restrictive generative assumptions. To better capture the continuous structure of behavior generation, we introduce motif-based continuous dynamics (MCD) discovery, a framework that (1) uncovers interpretable motif sets as latent basis functions of behavior by leveraging representations of behavioral transition structure, and (2) models behavioral dynamics as continuously evolving mixtures of these motifs. We validate MCD on a multi-task gridworld, a labyrinth navigation task, and freely moving animal behavior. Across settings, it identifies reusable motif components, captures continuous compositional dynamics, and generates realistic trajectories beyond the capabilities of traditional discrete segmentation models. By providing a generative account of how complex animal behaviors emerge from dynamic combinations of fundamental motor motifs, our approach advances the quantitative study of natural behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15190v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiyi Wang, Jingyang Ke, Bo Dai, Anqi Wu</dc:creator>
    </item>
  </channel>
</rss>
