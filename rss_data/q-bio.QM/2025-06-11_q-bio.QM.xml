<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 04:10:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Localizing synergies of hidden factors across complex systems: resting brain networks and HeLa gene expression profile as case studies</title>
      <link>https://arxiv.org/abs/2506.09053</link>
      <description>arXiv:2506.09053v1 Announce Type: new 
Abstract: Factor analysis is a well-known statistical method to describe the variability of observed variables in terms of a smaller number of unobserved latent variables called factors. Even though latent factors are conceptually independent of each other, their influence on the observed variables is often joint and synergistic. We propose to quantify the synergy of the joint influence of factors on the observed variables using the O-information, a recently introduced metrics to assess high order dependencies in complex systems, in a new framework where latent factors and observed variables are jointly analyzed in terms of their joint informational character. Two case studies are reported: analyzing resting fMRI data, we find that DMN and FP networks show the highest synergy, consistently with their crucial role in higher cognitive functions; concerning HeLa cells, we find that the most synergistic gene is STK-12 (AURKB), suggesting that this gene is involved in controlling the HeLa cell cycle. We believe that this approach, representing a bridge between factor analysis and the field of high-order interactions, will find wide application across several domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09053v1</guid>
      <category>q-bio.QM</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marlis Ontivero-Ortega, Gorana Mijatovic, Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia</dc:creator>
    </item>
    <item>
      <title>Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery</title>
      <link>https://arxiv.org/abs/2506.09063</link>
      <description>arXiv:2506.09063v1 Announce Type: new 
Abstract: Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called CryoSPIRE, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09063v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayan Shekarforoush, David B. Lindell, Marcus A. Brubaker, David J. Fleet</dc:creator>
    </item>
    <item>
      <title>Detecting malignant dynamics on very few blood sample using signature coefficients</title>
      <link>https://arxiv.org/abs/2506.09097</link>
      <description>arXiv:2506.09097v1 Announce Type: new 
Abstract: Recent discoveries have suggested that the promising avenue of using circulating tumor DNA (ctDNA) levels in blood samples provides reasonable accuracy for cancer monitoring, with extremely low burden on the patient's side. It is known that the presence of ctDNA can result from various mechanisms leading to DNA release from cells, such as apoptosis, necrosis or active secretion. One key idea in recent cancer monitoring studies is that monitoring the dynamics of ctDNA levels might be sufficient for early multi-cancer detection. This interesting idea has been turned into commercial products, e.g. in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for detecting aggressive cancer tumors based on the analysis of blood samples. Our approach combines tools from continuous time Markov modelling for the dynamics of ctDNA levels in the blood, with Signature theory for building efficient testing procedures. Signature theory is a topic of growing interest in the Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now recognised as a powerful feature extraction tool for irregularly sampled signals. The method proposed in the present paper is shown to correctly address the challenging problem of overcoming the inherent data scarsity due to the extremely small number of blood samples per patient. The relevance of our approach is illustrated with extensive numerical experiments that confirm the efficiency of the proposed pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09097v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Vaucher, St\'ephane Chr\'etien</dc:creator>
    </item>
    <item>
      <title>Simulation-trained conditional normalizing flows for likelihood approximation: a case study in stress regulation kinetics in yeast</title>
      <link>https://arxiv.org/abs/2506.09374</link>
      <description>arXiv:2506.09374v1 Announce Type: new 
Abstract: Physics-inspired inference often hinges on the ability to construct a likelihood, or the probability of observing a sequence of data given a model. These likelihoods can be directly maximized for parameter estimation, incorporated into Bayesian frameworks, or even used as loss functions in neural networks. Yet, many models, despite being conceptually simple, lack tractable likelihoods. A notable example arises in estimating protein production from snapshot measurements of actively dividing cells. Here, the challenge stems from cell divisions occurring at non-Exponentially distributed intervals with each division stochastically partitioning protein content between daughter cells, making protein counts in any given cell a function of its full division history. Such history dependence precludes a straightforward likelihood based on a (standard Markovian) master equation. Instead, we employ conditional normalizing flows (a class of neural network models designed to learn probability distributions) to approximate otherwise intractable likelihoods from simulated data. As a case study, we examine activation of the \emph{glc3} gene in yeast involved in glycogen synthesis and expressed under nutrient-limiting conditions. We monitor this activity using snapshot fluorescence measurements via flow cytometry, where GFP expression reflects \emph{glc3} promoter activity. A na\"ive analysis of flow cytometry data ignoring cell division suggests many cells are active with low expression. However, fluorescent proteins persist and can be inherited, so cells may appear active from retaining ancestral fluorescence. Explicitly accounting for the (non-Markovian) effects of cell division reveals \emph{glc3} is mostly inactive under stress, showing that while cells occasionally activate it, expression is brief and transient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09374v1</guid>
      <category>q-bio.QM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pedro Pessoa, Juan Andres Martinez, Vincent Vandenbroucke, Frank Delvigne, Steve Press\'e</dc:creator>
    </item>
    <item>
      <title>Geometry Reduced Order Modeling (GROM) with application to modeling of glymphatic function</title>
      <link>https://arxiv.org/abs/2506.09442</link>
      <description>arXiv:2506.09442v1 Announce Type: new 
Abstract: Computational modeling of the brain has become a key part of understanding how the brain clears metabolic waste, but patient-specific modeling on a significant scale is still out of reach with current methods. We introduce a novel approach for leveraging model order reduction techniques in computational models of brain geometries to alleviate computational costs involved in numerical simulations. Using image registration methods based on magnetic resonance imaging, we compute inter-brain mappings which allow previously computed solutions on other geometries to be mapped on to a new geometry. We investigate this approach on two example problems typical of modeling of glymphatic function, applied to a dataset of 101 MRI of human patients. We discuss the applicability of the method when applied to a patient with no known neurological disease, as well as a patient diagnosed with idiopathic Normal Pressure Hydrocephalus displaying significantly enlarged ventricles</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09442v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Solheim, Geir Ringstand, Per Kristian Eide, Kent-Andre Mardal</dc:creator>
    </item>
    <item>
      <title>Image-Based Biospeckle Contrast Analysis for Rapid Antimicrobial Susceptibility Testing</title>
      <link>https://arxiv.org/abs/2506.09604</link>
      <description>arXiv:2506.09604v1 Announce Type: new 
Abstract: Purpose: Antimicrobial resistance is a major global health concern, affecting hospital admissions and treatment success. This study aims to introduce an experimental setup for monitoring bacterial activity over time using image-based contrast as a key biomarker. Methods: The proposed method captures changes in bacterial activity by analyzing variations in image contrast. The approach is evaluated for its ability to detect antimicrobial effects over shorter time intervals compared to conventional clinical techniques. Results: The findings reveal a progressive decrease in contrast over time, suggesting its potential as an indicator of antimicrobial activity. The results highlight the method's capability for early detection of bacterial susceptibility. Conclusion: The study demonstrates that image-based contrast analysis can serve as a rapid and reliable tool for antimicrobial susceptibility testing, offering advantages over traditional methods in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09604v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. A. Gameiro, R. F. Pinto, N. V. Lopes</dc:creator>
    </item>
    <item>
      <title>Data-Driven Modeling of IRCU Patient Flow in the COVID-19 Pandemic</title>
      <link>https://arxiv.org/abs/2506.09751</link>
      <description>arXiv:2506.09751v1 Announce Type: new 
Abstract: Intermediate Respiratory Care Units (IRCUs) are vital during crises like COVID-19. This study evaluated clinical outcomes and operational dynamics of a new Spanish IRCU with specialized staffing. A prospective cohort study (April-August 2021) included 249 adult patients with COVID-19 respiratory failure (UHVN IRCU, Granada). Data on demographics, Non-Invasive Ventilation (NIV), length of stay (LOS), and outcomes (ICU transfer, exitus, recovery) were analyzed. Patient flow was simulated using a data-calibrated deterministic compartmental model (Ordinary Differential Equations, ODEs) that represented state transitions, and an empirical LOS-based stochastic convolution model that incorporated admission variability. The median age was 51; 31% of patients required NIV. NIV patients were older (median 61 vs 42, p&lt;0.001). Overall, 8% needed ICU transfer; 3% experienced in-IRCU exitus. Notably, no ICU transfers or deaths occurred among 172 non-NIV patients. Of 77 high-risk NIV patients, 68% recovered in IRCU without ICU escalation. The ODE model, based on transition rates between patient states, reflected aggregate outcomes. Both modeling approaches demonstrated system strain during admission surges (partially mitigated by simulated care efficiency improvements via parameter modulation) and yielded consistent peak occupancy estimates. This IRCU, with specialized staffing, effectively managed severe COVID-19. High recovery rates, especially for NIV patients, potentially eased ICU pressure. Dynamic modeling confirmed surge vulnerability but highlighted the benefits of care efficiency from modulated transition parameters. Findings underscore positive outcomes in this IRCU model and support such units in pandemic response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09751v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Carmen Navas-Ortega, Jos\'e Antonio S\'anchez-Mart\'inez, Paula Garc\'ia-Flores, Concepci\'on Morales-Garc\'ia, Rene Fabregas</dc:creator>
    </item>
    <item>
      <title>Branched Schr\"odinger Bridge Matching</title>
      <link>https://arxiv.org/abs/2506.09007</link>
      <description>arXiv:2506.09007v1 Announce Type: cross 
Abstract: Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09007v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sophia Tang, Yinuo Zhang, Alexander Tong, Pranam Chatterjee</dc:creator>
    </item>
    <item>
      <title>Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture</title>
      <link>https://arxiv.org/abs/2506.09052</link>
      <description>arXiv:2506.09052v1 Announce Type: cross 
Abstract: Antibody-facilitated immune responses are central to the body's defense against pathogens, viruses, and other foreign invaders. The ability of antibodies to specifically bind and neutralize antigens is vital for maintaining immunity. Over the past few decades, bioengineering advancements have significantly accelerated therapeutic antibody development. These antibody-derived drugs have shown remarkable efficacy, particularly in treating cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases. Traditionally, experimental methods for affinity measurement have been time-consuming and expensive. With the advent of artificial intelligence, in silico medicine has been revolutionized; recent developments in machine learning, particularly the use of large language models (LLMs) for representing antibodies, have opened up new avenues for AI-based design and improved affinity prediction. Herein, we present an advanced antibody-antigen binding affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3 backbone and antibody sequence data sourced from the Observed Antibody Space (OAS) database. The proposed approach shows significant improvement over existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy) across multiple evaluation metrics. Specifically, the model achieved an accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of 0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher computational efficiency, with a five-fold average cumulative training time of only 0.46 hours, significantly lower than in previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09052v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Delower Hossain, Ehsan Saghapour, Kevin Song, Jake Y. Chen</dc:creator>
    </item>
    <item>
      <title>Estimating Visceral Adiposity from Wrist-Worn Accelerometry</title>
      <link>https://arxiv.org/abs/2506.09167</link>
      <description>arXiv:2506.09167v1 Announce Type: cross 
Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and habitual physical activity (PA). Excess VAT is highly correlated with type 2 diabetes and insulin resistance. The mechanistic basis for this pathophysiology relates to overloading the liver with fatty acids. VAT is also a highly labile fat depot, with increased turnover stimulated by catecholamines during exercise. VAT can be measured with sophisticated imaging technologies, but can also be inferred directly from PA. We tested this relationship using National Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men; 2,427 women) [1]. Two approaches were used for estimating VAT from activity. The first used engineered features based on movements during gait and sleep, and then ridge regression to map summary statistics of these features into a VAT estimate. The second approach used deep neural networks trained on 24 hours of continuous accelerometry. A foundation model first mapped each 10s frame into a high-dimensional feature vector. A transformer model then mapped each day's feature vector time series into a VAT estimate, which were averaged over multiple days. For both approaches, the most accurate estimates were obtained with the addition of covariate information about subject demographics and body measurements. The best performance was obtained by combining the two approaches, resulting in VAT estimates with correlations of r=0.86. These findings demonstrate a strong relationship between PA and VAT and, by extension, between PA and metabolic health risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09167v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James R. Williamson, Andrew Alini, Brian A. Telfer, Adam W. Potter, Karl E. Friedl</dc:creator>
    </item>
    <item>
      <title>Predicting Nanoparticle Effects on Small Biomolecule Functionalities Using the Capability of Scikit-learn and PyTorch: A Case Study on Inhibitors of the DNA Damage-Inducible Transcript 3 (CHOP)</title>
      <link>https://arxiv.org/abs/2504.09537</link>
      <description>arXiv:2504.09537v2 Announce Type: replace 
Abstract: The presented study contributes to ongoing research that aims to overcome challenges in predicting the bio-applicability of nanoparticles (NPs). The approach explored a variety of combinations of nuclear magnetic resonance (NMR) spectroscopy data derived from the Simplified molecular-input line-entry system (SMILES) notations and small biomolecule features. The resulting datasets were utilised for machine learning (ML) with scikit-learn and deep neural networks (DNN) with PyTorch. Despite the obstacles in predicting how NPs influence biomolecule functionalities, the methodology was reasoned in terms of its applicability to compounds both with and without NPs. The methodology was illustrated through a quantitative high-throughput screening (qHTS) aimed at finding DNA Damage-Inducible Transcript 3 (CHOP) inhibitors. Based on this data, the optimal ML performance was obtained by the Random Forest Classifier, which was trained with 19,184 samples and tested with 4,000 achieving 81.1% accuracy, 83.4% precision, 77.7% recall, 80.4% F1-score, 81.1% ROC and 0.821 five-fold cross validation score. Complementing the main research, the paper introduces two computational applications for CHOP inhibition drug discovery. The first approach identifies the most desirable and undesirable functional groups/fragments for CHOP inhibition, with a hypothetical application to nanoformulations (NFs) as well. The second one developed a CID_SID ML model that solely relies on the PubChem identifiers to predict whether an already designed compound possesses CHOP inhibition potential (90.1 % accuracy) and thus contributes to the detection of such a side effect</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09537v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariya L. Ivanova, Nicola Russo, Konstantin Nikolic</dc:creator>
    </item>
    <item>
      <title>DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images</title>
      <link>https://arxiv.org/abs/2409.06694</link>
      <description>arXiv:2409.06694v3 Announce Type: replace-cross 
Abstract: Cancer is a complex disease characterized by uncontrolled cell growth. T cell receptors (TCRs), crucial proteins in the immune system, play a key role in recognizing antigens, including those associated with cancer. Recent advancements in sequencing technologies have facilitated comprehensive profiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity and enabling TCR-based immunotherapies. However, analyzing these intricate biomolecules necessitates efficient representations that capture their structural and functional information. T-cell protein sequences pose unique challenges due to their relatively smaller lengths compared to other biomolecules. An image-based representation approach becomes a preferred choice for efficient embeddings, allowing for the preservation of essential details and enabling comprehensive analysis of T-cell protein sequences. In this paper, we propose to generate images from the protein sequences using the idea of Chaos Game Representation (CGR) using the Kaleidoscopic images approach. This Deep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images (called DANCE) provides a unique way to visualize protein sequences by recursively applying chaos game rules around a central seed point. we perform the classification of the T cell receptors (TCRs) protein sequences in terms of their respective target cancer cells, as TCRs are known for their immune response against cancer disease. The TCR sequences are converted into images using the DANCE method. We employ deep-learning vision models to perform the classification to obtain insights into the relationship between the visual patterns observed in the generated kaleidoscopic images and the underlying protein properties. By combining CGR-based image generation with deep learning classification, this study opens novel possibilities in the protein analysis domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06694v3</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Taslim Murad, Prakash Chourasia, Sarwan Ali, Imdad Ullah Khan, Murray Patterson</dc:creator>
    </item>
    <item>
      <title>Dataset Properties Shape the Success of Neuroimaging-Based Patient Stratification: A Benchmarking Analysis Across Clustering Algorithms</title>
      <link>https://arxiv.org/abs/2503.12066</link>
      <description>arXiv:2503.12066v2 Announce Type: replace-cross 
Abstract: Background: Data driven stratification of patients into biologically informed subtypes holds promise for precision neuropsychiatry, yet neuroimaging-based clustering methods often fail to generalize across cohorts. While algorithmic innovations have focused on model complexity, the role of underlying dataset characteristics remains underexplored. We hypothesized that cluster separation, size imbalance, noise, and the direction and magnitude of disease-related effects in the input data critically determine both within-algorithm accuracy and reproducibility. Methods: We evaluated 4 widely used stratification algorithms, HYDRA, SuStaIn, SmileGAN, and SurrealGAN, on a suite of synthetic brain-morphometry cohorts derived from the Human Connectome Project Young Adult dataset. Three global transformation patterns were applied to 600 pseudo-patients against 508 controls, followed by 4 within-dataset variations varying cluster count (k=2-6), overlap, and effect magnitude. Algorithm performance was quantified by accuracy in recovering the known ground-truth clusters. Results: Across 122 synthetic scenarios, data complexity consistently outweighed algorithm choice in predicting stratification success. Well-separated clusters yielded high accuracy for all methods, whereas overlapping, unequal-sized, or subtle effects reduced accuracy by up to 50%. SuStaIn could not scale beyond 17 features, HYDRA's accuracy varied unpredictably with data heterogeneity. SmileGAN and SurrealGAN maintained robust pattern detection but did not assign discrete cluster labels to individuals. Conclusions: The study results demonstrate the impact of statistical properties of input data across algorithms and highlight the need for using realistic dataset distributions when new algorithms are being developed and suggest greater focus on data-centric strategies that actively shape and standardize the input distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12066v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuetong Yu, Ruiyang Ge, Ilker Hacihaliloglu, Alexander Rauscher, Roger Tam, Sophia Frangou</dc:creator>
    </item>
    <item>
      <title>ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2506.07459</link>
      <description>arXiv:2506.07459v2 Announce Type: replace-cross 
Abstract: Protein generative models have shown remarkable promise in protein design but still face limitations in success rate, due to the scarcity of high-quality protein datasets for supervised pretraining. We present ProteinZero, a novel framework that enables scalable, automated, and continuous self-improvement of the inverse folding model through online reinforcement learning. To achieve computationally tractable online feedback, we introduce efficient proxy reward models based on ESM-fold and a novel rapid ddG predictor that significantly accelerates evaluation speed. ProteinZero employs a general RL framework balancing multi-reward maximization, KL-divergence from a reference model, and a novel protein-embedding level diversity regularization that prevents mode collapse while promoting higher sequence diversity. Through extensive experiments, we demonstrate that ProteinZero substantially outperforms existing methods across every key metric in protein design, achieving significant improvements in structural accuracy, designability, thermodynamic stability, and sequence diversity. Most impressively, ProteinZero reduces design failure rates by approximately 36% - 48% compared to widely-used methods like ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates exceeding 90% across diverse and complex protein folds. Notably, the entire RL run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days, including reward computation. Our work establishes a new paradigm for protein design where models evolve continuously from their own generated outputs, opening new possibilities for exploring the vast protein design space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07459v2</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziwen Wang, Jiajun Fan, Ruihan Guo, Thao Nguyen, Heng Ji, Ge Liu</dc:creator>
    </item>
  </channel>
</rss>
