<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Jun 2025 04:01:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Revisiting mean estimation over $\ell_p$ balls: Is the MLE optimal?</title>
      <link>https://arxiv.org/abs/2506.10354</link>
      <description>arXiv:2506.10354v1 Announce Type: new 
Abstract: We revisit the problem of mean estimation on $\ell_p$ balls under additive Gaussian noise. When $p$ is strictly less than $2$, it is well understood that rate-optimal estimators must be nonlinear in the observations. In this work, we study the maximum likelihood estimator (MLE), which may be viewed as a nonlinear shrinkage procedure for mean estimation over $\ell_p$ balls. We demonstrate two phenomena for the behavior of the MLE, which depend on the noise level, the radius of the norm constraint, the dimension, and the norm index $p$. First, as a function of the dimension, for $p$ near $1$ or at least $2$, the MLE is minimax rate-optimal for all noise levels and all constraint radii. On the other hand, for $p$ between $1$ and $2$, there is a more striking behavior: for essentially all noise levels and radii for which nonlinear estimates are required, the MLE is minimax rate-suboptimal, despite being nonlinear in the observations. Our results also imply similar conclusions when given $n$ independent and identically distributed Gaussian samples, where we demonstrate that the MLE can be suboptimal by a polynomial factor in the sample size. Our lower bounds are constructive: whenever the MLE is rate-suboptimal, we provide explicit instances on which the MLE provably incurs suboptimal risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10354v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan, Reese Pathak, Annie Ulichney</dc:creator>
    </item>
    <item>
      <title>Estimating Signal-to-Noise Ratios for Multivariate High-dimensional Linear Models</title>
      <link>https://arxiv.org/abs/2506.10370</link>
      <description>arXiv:2506.10370v1 Announce Type: new 
Abstract: Signal-to-noise ratios (SNR) play a crucial role in various statistical models, with important applications in tasks such as estimating heritability in genomics. The method-of-moments estimator is a widely used approach for estimating SNR, primarily explored in single-response settings. In this study, we extend the method-of-moments SNR estimation framework to encompass both fixed effects and random effects linear models with multivariate responses. In particular, we establish and compare the asymptotic distributions of the proposed estimators. Furthermore, we extend our approach to accommodate cases with residual heteroskedasticity and derive asymptotic inference procedures based on standard error estimation. The effectiveness of our methods is demonstrated through extensive numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10370v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohan Hu, Zhentao Li, Xiaodong Li</dc:creator>
    </item>
    <item>
      <title>A note on the properties of the confidence set for the local average treatment effect obtained by inverting the score test</title>
      <link>https://arxiv.org/abs/2506.10449</link>
      <description>arXiv:2506.10449v1 Announce Type: new 
Abstract: We study the properties of the score confidence set for the local average treatment effect in non and semiparametric instrumental variable models. This confidence set is constructed by inverting a score test based on an estimate of the nonparametric influence function for the estimand, and is known to be uniformly valid in models that allow for arbitrarily weak instruments; because of this, the confidence set can have infinite diameter at some laws. We characterize the six possible forms the score confidence set can take: a finite interval, an infinite interval (or a union of them), the whole real line, an empty set, or a single point. Moreover, we show that, at any fixed law, the score confidence set asymptotically coincides, up to a term of order 1/n, with the Wald confidence interval based on the doubly robust estimator which solves the estimating equation associated with the nonparametric influence function. This result implies that, in models where the efficient influence function coincides with the nonparametric influence function, the score confidence set is, in a sense, optimal in terms of its diameter. We also show that under weak instrument asymptotics, where the strength of the instrument is modelled as local to zero, the doubly robust estimator is asymptotically biased and does not follow a normal distribution. A simulation study confirms that, as expected, the doubly robust estimator performs poorly when instruments are weak, whereas the score confidence set retains good finite-sample properties in both strong and weak instrument settings. Finally, we provide an algorithm to compute the score confidence set, which is now available in the DoubleML package for double machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10449v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, Ludovico Lanni, David Masip</dc:creator>
    </item>
    <item>
      <title>Computational Complexity of Statistics: New Insights from Low-Degree Polynomials</title>
      <link>https://arxiv.org/abs/2506.10748</link>
      <description>arXiv:2506.10748v1 Announce Type: new 
Abstract: This is a survey on the use of low-degree polynomials to predict and explain the apparent statistical-computational tradeoffs in a variety of average-case computational problems. In a nutshell, this framework measures the complexity of a statistical task by the minimum degree that a polynomial function must have in order to solve it. The main goals of this survey are to (1) describe the types of problems where the low-degree framework can be applied, encompassing questions of detection (hypothesis testing), recovery (estimation), and more; (2) discuss some philosophical questions surrounding the interpretation of low-degree lower bounds, and notably the extent to which they should be treated as evidence for inherent computational hardness; (3) explore the known connections between low-degree polynomials and other related approaches such as the sum-of-squares hierarchy and statistical query model; and (4) give an overview of the mathematical tools used to prove low-degree lower bounds. A list of open problems is also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10748v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander S. Wein</dc:creator>
    </item>
    <item>
      <title>Distributionally-Constrained Adversaries in Online Learning</title>
      <link>https://arxiv.org/abs/2506.10293</link>
      <description>arXiv:2506.10293v1 Announce Type: cross 
Abstract: There has been much recent interest in understanding the continuum from adversarial to stochastic settings in online learning, with various frameworks including smoothed settings proposed to bridge this gap. We consider the more general and flexible framework of distributionally constrained adversaries in which instances are drawn from distributions chosen by an adversary within some constrained distribution class [RST11]. Compared to smoothed analysis, we consider general distributional classes which allows for a fine-grained understanding of learning settings between fully stochastic and fully adversarial for which a learner can achieve non-trivial regret. We give a characterization for which distribution classes are learnable in this context against both oblivious and adaptive adversaries, providing insights into the types of interplay between the function class and distributional constraints on adversaries that enable learnability. In particular, our results recover and generalize learnability for known smoothed settings. Further, we show that for several natural function classes including linear classifiers, learning can be achieved without any prior knowledge of the distribution class -- in other words, a learner can simultaneously compete against any constrained adversary within learnable distribution classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10293v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo\"ise Blanchard, Samory Kpotufe</dc:creator>
    </item>
    <item>
      <title>Optimal Non-Adaptive Group Testing with One-Sided Error Guarantees</title>
      <link>https://arxiv.org/abs/2506.10374</link>
      <description>arXiv:2506.10374v1 Announce Type: cross 
Abstract: The group testing problem consists of determining a sparse subset of defective items from within a larger set of items via a series of tests, where each test outcome indicates whether at least one defective item is included in the test. We study the approximate recovery setting, where the recovery criterion of the defective set is relaxed to allow a small number of items to be misclassified. In particular, we consider one-sided approximate recovery criteria, where we allow either only false negative or only false positive misclassifications. Under false negatives only (i.e., finding a subset of defectives), we show that there exists an algorithm matching the optimal threshold of two-sided approximate recovery. Under false positives only (i.e., finding a superset of the defectives), we provide a converse bound showing that the better of two existing algorithms is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10374v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel McMorrow, Jonathan Scarlett</dc:creator>
    </item>
    <item>
      <title>Hessian Geometry of Latent Space in Generative Models</title>
      <link>https://arxiv.org/abs/2506.10632</link>
      <description>arXiv:2506.10632v1 Announce Type: cross 
Abstract: This paper presents a novel method for analyzing the latent space geometry of generative models, including statistical physics models and diffusion models, by reconstructing the Fisher information metric. The method approximates the posterior distribution of latent variables given generated samples and uses this to learn the log-partition function, which defines the Fisher metric for exponential families. Theoretical convergence guarantees are provided, and the method is validated on the Ising and TASEP models, outperforming existing baselines in reconstructing thermodynamic quantities. Applied to diffusion models, the method reveals a fractal structure of phase transitions in the latent space, characterized by abrupt changes in the Fisher metric. We demonstrate that while geodesic interpolations are approximately linear within individual phases, this linearity breaks down at phase boundaries, where the diffusion model exhibits a divergent Lipschitz constant with respect to the latent space. These findings provide new insights into the complex structure of diffusion model latent spaces and their connection to phenomena like phase transitions. Our source code is available at https://github.com/alobashev/hessian-geometry-of-diffusion-models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10632v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CV</category>
      <category>math.DG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Lobashev, Dmitry Guskov, Maria Larchenko, Mikhail Tamm</dc:creator>
    </item>
    <item>
      <title>Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods</title>
      <link>https://arxiv.org/abs/2506.10959</link>
      <description>arXiv:2506.10959v1 Announce Type: cross 
Abstract: While in-context learning (ICL) has achieved remarkable success in natural language and vision domains, its theoretical understanding--particularly in the context of structured geometric data--remains unexplored. In this work, we initiate a theoretical study of ICL for regression of H\"older functions on manifolds. By establishing a novel connection between the attention mechanism and classical kernel methods, we derive generalization error bounds in terms of the prompt length and the number of training tasks. When a sufficient number of training tasks are observed, transformers give rise to the minimax regression rate of H\"older functions on manifolds, which scales exponentially with the intrinsic dimension of the manifold, rather than the ambient space dimension. Our result also characterizes how the generalization error scales with the number of training tasks, shedding light on the complexity of transformers as in-context algorithm learners. Our findings provide foundational insights into the role of geometry in ICL and novels tools to study ICL of nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10959v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaiming Shen, Alexander Hsu, Rongjie Lai, Wenjing Liao</dc:creator>
    </item>
    <item>
      <title>Random effects estimation in a fractional diffusion model based on continuous observations</title>
      <link>https://arxiv.org/abs/2409.04331</link>
      <description>arXiv:2409.04331v2 Announce Type: replace 
Abstract: The purpose of the present work is to construct estimators for the random effects in a fractional diffusion model using a hybrid estimation method where we combine parametric and nonparametric thechniques. We precisely consider $n$ stochastic processes $\left\{X_t^j,\ 0\leq t\leq T\right\}$, $j=1,\ldots, n$ continuously observed over the time interval $[0,T]$, where the dynamics of each process are described by fractional stochastic differential equations with drifts depending on random effects. We first construct a parametric estimator for the random effects using the techniques of maximum likelihood estimation and we study its asymptotic properties when the time horizon $T$ is sufficiently large. Then by taking into account the obtained estimator for the random effects, we build a nonparametric estimator for their common unknown density function using Bernstein polynomials approximation. Some asymptotic properties of the density estimator, such as its asymptotic bias, variance and mean integrated squared error, are studied for an infinite time horizon $T$ and a fixed sample size $n$. The asymptotic normality and the uniform convergence of the estimator are investigated for an infinite time horizon $T$, a high frequency and as the order of Bernstein polynomials is sufficiently large. Some numerical simulations are also presented to illustrate the performance of the Bernstein polynomials based estimator compared to standard Kernel estimator for the random effects density function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04331v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nesrine Chebli, Hamdi Fathallah, Yousri Slaoui</dc:creator>
    </item>
    <item>
      <title>Spectrally Robust Covariance Shrinkage for Hotelling's $T^2$ in High Dimensions</title>
      <link>https://arxiv.org/abs/2502.02006</link>
      <description>arXiv:2502.02006v3 Announce Type: replace 
Abstract: We investigate covariance shrinkage for Hotelling's $T^2$ in the regime where the data dimension $p$ and the sample size $n$ grow in a fixed ratio -- without assuming that the population covariance matrix is spiked or well-conditioned. When $p/n\to\phi \in (0,1)$, we propose a practical finite-sample shrinker that, for any maximum-entropy signal prior and any fixed significance level, (a) asymptotically maximizes power under Gaussian data, and (b) asymptotically saturates the Hanson--Wright lower bound on power in the more general sub-Gaussian case. Our approach is to formulate and solve a variational problem characterizing the optimal limiting shrinker, and to show that our finite-sample method consistently approximates this limit by extending recent local random matrix laws. Empirical studies on simulated and real-world data, including the Crawdad UMich/RSS data set, demonstrate up to a $50\%$ gain in power over leading linear and nonlinear competitors at a significance level of $10^{-4}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02006v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin D. Robinson, Van Latimer</dc:creator>
    </item>
    <item>
      <title>Locally minimax optimal and dimension-agnostic discrete argmin inference</title>
      <link>https://arxiv.org/abs/2503.21639</link>
      <description>arXiv:2503.21639v3 Announce Type: replace 
Abstract: This paper tackles a fundamental inference problem: given $n$ observations from a $d$ dimensional vector with unknown mean $\boldsymbol{\mu}$, we must form a confidence set for the index (or indices) corresponding to the smallest component of $\boldsymbol{\mu}$. By duality, we reduce this to testing, for each $r$ in $1,\ldots,d$, whether $\mu_r$ is the smallest. Based on the sample splitting and self-normalization approach of Kim and Ramdas (2024), we propose "dimension-agnostic" tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in $\boldsymbol{\mu}$. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the local minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Furthermore, we develop robust variants that continue to achieve the same minimax rate under heavy-tailed distributions with only finite second moments. Empirical results on simulated and real data illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21639v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilmun Kim, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>The Laplace asymptotic expansion in high dimensions</title>
      <link>https://arxiv.org/abs/2406.12706</link>
      <description>arXiv:2406.12706v3 Announce Type: replace-cross 
Abstract: We prove that the classical Laplace asymptotic expansion (AE) of $\int_{\mathbb R^d} g(x)e^{-nu(x)}dx$, $n\gg1$ extends to the high-dimensional regime in which $d$ may grow large with $n$. More specifically, we use new techniques suitable to high-$d$ to derive an AE which formally coincides with the classical one because the terms are the same, but which now has a new small parameter. Namely under classical assumptions on $z$ and $g$ and additional bounds on the growth of $\|\nabla^kz\|$ and $\|\nabla^kg\|$ with $d$, we show the new small parameter is $d^2/n$, in the sense that $|\text{Rem}_L|\leq C_L(d^2/n)^L$ for each $L=1,2,3,\dots$, where $\text{Rem}_L$ is the $L$th order remainder. As an example, we show that the derivative bounds are satisfied with high probability for a random function $z$ arising in a standard statistical model. We also show that if the derivative bounds are relaxed, then we still obtain a valid AE in powers of a "larger" small parameter.
  To prove these results, we derive a very general nonasymptotic bound on $\text{Rem}_L$ which is explicit in its dependence on $g,z,d,n$. The bound holds with nearly no apriori restrictions on the magnitude of the derivative norms. We show the bound is tight for each $L$ by proving a matching lower bound for a quartic $z$ and $g\equiv1$. When $d,z,g$ are fixed and $n\to\infty$, our bound shows that $\text{Rem}_L=O(n^{-L})$. Thus our work subsumes the classical theory of the Laplace expansion, and significantly extends it into the high-$d$ regime. This broadened applicability of the expansion is extremely useful for the many modern applications requiring the computation of high-$d$ Laplace integrals. In settings where the expansion is already in use, our precise and explicit error bound is valuable both for numerical estimates and theoretical analysis, especially near the boundary of applicability of the expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12706v3</guid>
      <category>math.CA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anya Katsevich</dc:creator>
    </item>
    <item>
      <title>Neural Networks Generalize on Low Complexity Data</title>
      <link>https://arxiv.org/abs/2409.12446</link>
      <description>arXiv:2409.12446v3 Announce Type: replace-cross 
Abstract: We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d.~data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number. For primality testing, our theorem shows the following and more. Suppose that we draw an i.i.d.~sample of $n$ numbers uniformly at random from $1$ to $N$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then, the interpolating MDL network accurately answers, with probability $1- O((\ln N)/n)$, whether a newly drawn number between $1$ and $N$ is a prime or not. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so. Extensions to noisy data are also discussed, suggesting that MDL neural network interpolators can demonstrate tempered overfitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12446v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>On the Geometry of Receiver Operating Characteristic and Precision-Recall Curves</title>
      <link>https://arxiv.org/abs/2504.02169</link>
      <description>arXiv:2504.02169v2 Announce Type: replace-cross 
Abstract: We study the geometry of Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves in binary classification problems. The key finding is that many of the most commonly used binary classification metrics are merely functions of the composition function $G := F_p \circ F_n^{-1}$, where $F_p(\cdot)$ and $F_n(\cdot)$ are the class-conditional cumulative distribution functions of the classifier scores in the positive and negative classes, respectively. This geometric perspective facilitates the selection of operating points, understanding the effect of decision thresholds, and comparison between classifiers. It also helps explain how the shapes and geometry of ROC/PR curves reflect classifier behavior, providing objective tools for building classifiers optimized for specific applications with context-specific constraints. We further explore the conditions for classifier dominance, present analytical and numerical examples demonstrating the effects of class separability and variance on ROC and PR geometries, and derive a link between the positive-to-negative class leakage function $G(\cdot)$ and the Kullback--Leibler divergence. The framework highlights practical considerations, such as model calibration, cost-sensitive optimization, and operating point selection under real-world capacity constraints, enabling more informed approaches to classifier deployment and decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02169v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Sameni</dc:creator>
    </item>
  </channel>
</rss>
