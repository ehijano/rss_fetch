<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 04:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>High hopes for "Deep Medicine"? AI, economics, and the future of care</title>
      <link>https://arxiv.org/abs/2507.21054</link>
      <description>arXiv:2507.21054v1 Announce Type: new 
Abstract: In the much-celebrated book Deep Medicine, Eric Topol argues that the development of artificial intelligence for health care will lead to a dramatic shift in the culture and practice of medicine. In the next several decades, he suggests, AI will become sophisticated enough that many of the everyday tasks of physicians could be delegated to it. Topol is perhaps the most articulate advocate of the benefits of AI in medicine, but he is hardly alone in spruiking its potential to allow physicians to dedicate more of their time and attention to providing empathetic care for their patients in the future. Unfortunately, several factors suggest a radically different picture for the future of health care. Far from facilitating a return to a time of closer doctor-patient relationships, the use of medical AI seems likely to further erode therapeutic relationships and threaten professional and patient satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21054v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/hast.1079</arxiv:DOI>
      <arxiv:journal_reference>2020. Hastings Center Report 50(1): 14-17</arxiv:journal_reference>
      <dc:creator>Robert Sparrow, Joshua Hatherley</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models</title>
      <link>https://arxiv.org/abs/2507.21055</link>
      <description>arXiv:2507.21055v1 Announce Type: new 
Abstract: In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21055v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leyi Ouyang</dc:creator>
    </item>
    <item>
      <title>Examining the sentiment and emotional differences in product and service reviews: The moderating role of culture</title>
      <link>https://arxiv.org/abs/2507.21057</link>
      <description>arXiv:2507.21057v1 Announce Type: new 
Abstract: This study explores how emotions and sentiments differ in customer reviews of products and services on e-commerce platforms. Unlike earlier research that treats all reviews uniformly, this study distinguishes between reviews of products, typically fulfilling basic, functional needs, and services, which often cater to experiential and emotional desires. The findings reveal clear differences in emotional expression and sentiment between the two. Product reviews frequently focus on practicality, such as functionality, reliability, and value for money, and are generally more neutral or pragmatic in tone. In contrast, service reviews involve stronger emotional engagement, as services often entail personal interactions and subjective experiences. Customers express a broader spectrum of emotions, such as joy, frustration, or disappointment when reviewing services, as identified using advanced machine learning techniques. Cultural background further influences these patterns. Consumers from collectivist cultures, as defined by Hofstede cultural dimensions, often use more moderated and socially considerate language, reflecting an emphasis on group harmony. Conversely, consumers from individualist cultures tend to offer more direct, emotionally intense feedback. Notably, gender appears to have minimal impact on sentiment variation, reinforcing the idea that the nature of the offering (product vs. service) and cultural context are the dominant factors. Theoretically, the study extends Maslow hierarchy of needs and Hofstede cultural framework to the domain of online reviews, proposing a model that explains how these dimensions shape consumer expression. Practically, the insights offer valuable guidance for businesses looking to optimize their marketing and customer engagement strategies by aligning messaging and service design with customer expectations across product types and cultural backgrounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21057v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinh Truong (RMIT University)</dc:creator>
    </item>
    <item>
      <title>Dependency on Meta AI Chatbot in Messenger Among STEM and Non-STEM Students in Higher Education</title>
      <link>https://arxiv.org/abs/2507.21059</link>
      <description>arXiv:2507.21059v1 Announce Type: new 
Abstract: To understand the potential dependency of tertiary students regarding Meta AI in the academic context. This descriptive cross-sectional study surveyed 872 tertiary students from public and private institutions in Luzon, Philippines. Demographic information and perceptions on Meta AI dependency based on existing literature were collected. Descriptive statistics were used to summarize the data and differences between STEM and non-STEM students were analyzed using the Mann-Whitney U test. The results indicate a nuanced perspective on Meta AI chatbot use among students. While there is general disagreement with heavy reliance on the chatbot for academic tasks, psychological support, and social factors, there is moderate agreement on its technological benefits and academic utility. Students value the Meta AI convenience, availability, and problem-solving assistance, but prefer traditional resources and human interaction for academic and social support. Concerns about dependency risks and impacts on critical thinking are acknowledged, particularly among STEM students, who rely more on chatbots for academic purposes. This suggests that while Meta AI is a valuable resource, its role is complementary rather than transformative in educational contexts, with institutional encouragement and individual preferences influencing usage patterns. Students generally hesitate to rely heavily on meta-AI chatbots. This reflects a preference for traditional resources and independent problem-solving. While students acknowledge AI chatbots academic benefits and technological convenience, concerns about overreliance and its impact on critical thinking persist, particularly among STEM students, who appear more inclined to integrate these tools into their studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21059v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.25147/ijcsr.2017.001.1.242</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Computing Sciences Research 9 (2025) 3674-3690</arxiv:journal_reference>
      <dc:creator>Hilene E. Hernandez, Rhiziel P. Manalese, Roque Francis B. Dianelo, Jaymark A. Yambao, Almer B. Gamboa, Lloyd D. Feliciano, Mike Haizon M. David, Freneil R. Pampo, John Paul P. Miranda</dc:creator>
    </item>
    <item>
      <title>Automated but Atrophied? Student Over-Reliance vs Expert Augmentation of AI in Learning and Cybersecurity</title>
      <link>https://arxiv.org/abs/2507.21062</link>
      <description>arXiv:2507.21062v1 Announce Type: new 
Abstract: University students and working professionals are increasingly encountering generative artificial intelligence (AI) in education and practice, yet their approaches and outcomes differ markedly. This paper proposes an academic study contrasting novice over-reliance on AI with expert augmentation of AI, grounded in two real-world narratives. In one, a university student attempted to outsource learning entirely to AI, eschewing course engagement. In the other, seasoned cybersecurity professionals in the Tradewinds 2025 red/blue team exercise collaboratively employed AI tools to enhance (not replace) their domain expertise. This proposal outlines a comparative research design to investigate how students' perception of AI as a learning replacement versus professionals' use of AI as an expert tool impacts outcomes. Drawing on current literature in educational technology and workplace AI, we examine implications for curriculum design, AI literacy, and assessment reform in higher education. We hypothesize that blind reliance on AI can erode fundamental skills and academic integrity, whereas guided use of AI by knowledgeable users can amplify productivity without sacrificing quality. The paper details methodologies for classroom and workplace data collection, including student and professional surveys, interviews, and performance analyses. Anticipated findings aim to inform responsible AI integration in curricula, balancing innovation with the necessity of domain knowledge. We conclude with recommendations for pedagogical strategies, institutional policies to foster AI literacy, and a call for longitudinal studies tracking how AI usage during university affects professional competencies over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21062v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koffka Khan</dc:creator>
    </item>
    <item>
      <title>Cybroc: Cyborgizing Broccoli for Longevity</title>
      <link>https://arxiv.org/abs/2507.21064</link>
      <description>arXiv:2507.21064v1 Announce Type: new 
Abstract: Cybroc is a series of kinetic art installations exploring the recent proliferating populist longevity activism through the satirical cyborgization of broccoli. The artwork augments the symbol of health food-broccoli-with prosthetic limbs to perform so-called longevity-enhancing exercises such as cold plunges, treadmill running, brachiation (arm-swinging), sled pushing, etc.-all simulations of primal human survival tasks reframed as modern fitness routines. Despite its mechanical augmentations, the broccoli's inevitable decay and rotting after exhibiting high-intensity performances prompts reflection on the limits of biological enhancement and the ethics of human enhancement beyond natural capabilities, particularly transhumanist ideals. By juxtaposing a symbolic healthy vegetable with cutting-edge concepts of human enhancement, Cybroc challenges viewers to consider the intersection of nature, technology, and the human quest for extended lifespan in our transhuman era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21064v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ke Huang, Yue Zhou, Xi He, Weibo Chen, Botao Amber Hu</dc:creator>
    </item>
    <item>
      <title>Digital Sovereigns Big Tech and Nation-State Influence</title>
      <link>https://arxiv.org/abs/2507.21066</link>
      <description>arXiv:2507.21066v1 Announce Type: new 
Abstract: Technology companies have gained unprecedented power and influence in recent years, resembling quasi-nation-states globally. Corporations with trillion-dollar market capitalizations are no longer just providers of digital services; they now wield immense economic power, influence global infrastructure, and significantly impact political and social dynamics. This thesis examines how these corporations have transcended traditional business models, adopting characteristics typically associated with sovereign states. They now enforce regulations, shape public discourse, and influence legal frameworks in various countries. This shift presents unique challenges, including the undermining of democratic governance, the exacerbation of economic inequalities, and the enabling of unregulated data exploitation and privacy violations. The study will examine critical instances of tech companies acting as quasi-governmental bodies and assess the risks associated with unchecked corporate influence in global governance. Ultimately, the thesis aims to propose policy frameworks and regulatory interventions to curb the overreach of tech giants, restoring the balance between democratic institutions and corporate power and ensuring that the digital future aligns with the public good rather than creating Frankenstein-like monsters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21066v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Michael Bollerman</dc:creator>
    </item>
    <item>
      <title>Making a Case for Research Collaboration Between Artificial Intelligence and Operations Research Experts</title>
      <link>https://arxiv.org/abs/2507.21076</link>
      <description>arXiv:2507.21076v1 Announce Type: new 
Abstract: In 2021, INFORMS, ACM SIGAI, and the Computing Community Consortium (CCC) hosted three workshops to explore synergies between Artificial Intelligence (AI) and Operations Research (OR) to improve decision-making. The workshops aimed to create a unified research vision for AI/OR collaboration, focusing on overcoming cultural differences and maximizing societal impact. The first two workshops addressed technological innovations, applications, and trustworthy AI development, while the final workshop highlighted specific areas for AI/OR integration. Participants discussed "Challenge Problems" and strategies for combining AI and OR techniques. This report outlines five key recommendations to enhance AI/OR collaboration: 1) Funding Opportunities, 2) Joint Education, 3) Long-term Research Programs, 4) Aligning Conferences/Journals, and 5) Benchmark Creation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21076v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radhika Kulkarni, Gianluca Brero, Yu Ding, Swati Gupta, Sven Koenig, Ramayya Krishnan, Thiago Serra, Phebe Vayanos, Segev Wasserkrug, Holly Wiberg</dc:creator>
    </item>
    <item>
      <title>Safety Features for a Centralised AGI Project</title>
      <link>https://arxiv.org/abs/2507.21082</link>
      <description>arXiv:2507.21082v1 Announce Type: new 
Abstract: Recent AI progress has outpaced expectations, with some experts now predicting AI that matches or exceeds human capabilities in all cognitive areas (AGI) could emerge this decade, potentially posing grave national and global security threats. AI development is currently occurring primarily in the private sector with minimal oversight. This report analyzes a scenario where the US government centralizes AGI development under its direct control, and identifies four high-level priorities and seven safety features to reduce risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21082v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Hastings-Woodhouse</dc:creator>
    </item>
    <item>
      <title>The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment</title>
      <link>https://arxiv.org/abs/2507.21091</link>
      <description>arXiv:2507.21091v1 Announce Type: new 
Abstract: Conversational agents (CAs) based on generative artificial intelligence frequently face challenges ensuring ethical interactions that align with human values. Current value alignment efforts largely rely on top-down approaches, such as technical guidelines or legal value principles. However, these methods tend to be disconnected from the specific contexts in which CAs operate, potentially leading to misalignment with users interests. To address this challenge, we propose a novel, bottom-up approach to value alignment, utilizing the value ontology of the ISO Value-Based Engineering standard for ethical IT design. We analyse 593 ethically sensitive system outputs identified from 16,908 conversational logs of a major European employment service CA to identify core values and instances of value misalignment within real-world interactions. The results revealed nine core values and 32 different value misalignments that negatively impacted users. Our findings provide actionable insights for CA providers seeking to address ethical challenges and achieve more context-sensitive value alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21091v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lenart Motnikar, Katharina Baum, Alexander Kagan, Sarah Spiekermann-Hoff</dc:creator>
    </item>
    <item>
      <title>Barriers to Digital Mental Health Services among College Students</title>
      <link>https://arxiv.org/abs/2507.21093</link>
      <description>arXiv:2507.21093v1 Announce Type: new 
Abstract: This qualitative study explores barriers to utilization of digital mental health Intervention (DMHI) among college students. Data are from a large randomized clinical trial of an intervention, eBridge, that used motivational interviewing for online counseling to connect students with mental health issues to professional services. We applied thematic analysis to analyze the feedback from the student participants regarding their experience of using the DMHI platform. We identified nine key barriers to DMHI adoption and the use of in-person mental health services: emotional distress, time constraints, privacy concerns, resource accessibility, financial challenges, medication stigma, dissatisfaction with communication, content clarity, and treatment-related concerns. Our findings emphasize the need for personalized, culturally sensitive interventions and improved strategies to enhance the access and engagement in mental health support for young adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21093v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ha Na Cho, Kyuha Jung, Daniel Eisenberg, Cheryl A. King, Kai Zheng</dc:creator>
    </item>
    <item>
      <title>A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling</title>
      <link>https://arxiv.org/abs/2507.21100</link>
      <description>arXiv:2507.21100v1 Announce Type: new 
Abstract: This paper introduces TACTIC-GRAPHS, a system that combines spectral graph theory and multimodal graph neural reasoning for semantic understanding and threat detection in tactical video under high noise and weak structure. The framework incorporates spectral embedding, temporal causal edge modeling, and discriminative path inference across heterogeneous modalities. A semantic-aware keyframe extraction method fuses visual, acoustic, and action cues to construct temporal graphs. Using graph attention and Laplacian spectral mapping, the model performs cross-modal weighting and causal signal analysis. Experiments on TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal alignment and over 85 percent recognition of complete threat chains, with node latency within plus-minus 150 milliseconds. The approach enhances structural interpretability and supports applications in surveillance, defense, and intelligent security systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21100v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Meng</dc:creator>
    </item>
    <item>
      <title>Assessing the Ecological Impact of AI</title>
      <link>https://arxiv.org/abs/2507.21102</link>
      <description>arXiv:2507.21102v1 Announce Type: new 
Abstract: Philosophers of technology have recently started paying more attention to the environmental impacts of AI, in particular of large language models (LLMs) and generative AI (genAI) applications. Meanwhile, few developers of AI give concrete estimates of the ecological impact of their models and products, and even when they do so, their analysis is often limited to green house gas emissions of certain stages of AI development or use. The current proposal encourages practically viable analyses of the sustainability aspects of genAI informed by philosophical ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21102v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sylvia Wenmackers</dc:creator>
    </item>
    <item>
      <title>Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach</title>
      <link>https://arxiv.org/abs/2507.21118</link>
      <description>arXiv:2507.21118v1 Announce Type: new 
Abstract: MOOCs offer free and open access to a wide audience, but completion rates remain low, often due to a lack of personalized content. To address this issue, it is essential to predict learner performance in order to provide tailored feedback. Behavioral traces-such as clicks and events-can be analyzed as time series to anticipate learners' outcomes. This work compares multivariate time series classification methods to identify at-risk learners at different stages of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted on the Open University Learning Analytics Dataset (OULAD), focuses on three courses: two in STEM and one in SHS. Preliminary results show that the evaluated approaches are promising for predicting learner failure in MOOCs. The analysis also suggests that prediction accuracy is influenced by the amount of recorded interactions, highlighting the importance of rich and diverse behavioral data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21118v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anass El Ayady (Crem, IRIMAS), Maxime Devanne (IRIMAS), Germain Forestier (IRIMAS), Nour El Mawas (Crem)</dc:creator>
    </item>
    <item>
      <title>Trustworthy AI: UK Air Traffic Control Revisited</title>
      <link>https://arxiv.org/abs/2507.21169</link>
      <description>arXiv:2507.21169v1 Announce Type: new 
Abstract: Exploring the socio-technical challenges confronting the adoption of AI in organisational settings is something that has so far been largely absent from the related literature. In particular, research into requirements for trustworthy AI typically overlooks how people deal with the problems of trust in the tools that they use as part of their everyday work practices. This article presents some findings from an ongoing ethnographic study of how current tools are used in air traffic control work and what it reveals about requirements for trustworthy AI in air traffic control and other safety-critical application domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21169v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Procter, Mark Rouncefield</dc:creator>
    </item>
    <item>
      <title>A ChatGPT-based approach for questions generation in higher education</title>
      <link>https://arxiv.org/abs/2507.21174</link>
      <description>arXiv:2507.21174v1 Announce Type: new 
Abstract: Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a "Blind test" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21174v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643479</arxiv:DOI>
      <arxiv:journal_reference>Vu, Sinh Trong, et al. "A ChatGPT-based approach for questions generation in higher education." Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A Systems for Multimedia. 2024</arxiv:journal_reference>
      <dc:creator>Sinh Trong Vu, Huong Thu Truong, Oanh Tien Do, Tu Anh Le, Tai Tan Mai</dc:creator>
    </item>
    <item>
      <title>The Human Capital Ontology (Extended Abstract)</title>
      <link>https://arxiv.org/abs/2507.21175</link>
      <description>arXiv:2507.21175v1 Announce Type: new 
Abstract: The Human Capital Ontology (HCO) is an ontology that represents data standards maintained and employed by the Office of Personnel Management (OPM) to represent Human Capital Operations and to classify job positions. The HCO is an extension of the Common Core Ontologies and the upper level Basic Formal Ontology (BFO). HCO provides representation of OPM Nature of Action (NOA) codes that are used to describe human resource personnel actions. HCO also represents Occupational Groups and Job Families, the Occupational Series into which these subdivide, as well as their corresponding codes, used by OPM to classify and grade both white and blue collar jobs in the Federal Government. HCO also encodes crosswalks between OPM Occupational Series and corresponding Standard Occupational Classification Codes maintained by the U.S. Bureau of Labor Statistics. In addition to documenting and justifying the approach of HCO to modeling the above, we report on recent and planned applications of HCO across the US Government. We also report on parallel efforts of ours to enhance the state of the art in structured data informed Human Capital measurements. Keywords: Office of Personnel Management, ontology, occupational series, nature of action, personnel action, human capital, position classification standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21175v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shane Babcock, Maxwell Farrington, John Gugliotti</dc:creator>
    </item>
    <item>
      <title>When Proximity Falls Short: Inequalities in Commuting and Accessibility by Public Transport in Santiago, Chile</title>
      <link>https://arxiv.org/abs/2507.21743</link>
      <description>arXiv:2507.21743v1 Announce Type: new 
Abstract: Traditional measures of urban accessibility often rely on static models or survey data. However, location information from mobile networks now enables large-scale, dynamic analyses of how people navigate cities. This study uses eXtended Detail Records (XDRs) derived from mobile phone activity to analyze commuting patterns and accessibility inequalities in Santiago, Chile. First, we identify residential and work locations and model commuting routes using the R5 multimodal routing engine, which combines public transport and walking. To explore spatial patterns, we apply a bivariate spatial clustering analysis (LISA) alongside regression techniques to identify distinct commuting behaviors and their alignment with vulnerable population groups. Our findings reveal that average commuting times remain consistent across socioeconomic groups. However, despite residing in areas with greater opportunity density, higher-income populations do not consistently experience shorter commuting times. This highlights a disconnect between spatial proximity to opportunities and actual travel experience. Our analysis reveals significant disparities between sociodemographic groups, particularly regarding the distribution of indigenous populations and gender. Overall, the findings of our study suggest that commuting and accessibility inequalities in Santiago are closely linked to broader social and demographic structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21743v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cesar Marin-Flores, Leo Ferres, Henrikki Tenkanen</dc:creator>
    </item>
    <item>
      <title>Against racing to AGI: Cooperation, deterrence, and catastrophic risks</title>
      <link>https://arxiv.org/abs/2507.21839</link>
      <description>arXiv:2507.21839v1 Announce Type: new 
Abstract: AGI Racing is the view that it is in the self-interest of major actors in AI development, especially powerful nations, to accelerate their frontier AI development to build highly capable AI, especially artificial general intelligence (AGI), before competitors have a chance. We argue against AGI Racing. First, the downsides of racing to AGI are much higher than portrayed by this view. Racing to AGI would substantially increase catastrophic risks from AI, including nuclear instability, and undermine the prospects of technical AI safety research to be effective. Second, the expected benefits of racing may be lower than proponents of AGI Racing hold. In particular, it is questionable whether winning the race enables complete domination over losers. Third, international cooperation and coordination, and perhaps carefully crafted deterrence measures, constitute viable alternatives to racing to AGI which have much smaller risks and promise to deliver most of the benefits that racing to AGI is supposed to provide. Hence, racing to AGI is not in anyone's self-interest as other actions, particularly incentivizing and seeking international cooperation around AI issues, are preferable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21839v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonard Dung, Max Hellrigel-Holderbaum</dc:creator>
    </item>
    <item>
      <title>Prompt template for a fictitious LLM agent in a content-flagging experiment</title>
      <link>https://arxiv.org/abs/2507.21842</link>
      <description>arXiv:2507.21842v1 Announce Type: new 
Abstract: Digital regulations such as the European Union's Digital Services Act (DSA) represent major efforts to shape human-centered and human rights-based frameworks for society. Yet, as these laws are translated into practice, challenges emerge at the intersection of technology, law, and design. This paper presents a qualitative case study examining how designers act as mediators between abstract legal requirements and real-world digital experiences for users, focusing on the design of content reporting mechanisms under Article 16 of the DSA.
  Through an expert workshop with professional designers from diverse fields (N=9), we explore how legal obligations are interpreted by designers and reflected in discussions and design solutions. Our findings resonate with previous research on the design of reporting mechanisms and dark patterns, highlighting how UX design choices can mislead or hinder users' decision-making and therefore also highlighting the crucial role of design decisions.
  We show how participatory design methods can bridge disciplinary divides, making legal obligations accessible in compliance fostering design solutions.
  By using legal design as a lens, we argue that the co-creation of digital regulations and user experience is a core site for digital humanism; where designers, engineers, and legal scholars must collaborate to ensure that systems uphold legal standards to address the challenge the regulation poses to these disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21842v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marie-Therese Sekwenz, Daria Simons, Alina Wundsam</dc:creator>
    </item>
    <item>
      <title>Security practices in AI development</title>
      <link>https://arxiv.org/abs/2507.21061</link>
      <description>arXiv:2507.21061v1 Announce Type: cross 
Abstract: What makes safety claims about general purpose AI systems such as large language models trustworthy? We show that rather than the capabilities of security tools such as alignment and red teaming procedures, it is security practices based on these tools that contributed to reconfiguring the image of AI safety and made the claims acceptable. After showing what causes the gap between the capabilities of security tools and the desired safety guarantees, we critically investigate how AI security practices attempt to fill the gap and identify several shortcomings in diversity and participation. We found that these security practices are part of securitization processes aiming to support (commercial) development of general purpose AI systems whose trustworthiness can only be imperfectly tested instead of guaranteed. We conclude by offering several improvements to the current AI security practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21061v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00146-025-02247-4</arxiv:DOI>
      <dc:creator>Petr Spelda, Vit Stritecky</dc:creator>
    </item>
    <item>
      <title>Make Silence Speak for Itself: a multi-modal learning analytic approach with neurophysiological data</title>
      <link>https://arxiv.org/abs/2507.21063</link>
      <description>arXiv:2507.21063v1 Announce Type: cross 
Abstract: Background: Silence is a common phenomenon in classrooms, yet its implicit nature limits a clear understanding of students' underlying learning statuses. Aim: This study proposed a nuanced framework to classify classroom silence based on class events and student status, and examined neurophysiological markers to reveal similarities and differences in silent states across achievement groups. Sample: The study involved 54 middle school students during 34 math lessons, with simultaneous recordings of electroencephalogram (EEG), electrodermal activity (EDA), and heart rate signals, alongside video coding of classroom behaviors. Results: We found that high-achieving students showed no significant difference in mean EDA features between strategic silence (i.e., students choose silence deliberately) and active speaking during open questioning but exhibited higher EEG high-frequency relative power spectral density (RPSD) during strategic silence. In structural silence (i.e., students maintain silence following an external command) during directed questioning, they demonstrated significantly higher heart rates while listening to lectures compared to group activities, indicating heightened engagement. Both high- and medium-achieving students displayed elevated heart rates and EDA tonic components in structural silence during questioning compared to teaching. Furthermore, high-achieving students exhibited lower high-frequency RPSD during structural silence than strategic silence, a pattern not observed in other groups, highlighting group heterogeneity. Conclusions: The findings contribute to validating the complexity of silence, challenge its traditional association with passivity, and offer a novel classification framework along with preliminary empirical evidence to deepen the understanding of silent learning behaviors in classroom contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21063v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxuan Gao, Jingjing Chen, Yun Long, Xiaomeng Xu, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2507.21067</link>
      <description>arXiv:2507.21067v1 Announce Type: cross 
Abstract: Current AI systems rely on opaque reasoning processes that hinder human oversight and collaborative potential. Conventional explainable AI approaches offer post-hoc justifications and often fail to establish genuine symbiotic collaboration. In this paper, the Symbiotic Epistemology is presented as a philosophical foundation for human-AI cognitive partnerships. Unlike frameworks that treat AI as a mere tool or replacement, symbiotic epistemology positions AI as a reasoning partner, fostering calibrated trust by aligning human confidence with AI reliability through explicit reasoning patterns and confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as a formal protocol for transparent human-AI collaboration. The framework is empirically validated through actual human-AI dialogues demonstrating AI's adaptation to structured reasoning protocols and successful metacognitive intervention. The protocol defines two complementary mechanisms: TRACE for high-level reasoning patterns and TRACE_FE for detailed factor explanations. It also integrates confidence quantification, declarative control over AI behavior, and context inheritance for multi-agent coordination. By structuring communication and embedding confidence-calibrated transparency, SynLang, together with symbiotic epistemology, enables AI systems that enhance human intelligence, preserve human agency, and uphold ethical accountability in collaborative decision-making. Through dual-level transparency, beginning with high-level reasoning patterns and progressing to granular explanations, the protocol facilitates rapid comprehension and supports thorough verification of AI decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21067v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jan Kapusta</dc:creator>
    </item>
    <item>
      <title>Can LLMs Reason About Trust?: A Pilot Study</title>
      <link>https://arxiv.org/abs/2507.21075</link>
      <description>arXiv:2507.21075v1 Announce Type: cross 
Abstract: In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21075v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anushka Debnath, Stephen Cranefield, Emiliano Lorini, Bastin Tony Roy Savarimuthu</dc:creator>
    </item>
    <item>
      <title>Data-Driven and Participatory Approaches toward Neuro-Inclusive AI</title>
      <link>https://arxiv.org/abs/2507.21077</link>
      <description>arXiv:2507.21077v1 Announce Type: cross 
Abstract: Biased data representation in AI marginalizes up to 75 million autistic people worldwide through medical applications viewing autism as a deficit of neurotypical social skills rather than an aspect of human diversity, and this perspective is grounded in research questioning the humanity of autistic people. Turing defined artificial intelligence as the ability to mimic human communication, and as AI development increasingly focuses on human-like agents, this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as datasets and systems that move away from mimicking humanness as a benchmark for machine intelligence. Then, we explore the origins, prevalence, and impact of anti-autistic biases in current research. Our work finds that 90% of human-like AI agents exclude autistic perspectives, and AI creators continue to believe ethical considerations are beyond the scope of their work. To improve the autistic representation in data, we conduct empirical experiments with annotators and LLMs, finding that binary labeling schemes sufficiently capture the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can be used to evaluate or fine-tune models, and was developed to serve as a foundation for more neuro-inclusive future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21077v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naba Rizvi</dc:creator>
    </item>
    <item>
      <title>Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention</title>
      <link>https://arxiv.org/abs/2507.21079</link>
      <description>arXiv:2507.21079v1 Announce Type: cross 
Abstract: This study assessed metaverse-based support groups designed to reduce social isolation and suicide risk among LGBTQ+ youths. Using the Cluster platform, enhanced anonymity, avatar-based self-expression, and accessibility were provided. Key findings showed that 79.2% chose avatars matching their gender identity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean: 1.79/5). Social confidence significantly improved in virtual spaces compared to real-world interactions (p&lt;0.001), particularly among participants with initially low confidence, averaging an increase of 2.08 points. About half of the first-time participants were 16 or younger, highlighting potential for early intervention. The metaverse scored higher than real-world environments for safety/privacy (3.94/5), self-expression (4.02/5), and accessibility (4.21/5). Additionally, 73.6% reported feeling more accepted virtually. However, some highly confident individuals offline experienced mild adaptation challenges, averaging a confidence decrease of 0.58 points, indicating virtual support complements rather than replaces in-person services. These findings suggest metaverse-based support effectively lowers psychological barriers and provides affirming spaces, potentially reducing severe outcomes such as suicidal ideation. Future studies should focus on integrating virtual support with existing community and clinical frameworks to enhance long-term impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21079v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.57019/jmv.1639701</arxiv:DOI>
      <dc:creator>Joe Hasei, Yosuke Matsumoto, Hiroki Kawai, Yuko Okahisa, Manabu Takaki, Toshifumi Ozaki</dc:creator>
    </item>
    <item>
      <title>Intelligent ARP Spoofing Detection using Multi-layered Machine Learning (ML) Techniques for IoT Networks</title>
      <link>https://arxiv.org/abs/2507.21087</link>
      <description>arXiv:2507.21087v1 Announce Type: cross 
Abstract: Address Resolution Protocol (ARP) spoofing remains a critical threat to IoT networks, enabling attackers to intercept, modify, or disrupt data transmission by exploiting ARP's lack of authentication. The decentralized and resource-constrained nature of IoT environments amplifies this vulnerability, making conventional detection mechanisms ineffective at scale. This paper introduces an intelligent, multi-layered machine learning framework designed to detect ARP spoofing in real-time IoT deployments. Our approach combines feature engineering based on ARP header behavior, traffic flow analysis, and temporal packet anomalies with a hybrid detection pipeline incorporating decision trees, ensemble models, and deep learning classifiers. We propose a hierarchical architecture to prioritize lightweight models at edge gateways and deeper models at centralized nodes to balance detection accuracy and computational efficiency. The system is validated on both simulated IoT traffic and the CICIDS2017 dataset, achieving over 97% detection accuracy with low false positive rates. Comparative evaluations with signature-based and rule-based systems demonstrate the robustness and generalizability of our approach. Our results show that intelligent machine learning integration enables proactive ARP spoofing detection tailored for IoT scenarios, laying the groundwork for scalable and autonomous network security solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21087v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anas Ali, Mubashar Husain, Peter Hans</dc:creator>
    </item>
    <item>
      <title>Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism</title>
      <link>https://arxiv.org/abs/2507.21098</link>
      <description>arXiv:2507.21098v1 Announce Type: cross 
Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing sustainability and efficiency within the wine industry. It focuses on AI-driven intelligent management in viticulture, wine production, and enotourism. As the wine industry faces environmental and economic challenges, AI offers innovative solutions to optimize resource use, reduce environmental impact, and improve customer engagement. Understanding AI's potential in sustainable winemaking is crucial for fostering responsible and efficient industry practices. The research is based on a questionnaire survey conducted among Polish winemakers, combined with a comprehensive analysis of AI methods applicable to viticulture, production, and tourism. Key AI technologies, including predictive analytics, machine learning, and computer vision, are explored. The findings indicate that AI enhances vineyard monitoring, optimizes irrigation, and streamlines production processes, contributing to sustainable resource management. In enotourism, AI-powered chatbots, recommendation systems, and virtual tastings personalize consumer experiences. The study highlights AI's impact on economic, environmental, and social sustainability, supporting local wine enterprises and cultural heritage. Keywords: Artificial Intelligence, Sustainable Development, AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine Enterprises, Local Communities</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21098v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Sidorkiewicz, Karolina Kr\'olikowska, Berenika Dyczek, Edyta Pijet-Migon, Anna Dubel</dc:creator>
    </item>
    <item>
      <title>Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses</title>
      <link>https://arxiv.org/abs/2507.21132</link>
      <description>arXiv:2507.21132v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence. This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a "high-stakes" activation vector. Our results show that while some models exhibit sycophancy, others like o4-mini remain robust. Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model's cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21132v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Adrian Cahyono, Saran Subramanian</dc:creator>
    </item>
    <item>
      <title>TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</title>
      <link>https://arxiv.org/abs/2507.21134</link>
      <description>arXiv:2507.21134v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21134v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Hui, Yijiang River Dong, Ehsan Shareghi, Nigel Collier</dc:creator>
    </item>
    <item>
      <title>Mitigation of Social Media Platforms Impact on the Users</title>
      <link>https://arxiv.org/abs/2507.21181</link>
      <description>arXiv:2507.21181v1 Announce Type: cross 
Abstract: Social media platforms offer numerous benefits and allow people to come together for various causes. Many communities, academia, government agencies, institutions, healthcare, entertainment, and businesses are on social media platforms. They are intuitive and free for users. It has become unimaginable to live without social media. Their architecture and data handling are geared towards scalability, uninterrupted availability, and both personal and collaborative revenue generation. Primarily, artificial intelligence algorithms are employed on stored user data for optimization and feeds. This has the potential to impact user safety, privacy, and security, even when metadata is used. A new decentralized data arrangement framework based on the Fractal-tree and L-Systems algorithm is proposed to mitigate some of the impacts of social media platforms.
  Future work will focus on demonstrating the effectiveness of the new decentralized framework by comparing its results against state-of-the-art security methods currently used in databases. A cryptographic algorithm could also be implemented for the framework, employing a new key generation for each branch. This will strengthen database security; for example, if a user key is leaked, regenerating the key for each branch will keep the data secure by applying defense mechanisms in the proposed L-System-based tree framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21181v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.GR</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Smita Khapre, Sudhanshu Semwal</dc:creator>
    </item>
    <item>
      <title>Verification Cost Asymmetry in Cognitive Warfare: A Complexity-Theoretic Framework</title>
      <link>https://arxiv.org/abs/2507.21258</link>
      <description>arXiv:2507.21258v1 Announce Type: cross 
Abstract: Human verification under adversarial information flow operates as a cost-bounded decision procedure constrained by working memory limits and cognitive biases. We introduce the Verification Cost Asymmetry (VCA) coefficient, formalizing it as the ratio of expected verification work between populations under identical claim distributions. Drawing on probabilistically checkable proofs (PCP) and parameterized complexity theory, we construct dissemination protocols that reduce verification for trusted audiences to constant human effort while imposing superlinear costs on adversarial populations lacking cryptographic infrastructure. We prove theoretical guarantees for this asymmetry, validate the framework through controlled user studies measuring verification effort with and without spot-checkable provenance, and demonstrate practical encoding of real-world information campaigns. The results establish complexity-theoretic foundations for engineering democratic advantage in cognitive warfare, with immediate applications to content authentication, platform governance, and information operations doctrine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21258v1</guid>
      <category>cs.CR</category>
      <category>cs.CC</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joshua Luberisse</dc:creator>
    </item>
    <item>
      <title>Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems</title>
      <link>https://arxiv.org/abs/2507.21303</link>
      <description>arXiv:2507.21303v1 Announce Type: cross 
Abstract: Each year, over half of global traffic fatalities involve vulnerable road users (e.g. pedestrians), often due to human error. Level-5 automated driving systems (ADSs) could reduce driver errors contributing to pedestrian accidents, though effectiveness depends on clarity and understandability for other road users. External human-machine interfaces (eHMIs) have been proposed to facilitate pedestrian-ADS communication, though consensus on optimal eHMI features remains unclear. In an online survey, 153 participants responded to road-crossing scenarios involving level-5 ADSs, with and without eHMIs. With eHMIs, pedestrians crossed earlier and more confidently, and reported significantly increased perceptions of safety, trust, and understanding when interacting with level-5 ADSs. Visual eHMI features (including a text display and external speedometer) were ranked more necessary than auditory ones, though auditory cues received positive feedback. This study demonstrates that eHMIs can significantly improve pedestrians' understanding of level-5 ADS intent and enhance perceived safety and trust, facilitating more intuitive pedestrian-ADS interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21303v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viktoria Marcus, Griffin Pitts, Sanaz Motamedi</dc:creator>
    </item>
    <item>
      <title>Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning</title>
      <link>https://arxiv.org/abs/2507.21490</link>
      <description>arXiv:2507.21490v1 Announce Type: cross 
Abstract: This full research paper investigates the impact of generative AI (GenAI) on the learner experience, with a focus on how learners engage with and utilize the information it provides. In e-learning environments, learners often need to navigate a complex information space on their own. This challenge is further compounded in interdisciplinary fields like bioinformatics, due to the varied prior knowledge and backgrounds. In this paper, we studied how GenAI influences information search in bioinformatics research: (1) How do interactions with a GenAI chatbot influence learner orienteering behaviors?; and (2) How do learners identify information scent in GenAI chatbot responses? We adopted an autoethnographic approach to investigate these questions. GenAI was found to support orienteering once a learning plan was established, but it was counterproductive prior to that. Moreover, traditionally value-rich information sources such as bullet points and related terms proved less effective when applied to GenAI responses. Information scents were primarily recognized through the presence or absence of prior knowledge of the domain. These findings suggest that GenAI should be adopted into e-learning environments with caution, particularly in interdisciplinary learning contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21490v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Kim, Sergei L. Kosakovsky Pond, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs</title>
      <link>https://arxiv.org/abs/2507.21815</link>
      <description>arXiv:2507.21815v1 Announce Type: cross 
Abstract: Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21815v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kaixuan Wang, Chenxin Diao, Jason T. Jacques, Zhongliang Guo, Shuai Zhao</dc:creator>
    </item>
    <item>
      <title>Training language models to be warm and empathetic makes them less reliable and more sycophantic</title>
      <link>https://arxiv.org/abs/2507.21919</link>
      <description>arXiv:2507.21919v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21919v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujain Ibrahim, Franziska Sofia Hafner, Luc Rocher</dc:creator>
    </item>
    <item>
      <title>Information Fusion in Multimodal IoT Systems for physical activity level monitoring</title>
      <link>https://arxiv.org/abs/2403.14707</link>
      <description>arXiv:2403.14707v3 Announce Type: replace 
Abstract: This study exploits information fusion in IoT systems and uses a clustering method to identify similarities in behaviours and key characteristics within each cluster. This approach facilitates early detection of behaviour changes and provides a more in-depth understanding of behaviour routines for continuous health monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14707v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohsen Shirali, Zahra Ahmadi, Jose-Luis Bayo-Monton, Zoe Valero-Ramon, Carlos Fernandez-Llatas</dc:creator>
    </item>
    <item>
      <title>Not someone, but something: Rethinking trust in the age of medical AI</title>
      <link>https://arxiv.org/abs/2504.05331</link>
      <description>arXiv:2504.05331v3 Announce Type: replace 
Abstract: As artificial intelligence (AI) becomes embedded in healthcare, trust in medical decision-making is changing fast. Nowhere is this shift more visible than in radiology, where AI tools are increasingly embedded across the imaging workflow - from scheduling and acquisition to interpretation, reporting, and communication with referrers and patients. This opinion paper argues that trust in AI isn't a simple transfer from humans to machines - it is a dynamic, evolving relationship that must be built and maintained. Rather than debating whether AI belongs in medicine, it asks: what kind of trust must AI earn, and how? Drawing from philosophy, bioethics, and system design, it explores the key differences between human trust and machine reliability - emphasizing transparency, accountability, and alignment with the values of good care. It argues that trust in AI should not be built on mimicking empathy or intuition, but on thoughtful design, responsible deployment, and clear moral responsibility. The goal is a balanced view - one that avoids blind optimism and reflexive fear. Trust in AI must be treated not as a given, but as something to be earned over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05331v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Beger</dc:creator>
    </item>
    <item>
      <title>The Dual Personas of Social Media Bots</title>
      <link>https://arxiv.org/abs/2504.12498</link>
      <description>arXiv:2504.12498v2 Announce Type: replace 
Abstract: Social media bots are AI agents that participate in online conversations. Most studies focus on the general bot and the malicious nature of these agents. However, bots have many different personas, each specialized towards a specific behavioral or content trait. Neither are bots singularly bad, because they are used for both good and bad information dissemination. In this article, we introduce fifteen agent personas of social media bots. These personas have two main categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We also form yardsticks of the good-bad duality of the bots, elaborating on metrics of good and bad bot agents. Our work puts forth a guideline to inform bot detection regulation, emphasizing that policies should focus on how these agents are employed, rather than collectively terming bot agents as bad.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12498v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lynnette Hui Xian Ng, Kathleen M. Carley</dc:creator>
    </item>
    <item>
      <title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
      <link>https://arxiv.org/abs/2506.22493</link>
      <description>arXiv:2506.22493v2 Announce Type: replace 
Abstract: Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22493v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sadia Kamal, Lalu Prasad Yadav Prakash, S M Rafiuddin, Mohammed Rakib, Arunkumar Bagavathi, Atriya Sen, Sagnik Ray Choudhury</dc:creator>
    </item>
    <item>
      <title>The Carbon Cost of Conversation, Sustainability in the Age of Language Models</title>
      <link>https://arxiv.org/abs/2507.20018</link>
      <description>arXiv:2507.20018v2 Announce Type: replace 
Abstract: Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20018v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sayed Mahbub Hasan Amiri, Prasun Goswami, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Naznin Akter</dc:creator>
    </item>
    <item>
      <title>The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?</title>
      <link>https://arxiv.org/abs/2507.20525</link>
      <description>arXiv:2507.20525v2 Announce Type: replace 
Abstract: This paper presents a case study in the use of a large language model to generate a fictional Buddhist "sutra"', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20525v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murray Shanahan, Tara Das, Robert Thurman</dc:creator>
    </item>
    <item>
      <title>Demystifying Misconceptions in Social Bots Research</title>
      <link>https://arxiv.org/abs/2303.17251</link>
      <description>arXiv:2303.17251v4 Announce Type: replace-cross 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17251v4</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi</dc:creator>
    </item>
    <item>
      <title>Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges</title>
      <link>https://arxiv.org/abs/2406.06736</link>
      <description>arXiv:2406.06736v3 Announce Type: replace-cross 
Abstract: The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06736v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Usman Gohar, Zeyu Tang, Jialu Wang, Kun Zhang, Peter L. Spirtes, Yang Liu, Lu Cheng</dc:creator>
    </item>
    <item>
      <title>Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues</title>
      <link>https://arxiv.org/abs/2503.06424</link>
      <description>arXiv:2503.06424v2 Announce Type: replace-cross 
Abstract: Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06424v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-98414-3_18</arxiv:DOI>
      <arxiv:journal_reference>In Artificial Intelligence in Education. AIED 2025. Lecture Notes in Computer Science(), vol 15877. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Alexander Scarlatos, Naiming Liu, Jaewook Lee, Richard Baraniuk, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
      <link>https://arxiv.org/abs/2503.20797</link>
      <description>arXiv:2503.20797v2 Announce Type: replace-cross 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20797v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Haroon, Magdalena Wojcieszak, Anshuman Chhabra</dc:creator>
    </item>
  </channel>
</rss>
