<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Apr 2024 19:07:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The impact of complexity in the built environment on vehicular routing behavior: Insights from an empirical study of taxi mobility in Beijing, China</title>
      <link>https://arxiv.org/abs/2404.15589</link>
      <description>arXiv:2404.15589v1 Announce Type: new 
Abstract: The modeling of disaggregated vehicular mobility and its associations with the ambient urban built environment is essential for developing operative transport intervention and urban optimization plans. However, established vehicular route choice models failed to fully consider the bounded behavioral rationality and the complex characteristics of the urban built environment affecting drivers' route choice preference. Therefore, the spatio-temporal characteristics of vehicular mobility patterns were not fully explained, which limited the granular implementation of relevant transport interventions. To address this limitation, we proposed a vehicular route choice model that mimics the anchoring effect and the exposure preference while driving. The proposed model enables us to quantitatively examine the impact of the built environment on vehicular routing behavior, which has been largely neglected in previous studies. Results show that the proposed model performs 12% better than the conventional vehicular route choice model based on the shortest path principle. Our empirical analysis of taxi drivers' routing behavior patterns in Beijing, China uncovers that drivers are inclined to choose routes with shorter time duration and with less loss at traversal intersections. Counterintuitively, we also found that drivers heavily rely on circuitous ring roads and expressways to deliver passengers, which are unexpectedly longer than the shortest paths. Moreover, characteristics of the urban built environment including road eccentricity, centrality, average road length, land use diversity, sky visibility, and building coverage can affect drivers' route choice behaviors, accounting for about 5% of the increase in the proposed model's performance. We also refine the above explorations according to the modeling results of trips that differ in departure time, travel distance, and occupation status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15589v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaogui Kang, Zheren Liu</dc:creator>
    </item>
    <item>
      <title>A stochastic approach to estimate distribution grid state with confidence regions</title>
      <link>https://arxiv.org/abs/2404.15722</link>
      <description>arXiv:2404.15722v1 Announce Type: new 
Abstract: Widely available measurement equipment in electrical distribution grids, such as power-quality measurement devices, substation meters, or customer smart meters do not provide phasor measurements due to the lack of high resolution time synchronisation. Instead such measurement devices allow to obtain magnitudes of voltages and currents and the local phase angle between those. In addition, these measurements are subject to measurement errors of up to few percent of the measurand. In order to utilize such measurements for grid monitoring, this paper presents and assesses a stochastic grid calculation approach that allows to derive confidence regions for the resulting current and voltage phasors. Two different metering models are introduced: a PMU model, which is used to validate theoretical properties of the estimator, and an Electric Meter model for which a Gaussian approximation is introduced. The estimator results are compared for the two meter models and case study results for a real Danish distribution grid are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15722v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rasmus L. Olsen, Sina Hassani, Troels Pedersen, Jakob Gulddahl Rasmussen, Hans-Peter Schwefel</dc:creator>
    </item>
    <item>
      <title>Learning Car-Following Behaviors Using Bayesian Matrix Normal Mixture Regression</title>
      <link>https://arxiv.org/abs/2404.16023</link>
      <description>arXiv:2404.16023v1 Announce Type: new 
Abstract: Learning and understanding car-following (CF) behaviors are crucial for microscopic traffic simulation. Traditional CF models, though simple, often lack generalization capabilities, while many data-driven methods, despite their robustness, operate as "black boxes" with limited interpretability. To bridge this gap, this work introduces a Bayesian Matrix Normal Mixture Regression (MNMR) model that simultaneously captures feature correlations and temporal dynamics inherent in CF behaviors. This approach is distinguished by its separate learning of row and column covariance matrices within the model framework, offering an insightful perspective into the human driver decision-making processes. Through extensive experiments, we assess the model's performance across various historical steps of inputs, predictive steps of outputs, and model complexities. The results consistently demonstrate our model's adeptness in effectively capturing the intricate correlations and temporal dynamics present during CF. A focused case study further illustrates the model's outperforming interpretability of identifying distinct operational conditions through the learned mean and covariance matrices. This not only underlines our model's effectiveness in understanding complex human driving behaviors in CF scenarios but also highlights its potential as a tool for enhancing the interpretability of CF behaviors in traffic simulations and autonomous driving systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16023v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengyuan Zhang, Kehua Chen, Meixin Zhu, Hai Yang, Lijun Sun</dc:creator>
    </item>
    <item>
      <title>Unmasking the Role of Remote Sensors in Comfort, Energy and Demand Response</title>
      <link>https://arxiv.org/abs/2404.15368</link>
      <description>arXiv:2404.15368v1 Announce Type: cross 
Abstract: In single-zone multi-room houses (SZMRHs), temperature controls rely on a single probe near the thermostat, resulting in temperature discrepancies that cause thermal discomfort and energy waste. Augmenting smart thermostats (STs) with per-room sensors has gained acceptance by major ST manufacturers. This paper leverages additional sensory information to empirically characterize the services provided by buildings, including thermal comfort, energy efficiency, and demand response (DR). Utilizing room-level time-series data from 1,000 houses, metadata from 110,000 houses across the United States, and data from two real-world testbeds, we examine the limitations of SZMRHs and explore the potential of remote sensors. We discovered that comfortable DR durations (CDRDs) for rooms are typically 70% longer or 40% shorter than for the room with the thermostat. When averaging, rooms at the control temperature's bounds are typically deviated around -3{\deg}F to 2.5{\deg}F from the average. Moreover, in 95\% of houses, we identified rooms experiencing notably higher solar gains compared to the rest of the rooms, while 85% and 70% of houses demonstrated lower heat input and poor insulation, respectively. Lastly, it became evident that the consumption of cooling energy escalates with the increase in the number of sensors, whereas heating usage experiences fluctuations ranging from -19% to +25% This study serves as a benchmark for assessing the thermal comfort and DR services in the existing housing stock, while also highlighting the energy efficiency impacts of sensing technologies. Our approach sets the stage for more granular, precise control strategies of SZMRHs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15368v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ozan Baris Mulayim, Edson Severnini, Mario Berg\'es</dc:creator>
    </item>
    <item>
      <title>Exploring Convergence in Relation using Association Rules Mining: A Case Study in Collaborative Knowledge Production</title>
      <link>https://arxiv.org/abs/2404.15440</link>
      <description>arXiv:2404.15440v1 Announce Type: cross 
Abstract: This study delves into the pivotal role played by non-experts in knowledge production on open collaboration platforms, with a particular focus on the intricate process of tag development that culminates in the proposal of new glitch classes. Leveraging the power of Association Rule Mining (ARM), this research endeavors to unravel the underlying dynamics of collaboration among citizen scientists. By meticulously quantifying tag associations and scrutinizing their temporal dynamics, the study provides a comprehensive and nuanced understanding of how non-experts collaborate to generate valuable scientific insights. Furthermore, this investigation extends its purview to examine the phenomenon of ideological convergence within online citizen science knowledge production. To accomplish this, a novel measurement algorithm, based on the Mann-Kendall Trend Test, is introduced. This innovative approach sheds illuminating light on the dynamics of collaborative knowledge production, revealing both the vast opportunities and daunting challenges inherent in leveraging non-expert contributions for scientific research endeavors. Notably, the study uncovers a robust pattern of convergence in ideology, employing both the newly proposed convergence testing method and the traditional approach based on the stationarity of time series data. This groundbreaking discovery holds significant implications for understanding the dynamics of online citizen science communities and underscores the crucial role played by non-experts in shaping the scientific landscape of the digital age. Ultimately, this study contributes significantly to our understanding of online citizen science communities, highlighting their potential to harness collective intelligence for tackling complex scientific tasks and enriching our comprehension of collaborative knowledge production processes in the digital age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15440v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiahe Ling, Corey B. Jackson</dc:creator>
    </item>
    <item>
      <title>Correlations versus noise in the NFT market</title>
      <link>https://arxiv.org/abs/2404.15495</link>
      <description>arXiv:2404.15495v1 Announce Type: cross 
Abstract: The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market. To deepen the understanding of the dynamics of this market, in the current study, based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform, the degree of correlation in this market is examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix. It appears that correlation strength is lower here than that observed in previously studied markets. Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain. The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15495v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcin W\k{a}torek, Pawe{\l} Szyd{\l}o, Jaros{\l}aw Kwapie\'n, Stanis{\l}aw Dro\.zd\.z</dc:creator>
    </item>
    <item>
      <title>Mapping Incidence and Prevalence Peak Data for SIR Forecasting Applications</title>
      <link>https://arxiv.org/abs/2404.15572</link>
      <description>arXiv:2404.15572v1 Announce Type: cross 
Abstract: Infectious disease modeling and forecasting have played a key role in helping assess and respond to epidemics and pandemics. Recent work has leveraged data on disease peak infection and peak hospital incidence to fit compartmental models for the purpose of forecasting and describing the dynamics of a disease outbreak. Incorporating these data can greatly stabilize a compartmental model fit on early observations, where slight perturbations in the data may lead to model fits that project wildly unrealistic peak infection. We introduce a new method for incorporating historic data on the value and time of peak incidence of hospitalization into the fit for a Susceptible-Infectious-Recovered (SIR) model by formulating the relationship between an SIR model's starting parameters and peak incidence as a system of two equations that can be solved computationally. This approach is assessed for practicality in terms of accuracy and speed of computation via simulation. To exhibit the modeling potential, we update the Dirichlet-Beta State Space modeling framework to use hospital incidence data, as this framework was previously formulated to incorporate only data on total infections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15572v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander C. Murph, G. Casey Gibson, Lauren J. Beesley, Nishant Panda, Lauren A. Castro, Sara Y. Del Valle, Dave Osthus</dc:creator>
    </item>
    <item>
      <title>Comparing baseball players across eras via novel Full House Modeling</title>
      <link>https://arxiv.org/abs/2207.11332</link>
      <description>arXiv:2207.11332v2 Announce Type: replace 
Abstract: A new methodological framework suitable for era-adjusting baseball statistics is developed in this article. Within this methodological framework specific models are motivated. We call these models Full House Models. Full House Models work by balancing the achievements of Major League Baseball (MLB) players within a given season and the size of the MLB talent pool from which a player came. We demonstrate the utility of Full House Models in an application of comparing baseball players' performance statistics across eras. Our results reveal a new ranking of baseball's greatest players which include several modern players among the top all-time players. Modern players are elevated by Full House Modeling because they come from a larger talent pool. Sensitivity and multiverse analyses which investigate the how results change with changes to modeling inputs including the estimate of the talent pool are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.11332v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shen Yan, Adrian Burgos Jr., Christopher Kinson, Daniel J. Eck</dc:creator>
    </item>
    <item>
      <title>Probabilistic wind power forecasting resilient to missing values: an adaptive quantile regression approach</title>
      <link>https://arxiv.org/abs/2305.14662</link>
      <description>arXiv:2305.14662v3 Announce Type: replace 
Abstract: Probabilistic wind power forecasting approaches have significantly advanced in recent decades. However, forecasters often assume data completeness and overlook the challenge of missing values resulting from sensor failures, network congestion, etc. Traditionally, this issue is addressed during the data preprocessing procedure using methods such as deletion and imputation. Nevertheless, these ad-hoc methods pose challenges to probabilistic wind power forecasting at both parameter estimation and operational forecasting stages. In this paper, we propose a resilient probabilistic forecasting approach that smoothly adapts to missingness patterns without requiring preprocessing or retraining. Specifically, we design an adaptive quantile regression model with parameters capable of adapting to missing patterns, comprising two modules. The first is a feature extraction module where weights are kept static and biases are designed as a function of missingness patterns. The second is a non-crossing quantile neural network module, ensuring monotonicity of quantiles, with higher quantiles derived by adding non-negative amounts to lower quantiles. The proposed approach is applicable to cases under all missingness mechanisms including missing-not-at-random cases. Case studies demonstrate that our proposed approach achieves state-of-the-art results in terms of the continuous ranked probability score, with acceptable computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14662v3</guid>
      <category>stat.AP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honglin Wen</dc:creator>
    </item>
    <item>
      <title>Bayesian feedback in the framework of ecological sciences</title>
      <link>https://arxiv.org/abs/2305.17922</link>
      <description>arXiv:2305.17922v2 Announce Type: replace 
Abstract: In ecology we may find scenarios where the same phenomenon (species occurrence, species abundance, etc.) is observed using two different types of samplers. For instance, species data can be collected from scientific sampling with a completely random sample pattern, but also from opportunistic sampling (e.g., whale or bird watching fishery commercial vessels), in which observers tend to look for a specific species in areas where they expect to find
  Species Distribution Models (SDMs) are a widely used tool for analyzing this kind of ecological data. Specifically, we have two models available for the above data: a geostatistical model (GM) for the data coming from a complete random sampler and a preferential model (PM) for data from opportunistic sampling.
  Integration of information coming from different sources can be handled via expert elicitation and integrated models. We focus here in a sequential Bayesian procedure to connect two models through the update of prior distributions. Implementation of the Bayesian paradigm is done through the integrated nested Laplace approximation (INLA) methodology, a good option to make inference and prediction in spatial models with high performance and low computational costs. This sequential approach has been evaluated by simulating several scenarios and comparing the results of sharing information from one model to another using different criteria. The procedure has also been exemplified with a real dataset.
  Our main results imply that, in general, it is better to share information from the independent (completely random) to the preferential model than the alternative way. However, it depends on different factors such as the spatial range or the spatial arrangement of sampling locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17922v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Figueira-Pereira, Xavier Barber, David Conesa, Antonio L\'opez-Qu\'ilez, Joaqu\'in Mart\'inez-Minaya, Iosu Paradinas, Maria Grazia Pennino</dc:creator>
    </item>
    <item>
      <title>Tests for partial correlation between repeatedly observed nonstationary nonlinear timeseries</title>
      <link>https://arxiv.org/abs/2106.07096</link>
      <description>arXiv:2106.07096v2 Announce Type: replace-cross 
Abstract: We describe two families of statistical tests to detect partial correlation in vectorial timeseries. The tests measure whether an observed timeseries Y can be predicted from a second series X, even after accounting for a third series Z which may correlate with X. They do not make any assumptions on the nature of these timeseries, such as stationarity or linearity, but they do require that multiple statistically independent recordings of the 3 series are available. Intuitively, the tests work by asking if the series Y recorded on one experiment can be better predicted from X recorded on the same experiment than on a different experiment, after accounting for the prediction from Z recorded on both experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.07096v2</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth D. Harris, Alex E. Yuan</dc:creator>
    </item>
    <item>
      <title>Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters</title>
      <link>https://arxiv.org/abs/2402.01598</link>
      <description>arXiv:2402.01598v3 Announce Type: replace-cross 
Abstract: Hypertension is a global health concern with an increasing prevalence, underscoring the need for effective monitoring and analysis of blood pressure (BP) dynamics. We analyzed a substantial BP dataset comprising 75,636,128 records from 2,054,462 unique patients collected between 2000 and 2022 at Emory Healthcare in Georgia, USA, representing a demographically diverse population. We examined and compared population-wide statistics of bivariate changes in systolic BP (SBP) and diastolic BP (DBP) across sex, age, and race/ethnicity. The analysis revealed that males have higher BP levels than females and exhibit a distinct BP profile with age. Notably, average SBP consistently rises with age, whereas average DBP peaks in the forties age group. Among the ethnic groups studied, Blacks have marginally higher BPs and a greater standard deviation. We also discovered a significant correlation between SBP and DBP at the population level, a phenomenon not previously researched. These results emphasize the importance of demography-specific BP analysis for clinical diagnosis and provide valuable insights for developing personalized, demography-specific healthcare interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01598v3</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyedeh Somayyeh Mousavi, Yuting Guo, Abeed Sarker, Reza Sameni</dc:creator>
    </item>
    <item>
      <title>Algorithmic Changes Are Not Enough: Evaluating the Removal of Race Adjustment from the eGFR Equation</title>
      <link>https://arxiv.org/abs/2404.12812</link>
      <description>arXiv:2404.12812v2 Announce Type: replace-cross 
Abstract: Changing clinical algorithms to remove race adjustment has been proposed and implemented for multiple health conditions. Removing race adjustment from estimated glomerular filtration rate (eGFR) equations may reduce disparities in chronic kidney disease (CKD), but has not been studied in clinical practice after implementation. Here, we assessed whether implementing an eGFR equation (CKD-EPI 2021) without adjustment for Black or African American race modified quarterly rates of nephrology referrals and visits within a single healthcare system, Stanford Health Care (SHC). Our cohort study analyzed 547,194 adult patients aged 21 and older who had at least one recorded serum creatinine or serum cystatin C between January 1, 2019 and September 1, 2023. During the study period, implementation of CKD-EPI 2021 did not modify rates of quarterly nephrology referrals in those documented as Black or African American or in the overall cohort. After adjusting for capacity at SHC nephrology clinics, estimated rates of nephrology referrals and visits with CKD-EPI 2021 were 34 (95% CI 29, 39) and 188 (175, 201) per 10,000 patients documented as Black or African American. If race adjustment had not been removed, estimated rates were nearly identical: 38 (95% CI: 28, 53) and 189 (165, 218) per 10,000 patients. Changes to the eGFR equation are likely insufficient to achieve health equity in CKD care decision-making as many other structural inequities remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12812v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marika M. Cusick, Glenn M. Chertow, Douglas K. Owens, Michelle Y. Williams, Sherri Rose</dc:creator>
    </item>
  </channel>
</rss>
