<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 02:38:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Fast Counting-Free Algorithm for Computing Atomic Sets in Feature Models</title>
      <link>https://arxiv.org/abs/2501.12490</link>
      <description>arXiv:2501.12490v1 Announce Type: new 
Abstract: In the context of product-line engineering and feature models, atomic sets are sets of features that must always be selected together in order for a configuration to be valid. For many analyses and applications, these features may be condensed into one feature, without affecting, for instance, satisfiability, model counting, sampling, or knowledge compilation. However, the performance of current approaches tends to be insufficient in practice. This is especially true but not limited to approaches based on model counting. In this work, we present a counting-free algorithm for computing atomic sets that only relies on SAT solving. Our evaluation shows that it scales with ease to hard real-world systems and even succeeds for a contemporary version of the Linux kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12490v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tobias He{\ss}, Aaron Molt</dc:creator>
    </item>
    <item>
      <title>Stable Matching with Interviews</title>
      <link>https://arxiv.org/abs/2501.12503</link>
      <description>arXiv:2501.12503v1 Announce Type: new 
Abstract: In several two-sided markets, including labor and dating, agents typically have limited information about their preferences prior to mutual interactions. This issue can result in matching frictions, as arising in the labor market for medical residencies, where high application rates are followed by a large number of interviews. Yet, the extensive literature on two-sided matching primarily focuses on models where agents know their preferences, leaving the interactions necessary for preference discovery largely overlooked. This paper studies this problem using an algorithmic approach, extending Gale-Shapley's deferred acceptance to this context.
  Two algorithms are proposed. The first is an adaptive algorithm that expands upon Gale-Shapley's deferred acceptance by incorporating interviews between applicants and positions. Similar to deferred acceptance, one side sequentially proposes to the other. However, the order of proposals is carefully chosen to ensure an interim stable matching is found. Furthermore, with high probability, the number of interviews conducted by each applicant or position is limited to $O(\log^2 n)$.
  In many seasonal markets, interactions occur more simultaneously, consisting of an initial interview phase followed by a clearing stage. We present a non-adaptive algorithm for generating a single stage set of in tiered random markets. The algorithm finds an interim stable matching in such markets while assigning no more than $O(\log^3 n)$ interviews to each applicant or position.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12503v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itai Ashlagi, Jiale Chen, Mohammad Roghani, Amin Saberi</dc:creator>
    </item>
    <item>
      <title>Making Temporal Betweenness Computation Faster and Restless</title>
      <link>https://arxiv.org/abs/2501.12708</link>
      <description>arXiv:2501.12708v1 Announce Type: new 
Abstract: Bu{\ss} et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n 3 T 2 ) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M ) space and runs in time where M is the number of temporal edges, thus significantly improving the algorithm of Bu{\ss} et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Bu{\ss} et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM )) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n 2 M T 2 ). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12708v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671825</arxiv:DOI>
      <arxiv:journal_reference>KDD '24: The 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Aug 2024, Barcelona, Spain. pp.163-174</arxiv:journal_reference>
      <dc:creator>Filippo Brunelli (JRC), Pierluigi Crescenzi (GSSI), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>On Tradeoffs in Learning-Augmented Algorithms</title>
      <link>https://arxiv.org/abs/2501.12770</link>
      <description>arXiv:2501.12770v1 Announce Type: new 
Abstract: The field of learning-augmented algorithms has gained significant attention in recent years. These algorithms, using potentially inaccurate predictions, must exhibit three key properties: consistency, robustness, and smoothness. In scenarios where distributional information about predictions is available, a strong expected performance is required. Typically, the design of these algorithms involves a natural tradeoff between consistency and robustness, and previous works aimed to achieve Pareto-optimal tradeoffs for specific problems. However, in some settings, this comes at the expense of smoothness. This paper demonstrates that certain problems involve multiple tradeoffs between consistency, robustness, smoothness, and average performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12770v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyad Benomar, Vianney Perchet</dc:creator>
    </item>
    <item>
      <title>A Note on Deterministic FPTAS for Partition</title>
      <link>https://arxiv.org/abs/2501.12848</link>
      <description>arXiv:2501.12848v1 Announce Type: new 
Abstract: We consider the Partition problem and propose a deterministic FPTAS (Fully Polynomial-Time Approximation Scheme) that runs in $\widetilde{O}(n + 1/\varepsilon)$-time. This is the best possible (up to a polylogarithmic factor) assuming the Strong Exponential Time Hypothesis~[Abboud, Bringmann, Hermelin, and Shabtay'22]. Prior to our work, only a randomized algorithm can achieve a running time of $\widetilde{O}(n + 1/\varepsilon)$~[Chen, Lian, Mao and Zhang '24], while the best deterministic algorithm runs in $\widetilde{O}(n+1/\varepsilon^{5/4})$ time~[Deng, Jin and Mao '23] and [Wu and Chen '22].</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12848v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Jiayi Lian, Yuchen Mao, Guochuan Zhang</dc:creator>
    </item>
    <item>
      <title>QuaRs: A Transform for Better Lossless Compression of Integers</title>
      <link>https://arxiv.org/abs/2501.12929</link>
      <description>arXiv:2501.12929v1 Announce Type: new 
Abstract: The rise of integer-valued data, partly driven by the Internet of Things (IoT), has increased demand for efficient compression methods to reduce storage and transmission costs. Existing, speed-oriented methods rely on the ``smaller-numbers-less-bits'' principle, assuming unimodal distributions centered around zero. This assumption is often violated in practice, leading to suboptimal compression. We propose QuaRs, a transformation that reshapes arbitrary distributions into unimodal ones centered around zero, improving compatibility with fast integer compression methods. QuaRs remaps data based on quantiles, assigning smaller magnitudes to frequent values. The method is fast, invertible, and has sub-quadratic complexity. QuaRs enhances compression efficiency, even for challenging distributions, while integrating seamlessly with existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12929v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas G. Matt</dc:creator>
    </item>
    <item>
      <title>An O(log n)-Approximation Algorithm for (p,q)-Flexible Graph Connectivity via Independent Rounding</title>
      <link>https://arxiv.org/abs/2501.12549</link>
      <description>arXiv:2501.12549v1 Announce Type: cross 
Abstract: In the $(p,q)$-Flexible Graph Connectivity problem, the input is a graph $G = (V,E)$ with the edge set $E = \mathscr{S} \cup \mathscr{U}$ partitioned into safe and unsafe edges, and the goal is to find a minimum cost set of edges $F$ such that the subgraph $(V,F)$ remains $p$-edge-connected after removing any $q$ unsafe edges from $F$. We give a new integer programming formulation for the problem, by adding knapsack cover constraints to the $p(p+q)$-connected capacitated edge-connectivity formulation studied in previous work, and show that the corresponding linear relaxation can be solved in polynomial time by giving an efficient separation oracle. Further, we show that independent randomized rounding yields an $O(\log n)$-approximation for arbitrary values of $p$ and $q$, improving the state-of-the-art $O(q\log n)$. For both separation and rounding, a key insight is to use Karger's bound on the number of near-minimum cuts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12549v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharat Ibrahimpur, L\'aszl\'o A. V\'egh</dc:creator>
    </item>
    <item>
      <title>Online Rack Placement in Large-Scale Data Centers</title>
      <link>https://arxiv.org/abs/2501.12725</link>
      <description>arXiv:2501.12725v1 Announce Type: cross 
Abstract: This paper optimizes the configuration of large-scale data centers toward cost-effective, reliable and sustainable cloud supply chains. We formulate an integer optimization model that optimizes the placement of racks of servers within a data center to maximize demand coverage, adhere to space, power and cooling restrictions, and pace resource utilization for future demand. We also define a tractable single-sample online approximation (SSOA) approach to multi-stage stochastic optimization, which approximates unknown parameters with a single realization and re-optimizes decisions dynamically. Theoretical results provide strong performance guarantees of SSOA in the canonical online generalized assignment and online bin packing settings. Computational results using real-world data show that our optimization approach can enhance utilization and reduce power stranding in data centers. Following iterative improvements in collaboration with data center managers, our algorithm has been packaged into a software solution deployed in Microsoft's data centers worldwide. Deployment data indicate a significant increase in adoption, leading to improved power utilization, multi-million-dollar annual cost savings, and concomitant savings in greenhouse gas emissions. Ultimately, this paper constitutes one of the first large-scale deployments of a decision-making tool in data centers, contributing an interactive decision-making process at the human-machine interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12725v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saumil Baxi, Kayla Cummings, Alexandre Jacquillat, Sean Lo, Rob McDonald, Konstantina Mellou, Ishai Menache, Marco Molinaro</dc:creator>
    </item>
    <item>
      <title>Non-adaptive Learning of Random Hypergraphs with Queries</title>
      <link>https://arxiv.org/abs/2501.12771</link>
      <description>arXiv:2501.12771v1 Announce Type: cross 
Abstract: We study the problem of learning a hidden hypergraph $G=(V,E)$ by making a single batch of queries (non-adaptively). We consider the hyperedge detection model, in which every query must be of the form:
  ``Does this set $S\subseteq V$ contain at least one full hyperedge?''
  In this model, it is known that there is no algorithm that allows to non-adaptively learn arbitrary hypergraphs by making fewer than $\Omega(\min\{m^2\log n, n^2\})$ even when the hypergraph is constrained to be $2$-uniform (i.e. the hypergraph is simply a graph). Recently, Li et al. overcame this lower bound in the setting in which $G$ is a graph by assuming that the graph learned is sampled from an Erd\H{o}s-R\'enyi model. We generalize the result of Li et al. to the setting of random $k$-uniform hypergraphs. To achieve this result, we leverage a novel equivalence between the problem of learning a single hyperedge and the standard group testing problem. This latter result may also be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12771v1</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bethany Austhof, Lev Reyzin, Erasmo Tani</dc:creator>
    </item>
    <item>
      <title>Guaranteed Recovery of Unambiguous Clusters</title>
      <link>https://arxiv.org/abs/2501.13093</link>
      <description>arXiv:2501.13093v2 Announce Type: cross 
Abstract: Clustering is often a challenging problem because of the inherent ambiguity in what the "correct" clustering should be. Even when the number of clusters $K$ is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density. In this paper we propose an information-theoretic characterization of when a $K$-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous. This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the clustering. The algorithm first identifies $K$ partial clusters (or "seeds") using a density-based approach, and then adds unclustered points to the initial $K$ partial clusters in a greedy manner to form a complete clustering. We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters, and observe that it requires little parameter selection and displays improved performance on many datasets compared to widely used algorithms for non-convex cluster recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13093v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kayvon Mazooji, Ilan Shomorony</dc:creator>
    </item>
    <item>
      <title>Data reduction for directed feedback vertex set on graphs without long induced cycles</title>
      <link>https://arxiv.org/abs/2308.15900</link>
      <description>arXiv:2308.15900v2 Announce Type: replace 
Abstract: We study reduction rules for Directed Feedback Vertex Set (DFVS) on directed graphs without long cycles. A DFVS instance without cycles longer than $d$ naturally corresponds to an instance of $d$-Hitting Set, however, enumerating all cycles in an $n$-vertex graph and then kernelizing the resulting $d$-Hitting Set instance can be too costly, as already enumerating all cycles can take time $\Omega(n^d)$. We show how to compute a kernel with at most $2^dk^d$ vertices and at most $d^{3d}k^d$ induced cycles of length at most $d$, where $k$ is the size of a minimum directed feedback vertex set.
  We then study classes of graphs whose underlying undirected graphs have bounded expansion or are nowhere dense. We prove that for every nowhere dense class $\mathscr{C}$ there is a function $f_\mathscr{C}(d,\epsilon)$ such that for graphs $G\in \mathscr{C}$ without induced cycles of length greater than $d$ we can compute a kernel with $f_\mathscr{C}(d,\epsilon)\cdot k^{1+\epsilon}$ vertices for any $\epsilon&gt;0$ in time $f_\mathscr{C}(d,\epsilon)\cdot n^{O(1)}$.
  The most restricted classes we consider are strongly connected planar graphs without any (induced or non-induced) long cycles. We show that these classes have treewidth $O(d)$ and hence DFVS on planar graphs without cycles of length greater than $d$ can be solved in time $2^{O(d)}\cdot n^{O(1)}$. We finally present a new data reduction rule for general DFVS and prove that the rule together with two standard rules subsumes all rules applied in the work of Bergougnoux et al.\ to obtain a polynomial kernel for DFVS[FVS], i.e., DFVS parameterized by the feedback vertex set number of the underlying (undirected) graph. We conclude by studying the LP-based approximation of DFVS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15900v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jona Dirks, Enna Gerhard, Mario Grobler, Amer E. Mouawad, Sebastian Siebertz</dc:creator>
    </item>
    <item>
      <title>Generalising the maximum independent set algorithm via Boolean networks</title>
      <link>https://arxiv.org/abs/2403.17658</link>
      <description>arXiv:2403.17658v2 Announce Type: replace 
Abstract: A simple greedy algorithm to find a maximal independent set (MIS) in a graph starts with the empty set and visits every vertex, adding it to the set if and only if none of its neighbours are already in the set. In this paper, we consider (the complexity of decision problems related to) the generalisation of this MIS algorithm wherein any starting set is allowed. Two main approaches are leveraged. Firstly, we view the MIS algorithm as a sequential update of a Boolean network according to a permutation of the vertex set. Secondly, we introduce the concept of a constituency of a graph: a set of vertices that is dominated by an independent set. Recognizing a constituency is NP-complete, a fact we leverage repeatedly in our investigation.
  Our contributions are multiple: we establish that deciding whether all maximal independent sets can be reached from some configuration is coNP-complete; that fixing words (which reach a MIS from any starting configuration) and fixing permutations (briefly, permises) are coNP-complete to recognize; and that permissible graphs (graphs with a permis) are coNP-hard to recognize. We also exhibit large classes of permissible and non-permissible graphs, notably near-comparability graphs which may be of independent interest.
  Lastly, we extend our study to digraphs, where we search for kernels. Since the natural generalisation of our approach may not necessarily find a kernel, we introduce two further Boolean networks for digraphs: one always finds an independent set, and the other always finds a dominating set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17658v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilien Gadouleau, David C. Kutner</dc:creator>
    </item>
    <item>
      <title>Solving the all pairs shortest path problem after minor update of a large dense graph</title>
      <link>https://arxiv.org/abs/2412.15122</link>
      <description>arXiv:2412.15122v5 Announce Type: replace 
Abstract: The all pairs shortest path problem is a fundamental optimization problem in graph theory. We deal with re-calculating the all-pairs shortest path (APSP) matrix after a minor modification of a weighted dense graph, e.g., adding a node, removing a node, or updating an edge. We assume the APSP matrix for the original graph is already known. The graph can be directed or undirected. A cold-start calculation of the new APSP matrix by traditional algorithms, like the Floyd-Warshall algorithm or Dijkstra's algorithm, needs $ O(n^3) $ time. We propose two algorithms for warm-start calculation of the new APSP matrix. The best case complexity for a warm-start calculation is $ O(n^2) $, the worst case complexity is $ O(n^3) $. We implemented the algorithms and tested their performance with experiments. The result shows a warm-start calculation can save a great portion of calculation time, compared with cold-start calculation. In addition, another algorithm is devised to warm-start calculate of the shortest path between two nodes. Experiment shows warm-start calculation can save 99\% of calculation time, compared with cold-start calculation by Dijkstra's algorithm, on directed complete graphs of large sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15122v5</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gangli Liu</dc:creator>
    </item>
    <item>
      <title>An Efficient Algorithm for Permutation Iteration Using a Singly Linked List</title>
      <link>https://arxiv.org/abs/2501.10102</link>
      <description>arXiv:2501.10102v3 Announce Type: replace 
Abstract: We present a new algorithm for iterating over all permutations of a sequence. The algorithm leverages elementary $O(1)$ operations on recursive lists. As a result, no new nodes are allocated during the computation. Instead, all elements are rearranged within the original nodes of the singly linked list throughout the process. While permutations are generated in an unusual order, the transitions between consecutive permutations remain smooth. A proof of concept written in the Lisp programming language is proposed and discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10102v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Baruchel</dc:creator>
    </item>
    <item>
      <title>On the thinness of trees</title>
      <link>https://arxiv.org/abs/2501.11157</link>
      <description>arXiv:2501.11157v2 Announce Type: replace 
Abstract: The study of structural graph width parameters like tree-width, clique-width and rank-width has been ongoing during the last five decades, and their algorithmic use has also been increasing [Cygan et al., 2015]. New width parameters continue to be defined, for example, MIM-width in 2012, twin-width in 2020, and mixed-thinness, a generalization of thinness, in 2022.
  The concept of thinness of a graph was introduced in 2007 by Mannino, Oriolo, Ricci and Chandran, and it can be seen as a generalization of interval graphs, which are exactly the graphs with thinness equal to one. This concept is interesting because if a representation of a graph as a $k$-thin graph is given for a constant value $k$, then several known NP-complete problems can be solved in polynomial time. Some examples are the maximum weighted independent set problem, solved in the seminal paper by Mannino et al., and the capacitated coloring with fixed number of colors [Bonomo, Mattia and Oriolo, 2011].
  In this work we present a constructive $O(n\log(n))$-time algorithm to compute the thinness for any given $n$-vertex tree, along with a corresponding thin representation. We use intermediate results of this construction to improve known bounds of the thinness of some special families of trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11157v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.dam.2024.12.027</arxiv:DOI>
      <arxiv:journal_reference>Discrete Applied Mathematics, Volume 365, 15 April 2025, Pages 39-60 Discrete Applied Mathematics, Volume 365, 2025, Pages 39-60,</arxiv:journal_reference>
      <dc:creator>Flavia Bonomo-Braberman, Eric Brandwein, Carolina Luc\'ia Gonz\'alez, Agust\'in Sansone</dc:creator>
    </item>
    <item>
      <title>Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback</title>
      <link>https://arxiv.org/abs/2303.13604</link>
      <description>arXiv:2303.13604v2 Announce Type: replace-cross 
Abstract: This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13604v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Pedramfar, Vaneet Aggarwal</dc:creator>
    </item>
    <item>
      <title>Enumeration and updates for conjunctive linear algebra queries through expressibility</title>
      <link>https://arxiv.org/abs/2310.04118</link>
      <description>arXiv:2310.04118v3 Announce Type: replace-cross 
Abstract: Due to the importance of linear algebra and matrix operations in data analytics, there is significant interest in using relational query optimization and processing techniques for evaluating (sparse) linear algebra programs. In particular, in recent years close connections have been established between linear algebra programs and relational algebra that allow transferring optimization techniques of the latter to the former. In this paper, we ask ourselves which linear algebra programs in MATLANG correspond to the free-connex and q-hierarchical fragments of conjunctive first-order logic. Both fragments have desirable query processing properties: free-connex conjunctive queries support constant-delay enumeration after a linear-time preprocessing phase, and q-hierarchical conjunctive queries further allow constant-time updates. By characterizing the corresponding fragments of MATLANG, we hence identify the fragments of linear algebra programs that one can evaluate with constant-delay enumeration after linear-time preprocessing and with constant-time updates. To derive our results, we improve and generalize previous correspondences between MATLANG and relational algebra evaluated over semiring-annotated relations. In addition, we identify properties on semirings that allow to generalize the complexity bounds for free-connex and q-hierarchical conjunctive queries from Boolean annotations to general semirings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04118v3</guid>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mu\~noz, Cristian Riveros, Stijn Vansummeren</dc:creator>
    </item>
    <item>
      <title>The Blocklace: A Byzantine-repelling and Universal Conflict-free Replicated Data Type</title>
      <link>https://arxiv.org/abs/2402.08068</link>
      <description>arXiv:2402.08068v4 Announce Type: replace-cross 
Abstract: Conflict-free Replicated Data Types (CRDTs) are designed for replica convergence without global coordination or consensus. Recent work has achieved the same in a Byzantine environment, through DAG-like structures based on cryptographic hashes of content. The blocklace is a partially-ordered generalization of the blockchain in which each block has any finite number of signed hash pointers to preceding blocks. We show that the blocklace datatype, with the sole operation of adding a single block, is a CRDT: it is both a pure operation-based CRDT, with self-tagging; and a delta-state CRDT, under a slight generalization of the delta framework. Allowing arbitrary values as payload, the blocklace can also be seen as a universal Byzantine fault-tolerant implementation for arbitrary CRDTs, under the operation-based approach. Current approaches only care about CRDT convergence, being equivocation-tolerant (they do not detect or prevent equivocations), allowing a Byzantine node to cause an arbitrary amount of harm by polluting the CRDT state with an unbounded number of equivocations. We show that the blocklace can be used not only in an equivocation-tolerant way, but also so as to detect and eventually exclude Byzantine nodes, including equivocators, even under the presence of undetectable colluders. The blocklace CRDT protocol ensures that a Byzantine node may harm only a finite prefix of the computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08068v4</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulo S\'ergio Almeida, Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>Complexity of polytope diameters via perfect matchings</title>
      <link>https://arxiv.org/abs/2404.04158</link>
      <description>arXiv:2404.04158v4 Announce Type: replace-cross 
Abstract: The Circuit diameter of polytopes was introduced by Borgwardt, Finhold and Hemmecke as a fundamental tool for the study of circuit augmentation schemes for linear programming and for estimating combinatorial diameters. Determining the complexity of computing the circuit diameter of polytopes was posed as an open problem by Sanit\`a as well as by Kafer, and was recently reiterated by Borgwardt, Grewe, Kafer, Lee and Sanit\`a.
  In this paper, we solve this problem by showing that computing the circuit diameter of a polytope given in halfspace-description is strongly NP-hard. To prove this result, we show that computing the combinatorial diameter of the perfect matching polytope of a bipartite graph is NP-hard. This complements a result by Sanit\`a (FOCS 2018) on the NP-hardness of computing the diameter of fractional matching polytopes and implies the new result that computing the diameter of a $\{0,1\}$-polytope is strongly NP-hard, which may be of independent interest. In our second main result, we give a precise graph-theoretic description of the monotone diameter of perfect matching polytopes and use this description to prove that computing the monotone (circuit) diameter of a given input polytope is strongly NP-hard as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04158v4</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian N\"obel, Raphael Steiner</dc:creator>
    </item>
    <item>
      <title>Noisy Nonadaptive Group Testing with Binary Splitting: New Test Design and Improvement on Price-Scarlett-Tan's Scheme</title>
      <link>https://arxiv.org/abs/2410.14566</link>
      <description>arXiv:2410.14566v2 Announce Type: replace-cross 
Abstract: In Group Testing, the objective is to identify $K$ defective items out of $N$, $K\ll N$, by testing pools of items together and using the least amount of tests possible. Recently, a fast decoding method based on binary splitting (Price and Scarlett, 2020) has been proposed that simultaneously achieve optimal number of tests and decoding complexity for Non-Adaptive Probabilistic Group Testing (NAPGT). However, the method works only when the test results are noiseless. In this paper, we further study the binary splitting method and propose (1) A NAPGT scheme that generalizes the original binary splitting method from the noiseless case into tests with $\rho$ proportion of false positives (the $\rho$-False Positive Channel), where $\rho$ is a constant, with asymptotically-optimal number of tests and decoding complexity, i.e. $\mathcal{O}(K\log N)$, and (2) A NAPGT scheme in the presence of both false positives and false negatives in test outcomes, improving and generalizing the work of Price, Scarlett and Tan~\cite{price2023fast} in two ways: First, under $\rho$-proportion of test results flipped ($\rho$-Binary Symmetric Channel) and within the general sublinear regime $K=\Theta(N^\alpha)$ where $0&lt;\alpha&lt;1$, our algorithm has a decoding complexity of $\mathcal{O}(\epsilon^{-2}K^{1+\epsilon})$ where $\epsilon&gt;0$ is a constant parameter. Second, when the false negative flipping probability $\rho'$ satisfies $\rho'=\mathcal{O}(K^{-\epsilon})$ and the false positive flipping probability $\rho$ is a constant, we can simultaneously achieve $\mathcal{O}(\epsilon^{-1}K\log N)$ for both the number of tests and the decoding complexity. It remains open to achieve these optimals under the general BSC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14566v2</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaxin Li, Arya Mazumdar</dc:creator>
    </item>
    <item>
      <title>Omnipredicting Single-Index Models with Multi-Index Models</title>
      <link>https://arxiv.org/abs/2411.13083</link>
      <description>arXiv:2411.13083v2 Announce Type: replace-cross 
Abstract: Recent work on supervised learning [GKR+22] defined the notion of omnipredictors, i.e., predictor functions $p$ over features that are simultaneously competitive for minimizing a family of loss functions $\mathcal{L}$ against a comparator class $\mathcal{C}$. Omniprediction requires approximating the Bayes-optimal predictor beyond the loss minimization paradigm, and has generated significant interest in the learning theory community. However, even for basic settings such as agnostically learning single-index models (SIMs), existing omnipredictor constructions require impractically-large sample complexities and runtimes, and output complex, highly-improper hypotheses.
  Our main contribution is a new, simple construction of omnipredictors for SIMs. We give a learner outputting an omnipredictor that is $\varepsilon$-competitive on any matching loss induced by a monotone, Lipschitz link function, when the comparator class is bounded linear predictors. Our algorithm requires $\approx \varepsilon^{-4}$ samples and runs in nearly-linear time, and its sample complexity improves to $\approx \varepsilon^{-2}$ if link functions are bi-Lipschitz. This significantly improves upon the only prior known construction, due to [HJKRR18, GHK+23], which used $\gtrsim \varepsilon^{-10}$ samples.
  We achieve our construction via a new, sharp analysis of the classical Isotron algorithm [KS09, KKKS11] in the challenging agnostic learning setting, of potential independent interest. Previously, Isotron was known to properly learn SIMs in the realizable setting, as well as constant-factor competitive hypotheses under the squared loss [ZWDD24]. As they are based on Isotron, our omnipredictors are multi-index models with $\approx \varepsilon^{-2}$ prediction heads, bringing us closer to the tantalizing goal of proper omniprediction for general loss families and comparators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13083v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lunjia Hu, Kevin Tian, Chutong Yang</dc:creator>
    </item>
  </channel>
</rss>
