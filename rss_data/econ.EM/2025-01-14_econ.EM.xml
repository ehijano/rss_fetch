<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 05:01:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Hybrid Framework for Reinsurance Optimization: Integrating Generative Models and Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2501.06404</link>
      <description>arXiv:2501.06404v1 Announce Type: new 
Abstract: Reinsurance optimization is critical for insurers to manage risk exposure, ensure financial stability, and maintain solvency. Traditional approaches often struggle with dynamic claim distributions, high-dimensional constraints, and evolving market conditions. This paper introduces a novel hybrid framework that integrates {Generative Models}, specifically Variational Autoencoders (VAEs), with {Reinforcement Learning (RL)} using Proximal Policy Optimization (PPO). The framework enables dynamic and scalable optimization of reinsurance strategies by combining the generative modeling of complex claim distributions with the adaptive decision-making capabilities of reinforcement learning.
  The VAE component generates synthetic claims, including rare and catastrophic events, addressing data scarcity and variability, while the PPO algorithm dynamically adjusts reinsurance parameters to maximize surplus and minimize ruin probability. The framework's performance is validated through extensive experiments, including out-of-sample testing, stress-testing scenarios (e.g., pandemic impacts, catastrophic events), and scalability analysis across portfolio sizes. Results demonstrate its superior adaptability, scalability, and robustness compared to traditional optimization techniques, achieving higher final surpluses and computational efficiency.
  Key contributions include the development of a hybrid approach for high-dimensional optimization, dynamic reinsurance parameterization, and validation against stochastic claim distributions. The proposed framework offers a transformative solution for modern reinsurance challenges, with potential applications in multi-line insurance operations, catastrophe modeling, and risk-sharing strategy design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06404v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stella C. Dong, James R. Finlay</dc:creator>
    </item>
    <item>
      <title>Optimizing Financial Data Analysis: A Comparative Study of Preprocessing Techniques for Regression Modeling of Apple Inc.'s Net Income and Stock Prices</title>
      <link>https://arxiv.org/abs/2501.06587</link>
      <description>arXiv:2501.06587v1 Announce Type: new 
Abstract: This article presents a comprehensive methodology for processing financial datasets of Apple Inc., encompassing quarterly income and daily stock prices, spanning from March 31, 2009, to December 31, 2023. Leveraging 60 observations for quarterly income and 3774 observations for daily stock prices, sourced from Macrotrends and Yahoo Finance respectively, the study outlines five distinct datasets crafted through varied preprocessing techniques. Through detailed explanations of aggregation, interpolation (linear, polynomial, and cubic spline) and lagged variables methods, the study elucidates the steps taken to transform raw data into analytically rich datasets. Subsequently, the article delves into regression analysis, aiming to decipher which of the five data processing methods best suits capital market analysis, by employing both linear and polynomial regression models on each preprocessed dataset and evaluating their performance using a range of metrics, including cross-validation score, MSE, MAE, RMSE, R-squared, and Adjusted R-squared. The research findings reveal that linear interpolation with polynomial regression emerges as the top-performing method, boasting the lowest validation MSE and MAE values, alongside the highest R-squared and Adjusted R-squared values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06587v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2478/sues-2025-0004</arxiv:DOI>
      <arxiv:journal_reference>Studia Universitatis Vasile Goldis Arad, Seria Stiinte Economice, 35(1), 83-112, 2025</arxiv:journal_reference>
      <dc:creator>Kevin Ungar, Camelia Oprean-Stan</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Simultaneous Equation Models Using Higher-Order Cumulant Restrictions</title>
      <link>https://arxiv.org/abs/2501.06777</link>
      <description>arXiv:2501.06777v1 Announce Type: new 
Abstract: Identifying structural parameters in linear simultaneous equation models is a fundamental challenge in economics and related fields. Recent work leverages higher-order distributional moments, exploiting the fact that non-Gaussian data carry more structural information than the Gaussian framework. While many of these contributions still require zero-covariance assumptions for structural errors, this paper shows that such an assumption can be dispensed with. Specifically, we demonstrate that under any diagonal higher-cumulant condition, the structural parameter matrix can be identified by solving an eigenvector problem. This yields a direct identification argument and motivates a simple sample-analogue estimator that is both consistent and asymptotically normal. Moreover, when uncorrelatedness may still be plausible -- such as in vector autoregression models -- our framework offers a transparent way to test for it, all within the same higher-order orthogonality setting employed by earlier studies. Monte Carlo simulations confirm desirable finite-sample performance, and we further illustrate the method's practical value in two empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06777v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyu Jiang</dc:creator>
    </item>
    <item>
      <title>Forecasting for monetary policy</title>
      <link>https://arxiv.org/abs/2501.07386</link>
      <description>arXiv:2501.07386v1 Announce Type: new 
Abstract: This paper discusses three key themes in forecasting for monetary policy highlighted in the Bernanke (2024) review: the challenges in economic forecasting, the conditional nature of central bank forecasts, and the importance of forecast evaluation. In addition, a formal evaluation of the Bank of England's inflation forecasts indicates that, despite the large forecast errors in recent years, they were still accurate relative to common benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07386v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Coroneo</dc:creator>
    </item>
    <item>
      <title>Estimating Sequential Search Models Based on a Partial Ranking Representation</title>
      <link>https://arxiv.org/abs/2501.07514</link>
      <description>arXiv:2501.07514v1 Announce Type: new 
Abstract: Consumers are increasingly shopping online, and more and more datasets documenting consumer search are becoming available. While sequential search models provide a framework for utilizing such data, they present empirical challenges. A key difficulty arises from the inequality conditions implied by these models, which depend on multiple unobservables revealed during the search process and necessitate solving or simulating high-dimensional integrals for likelihood-based estimation methods. This paper introduces a novel representation of inequalities implied by a broad class of sequential search models, demonstrating that the empirical content of such models can be effectively captured through a specific partial ranking of available actions. This representation reduces the complexity caused by unobservables and provides a tractable expression for joint probabilities. Leveraging this insight, we propose a GHK-style simulation-based likelihood estimator that is simpler to implement than existing ones. It offers greater flexibility for handling incomplete search data, incorporating additional ranking information, and accommodating complex search processes, including those involving product discovery. We show that the estimator achieves robust performance while maintaining relatively low computational costs, making it a practical and versatile tool for researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07514v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tinghan Zhang</dc:creator>
    </item>
    <item>
      <title>disco: Distributional Synthetic Controls</title>
      <link>https://arxiv.org/abs/2501.07550</link>
      <description>arXiv:2501.07550v1 Announce Type: new 
Abstract: The method of synthetic controls is widely used for evaluating causal effects of policy changes in settings with observational data. Often, researchers aim to estimate the causal impact of policy interventions on a treated unit at an aggregate level while also possessing data at a finer granularity. In this article, we introduce the new disco command, which implements the Distributional Synthetic Controls method introduced in Gunsilius (2023). This command allows researchers to construct entire synthetic distributions for the treated unit based on an optimally weighted average of the distributions of the control units. Several aggregation schemes are provided to facilitate clear reporting of the distributional effects of the treatment. The package offers both quantile-based and CDF-based approaches, comprehensive inference procedures via bootstrap and permutation methods, and visualization capabilities. We empirically illustrate the use of the package by replicating the results in Van Dijcke et al. (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07550v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Gunsilius, David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Sectorial Exclusion Criteria in the Marxist Analysis of the Average Rate of Profit: The United States Case (1960-2020)</title>
      <link>https://arxiv.org/abs/2501.06270</link>
      <description>arXiv:2501.06270v1 Announce Type: cross 
Abstract: The long-term estimation of the Marxist average rate of profit does not adhere to a theoretically grounded standard regarding which economic activities should or should not be included for such purposes, which is relevant because methodological non-uniformity can be a significant source of overestimation or underestimation, generating a less accurate reflection of the capital accumulation dynamics. This research aims to provide a standard Marxist decision criterion regarding the inclusion and exclusion of economic activities for the calculation of the Marxist average profit rate for the case of United States economic sectors from 1960 to 2020, based on the Marxist definition of productive labor, its location in the circuit of capital, and its relationship with the production of surplus value. Using wavelet-transformed Daubechies filters with increased symmetry, empirical mode decomposition, Hodrick-Prescott filter embedded in unobserved components model, and a wide variety of unit root tests the internal theoretical consistency of the presented criteria is evaluated. Also, the objective consistency of the theory is evaluated by a dynamic factor auto-regressive model, Principal Component Analysis, Singular Value Decomposition and Backward Elimination with Linear and Generalized Linear Models. The results are consistent both theoretically and econometrically with the logic of Marx's political economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06270v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Mauricio Gomez Julian</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference on Causal Derivative Effects for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2501.06969</link>
      <description>arXiv:2501.06969v1 Announce Type: cross 
Abstract: Statistical methods for causal inference with continuous treatments mainly focus on estimating the mean potential outcome function, commonly known as the dose-response curve. However, it is often not the dose-response curve but its derivative function that signals the treatment effect. In this paper, we investigate nonparametric inference on the derivative of the dose-response curve with and without the positivity condition. Under the positivity and other regularity conditions, we propose a doubly robust (DR) inference method for estimating the derivative of the dose-response curve using kernel smoothing. When the positivity condition is violated, we demonstrate the inconsistency of conventional inverse probability weighting (IPW) and DR estimators, and introduce novel bias-corrected IPW and DR estimators. In all settings, our DR estimator achieves asymptotic normality at the standard nonparametric rate of convergence. Additionally, our approach reveals an interesting connection to nonparametric support and level set estimation problems. Finally, we demonstrate the applicability of our proposed estimators through simulations and a case study of evaluating a job training program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06969v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Randomization Inference of Heterogeneous Treatment Effects under Network Interference</title>
      <link>https://arxiv.org/abs/2308.00202</link>
      <description>arXiv:2308.00202v3 Announce Type: replace 
Abstract: We design randomization tests of heterogeneous treatment effects when units interact on a single connected network. Our modeling strategy allows network interference into the potential outcomes framework using the concept of exposure mapping. We consider a general class of null hypotheses -- representing different notions of constant and no treatment effects -- that are not sharp due to unknown parameters and multiple potential outcomes. To make the nulls sharp, we propose a conditional randomization method that expands on existing procedures. Our conditioning approach permits the use of functions of treatment as a conditioning variable, widening the scope of application of the randomization method of inference. We show that the resulting testing procedures based on our conditioning approach are valid. We demonstrate the testing methods using a network data set and also present the findings of an extensive Monte Carlo study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00202v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julius Owusu</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayes in Latent Variable Models</title>
      <link>https://arxiv.org/abs/2311.06831</link>
      <description>arXiv:2311.06831v2 Announce Type: replace 
Abstract: Latent variable models are widely used to account for unobserved determinants of economic behavior. This paper introduces a quasi-Bayes approach to nonparametrically estimate a large class of latent variable models. As an application, we model U.S. individual log earnings from the Panel Study of Income Dynamics (PSID) as the sum of latent permanent and transitory components. Simulations illustrate the favorable performance of quasi-Bayes estimators relative to common alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06831v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sid Kankanala</dc:creator>
    </item>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v2 Announce Type: replace 
Abstract: When we interpret linear regression estimates as causal effects justified by quasi-experiments, what do we mean? This paper characterizes the necessary implications when researchers ascribe a design-based interpretation to a given regression. To do so, we define a notion of potential weights, which encode counterfactual decisions a given regression makes to unobserved potential outcomes. A plausible design-based interpretation for a regression estimand implies linear restrictions on the true distribution of treatment; the coefficients in these linear equations are exactly potential weights. Solving these linear restrictions leads to a set of implicit designs that necessarily include the true design if the regression were to admit a causal interpretation. These necessary implications lead to practical diagnostics that add transparency and robustness when design-based interpretation is invoked for a regression. They also lead to new theoretical insights: They serve as a framework that unifies and extends existing results, and they lead to new results for widely used but less understood specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>ARMA-Design: Optimal Treatment Allocation Strategies for A/B Testing in Partially Observable Time Series Experiments</title>
      <link>https://arxiv.org/abs/2408.05342</link>
      <description>arXiv:2408.05342v4 Announce Type: replace 
Abstract: Online experiments %in which experimental units receive a sequence of treatments over time are frequently employed in many technological companies to evaluate the performance of a newly developed policy, product, or treatment relative to a baseline control. In many applications, the experimental units receive a sequence of treatments over time. To handle these time-dependent settings, existing A/B testing solutions typically assume a fully observable experimental environment that satisfies the Markov condition. However, this assumption often does not hold in practice.
  This paper studies the optimal design for A/B testing in partially observable online experiments. We introduce a controlled (vector) autoregressive moving average model to capture partial observability. We introduce a small signal asymptotic framework to simplify the calculation of asymptotic mean squared errors of average treatment effect estimators under various designs. We develop two algorithms to estimate the optimal design: one utilizing constrained optimization and the other employing reinforcement learning. We demonstrate the superior performance of our designs using two dispatch simulators that realistically mimic the behaviors of drivers and passengers to create virtual environments, along with two real datasets from a ride-sharing company. A Python implementation of our proposal is available at https://github.com/datake/ARMADesign.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05342v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Sun, Linglong Kong, Hongtu Zhu, Chengchun Shi</dc:creator>
    </item>
    <item>
      <title>Lee Bounds with a Continuous Treatment in Sample Selection</title>
      <link>https://arxiv.org/abs/2411.04312</link>
      <description>arXiv:2411.04312v2 Announce Type: replace 
Abstract: Sample selection bias arises in causal inference when a treatment affects both the outcome and the researcher's ability to observe it. This paper generalizes the sharp bounds in Lee (2009) for the average treatment effect of a binary treatment to a continuous/multivalued treatment. We revisit the Imbens, Rubin, and Sacerdote (2001) lottery data to study the effect of the prize on earnings that are only observed for the employed and the survey respondents. We evaluate the Job Crops program to study the effect of training hours on wages. To identify the average treatment effect of always-takers who are selected into samples with observed outcomes regardless of the treatment value they receive, we assume that if a subject is selected at some sufficient treatment values, then it remains selected at all treatment values. For example, if program participants are employed with one week of training, then they remain employed with any training hours. This sufficient treatment values assumption includes the monotone assumption on the treatment effect on selection as a special case. We further allow the conditional independence assumption and subjects with different pretreatment covariates to have different sufficient treatment values. The practical estimation and inference theory utilize the orthogonal moment function and cross-fitting for double debiased machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04312v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Ying Lee, Chu-An Liu</dc:creator>
    </item>
  </channel>
</rss>
