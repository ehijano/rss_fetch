<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 May 2024 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Revisit of the Optimal Excess-of-Loss Contract</title>
      <link>https://arxiv.org/abs/2405.00188</link>
      <description>arXiv:2405.00188v1 Announce Type: new 
Abstract: It is well-known that Excess-of-Loss reinsurance has more marketability than Stop-Loss reinsurance, though Stop-Loss reinsurance is the most prominent setting discussed in the optimal (re)insurance design literature. We point out that optimal reinsurance policy under Stop-Loss leads to a zero insolvency probability, which motivates our paper. We provide a remedy to this peculiar property of the optimal Stop-Loss reinsurance contract by investigating the optimal Excess-of-Loss reinsurance contract instead. We also provide estimators for the optimal Excess-of-Loss and Stop-Loss contracts and investigate their statistical properties under many premium principle assumptions and various risk preferences, which according to our knowledge, have never been investigated in the literature. Simulated data and real-life data are used to illustrate our main theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00188v1</guid>
      <category>stat.AP</category>
      <category>econ.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ernest Aboagye, Vali Asimit, Tsz Chai Fung, Liang Peng, Qiuqi Wang</dc:creator>
    </item>
    <item>
      <title>Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools</title>
      <link>https://arxiv.org/abs/2405.00582</link>
      <description>arXiv:2405.00582v1 Announce Type: new 
Abstract: The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00582v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shujie Yan (Grace), Jiwei Zou (Grace), Chang Shu (Grace), Justin Berquist (Grace), Vincent Brochu (Grace), Marc Veillette (Grace), Danlin Hou (Grace), Caroline Duchaine (Grace),  Liang (Grace),  Zhou (John),  Zhiqiang (John),  Zhai (Leon),  Liangzhu (Leon),  Wang</dc:creator>
    </item>
    <item>
      <title>Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles</title>
      <link>https://arxiv.org/abs/2405.00190</link>
      <description>arXiv:2405.00190v1 Announce Type: cross 
Abstract: We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently established result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00190v1</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N. D. Chavda, Priyanka Rao, V. K. B. Kota, Manan Vyas</dc:creator>
    </item>
    <item>
      <title>FPGA Digital Dice using Pseudo Random Number Generator</title>
      <link>https://arxiv.org/abs/2405.00308</link>
      <description>arXiv:2405.00308v1 Announce Type: cross 
Abstract: The goal of this project is to design a digital dice that displays dice numbers in real-time. The number is generated by a pseudo-random number generator (PRNG) using XORshift algorithm that is implemented in Verilog HDL on an FPGA. The digital dice is equipped with tilt sensor, display, power management circuit, and rechargeable battery hosted in a 3D printed dice casing. By shaking the digital dice, the tilt sensor signal produces a seed for the PRNG. This digital dice demonstrates a set of possible random numbers of 2, 4, 6, 8, 10, 12, 20, 100 that simulate the number of dice sides. The kit is named SUTDicey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00308v1</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Lim Kee Hian, Ten Wei Lin, Zachary Wu Xuan, Stephanie-Ann Loy, Maoyang Xiang, T. Hui Teo</dc:creator>
    </item>
    <item>
      <title>Reevaluating coexistence and stability in ecosystem networks to address ecological transients: methods and implications</title>
      <link>https://arxiv.org/abs/2405.00333</link>
      <description>arXiv:2405.00333v1 Announce Type: cross 
Abstract: Representing ecosystems at equilibrium has been foundational for building ecological theories, forecasting species populations and planning conservation actions. The equilibrium "balance of nature" ideal suggests that populations will eventually stabilise to a coexisting balance of species. However, a growing body of literature argues that the equilibrium ideal is inappropriate for ecosystems. Here, we develop and demonstrate a new framework for representing ecosystems without considering equilibrium dynamics. Instead, far more pragmatic ecosystem models are constructed by considering population trajectories, regardless of whether they exhibit equilibrium or transient (i.e. non-equilibrium) behaviour. This novel framework maximally utilises readily available, but often overlooked, knowledge from field observations and expert elicitation, rather than relying on theoretical ecosystem properties. We developed innovative Bayesian algorithms to generate ecosystem models in this new statistical framework, without excessive computational burden. Our results reveal that our pragmatic framework could have a dramatic impact on conservation decision-making and enhance the realism of ecosystem models and forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00333v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah A. Vollert, Christopher Drovandi, Matthew P. Adams</dc:creator>
    </item>
    <item>
      <title>L0-regularized compressed sensing with Mean-field Coherent Ising Machines</title>
      <link>https://arxiv.org/abs/2405.00366</link>
      <description>arXiv:2405.00366v1 Announce Type: cross 
Abstract: Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM's stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.'s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00366v1</guid>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mastiyage Don Sudeera Hasaranga Gunathilaka, Yoshitaka Inui, Satoshi Kako, Kazushi Mimura, Masato Okada, Yoshihisa Yamamoto, Toru Aonishi</dc:creator>
    </item>
    <item>
      <title>Explainable Automatic Grading with Neural Additive Models</title>
      <link>https://arxiv.org/abs/2405.00489</link>
      <description>arXiv:2405.00489v1 Announce Type: cross 
Abstract: The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum. However, current state-of-the-art ASAG models are large neural networks (NN) often described as "black box", providing no explanation for which characteristics of an input are important for the produced output. This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade. To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model. We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response. We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric. We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00489v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aubrey Condor, Zachary Pardos</dc:creator>
    </item>
    <item>
      <title>Bayesian Posterior Interval Calibration to Improve the Interpretability of Observational Studies</title>
      <link>https://arxiv.org/abs/2003.06002</link>
      <description>arXiv:2003.06002v3 Announce Type: replace 
Abstract: Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p-values produced by observational studies only account for random error and fail to account for systematic error. As a consequence, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as "falsification hypotheses") and positive controls. The basic idea is to adjust confidence intervals and p-values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95% coverage of the true effect size by the 95% posterior interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.06002v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jami J. Mulgrave, David Madigan, George Hripcsak</dc:creator>
    </item>
    <item>
      <title>COVID anomaly in the correlation analysis of S&amp;P 500 market states</title>
      <link>https://arxiv.org/abs/2308.14830</link>
      <description>arXiv:2308.14830v2 Announce Type: replace 
Abstract: Analyzing market states of the S&amp;P 500 components on a time horizon January 3, 2006 to August 10, 2023, we found the appearance of a new market state not previously seen and we shall discuss its possible implications as an isolated state or as a beginning of a new general market condition. We study this in terms of the Pearson correlation matrix and relative correlation with respect to the S&amp;P 500 index. In both cases the anomaly shows strongly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14830v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0301238</arxiv:DOI>
      <arxiv:journal_reference>(2024) PLoS ONE 19(4): e0301238</arxiv:journal_reference>
      <dc:creator>M. Mija\'il Mart\'inez-Ramos, Manan Vyas, Parisa Majai, Thomas H. Seligman</dc:creator>
    </item>
    <item>
      <title>Regularizing threshold priors with sparse response patterns in Bayesian factor analysis with categorical indicators</title>
      <link>https://arxiv.org/abs/2307.10503</link>
      <description>arXiv:2307.10503v2 Announce Type: replace-cross 
Abstract: Using instruments comprising ordered responses to items are ubiquitous for studying many constructs of interest. However, using such an item response format may lead to items with response categories infrequently endorsed or unendorsed completely. In maximum likelihood estimation, this results in non-existing estimates for thresholds. This work focuses on a Bayesian estimation approach to counter this issue. The issue changes from the existence of an estimate to how to effectively construct threshold priors. The proposed prior specification reconceptualizes the threshold prior as prior on the probability of each response category. A metric that is easier to manipulate while maintaining the necessary ordering constraints on the thresholds. The resulting induced-prior is more communicable, and we demonstrate comparable statistical efficiency that existing threshold priors. Evidence is provided using a simulated data set, a Monte Carlo simulation study, and an example multi-group item-factor model analysis. All analyses demonstrate how at least a relatively informative threshold prior is necessary to avoid inefficient posterior sampling and increase confidence in the coverage rates of posterior credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10503v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Noah Padgett, Grant B. Morgan, Tim Lomas</dc:creator>
    </item>
    <item>
      <title>Group integrative dynamic factor models with application to multiple subject brain connectivity</title>
      <link>https://arxiv.org/abs/2307.15330</link>
      <description>arXiv:2307.15330v3 Announce Type: replace-cross 
Abstract: This work introduces a novel framework for dynamic factor model-based group-level analysis of multiple subjects time series data, called GRoup Integrative DYnamic factor (GRIDY) models. The framework identifies and characterizes inter-subject similarities and differences between two pre-determined groups by considering a combination of group spatial information and individual temporal dynamics. Furthermore, it enables the identification of intra-subject similarities and differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a non-iterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the GRIDY models is evaluated through simulations conducted under various scenarios. An application is also presented to compare resting-state functional MRI data collected from multiple subjects in autism spectrum disorder and control groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15330v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Zachary F. Fisher, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>Noisy Measurements Are Important, the Design of Census Products Is Much More Important</title>
      <link>https://arxiv.org/abs/2312.14191</link>
      <description>arXiv:2312.14191v2 Announce Type: replace-cross 
Abstract: McCartan et al. (2023) call for "making differential privacy work for census data users." This commentary explains why the 2020 Census Noisy Measurement Files (NMFs) are not the best focus for that plea. The August 2021 letter from 62 prominent researchers asking for production of the direct output of the differential privacy system deployed for the 2020 Census signaled the engagement of the scholarly community in the design of decennial census data products. NMFs, the raw statistics produced by the 2020 Census Disclosure Avoidance System before any post-processing, are one component of that design-the query strategy output. The more important component is the query workload output-the statistics released to the public. Optimizing the query workload-the Redistricting Data (P.L. 94-171) Summary File, specifically-could allow the privacy-loss budget to be more effectively managed. There could be fewer noisy measurements, no post-processing bias, and direct estimates of the uncertainty from disclosure avoidance for each published statistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14191v2</guid>
      <category>cs.CR</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.79d4660d</arxiv:DOI>
      <arxiv:journal_reference>Harvard Data Science Review, Volume 6, Number 2 (Spring, 2024)</arxiv:journal_reference>
      <dc:creator>John M. Abowd</dc:creator>
    </item>
    <item>
      <title>Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots</title>
      <link>https://arxiv.org/abs/2404.18702</link>
      <description>arXiv:2404.18702v2 Announce Type: replace-cross 
Abstract: The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18702v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Xin, Giles Hooker, Fei Huang</dc:creator>
    </item>
  </channel>
</rss>
