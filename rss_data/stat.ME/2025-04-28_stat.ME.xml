<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 03:05:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SOFARI-R: High-Dimensional Manifold-Based Inference for Latent Responses</title>
      <link>https://arxiv.org/abs/2504.17874</link>
      <description>arXiv:2504.17874v1 Announce Type: new 
Abstract: Data reduction with uncertainty quantification plays a key role in various multi-task learning applications, where large numbers of responses and features are present. To this end, a general framework of high-dimensional manifold-based SOFAR inference (SOFARI) was introduced recently in Zheng, Zhou, Fan and Lv (2024) for interpretable multi-task learning inference focusing on the left factor vectors and singular values exploiting the latent singular value decomposition (SVD) structure. Yet, designing a valid inference procedure on the latent right factor vectors is not straightforward from that of the left ones and can be even more challenging due to asymmetry of left and right singular vectors in the response matrix. To tackle these issues, in this paper we suggest a new method of high-dimensional manifold-based SOFAR inference for latent responses (SOFARI-R), where two variants of SOFARI-R are introduced. The first variant deals with strongly orthogonal factors by coupling left singular vectors with the design matrix and then appropriately rescaling them to generate new Stiefel manifolds. The second variant handles the more general weakly orthogonal factors by employing the hard-thresholded SOFARI estimates and delicately incorporating approximation errors into the distribution. Both variants produce bias-corrected estimators for the latent right factor vectors that enjoy asymptotically normal distributions with justified asymptotic variance estimates. We demonstrate the effectiveness of the newly suggested method using extensive simulation studies and an economic application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17874v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zemin Zheng, Xin Zhou, Jinchi Lv</dc:creator>
    </item>
    <item>
      <title>Bernstein Polynomial Processes for Continuous Time Change Detection</title>
      <link>https://arxiv.org/abs/2504.17876</link>
      <description>arXiv:2504.17876v1 Announce Type: new 
Abstract: There is a lack of methodological results for continuous time change detection due to the challenges of noninformative prior specification and efficient posterior inference in this setting. Most methodologies to date assume data are collected according to uniformly spaced time intervals. This assumption incurs bias in the continuous time setting where, a priori, two consecutive observations measured closely in time are less likely to change than two consecutive observations that are far apart in time. Models proposed in this setting have required MCMC sampling which is not ideal. To address these issues, we derive the heterogeneous continuous time Markov chain that models change point transition probabilities noninformatively. By construction, change points under this model can be inferred efficiently using the forward backward algorithm and do not require MCMC sampling. We then develop a novel loss function for the continuous time setting, derive its Bayes estimator, and demonstrate its performance on synthetic data. A case study using time series of remotely sensed observations is then carried out on three change detection applications. To reduce falsely detected changes in this setting, we develop a semiparametric mean function that captures interannual variability due to weather in addition to trend and seasonal components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17876v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Cunha, Mark Friedl, Luis Carvalho</dc:creator>
    </item>
    <item>
      <title>Model Error Covariance Estimation for Weak Constraint Data Assimilation</title>
      <link>https://arxiv.org/abs/2504.17900</link>
      <description>arXiv:2504.17900v1 Announce Type: new 
Abstract: State estimates from weak constraint 4D-Var data assimilation can vary significantly depending on the data and model error covariances. As a result, the accuracy of these estimates heavily depends on the correct specification of both model and observational data error covariances. In this work, we assume that the data error is known and and focus on estimating the model error covariance by framing weak constraint 4D-Var as a regularized inverse problem, where the inverse model error covariance serves as the regularization matrix. We consider both isotropic and non-isotropic forms of the model error covariance. Using the representer method, we reduce the 4D-Var problem from state space to data space, enabling the efficient application of regularization parameter selection techniques. The Representer method also provides an analytic expression for the optimal state estimate, allowing us to derive matrix expressions for the three regularization parameter selection methods i.e. the L-curve, generalized cross-validation (GCV), and the Chi-square method. We validate our approach by assimilating simulated data into a 1D transport equation modeling wildfire smoke transport under various observational noise and forward model perturbations. In these experiments the goal is to identify the model error covariances that accurately capture the influence of observational data versus model predictions on assimilated state estimates. The regularization parameter selection methods successfully estimate hyperparameters for both isotropic and non-isotropic model error covariances, that reflect whether the first guess model predictions are more or less reliable than the observational data. The results further indicate that isotropic variances are sufficient when the first guess is more accurate than the data whereas non-isotropic covariances are preferred when the observational data is more reliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17900v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sandra R. Babyale, Jodi Mead, Donna Calhoun, Patricia O. Azike</dc:creator>
    </item>
    <item>
      <title>A Directional Measure of Marginal Inhomogeneity for Square Contingency Tables using Discrete-time hazard</title>
      <link>https://arxiv.org/abs/2504.18100</link>
      <description>arXiv:2504.18100v1 Announce Type: new 
Abstract: In the analysis of square contingency tables with ordered categories, it is essential to assess deviations from marginal homogeneity (MH) when marginal equivalency between row and column variables does not hold. Some measures for evaluating the degree of departure from the MH model have been proposed. This study proposes a new directional measure using the discrete-time hazard, assuming that categories represent discrete time points. The proposed measure is capable of capturing both the magnitude and direction of deviation from the MH model. It is defined on a continuous scale from $-1$ to $1$, which allows for intuitive interpretation of the nature of marginal change.
  An estimator of the proposed measure and an asymptotic confidence interval are derived using the delta method. The theoretical properties of the measure are also discussed. The proposed measure provides a flexible tool for characterizing marginal inhomogeneity in square contingency tables under ordinal settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18100v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Tamura, Satoru Shinoda</dc:creator>
    </item>
    <item>
      <title>Debiased Continuous Updating GMM with Many Weak Instruments</title>
      <link>https://arxiv.org/abs/2504.18107</link>
      <description>arXiv:2504.18107v1 Announce Type: new 
Abstract: Many weak instrumental variables (IVs) are routinely used in the health and social sciences to improve identification and inference, but can lead to bias in the usual two-step generalized method of moments methods. We propose a new debiased continuous updating estimator (CUE) which simultaneously address the biases from the diverging number of weak IVs, and concomitant first-step nonparametric or high-dimensional estimation of regression functions in the measured covariates. We establish mean-square rate requirements on the first-step estimators so that debiased CUE remains consistent and asymptotically normal under a many weak IVs asymptotic regime, in which the number of IVs diverges with sample size while identification shrinks. We evaluate the proposed method via extensive Monte Carlo studies and an empirical application to estimate the returns to education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18107v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Zhang, Baoluo Sun</dc:creator>
    </item>
    <item>
      <title>Fast approximative estimation of conditional Shapley values when using a linear regression model or a polynomial regression model</title>
      <link>https://arxiv.org/abs/2504.18167</link>
      <description>arXiv:2504.18167v1 Announce Type: new 
Abstract: We develop a new approximative estimation method for conditional Shapley values obtained using a linear regression model. We develop a new estimation method and outperform existing methodology and implementations. Compared to the sequential method in the shapr-package (i.e fit one and one model), our method runs in minutes and not in hours. Compared to the iterative method in the shapr-package, we obtain better estimates in less than or almost the same amount of time. When the number of covariates becomes too large, one can still fit thousands of regression models at once using our method. We focus on a linear regression model, but one can easily extend the method to accommodate several types of splines that can be estimated using multivariate linear regression due to linearity in the parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18167v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fredrik Lohne Aanes</dc:creator>
    </item>
    <item>
      <title>Structured Bayesian Regression Tree Models for Estimating Distributed Lag Effects: The R Package dlmtree</title>
      <link>https://arxiv.org/abs/2504.18452</link>
      <description>arXiv:2504.18452v1 Announce Type: new 
Abstract: When examining the relationship between an exposure and an outcome, there is often a time lag between exposure and the observed effect on the outcome. A common statistical approach for estimating the relationship between the outcome and lagged measurements of exposure is a distributed lag model (DLM). Because repeated measurements are often autocorrelated, the lagged effects are typically constrained to vary smoothly over time. A recent statistical development on the smoothing constraint is a tree structured DLM framework. We present an R package dlmtree, available on CRAN, that integrates tree structured DLM and extensions into a comprehensive software package with user-friendly implementation. A conceptual background on tree structured DLMs and demonstration of the fitting process of each model using simulated data are provided. We also demonstrate inference and interpretation using the fitted models, including summary and visualization. Additionally, a built-in shiny app for heterogeneity analysis is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18452v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongwon Im, Ander Wilson, Daniel Mork</dc:creator>
    </item>
    <item>
      <title>Probabilities of causation and post-infection outcomes</title>
      <link>https://arxiv.org/abs/2504.17992</link>
      <description>arXiv:2504.17992v1 Announce Type: cross 
Abstract: Probabilities of causation provide explanatory information on the observed occurrence (causal necessity) and non-occurrence (causal sufficiency) of events. Here, we adapt these probabilities (probability of necessity, probability of sufficiency, and probability of necessity and sufficiency) to an important class of epidemiologic outcomes, post-infection outcomes. A defining feature of studies on these outcomes is that they account for the post-treatment variable, infection acquisition, which means that, for individuals who remain uninfected, the outcome is not defined. Following previous work by Hudgens and Halloran, we describe analyses of post-infection outcomes using the principal stratification framework, and then derive expressions for the probabilities of causation in terms of principal strata-related parameters. Finally, we show that these expressions provide insights into the contributions of different processes (absence or occurrence of infection, and disease severity), implicitly encoded in the definition of the outcome, to causation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17992v1</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bronner P. Gon\c{c}alves</dc:creator>
    </item>
    <item>
      <title>Numerical Generalized Randomized Hamiltonian Monte Carlo for piecewise smooth target densities</title>
      <link>https://arxiv.org/abs/2504.18210</link>
      <description>arXiv:2504.18210v1 Announce Type: cross 
Abstract: Traditional gradient-based sampling methods, like standard Hamiltonian Monte Carlo, require that the desired target distribution is continuous and differentiable. This limits the types of models one can define, although the presented models capture the reality in the observations better. In this project, Generalized Randomized Hamiltonian Monte Carlo (GRHMC) processes for sampling continuous densities with discontinuous gradient and piecewise smooth targets are proposed. The methods combine the advantages of Hamiltonian Monte Carlo methods with the nature of continuous time processes in the form of piecewise deterministic Markov processes to sample from such distributions. It is argued that the techniques lead to GRHMC processes that admit the desired target distribution as the invariant distribution in both scenarios. Simulation experiments verifying this fact and several relevant real-life models are presented, including a new parameterization of the spike and slab prior for regularized linear regression that returns sparse coefficient estimates and a regime switching volatility model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18210v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Huy Tran, Tore Selland Kleppe</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap: Introducing Joint Models for Longitudinal and Time-to-event Data in the Social Sciences</title>
      <link>https://arxiv.org/abs/2504.18288</link>
      <description>arXiv:2504.18288v1 Announce Type: cross 
Abstract: In time-to-event analyses in social sciences, there often exist endogenous time-varying variables, where the event status is correlated with the trajectory of the covariate itself. Ignoring this endogeneity will result in biased estimates. In the field of biostatistics this issue is tackled by estimating a joint model for longitudinal and time-to-event data as it handles endogenous covariates properly. This method is underused in the social sciences even though it is very useful to model longitudinal and time-to-event processes appropriately. Therefore, this paper provides a gentle introduction to the method of joint models and highlights its advantages for social science research questions. We demonstrate its usage on an example on marital satisfaction and marriage dissolution and compare the results with classical approaches such as a time-to-event model with a time-varying covariate. In addition to demonstrating the method, our results contribute to the understanding of the relationship between marriage satisfaction, marriage dissolution and other covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18288v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sophie Potts, Anja Rappl, Karin Kurz, Elisabeth Bergherr</dc:creator>
    </item>
    <item>
      <title>Enhancing Visual Interpretability and Explainability in Functional Survival Trees and Forests</title>
      <link>https://arxiv.org/abs/2504.18498</link>
      <description>arXiv:2504.18498v1 Announce Type: cross 
Abstract: Functional survival models are key tools for analyzing time-to-event data with complex predictors, such as functional or high-dimensional inputs. Despite their predictive strength, these models often lack interpretability, which limits their value in practical decision-making and risk analysis. This study investigates two key survival models: the Functional Survival Tree (FST) and the Functional Random Survival Forest (FRSF). It introduces novel methods and tools to enhance the interpretability of FST models and improve the explainability of FRSF ensembles. Using both real and simulated datasets, the results demonstrate that the proposed approaches yield efficient, easy-to-understand decision trees that accurately capture the underlying decision-making processes of the model ensemble.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18498v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Loffredo, Elvira Romano, Fabrizio MAturo</dc:creator>
    </item>
    <item>
      <title>Time to adjust: Improving replicability in experimental psychology by adjustment for evident selective inference</title>
      <link>https://arxiv.org/abs/2006.11585</link>
      <description>arXiv:2006.11585v3 Announce Type: replace 
Abstract: The field of psychological sciences has been grappling with the replicability crisis. Various issues have been identified as potential sources of this problem. We bring to light a potential source that has largely been overlooked and demonstrate its significant contribution to the problem: the practice of multiple comparisons. We analyzed 88 papers from the Reproducibility Project in Psychology and found that multiple results are commonly reported in a single paper, ranging from 4 to 730 (M=77.7), without multiple comparison adjustments. We retroactively applied such an adjustment using a hierarchical FDR controlling procedure (TreeBH; Bogomolov et al., 2021). 21 of 88 results were deemed insignificant after adjustment. Twenty of these 21 results indeed failed to replicate, constituting over a third of the non-replicable findings, while maintaining 97% power. We propose that this should become a common practice as an essential means to increase replicability in experimental psychology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.11585v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoav Zeevi, Sofi Astashenko, Liad Mudrik, Yoav Benjamini</dc:creator>
    </item>
    <item>
      <title>Minimax estimation of Functional Principal Components from noisy discretized functional data</title>
      <link>https://arxiv.org/abs/2110.12739</link>
      <description>arXiv:2110.12739v2 Announce Type: replace 
Abstract: Functional Principal Component Analysis is a reference method for dimension reduction of curve data. Its theoretical properties are now well understood in the simplified case where the sample curves are fully observed without noise. However, functional data are noisy and necessarily observed on a finite discretization grid. Common practice consists in smoothing the data and then to compute the functional estimates, but the impact of this denoising step on the procedure's statistical performance are rarely considered. Here we prove new convergence rates for functional principal component estimators. We introduce a double asymptotic framework: one corresponding to the sampling size and a second to the size of the grid. We prove that estimates based on projection onto histograms show optimal rates in a minimax sense. Theoretical results are illustrated on simulated data and the method is applied to the visualization of genomic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.12739v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryad Belhakem, Franck Picard, Vincent Rivoirard, Angelina Roche</dc:creator>
    </item>
    <item>
      <title>Geometry-driven Bayesian Inference for Ultrametric Covariance Matrices</title>
      <link>https://arxiv.org/abs/2401.11515</link>
      <description>arXiv:2401.11515v2 Announce Type: replace 
Abstract: Ultrametric matrices are a class of covariance matrices that arise in latent tree models. As a parameter space in a statistical model, the set of ultrametric matrices is neither convex nor a smooth manifold. Focus in the literature has hitherto been restricted to estimation through projections and relaxation-based techniques, and inferential methods are lacking. Motivated by this, we establish a bijection between the set of positive definite ultrametric matrices and the set of rooted, leaf-labeled trees equipped with the stratified geometry of the well-known phylogenetic treespace. Using the pullback geometry under the bijection and by adapting sampling algorithms in Bayesian phylogenetics, we develop algorithms to sample from the posterior distribution on the set of ultrametric matrices in a Bayesian latent tree model where the tree may be binary or multifurcating. We demonstrate the utility of the algorithms in simulation studies, and illustrate them on a pre-clinical cancer application to quantify uncertainty about treatment trees that identify treatments with high mechanism similarity that target correlated pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11515v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsung-Hung Yao, Zhenke Wu, Karthik Bharath, Veerabhadran Baladandayuthapani</dc:creator>
    </item>
    <item>
      <title>Adaptive Uncertainty Quantification for Generative AI</title>
      <link>https://arxiv.org/abs/2408.08990</link>
      <description>arXiv:2408.08990v2 Announce Type: replace 
Abstract: This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08990v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Sean O'Hagan, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Comparing the morphology of molecular clouds without supervision</title>
      <link>https://arxiv.org/abs/2407.09832</link>
      <description>arXiv:2407.09832v2 Announce Type: replace-cross 
Abstract: Molecular clouds show complex structures reflecting their non-linear dynamics. Many studies investigating the bridge between their morphology and physical properties have shown the value of non-Gaussian higher-order statistics in capturing physical information. Yet, as this bridge is usually characterized in the supervised world of simulations, transferring it to observations can be hazardous, especially when the discrepancy between simulations and observations remains unknown. In this paper, we aim to identify relevant summary statistics, directly from the observation data. To do so, we developed a test to compare the informative power of two sets of summary statistics for a given unlabeled dataset. Contrary to supervised approaches, this test does not require knowledge of any class label or parameter associated with the data. Instead, it evaluates and compares the degeneracy levels of the summary statistics based on a notion of statistical compatibility. We applied this test to column density maps of 14 nearby molecular clouds observed by Herschel and iteratively compared different sets of typical summary statistics. We show that a standard Gaussian description of these clouds is highly degenerate but can be substantially improved when being estimated on the logarithm of the maps. This illustrates that low-order statistics, when properly used, remain very powerful. We further show that such descriptions still exhibit a small quantity of degeneracies, some of which are lifted by the higher-order statistics provided by reduced wavelet scattering transforms. These degeneracies quantitatively differ between observations and state-of-the-art simulations of dense cloud collapse, and they are not present for logFBM models. Finally, we show how to cooperatively use the summary statistics identified to build a morphological distance, which is evaluated visually and gives convincing results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09832v2</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1051/0004-6361/202451493</arxiv:DOI>
      <arxiv:journal_reference>A&amp;A, 696, A217 (2025)</arxiv:journal_reference>
      <dc:creator>Pablo Richard, Erwan Allys, Fran\c{c}ois Levrier, Antoine Gusdorf, Constant Auclair</dc:creator>
    </item>
    <item>
      <title>Efficient Budget Allocation for Large-Scale LLM-Enabled Virtual Screening</title>
      <link>https://arxiv.org/abs/2408.09537</link>
      <description>arXiv:2408.09537v2 Announce Type: replace-cross 
Abstract: Screening tasks that aim to identify a small subset of top alternatives from a large pool are common in business decision-making processes. These tasks often require substantial human effort to evaluate each alternative's performance, making them time-consuming and costly. Motivated by recent advances in large language models (LLMs), particularly their ability to generate outputs that align well with human evaluations, we consider an LLM-as-human-evaluator approach for conducting screening virtually, thereby reducing the cost burden. To achieve scalability and cost-effectiveness in virtual screening, we identify that the stochastic nature of LLM outputs and their cost structure necessitate efficient budget allocation across all alternatives. To address this, we propose using a top-$m$ greedy evaluation mechanism, a simple yet effective approach that keeps evaluating the current top-$m$ alternatives, and design the explore-first top-$m$ greedy (EFG-$m$) algorithm. We prove that EFG-$m$ is both sample-optimal and consistent in large-scale virtual screening. Surprisingly, we also uncover a bonus ranking effect, where the algorithm naturally induces an indifference-based ranking within the selected subset. To further enhance practicality, we design a suite of algorithm variants to improve screening performance and computational efficiency. Numerical experiments validate our results and demonstrate the effectiveness of our algorithms. Lastly, we conduct a case study on LLM-based virtual screening. The study shows that while LLMs alone may not provide meaningful screening and ranking results when directly queried, integrating them with our sample-optimal algorithms unlocks their potential for cost-effective, large-scale virtual screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09537v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaile Li, Weiwei Fan, L. Jeff Hong</dc:creator>
    </item>
    <item>
      <title>Integrative Learning of Quantum Dot Intensity Fluctuations under Excitation via Tailored Dynamic Mixture Modeling</title>
      <link>https://arxiv.org/abs/2501.01292</link>
      <description>arXiv:2501.01292v2 Announce Type: replace-cross 
Abstract: Semiconductor nano-crystals, known as quantum dots (QDs), have attracted significant attention for their unique fluorescence properties. Under continuous excitation, QDs emit photons with intricate intensity fluctuation: the intensity of photon emission fluctuates during the excitation, and such a fluctuation pattern can vary across different QDs even under the same experimental conditions. What adding to the complication is that the processed intensity series are non-Gaussian and truncated due to necessary thresholding and normalization. Conventional normality-based single-dot analysis fall short of addressing these complexities. In collaboration with chemists, we develop an integrative learning approach to simultaneously analyzing intensity series from multiple QDs. Motivated by the unique data structure and the hypothesized behaviors of the QDs, our approach leverages the celebrated hidden Markov model as its structural backbone to characterize individual dot intensity fluctuations, while assuming that, in each state the normalized intensity follows a 0/1 inflated Beta distribution, the state/emission distributions are shared across the QDs, and the state transition dynamics can vary among a few QD clusters. This framework allows for a precise, collective characterization of intensity fluctuation patterns and have the potential to transform current practice in chemistry. Applying our method to experimental data from 128 QDs, we reveal three shared intensity states and capture several distinct intensity transition patterns, underscoring the effectiveness of our approach in providing deeper insights into QD behaviors and their design and application potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01292v2</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Hawi Nyiera, Yonglei Sun, Jing Zhao, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Distributed Multiple Testing with False Discovery Rate Control in the Presence of Byzantines</title>
      <link>https://arxiv.org/abs/2501.13242</link>
      <description>arXiv:2501.13242v2 Announce Type: replace-cross 
Abstract: This work studies distributed multiple testing with false discovery rate (FDR) control in the presence of Byzantine attacks, where an adversary captures a fraction of the nodes and corrupts their reported p-values. We focus on two baseline attack models: an oracle model with the full knowledge of which hypotheses are true nulls, and a practical attack model that leverages the Benjamini-Hochberg (BH) procedure locally to classify which p-values follow the true null hypotheses. We provide a thorough characterization of how both attack models affect the global FDR, which in turn motivates counter-attack strategies and stronger attack models. Our extensive simulation studies confirm the theoretical results, highlight key design trade-offs under attacks and countermeasures, and provide insights into more sophisticated attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13242v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daofu Zhang, Mehrdad Pournaderi, Yu Xiang, Pramod Varshney</dc:creator>
    </item>
  </channel>
</rss>
