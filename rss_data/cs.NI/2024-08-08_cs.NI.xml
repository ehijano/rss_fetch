<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>GNN-Based Joint Channel and Power Allocation in Heterogeneous Wireless Networks</title>
      <link>https://arxiv.org/abs/2408.03957</link>
      <description>arXiv:2408.03957v1 Announce Type: new 
Abstract: The optimal allocation of channels and power resources plays a crucial role in ensuring minimal interference, maximal data rates, and efficient energy utilisation. As a successful approach for tackling resource management problems in wireless networks, Graph Neural Networks (GNNs) have attracted a lot of attention. This article proposes a GNN-based algorithm to address the joint resource allocation problem in heterogeneous wireless networks. Concretely, we model the heterogeneous wireless network as a heterogeneous graph and then propose a graph neural network structure intending to allocate the available channels and transmit power to maximise the network throughput. Our proposed joint channel and power allocation graph neural network (JCPGNN) comprises a shared message computation layer and two task-specific layers, with a dedicated focus on channel and power allocation tasks, respectively. Comprehensive experiments demonstrate that the proposed algorithm achieves satisfactory performance but with higher computational efficiency compared to traditional optimisation algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03957v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lili Chen, Jingge Zhu, Jamie Evans</dc:creator>
    </item>
    <item>
      <title>Semantic Enabled 6G LEO Satellite Communication for Earth Observation: A Resource-Constrained Network Optimization</title>
      <link>https://arxiv.org/abs/2408.03959</link>
      <description>arXiv:2408.03959v1 Announce Type: new 
Abstract: Earth observation satellites generate large amounts of real-time data for monitoring and managing time-critical events such as disaster relief missions. This presents a major challenge for satellite-to-ground communications operating under limited bandwidth capacities. This paper explores semantic communication (SC) as a potential alternative to traditional communication methods. The rationality for adopting SC is its inherent ability to reduce communication costs and make spectrum efficient for 6G non-terrestrial networks (6G-NTNs). We focus on the critical satellite imagery downlink communications latency optimization for Earth observation through SC techniques. We formulate the latency minimization problem with SC quality-of-service (SC-QoS) constraints and address this problem with a meta-heuristic discrete whale optimization algorithm (DWOA) and a one-to-one matching game. The proposed approach for captured image processing and transmission includes the integration of joint semantic and channel encoding to ensure downlink sum-rate optimization and latency minimization. Empirical results from experiments demonstrate the efficiency of the proposed framework for latency optimization while preserving high-quality data transmission when compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03959v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheikh Salman Hassan, Loc X. Nguyen, Yan Kyaw Tun, Zhu Han, Choong Seon Hong</dc:creator>
    </item>
    <item>
      <title>Telecom Foundation Models: Applications, Challenges, and Future Trends</title>
      <link>https://arxiv.org/abs/2408.03964</link>
      <description>arXiv:2408.03964v1 Announce Type: new 
Abstract: Telecom networks are becoming increasingly complex, with diversified deployment scenarios, multi-standards, and multi-vendor support. The intricate nature of the telecom network ecosystem presents challenges to effectively manage, operate, and optimize networks. To address these hurdles, Artificial Intelligence (AI) has been widely adopted to solve different tasks in telecom networks. However, these conventional AI models are often designed for specific tasks, rely on extensive and costly-to-collect labeled data that require specialized telecom expertise for development and maintenance. The AI models usually fail to generalize and support diverse deployment scenarios and applications. In contrast, Foundation Models (FMs) show effective generalization capabilities in various domains in language, vision, and decision-making tasks. FMs can be trained on multiple data modalities generated from the telecom ecosystem and leverage specialized domain knowledge. Moreover, FMs can be fine-tuned to solve numerous specialized tasks with minimal task-specific labeled data and, in some instances, are able to leverage context to solve previously unseen problems. At the dawn of 6G, this paper investigates the potential opportunities of using FMs to shape the future of telecom technologies and standards. In particular, the paper outlines a conceptual process for developing Telecom FMs (TFMs) and discusses emerging opportunities for orchestrating specialized TFMs for network configuration, operation, and maintenance. Finally, the paper discusses the limitations and challenges of developing and deploying TFMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03964v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tahar Zanouda, Meysam Masoudi, Fitsum Gaim Gebre, Mischa Dohler</dc:creator>
    </item>
    <item>
      <title>TupleChain: Fast Lookup of OpenFlow Table with Multifaceted Scalability</title>
      <link>https://arxiv.org/abs/2408.04390</link>
      <description>arXiv:2408.04390v1 Announce Type: new 
Abstract: OpenFlow switches are fundamental components of software defined networking, where the key operation is to look up flow tables to determine which flow an incoming packet belongs to. This needs to address the same multi-field rule-matching problem as legacy packet classification, but faces more serious scalability challenges. The demand of fast on-line updates makes most existing solutions unfit, while the rest still lacks the scalability to either large data sets or large number of fields to match for a rule. In this work, we propose TupleChain for fast OpenFlow table lookup with multifaceted scalability. We group rules based on their masks, each being maintained with a hash table, and explore the connections among rule groups to skip unnecessary hash probes for fast search. We show via theoretical analysis and extensive experiments that the proposed scheme not only has competitive computing complexity, but is also scalable and can achieve high performance in both search and update. It can process multiple millions of packets per second, while dealing with millions of on-line updates per second at the same time, and its lookup speed maintains at the same level no mater it handles a large flow table with 10 million rules or a flow table with every entry having as many as 100 match fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04390v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbiao Li, Neng Ren, Xin Wang, Yuxuan Chen, Xinyi Zhang, Lingbo Guo, Gaogang Xie</dc:creator>
    </item>
    <item>
      <title>Towards Explainable Network Intrusion Detection using Large Language Models</title>
      <link>https://arxiv.org/abs/2408.04342</link>
      <description>arXiv:2408.04342v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents. However, their applicability to threat detection problems remains unclear. This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explainability. Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS. Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments. Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge. Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS. Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04342v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul R. B. Houssel, Priyanka Singh, Siamak Layeghy, Marius Portmann</dc:creator>
    </item>
    <item>
      <title>Role of Error Syndromes in Teleportation Scheduling</title>
      <link>https://arxiv.org/abs/2408.04536</link>
      <description>arXiv:2408.04536v1 Announce Type: cross 
Abstract: Quantum teleportation enables quantum information transmission, but requires distribution of entangled resource states. Unfortunately, decoherence, caused by environmental interference during quantum state storage, can degrade quantum states, leading to entanglement loss in the resource state and reduction of the fidelity of the teleported information. In this work, we investigate the use of error correction and error syndrome information in scheduling teleportation at a quantum network node in the presence of multiple teleportation requests and a finite rate of remote entanglement distribution. Specifically, we focus on the scenario where stored qubits undergo decoherence over time due to imperfect memories. To protect the qubits from the resulting errors, we employ quantum encodings, and the stored qubits undergo repeated error correction, generating error syndromes in each round. These error syndromes can provide additional benefits, as they can be used to calculate qubit-specific error likelihoods, which can then be utilized to make better scheduling decisions. By integrating error correction techniques into the scheduling process, our goal is to minimize errors and decoherence effects, thereby enhancing the fidelity and efficiency of teleportation in a quantum network setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04536v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparimit Chandra, Filip Rozp\k{e}dek, Don Towsley</dc:creator>
    </item>
    <item>
      <title>Discovery of 6G Services and Resources in Edge-Cloud-Continuum</title>
      <link>https://arxiv.org/abs/2407.21751</link>
      <description>arXiv:2407.21751v2 Announce Type: replace 
Abstract: The advent of 6G networks will present a pivotal juncture in the evolution of telecommunications, marked by the proliferation of devices, dynamic service requests, and the integration of edge and cloud computing. In response to these transformative shifts, this paper proposes a service and resource discovery architecture as part of service provisioning for the future 6G edge-cloud-continuum. Through the architecture's orchestration and platform components, users will have access to services efficiently and on time. Blockchain underpins trust in this inherently trustless environment, while semantic networking dynamically extracts context from service requests, fostering efficient communication and service delivery. A key innovation lies in dynamic overlay zoning, which not only optimizes resource allocation but also endows our architecture with scalability, adaptability, and resilience. Notably, our architecture excels at predictive capabilities, harnessing learning algorithms to anticipate user and service instance behavior, thereby enhancing network responsiveness and preserving service continuity. This comprehensive architecture paves the way for unparalleled resource optimization, latency reduction, and seamless service delivery, positioning it as an instrumental pillar in the unfolding 6G landscape. Simulation results show that our architecture provides near-optimal timely responses that significantly improve the network's potential, offering scalable and efficient service and resource discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21751v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MNET.2024.3438096</arxiv:DOI>
      <dc:creator>Mohammad Farhoudi, Masoud Shokrnezhad, Tarik Taleb, Richard Li, JaeSeung Song</dc:creator>
    </item>
    <item>
      <title>Post-Quantum Cryptography (PQC) Network Instrument: Measuring PQC Adoption Rates and Identifying Migration Pathways</title>
      <link>https://arxiv.org/abs/2408.00054</link>
      <description>arXiv:2408.00054v3 Announce Type: replace 
Abstract: The problem of adopting quantum-resistant cryptographic network protocols or post-quantum cryptography (PQC) is critically important to democratizing quantum computing. The problem is urgent because practical quantum computers will break classical encryption in the next few decades. Past encrypted data has already been collected and can be decrypted in the near future. The main challenges of adopting post-quantum cryptography lie in algorithmic complexity and hardware/software/network implementation. The grand question of how existing cyberinfrastructure will support post-quantum cryptography remains unanswered.
  This paper describes: i) the design of a novel Post-Quantum Cryptography (PQC) network instrument placed at the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign and a part of the FABRIC testbed; ii) the latest results on PQC adoption rate across a wide spectrum of network protocols (Secure Shell -- SSH, Transport Layer Security -- TLS, etc.); iii) the current state of PQC implementation in key scientific applications (e.g., OpenSSH or SciTokens); iv) the challenges of being quantum-resistant; and v) discussion of potential novel attacks.
  This is the first large-scale measurement of PQC adoption at national-scale supercomputing centers and FABRIC testbeds. Our results show that only OpenSSH and Google Chrome have successfully implemented PQC and achieved an initial adoption rate of 0.029% (6,044 out of 20,556,816) for OpenSSH connections at NCSA coming from major Internet Service Providers or Autonomous Systems (ASes) such as OARNET, GTT, Google Fiber Webpass (U.S.) and Uppsala Lans Landsting (Sweden), with an overall increasing adoption rate year-over-year for 2023-2024. Our analyses identify pathways to migrate current applications to be quantum-resistant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00054v3</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jakub Sowa, Bach Hoang, Advaith Yeluru, Steven Qie, Anita Nikolich, Ravishankar Iyer, Phuong Cao</dc:creator>
    </item>
    <item>
      <title>FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression</title>
      <link>https://arxiv.org/abs/2403.16677</link>
      <description>arXiv:2403.16677v2 Announce Type: replace-cross 
Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16677v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Digital Twins and Testbeds for Supporting AI Research with Autonomous Vehicle Networks</title>
      <link>https://arxiv.org/abs/2404.00954</link>
      <description>arXiv:2404.00954v2 Announce Type: replace-cross 
Abstract: Digital twins (DTs), which are virtual environments that simulate, predict, and optimize the performance of their physical counterparts, hold great promise in revolutionizing next-generation wireless networks. While DTs have been extensively studied for wireless networks, their use in conjunction with autonomous vehicles featuring programmable mobility remains relatively under-explored. In this paper, we study DTs used as a development environment to design, deploy, and test artificial intelligence (AI) techniques that utilize real-world (RW) observations, e.g. radio key performance indicators, for vehicle trajectory and network optimization decisions in autonomous vehicle networks (AVN). We first compare and contrast the use of simulation, digital twin (software in the loop (SITL)), sandbox (hardware-in-the-loop (HITL)), and physical testbed (PT) environments for their suitability in developing and testing AI algorithms for AVNs. We then review various representative use cases of DTs for AVN scenarios. Finally, we provide an example from the NSF AERPAW platform where a DT is used to develop and test AI-aided solutions for autonomous unmanned aerial vehicles for localizing a signal source based solely on link quality measurements. Our results in the physical testbed show that SITL DTs, when supplemented with data from RW measurements and simulations, can serve as an ideal environment for developing and testing innovative AI solutions for AVNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00954v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>An{\i}l G\"urses, Gautham Reddy, Saad Masrur, \"Ozg\"ur \"Ozdemir, \.Ismail G\"uven\c{c}, Mihail L. Sichitiu, Alphan \c{S}ahin, Ahmed Alkhateeb, Magreth Mushi, Rudra Dutta</dc:creator>
    </item>
    <item>
      <title>Applying Transparent Shaping for Zero Trust Architecture Implementation in AWS: A Case Study</title>
      <link>https://arxiv.org/abs/2405.01412</link>
      <description>arXiv:2405.01412v2 Announce Type: replace-cross 
Abstract: This study introduces a methodology integrating Zero Trust Architecture (ZTA) principles and Transparent Shaping into an AWS-hosted Online File Manager (OFM) application, enhancing security without substantial code modifications. We evaluate our approach with the Mozilla Observatory, highlighting significant security improvements and outlining a promising direction for applying Transparent Shaping and ZTA in cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01412v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjia Wang, Seyed Masoud Sadjadi, Naphtali Rishe, Arpan Mahara</dc:creator>
    </item>
  </channel>
</rss>
