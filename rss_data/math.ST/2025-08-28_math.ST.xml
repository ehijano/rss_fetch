<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 01:32:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simultaneous Detection and Localization of Mean and Covariance Changes in High Dimensions</title>
      <link>https://arxiv.org/abs/2508.19523</link>
      <description>arXiv:2508.19523v1 Announce Type: new 
Abstract: Existing methods for high-dimensional changepoint detection and localization typically focus on changes in either the mean vector or the covariance matrix separately. This separation reduces detection power and localization accuracy when both parameters change simultaneously. We propose a simple yet powerful method that jointly monitors shifts in both the mean and covariance structures. Under mild conditions, the test statistics for detecting these shifts jointly converge in distribution to a bivariate standard normal distribution, revealing their asymptotic independence. This independence enables the combination of the individual p-values using Fisher's method, and the development of an adaptive p-value-based estimator for the changepoint. Theoretical analysis and extensive simulations demonstrate the superior performance of our method in terms of both detection power and localization accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19523v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Cui, Guangming Pan, Guanghui Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Optimal Cox regression under federated differential privacy: coefficients and cumulative hazards</title>
      <link>https://arxiv.org/abs/2508.19640</link>
      <description>arXiv:2508.19640v1 Announce Type: new 
Abstract: We study two foundational problems in distributed survival analysis: estimating Cox regression coefficients and cumulative hazard functions, under federated differential privacy constraints, allowing for heterogeneous per-sever sample sizes and privacy budgets. To quantify the fundamental cost of privacy, we derive minimax lower bounds along with matching (up to poly-logarithmic factors) upper bounds. In particular, to estimate the cumulative hazard function, we design a private tree-based algorithm for nonparametric integral estimation. Our results reveal server-level phase transitions between the private and non-private rates, as well as the reduced estimation accuracy from imposing privacy constraints on distributed subsets of data.
  To address scenarios with partially public information, we also consider a relaxed differential privacy framework and provide a corresponding minimax analysis. To our knowledge, this is the first treatment of partially public data in survival analysis, and it establishes a no-gain in accuracy phenomenon. Finally, we conduct extensive numerical experiments, with an accompanying R package FDPCox, validating our theoretical findings. These experiments also include a fully-interactive algorithm with tighter privacy composition, which demonstrates improved estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19640v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elly K. H. Hung, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Estimating non-linear functionals of trawl processes</title>
      <link>https://arxiv.org/abs/2508.19949</link>
      <description>arXiv:2508.19949v2 Announce Type: cross 
Abstract: Trawl processes is a family of continuous-time, infinitely divisible, stationary processes whose correlation structure is entirely characterized by its so-called trawl function. This paper investigates the problem of estimating non-linear functionals of a trawl function under an in-fill and a long-span sampling scheme. Specifically, building on the work of \cite{SauriVeraart23}, we introduce non-parametric estimators for functionals of the type $\Psi_{t}(g)=\int_{0}^{t}g(a(s))\mathrm{d}s$ and $ \Lambda_t(g)=\int_{t}^{\infty}g(a(s))\mathrm{d}s$ where $a$ represents the trawl function of interest and $g$ a non-linear test function. We show that our estimator for $\Psi_{t}(g)$ is consistent and asymptotically Gaussian regardless of the memory of the process. We further demonstrates that the same phenomenon occurs for the estimation of $\Lambda_t(g)$ as long as $g(x)= \mathrm{O} (\lvert x\rvert^p)$, as $x\to0$, for some $p&gt;3$. Additionally, we illustrate how our results can be used to test the presence of $T$-dependent data that is robust to persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19949v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orimar Sauri</dc:creator>
    </item>
    <item>
      <title>On Gaussian Approximation for M-Estimator</title>
      <link>https://arxiv.org/abs/2012.15678</link>
      <description>arXiv:2012.15678v3 Announce Type: replace 
Abstract: This study develops a non-asymptotic Gaussian approximation theory for distributions of M-estimators, which are defined as maximizers of empirical criterion functions. In existing mathematical statistics literature, numerous studies have focused on approximating the distributions of the M-estimators for statistical inference. In contrast to the existing approaches, which mainly focus on limiting behaviors, this study employs a non-asymptotic approach, establishes abstract Gaussian approximation results for maximizers of empirical criteria, and proposes a Gaussian multiplier bootstrap approximation method. Our developments can be considered as extensions of the seminal works (Chernozhukov, Chetverikov and Kato (2013, 2014, 2015)) on the approximation theory for distributions of suprema of empirical processes toward their maximizers. Through this work, we shed new lights on the statistical theory of M-estimators. Our theory covers not only regular estimators, such as the least absolute deviations, but also some non-regular cases where it is difficult to derive or to approximate numerically the limiting distributions such as non-Donsker classes and cube root estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.15678v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masaaki Imaizumi, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules</title>
      <link>https://arxiv.org/abs/2404.01245</link>
      <description>arXiv:2404.01245v4 Announce Type: replace 
Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01245v4</guid>
      <category>math.ST</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2468</arxiv:DOI>
      <arxiv:journal_reference>Ann. Statist. 53(1): 322-351 (February 2025)</arxiv:journal_reference>
      <dc:creator>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>On the tails of log-concave density estimators</title>
      <link>https://arxiv.org/abs/2409.17910</link>
      <description>arXiv:2409.17910v2 Announce Type: replace 
Abstract: It is shown that the nonparametric maximum likelihood estimator of a univariate log-concave probability density satisfies some consistency properties in the tail regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17910v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Didier B. Ryter, Lutz Duembgen</dc:creator>
    </item>
    <item>
      <title>A projector--rank partition theorem for exact degrees of freedom in experimental design</title>
      <link>https://arxiv.org/abs/2506.01619</link>
      <description>arXiv:2506.01619v3 Announce Type: replace 
Abstract: In many experimental designs\textemdash split-plots, blocked or nested layouts, fractional factorials, and studies with missing or unequal replication\textemdash standard ANOVA procedures no longer tell us exactly how many independent pieces of information each effect truly carries. We provide a general degrees of freedom $(\mathrm{df})$ partition theorem that resolves this ambiguity. For $N$ observations, we show that the total information in the data ({\ie}, $N-1$ $\mathrm{df}$) can be split exactly across experimental effects and randomization strata by projecting the data onto each stratum and counting the $\mathrm{df}$ each effect contributes there. This yields integer $\mathrm{df}$\textemdash not approximations\textemdash for any mix of fixed and random effects, blocking structures, fractionation, or imbalance. This result yields closed-form $\mathrm{df}$ tables for unbalanced split-plot, row-column, lattice, and crossed-nested designs. We introduce practical diagnostics\textemdash the $\mathrm{df}$-retention ratio $\rho$, df deficiency $\delta$, and variance-inflation index $\alpha$\textemdash that measure exactly how many $\mathrm{df}$ an effect retains under blocking or fractionation and the resulting loss of precision, thereby extending Box--Hunter's resolution idea to multi-stratum and incomplete designs. Classical results emerge as corollaries: Cochran's one-stratum identity; Yates's split-plot $\mathrm{df}$; resolution-$R$ identified when an effect retains no $\mathrm{df}$. Empirical studies on split-plot and nested designs, a blocked fractional-factorial design-selection experiment, and timing benchmarks show that our approach delivers calibrated error rates, recovers information to raise power by up to 60\% without additional runs, and is orders of magnitude faster than bootstrap-based $\mathrm{df}$ approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01619v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nagananda K G</dc:creator>
    </item>
    <item>
      <title>Convergence Rates for Realizations of Gaussian Random Variables</title>
      <link>https://arxiv.org/abs/2508.13940</link>
      <description>arXiv:2508.13940v2 Announce Type: replace 
Abstract: This paper investigates the approximation of Gaussian random variables in Banach spaces, focusing on the high-probability bounds for the approximation of Gaussian random variables using finitely many observations. We derive non-asymptotic error bounds for the approximation of a Gaussian process $ X $ by its conditional expectation, given finitely many linear functionals. Specifically, we quantify the difference between the covariance of $ X $ and its finite-dimensional approximation, establishing a direct relationship between the quality of the covariance approximation and the convergence of the process in the Banach space norm. Our approach avoids the reliance on spectral methods or eigenfunction expansions commonly used in Hilbert space settings, and instead uses finite, linear observations. This makes our result particularly suitable for practical applications in nonparametric statistics, machine learning, and Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13940v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Winkle, Ingo Steinwart, Bernard Haasdonk</dc:creator>
    </item>
    <item>
      <title>Reproducing kernel methods for machine learning, PDEs, and statistics with Python</title>
      <link>https://arxiv.org/abs/2402.07084</link>
      <description>arXiv:2402.07084v2 Announce Type: replace-cross 
Abstract: This monograph offers an introduction to a collection of numerical algorithms implemented in the library CodPy (an acronym that stands for the Curse Of Dimensionality in PYthon), which has found widespread applications across various areas, including machine learning, statistics, and computational physics. We develop here a strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport. Initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry. In order to present the general principles and techniques employed in CodPy and its applications, we have structured this monograph into two main parts. First of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only. Next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to (supervised, unsupervised) machine learning, extending to generative methods with a focus on stochastic aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07084v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov</dc:creator>
    </item>
    <item>
      <title>Which Spaces can be Embedded in $L_p$-type Reproducing Kernel Banach Space? A Characterization via Metric Entropy</title>
      <link>https://arxiv.org/abs/2410.11116</link>
      <description>arXiv:2410.11116v3 Announce Type: replace-cross 
Abstract: In this paper, we establish a novel connection between the metric entropy growth and the embeddability of function spaces into reproducing kernel Hilbert/Banach spaces. Metric entropy characterizes the information complexity of function spaces and has implications for their approximability and learnability. Classical results show that embedding a function space into a reproducing kernel Hilbert space (RKHS) implies a bound on its metric entropy growth. Surprisingly, we prove a \textbf{converse}: a bound on the metric entropy growth of a function space allows its embedding to a $L_p-$type Reproducing Kernel Banach Space (RKBS). This shows that the ${L}_p-$type RKBS provides a broad modeling framework for learnable function classes with controlled metric entropies. Our results shed new light on the power and limitations of kernel methods for learning complex function spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11116v3</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiping Lu, Daozhe Lin, Qiang Du</dc:creator>
    </item>
    <item>
      <title>On MCMC mixing for predictive inference under unidentified transformation models</title>
      <link>https://arxiv.org/abs/2411.01382</link>
      <description>arXiv:2411.01382v4 Announce Type: replace-cross 
Abstract: Reliable Bayesian predictive inference has long been an open problem under unidentified transformation models, since the Markov Chain Monte Carlo (MCMC) chains of posterior predictive distribution (PPD) values are generally poorly mixed. We address the poorly mixed PPD value chains under unidentified transformation models through an adaptive scheme for prior adjustment. Specifically, we originate a conception of sufficient informativeness, which explicitly quantifies the information level provided by nonparametric priors, and assesses MCMC mixing by comparison with the within-chain MCMC variance. We formulate the prior information level by a set of hyperparameters induced from the nonparametric prior elicitation with an analytic expression, which is guaranteed by asymptotic theory for the posterior variance under unidentified transformation models. The analytic prior information level consequently drives a hyperparameter tuning procedure to achieve MCMC mixing. The proposed method is general enough to cover various data domains through a multiplicative error working model. Comprehensive simulations and real-world data analysis demonstrate that our method successfully achieves MCMC mixing and outperforms state-of-the-art competitors in predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01382v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Jin Yang, Junshan Shen, Zhaohai Li, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>Robust Detection of Watermarks for Large Language Models Under Human Edits</title>
      <link>https://arxiv.org/abs/2411.13868</link>
      <description>arXiv:2411.13868v3 Announce Type: replace-cross 
Abstract: Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13868v3</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Statistical learning does not always entail knowledge</title>
      <link>https://arxiv.org/abs/2501.01963</link>
      <description>arXiv:2501.01963v2 Announce Type: replace-cross 
Abstract: In this paper, we study learning and knowledge acquisition (LKA) of an agent about a proposition that is either true or false. We use a Bayesian approach, where the agent receives data to update his beliefs about the proposition according to a posterior distribution. The LKA is formulated in terms of active information, with data representing external or exogenous information that modifies the agent's beliefs. It is assumed that data provide details about a number of features that are relevant to the proposition. We show that this leads to a Gibbs distribution posterior, which is in maximum entropy relative to the prior, conditioned on the side constraints that the data provide in terms of the features. We demonstrate that full learning is sometimes not possible and full knowledge acquisition is never possible when the number of extracted features is too small. We also distinguish between primary learning (receiving data about features of relevance for the proposition) and secondary learning (receiving data about the learning of another agent). We argue that this type of secondary learning does not represent true knowledge acquisition. Our results have implications for statistical learning algorithms, and we claim that such algorithms do not always generate true knowledge. The theory is illustrated with several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01963v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Andr\'es D\'iaz-Pach\'on, H. Renata Gallegos, Ola H\"ossjer, J. Sunil Rao</dc:creator>
    </item>
  </channel>
</rss>
