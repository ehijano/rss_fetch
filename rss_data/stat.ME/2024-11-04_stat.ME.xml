<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Nov 2024 05:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sharp Bounds on the Variance of General Regression Adjustment in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2411.00191</link>
      <description>arXiv:2411.00191v1 Announce Type: new 
Abstract: Building on statistical foundations laid by Neyman [1923] a century ago, a growing literature focuses on problems of causal inference that arise in the context of randomized experiments where the target of inference is the average treatment effect in a finite population and random assignment determines which subjects are allocated to one of the experimental conditions. In this framework, variances of average treatment effect estimators remain unidentified because they depend on the covariance between treated and untreated potential outcomes, which are never jointly observed. Aronow et al. [2014] provide an estimator for the variance of the difference-in-means estimator that is asymptotically sharp. In practice, researchers often use some form of covariate adjustment, such as linear regression when estimating the average treatment effect. Here we extend the Aronow et al. [2014] result, providing asymptotically sharp variance bounds for general regression adjustment. We apply these results to linear regression adjustment and show benefits both in a simulation as well as an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00191v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Bayesian Smoothing and Feature Selection Using variational Automatic Relevance Determination</title>
      <link>https://arxiv.org/abs/2411.00256</link>
      <description>arXiv:2411.00256v1 Announce Type: new 
Abstract: This study introduces Variational Automatic Relevance Determination (VARD), a novel approach tailored for fitting sparse additive regression models in high-dimensional settings. VARD distinguishes itself by its ability to independently assess the smoothness of each feature while enabling precise determination of whether a feature's contribution to the response is zero, linear, or nonlinear. Further, an efficient coordinate descent algorithm is introduced to implement VARD. Empirical evaluations on simulated and real-world data underscore VARD's superiority over alternative variable selection methods for additive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00256v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihe Liu, Diptarka Saha, Feng Liang</dc:creator>
    </item>
    <item>
      <title>Estimating Broad Sense Heritability via Kernel Ridge Regression</title>
      <link>https://arxiv.org/abs/2411.00346</link>
      <description>arXiv:2411.00346v1 Announce Type: new 
Abstract: The broad sense genetic heritability, which quantifies the total proportion of phenotypic variation in a population due to genetic factors, is crucial for understanding trait inheritance. While many existing methods focus on estimating narrow sense heritability, which accounts only for additive genetic variation, this paper introduces a kernel ridge regression approach to estimate broad-sense heritability. We provide both upper and lower bounds for the estimator. The effectiveness of the proposed method was evaluated through extensive simulations of both synthetic data and real data from the 1000 Genomes Project. Additionally, the estimator was applied to data from the Alzheimer's Disease Neuroimaging Initiative to demonstrate its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00346v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Olivia Bley, Elizabeth Lei, Andy Zhou, Xiaoxi Shen</dc:creator>
    </item>
    <item>
      <title>Unbiased mixed variables distance</title>
      <link>https://arxiv.org/abs/2411.00429</link>
      <description>arXiv:2411.00429v1 Announce Type: new 
Abstract: Defining a distance in a mixed setting requires the quantification of observed differences of variables of different types and of variables that are measured on different scales. There exist several proposals for mixed variable distances, however, such distances tend to be biased towards specific variable types and measurement units. That is, the variable types and scales influence the contribution of individual variables to the overall distance. In this paper, we define unbiased mixed variable distances for which the contributions of individual variables to the overall distance are not influenced by measurement types or scales. We define the relevant concepts to quantify such biases and we provide a general formulation that can be used to construct unbiased mixed variable distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00429v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michel van de Velden, Alfonso Iodice D'Enza, Angelos Markos, Carlo Cavicchia</dc:creator>
    </item>
    <item>
      <title>Dirichlet process mixtures of block $g$ priors for model selection and prediction in linear models</title>
      <link>https://arxiv.org/abs/2411.00471</link>
      <description>arXiv:2411.00471v1 Announce Type: new 
Abstract: This paper introduces Dirichlet process mixtures of block $g$ priors for model selection and prediction in linear models. These priors are extensions of traditional mixtures of $g$ priors that allow for differential shrinkage for various (data-selected) blocks of parameters while fully accounting for the predictors' correlation structure, providing a bridge between the literatures on model selection and continuous shrinkage priors. We show that Dirichlet process mixtures of block $g$ priors are consistent in various senses and, in particular, that they avoid the conditional Lindley ``paradox'' highlighted by Som et al.(2016). Further, we develop a Markov chain Monte Carlo algorithm for posterior inference that requires only minimal ad-hoc tuning. Finally, we investigate the empirical performance of the prior in various real and simulated datasets. In the presence of a small number of very large effects, Dirichlet process mixtures of block $g$ priors lead to higher power for detecting smaller but significant effects without only a minimal increase in the number of false discoveries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00471v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anupreet Porwal, Abel Rodriguez</dc:creator>
    </item>
    <item>
      <title>Calibrated quantile prediction for Growth-at-Risk</title>
      <link>https://arxiv.org/abs/2411.00520</link>
      <description>arXiv:2411.00520v1 Announce Type: new 
Abstract: Accurate computation of robust estimates for extremal quantiles of empirical distributions is an essential task for a wide range of applicative fields, including economic policymaking and the financial industry. Such estimates are particularly critical in calculating risk measures, such as Growth-at-Risk (GaR). % and Value-at-Risk (VaR). This work proposes a conformal framework to estimate calibrated quantiles, and presents an extensive simulation study and a real-world analysis of GaR to examine its benefits with respect to the state of the art. Our findings show that CP methods consistently improve the calibration and robustness of quantile estimates at all levels. The calibration gains are appreciated especially at extremal quantiles, which are critical for risk assessment and where traditional methods tend to fall short. In addition, we introduce a novel property that guarantees coverage under the exchangeability assumption, providing a valuable tool for managing risks by quantifying and controlling the likelihood of future extreme observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00520v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietro Bogani, Matteo Fontana, Luca Neri, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>What can we learn from marketing skills as a bipartite network from accredited programs?</title>
      <link>https://arxiv.org/abs/2411.00644</link>
      <description>arXiv:2411.00644v1 Announce Type: new 
Abstract: The relationship between professional skills and higher education programs is modeled as a non-directed bipartite network with binary entries representing the links between 28 skills (as captured by the occupational information network, O*NET) and 258 graduate program summaries (as captured by commercial brochures of graduate programs in marketing with accreditation standards of the Association to Advance Collegiate Schools of Business). While descriptive analysis for skills suggests a qualitative lack of alignment between the job demands captured by O*NET, inferential analyses based on exponential random graph model estimates show that skills' popularity and homophily coexist with a systematic yet weak alignment to job demands for marketing managers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00644v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maria del Pilar Garcia-Chitiva, Silvana Dakduk, Juan C. Correa</dc:creator>
    </item>
    <item>
      <title>Change-point detection in functional time series: Applications to age-specific mortality and fertility</title>
      <link>https://arxiv.org/abs/2411.00534</link>
      <description>arXiv:2411.00534v1 Announce Type: cross 
Abstract: We consider determining change points in a time series of age-specific mortality and fertility curves observed over time. We propose two detection methods for identifying these change points. The first method uses a functional cumulative sum statistic to pinpoint the change point. The second method computes a univariate time series of integrated squared forecast errors after fitting a functional time-series model before applying a change-point detection method to the errors to determine the change point. Using Australian age-specific fertility and mortality data, we apply these methods to locate the change points and identify the optimal training period to achieve improved forecast accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00534v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of Hawkes processes with RKHSs</title>
      <link>https://arxiv.org/abs/2411.00621</link>
      <description>arXiv:2411.00621v1 Announce Type: cross 
Abstract: This paper addresses nonparametric estimation of nonlinear multivariate Hawkes processes, where the interaction functions are assumed to lie in a reproducing kernel Hilbert space (RKHS). Motivated by applications in neuroscience, the model allows complex interaction functions, in order to express exciting and inhibiting effects, but also a combination of both (which is particularly interesting to model the refractory period of neurons), and considers in return that conditional intensities are rectified by the ReLU function. The latter feature incurs several methodological challenges, for which workarounds are proposed in this paper. In particular, it is shown that a representer theorem can be obtained for approximated versions of the log-likelihood and the least-squares criteria. Based on it, we propose an estimation method, that relies on two simple approximations (of the ReLU function and of the integral operator). We provide an approximation bound, justifying the negligible statistical effect of these approximations. Numerical results on synthetic data confirm this fact as well as the good asymptotic behavior of the proposed estimator. It also shows that our method achieves a better performance compared to related nonparametric estimation techniques and suits neuronal applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00621v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bonnet, Maxime Sangnier</dc:creator>
    </item>
    <item>
      <title>Structure and Sensitivity in Differential Privacy: Comparing K-Norm Mechanisms</title>
      <link>https://arxiv.org/abs/1801.09236</link>
      <description>arXiv:1801.09236v4 Announce Type: replace 
Abstract: Differential privacy (DP), provides a framework for provable privacy protection against arbitrary adversaries, while allowing the release of summary statistics and synthetic data. We address the problem of releasing a noisy real-valued statistic vector $T$, a function of sensitive data under DP, via the class of $K$-norm mechanisms with the goal of minimizing the noise added to achieve privacy. First, we introduce the sensitivity space of $T$, which extends the concepts of sensitivity polytope and sensitivity hull to the setting of arbitrary statistics $T$. We then propose a framework consisting of three methods for comparing the $K$-norm mechanisms: 1) a multivariate extension of stochastic dominance, 2) the entropy of the mechanism, and 3) the conditional variance given a direction, to identify the optimal $K$-norm mechanism. In all of these criteria, the optimal $K$-norm mechanism is generated by the convex hull of the sensitivity space. Using our methodology, we extend the objective perturbation and functional mechanisms and apply these tools to logistic and linear regression, allowing for private releases of statistical results. Via simulations and an application to a housing price dataset, we demonstrate that our proposed methodology offers a substantial improvement in utility for the same level of risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:1801.09236v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Aleksandra Slavkovic</dc:creator>
    </item>
    <item>
      <title>Evaluating Forecasts with scoringutils in R</title>
      <link>https://arxiv.org/abs/2205.07090</link>
      <description>arXiv:2205.07090v2 Announce Type: replace 
Abstract: Evaluating forecasts is essential to understand and improve forecasting and make forecasts useful to decision makers. A variety of R packages provide a broad variety of scoring rules, visualisations and diagnostic tools. One particular challenge, which scoringutils aims to address, is handling the complexity of evaluating and comparing forecasts from several forecasters across multiple dimensions such as time, space, and different types of targets. scoringutils extends the existing landscape by offering a convenient and flexible data.table-based framework for evaluating and comparing probabilistic forecasts (forecasts represented by a full predictive distribution). Notably, scoringutils is the first package to offer extensive support for probabilistic forecasts in the form of predictive quantiles, a format that is currently used by several infectious disease Forecast Hubs. The package is easily extendable, meaning that users can supply their own scoring rules or extend existing classes to handle new types of forecasts. scoringutils provides broad functionality to check the data and diagnose issues, to visualise forecasts and missing data, to transform data before scoring, to handle missing forecasts, to aggregate scores, and to visualise the results of the evaluation. The paper presents the package and its core functionality and illustrates common workflows using example data of forecasts for COVID-19 cases and deaths submitted to the European COVID-19 Forecast Hub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.07090v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikos I. Bosse, Hugo Gruson, Anne Cori, Edwin van Leeuwen, Sebastian Funk, Sam Abbott</dc:creator>
    </item>
    <item>
      <title>The Target Study: A Conceptual Model and Framework for Measuring Disparity</title>
      <link>https://arxiv.org/abs/2207.00530</link>
      <description>arXiv:2207.00530v3 Announce Type: replace 
Abstract: We present a conceptual model to measure disparity--the target study--where social groups may be similarly situated (i.e., balanced) on allowable covariates. Our model, based on a sampling design, does not intervene to assign social group membership or alter allowable covariates. To address non-random sample selection, we extend our model to generalize or transport disparity or to assess disparity after an intervention on eligibility-related variables that eliminates forms of collider-stratification. To avoid bias from differential timing of enrollment, we aggregate time-specific study results by balancing calendar time of enrollment across social groups. To provide a framework for emulating our model, we discuss study designs, data structures, and G-computation and weighting estimators. We compare our sampling-based model to prominent decomposition-based models used in healthcare and algorithmic fairness. We provide R code for all estimators and apply our methods to measure health system disparities in hypertension control using electronic medical records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.00530v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John W. Jackson, Yea-Jen Hsu, Raquel C. Greer, Romsai T. Boonyasai, Chanelle J. Howe</dc:creator>
    </item>
    <item>
      <title>Pochhammer Priors for Sparse Count Models</title>
      <link>https://arxiv.org/abs/2402.09583</link>
      <description>arXiv:2402.09583v3 Announce Type: replace 
Abstract: Bayesian hierarchical models are commonly employed for inference in count datasets, as they account for multiple levels of variation by incorporating prior distributions for parameters at different levels. Examples include Beta-Binomial, Negative-Binomial (NB), Dirichlet-Multinomial (DM) distributions. In this paper, we address two crucial challenges that arise in various Bayesian count models: inference for the concentration parameter in the ratio of Gamma functions and the inability of these models to effectively handle excessive zeros and small nonzero counts. We propose a novel class of prior distributions that facilitates conjugate updating of the concentration parameter in Gamma ratios, enabling full Bayesian inference for the aforementioned count distributions. We use DM models as our running examples. Our methodology leverages fast residue computation and admits closed-form posterior moments. Additionally, we recommend a default horseshoe type prior which has a heavy tail and substantial mass around zero. It admits continuous shrinkage, making the posterior highly adaptable to sparsity or quasi-sparsity in the data. Furthermore, we offer insights and potential generalizations to other count models facing the two challenges. We demonstrate the usefulness of our approach on both simulated examples and on real-world applications. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09583v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuexi Wang, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal count autoregression</title>
      <link>https://arxiv.org/abs/2404.02982</link>
      <description>arXiv:2404.02982v4 Announce Type: replace 
Abstract: We study the problem of modeling and inference for spatio-temporal count processes. Our approach uses parsimonious parameterisations of multivariate autoregressive count time series models, including possible regression on covariates. We control the number of parameters by specifying spatial neighbourhood structures for possibly huge matrices that take into account spatio-temporal dependencies. This work is motivated by real data applications which call for suitable models. Extensive simulation studies show that our approach yields reliable estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02982v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen Maletz, Konstantinos Fokianos, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Asymptotically-exact selective inference for quantile regression</title>
      <link>https://arxiv.org/abs/2404.03059</link>
      <description>arXiv:2404.03059v2 Announce Type: replace 
Abstract: In modern data analysis, it is common to select a model before performing statistical inference. Selective inference tools make adjustments for the model selection process in order to ensure reliable inference post selection. In this paper, we introduce an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions. Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and yields asymptotically-exact selective inference without making strict distributional assumptions about the response variable. At the core of our pivot is the use of external randomization variables, which allows us to utilize all available samples for both selection and inference without partitioning the data into independent subsets or discarding any samples at any step. From simulation studies, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings. We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03059v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumeng Wang, Snigdha Panigrahi, Xuming He</dc:creator>
    </item>
    <item>
      <title>Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path</title>
      <link>https://arxiv.org/abs/2406.00493</link>
      <description>arXiv:2406.00493v2 Announce Type: replace 
Abstract: We study case influence in the Lasso regression using Cook's distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\ell_1$ penalty, and neither does Cook's distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook's distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook's distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00493v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenbang Jiao, Yoonkyung Lee</dc:creator>
    </item>
    <item>
      <title>Modelling multivariate spatio-temporal data with identifiable variational autoencoders</title>
      <link>https://arxiv.org/abs/2409.04162</link>
      <description>arXiv:2409.04162v2 Announce Type: replace 
Abstract: Modelling multivariate spatio-temporal data with complex dependency structures is a challenging task but can be simplified by assuming that the original variables are generated from independent latent components. If these components are found, they can be modelled univariately. Blind source separation aims to recover the latent components by estimating the unmixing transformation based on the observed data only. The current methods for spatio-temporal blind source separation are restricted to linear unmixing, and nonlinear variants have not been implemented. In this paper, we extend identifiable variational autoencoder to the nonlinear nonstationary spatio-temporal blind source separation setting and demonstrate its performance using comprehensive simulation studies. Additionally, we introduce two alternative methods for the latent dimension estimation, which is a crucial task in order to obtain the correct latent representation. Finally, we illustrate the proposed methods using a meteorological application, where we estimate the latent dimension and the latent components, interpret the components, and show how nonstationarity can be accounted and prediction accuracy can be improved by using the proposed nonlinear blind source separation method as a preprocessing method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04162v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neunet.2024.106774</arxiv:DOI>
      <arxiv:journal_reference>Neural Networks 2025, 181, 106774</arxiv:journal_reference>
      <dc:creator>Mika Sipil\"a, Claudia Cappello, Sandra De Iaco, Klaus Nordhausen, Sara Taskinen</dc:creator>
    </item>
    <item>
      <title>Bounds and Sensitivity Analysis of the Causal Effect Under Outcome-Independent MNAR Confounding</title>
      <link>https://arxiv.org/abs/2410.06726</link>
      <description>arXiv:2410.06726v2 Announce Type: replace 
Abstract: We report assumption-free bounds for any contrast between the probabilities of the potential outcome under exposure and non-exposure when the confounders are missing not at random. We assume that the missingness mechanism is outcome-independent. We also report a sensitivity analysis method to complement our bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06726v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose M. Pe\~na</dc:creator>
    </item>
    <item>
      <title>Assessing mediation in cross-sectional stepped wedge cluster randomized trials</title>
      <link>https://arxiv.org/abs/2410.15596</link>
      <description>arXiv:2410.15596v2 Announce Type: replace 
Abstract: Mediation analysis has been comprehensively studied for independent data but relatively little work has been done for correlated data, especially for the increasingly adopted stepped wedge cluster randomized trials (SW-CRTs). Motivated by challenges in underlying the effect mechanisms in pragmatic and implementation science clinical trials, we develop new methods for mediation analysis in SW-CRTs. Specifically, based on a linear and generalized linear mixed models, we demonstrate how to estimate the natural indirect effect and mediation proportion in typical SW-CRTs with four data types, including both continuous and binary mediators and outcomes. Furthermore, to address the emerging challenges in exposure-time treatment effect heterogeneity, we derive the mediation expressions in SW-CRTs when the total effect varies as a function of the exposure time. The cluster jackknife approach is considered for inference across all data types and treatment effect structures. We conduct extensive simulations to evaluate the finite-sample performances of proposed mediation estimators and demonstrate the proposed approach in a real data example. A user-friendly R package mediateSWCRT has been developed to facilitate the practical implementation of the estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15596v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiqiang Cao, Fan Li</dc:creator>
    </item>
    <item>
      <title>Hierarchical Regularizers for Reverse Unrestricted Mixed Data Sampling Regressions</title>
      <link>https://arxiv.org/abs/2301.10592</link>
      <description>arXiv:2301.10592v2 Announce Type: replace-cross 
Abstract: Reverse Unrestricted MIxed DAta Sampling (RU-MIDAS) regressions are used to model high-frequency responses by means of low-frequency variables. However, due to the periodic structure of RU-MIDAS regressions, the dimensionality grows quickly if the frequency mismatch between the high- and low-frequency variables is large. Additionally the number of high-frequency observations available for estimation decreases. We propose to counteract this reduction in sample size by pooling the high-frequency coefficients and further reduce the dimensionality through a sparsity-inducing convex regularizer that accounts for the temporal ordering among the different lags. To this end, the regularizer prioritizes the inclusion of lagged coefficients according to the recency of the information they contain. We demonstrate the proposed method on two empirical applications, one on realized volatility forecasting with macroeconomic data and another on demand forecasting for a bicycle-sharing system with ridership data on other transportation types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.10592v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Hecq, Marie Ternes, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand</title>
      <link>https://arxiv.org/abs/2402.07419</link>
      <description>arXiv:2402.07419v2 Announce Type: replace-cross 
Abstract: Causal inference from observational data plays critical role in many applications in trustworthy machine learning. While sound and complete algorithms exist to compute causal effects, many of them assume access to conditional likelihoods, which is difficult to estimate for high-dimensional (particularly image) data. Researchers have alleviated this issue by simulating causal relations with neural models. However, when we have high-dimensional variables in the causal graph along with some unobserved confounders, no existing work can effectively sample from the un/conditional interventional distributions. In this work, we show how to sample from any identifiable interventional distribution given an arbitrary causal graph through a sequence of push-forward computations of conditional generative models, such as diffusion models. Our proposed algorithm follows the recursive steps of the existing likelihood-based identification algorithms to train a set of feed-forward models, and connect them in a specific way to sample from the desired distribution. We conduct experiments on a Colored MNIST dataset having both the treatment ($X$) and the target variables ($Y$) as images and sample from $P(y|do(x))$. Our algorithm also enables us to conduct a causal analysis to evaluate spurious correlations among input features of generative models pre-trained on the CelebA dataset. Finally, we generate high-dimensional interventional samples from the MIMIC-CXR dataset involving text and image variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07419v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Musfiqur Rahman, Matt Jordan, Murat Kocaoglu</dc:creator>
    </item>
  </channel>
</rss>
