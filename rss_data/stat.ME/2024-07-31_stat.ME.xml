<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2024 01:38:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 31 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>High dimensional inference for extreme value indices</title>
      <link>https://arxiv.org/abs/2407.20491</link>
      <description>arXiv:2407.20491v1 Announce Type: new 
Abstract: When applying multivariate extreme values statistics to analyze tail risk in compound events defined by a multivariate random vector, one often assumes that all dimensions share the same extreme value index. While such an assumption can be tested using a Wald-type test, the performance of such a test deteriorates as the dimensionality increases. This paper introduces a novel test for testing extreme value indices in a high dimensional setting. We show the asymptotic behavior of the test statistic and conduct simulation studies to evaluate its finite sample performance. The proposed test significantly outperforms existing methods in high dimensional settings. We apply this test to examine two datasets previously assumed to have identical extreme value indices across all dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20491v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liujun Chen, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification under Noisy Constraints, with Applications to Raking</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v1 Announce Type: new 
Abstract: We consider statistical inference problems under uncertain equality constraints, and provide asymptotically valid uncertainty estimates for inferred parameters. The proposed approach leverages the implicit function theorem and primal-dual optimality conditions for a particular problem class. The motivating application is multi-dimensional raking, where observations are adjusted to match marginals; for example, adjusting estimated deaths across race, county, and cause in order to match state all-race all-cause totals. We review raking from a convex optimization perspective, providing explicit primal-dual formulations, algorithms, and optimality conditions for a wide array of raking applications, which are then leveraged to obtain the uncertainty estimates. Empirical results show that the approach obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and of marginal draws through the entire raking process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Laplace approximation for Bayesian variable selection via Le Cam's one-step procedure</title>
      <link>https://arxiv.org/abs/2407.20580</link>
      <description>arXiv:2407.20580v1 Announce Type: new 
Abstract: Variable selection in high-dimensional spaces is a pervasive challenge in contemporary scientific exploration and decision-making. However, existing approaches that are known to enjoy strong statistical guarantees often struggle to cope with the computational demands arising from the high dimensionality. To address this issue, we propose a novel Laplace approximation method based on Le Cam's one-step procedure (\textsf{OLAP}), designed to effectively tackles the computational burden. Under some classical high-dimensional assumptions we show that \textsf{OLAP} is a statistically consistent variable selection procedure. Furthermore, we show that the approach produces a posterior distribution that can be explored in polynomial time using a simple Gibbs sampling algorithm. Toward that polynomial complexity result, we also made some general, noteworthy contributions to the mixing time analysis of Markov chains. We illustrate the method using logistic and Poisson regression models applied to simulated and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20580v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianrui Hou, Liwei Wang, Yves Atchad\'e</dc:creator>
    </item>
    <item>
      <title>An online generalization of the e-BH procedure</title>
      <link>https://arxiv.org/abs/2407.20683</link>
      <description>arXiv:2407.20683v1 Announce Type: new 
Abstract: In online multiple testing the hypotheses arrive one-by-one over time and at each time it must be decided on the current hypothesis solely based on the data and hypotheses observed so far. In this paper we relax this setup by allowing initially accepted hypotheses to be rejected due to information gathered at later steps. We propose online e-BH, an online ARC (online with acceptance-to-rejection changes) version of the e-BH procedure. Online e-BH is the first nontrivial online procedure which provably controls the FDR at data-adaptive stopping times and under arbitrary dependence between the test statistics. Online e-BH uniformly improves e-LOND, the existing method for e-value based online FDR control. In addition, we introduce new boosting techniques for online e-BH to increase the power in case of locally dependent e-values. Furthermore, based on the same proof technique as used for online e-BH, we show that all existing online procedures with valid FDR control under arbitrary dependence also control the FDR at data-adaptive stopping times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20683v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Local Modal Outer-Product-Gradient Estimator for Dimension Reduction</title>
      <link>https://arxiv.org/abs/2407.20738</link>
      <description>arXiv:2407.20738v1 Announce Type: new 
Abstract: Sufficient dimension reduction (SDR) is a valuable approach for handling high-dimensional data. Outer Product Gradient (OPG) is an popular approach. However, because of focusing the mean regression function, OPG may ignore some directions of central subspace (CS) when the distribution of errors is symmetric about zero. The mode of a distribution can provide an important summary of data. A Local Modal OPG (LMOPG) and its algorithm through mode regression are proposed to estimate the basis of CS with skew errors distribution. The estimator shows the consistent and asymptotic normal distribution under some mild conditions. Monte Carlo simulation is used to evaluate the performance and demonstrate the efficiency and robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20738v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Li, Chong Ding, Wei Gao</dc:creator>
    </item>
    <item>
      <title>Linear mixed modelling of federated data when only the mean, covariance, and sample size are available</title>
      <link>https://arxiv.org/abs/2407.20796</link>
      <description>arXiv:2407.20796v1 Announce Type: new 
Abstract: In medical research, individual-level patient data provide invaluable information, but the patients' right to confidentiality remains of utmost priority. This poses a huge challenge when estimating statistical models such as linear mixed models, which is an extension of linear regression models that can account for potential heterogeneity whenever data come from different data providers. Federated learning algorithms tackle this hurdle by estimating parameters without retrieving individual-level data. Instead, iterative communication of parameter estimate updates between the data providers and analyst is required. In this paper, we propose an alternative framework to federated learning algorithms for fitting linear mixed models. Specifically, our approach only requires the mean, covariance, and sample size of multiple covariates from different data providers once. Using the principle of statistical sufficiency within the framework of likelihood as theoretical support, this proposed framework achieves estimates identical to those derived from actual individual-level data. We demonstrate this approach through real data on 15 068 patient records from 70 clinics at the Children's Hospital of Pennsylvania (CHOP). Assuming that each clinic only shares summary statistics once, we model the COVID-19 PCR test cycle threshold as a function of patient information. Simplicity, communication efficiency, and wider scope of implementation in any statistical software distinguish our approach from existing strategies in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20796v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Analiz April Limpoco, Christel Faes, Niel Hens</dc:creator>
    </item>
    <item>
      <title>Design and inference for multi-arm clinical trials with informational borrowing: the interacting urns design</title>
      <link>https://arxiv.org/abs/2407.20819</link>
      <description>arXiv:2407.20819v1 Announce Type: new 
Abstract: This paper deals with a new design methodology for stratified comparative experiments based on interacting reinforced urn systems. The key idea is to model the interaction between urns for borrowing information across strata and to use it in the design phase in order to i) enhance the information exchange at the beginning of the study, when only few subjects have been enrolled and the stratum-specific information on treatments' efficacy could be scarce, ii) let the information sharing adaptively evolves via a reinforcement mechanism based on the observed outcomes, for skewing at each step the allocations towards the stratum-specific most promising treatment and iii) make the contribution of the strata with different treatment efficacy vanishing as the stratum information grows. In particular, we introduce the Interacting Urns Design, namely a new Covariate-Adjusted Response-Adaptive procedure, that randomizes the treatment allocations according to the evolution of the urn system. The theoretical properties of this proposal are described and the corresponding asymptotic inference is provided. Moreover, by a functional central limit theorem, we obtain the asymptotic joint distribution of the Wald-type sequential test statistics, which allows to sequentially monitor the suggested design in the clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20819v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Alessandro Baldi Antognini, Irene Crimaldi, Rosamarie Frieri, Andrea Ghiglietti</dc:creator>
    </item>
    <item>
      <title>ROC curve analysis for functional markers</title>
      <link>https://arxiv.org/abs/2407.20929</link>
      <description>arXiv:2407.20929v1 Announce Type: new 
Abstract: Functional markers become a more frequent tool in medical diagnosis. In this paper, we aim to define an index allowing to discriminate between populations when the observations are functional data belonging to a Hilbert space. We discuss some of the problems arising when estimating optimal directions defined to maximize the area under the curve of a projection index and we construct the corresponding ROC curve. We also go one step forward and consider the case of possibly different covariance operators, for which we recommend a quadratic discrimination rule. Consistency results are derived for both linear and quadratic indexes, under mild conditions. The results of our numerical experiments allow to see the advantages of the quadratic rule when the populations have different covariance operators. We also illustrate the considered methods on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20929v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana M. Bianco, Graciela Boente, Juan Carlos Pardo-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Generalized Multivariate Functional Additive Mixed Models for Location, Scale, and Shape</title>
      <link>https://arxiv.org/abs/2407.20995</link>
      <description>arXiv:2407.20995v1 Announce Type: new 
Abstract: We propose a flexible regression framework to model the conditional distribution of multilevel generalized multivariate functional data of potentially mixed type, e.g. binary and continuous data. We make pointwise parametric distributional assumptions for each dimension of the multivariate functional data and model each distributional parameter as an additive function of covariates. The dependency between the different outcomes and, for multilevel functional data, also between different functions within a level is modelled by shared latent multivariate Gaussian processes. For a parsimonious representation of the latent processes, (generalized) multivariate functional principal components are estimated from the data and used as an empirical basis for these latent processes in the regression framework. Our modular two-step approach is very general and can easily incorporate new developments in the estimation of functional principal components for all types of (generalized) functional data. Flexible additive covariate effects for scalar or even functional covariates are available and are estimated in a Bayesian framework. We provide an easy-to-use implementation in the accompanying R package 'gmfamm' on CRAN and conduct a simulation study to confirm the validity of our regression framework and estimation strategy. The proposed multivariate functional model is applied to four dimensional traffic data in Berlin, which consists of the hourly numbers and mean speed of cars and trucks at different locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20995v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Volkmann, Nikolaus Umlauf, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>Warped multifidelity Gaussian processes for data fusion of skewed environmental data</title>
      <link>https://arxiv.org/abs/2407.20295</link>
      <description>arXiv:2407.20295v1 Announce Type: cross 
Abstract: Understanding the dynamics of climate variables is paramount for numerous sectors, like energy and environmental monitoring. This study focuses on the critical need for a precise mapping of environmental variables for national or regional monitoring networks, a task notably challenging when dealing with skewed data. To address this issue, we propose a novel data fusion approach, the \textit{warped multifidelity Gaussian process} (WMFGP). The method performs prediction using multiple time-series, accommodating varying reliability and resolutions and effectively handling skewness. In an extended simulation experiment the benefits and the limitations of the methods are explored, while as a case study, we focused on the wind speed monitored by the network of ARPA Lombardia, one of the regional environmental agencies operting in Italy. ARPA grapples with data gaps, and due to the connection between wind speed and air quality, it struggles with an effective air quality management. We illustrate the efficacy of our approach in filling the wind speed data gaps through two extensive simulation experiments. The case study provides more informative wind speed predictions crucial for predicting air pollutant concentrations, enhancing network maintenance, and advancing understanding of relevant meteorological and climatic phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20295v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Colombo, Claire Miller, Xiaochen Yang, Ruth O'Donnell, Paolo Maranzano</dc:creator>
    </item>
    <item>
      <title>Designing Time-Series Models With Hypernetworks &amp; Adversarial Portfolios</title>
      <link>https://arxiv.org/abs/2407.20352</link>
      <description>arXiv:2407.20352v1 Announce Type: cross 
Abstract: This article describes the methods that achieved 4th and 6th place in the forecasting and investment challenges, respectively, of the M6 competition, ultimately securing the 1st place in the overall duathlon ranking. In the forecasting challenge, we tested a novel meta-learning model that utilizes hypernetworks to design a parametric model tailored to a specific family of forecasting tasks. This approach allowed us to leverage similarities observed across individual forecasting tasks while also acknowledging potential heterogeneity in their data generating processes. The model's training can be directly performed with backpropagation, eliminating the need for reliance on higher-order derivatives and is equivalent to a simultaneous search over the space of parametric functions and their optimal parameter values. The proposed model's capabilities extend beyond M6, demonstrating superiority over state-of-the-art meta-learning methods in the sinusoidal regression task and outperforming conventional parametric models on time-series from the M4 competition. In the investment challenge, we adjusted portfolio weights to induce greater or smaller correlation between our submission and that of other participants, depending on the current ranking, aiming to maximize the probability of achieving a good rank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20352v1</guid>
      <category>cs.LG</category>
      <category>q-fin.PM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Stan\v{e}k</dc:creator>
    </item>
    <item>
      <title>Leveraging Natural Language and Item Response Theory Models for ESG Scoring</title>
      <link>https://arxiv.org/abs/2407.20377</link>
      <description>arXiv:2407.20377v1 Announce Type: cross 
Abstract: This paper explores an innovative approach to Environmental, Social, and Governance (ESG) scoring by integrating Natural Language Processing (NLP) techniques with Item Response Theory (IRT), specifically the Rasch model. The study utilizes a comprehensive dataset of news articles in Portuguese related to Petrobras, a major oil company in Brazil, collected from 2022 and 2023. The data is filtered and classified for ESG-related sentiments using advanced NLP methods. The Rasch model is then applied to evaluate the psychometric properties of these ESG measures, providing a nuanced assessment of ESG sentiment trends over time. The results demonstrate the efficacy of this methodology in offering a more precise and reliable measurement of ESG factors, highlighting significant periods and trends. This approach may enhance the robustness of ESG metrics and contribute to the broader field of sustainability and finance by offering a deeper understanding of the temporal dynamics in ESG reporting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20377v1</guid>
      <category>cs.AI</category>
      <category>q-fin.GN</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C\'esar Pedrosa Soares</dc:creator>
    </item>
    <item>
      <title>DiffusionCounterfactuals: Inferring High-dimensional Counterfactuals with Guidance of Causal Representations</title>
      <link>https://arxiv.org/abs/2407.20553</link>
      <description>arXiv:2407.20553v1 Announce Type: cross 
Abstract: Accurate estimation of counterfactual outcomes in high-dimensional data is crucial for decision-making and understanding causal relationships and intervention outcomes in various domains, including healthcare, economics, and social sciences. However, existing methods often struggle to generate accurate and consistent counterfactuals, particularly when the causal relationships are complex. We propose a novel framework that incorporates causal mechanisms and diffusion models to generate high-quality counterfactual samples guided by causal representation. Our approach introduces a novel, theoretically grounded training and sampling process that enables the model to consistently generate accurate counterfactual high-dimensional data under multiple intervention steps. Experimental results on various synthetic and real benchmarks demonstrate the proposed approach outperforms state-of-the-art methods in generating accurate and high-quality counterfactuals, using different evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20553v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiageng Zhu, Hanchen Xie, Jiazhi Li, Wael Abd-Almageed</dc:creator>
    </item>
    <item>
      <title>Industrial-Grade Smart Troubleshooting through Causal Technical Language Processing: a Proof of Concept</title>
      <link>https://arxiv.org/abs/2407.20700</link>
      <description>arXiv:2407.20700v1 Announce Type: cross 
Abstract: This paper describes the development of a causal diagnosis approach for troubleshooting an industrial environment on the basis of the technical language expressed in Return on Experience records. The proposed method leverages the vectorized linguistic knowledge contained in the distributed representation of a Large Language Model, and the causal associations entailed by the embedded failure modes and mechanisms of the industrial assets. The paper presents the elementary but essential concepts of the solution, which is conceived as a causality-aware retrieval augmented generation system, and illustrates them experimentally on a real-world Predictive Maintenance setting. Finally, it discusses avenues of improvement for the maturity of the utilized causal technology to meet the robustness challenges of increasingly complex scenarios in the industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20700v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Trilla, Ossee Yiboe, Nenad Mijatovic, Jordi Vitri\`a</dc:creator>
    </item>
    <item>
      <title>Inferring a population composition from survey data with nonignorable nonresponse: Borrowing information from external sources</title>
      <link>https://arxiv.org/abs/2210.08346</link>
      <description>arXiv:2210.08346v2 Announce Type: replace 
Abstract: We introduce a method to make inference on the composition of a heterogeneous population using survey data, accounting for the possibility that capture heterogeneity is related to key survey variables. To deal with nonignorable nonresponse, we combine different data sources and propose the use of Fisher's noncentral hypergeometric model in a Bayesian framework. To illustrate the potentialities of our methodology, we focus on a case study aimed at estimating the composition of the population of Italian graduates by their occupational status one year after graduating, stratifying by gender and degree program. We account for the possibility that surveys inquiring about the occupational status of new graduates may have response rates that depend on individuals' employment status, implying the nonignorability of the nonresponse. Our findings show that employed people are generally more inclined to answer the questionnaire. Neglecting the nonresponse bias in such contexts might lead to overestimating the employment rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08346v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Veronica Ballerini, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Monte Carlo inference for semiparametric Bayesian regression</title>
      <link>https://arxiv.org/abs/2306.05498</link>
      <description>arXiv:2306.05498v2 Announce Type: replace 
Abstract: Data transformations are essential for broad applicability of parametric regression models. However, for Bayesian analysis, joint inference of the transformation and model parameters typically involves restrictive parametric transformations or nonparametric representations that are computationally inefficient and cumbersome for implementation and theoretical analysis, which limits their usability in practice. This paper introduces a simple, general, and efficient strategy for joint posterior inference of an unknown transformation and all regression model parameters. The proposed approach directly targets the posterior distribution of the transformation by linking it with the marginal distributions of the independent and dependent variables, and then deploys a Bayesian nonparametric model via the Bayesian bootstrap. Crucially, this approach delivers (1) joint posterior consistency under general conditions, including multiple model misspecifications, and (2) efficient Monte Carlo (not Markov chain Monte Carlo) inference for the transformation and all parameters for important special cases. These tools apply across a variety of data domains, including real-valued, positive, and compactly-supported data. Simulation studies and an empirical application demonstrate the effectiveness and efficiency of this strategy for semiparametric Bayesian analysis with linear models, quantile regression, and Gaussian processes. The R package SeBR is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05498v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel R. Kowal, Bohan Wu</dc:creator>
    </item>
    <item>
      <title>Causal Effect Estimation after Propensity Score Trimming with Continuous Treatments</title>
      <link>https://arxiv.org/abs/2309.00706</link>
      <description>arXiv:2309.00706v2 Announce Type: replace 
Abstract: Propensity score trimming, which discards subjects with propensity scores below a threshold, is a common way to address positivity violations that complicate causal effect estimation. However, most works on trimming assume treatment is discrete and models for the outcome regression and propensity score are parametric. This work proposes nonparametric estimators for trimmed average causal effects in the case of continuous treatments based on efficient influence functions. For continuous treatments, an efficient influence function for a trimmed causal effect does not exist, due to a lack of pathwise differentiability induced by trimming and a continuous treatment. Thus, we target a smoothed version of the trimmed causal effect for which an efficient influence function exists. Our resulting estimators exhibit doubly-robust style guarantees, with error involving products or squares of errors for the outcome regression and propensity score, which allows for valid inference even when nonparametric models are used. Our results allow the trimming threshold to be fixed or defined as a quantile of the propensity score, such that confidence intervals incorporate uncertainty involved in threshold estimation. These findings are validated via simulation and an application, thereby showing how to efficiently-but-flexibly estimate trimmed causal effects with continuous treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00706v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zach Branson, Edward H. Kennedy, Sivaraman Balakrishnan, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Learning the Covariance of Treatment Effects Across Many Weak Experiments</title>
      <link>https://arxiv.org/abs/2402.17637</link>
      <description>arXiv:2402.17637v2 Announce Type: replace 
Abstract: When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes. For example, technology companies often infer the long-term impacts of product interventions from their effects on short-term user engagement signals. We consider the meta-analysis of many historical experiments to learn the covariance of treatment effects on these outcomes, which can support the construction of such proxies. Even when experiments are plentiful, if treatment effects are weak, the covariance of estimated treatment effects across experiments can be highly biased. We overcome this with techniques inspired by weak instrumental variable analysis. We show that Limited Information Maximum Likelihood (LIML) learns a parameter equivalent to fitting total least squares to a transformation of the scatterplot of treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter computable from the average of Jackknifed covariance matrices across experiments. We also present a total covariance estimator for the latter estimand under homoskedasticity, which is equivalent to a $k$-class estimator. We show how these parameters can be used to construct unbiased proxy metrics under various structural models. Lastly, we discuss the real-world application of our methods at Netflix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17637v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Bibaut, Winston Chou, Simon Ejdemyr, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Model-based Clustering of Zero-Inflated Single-Cell RNA Sequencing Data via the EM Algorithm</title>
      <link>https://arxiv.org/abs/2406.00245</link>
      <description>arXiv:2406.00245v2 Announce Type: replace 
Abstract: Biological cells can be distinguished by their phenotype or at the molecular level, based on their genome, epigenome, and transcriptome. This paper focuses on the transcriptome, which encompasses all the RNA transcripts in a given cell population, indicating the genes being expressed at a given time. We consider single-cell RNA sequencing data and develop a novel model-based clustering method to group cells based on their transcriptome profiles. Our clustering approach takes into account the presence of zero inflation in the data, which can occur due to genuine biological zeros or technological noise. The proposed model for clustering involves a mixture of zero-inflated Poisson or zero-inflated negative binomial distributions, and parameter estimation is carried out using the EM algorithm. We evaluate the performance of our proposed methodology through simulation studies and analyses of publicly available datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00245v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra AghahosseinaliShirazi, Pedro A. Rangel, Camila P. E. de Souza</dc:creator>
    </item>
    <item>
      <title>A Bayesian modelling framework for health care resource use and costs in trial-based economic evaluations</title>
      <link>https://arxiv.org/abs/2407.17036</link>
      <description>arXiv:2407.17036v2 Announce Type: replace 
Abstract: Individual-level effectiveness and healthcare resource use (HRU) data are routinely collected in trial-based economic evaluations. While effectiveness is often expressed in terms of utility scores derived from some health-related quality of life instruments (e.g.~EQ-5D questionnaires), different types of HRU may be included. Costs are usually generated by applying unit prices to HRU data and statistical methods have been traditionally implemented to analyse costs and utilities or after combining them into aggregated variables (e.g. Quality-Adjusted Life Years). When outcome data are not fully observed, e.g. some patients drop out or only provided partial information, the validity of the results may be hindered both in terms of efficiency and bias. Often, partially-complete HRU data are handled using "ad-hoc" methods, implicitly relying on some assumptions (e.g. fill-in a zero) which are hard to justify beside the practical convenience of increasing the completion rate. We present a general Bayesian framework for the modelling of partially-observed HRUs which allows a flexible model specification to accommodate the typical complexities of the data and to quantify the impact of different types of uncertainty on the results. We show the benefits of using our approach using a motivating example and compare the results to those from traditional analyses focussed on the modelling of cost variables after adopting some ad-hoc imputation strategy for HRU data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17036v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Gabrio</dc:creator>
    </item>
    <item>
      <title>Location- and scale-free procedures for distinguishing between distribution tail models</title>
      <link>https://arxiv.org/abs/2301.03894</link>
      <description>arXiv:2301.03894v2 Announce Type: replace-cross 
Abstract: We consider distinguishing between two distribution tail models when tails of one model are lighter (or heavier) than those of the other. Two procedures are proposed: one scale-free and one location- and scale-free, and their asymptotic properties are established. We show the advantage of using these procedures for distinguishing between certain tail models in comparison with the tests proposed in the literature by simulation and apply them to data on daily precipitation in Green Bay, US and Saentis, Switzerland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.03894v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Rodionov</dc:creator>
    </item>
    <item>
      <title>Multi-Scale CUSUM Tests for Time Dependent Spherical Random Fields</title>
      <link>https://arxiv.org/abs/2305.01392</link>
      <description>arXiv:2305.01392v2 Announce Type: replace-cross 
Abstract: This paper investigates the asymptotic behavior of structural break tests in the harmonic domain for time dependent spherical random fields. In particular, we prove a functional central limit theorem result for the fluctuations over time of the sample spherical harmonic coefficients, under the null of isotropy and stationarity; furthermore, we prove consistency of the corresponding CUSUM test, under a broad range of alternatives, including deterministic trend, abrupt change, and a nontrivial power alternative. Our results are then applied to NCEP data on global temperature: our estimates suggest that Climate Change does not simply affect global average temperatures, but also the nature of spatial fluctuations at different scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.01392v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessia Caponera, Domenico Marinucci, Anna Vidotto</dc:creator>
    </item>
    <item>
      <title>Symmetrisation of a class of two-sample tests by mutually considering depth ranks including functional spaces</title>
      <link>https://arxiv.org/abs/2308.09869</link>
      <description>arXiv:2308.09869v2 Announce Type: replace-cross 
Abstract: Statistical depth functions provide measures of the outlyingness, or centrality, of the elements of a space with respect to a distribution. It is a nonparametric concept applicable to spaces of any dimension, for instance, multivariate and functional. Liu and Singh (1993) presented a multivariate two-sample test based on depth-ranks. We dedicate this paper to improving the power of the associated test statistic and incorporating its applicability to functional data. In doing so, we obtain a more natural test statistic that is symmetric in both samples. We derive the null asymptotic of the proposed test statistic, also proving the validity of the testing procedure for functional data. Finally, the finite sample performance of the test for functional data is illustrated by means of a simulation study and a real data analysis on annual temperature curves of ocean drifters is executed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09869v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-EJS2250</arxiv:DOI>
      <arxiv:journal_reference>Electron. J. Statist. 18 (2) 3021 - 3106, 2024</arxiv:journal_reference>
      <dc:creator>Felix Gnettner, Claudia Kirch, Alicia Nieto-Reyes</dc:creator>
    </item>
    <item>
      <title>A changepoint approach to modelling non-stationary soil moisture dynamics</title>
      <link>https://arxiv.org/abs/2310.17546</link>
      <description>arXiv:2310.17546v2 Announce Type: replace-cross 
Abstract: Soil moisture dynamics provide an indicator of soil health that scientists model via drydown curves. The typical modelling process requires the soil moisture time series to be manually separated into drydown segments and then exponential decay models are fitted to them independently. Sensor development over recent years means that experiments that were previously conducted over a few field campaigns can now be scaled to months or years at a higher sampling rate. To better meet the challenge of increasing data size, this paper proposes a novel changepoint-based approach to automatically identify structural changes in the soil drying process and simultaneously estimate the drydown parameters that are of interest to soil scientists. A simulation study is carried out to demonstrate the performance of the method in detecting changes and retrieving model parameters. Practical aspects of the method such as adding covariates and penalty learning are discussed. The method is applied to hourly soil moisture time series from the NEON data portal to investigate the temporal dynamics of soil moisture drydown. We recover known relationships previously identified manually, alongside delivering new insights into the temporal variability across soil types and locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17546v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyi Gong, Rebecca Killick, Christopher Nemeth, John Quinton</dc:creator>
    </item>
    <item>
      <title>New methods to compute the generalized chi-square distribution</title>
      <link>https://arxiv.org/abs/2404.05062</link>
      <description>arXiv:2404.05062v2 Announce Type: replace-cross 
Abstract: We present several new mathematical methods (ray-trace, inverse Fourier transform and ellipse) and open-source software to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution. Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index d' between multinormals. We characterize the performance and limitations of these and previous methods, and recommend the best methods to use for each part of each type of distribution. We also demonstrate the speed and accuracy of our new methods against previous methods across a wide sample of distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05062v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhranil Das</dc:creator>
    </item>
  </channel>
</rss>
