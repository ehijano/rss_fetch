<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 May 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FTNILO: Explicit Multivariate Function Inversion, Optimization and Counting, Cryptography Weakness and Riemann Hypothesis Solution Equation with Tensor Networks</title>
      <link>https://arxiv.org/abs/2505.05493</link>
      <description>arXiv:2505.05493v1 Announce Type: new 
Abstract: In this paper, we present a new formalism, the Field Tensor Network Integral Logical Operator (FTNILO), to obtain the explicit equation that returns the minimum, maximum, and zeros of a multivariable injective function, and an algorithm for non-injective ones. This method extends the MeLoCoToN algorithm for inversion and optimization problems with continuous variables, by using Field Tensor Networks. The fundamentals of the method are the conversion of the problem of minimization of $N$ continuous variables into a problem of maximization of a dependent functional of a single variable. It can also be adapted to determine other properties, such as the zeros of any function. For this purpose, we use an extension of the imaginary time evolution, the new method of continuous signals, and partial or total integration, depending on the case. In addition, we show a direct way to recover both the tensor networks and the MeLoCoToN from this formalism. We show some examples of application, such as the Riemann hypothesis resolution. We provide an explicit integral equation that gives the solution of the Riemann hypothesis, being that if it results in a zero value, it is correct; otherwise, it is wrong. This algorithm requires no deep mathematical knowledge and is based on simple mathematical properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05493v1</guid>
      <category>math.OC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Mata Ali</dc:creator>
    </item>
    <item>
      <title>Constraint Selection in Optimization-Based Controllers</title>
      <link>https://arxiv.org/abs/2505.05502</link>
      <description>arXiv:2505.05502v1 Announce Type: new 
Abstract: Human-machine collaboration often involves constrained optimization problems for decision-making processes. However, when the machine is a dynamical system with a continuously evolving state, infeasibility due to multiple conflicting constraints can lead to dangerous outcomes. In this work, we propose a heuristic-based method that resolves infeasibility at every time step by selectively disregarding a subset of soft constraints based on the past values of the Lagrange multipliers. Compared to existing approaches, our method requires the solution of a smaller optimization problem to determine feasibility, resulting in significantly faster computation. Through a series of simulations, we demonstrate that our algorithm achieves performance comparable to state-of-the-art methods while offering improved computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05502v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haejoon Lee, Panagiotis Rousseas, Dimitra Panagou</dc:creator>
    </item>
    <item>
      <title>Multi-armed Bandit for Stochastic Shortest Path in Mixed Autonomy</title>
      <link>https://arxiv.org/abs/2505.05878</link>
      <description>arXiv:2505.05878v1 Announce Type: new 
Abstract: In mixed-autonomy traffic networks, autonomous vehicles (AVs) are required to make sequential routing decisions under uncertainty caused by dynamic and heterogeneous interactions with human-driven vehicles (HDVs). Early-stage greedy decisions made by AVs during interactions with the environment often result in insufficient exploration, leading to failures in discovering globally optimal strategies. The exploration-exploitation balancing mechanism inherent in multi-armed bandit (MAB) methods is well-suited for addressing such problems. Based on the Real-Time Dynamic Programming (RTDP) framework, we introduce the Upper Confidence Bound (UCB) exploration strategy from the MAB paradigm and propose a novel algorithm. We establish the path-level regret upper bound under the RTDP framework, which guarantees the worst-case convergence of the proposed algorithm. Extensive numerical experiments conducted on a real-world local road network in Shanghai demonstrate that the proposed algorithm effectively overcomes the failure of standard RTDP to converge to the optimal policy under highly stochastic environments. Moreover, compared to the standard Value Iteration (VI) framework, the RTDP-based framework demonstrates superior computational efficiency. Our results highlight the effectiveness of the proposed algorithm in routing within large-scale stochastic mixed-autonomy environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05878v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Bai, Yiming Li, Xi Xiong</dc:creator>
    </item>
    <item>
      <title>Data-driven Model Predictive Control: Asymptotic Stability despite Approximation Errors exemplified in the Koopman framework</title>
      <link>https://arxiv.org/abs/2505.05951</link>
      <description>arXiv:2505.05951v1 Announce Type: new 
Abstract: In this paper, we analyze nonlinear model predictive control (MPC) using data-driven surrogates in the prediction and optimization step. First, we establish asymptotic stability of the origin, a controlled steady state, w.r.t. the MPC closed loop without stabilizing terminal conditions. To this end, we prove that cost controllability of the original system is preserved if proportional bounds on the approximation error hold. Here, proportional refers to state and control, while the respective constants depend on the approximation accuracy. The proportionality of the error bounds is a key element to derive asymptotic stability in presence of modeling errors and not only practical asymptotic stability. Second, we exemplarily verify the imposed assumptions for data-driven surrogates generated with kernel extended dynamic mode decomposition based on the Koopman operator. Hereby, we do not impose invariance assumptions on finite dictionaries, but rather derive all conditions under non-restrictive data requirements. Finally, we verify our findings with numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05951v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irene Schimperna, Karl Worthmann, Manuel Schaller, Lea Bold, Lalo Magni</dc:creator>
    </item>
    <item>
      <title>Superquantile-Gibbs Relaxation for Minima-selection in Bi-Level Optimization</title>
      <link>https://arxiv.org/abs/2505.05991</link>
      <description>arXiv:2505.05991v1 Announce Type: new 
Abstract: Minima selection is essential for defining Bi-Level Optimization (BLO) when the lower-level objective has multiple minimizers. While BLO is intractable in its general form, we show that restricting the lower-level objective to the recently proposed PL-circle functions (Gong et al., 2024) guarantees a continuous hyper-objective F_max. The PL-circle condition is strictly weaker than the global Polyak-Lojasiewicz condition used in prior BLO work and allows modeling of practical settings such as hyperparameter tuning in over-parameterized deep learning. However, even under this condition, F_max remains non-convex and non-smooth. To address this, we propose a relaxed solution concept: we approximate F_max with a continuously differentiable function F_max_tilde that is pointwise epsilon_v-close to F_max and seek an epsilon_g-stationary point of F_max_tilde. In this framework, we reduce the minima-selection subproblem to a sampling task using a novel Superquantile-Gibbs relaxation. By leveraging the manifold structure of the lower-level solution set under the PL-circle condition, our method finds a relaxed solution using poly(epsilon_v^{-k} epsilon_g^{-1}) queries to a Gibbs sampling oracle, which is efficiently implemented using Langevin dynamics. Here, k is the intrinsic dimension of the manifolds defining the lower-level solutions. This is the first work to characterize the complexity of BLO in terms of this intrinsic dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05991v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeed Masiha, Zebang Shen, Negar Kiyavash, Niao He</dc:creator>
    </item>
    <item>
      <title>Towards time series aggregation with exact error quantification for optimization of energy systems</title>
      <link>https://arxiv.org/abs/2505.06083</link>
      <description>arXiv:2505.06083v1 Announce Type: new 
Abstract: Energy system optimization models are becoming increasingly popular for analyzing energy markets, such as the impact of new policies or interactions between energy carriers. One key challenge of these models is the trade-off between modeling accuracy and computational tractability. A recently proposed mathematical framework addresses this challenge by achieving exact time series aggregations merging time periods sharing the same active constraint sets. This aggregation, however, is insufficient when the number of unique active constraints is large. We overcome this issue by aggregating data points from different active constraint sets. While this further reduces model size, it inevitably introduces an error compared to the full model. Yet, we show how this error can be exactly quantified without re-solving the optimization problem, enabling users to trade off computational efficiency and model accuracy proactively. This may be especially useful in energy markets to accommodate varying granularity across short- and long-term time horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06083v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beltr\'an Castro G\'omez, Yannick Werner, Sonja Wogrin</dc:creator>
    </item>
    <item>
      <title>ABAMGuid+: An Enhanced Aerocapture Guidance Framework using Augmented Bank Angle Modulation</title>
      <link>https://arxiv.org/abs/2505.06161</link>
      <description>arXiv:2505.06161v1 Announce Type: new 
Abstract: Aerocapture consists of converting a hyperbolic approach trajectory into a captured target orbit utilizing the aerodynamic forces generated via a single pass through the atmosphere. Aerocapture guidance systems must be robust to significant environmental variations and modeling uncertainty, particularly regarding atmospheric properties and delivery conditions. Recent work has shown that enabling control over both bank angle and angle of attack, a strategy referred to as augmented bank angle modulation (ABAM), can improve robustness to entry state and atmospheric uncertainties. In this work, we derive optimal control solutions for an aerocapture vehicle using ABAM. We first formulate the problem using a linear aerodynamic model and derive closed-form optimal control profiles using Pontryagin's Minimum Principle. To increase modeling fidelity, we also consider a quadratic aerodynamic model and obtain the solution directly using the optimality conditions. Both formulations are solved numerically using Gauss pseudospectral methods (via GPOPS, a software tool for pseudospectral optimal control), to validate the analytic solutions. We then introduce a novel aerocapture guidance algorithm, ABAMGuid+, which indirectly minimizes propellant usage by mimicking the structure of the optimal control solution, enabling efficient guidance by avoiding the complexity of solving the full optimal control problem online. Extensive Monte Carlo simulations of a Uranus aerocapture mission demonstrate that ABAMGuid+ increases capture success rates and reduces post-capture propellant requirements relative to previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06161v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyle A. Sonandres, Thomas R. Palazzo, Jonathan P. How</dc:creator>
    </item>
    <item>
      <title>Weak convergence of projection algorithm with momentum terms and new step size rule for quasimonotone variational inequalities</title>
      <link>https://arxiv.org/abs/2505.06170</link>
      <description>arXiv:2505.06170v1 Announce Type: new 
Abstract: This article analyses the simple projection method proposed by Izuchukwu et al. [8, Algorithm 3.2] for solving variational inequality problems by incorporating momentum terms. A new step size strategy is also introduced, in which the step size sequence increases after a finite number of iterations. Under the assumptions that the underlying operator is quasimonotone and Lipschitz continuous, we establish weak convergence of the proposed method. The effectiveness and efficiency of the algorithm are demonstrated through numerical experiments and are compared with existing approaches from the literature. Finally, we apply the proposed algorithm to a signal recovery problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06170v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gourav Kumar, Santanu Soe, V. Vetrivel</dc:creator>
    </item>
    <item>
      <title>Average Optimal Control of Uncertain Control-Affine Systems</title>
      <link>https://arxiv.org/abs/2505.06204</link>
      <description>arXiv:2505.06204v1 Announce Type: new 
Abstract: This work studies optimal control problems of systems with uncertain, probabilistically distributed parameters to optimize average performance. Known as Riemann-Stieltjes, average, or ensemble optimal control, this kind of problem is crucial when parameter uncertainty matters. We derive necessary optimality conditions and characterize feedback controls for control-affine systems. Two scenarios are examined: known initial conditions (finite-dimensional case) and uncertain initial conditions (infinite-dimensional framework). The Pontryagin Maximum Principle is extended using a Hilbert space formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06204v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Soledad Aronna, Gabriel de Lima Monteiro, Oscar Sierra Fonseca</dc:creator>
    </item>
    <item>
      <title>Alternating Methods for Large-Scale AC Optimal Power Flow with Unit Commitment</title>
      <link>https://arxiv.org/abs/2505.06211</link>
      <description>arXiv:2505.06211v1 Announce Type: new 
Abstract: Security-constrained unit commitment with alternating current optimal power flow (SCUC-ACOPF) is a central problem in power grid operations that optimizes commitment and dispatch of generators under a physically accurate power transmission model while encouraging robustness against component failures. SCUC-ACOPF requires solving large-scale problems that involve multiple time periods and networks with thousands of buses within strict time limits. In this work, we study a detailed SCUC-ACOPF model with a rich set of features of modern power grids, including price-sensitive load, reserve products, transformer controls, and energy-limited devices. We propose a decomposition scheme and a penalty alternating direction method to find high-quality solutions to this model. Our methodology leverages spatial and temporal decomposition, separating the problem into a set of mixed-integer linear programs for each bus and a set of continuous nonlinear programs for each time period. To improve the performance of the algorithm, we introduce a variety of heuristics, including restrictions of temporal linking constraints, a second-order cone relaxation, and a contingency screening algorithm. We quantify the quality of feasible solutions through a dual bound from a convex second-order cone program. To evaluate our algorithm, we use large-scale test cases from Challenge 3 of the U.S. Department of Energy's Grid Optimization Competition that resemble real power grid data under a variety of operating conditions and decision horizons. The experiments yield feasible solutions with an average optimality gap of 1.33%, demonstrating that this approach generates near-optimal solutions within stringent time limits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06211v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Brun, Thomas Lee, Dirk Lauinger, Xin Chen, Xu Andy Sun</dc:creator>
    </item>
    <item>
      <title>ADMM-Based Training for Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2505.05527</link>
      <description>arXiv:2505.05527v1 Announce Type: cross 
Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to their high potential in time-series processing combined with minimal energy consumption. However, they still lack a dedicated and efficient training algorithm. The popular backpropagation with surrogate gradients, adapted from stochastic gradient descent (SGD)-derived algorithms, has several drawbacks when used as an optimizer for SNNs. Specifically, it suffers from low scalability and numerical imprecision. In this paper, we propose a novel SNN training method based on the alternating direction method of multipliers (ADMM). Our ADMM-based training aims to solve the problem of the SNN step function's non-differentiability. We formulate the problem, derive closed-form updates, and empirically show the optimizer's convergence properties, great potential, and possible new research directions to improve the method in a simulated proof-of-concept.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05527v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Perin, Cesare Bidini, Riccardo Mazzieri, Michele Rossi</dc:creator>
    </item>
    <item>
      <title>A Feedback Control Framework for Incentivised Suburban Parking Utilisation and Urban Core Traffic Relief</title>
      <link>https://arxiv.org/abs/2505.05742</link>
      <description>arXiv:2505.05742v1 Announce Type: cross 
Abstract: Urban traffic congestion, exacerbated by inefficient parking management and cruising for parking, significantly hampers mobility and sustainability in smart cities. Drivers often face delays searching for parking spaces, influenced by factors such as accessibility, cost, distance, and available services such as charging facilities in the case of electric vehicles. These inefficiencies contribute to increased urban congestion, fuel consumption, and environmental impact. Addressing these challenges, this paper proposes a feedback control incentivisation-based system that aims to better distribute vehicles between city and suburban parking facilities offering park-and-charge/-ride services. Individual driver behaviours are captured via discrete choice models incorporating factors of importance to parking location choice among drivers, such as distance to work, public transport connectivity, charging infrastructure availability, and amount of incentive offered; and are regulated through principles of ergodic control theory. The proposed framework is applied to an electric vehicle park-and-charge/-ride problem, and demonstrates how predictable long-term behaviour of the system can be guaranteed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05742v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdul Baseer Satti, James Saunderson, Wynita Griggs, S. M. Nawazish Ali, Nameer Al Khafaf, Saman Ahmadi, Mahdi Jalili, Jakub Marecek, Robert Shorten</dc:creator>
    </item>
    <item>
      <title>Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency</title>
      <link>https://arxiv.org/abs/2505.05796</link>
      <description>arXiv:2505.05796v1 Announce Type: cross 
Abstract: Heating, Ventilation, and Air Conditioning (HVAC) systems account for approximately 38% of building energy consumption globally, making them one of the most energy-intensive services. The increasing emphasis on energy efficiency and sustainability, combined with the need for enhanced occupant comfort, presents a significant challenge for traditional HVAC systems. These systems often fail to dynamically adjust to real-time changes in electricity market rates or individual comfort preferences, leading to increased energy costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL) Artificial Intelligence framework that optimizes HVAC performance by incorporating real-time user feedback and responding to fluctuating electricity prices. Unlike conventional systems that require predefined information about occupancy or comfort levels, our approach learns and adapts based on ongoing user input. By integrating the occupancy prediction model with reinforcement learning, the system improves operational efficiency and reduces energy costs in line with electricity market dynamics, thereby contributing to demand response initiatives. Through simulations, we demonstrate that our method achieves significant cost reductions compared to baseline approaches while maintaining or enhancing occupant comfort. This feedback-driven approach ensures personalized comfort control without the need for predefined settings, offering a scalable solution that balances individual preferences with economic and environmental goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05796v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3679240.3734587</arxiv:DOI>
      <dc:creator>Xinyu Liang, Frits de Nijs, Buser Say, Hao Wang</dc:creator>
    </item>
    <item>
      <title>Mixed-Integer Optimization for Responsible Machine Learning</title>
      <link>https://arxiv.org/abs/2505.05857</link>
      <description>arXiv:2505.05857v1 Announce Type: cross 
Abstract: In the last few decades, Machine Learning (ML) has achieved significant success across domains ranging from healthcare, sustainability, and the social sciences, to criminal justice and finance. But its deployment in increasingly sophisticated, critical, and sensitive areas affecting individuals, the groups they belong to, and society as a whole raises critical concerns around fairness, transparency, robustness, and privacy, among others. As the complexity and scale of ML systems and of the settings in which they are deployed grow, so does the need for responsible ML methods that address these challenges while providing guaranteed performance in deployment.
  Mixed-integer optimization (MIO) offers a powerful framework for embedding responsible ML considerations directly into the learning process while maintaining performance. For example, it enables learning of inherently transparent models that can conveniently incorporate fairness or other domain specific constraints. This tutorial paper provides an accessible and comprehensive introduction to this topic discussing both theoretical and practical aspects. It outlines some of the core principles of responsible ML, their importance in applications, and the practical utility of MIO for building ML models that align with these principles. Through examples and mathematical formulations, it illustrates practical strategies and available tools for efficiently solving MIO problems for responsible ML. It concludes with a discussion on current limitations and open research questions, providing suggestions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05857v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Justin, Qingshi Sun, Andr\'es G\'omez, Phebe Vayanos</dc:creator>
    </item>
    <item>
      <title>Variational quantum algorithms for permutation-based combinatorial problems: Optimal ansatz generation with applications to quadratic assignment problems and beyond</title>
      <link>https://arxiv.org/abs/2505.05981</link>
      <description>arXiv:2505.05981v1 Announce Type: cross 
Abstract: We present a quantum variational algorithm based on a novel circuit that generates all permutations that can be spanned by one- and two-qubits permutation gates. The construction of the circuits follows from group-theoretical results, most importantly the Bruhat decomposition of the group generated by the \(\mathtt{cx}\) gates. These circuits require a number of qubits that scale logarithmically with the permutation dimension, and are therefore employable in near-term applications. We further augment the circuits with ancilla qubits to enlarge their span, and with these we build ansatze to tackle permutation-based optimization problems such as quadratic assignment problems, and graph isomorphisms. The resulting quantum algorithm, \textsc{QuPer}, is competitive with respect to classical heuristics and we could simulate its behavior up to a problem with $256$ variables, requiring $20$ qubits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05981v1</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Laplace Mermoud, Andrea Simonetto, Sourour Elloumi</dc:creator>
    </item>
    <item>
      <title>Universal Approximation Theorem for Deep Q-Learning via FBSDE System</title>
      <link>https://arxiv.org/abs/2505.06023</link>
      <description>arXiv:2505.06023v1 Announce Type: cross 
Abstract: The approximation capabilities of Deep Q-Networks (DQNs) are commonly justified by general Universal Approximation Theorems (UATs) that do not leverage the intrinsic structural properties of the optimal Q-function, the solution to a Bellman equation. This paper establishes a UAT for a class of DQNs whose architecture is designed to emulate the iterative refinement process inherent in Bellman updates. A central element of our analysis is the propagation of regularity: while the transformation induced by a single Bellman operator application exhibits regularity, for which Backward Stochastic Differential Equations (BSDEs) theory provides analytical tools, the uniform regularity of the entire sequence of value iteration iterates--specifically, their uniform Lipschitz continuity on compact domains under standard Lipschitz assumptions on the problem data--is derived from finite-horizon dynamic programming principles. We demonstrate that layers of a deep residual network, conceived as neural operators acting on function spaces, can approximate the action of the Bellman operator. The resulting approximation theorem is thus intrinsically linked to the control problem's structure, offering a proof technique wherein network depth directly corresponds to iterations of value function refinement, accompanied by controlled error propagation. This perspective reveals a dynamic systems view of the network's operation on a space of value functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06023v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Qi</dc:creator>
    </item>
    <item>
      <title>Safe-EF: Error Feedback for Nonsmooth Constrained Optimization</title>
      <link>https://arxiv.org/abs/2505.06053</link>
      <description>arXiv:2505.06053v1 Announce Type: cross 
Abstract: Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-K) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06053v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rustem Islamov, Yarden As, Ilyas Fatkhullin</dc:creator>
    </item>
    <item>
      <title>Distributed Stochastic Optimization under Heavy-Tailed Noises</title>
      <link>https://arxiv.org/abs/2312.15847</link>
      <description>arXiv:2312.15847v3 Announce Type: replace 
Abstract: This paper studies the distributed optimization problem under the influence of heavy-tailed gradient noises. Here, a heavy-tailed noise means that the noise does not necessarily satisfy the bounded variance assumption. Instead, it satisfies a more general assumption. The commonly-used bounded variance assumption is a special case of the considered noise assumption. A typical example of this kind of noise is a Pareto distribution noise with tail index within (1,2], which has infinite variance. Despite that there has been several distributed optimization algorithms proposed for the heavy-tailed noise scenario, these algorithms need a centralized server in the network which collects the information of all clients. Different from these algorithms, this paper considers that there is no centralized server and the agents can only exchange information with neighbors in a communication graph. A distributed method combining gradient clipping and distributed stochastic subgradient projection is proposed. It is proven that when the gradient descent step-size and the gradient clipping step-size meet certain conditions, the state of each agent converges to the optimal solution of the distributed optimization problem with probability 1. The simulation results validate the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15847v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Sun, Huiming Zhang, Bo Chen, Li Yu</dc:creator>
    </item>
    <item>
      <title>Convergence of Decentralized Stochastic Subgradient-based Methods for Nonsmooth Nonconvex functions</title>
      <link>https://arxiv.org/abs/2403.11565</link>
      <description>arXiv:2403.11565v3 Announce Type: replace 
Abstract: In this paper, we focus on the decentralized stochastic subgradient-based methods in minimizing nonsmooth nonconvex functions without Clarke regularity, especially in the decentralized training of nonsmooth neural networks. We propose a general framework that unifies various decentralized subgradient-based methods, such as decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGD-M). To establish the convergence properties of our proposed framework, we relate the discrete iterates to the trajectories of a continuous-time differential inclusion, which is assumed to have a coercive Lyapunov function with a stable set $\mathcal{A}$. We prove the asymptotic convergence of the iterates to the stable set $\mathcal{A}$ with sufficiently small and diminishing step-sizes. These results provide first convergence guarantees for some well-recognized of decentralized stochastic subgradient-based methods without Clarke regularity of the objective function. Preliminary numerical experiments demonstrate that our proposed framework yields highly efficient decentralized stochastic subgradient-based methods with convergence guarantees in the training of nonsmooth neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11565v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyuan Zhang, Nachuan Xiao, Xin Liu</dc:creator>
    </item>
    <item>
      <title>A First-Order Gradient Approach for the Connectivity Optimization of Markov Chains</title>
      <link>https://arxiv.org/abs/2403.11744</link>
      <description>arXiv:2403.11744v3 Announce Type: replace 
Abstract: Graphs are commonly used to model various complex systems, including social networks, power grids, transportation networks, and biological systems. In many applications, the connectivity of these networks can be expressed through the Mean First Passage Times (MFPTs) of a Markov chain modeling a random walker on the graph. In this paper, we generalize the network metrics based on Markov chains' MFPTs and extend them to networks affected by uncertainty, in which edges may fail and hence not be present according to a pre-determined stochastic model. To find optimally connected Markov chains, we present a parameterization-free method for optimizing the MFPTs of the Markov chain. More specifically, we present an efficient Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm in the context of Markov chain optimization. The proposed algorithm is suitable for both fixed and random networks. Using various numerical experiments, we demonstrate scalability compared to established benchmarks. Importantly, our algorithm finds an optimal solution without requiring prior knowledge of edge failure probabilities, allowing for an online optimization approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11744v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian P. C. Franssen, Alessandro Zocca, Bernd F. Heidergott</dc:creator>
    </item>
    <item>
      <title>A Bilevel Hierarchy of Strengthened Complex Moment Relaxations for Complex Polynomial Optimization</title>
      <link>https://arxiv.org/abs/2404.07125</link>
      <description>arXiv:2404.07125v3 Announce Type: replace 
Abstract: This paper proposes a bilevel hierarchy of strengthened complex moment relaxations for complex polynomial optimization. The key trick entails considering a class of positive semidefinite conditions that arise naturally in characterizing the normality of the so-called shift operators. The relaxation problem in this new hierarchy is parameterized by the usual relaxation order as well as an extra normal order, thus providing more space of flexibility to balance the strength of relaxation and computational complexity. Extensive numerical experiments demonstrate the superior performance of the new hierarchy compared to the usual hierarchy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07125v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Wang</dc:creator>
    </item>
    <item>
      <title>Convergence of machine learning methods for feedback control laws: averaged feedback learning scheme and data driven methods</title>
      <link>https://arxiv.org/abs/2407.18403</link>
      <description>arXiv:2407.18403v2 Announce Type: replace 
Abstract: This work addresses the synthesis of optimal feedback control laws via machine learning. In particular, the Averaged Feedback Learning Scheme (AFLS) and a data driven method are considered. Hypotheses for each method ensuring the convergence of the evaluation of the objective function of the underlying control problem at the obtained feedback-laws towards the optimal value function are provided. These hypotheses are connected to the regularity of the value function and the stability of the dynamics. In the case of AFLS these hypotheses only require H\"older continuity of the value function, whereas for the data driven method the value function must be at least $C^2$. It is demonstrated that these methods are connected via their optimality conditions. Additionally, numerical experiments are provided by applying both methods to a family control problems, parameterized by a positive real number which controls the regularity of the value function. For small parameters the value function is smooth and in contrast for large parameters it is non-differentiable, but semi-concave. The results of the experiments indicate that both methods have a similar performance for the case that the value function is smooth. On the other hand, if the value function is not differentiable, AFLS has a better performance which is consistent with the obtained convergence results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18403v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Kunisch, Donato V\'asquez-Varas</dc:creator>
    </item>
    <item>
      <title>Optimal Control and Potential Games in the Mean Field</title>
      <link>https://arxiv.org/abs/2408.00733</link>
      <description>arXiv:2408.00733v2 Announce Type: replace 
Abstract: We study a mean field optimal control problem with general non-Markovian dynamics, including both common noise and jumps. We show that its minimizers are Nash equilibria of an associated mean field game of controls. These types of games are necessarily potential, and the Nash equilibria derived as the minimizers of the control problem are closely connected to McKean-Vlasov equations of Langevin type. To illustrate the general theory, we present several examples, including a mean field game of controls with interactions through a price variable, and mean field Cucker-Smale Flocking and Kuramoto models. We also establish the invariance property of the value function, a key ingredient used in our proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00733v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix H\"ofer, H. Mete Soner</dc:creator>
    </item>
    <item>
      <title>Cubic NK-SVD: An Algorithm for Designing Parametric Dictionary in Frequency Estimation</title>
      <link>https://arxiv.org/abs/2408.03708</link>
      <description>arXiv:2408.03708v2 Announce Type: replace 
Abstract: We propose a novel parametric dictionary learning algorithm for line spectral estimation, applicable in both single measurement vector (SMV) and multiple measurement vectors (MMV) scenarios. This algorithm, termed cubic Newtonized K-SVD (NK-SVD), extends the traditional K-SVD method by incorporating cubic regularization into Newton refinements. The proposed Gauss-Seidel scheme not only enhances the accuracy of frequency estimation over the continuum but also achieves better convergence by incorporating higher-order derivative information. A key contribution of this work is the rigorous convergence analysis of the proposed algorithm within the Block Coordinate Descent (BCD) framework. To the best of our knowledge, this is the first convergence analysis of BCD with a higher-order regularization scheme. Moreover, the convergence framework we develop is generalizable, providing a foundation for designing alternating minimization algorithms with higher-order regularization techniques. Extensive simulations demonstrate that cubic NK-SVD outperforms state-of-the-art methods in both SMV and MMV settings, particularly excelling in the challenging task of recovering closely-spaced frequencies. The code for our method is available at https://github.com/xzliu-opt/Cubic-NK-SVD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03708v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.sigpro.2025.110029</arxiv:DOI>
      <dc:creator>Xiaozhi Liu, Yong Xia</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Gradient-Based Optimization Techniques Using Multidimensional Surface 3D Visualizations and Initial Point Sensitivity</title>
      <link>https://arxiv.org/abs/2409.04470</link>
      <description>arXiv:2409.04470v3 Announce Type: replace 
Abstract: This study examines several renowned gradient-based optimization techniques and focuses on their computational efficiency and precision. In the study, the steepest descent, conjugate gradient (Fletcher-Reeves and Polak-Ribiere variants), Newton-Raphson, quasi-Newton (BFGS), and Levenberg-Marquardt techniques were evaluated. These methods were benchmarked using Rosenbrock's, Spring Force Vanderplaats', Ackley's, and Himmelblau's functions. We emphasize the critical role that initial point selection plays in optimizing optimization outcomes in our analysis. It is also important to distinguish between local and global optima since gradient-based methods may have difficulties dealing with nonlinearity and multimodality. We illustrate optimization trajectories using 3D surface visualizations in order to increase understanding. While gradient-based methods have been demonstrated to be effective, they may be limited by computational constraints and by the nature of the objective functions, necessitating the use of heuristic and metaheuristic algorithms in more complex situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04470v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saeed Asadi, Sonia Gharibzadeh, Hajar Kazemi Naeini, Masoud Reihanifar, Morteza Rahimi, Shiva Zangeneh, Aseel Smerat, Lazim Abdullah</dc:creator>
    </item>
    <item>
      <title>Presolving and cutting planes for the generalized maximal covering location problem</title>
      <link>https://arxiv.org/abs/2409.09834</link>
      <description>arXiv:2409.09834v2 Announce Type: replace 
Abstract: This paper considers the generalized maximal covering location problem (GMCLP) which establishes a fixed number of facilities to maximize the weighted sum of the covered customers, allowing customer weights to be positive or negative. Due to the huge number of linear constraints to model the covering relations between the candidate facility locations and customers, and particularly the poor linear programming (LP) relaxation, the GMCLP is extremely difficult to solve by state-of-the-art mixed integer programming (MIP) solvers. To improve the computational performance of MIP-based approaches for solving GMCLPs, we propose customized presolving and cutting plane techniques, which are isomorphic aggregation, dominance reduction, and two-customer inequalities. The isomorphic aggregation and dominance reduction can not only reduce the problem size but also strengthen the LP relaxation of the MIP formulation of the GMCLP. The two-customer inequalities can be embedded into a branch-and-cut framework to further strengthen the LP relaxation of the MIP formulation on the fly. By extensive computational experiments, we show that all three proposed techniques can substantially improve the capability of MIP solvers in solving GMCLPs. In particular, for a testbed of 40 instances with identical numbers of customers and candidate facility locations in the literature, the proposed techniques enable us to provide optimal solutions for 13 previously unsolved benchmark instances; for a testbed of 336 instances where the number of customers is much larger than the number of candidate facility locations, the proposed techniques can turn most of them from intractable to easily solvable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09834v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Lv, Cheng-Yang Yu, Jie Liang, Wei-Kun Chen, Yu-Hong Dai</dc:creator>
    </item>
    <item>
      <title>Analysis of a toy model for optimal crop protection</title>
      <link>https://arxiv.org/abs/2410.11733</link>
      <description>arXiv:2410.11733v2 Announce Type: replace 
Abstract: In this paper we investigate an optimal control problem involving a toy model for the protection on a crop field. Precisely, we consider a protection on a crop field and we want to place intervention zones represented by a control, in order to maximise the protection on the field during a given period. Using a relaxation method, we prove that there exists a control which maximises the protection and, moreover, it must be a bang-bang control. Furthermore, with additional assumptions on the crop field geometry, some results on the shape of the optimal intervention are proved using comparison results for elliptic equations via Schwarz and Steiner symmetrizations. Finally, some numerical simulations are performed in order to illustrate those results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11733v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Almeida, Aymeric Jacob de Cordemoy, Ayman Moussa, Nicolas Vauchelet</dc:creator>
    </item>
    <item>
      <title>Univariate representations of solutions to generic polynomial complementarity problems</title>
      <link>https://arxiv.org/abs/2410.21810</link>
      <description>arXiv:2410.21810v2 Announce Type: replace 
Abstract: By using the squared slack variables technique, we show that a general polynomial complementarity problem can be formulated as a system of polynomial equations. Thus, the solution set of such a problem is the image of a real algebraic set under a certain projection. This paper points out that, generically, this polynomial system has finitely many complex zeros. In such a case, we use techniques from symbolic computation to compute a univariate representation of the solution set. Consequently, univariate representations of special solutions, such as least-norm and sparse solutions, are obtained. After that, enumerating solutions boils down to solving problems governed by univariate polynomials. We also provide some experiments on small-scale problems with worst-case scenarios. At the end of the paper, we propose a method for computing approximate solutions to copositive polynomial complementarity problems that may have infinitely many solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21810v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vu Trung Hieu, Alfredo Noel Iusem, Paul Hugo Schm\"olling, Akiko Takeda</dc:creator>
    </item>
    <item>
      <title>Delayed Feedback in Online Non-Convex Optimization: A Non-Stationary Approach with Applications</title>
      <link>https://arxiv.org/abs/2412.14506</link>
      <description>arXiv:2412.14506v3 Announce Type: replace 
Abstract: We study non-convex delayed-noise online optimization problems by evaluating dynamic regret in the non-stationary setting when the loss functions are quasar-convex. In particular, we consider scenarios involving quasar-convex functions either with a Lipschitz gradient or weakly smooth and, for each case, we ensure bounded dynamic regret in terms of cumulative path variation achieving sub-linear regret rates. Furthermore, we illustrate the flexibility of our framework by applying it to both theoretical settings such as zeroth-order (bandit) and also to practical applications with quadratic fractional functions. Moreover, we provide new examples of non-convex functions that are quasar-convex by proving that the class of differentiable strongly quasiconvex functions (Polyak 1966) are strongly quasar-convex on convex compact sets. Finally, several numerical experiments validate our theoretical findings, illustrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14506v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Lara, Cristian Vega</dc:creator>
    </item>
    <item>
      <title>A review of minimum cost box searching games</title>
      <link>https://arxiv.org/abs/2502.10551</link>
      <description>arXiv:2502.10551v2 Announce Type: replace 
Abstract: We consider a class of zero-sum search games in which a Hider hides one or more target among a set of $n$ boxes. The boxes may require differing amount of time to search, and detection may be imperfect, so that there is a certain probability that a target may not be found when a box is searched, even when it is there. A Searcher must choose how to search the boxes sequentially, and wishes to minimize the expected time to find the target(s), whereas the Hider wishes to maximize this payoff. We review some known solutions to different cases of this game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10551v2</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Lidbetter</dc:creator>
    </item>
    <item>
      <title>On submodularity of the expected information gain</title>
      <link>https://arxiv.org/abs/2505.04145</link>
      <description>arXiv:2505.04145v2 Announce Type: replace 
Abstract: We consider finite-dimensional linear Gaussian Bayesian inverse problems with uncorrelated sensor measurements. In this setting, it is known that the expected information gain, quantified by the expected Kullback-Leibler divergence from the posterior measure to the prior measure, is submodular. We present a simple alternative proof of this fact tailored to a weighted inner product space setting arising from discretization of infinite-dimensional inverse problems constrained by partial differential equations (PDEs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04145v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steven Maio, Alen Alexanderian</dc:creator>
    </item>
    <item>
      <title>A Gradient-thresholding Algorithm for Sparse Regularization</title>
      <link>https://arxiv.org/abs/2006.03437</link>
      <description>arXiv:2006.03437v2 Announce Type: replace-cross 
Abstract: Inverse problems arise in a wide spectrum of applications in fields ranging from engineering to scientific computation. Connected with the rise of interest in inverse problems is the development and analysis of regularization methods, such as Tikhonov-type regularization methods or iterative regularization methods, which are a necessity in most of the inverse problems. In the last few decades, regularization methods motivating sparsity has been the focus of research, due to the high dimensionalty of the real-life data, and $\mathcal{L}^1$-regularization methods (such as LASSO or FISTA) has been in its center (due to their computational simplicity). In this paper we propose a new (semi-) iterative regularization method which is not only simpler than the mentioned algorithms but also yields better results, in terms of accuracy and sparsity of the recovered solution. Furthermore, we also present a very effective and practical stopping criterion to choose an appropriate regularization parameter (here, it's iteration index) so as to recover a regularized (sparse) solution. To illustrate the computational efficiency of this algorithm we apply it to numerically solve the image deblurring problem and compare our results with certain standard regularization methods, like total variation, FISTA, LSQR etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.03437v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abinash Nayak</dc:creator>
    </item>
    <item>
      <title>GreenLight-Gym: Reinforcement learning benchmark environment for control of greenhouse production systems</title>
      <link>https://arxiv.org/abs/2410.05336</link>
      <description>arXiv:2410.05336v2 Announce Type: replace-cross 
Abstract: This study presents GreenLight-Gym, a new, fast, open-source benchmark environment for developing reinforcement learning (RL) methods in greenhouse crop production control. Built on the state-of-the-art GreenLight model, it features a differentiable C++ implementation leveraging the CasADi framework for efficient numerical integration. GreenLight-Gym improves simulation speed by a factor of 17 over the original GreenLight implementation. A modular Python environment wrapper enables flexible configuration of control tasks and RL-based controllers. This flexibility is demonstrated by learning controllers under parametric uncertainty using two well-known RL algorithms. GreenLight-Gym provides a standardized benchmark for advancing RL methodologies and evaluating greenhouse control solutions under diverse conditions. The greenhouse control community is encouraged to use and extend this benchmark to accelerate innovation in greenhouse crop production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05336v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bart van Laatum, Eldert J. van Henten, Sjoerd Boersma</dc:creator>
    </item>
  </channel>
</rss>
