<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:03:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient Beam Selection for ISAC in Cell-Free Massive MIMO via Digital Twin-Assisted Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2506.18560</link>
      <description>arXiv:2506.18560v1 Announce Type: new 
Abstract: Beamforming enhances signal strength and quality by focusing energy in specific directions. This capability is particularly crucial in cell-free integrated sensing and communication (ISAC) systems, where multiple distributed access points (APs) collaborate to provide both communication and sensing services. In this work, we first derive the distribution of joint target detection probabilities across multiple receiving APs under false alarm rate constraints, and then formulate the beam selection procedure as a Markov decision process (MDP). We establish a deep reinforcement learning (DRL) framework, in which reward shaping and sinusoidal embedding are introduced to facilitate agent learning. To eliminate the high costs and associated risks of real-time agent-environment interactions, we further propose a novel digital twin (DT)-assisted offline DRL approach. Different from traditional online DRL, a conditional generative adversarial network (cGAN)-based DT module, operating as a replica of the real world, is meticulously designed to generate virtual state-action transition pairs and enrich data diversity, enabling offline adjustment of the agent's policy. Additionally, we address the out-of-distribution issue by incorporating an extra penalty term into the loss function design. The convergency of agent-DT interaction and the upper bound of the Q-error function are theoretically derived. Numerical results demonstrate the remarkable performance of our proposed approach, which significantly reduces online interaction overhead while maintaining effective beam selection across diverse conditions including strict false alarm control, low signal-to-noise ratios, and high target velocities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18560v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiexin Zhang, Shu Xu, Chunguo Li, Yongming Huang, Luxi Yang</dc:creator>
    </item>
    <item>
      <title>Optimal Operating Strategy for PV-BESS Households: Balancing Self-Consumption and Self-Sufficiency</title>
      <link>https://arxiv.org/abs/2506.17268</link>
      <description>arXiv:2506.17268v1 Announce Type: cross 
Abstract: High penetration of Photovoltaic (PV) generation and Battery Energy Storage System (BESS) in individual households increases the demand for solutions to determine the optimal PV generation power and the capacity of BESS. Self-consumption and self-sufficiency are essential for optimising the operation of PV-BESS systems in households, aiming to minimise power import from and export to the main grid. However, self-consumption and self-sufficiency are not independent; they share a linear relationship. This paper demonstrates this relationship and proposes an optimal operating strategy that considers power generation and consumption profiles to maximise self-consumption and self-sufficiency in households equipped with a PV-BESS. We classify self-consumption and self-sufficiency patterns into four categories based on the ratio of self-sufficiency to self-consumption for each household and determine the optimal PV generation and BESS capacities using both a mathematical calculation and this ratio. These optimal operation values for each category are then simulated using Model Predictive Control (MPC) and Reinforcement Learning (RL)-based battery charging and discharging scheduling models. The results show that the ratio between self-consumption and self-sufficiency is a useful metric for determining the optimal capacity of PV-BESS systems to maximise the local utilisation of PV-generated power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17268v1</guid>
      <category>eess.SY</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Wook Heo, Raja Jurdak, Sara Khalifa</dc:creator>
    </item>
    <item>
      <title>Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation</title>
      <link>https://arxiv.org/abs/2506.17442</link>
      <description>arXiv:2506.17442v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is increasingly integrated into modern healthcare, offering powerful support for clinical decision-making. However, in real-world settings, AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes. This review presents a forward-looking perspective on monitoring and maintaining the "health" of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms. The paper begins by reviewing common causes of performance degradation at both data and model levels. We then summarize key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis. Correction strategies are further reviewed, ranging from model retraining to test-time adaptation. Our survey spans both traditional machine learning models and state-of-the-art large language models (LLMs), offering insights into their strengths and limitations. Finally, we discuss ongoing technical challenges and propose future research directions. This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17442v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Guan, David Bates, Li Zhou</dc:creator>
    </item>
    <item>
      <title>Mapping the Evolution of Research Contributions using KnoVo</title>
      <link>https://arxiv.org/abs/2506.17508</link>
      <description>arXiv:2506.17508v1 Announce Type: cross 
Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework designed for quantifying and analyzing the evolution of research novelty in the scientific literature. Moving beyond traditional citation analysis, which primarily measures impact, KnoVo determines a paper's novelty relative to both prior and subsequent work within its multilayered citation network. Given a target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to dynamically extract dimensions of comparison (e.g., methodology, application, dataset). The target paper is then compared to related publications along these same extracted dimensions. This comparative analysis, inspired by tournament selection, yields quantitative novelty scores reflecting the relative improvement, equivalence, or inferiority of the target paper in specific aspects. By aggregating these scores and visualizing their progression, for instance, through dynamic evolution graphs and comparative radar charts, KnoVo facilitates researchers not only to assess originality and identify similar work, but also to track knowledge evolution along specific research dimensions, uncover research gaps, and explore cross-disciplinary connections. We demonstrate these capabilities through a detailed analysis of 20 diverse papers from multiple scientific fields and report on the performance of various open-source LLMs within the KnoVo framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17508v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajratul Y. Rubaiat, Syed N. Sakib, Hasan M. Jamil</dc:creator>
    </item>
    <item>
      <title>Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models</title>
      <link>https://arxiv.org/abs/2506.17580</link>
      <description>arXiv:2506.17580v1 Announce Type: cross 
Abstract: The exponential growth of scientific literature challenges researchers extracting and synthesizing knowledge. Traditional search engines return many sources without direct, detailed answers, while general-purpose LLMs may offer concise responses that lack depth or omit current information. LLMs with search capabilities are also limited by context window, yielding short, incomplete answers. This paper introduces WISE (Workflow for Intelligent Scientific Knowledge Extraction), a system addressing these limits by using a structured workflow to extract, refine, and rank query-specific knowledge. WISE uses an LLM-powered, tree-based architecture to refine data, focusing on query-aligned, context-aware, and non-redundant information. Dynamic scoring and ranking prioritize unique contributions from each source, and adaptive stopping criteria minimize processing overhead. WISE delivers detailed, organized answers by systematically exploring and synthesizing knowledge from diverse sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information. We also explore how the WISE workflow can be adapted for diverse domains like drug discovery, material science, and social science, enabling efficient knowledge extraction and synthesis from unstructured scientific papers and web sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17580v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajratul Y. Rubaiat, Hasan M. Jamil</dc:creator>
    </item>
    <item>
      <title>Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks</title>
      <link>https://arxiv.org/abs/2506.17672</link>
      <description>arXiv:2506.17672v1 Announce Type: cross 
Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride requests based on factors such as order characteristics, traffic conditions, and personal preferences. Accurately predicting these decisions is essential for improving the efficiency and reliability of these systems. Traditional models, such as the Random Utility Maximization (RUM) approach, typically predict drivers' decisions by assuming linear correlations among attributes. However, these models often fall short because they fail to account for non-linear interactions between attributes and do not cater to the unique, personalized preferences of individual drivers. In this paper, we develop a method for learning personalized utility functions using hypernetwork and ensemble learning. Hypernetworks dynamically generate weights for a linear utility function based on trip request data and driver profiles, capturing the non-linear relationships. An ensemble of hypernetworks trained on different data segments further improve model adaptability and generalization by introducing controlled randomness, thereby reducing over-fitting. We validate the performance of our ensemble hypernetworks model in terms of prediction accuracy and uncertainty estimation in a real-world dataset. The results demonstrate that our approach not only accurately predicts each driver's utility but also effectively balances the needs for explainability and uncertainty quantification. Additionally, our model serves as a powerful tool for revealing the personalized preferences of different drivers, clearly illustrating which attributes largely impact their rider acceptance decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17672v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiming Mai, Jie Gao, Oded Cats</dc:creator>
    </item>
    <item>
      <title>LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation</title>
      <link>https://arxiv.org/abs/2506.17865</link>
      <description>arXiv:2506.17865v1 Announce Type: cross 
Abstract: Ensuring the security of modern System-on-Chip (SoC) designs poses significant challenges due to increasing complexity and distributed assets across the intellectual property (IP) blocks. Formal property verification (FPV) provides the capability to model and validate design behaviors through security properties with model checkers; however, current practices require significant manual efforts to create such properties, making them time-consuming, costly, and error-prone. The emergence of Large Language Models (LLMs) has showcased remarkable proficiency across diverse domains, including HDL code generation and verification tasks. Current LLM-based techniques often produce vacuous assertions and lack efficient prompt generation, comprehensive verification, and bug detection. This paper presents LASA, a novel framework that leverages LLMs and retrieval-augmented generation (RAG) to produce non-vacuous security properties and SystemVerilog Assertions (SVA) from design specifications and related documentation for bus-based SoC designs. LASA integrates commercial EDA tool for FPV to generate coverage metrics and iteratively refines prompts through a feedback loop to enhance coverage. The effectiveness of LASA is validated through various open-source SoC designs, demonstrating high coverage values with an average of ~88\%, denoting comprehensive verification through efficient generation of security properties and SVAs. LASA also demonstrates bug detection capabilities, identifying five unique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17865v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dinesh Reddy Ankireddy, Sudipta Paria, Aritra Dasgupta, Sandip Ray, Swarup Bhunia</dc:creator>
    </item>
    <item>
      <title>A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction</title>
      <link>https://arxiv.org/abs/2506.18424</link>
      <description>arXiv:2506.18424v1 Announce Type: cross 
Abstract: In the design process of the analog circuit pre-layout phase, device sizing is an important step in determining whether an analog circuit can meet the required performance metrics. Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space. To alleviate this problem, we propose a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers. The search space in the sizing process can be effectively pruned based on the sizing relationship extracted by this framework. Eventually, we conducted tests on 3 types of circuits, and the optimization efficiency was improved by $2.32 \sim 26.6 \times$. This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18424v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chengjie Liu, Weiyu Chen, Huiyao Xu, Yuan Du, Jun Yang, Li Du</dc:creator>
    </item>
    <item>
      <title>End-Edge Model Collaboration: Bandwidth Allocation for Data Upload and Model Transmission</title>
      <link>https://arxiv.org/abs/2504.14310</link>
      <description>arXiv:2504.14310v2 Announce Type: replace 
Abstract: The widespread adoption of large artificial intelligence (AI) models has enabled numerous applications of the Internet of Things (IoT). However, large AI models require substantial computational and memory resources, which exceed the capabilities of resource-constrained IoT devices. End-edge collaboration paradigm is developed to address this issue, where a small model on the end device performs inference tasks, while a large model on the edge server assists with model updates. To improve the accuracy of the inference tasks, the data generated on the end devices will be periodically uploaded to edge server to update model, and a distilled model of the updated one will be transmitted back to the end device. Subjected to the limited bandwidth for the communication link between the end device and the edge server, it is important to investigate whether the system should allocate more bandwidth to data upload or to model transmission. In this paper, we characterize the impact of data upload and model transmission on inference accuracy. Subsequently, we formulate a bandwidth allocation problem. By solving this problem, we derive an efficient optimization framework for the end-edge collaboration system. The simulation results demonstrate our framework significantly enhances mean average precision (mAP) under various bandwidths and datasizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14310v2</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dailin Yang, Shuhang Zhang, Hongliang Zhang, Lingyang Song</dc:creator>
    </item>
    <item>
      <title>GainSight: Application-Guided Profiling for Composing Heterogeneous On-Chip Memories in AI Hardware Accelerators</title>
      <link>https://arxiv.org/abs/2504.14866</link>
      <description>arXiv:2504.14866v3 Announce Type: replace-cross 
Abstract: As AI workloads drive soaring memory requirements, higher-density on-chip memory is needed for domain-specific accelerators beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, little work has incorporated dynamic application profiles into these design decisions, and no existing tools are expressly designed for this purpose. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and data lifetimes in domain-specific accelerators. By instrumenting retargetable architectural simulator backends with application- and device-agnostic analytical frontends, GainSight aligns workload-specific traffic and lifetime metrics with mockups of emerging memory devices, informing system-level heterogeneous memory design. We also present a set of case studies on MLPerf Inference and PolyBench workloads using simulated GPU and systolic array architectures, highlighting the utility of GainSight and the insights it provides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory arrays that augment SRAM with GCRAM can reduce active energy consumption by up to 66.8%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14866v3</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peijing Li, Matthew Hung, Yiming Tan, Konstantin Ho{\ss}feld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe</dc:creator>
    </item>
  </channel>
</rss>
