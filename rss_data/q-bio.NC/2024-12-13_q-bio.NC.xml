<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2024 05:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Vision CNNs trained to estimate spatial latents learned similar ventral-stream-aligned representations</title>
      <link>https://arxiv.org/abs/2412.09115</link>
      <description>arXiv:2412.09115v1 Announce Type: new 
Abstract: Studies of the functional role of the primate ventral visual stream have traditionally focused on object categorization, often ignoring -- despite much prior evidence -- its role in estimating "spatial" latents such as object position and pose. Most leading ventral stream models are derived by optimizing networks for object categorization, which seems to imply that the ventral stream is also derived under such an objective. Here, we explore an alternative hypothesis: Might the ventral stream be optimized for estimating spatial latents? And a closely related question: How different -- if at all -- are representations learned from spatial latent estimation compared to categorization? To ask these questions, we leveraged synthetic image datasets generated by a 3D graphic engine and trained convolutional neural networks (CNNs) to estimate different combinations of spatial and category latents. We found that models trained to estimate just a few spatial latents achieve neural alignment scores comparable to those trained on hundreds of categories, and the spatial latent performance of models strongly correlates with their neural alignment. Spatial latent and category-trained models have very similar -- but not identical -- internal representations, especially in their early and middle layers. We provide evidence that this convergence is partly driven by non-target latent variability in the training data, which facilitates the implicit learning of representations of those non-target latents. Taken together, these results suggest that many training objectives, such as spatial latents, can lead to similar models aligned neurally with the ventral stream. Thus, one should not assume that the ventral stream is optimized for object categorization only. As a field, we need to continue to sharpen our measures of comparing models to brains to better understand the functional roles of the ventral stream.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09115v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo</dc:creator>
    </item>
    <item>
      <title>Network Dynamics of Emotional Processing: A Structural Balance Theory Approach</title>
      <link>https://arxiv.org/abs/2412.09554</link>
      <description>arXiv:2412.09554v1 Announce Type: new 
Abstract: Understanding emotional processing in the human brain requires examining the complex interactions between different brain regions. While previous studies have identified specific regions involved in emotion processing, a holistic network approach may provide deeper insights. We use Structural Balance Theory to investigate the stability and triadic structures of signed brain networks during resting state and emotional processing, specifically in response to fear-related stimuli. We hypothesized that imbalanced triadic interactions would be more prevalent during emotional processing, especially in response to fear-related stimuli, potentially reflecting the brain's adaptation to emotional challenges. By analyzing fMRI data from 138 healthy, right-handed participants, we found that emotional processing was marked by an increase in positive connections and a decrease in negative connections compared to the resting state. Our findings clearly show that balanced triads significantly decreased while imbalanced triads increased, indicating a shift toward instability in the brain's functional network during emotional processing. Additionally, the number of influential hubs was significantly lower during fear processing than in neutral conditions, suggesting a more centralized network and higher levels of network energy. These findings reveal the brain's remarkable adaptive capacity during emotional processing, demonstrating how network stability dynamically shifts through changes in balanced and imbalanced triads, hub tendencies, and energy dynamics. Our research illuminates a complex mechanism by which the brain flexibly reconfigures its functional network in response to emotional stimuli with potential implications for understanding emotional resilience and neurological disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09554v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sepehr Gourabi, Parinaz Khosravani, Shahrzad Nosrat, Roya Mohammadi, Masoud Lotfalipour</dc:creator>
    </item>
    <item>
      <title>DeepNose: An Equivariant Convolutional Neural Network Predictive Of Human Olfactory Percepts</title>
      <link>https://arxiv.org/abs/2412.08747</link>
      <description>arXiv:2412.08747v1 Announce Type: cross 
Abstract: The olfactory system employs responses of an ensemble of odorant receptors (ORs) to sense molecules and to generate olfactory percepts. Here we hypothesized that ORs can be viewed as 3D spatial filters that extract molecular features relevant to the olfactory system, similarly to the spatio-temporal filters found in other sensory modalities. To build these filters, we trained a convolutional neural network (CNN) to predict human olfactory percepts obtained from several semantic datasets. Our neural network, the DeepNose, produced responses that are approximately invariant to the molecules' orientation, due to its equivariant architecture. Our network offers high-fidelity perceptual predictions for different olfactory datasets. In addition, our approach allows us to identify molecular features that contribute to specific perceptual descriptors. Because the DeepNose network is designed to be aligned with the biological system, our approach predicts distinct perceptual qualities for different stereoisomers. The architecture of the DeepNose relying on the processing of several molecules at the same time permits inferring the perceptual quality of odor mixtures. We propose that the DeepNose network can use 3D molecular shapes to generate high-quality predictions for human olfactory percepts and help identify molecular features responsible for odor quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08747v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergey Shuvaev, Khue Tran, Khristina Samoilova, Cyrille Mascart, Alexei Koulakov</dc:creator>
    </item>
    <item>
      <title>Brain-inspired AI Agent: The Way Towards AGI</title>
      <link>https://arxiv.org/abs/2412.08875</link>
      <description>arXiv:2412.08875v1 Announce Type: cross 
Abstract: Artificial General Intelligence (AGI), widely regarded as the fundamental goal of artificial intelligence, represents the realization of cognitive capabilities that enable the handling of general tasks with human-like proficiency. Researchers in brain-inspired AI seek inspiration from the operational mechanisms of the human brain, aiming to replicate its functional rules in intelligent models. Moreover, with the rapid development of large-scale models in recent years, the concept of agents has garnered increasing attention, with researchers widely recognizing it as a necessary pathway toward achieving AGI. In this article, we propose the concept of a brain-inspired AI agent and analyze how to extract relatively feasible and agent-compatible cortical region functionalities and their associated functional connectivity networks from the complex mechanisms of the human brain. Implementing these structures within an agent enables it to achieve basic cognitive intelligence akin to human capabilities. Finally, we explore the limitations and challenges for realizing brain-inspired agents and discuss their future development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08875v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Yu, Jiangning Wei, Minzhen Hu, Zejie Han, Tianjian Zou, Ye He, Jun Liu</dc:creator>
    </item>
    <item>
      <title>The Parameters of Educability</title>
      <link>https://arxiv.org/abs/2412.09480</link>
      <description>arXiv:2412.09480v1 Announce Type: cross 
Abstract: The educability model is a computational model that has been recently proposed to describe the cognitive capability that makes humans unique among existing biological species on Earth in being able to create advanced civilizations. Educability is defined as a capability for acquiring and applying knowledge. It is intended both to describe human capabilities and, equally, as an aspirational description of what can be usefully realized by machines. While the intention is to have a mathematically well-defined computational model, in constructing an instance of the model there are a number of decisions to make. We call these decisions {\it parameters}. In a standard computer, two parameters are the memory capacity and clock rate. There is no universally optimal choice for either one, or even for their ratio. Similarly, in a standard machine learning system, two parameters are the learning algorithm and the dataset used for training. Again, there are no universally optimal choices known for either. An educable system has many more parameters than either of these two kinds of system. This short paper discusses some of the main parameters of educable systems, and the broader implications of their existence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09480v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leslie G. Valiant</dc:creator>
    </item>
    <item>
      <title>Metastability in networks of nonlinear stochastic integrate-and-fire neurons</title>
      <link>https://arxiv.org/abs/2406.07445</link>
      <description>arXiv:2406.07445v2 Announce Type: replace 
Abstract: Neurons in the brain continuously process the barrage of sensory inputs they receive from the environment. A wide array of experimental work has shown that the collective activity of neural populations encodes and processes this constant bombardment of information. How these collective patterns of activity depend on single-neuron properties is often unclear. Single-neuron recordings have shown that individual neurons' responses to inputs are nonlinear, which prevents a straightforward extrapolation from single neuron features to emergent collective states. Here, we use a field-theoretic formulation of a stochastic leaky integrate-and-fire model to study the impact of single-neuron nonlinearities on macroscopic network activity. In this model, a neuron integrates spiking output from other neurons in its membrane voltage and emits spikes stochastically with an intensity depending on the membrane voltage, after which the voltage resets. We show that the interplay between nonlinear spike intensity functions and membrane potential resets can i) give rise to metastable active firing rate states in recurrent networks, and ii) can enhance or suppress mean firing rates and membrane potentials in the same or paradoxically opposite directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07445v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Paliwal, Gabriel Koch Ocker, Braden A. W. Brinkman</dc:creator>
    </item>
    <item>
      <title>More variable circadian rhythms in epilepsy: a retrospective cross-sectional study using long-term heart rate recordings from wearable sensors</title>
      <link>https://arxiv.org/abs/2411.04634</link>
      <description>arXiv:2411.04634v3 Announce Type: replace 
Abstract: Background: The circadian rhythm aligns physiology and behaviour with the 24-hour light-dark cycle, and its disruption is linked to neurological disorders such as epilepsy. However, how to best quantify circadian disruption remains unclear, as it can manifest across various properties and timescales. A promising but under-explored approach is to assess the intra-individual variability in circadian rhythms over timescales of weeks to years. This is yet to be studied in epilepsy.
  Methods: We retrospectively used wearable smartwatch data (Fitbit) from 143 people with epilepsy (PWE) and 31 controls. For each participant, we extracted the circadian oscillation underlying their heart rate time series and analysed the intra-individual variability of three circadian properties: period, acrophase, and amplitude.
  Findings: We found increased intra-individual variability in period (77 min vs. 62 min, z=3.32, p&lt;0.001) and acrophase (68 min vs. 54 min, z=2.97, p=0.003) for PWE compared to controls, but not in amplitude (1.98 bpm vs. 2.05 bpm, z=-0.66, p=0.51). For PWE, we did not find any correlations between seizure frequency and intra-individual variability in circadian properties, or any difference between weeks with and without seizures.
  Interpretation: This finding indicates that the circadian rhythm of heart rate is more variable for people with epilepsy and that this can be detected using a wearable device. However, we were unable to find any associations with seizure frequency or occurrence, suggesting intra-individual variability could be another manifestation of epilepsy aetiology. Future work should investigate the combined role of anti-seizure medications, demographics, co-morbidities, and health behaviours in driving the increased intra-individual variability of circadian properties in epilepsy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04634v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy C. Smith, Christopher Thornton, Rachel E. Stirling, Guillermo M. Besne, Peter N. Taylor, Philippa J. Karoly, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>Gradient Diffusion: Enhancing Multicompartmental Neuron Models for Gradient-Based Self-Tuning and Homeostatic Control</title>
      <link>https://arxiv.org/abs/2412.07327</link>
      <description>arXiv:2412.07327v2 Announce Type: replace 
Abstract: Realistic brain models contain many parameters. Traditionally, gradient-free methods are used for estimating these parameters, but gradient-based methods offer many advantages including scalability. However, brain models are tied to existing brain simulators, which do not support gradient calculation. Here we show how to extend -- within the public interface of such simulators -- these neural models to also compute the gradients with respect to their parameters. We demonstrate that the computed gradients can be used to optimize a biophysically realistic multicompartmental neuron model with the gradient-based Adam optimizer. Beyond tuning, gradient-based optimization could lead the way towards dynamics learning and homeostatic control within simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07327v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lennart P. L. Landsmeer, Mario Negrello, Said Hamdioui, Christos Strydis</dc:creator>
    </item>
    <item>
      <title>Differential learning kinetics govern the transition from memorization to generalization during in-context learning</title>
      <link>https://arxiv.org/abs/2412.00104</link>
      <description>arXiv:2412.00104v2 Announce Type: replace-cross 
Abstract: Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative rates at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00104v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Nguyen, Gautam Reddy</dc:creator>
    </item>
  </channel>
</rss>
