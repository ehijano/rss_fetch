<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Apr 2025 01:44:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Cohort Revenue &amp; Retention Analysis: A Bayesian Approach</title>
      <link>https://arxiv.org/abs/2504.16216</link>
      <description>arXiv:2504.16216v1 Announce Type: new 
Abstract: We present a Bayesian approach to model cohort-level retention rates and revenue over time. We use Bayesian additive regression trees (BART) to model the retention component which we couple with a linear model for the revenue component. This method is flexible enough to allow adding additional covariates to both model components. This Bayesian framework allows us to quantify uncertainty in the estimation, understand the effect of covariates on retention through partial dependence plots (PDP) and individual conditional expectation (ICE) plots, and most importantly, forecast future revenue and retention rates with well-calibrated uncertainty through highest density intervals. We also provide alternative approaches to model the retention component using neural networks and inference through stochastic variational inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16216v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Camilo Orduz</dc:creator>
    </item>
    <item>
      <title>Toward a Principled Workflow for Prevalence Mapping Using Household Survey Data</title>
      <link>https://arxiv.org/abs/2504.16435</link>
      <description>arXiv:2504.16435v1 Announce Type: new 
Abstract: Understanding the prevalence of key demographic and health indicators in small geographic areas and domains is of global interest, especially in low- and middle-income countries (LMICs), where vital registration data is sparse and household surveys are the primary source of information. Recent advances in computation and the increasing availability of spatially detailed datasets have led to much progress in sophisticated statistical modeling of prevalence. As a result, high-resolution prevalence maps for many indicators are routinely produced in the literature. However, statistical and practical guidance for producing prevalence maps in LMICs has been largely lacking. In particular, advice in choosing and evaluating models and interpreting results is needed, especially when data is limited. Software and analysis tools are also usually inaccessible to researchers in low-resource settings to conduct their own analysis or reproduce findings in the literature. In this paper, we propose a general workflow for prevalence mapping using household survey data. We consider all stages of the analysis pipeline, with particular emphasis on model choice and interpretation. We illustrate the proposed workflow using a case study mapping the proportion of pregnant women who had at least four antenatal care visits in Kenya. Reproducible code is provided in the Supplementary Materials and can be readily extended to a broad collection of indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16435v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianyu Dong, Yunhan Wu, Zehang Richard Li, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Robust Causal Inference for EHR-based Studies of Point Exposures with Missingness in Eligibility Criteria</title>
      <link>https://arxiv.org/abs/2504.16230</link>
      <description>arXiv:2504.16230v1 Announce Type: cross 
Abstract: Missingness in variables that define study eligibility criteria is a seldom addressed challenge in electronic health record (EHR)-based settings. It is typically the case that patients with incomplete eligibility information are excluded from analysis without consideration of (implicit) assumptions that are being made, leaving study conclusions subject to potential selection bias. In an effort to ascertain eligibility for more patients, researchers may look back further in time prior to study baseline, and in using outdated values of eligibility-defining covariates may inappropriately be including individuals who, unbeknownst to the researcher, fail to meet eligibility at baseline. To the best of our knowledge, however, very little work has been done to mitigate these concerns. We propose a robust and efficient estimator of the causal average treatment effect on the treated, defined in the study eligible population, in cohort studies where eligibility-defining covariates are missing at random. The approach facilitates the use of flexible machine-learning strategies for component nuisance functions while maintaining appropriate convergence rates for valid asymptotic inference. EHR data from Kaiser Permanente are used as motivation as well as a basis for extensive simulations that verify robustness properties under various degrees of model misspecification. The data are also used to demonstrate the use of the method to analyze differences between two common bariatric surgical interventions for long-term weight and glycemic outcomes among a cohort of severely obese patients with type II diabetes mellitus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16230v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Rui Wang, David Arterburn, Heidi Fischer, Catherine Lee, Susan M. Shortreed, Sebastien Haneuse, Alexander W. Levis</dc:creator>
    </item>
    <item>
      <title>Detecting Correlation between Multiple Unlabeled Gaussian Networks</title>
      <link>https://arxiv.org/abs/2504.16279</link>
      <description>arXiv:2504.16279v1 Announce Type: cross 
Abstract: This paper studies the hypothesis testing problem to determine whether m &gt; 2 unlabeled graphs with Gaussian edge weights are correlated under a latent permutation. Previously, a sharp detection threshold for the correlation parameter \rho was established by Wu, Xu and Yu for this problem when m = 2. Presently, their result is leveraged to derive necessary and sufficient conditions for general m. In doing so, an interval for \rho is uncovered for which detection is impossible using 2 graphs alone but becomes possible with m &gt; 2 graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16279v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha Ameen, Bruce Hajek</dc:creator>
    </item>
    <item>
      <title>Online model learning with data-assimilated reservoir computers</title>
      <link>https://arxiv.org/abs/2504.16767</link>
      <description>arXiv:2504.16767v1 Announce Type: cross 
Abstract: We propose an online learning framework for forecasting nonlinear spatio-temporal signals (fields). The method integrates (i) dimensionality reduction, here, a simple proper orthogonal decomposition (POD) projection; (ii) a generalized autoregressive model to forecast reduced dynamics, here, a reservoir computer; (iii) online adaptation to update the reservoir computer (the model), here, ensemble sequential data assimilation.We demonstrate the framework on a wake past a cylinder governed by the Navier-Stokes equations, exploring the assimilation of full flow fields (projected onto POD modes) and sparse sensors. Three scenarios are examined: a na\"ive physical state estimation; a two-fold estimation of physical and reservoir states; and a three-fold estimation that also adjusts the model parameters. The two-fold strategy significantly improves ensemble convergence and reduces reconstruction error compared to the na\"ive approach. The three-fold approach enables robust online training of partially-trained reservoir computers, overcoming limitations of a priori training. By unifying data-driven reduced order modelling with Bayesian data assimilation, this work opens new opportunities for scalable online model learning for nonlinear time series forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16767v1</guid>
      <category>cs.LG</category>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea N\'ovoa, Luca Magri</dc:creator>
    </item>
    <item>
      <title>MLOps Monitoring at Scale for Digital Platforms</title>
      <link>https://arxiv.org/abs/2504.16789</link>
      <description>arXiv:2504.16789v1 Announce Type: cross 
Abstract: Machine learning models are widely recognized for their strong performance in forecasting. To keep that performance in streaming data settings, they have to be monitored and frequently re-trained. This can be done with machine learning operations (MLOps) techniques under supervision of an MLOps engineer. However, in digital platform settings where the number of data streams is typically large and unstable, standard monitoring becomes either suboptimal or too labor intensive for the MLOps engineer. As a consequence, companies often fall back on very simple worse performing ML models without monitoring. We solve this problem by adopting a design science approach and introducing a new monitoring framework, the Machine Learning Monitoring Agent (MLMA), that is designed to work at scale for any ML model with reasonable labor cost. A key feature of our framework concerns test-based automated re-training based on a data-adaptive reference loss batch. The MLOps engineer is kept in the loop via key metrics and also acts, pro-actively or retrospectively, to maintain performance of the ML model in the production stage. We conduct a large-scale test at a last-mile delivery platform to empirically validate our monitoring framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16789v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Jeffrey Hu, Jeroen Rombouts, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Lead Times in Flux: Analyzing Airbnb Booking Dynamics During Global Upheavals (2018-2022)</title>
      <link>https://arxiv.org/abs/2501.10535</link>
      <description>arXiv:2501.10535v2 Announce Type: replace 
Abstract: Short-term shifts in booking behaviors can disrupt forecasting in the travel and hospitality industry, especially during global crises. Traditional metrics like average or median lead times often overlook important distribution changes. This study introduces a normalized L1 (Manhattan) distance to assess Airbnb booking lead time divergences from 2018 to 2022, focusing on the COVID-19 pandemic across four major U.S. cities. We identify a two-phase disruption: an abrupt change at the pandemic's onset followed by partial recovery with persistent deviations from pre-2018 patterns. Our method reveals changes in travelers' planning horizons that standard statistics miss, highlighting the need to analyze the entire lead-time distribution for more accurate demand forecasting and pricing strategies. The normalized L1 metric provides valuable insights for tourism stakeholders navigating ongoing market volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10535v2</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Erica Savage, Peter Coles</dc:creator>
    </item>
    <item>
      <title>On misconceptions about the Brier score in binary prediction models</title>
      <link>https://arxiv.org/abs/2504.04906</link>
      <description>arXiv:2504.04906v3 Announce Type: replace 
Abstract: The Brier score is a widely used metric evaluating overall performance of predictions for binary outcome probabilities in clinical research. However, its interpretation can be complex, as it does not align with commonly taught concepts in medical statistics. Consequently, the Brier score is often misinterpreted, sometimes to a significant extent, a fact that has not been adequately addressed in the literature. This commentary aims to explore prevalent misconceptions surrounding the Brier score and elucidate the reasons these interpretations are incorrect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04906v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linard Hoessly</dc:creator>
    </item>
    <item>
      <title>Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Auxiliary Data</title>
      <link>https://arxiv.org/abs/2204.10736</link>
      <description>arXiv:2204.10736v2 Announce Type: replace-cross 
Abstract: Evidence-based policy-making requires reliable, spatially disaggregated indicators. The framework of mixed effects random forests leverages the advantages of random forests and hierarchical data in small area estimation. These methods require typically access to auxiliary information on population-level, which is a strong limitation for practitioners. In contrast, our proposed method - for point and uncertainty estimation - abstains from access to unitlevel population data but adaptively incorporates aggregated auxiliary information through calibration-weights. We demonstrate its usage for estimating opportunity cost of care work for Germany from the Socio-Economic Panel and census aggregates. Simulation studies evaluate our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10736v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Krennmair, Nora W\"urz, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v2 Announce Type: replace-cross 
Abstract: Economists often estimate treatment effects in experiments using remotely sensed variables (RSVs), e.g. satellite images or mobile phone activity, in place of directly measured economic outcomes. A common practice is to use an observational sample to train a predictor of the economic outcome from the RSV, and then to use its predictions as the outcomes in the experiment. We show that this method is biased whenever the RSV is post-outcome, i.e. if variation in the economic outcome causes variation in the RSV. In program evaluation, changes in poverty or environmental quality cause changes in satellite images, but not vice versa. As our main result, we nonparametrically identify the treatment effect by formalizing the intuition that underlies common practice: the conditional distribution of the RSV given the outcome and treatment is stable across the samples.Based on our identifying formula, we find that the efficient representation of RSVs for causal inference requires three predictions rather than one. Valid inference does not require any rate conditions on RSV predictions, justifying the use of complex deep learning algorithms with unknown statistical properties. We re-analyze the effect of an anti-poverty program in India using satellite images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v3 Announce Type: replace-cross 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For hierarchical multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. Finally, we validate our approach using synthetic data and on CRASH-3, a major clinical trial focused on assessing the effects of tranexamic acid in patients with traumatic brain injury.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>On the Proportional Principal Stratum Hazards Model</title>
      <link>https://arxiv.org/abs/2503.10481</link>
      <description>arXiv:2503.10481v2 Announce Type: replace-cross 
Abstract: In clinical trials involving both mortality and morbidity, an active treatment can influence the observed risk of the first non-fatal event either directly, through its effect on the underlying non-fatal event process, or indirectly, through its effect on the death process, or both. Discerning the direct effect of treatment on the underlying first non-fatal event process holds clinical interest. However, with the competing risk of death, the Cox proportional hazards model that treats death as non-informative censoring and evaluates treatment effects on time to the first non-fatal event provides an estimate of the cause-specific hazard ratio, which may not correspond to the direct effect. To obtain the direct effect on the underlying first non-fatal event process, within the principal stratification framework, we define the principal stratum hazard and introduce the Proportional Principal Stratum Hazards model. This model estimates the principal stratum hazard ratio, which reflects the direct effect on the underlying first non-fatal event process in the presence of death and simplifies to the hazard ratio in the absence of death. The principal stratum membership is identified probabilistically using the shared frailty model, which assumes independence between the first non-fatal event process and the potential death processes, conditional on per-subject random frailty. Simulation studies are conducted to verify the reliability of our estimators. We illustrate the method using the Carvedilol Prospective Randomized Cumulative Survival trial, which involves heart-failure events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10481v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Thomas D. Cook</dc:creator>
    </item>
    <item>
      <title>On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction</title>
      <link>https://arxiv.org/abs/2504.08169</link>
      <description>arXiv:2504.08169v3 Announce Type: replace-cross 
Abstract: The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including:
  First, we propose a multitask learning framework with DHEN as the single backbone model architecture to predict all CVR tasks, with a detailed study on how to make DHEN work effectively in practice; Second, we build both on-site real-time user behavior sequences and off-site conversion event sequences for CVR prediction purposes, and conduct ablation study on its importance; Last but not least, we propose a self-supervised auxiliary loss to predict future actions in the input sequence, to help resolve the label sparseness issue in CVR prediction.
  Our method achieves state-of-the-art performance compared to previous single feature crossing modules with pre-trained user personalization features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08169v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Zhifang Liu, Qifei Shen, Aayush Mudgal, Caleb Lu, Jie Liu, Hongda Shen</dc:creator>
    </item>
  </channel>
</rss>
