<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 04:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>rodeo: Probabilistic Methods of Parameter Inference for Ordinary Differential Equations</title>
      <link>https://arxiv.org/abs/2506.21776</link>
      <description>arXiv:2506.21776v1 Announce Type: new 
Abstract: Parameter estimation for ordinary differential equations (ODEs) plays a fundamental role in the analysis of dynamical systems. Generally lacking closed-form solutions, ODEs are traditionally approximated using deterministic solvers. However, there is a growing body of evidence to suggest that probabilistic ODE solvers produce more reliable parameter estimates by better accounting for numerical uncertainty. Here we present rodeo, a Python library providing a fast, lightweight, and extensible interface to a broad class of probabilistic ODE solvers, along with several associated methods for parameter inference. At its core, rodeo provides a probabilistic solver that scales linearly in both the number of evaluation points and system variables. Furthermore, by leveraging state-of-the-art automatic differentiation (AD) and just-in-time (JIT) compiling techniques, rodeo is shown across several examples to provide fast, accurate, and scalable parameter inference for a variety of ODE systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21776v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohan Wu, Martin Lysy</dc:creator>
    </item>
    <item>
      <title>Explainable anomaly detection for sound spectrograms using pooling statistics with quantile differences</title>
      <link>https://arxiv.org/abs/2506.21921</link>
      <description>arXiv:2506.21921v1 Announce Type: cross 
Abstract: Anomaly detection is the task of identifying rarely occurring (i.e. anormal or anomalous) samples that differ from almost all other samples in a dataset. As the patterns of anormal samples are usually not known a priori, this task is highly challenging. Consequently, anomaly detection lies between semi- and unsupervised learning. The detection of anomalies in sound data, often called 'ASD' (Anomalous Sound Detection), is a sub-field that deals with the identification of new and yet unknown effects in acoustic recordings. It is of great importance for various applications in Industry 4.0. Here, vibrational or acoustic data are typically obtained from standard sensor signals used for predictive maintenance. Examples cover machine condition monitoring or quality assurance to track the state of components or products. However, the use of intelligent algorithms remains a controversial topic. Management generally aims for cost-reduction and automation, while quality and maintenance experts emphasize the need for human expertise and comprehensible solutions. In this work, we present an anomaly detection approach specifically designed for spectrograms. The approach is based on statistical evaluations and is theoretically motivated. In addition, it features intrinsic explainability, making it particularly suitable for applications in industrial settings. Thus, this algorithm is of relevance for applications in which black-box algorithms are unwanted or unsuitable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21921v1</guid>
      <category>stat.AP</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicolas Thewes, Philipp Steinhauer, Patrick Trampert, Markus Pauly, Georg Schneider</dc:creator>
    </item>
    <item>
      <title>Closed-form approximations of the two-sample Pearson Bayes factor</title>
      <link>https://arxiv.org/abs/2310.11313</link>
      <description>arXiv:2310.11313v2 Announce Type: replace 
Abstract: In this paper, I present three closed-form approximations of the two-sample Pearson Bayes factor, a recently developed index of evidential value for data in two-group designs. The techniques rely on some classical asymptotic results about Gamma functions. These approximations permit simple closed-form calculation of the Pearson Bayes factor in cases where only minimal summary statistics are available (i.e., the t-score and degrees of freedom). Moreover, these approximations vastly outperform the classic BIC method for approximating Bayes factors from experimental designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11313v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas J. Faulkenberry</dc:creator>
    </item>
    <item>
      <title>Diffusion-Based Surrogate Modeling and Multi-Fidelity Calibration</title>
      <link>https://arxiv.org/abs/2407.17720</link>
      <description>arXiv:2407.17720v2 Announce Type: replace 
Abstract: Physics simulations have become fundamental tools to study myriad engineering systems. As physics simulations often involve simplifications, their outputs should be calibrated using real-world data. In this paper, we present a diffusion-based surrogate (DBS) that calibrates multi-fidelity physics simulations with diffusion generative processes. DBS categorizes multi-fidelity physics simulations into inexpensive and expensive simulations, depending on the computational costs. The inexpensive simulations, which can be obtained with low latency, directly inject contextual information into diffusion models. Furthermore, when results from expensive simulations are available, \name refines the quality of generated samples via a guided diffusion process. This design circumvents the need for large amounts of expensive physics simulations to train denoising diffusion models, thus lending flexibility to practitioners. DBS builds on Bayesian probabilistic models and is equipped with a theoretical guarantee that provides upper bounds on the Wasserstein distance between the sample and underlying true distribution. The probabilistic nature of DBS also provides a convenient approach for uncertainty quantification in prediction. Our models excel in cases where physics simulations are imperfect and sometimes inaccessible. We use a numerical simulation in fluid dynamics and a case study in laser-based metal powder deposition additive manufacturing to demonstrate how DBS calibrates multi-fidelity physics simulations with observations to obtain surrogates with superior predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17720v2</guid>
      <category>stat.CO</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE Transactions on Automation Science and Engineering, 2025</arxiv:journal_reference>
      <dc:creator>Naichen Shi, Hao Yan, Shenghan Guo, Raed Al Kontar</dc:creator>
    </item>
    <item>
      <title>Model-Based Clustering with Sequential Outlier Identification using the Distribution of Mahalanobis Distances</title>
      <link>https://arxiv.org/abs/2505.11668</link>
      <description>arXiv:2505.11668v2 Announce Type: replace-cross 
Abstract: The presence of outliers can prevent clustering algorithms from accurately determining an appropriate group structure within a data set. We present outlierMBC, a model-based approach for sequentially removing outliers and clustering the remaining observations. Our method identifies outliers one at a time while fitting a multivariate Gaussian mixture model to data. Since it can be difficult to classify observations as outliers without knowing what the correct cluster structure is a priori, and the presence of outliers interferes with the process of modelling clusters correctly, we use an iterative method to identify outliers one by one. At each iteration, outlierMBC removes the observation with the lowest density and fits a Gaussian mixture model to the remaining data. The method continues to remove potential outliers until a pre-set maximum number of outliers is reached, then retrospectively identifies the optimal number of outliers. To decide how many outliers to remove, it uses the fact that the squared sample Mahalanobis distances of Gaussian distributed observations are Beta distributed when scaled appropriately. outlierMBC chooses the number of outliers which minimises a dissimilarity between this theoretical Beta distribution and the observed distribution of the scaled squared sample Mahalanobis distances. This means that our method both clusters the data using a Gaussian mixture model and implements a model-based procedure to identify the optimal outliers to remove without requiring the number of outliers to be pre-specified. Unlike leading methods in the literature, outlierMBC does not assume that the outliers follow a known distribution or that the number of outliers can be pre-specified. Moreover, outlierMBC performs strongly compared to these algorithms when applied to a range of simulated and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11668v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ult\'an P. Doherty, Paul D. McNicholas, Arthur White</dc:creator>
    </item>
  </channel>
</rss>
