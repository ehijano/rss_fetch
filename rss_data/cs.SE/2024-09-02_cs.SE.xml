<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Collaborative Design and Planning of Software Architecture Changes via Software City Visualization</title>
      <link>https://arxiv.org/abs/2408.16777</link>
      <description>arXiv:2408.16777v1 Announce Type: new 
Abstract: Developers usually use diagrams and source code to jointly discuss and plan software architecture changes. With this poster, we present our on-going work on a novel approach that enables developers to collaboratively use software city visualization to design and plan software architecture changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16777v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Krause-Glau, Malte Hansen, Wilhelm Hasselbring</dc:creator>
    </item>
    <item>
      <title>Raising AI Ethics Awareness through an AI Ethics Quiz for Software Practitioners</title>
      <link>https://arxiv.org/abs/2408.16796</link>
      <description>arXiv:2408.16796v1 Announce Type: new 
Abstract: Today, ethical issues surrounding AI systems are increasingly prevalent, highlighting the critical need to integrate AI ethics into system design to prevent societal harm. Raising awareness and fostering a deep understanding of AI ethics among software practitioners is essential for achieving this goal. However, research indicates a significant gap in practitioners' awareness and knowledge of AI ethics and ethical principles. While much effort has been directed toward helping practitioners operationalise AI ethical principles such as fairness, transparency, accountability, and privacy, less attention has been paid to raising initial awareness, which should be the foundational step. Addressing this gap, we developed a software-based tool, the AI Ethics Quiz, to raise awareness and enhance the knowledge of AI ethics among software practitioners. Our objective was to organise interactive workshops, introduce the AI Ethics Quiz, and evaluate its effectiveness in enhancing awareness and knowledge of AI ethics and ethical principles among practitioners. We conducted two one-hour workshops (one in-person and one online) involving 29 software practitioners. Data was collected through pre-quiz questionnaire, the AI Ethics Quiz, and a post-quiz questionnaire. The anonymous responses revealed that the quiz significantly improved practitioners' awareness and understanding of AI ethics. Additionally, practitioners found the quiz engaging and reported it created a meaningful learning experience regarding AI ethics. In this paper, we share insights gained from conducting these interactive workshops and introducing the AI Ethics Quiz to practitioners. We also provide recommendations for software companies and leaders to adopt similar initiatives, which may help them enhance practitioners' awareness and understanding of AI ethics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16796v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aastha Pant, Rashina Hoda, Paul McIntosh</dc:creator>
    </item>
    <item>
      <title>Continuations: What Have They Ever Done for Us? (Experience Report)</title>
      <link>https://arxiv.org/abs/2408.17001</link>
      <description>arXiv:2408.17001v1 Announce Type: new 
Abstract: Surveys and experiments in economics involve stateful interactions: participants receive different messages based on earlier answers, choices, and performance, or trade across many rounds with other participants. In the design of Congame, a platform for running such economic studies, we decided to use delimited continuations to manage the common flow of participants through a study. Here we report on the positives of this approach, as well as some challenges of using continuations, such as persisting data across requests, working with dynamic variables, avoiding memory leaks, and the difficulty of debugging continuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17001v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3677998.3678223</arxiv:DOI>
      <arxiv:journal_reference>FUNARCH 2024: Proceedings of the 2nd ACM SIGPLAN International Workshop on Functional Software Architecture</arxiv:journal_reference>
      <dc:creator>Marc Kaufmann, Bogdan Popa</dc:creator>
    </item>
    <item>
      <title>Causal Reasoning in Software Quality Assurance: A Systematic Review</title>
      <link>https://arxiv.org/abs/2408.17183</link>
      <description>arXiv:2408.17183v1 Announce Type: new 
Abstract: Context: Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders that software products work as expected after release in operation. Machine Learning (ML) has proven to be able to boost SQA activities and contribute to the development of quality software systems. In this context, Causal Reasoning is gaining increasing interest as a methodology to solve some of the current ML limitations. It aims to go beyond a purely data-driven approach by exploiting the use of causality for more effective SQA strategies. Objective: Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support researchers to access this research field, identifying room for application, main challenges and research opportunities. Methods: A systematic literature review of causal reasoning in the SQA research area. Scientific papers have been searched, classified, and analyzed according to established guidelines for software engineering secondary studies. Results: Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant methodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where causal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing are rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl's graphical formulation of causality being preferred, likely due to its intuitiveness. Tools to favour their application are appearing at a fast pace - most of them after 2021. Conclusions: The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple quality attributes, especially during V&amp;V, evolution and maintenance to ensure reliability, while it is not yet fully exploited for phases like ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17183v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Giamattei, Antonio Guerriero, Roberto Pietrantuono, Stefano Russo</dc:creator>
    </item>
    <item>
      <title>Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach</title>
      <link>https://arxiv.org/abs/2408.17404</link>
      <description>arXiv:2408.17404v1 Announce Type: new 
Abstract: Over the past decade, app store (AppStore)-inspired requirements elicitation has proven to be highly beneficial. Developers often explore competitors' apps to gather inspiration for new features. With the advance of Generative AI, recent studies have demonstrated the potential of large language model (LLM)-inspired requirements elicitation. LLMs can assist in this process by providing inspiration for new feature ideas. While both approaches are gaining popularity in practice, there is a lack of insight into their differences. We report on a comparative study between AppStore- and LLM-based approaches for refining features into sub-features. By manually analyzing 1,200 sub-features recommended from both approaches, we identified their benefits, challenges, and key differences. While both approaches recommend highly relevant sub-features with clear descriptions, LLMs seem more powerful particularly concerning novel unseen app scopes. Moreover, some recommended features are imaginary with unclear feasibility, which suggests the importance of a human-analyst in the elicitation loop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17404v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, G\'erard Dray, Walid Maalej</dc:creator>
    </item>
    <item>
      <title>Configuration Monitor Synthesis</title>
      <link>https://arxiv.org/abs/2408.17368</link>
      <description>arXiv:2408.17368v1 Announce Type: cross 
Abstract: The observable behavior of a system usually carries useful information about its internal state, properties, and potential future behaviors. In this paper, we introduce configuration monitoring to determine an unknown configuration of a running system based on observations of its behavior. We develop a modular and generic pipeline to synthesize automata-theoretic configuration monitors from a featured transition system model of the configurable system to be monitored. The pipeline further allows synthesis under partial observability and network-induced losses as well as predictive configuration monitors taking the potential future behavior of a system into account. Beyond the novel application of configuration monitoring, we show that our approach also generalizes and unifies existing work on runtime monitoring and fault diagnosis, which aim at detecting the satisfaction or violation of properties and the occurrence of faults, respectively. We empirically demonstrate the efficacy of our approach with a case study on configuration monitors synthesized from configurable systems community benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17368v1</guid>
      <category>cs.FL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian A. K\"ohl, Clemens Dubslaff, Holger Hermanns</dc:creator>
    </item>
    <item>
      <title>WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models</title>
      <link>https://arxiv.org/abs/2310.15991</link>
      <description>arXiv:2310.15991v2 Announce Type: replace 
Abstract: Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences. Fuzzing has been studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates tests without sufficient understanding of internal compiler behaviors. Meanwhile, traditional white-box techniques, like symbolic execution, are computationally inapplicable to the giant codebase of compilers. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing.
  To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are used as feedback to enhance the generation on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox can generate high-quality test programs to exercise deep optimizations, practicing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101 bugs for the DL compilers, with 92 confirmed as previously unknown and 70 fixed. WhiteFox has been acknowledged by the PyTorch team and is being incorporated into its development workflow. Beyond DL compilers, WhiteFox can also be adapted for compilers in different domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15991v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689736</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 296. Publication date: October 2024</arxiv:journal_reference>
      <dc:creator>Chenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbarvand, Lingming Zhang</dc:creator>
    </item>
    <item>
      <title>ConCodeEval: Evaluating Large Language Models for Code Constraints in Domain-Specific Languages</title>
      <link>https://arxiv.org/abs/2407.03387</link>
      <description>arXiv:2407.03387v2 Announce Type: replace 
Abstract: Recent work shows Large Language Models (LLMs) struggle to understand natural language constraints for various text generation tasks in zero- and few-shot settings. While, in the code domain, there is wide usage of constraints in code format to maintain the integrity of code written in Domain-Specific Languages (DSLs) like JSON and YAML which are widely used for system-level programming tasks in enterprises. Given that LLMs are increasingly used for system-level code tasks, evaluating if they can comprehend these code constraints is crucial. However, no work has been done to evaluate their controllability over code constraints. Hence, we introduce ConCodeEval, a first-of-its-kind benchmark having two novel tasks for code constraints across five representations. Our findings suggest that language models struggle with code constraints. Code languages that perform excellently for normal code tasks do not perform well when the same languages represent fine-grained constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03387v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehant Kammakomati, Sameer Pimparkhede, Srikanth Tamilselvam, Prince Kumar, Pushpak Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Automatic Library Migration Using Large Language Models: First Results</title>
      <link>https://arxiv.org/abs/2408.16151</link>
      <description>arXiv:2408.16151v2 Announce Type: replace 
Abstract: Despite being introduced only a few years ago, Large Language Models (LLMs) are already widely used by developers for code generation. However, their application in automating other Software Engineering activities remains largely unexplored. Thus, in this paper, we report the first results of a study in which we are exploring the use of ChatGPT to support API migration tasks, an important problem that demands manual effort and attention from developers. Specifically, in the paper, we share our initial results involving the use of ChatGPT to migrate a client application to use a newer version of SQLAlchemy, an ORM (Object Relational Mapping) library widely used in Python. We evaluate the use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts) and show that the best results are achieved by the One-Shot prompt, followed by the Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to successfully migrate all columns of our target application and upgrade its code to use new functionalities enabled by SQLAlchemy's latest version, such as Python's asyncio and typing modules, while preserving the original code behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16151v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3674805.3690746</arxiv:DOI>
      <dc:creator>Aylton Almeida, Laerte Xavier, Marco Tulio Valente</dc:creator>
    </item>
    <item>
      <title>Docling Technical Report</title>
      <link>https://arxiv.org/abs/2408.09869</link>
      <description>arXiv:2408.09869v3 Announce Type: replace-cross 
Abstract: This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09869v3</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, Lokesh Mishra, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar</dc:creator>
    </item>
    <item>
      <title>Automating Semantic Analysis of System Assurance Cases using Goal-directed ASP</title>
      <link>https://arxiv.org/abs/2408.11699</link>
      <description>arXiv:2408.11699v2 Announce Type: replace-cross 
Abstract: Assurance cases offer a structured way to present arguments and evidence for certification of systems where safety and security are critical. However, creating and evaluating these assurance cases can be complex and challenging, even for systems of moderate complexity. Therefore, there is a growing need to develop new automation methods for these tasks. While most existing assurance case tools focus on automating structural aspects, they lack the ability to fully assess the semantic coherence and correctness of the assurance arguments.
  In prior work, we introduced the Assurance 2.0 framework that prioritizes the reasoning process, evidence utilization, and explicit delineation of counter-claims (defeaters) and counter-evidence. In this paper, we present our approach to enhancing Assurance 2.0 with semantic rule-based analysis capabilities using common-sense reasoning and answer set programming solvers, specifically s(CASP). By employing these analysis techniques, we examine the unique semantic aspects of assurance cases, such as logical consistency, adequacy, indefeasibility, etc. The application of these analyses provides both system developers and evaluators with increased confidence about the assurance case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11699v2</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anitha Murugesan, Isaac Wong, Joaqu\'in Arias, Robert Stroud, Srivatsan Varadarajan, Elmer Salazar, Gopal Gupta, Robin Bloomfield, John Rushby</dc:creator>
    </item>
  </channel>
</rss>
