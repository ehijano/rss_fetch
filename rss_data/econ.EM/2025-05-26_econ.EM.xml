<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 May 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions</title>
      <link>https://arxiv.org/abs/2505.17479</link>
      <description>arXiv:2505.17479v1 Announce Type: cross 
Abstract: LLM-based digital twin simulation, where large language models are used to emulate individual human behavior, holds great promise for research in AI, social science, and digital experimentation. However, progress in this area has been hindered by the scarcity of real, individual-level datasets that are both large and publicly available. This lack of high-quality ground truth limits both the development and validation of digital twin methodologies. To address this gap, we introduce a large-scale, public dataset designed to capture a rich and holistic view of individual human behavior. We survey a representative sample of $N = 2,058$ participants (average 2.42 hours per person) in the US across four waves with 500 questions in total, covering a comprehensive battery of demographic, psychological, economic, personality, and cognitive measures, as well as replications of behavioral economics experiments and a pricing survey. The final wave repeats tasks from earlier waves to establish a test-retest accuracy baseline. Initial analyses suggest the data are of high quality and show promise for constructing digital twins that predict human behavior well at the individual and aggregate levels. By making the full dataset publicly available, we aim to establish a valuable testbed for the development and benchmarking of LLM-based persona simulations. Beyond LLM applications, due to its unique breadth and scale the dataset also enables broad social science research, including studies of cross-construct correlations and heterogeneous treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17479v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.EM</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Toubia, George Z. Gui, Tianyi Peng, Daniel J. Merlau, Ang Li, Haozhe Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Learning for Discrete Choice</title>
      <link>https://arxiv.org/abs/2505.18077</link>
      <description>arXiv:2505.18077v1 Announce Type: cross 
Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference on key economic variables, such as marginal rates of substitution, rather than focusing solely on predicting choices on new unlabeled data. However, while traditional DCMs offer high interpretability and support for point and interval estimation of economic quantities, these models often underperform in predictive tasks compared to deep learning (DL) models. Despite their predictive advantages, DL models remain largely underutilized in discrete choice due to concerns about their lack of interpretability, unstable parameter estimates, and the absence of established methods for uncertainty quantification. Here, we introduce a deep learning model architecture specifically designed to integrate with approximate Bayesian inference methods, such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model collapses to behaviorally informed hypotheses when data is limited, mitigating overfitting and instability in underspecified settings while retaining the flexibility to capture complex nonlinear relationships when sufficient data is available. We demonstrate our approach using SGLD through a Monte Carlo simulation study, evaluating both predictive metrics--such as out-of-sample balanced accuracy--and inferential metrics--such as empirical coverage for marginal rates of substitution interval estimates. Additionally, we present results from two empirical case studies: one using revealed mode choice data in NYC, and the other based on the widely used Swiss train choice stated preference data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18077v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel F. Villarraga, Ricardo A. Daziano</dc:creator>
    </item>
    <item>
      <title>Local Projections or VARs? A Primer for Macroeconomists</title>
      <link>https://arxiv.org/abs/2503.17144</link>
      <description>arXiv:2503.17144v2 Announce Type: replace 
Abstract: What should applied macroeconomists know about local projection (LP) and vector autoregression (VAR) impulse response estimators? The two methods share the same estimand, but in finite samples lie on opposite ends of a bias-variance trade-off. While the low bias of LPs comes at a quite steep variance cost, this cost must be paid to achieve robust uncertainty assessments. Hence, when the goal is to convey what can be learned about dynamic causal effects from the data, VARs should only be used with long lag lengths, ensuring equivalence with LP. For LP estimation, we provide guidance on selection of lag length and controls, bias correction, and confidence interval construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17144v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e Luis Montiel Olea, Mikkel Plagborg-M{\o}ller, Eric Qian, Christian K. Wolf</dc:creator>
    </item>
    <item>
      <title>Forecasting Thai inflation from univariate Bayesian regression perspective</title>
      <link>https://arxiv.org/abs/2505.05334</link>
      <description>arXiv:2505.05334v2 Announce Type: replace 
Abstract: This study investigates the forecasting performance of Bayesian shrinkage priors in predicting Thai inflation in a univariate setup, with a particular interest in comparing those more advance shrinkage prior to a likelihood dominated/noninformative prior. Our forecasting exercises are evaluated using Root Mean Squared Error (RMSE), Quantile-Weighted Continuous Ranked Probability Scores (qwCRPS), and Log Predictive Likelihood (LPL). The empirical results reveal several interesting findings: SV-augmented models consistently underperform compared to their non-SV counterparts, particularly in large predictor settings. Notably, HS, DL and LASSO in large-sized model setting without SV exhibit superior performance across multiple horizons. This indicates that a broader range of predictors captures economic dynamics more effectively than modeling time-varying volatility. Furthermore, while left-tail risks (deflationary pressures) are well-controlled by advanced priors (HS, HS+, and DL), right-tail risks (inflationary surges) remain challenging to forecast accurately. The results underscore the trade-off between model complexity and forecast accuracy, with simpler models delivering more reliable predictions in both normal and crisis periods (e.g., the COVID-19 pandemic). This study contributes to the literature by highlighting the limitations of SV models in high-dimensional environments and advocating for a balanced approach that combines advanced shrinkage techniques with broad predictor coverage. These insights are crucial for policymakers and researchers aiming to enhance the precision of inflation forecasts in emerging economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05334v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paponpat Taveeapiradeecharoen, Popkarn Arwatchanakarn</dc:creator>
    </item>
  </channel>
</rss>
