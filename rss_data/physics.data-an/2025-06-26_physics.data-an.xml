<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 04:07:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sparse Infrared Spectroscopy for Detection of Volatile Organic Compounds</title>
      <link>https://arxiv.org/abs/2506.20678</link>
      <description>arXiv:2506.20678v1 Announce Type: cross 
Abstract: To reduce the complexity of infrared spectroscopy hardware while maintaining performance, a data informed, task-specific, spectral collection approach termed Sparse Infrared Spectroscopy (SIRS) is developed. Using a numerically based virtual experiment based on a quantitatively accurate infrared database, non-negative matrix factorization is used to identify the spectral pass bands of a minimal number of filters necessary to identify volatile organic compounds (VOC) within either an inert background or mixture of gases. The data-driven approach is found capable of identifying contaminants at the 1-10 part per million level (PPM) with $\mathrm{\sim~20-50}$ spectral samples as opposed to the more than 1,000 typical of a traditional infrared spectrum. Reasonably robust to both noise and the characteristics of the base compound in a mixture, the task-specific spectral sampling points to simplified hardware design that maintains performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20678v1</guid>
      <category>physics.chem-ph</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mira Welner, Andre Hazbun, Thomas Beechem</dc:creator>
    </item>
    <item>
      <title>Evolution and determinants of firm-level systemic risk in local production networks</title>
      <link>https://arxiv.org/abs/2506.21426</link>
      <description>arXiv:2506.21426v1 Announce Type: cross 
Abstract: Recent crises like the COVID-19 pandemic and geopolitical tensions have exposed vulnerabilities and caused disruptions of supply chains, leading to product shortages, increased costs, and economic instability. This has prompted increasing efforts to assess systemic risk, namely the effects of firm disruptions on entire economies. However, the ability of firms to react to crises by rewiring their supply links has been largely overlooked, limiting our understanding of production networks resilience. Here we study dynamics and determinants of firm-level systemic risk in the Hungarian production network from 2015 to 2022. We use as benchmark a heuristic maximum entropy null model that generates an ensemble of production networks at equilibrium, by preserving the total input (demand) and output (supply) of each firm at the sector level. We show that the fairly stable set of firms with highest systemic risk undergoes a structural change during COVID-19, as those enabling economic exchanges become key players in the economy -- a result which is not reproduced by the null model. Although the empirical systemic risk aligns well with the null value until the onset of the pandemic, it becomes significantly smaller afterwards as the adaptive behavior of firms leads to a more resilient economy. Furthermore, firms' international trade volume (being a subject of disruption) becomes a significant predictor of their systemic risk. However, international links cannot provide an unequivocal explanation for the observed trends, as imports and exports have opposing effects on local systemic risk through the supply and demand channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21426v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>physics.data-an</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anna Mancini, Bal\'azs Lengyel, Riccardo Di Clemente, Giulio Cimini</dc:creator>
    </item>
    <item>
      <title>Graph Neural Network for Neutrino Physics Event Reconstruction</title>
      <link>https://arxiv.org/abs/2403.11872</link>
      <description>arXiv:2403.11872v2 Announce Type: replace 
Abstract: Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\% efficiency and labelling them according to particle type with 94.9\% efficiency. The network operates directly on detector observables across multiple 2D representations, but utilizes a 3D-context-aware mechanism to encourage consistency between these representations. Model inference takes 0.12~s/event on a CPU, and 0.005s/event batched on a GPU. This architecture is designed to be a general-purpose solution for particle reconstruction in neutrino physics, with the potential for deployment across a broad range of detector technologies, and offers a core convolution engine that can be leveraged for a variety of tasks beyond the two described in this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11872v2</guid>
      <category>physics.data-an</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevD.110.032008</arxiv:DOI>
      <arxiv:journal_reference>Phys.Rev.D 110 (2024) 3, 032008</arxiv:journal_reference>
      <dc:creator>V Hewes, Adam Aurisano, Giuseppe Cerati, Jim Kowalkowski, Claire Lee, Wei-keng Liao, Daniel Grzenda, Kaushal Gumpula, Xiaohe Zhang</dc:creator>
    </item>
    <item>
      <title>Linear scaling causal discovery from high-dimensional time series by dynamical community detection</title>
      <link>https://arxiv.org/abs/2501.10886</link>
      <description>arXiv:2501.10886v2 Announce Type: replace 
Abstract: Understanding which parts of a dynamical system cause each other is extremely relevant in fundamental and applied sciences. However, inferring causal links from observational data, namely without direct manipulations of the system, is still computationally challenging, especially if the data are high-dimensional. In this study we introduce a framework for constructing causal graphs from high-dimensional time series, whose computational cost scales linearly with the number of variables. The approach is based on the automatic identification of dynamical communities, groups of variables which mutually influence each other and can therefore be described as a single node in a causal graph. These communities are efficiently identified by optimizing the Information Imbalance, a statistical quantity that assigns a weight to each putative causal variable based on its information content relative to a target variable. The communities are then ordered starting from the fully autonomous ones, whose evolution is independent from all the others, to those that are progressively dependent on other communities, building in this manner a community causal graph. We demonstrate the computational efficiency and the accuracy of our approach on time-discrete and time-continuous dynamical systems including up to 80 variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10886v2</guid>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1103/kd73-93cg</arxiv:DOI>
      <dc:creator>Matteo Allione, Vittorio Del Tatto, Alessandro Laio</dc:creator>
    </item>
    <item>
      <title>Multi-stage tomography based on eigenanalysis for high-dimensional dense unitary processes in gate-based quantum computers</title>
      <link>https://arxiv.org/abs/2407.13542</link>
      <description>arXiv:2407.13542v2 Announce Type: replace-cross 
Abstract: Quantum Process Tomography (QPT) methods aim at identifying, i.e. estimating, a quantum process. QPT is a major quantum information processing tool, since it especially allows one to experimentally characterize the actual behavior of quantum gates, that may be used as the building blocks of quantum computers. We here consider unitary, possibly dense (i.e. without sparsity constraints) processes, which corresponds to isolated systems. Moreover, we develop QPT methods that are applicable to a significant number of qubits and hence to a high state space dimension, which allows one to tackle more complex problems. Using the unitarity of the process allows us to develop methods that first achieve part of QPT by performing an eigenanalysis of the estimated density matrix of a process output. Building upon this idea, we first develop a class of complete algorithms that are single-stage, in the sense that they use only one eigendecomposition. We then extend them to multiple-stage algorithms (i.e. with several eigendecompositions), in order to address high-dimensional state spaces while being less limited by the estimation errors made when using an arbitrary given Quantum State Tomography (QST) algorithm as a building block of our overall methods. We first propose two-stage methods and we then extend them to dichotomic methods, whose number of stages increases with the considered state space dimension. The relevance of our methods is validated with simulations. Single-stage and two-stage methods efficiently apply up to 13 qubits on a standard PC (with 16 GB of RAM). Multi-stage methods yield an even higher accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13542v2</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannick Deville, Alain Deville</dc:creator>
    </item>
  </channel>
</rss>
