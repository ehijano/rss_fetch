<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 05:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>In-sample calibration yields conformal calibration guarantees</title>
      <link>https://arxiv.org/abs/2503.03841</link>
      <description>arXiv:2503.03841v1 Announce Type: new 
Abstract: Conformal predictive systems allow forecasters to issue predictive distributions for real-valued future outcomes that have out-of-sample calibration guarantees. On a more abstract level, conformal prediction makes use of in-sample calibration guarantees to construct bands of predictions with out-of-sample guarantees under exchangeability. The calibration guarantees are typically that prediction intervals derived from the predictive distributions have the correct marginal coverage. We extend this line of reasoning to stronger notions of calibration that are common in statistical forecasting theory. We take two prediction methods that are calibrated in-sample, and conformalize them to obtain conformal predictive systems with stronger out-of-sample calibration guarantees than existing approaches. The first method corresponds to a binning of the data, while the second leverages isotonic distributional regression (IDR), a non-parametric distributional regression method under order constraints. We study the theoretical properties of these new conformal predictive systems, and compare their performance in a simulation experiment. They are then applied to two case studies on European temperature forecasts and on predictions for the length of patient stay in Swiss intensive care units. Both approaches are found to outperform existing conformal predictive systems, while conformal IDR additionally provides a natural method for quantifying epistemic uncertainty of the predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03841v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Allen, Georgios Gavrilopoulos, Alexander Henzi, Gian-Reto Kleger, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>How Balanced Should Causal Covariates Be?</title>
      <link>https://arxiv.org/abs/2503.03860</link>
      <description>arXiv:2503.03860v1 Announce Type: new 
Abstract: Covariate balancing is a popular technique for controlling confounding in observational studies. It finds weights for the treatment group which are close to uniform, but make the group's covariate means (approximately) equal to those of the entire sample. A crucial question is: how approximate should the balancing be, in order to minimize the error of the final estimate? Current guidance is derived from heuristic or asymptotic analyses, which are uninformative when the size of the sample is small compared to the number of covariates. This paper presents the first rigorous, nonasymptotic analysis of covariate balancing; specifically, we use PAC-Bayesian techniques to derive valid, finite-sample confidence intervals for the treatment effect. More generally, we prove these guarantees for a flexible form of covariate balancing where the regularization parameters weighting the tradeoff between bias (imbalance) and variance (divergence from uniform) are optimized, not fixed. This gives rise to a new balancing algorithm which empirically delivers superior adaptivity. Our overall contribution is to make covariate balancing a more reliable method for causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03860v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiva Kaul, Min-Gyu Kim</dc:creator>
    </item>
    <item>
      <title>Evaluating and Testing for Actionable Treatment Effect Heterogeneity</title>
      <link>https://arxiv.org/abs/2503.04093</link>
      <description>arXiv:2503.04093v1 Announce Type: new 
Abstract: Developing tools for estimating heterogeneous treatment effects (HTE) and individualized treatment effects has been an area of active research in recent years. While these tools have proven to be useful in many contexts, a concern when deploying such methods is the degree to which incorporating HTE into a prediction model provides an advantage over predictive methods which do not allow for variation in treatment effect across individuals. To address this concern, we propose a procedure which evaluates the extent to which an HTE model provides a predictive advantage. Specifically, our procedure targets the gain in predictive performance from using a flexible predictive model incorporating HTE versus an alternative model which is similar to the HTE-utilizing model except that it is constrained to not allow variation in treatment effect. By drawing upon recent work in using nested cross-validation techniques for prediction error inference, we generate confidence intervals for this measure of gain in predictive performance which allows one to directly calculate the level at which one is confident of a substantial HTE-modeling gain in prediction -- a quantity which we refer to as the h-value. Our procedure is generic and can be directly used to assess the benefit of modeling HTE for any method that incorporates treatment effect variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04093v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahsa Ashouri, Nicholas C. Henderson</dc:creator>
    </item>
    <item>
      <title>Fiducial Confidence Intervals for Agreement Measures Among Raters Under a Generalized Linear Mixed Effects Model</title>
      <link>https://arxiv.org/abs/2503.04117</link>
      <description>arXiv:2503.04117v1 Announce Type: new 
Abstract: A generalization of the classical concordance correlation coefficient (CCC) is considered under a three-level design where multiple raters rate every subject over time, and each rater is rating every subject multiple times at each measuring time point. The ratings can be discrete or continuous. A methodology is developed for the interval estimation of the CCC based on a suitable linearization of the model along with an adaptation of the fiducial inference approach. The resulting confidence intervals have satisfactory coverage probabilities and shorter expected widths compared to the interval based on Fisher Z-transformation, even under moderate sample sizes. Two real applications available in the literature are discussed. The first application is based on a clinical trial to determine if various treatments are more effective than a placebo for treating knee pain associated with osteoarthritis. The CCC was used to assess agreement among the manual measurements of the joint space widths on plain radiographs by two raters, and the computer-generated measurements of digitalized radiographs. The second example is on a corticospinal tractography, and the CCC was once again applied in order to evaluate the agreement between a well-trained technologist and a neuroradiologist regarding the measurements of fiber number in both the right and left corticospinal tracts. Other relevant applications of our general approach are highlighted in many areas including artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04117v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Thomas Mathew, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Stepwise regression revisited</title>
      <link>https://arxiv.org/abs/2503.04330</link>
      <description>arXiv:2503.04330v1 Announce Type: new 
Abstract: This paper shows that the degree of approximate multicollinearity in a linear regression model increases simply by including independent variables, even if these are not highly linearly related. In the current situation where it is relatively easy to find linear models with a large number of independent variables, it is shown that this issue can lead to the erroneous conclusion that there is a worrying problem of approximate multicollinearity. To avoid this situation, an adjusted variance inflation factor is proposed to compensate the presence of a large number of independent variables in the multiple linear regression model. It is shown that this proposal has a direct impact on variable selection models based on influence relationships, which translates into a new decision criterion in the individual significance contrast to be considered in stepwise regression models or even directly in a multiple linear regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04330v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rom\'an Salmer\'on G\'omez, Catalina Garc\'ia Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Interpretable Transformation and Analysis of Timelines through Learning via Surprisability</title>
      <link>https://arxiv.org/abs/2503.04502</link>
      <description>arXiv:2503.04502v1 Announce Type: new 
Abstract: The analysis of high-dimensional timeline data and the identification of outliers and anomalies is critical across diverse domains, including sensor readings, biological and medical data, historical records, and global statistics. However, conventional analysis techniques often struggle with challenges such as high dimensionality, complex distributions, and sparsity. These limitations hinder the ability to extract meaningful insights from complex temporal datasets, making it difficult to identify trending features, outliers, and anomalies effectively. Inspired by surprisability -- a cognitive science concept describing how humans instinctively focus on unexpected deviations - we propose Learning via Surprisability (LvS), a novel approach for transforming high-dimensional timeline data. LvS quantifies and prioritizes anomalies in time-series data by formalizing deviations from expected behavior. LvS bridges cognitive theories of attention with computational methods, enabling the detection of anomalies and shifts in a way that preserves critical context, offering a new lens for interpreting complex datasets. We demonstrate the usefulness of LvS on three high-dimensional timeline use cases: a time series of sensor data, a global dataset of mortality causes over multiple years, and a textual corpus containing over two centuries of State of the Union Addresses by U.S. presidents. Our results show that the LvS transformation enables efficient and interpretable identification of outliers, anomalies, and the most variable features along the timeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04502v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Osnat Mokryn, Teddy Lazebnik, Hagit Ben Shoshan</dc:creator>
    </item>
    <item>
      <title>Fiducial Inference for Random-Effects Calibration Models: Advancing Reliable Quantification in Environmental Analytical Chemistry</title>
      <link>https://arxiv.org/abs/2503.04588</link>
      <description>arXiv:2503.04588v1 Announce Type: new 
Abstract: This article addresses calibration challenges in analytical chemistry by employing a random-effects calibration curve model and its generalizations to capture variability in analyte concentrations. The model is motivated by specific issues in analytical chemistry, where measurement errors remain constant at low concentrations but increase proportionally as concentrations rise. To account for this, the model permits the parameters of the calibration curve, which relate instrument responses to true concentrations, to vary across different laboratories, thereby reflecting real-world variability in measurement processes. Traditional large-sample interval estimation methods are inadequate for small samples, leading to the use of an alternative approach, namely the fiducial approach. The calibration curve that accurately captures the heteroscedastic nature of the data, results in more reliable estimates across diverse laboratory conditions. It turns out that the fiducial approach, when used to construct a confidence interval for an unknown concentration, produces a slightly wider width while achieving the desired coverage probability. Applications considered include the determination of the presence of an analyte and the interval estimation of an unknown true analyte concentration. The proposed method is demonstrated for both simulated and real interlaboratory data, including examples involving copper and cadmium in distilled water.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04588v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Thomas Mathew, Robert Gibbons, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of a multivariate TAR model when the noise process distribution belongs to the class of Gaussian variance mixtures</title>
      <link>https://arxiv.org/abs/2503.04593</link>
      <description>arXiv:2503.04593v1 Announce Type: new 
Abstract: A threshold autoregressive (TAR) model is a powerful tool for analyzing nonlinear multivariate time series, which includes special cases like self-exciting threshold autoregressive (SETAR) models and vector autoregressive (VAR) models. In this paper, estimation, inference, and forecasting using the Bayesian approach are developed for multivariate TAR (MTAR) models considering a flexible setup, under which the noise process behavior can be described using not only the Gaussian distribution but also other distributions that belong to the class of Gaussian variance mixtures, which includes Student-t, Slash, symmetric hyperbolic, and contaminated normal distributions, which are also symmetric but are more flexible and with heavier tails than the Gaussian one. Inferences from MTAR models based on that kind of distribution may be less affected by extreme or outlying observations than those based on the Gaussian one. All parameters in the MTAR model are included in the proposed MCMC-type algorithm, except the number of regimes and the autoregressive orders, which can be chosen using the Deviance Information Criterion (DIC) and/or the Watanabe-Akaike Information Criterion (WAIC). A library for the language and environment for statistical computing R was also developed to assess the effectiveness of the proposed methodology using simulation studies and analysis of two real multivariate time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04593v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. H. Vanegas, S. A. Calder\'on, L. M. Rond\'on</dc:creator>
    </item>
    <item>
      <title>A Bayesian hierarchical framework for fusion of remote sensing data: An example with solar-induced fluorescence</title>
      <link>https://arxiv.org/abs/2503.03901</link>
      <description>arXiv:2503.03901v1 Announce Type: cross 
Abstract: Solar-induced chlorophyll fluorescence (SIF) has emerged as an effective indicator of vegetation productivity and plant health. The global quantification of SIF and its associated uncertainties yields many important capabilities, including improving carbon flux estimation, improving the identification of carbon sources and sinks, monitoring a variety of ecosystems, and evaluating carbon sequestration efforts. Long-term, regional-to-global scale monitoring is now feasible with the availability of SIF estimates from multiple Earth-observing satellites. These efforts can be aided by a rigorous accounting of the sources of uncertainty present in satellite SIF data products. In this paper, we introduce a Bayesian Hierarchical Model (BHM) for the estimation of SIF and associated uncertainties from Orbiting Carbon Observatory-2 (OCO-2) satellite observations at one-degree resolution with global coverage. The hierarchical structure of our modeling framework allows for convenient model specification, quantification of various sources of variation, and the incorporation of seasonal SIF information through Fourier terms in the regression model. The modeling framework leverages the predictable seasonality of SIF in most temperate land areas. The resulting data product complements existing atmospheric carbon dioxide estimates at the same spatio-temporal resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03901v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manju Johny, Jonathan Hobbs, Vineet Yadav, Margaret Johnson, Nicholas Parazoo, Hai Nguyen, Amy Braverman</dc:creator>
    </item>
    <item>
      <title>Spectral Extremal Connectivity of Two-State Seizure Brain Waves</title>
      <link>https://arxiv.org/abs/2503.04169</link>
      <description>arXiv:2503.04169v1 Announce Type: cross 
Abstract: Coherence analysis plays a vital role in the study of functional brain connectivity. However, coherence captures only linear spectral associations, and thus can produce misleading findings when ignoring variations of connectivity in the tails of the distribution. This limitation becomes important when investigating extreme neural events that are characterized by large signal amplitudes. The focus of this paper is to examine connectivity in the tails of the distribution, as this reveals salient information that may be overlooked by standard methods. We develop a novel notion of spectral tail association of periodograms to study connectivity in the network of electroencephalogram (EEG) signals of seizure-prone neonates. We further develop a novel non-stationary extremal dependence model for multivariate time series that captures differences in extremal dependence during different brain phases, namely burst-suppression and non-burst-suppression. One advantage of our proposed approach is its ability to identify tail connectivity at key frequency bands that could be associated with outbursts of energy which may lead to seizures. We discuss these novel scientific findings alongside a comparison of the extremal behavior of brain signals for epileptic and non-epileptic patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04169v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mara Sherlin D. Talento, Jordan Richards, Marco Pinto-Orellana, Raphael Huser, Hernando C. Ombao</dc:creator>
    </item>
    <item>
      <title>A Spatiotemporal, Quasi-experimental Causal Inference Approach to Characterize the Effects of Global Plastic Waste Export and Burning on Air Quality Using Remotely Sensed Data</title>
      <link>https://arxiv.org/abs/2503.04491</link>
      <description>arXiv:2503.04491v1 Announce Type: cross 
Abstract: Open burning of plastic waste may pose a significant threat to global health by degrading air quality, but quantitative research on this problem -- crucial for policy making -- has previously been stunted by lack of data. Critically, many low- and middle-income countries, where open burning is of greatest concern, have little to no air quality monitoring. Here, we propose an approach, at the intersection of modern causal inference and environmental data science, to leverage remotely sensed data products combined with spatiotemporal causal analytic techniques to evaluate the impact of large-scale plastic waste policies on air quality. Throughout, we use the case study of Indonesia before and after 2018, when China halted its import of plastic waste, resulting in diversion of this massive waste stream to other countries in the East Asia &amp; Pacific region, including Indonesia. We tailor cutting-edge statistical methods to this setting, estimating effects of the increase in plastic waste imports on fine particulate matter near waste dump sites in Indonesia and allowing effects to vary as a function of the site's proximity to ports (from which international plastic waste enters the country), which serves as an induced continuous exposure or "dose" of treatment. We observe a statistically significant increase in monthly fine particulate matter concentrations near dump sites after China's ban took effect (2018-2019) compared to concentrations expected under business-as-usual (2012-2017), with increases ranging from 0.76--1.72$\mu$g/m$^3$ (15--34\% of the World Health Organization's recommended limit for exposure on an annual basis) depending on the site's port proximity, at sites with port proximity above the 20th quantile. Sites with lower port proximity had smaller and not statistically significant effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04491v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Method for recovering data on unreported low-severity crashes</title>
      <link>https://arxiv.org/abs/2503.04529</link>
      <description>arXiv:2503.04529v1 Announce Type: cross 
Abstract: Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04529v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Morando</dc:creator>
    </item>
    <item>
      <title>Estimating network-mediated causal effects via principal components network regression</title>
      <link>https://arxiv.org/abs/2212.12041</link>
      <description>arXiv:2212.12041v4 Announce Type: replace 
Abstract: We develop a method to decompose causal effects on a social network into an indirect effect mediated by the network, and a direct effect independent of the social network. To handle the complexity of network structures, we assume that latent social groups act as causal mediators. We develop principal components network regression models to differentiate the social effect from the non-social effect. Fitting the regression models is as simple as principal components analysis followed by ordinary least squares estimation. We prove asymptotic theory for regression coefficients from this procedure and show that it is widely applicable, allowing for a variety of distributions on the regression errors and network edges. We carefully characterize the counterfactual assumptions necessary to use the regression models for causal inference, and show that current approaches to causal network regression may result in over-control bias. The method is very general, so that it is applicable to many types of structured data beyond social networks, such as text, areal data, psychometrics, images and omics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12041v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 26 (2025): 1-99</arxiv:journal_reference>
      <dc:creator>Alex Hayes, Mark M. Fredrickson, Keith Levin</dc:creator>
    </item>
    <item>
      <title>Nonparametric data segmentation in multivariate time series via joint characteristic functions</title>
      <link>https://arxiv.org/abs/2305.07581</link>
      <description>arXiv:2305.07581v4 Announce Type: replace 
Abstract: Modern time series data often exhibit complex dependence and structural changes which are not easily characterised by shifts in the mean or model parameters. We propose a nonparametric data segmentation methodology for multivariate time series termed NP-MOJO. By considering joint characteristic functions between the time series and its lagged values, NP-MOJO is able to detect change points in the marginal distribution, but also those in possibly non-linear serial dependence, all without the need to pre-specify the type of changes. We show the theoretical consistency of NP-MOJO in estimating the total number and the locations of the change points, and demonstrate the good performance of NP-MOJO against a variety of change point scenarios. We further demonstrate its usefulness in applications to seismology and economic time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07581v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Euan T. McGonigle, Haeran Cho</dc:creator>
    </item>
    <item>
      <title>Sparse estimation of parameter support sets for generalized vector autoregressions by resampling and model aggregation</title>
      <link>https://arxiv.org/abs/2307.09684</link>
      <description>arXiv:2307.09684v2 Announce Type: replace 
Abstract: The central problem we address in this work is estimation of the parameter support set S, the set of indices corresponding to nonzero parameters, in the context of a sparse parametric likelihood model for discrete multivariate time series. We develop an algorithm that performs the estimation by aggregating support sets obtained by applying the LASSO to data subsamples. Our approach is to identify several candidate models and estimate S by selecting common parameters, thus "aggregating" candidate models. While our method is broadly applicable to any selection problem, we focus on the generalized vector autoregressive (GVAR) model class, and particularly the Poisson case, emphasizing applications in network recovery from discrete multivariate time series. We propose benchmark methods based on the LASSO, develop simulation strategies for GVAR processes, and present empirical results demonstrating the superior performance of our method. Additionally, we present an application estimating ecological interaction networks from paleoclimatology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09684v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00949655.2025.2469905</arxiv:DOI>
      <dc:creator>Trevor D. Ruiz, Sharmodeep Bhattacharyya, Sarah C. Emerson</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Multinomial Response Models with Random Consideration Sets</title>
      <link>https://arxiv.org/abs/2308.12470</link>
      <description>arXiv:2308.12470v4 Announce Type: replace 
Abstract: A common assumption in the fitting of unordered multinomial response models for $J$ mutually exclusive categories is that the responses arise from the same set of $J$ categories across subjects. However, when responses measure a choice made by the subject, it is more appropriate to condition the distribution of multinomial responses on a subject-specific consideration set, drawn from the power set of $\{1,2,\ldots,J\}$. This leads to a mixture of multinomial response models governed by a probability distribution over the $J^{\ast} = 2^J -1$ consideration sets. We introduce a novel method for estimating such generalized multinomial response models based on the fundamental result that any mass distribution over $J^{\ast}$ consideration sets can be represented as a mixture of products of $J$ component-specific inclusion-exclusion probabilities. Moreover, under time-invariant consideration sets, the conditional posterior distribution of consideration sets is sparse. These features enable a scalable MCMC algorithm for sampling the posterior distribution of parameters, random effects, and consideration sets. Under regularity conditions, the posterior distributions of the marginal response probabilities and the model parameters satisfy consistency. The methodology is demonstrated in a longitudinal data set on weekly cereal purchases that cover $J = 101$ brands, a dimension substantially beyond the reach of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12470v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Multivariate Confluent Hypergeometric Covariance Functions with Simultaneous Flexibility over Smoothness and Tail Decay</title>
      <link>https://arxiv.org/abs/2312.05682</link>
      <description>arXiv:2312.05682v3 Announce Type: replace 
Abstract: Spatially-indexed multivariate data appear frequently in geostatistics and related fields including oceanography and environmental science. To take full advantage of this data structure, cross-covariance functions are constructed to describe the dependence between any two component variables at different spatial locations. Modeling of multivariate spatial random fields requires these constructed cross-covariance functions to be valid, which often presents challenges that lead to complicated restrictions on the parameter space. The purpose of this work is to present techniques using multivariate mixtures for establishing validity that are simultaneously simplified and comprehensive. In particular, cross-covariances are constructed for the recently-introduced confluent hypergeometric (CH) class of covariance functions, which has slow (polynomial) decay in the tails of the covariance that better handles large gaps between observations in comparison with other covariance models. In addition, the spectral density of the confluent hypergeometric covariance is established and used to construct new valid cross-covariance models. The approach leads to valid multivariate cross-covariance models that inherit the desired marginal properties of the confluent hypergeometric model and outperform the multivariate Mat\'ern model in out-of-sample prediction under slowly-decaying correlation of the underlying multivariate random field. The model captures heavy tail decay and dependence between variables in an oceanography dataset of temperature, salinity and oxygen, as measured by autonomous floats in the Southern Ocean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05682v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drew Yarger, Anindya Bhadra</dc:creator>
    </item>
    <item>
      <title>Methods for differential network estimation: an empirical comparison</title>
      <link>https://arxiv.org/abs/2412.17922</link>
      <description>arXiv:2412.17922v2 Announce Type: replace 
Abstract: We provide a review and a comparison of methods for differential network estimation in Gaussian graphical models with focus on structure learning. We consider the case of two datasets from distributions associated with two graphical models. In our simulations, we use five different methods to estimate differential networks. We vary graph structure and sparsity to explore their influence on performance in terms of power and false discovery rate. We demonstrate empirically that presence of hubs proves to be a challenge for all the methods, as well as increased density. We suggest local and global properties that are associated with this challenge. Direct estimation with lasso penalized D-trace loss is shown to perform the best across all combinations of network structure and sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17922v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Plaksienko, Magne Thoresen, Vera Djordjilovi\'c</dc:creator>
    </item>
    <item>
      <title>A critical evaluation of longitudinal proportional effect models</title>
      <link>https://arxiv.org/abs/2502.00214</link>
      <description>arXiv:2502.00214v3 Announce Type: replace 
Abstract: Nonlinear longitudinal proportional effect models have been proposed to improve power and provide direct estimates of the proportional treatment effect in randomized clinical trials. These models assume a fixed proportional treatment effect over time, which can lead to bias and Type I error inflation when the assumption is violated. Even when the proportional effect assumption holds, these models are biased, and their inference is sensitive to the labeling of treatment groups. Typically, this bias favors the active group, inflates Type I error, and can result in one-sided testing. Conversely, the bias can make it more difficult to detect treatment harm, creating a safety concern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00214v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. Donohue, Philip S. Insel, Oliver Langford</dc:creator>
    </item>
    <item>
      <title>Semiparametric Growth-Curve Modeling in Hierarchical, Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2503.03550</link>
      <description>arXiv:2503.03550v2 Announce Type: replace 
Abstract: Modeling of growth (or decay) curves arises in many fields such as microbiology, epidemiology, marketing, and econometrics. Parametric forms like Logistic and Gompertz are often used for modeling such monotonic patterns. While useful for compact description, the real-life growth curves rarely follow these parametric forms perfectly. Therefore, the curve estimation methods that strike a balance between prior information in the parametric form and fidelity with the observed data are preferred. In hierarchical, longitudinal studies the interest lies in comparing the growth curves of different groups while accounting for the differences between the within-group subjects. This article describes a flexible state space modeling framework that enables semiparametric growth curve modeling for the data generated from hierarchical, longitudinal studies. The methodology, a type of functional mixed effects modeling, is illustrated with a real-life example of bacterial growth in different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03550v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajesh Selukar</dc:creator>
    </item>
    <item>
      <title>You Are the Best Reviewer of Your Own Papers: The Isotonic Mechanism</title>
      <link>https://arxiv.org/abs/2206.08149</link>
      <description>arXiv:2206.08149v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) conferences including NeurIPS and ICML have experienced a significant decline in peer review quality in recent years. To address this growing challenge, we introduce the Isotonic Mechanism, a computationally efficient approach to enhancing the accuracy of noisy review scores by incorporating authors' private assessments of their submissions. Under this mechanism, authors with multiple submissions are required to rank their papers in descending order of perceived quality. Subsequently, the raw review scores are calibrated based on this ranking to produce adjusted scores. We prove that authors are incentivized to truthfully report their rankings because doing so maximizes their expected utility, modeled as an additive convex function over the adjusted scores. Moreover, the adjusted scores are shown to be more accurate than the raw scores, with improvements being particularly significant when the noise level is high and the author has many submissions -- a scenario increasingly prevalent at large-scale ML/AI conferences.
  We further investigate whether submission quality information beyond a simple ranking can be truthfully elicited from authors. We establish that a necessary condition for truthful elicitation is that the mechanism be based on pairwise comparisons of the author's submissions. This result underscores the optimality of the Isotonic Mechanism, as it elicits the most fine-grained truthful information among all mechanisms we consider. We then present several extensions, including a demonstration that the mechanism maintains truthfulness even when authors have only partial rather than complete information about their submission quality. Finally, we discuss future research directions, focusing on the practical implementation of the mechanism and the further development of a theoretical framework inspired by our mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08149v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation</title>
      <link>https://arxiv.org/abs/2402.04355</link>
      <description>arXiv:2402.04355v2 Announce Type: replace-cross 
Abstract: We propose a likelihood-free method for comparing two distributions given samples from each, with the goal of assessing the quality of generative models. The proposed approach, PQMass, provides a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models. PQMass divides the sample space into non-overlapping regions and applies chi-squared tests to the number of data samples that fall within each region, giving a p-value that measures the probability that the bin counts derived from two sets of samples are drawn from the same multinomial distribution. PQMass does not depend on assumptions regarding the density of the true distribution, nor does it rely on training or fitting any auxiliary models. We evaluate PQMass on data of various modalities and dimensions, demonstrating its effectiveness in assessing the quality, novelty, and diversity of generated samples. We further show that PQMass scales well to moderately high-dimensional data and thus obviates the need for feature extraction in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04355v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pablo Lemos, Sammy Sharief, Nikolay Malkin, Salma Salhi, Conner Stone, Laurence Perreault-Levasseur, Yashar Hezaveh</dc:creator>
    </item>
    <item>
      <title>Wasserstein Spatial Depth</title>
      <link>https://arxiv.org/abs/2411.10646</link>
      <description>arXiv:2411.10646v2 Announce Type: replace-cross 
Abstract: Modeling observations as random distributions embedded within Wasserstein spaces is becoming increasingly popular across scientific fields, as it captures the variability and geometric structure of the data more effectively. However, the distinct geometry and unique properties of Wasserstein space pose challenges to the application of conventional statistical tools, which are primarily designed for Euclidean spaces. Consequently, adapting and developing new methodologies for analysis within Wasserstein spaces has become essential. The space of distributions on $\mathbb{R}^d$ with $d&gt;1$ is not linear, and "mimic" the geometry of a Riemannian manifold. In this paper, we extend the concept of statistical depth to distribution-valued data, introducing the notion of Wasserstein spatial depth. This new measure provides a way to rank and order distributions, enabling the development of order-based clustering techniques and inferential tools. We show that Wasserstein spatial depth (WSD) preserves critical properties of conventional statistical depths, notably, ranging within $[0,1]$, transformation invariance, vanishing at infinity, reaching a maximum at the geometric median, and continuity. Additionally, the population WSD has a straightforward plug-in estimator based on sampled empirical distributions. We establish the estimator's consistency and asymptotic normality. Extensive simulation and real-data application showcase the practical efficacy of WSD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10646v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Bachoc, Alberto Gonz\'alez-Sanz, Jean-Michel Loubes, Yisha Yao</dc:creator>
    </item>
  </channel>
</rss>
