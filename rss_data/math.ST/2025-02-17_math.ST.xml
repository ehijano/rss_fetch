<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Feb 2025 15:17:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improved dependence on coherence in eigenvector and eigenvalue estimation error bounds</title>
      <link>https://arxiv.org/abs/2502.09840</link>
      <description>arXiv:2502.09840v1 Announce Type: new 
Abstract: Spectral estimators are fundamental in lowrank matrix models and arise throughout machine learning and statistics, with applications including network analysis, matrix completion and PCA. These estimators aim to recover the leading eigenvalues and eigenvectors of an unknown signal matrix observed subject to noise. While extensive research has addressed the statistical accuracy of spectral estimators under a variety of conditions, most previous work has assumed that the signal eigenvectors are incoherent with respect to the standard basis. This assumption typically arises because of suboptimal dependence on coherence in one or more concentration inequalities. Using a new matrix concentration result that may be of independent interest, we establish estimation error bounds for eigenvector and eigenvalue recovery whose dependence on coherence significantly improves upon prior work. Our results imply that coherence-free bounds can be achieved when the standard deviation of the noise is comparable to its Orlicz 1-norm (i.e., its subexponential norm). This matches known minimax lower bounds under Gaussian noise up to logarithmic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09840v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yan, Keith Levin</dc:creator>
    </item>
    <item>
      <title>Testing degree heterogeneity in directed networks</title>
      <link>https://arxiv.org/abs/2502.09865</link>
      <description>arXiv:2502.09865v1 Announce Type: new 
Abstract: We are concerned with the likelihood ratio tests in the $p_0$ model for testing degree heterogeneity in directed networks. It is an exponential family distribution on directed graphs with the out-degree sequence and the in-degree sequence as naturally sufficient statistics. For two growing dimensional null hypotheses: a specified null $H_{0}: \theta_{i}=\theta_{i}^{0}$ for $i=1,\ldots,r$ and a homogenous null $H_{0}: \theta_{1}=\cdots=\theta_{r}$, we reveal high dimensional Wilks' phenomena that the normalized log-likelihood ratio statistic, $[2\{\ell(\widehat{\bs\theta})-\ell(\widehat{\bs\theta}^{0})\}-r]/(2r)^{1/2}$, converges in distribution to a standard normal distribution as $r\rightarrow \infty$. Here, $\ell( \bs{\theta})$ is the log-likelihood function, $\widehat{\bs{\theta}}$ is the unrestricted maximum likelihood estimator (MLE) of $\bs\theta$, and $\widehat{\bs{\theta}}^0$ is the restricted MLE for $\bs\theta$ under the null $H_{0}$. For the homogenous null $H_0: \theta_1=\cdots=\theta_r$ with a fixed $r$, we establish the Wilks-type theorem that $2\{\ell(\widehat{\bs{\theta}}) - \ell(\widehat{\bs{\theta}}^0)\}$ converges in distribution to a chi-square distribution with $r-1$ degrees of freedom as $n\rightarrow \infty$, not depending on the nuisance parameters. These results extend a recent work by \cite{yan2023likelihood} to directed graphs. Simulation studies and real data analyses illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09865v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Pan, Qiuping Wang, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Zero patterns in multi-way binary contingency tables with uniform margins</title>
      <link>https://arxiv.org/abs/2502.10143</link>
      <description>arXiv:2502.10143v1 Announce Type: new 
Abstract: We study the problem of transforming a multi-way contingency table into an equivalent table with uniform margins and same dependence structure. This is an old question which relates to recent advances in copula modeling for discrete random vectors. In this work, we focus on multi-way binary tables and develop novel theory to show how the zero patterns affect the existence and uniqueness of the transformation as well as its statistical interpretability in terms of dependence structure. The implementation of the theory relies on combinatorial and linear programming techniques. Several examples are described to illustrate the approach and point to interesting future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10143v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Fontana, Elisa Perrone, Fabio Rapallo</dc:creator>
    </item>
    <item>
      <title>Bayesian calculus and predictive characterizations of extended feature allocation models</title>
      <link>https://arxiv.org/abs/2502.10257</link>
      <description>arXiv:2502.10257v1 Announce Type: new 
Abstract: We introduce and study a unified Bayesian framework for extended feature allocations which flexibly captures interactions -- such as repulsion or attraction -- among features and their associated weights. We provide a complete Bayesian analysis of the proposed model and specialize our general theory to noteworthy classes of priors. This includes a novel prior based on determinantal point processes, for which we show promising results in a spatial statistics application. Within the general class of extended feature allocations, we further characterize those priors that yield predictive probabilities of discovering new features depending either solely on the sample size or on both the sample size and the distinct number of observed features. These predictive characterizations, known as "sufficientness" postulates, have been extensively studied in the literature on species sampling models starting from the seminal contribution of the English philosopher W.E. Johnson for the Dirichlet distribution. Within the feature allocation setting, existing predictive characterizations are limited to very specific examples; in contrast, our results are general, providing practical guidance for prior selection. Additionally, our approach, based on Palm calculus, is analytical in nature and yields a novel characterization of the Poisson point process through its reduced Palm kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10257v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Federico Camerlenghi, Lorenzo Ghilotti</dc:creator>
    </item>
    <item>
      <title>A new and flexible class of sharp asymptotic time-uniform confidence sequences</title>
      <link>https://arxiv.org/abs/2502.10380</link>
      <description>arXiv:2502.10380v1 Announce Type: new 
Abstract: Confidence sequences are anytime-valid analogues of classical confidence intervals that do not suffer from multiplicity issues under optional continuation of the data collection. As in classical statistics, asymptotic confidence sequences are a nonparametric tool showing under which high-level assumptions asymptotic coverage is achieved so that they also give a certain robustness guarantee against distributional deviations. In this paper, we propose a new flexible class of confidence sequences yielding sharp asymptotic time-uniform confidence sequences under mild assumptions. Furthermore, we highlight the connection to corresponding sequential testing problems and detail the underlying limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10380v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Gnettner, Claudia Kirch</dc:creator>
    </item>
    <item>
      <title>Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs</title>
      <link>https://arxiv.org/abs/2502.09832</link>
      <description>arXiv:2502.09832v1 Announce Type: cross 
Abstract: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erd\H{o}s-R\'enyi graphs $\mathcal G(n,q;\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $\rho&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\mathcal S(n,\tfrac{\lambda}{n};k,\epsilon;s)$ and a pair of independent stochastic block models $\mathcal S(n,\tfrac{\lambda s}{n};k,\epsilon)$ when $\epsilon^2 \lambda s&lt;1$ lies below the Kesten-Stigum (KS) threshold and $s&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{CDGL24+}.
  One of the main ingredient in our proof is to derive certain forms of \emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\mathbb{P}$ and $\mathbb{Q}$ based on the sample $\mathsf Y$. We show that if the low-degree advantage $\mathsf{Adv}_{\leq D} \big( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}} \big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\mathcal A$ such that $\mathbb{Q}(\mathcal A(\mathsf Y)=0)=1-o(1)$ and $\mathbb{P}(\mathcal A(\mathsf Y)=1)=\Omega(1)$. This framework provides a useful tool for performing reductions between different inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09832v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions for Time Series</title>
      <link>https://arxiv.org/abs/2502.10065</link>
      <description>arXiv:2502.10065v1 Announce Type: cross 
Abstract: This paper is the first to propose valid inference tools, based on self-normalization, in time series expected shortfall regressions. In doing so, we propose a novel two-step estimator for expected shortfall regressions which is based on convex optimization in both steps (rendering computation easy) and it only requires minimization of quantile losses and squared error losses (methods for both of which are implemented in every standard statistical computing package). As a corollary, we also derive self-normalized inference tools in time series quantile regressions. Extant methods, based on a bootstrap or direct estimation of the long-run variance, are computationally more involved, require the choice of tuning parameters and have serious size distortions when the regression errors are strongly serially dependent. In contrast, our inference tools only require estimates of the quantile regression parameters that are computed on an expanding window and are correctly sized. Simulations show the advantageous finite-sample properties of our methods. Finally, two applications to stock return predictability and to Growth-at-Risk demonstrate the practical usefulness of the developed inference tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10065v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yannick Hoga, Christian Schulz</dc:creator>
    </item>
    <item>
      <title>Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses</title>
      <link>https://arxiv.org/abs/2502.10161</link>
      <description>arXiv:2502.10161v1 Announce Type: cross 
Abstract: Reasoning about fairness through correlation-based notions is rife with pitfalls. The 1973 University of California, Berkeley graduate school admissions case from Bickel et. al. (1975) is a classic example of one such pitfall, namely Simpson's paradox. The discrepancy in admission rates among males and female applicants, in the aggregate data over all departments, vanishes when admission rates per department are examined. We reason about the Berkeley graduate school admissions case through a causal lens. In the process, we introduce a statistical test for causal hypothesis testing based on Pearl's instrumental-variable inequalities (Pearl 1995). We compare different causal notions of fairness that are based on graphical, counterfactual and interventional queries on the causal model, and develop statistical tests for these notions that use only observational data. We study the logical relations between notions, and show that while notions may not be equivalent, their corresponding statistical tests coincide for the case at hand. We believe that a thorough case-based causal analysis helps develop a more principled understanding of both causal hypothesis testing and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10161v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourbh Bhadane, Joris M. Mooij, Philip Boeken, Onno Zoeter</dc:creator>
    </item>
    <item>
      <title>A Mechanistic Framework for Collider Detection in Observational Data</title>
      <link>https://arxiv.org/abs/2502.10317</link>
      <description>arXiv:2502.10317v1 Announce Type: cross 
Abstract: Understanding directionality is crucial for identifying causal structures from observational data. A key challenge lies in detecting collider structures, where a $V$--structure is formed between a child node $Z$ receiving directed edges from parents $X$ and $Y$, denoted by $X \rightarrow Z \leftarrow Y$. Traditional causal discovery approaches, such as constraint-based and score-based structure learning algorithms, do not provide statistical inference on estimated pathways and are often sensitive to latent confounding. To overcome these issues, we introduce methodology to quantify directionality in collider structures using a pair of conditional asymmetry coefficients to simultaneously examine validity of the pathways $Y \rightarrow Z$ and $X \rightarrow Z$ in the collider structure. These coefficients are based on Shannon's differential entropy. Leveraging kernel-based conditional density estimation and a nonparametric smoothing technique, we utilise our proposed method to estimate collider structures and provide uncertainty quantification.
  Simulation studies demonstrate that our method outperforms existing structure learning algorithms in accurately identifying collider structures. We further apply our approach to investigate the role of blood pressure as a collider in epigenetic DNA methylation, uncovering novel insights into the genetic regulation of blood pressure. This framework represents a significant advancement in causal structure learning, offering a robust, nonparametric method for collider detection with practical applications in biostatistics and epidemiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10317v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumik Purkayastha, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Dimension-free Score Matching and Time Bootstrapping for Diffusion Models</title>
      <link>https://arxiv.org/abs/2502.10354</link>
      <description>arXiv:2502.10354v1 Announce Type: cross 
Abstract: Diffusion models generate samples by estimating the score function of the target distribution at various noise levels. The model is trained using samples drawn from the target distribution, progressively adding noise. In this work, we establish the first (nearly) dimension-free sample complexity bounds for learning these score functions, achieving a double exponential improvement in dimension over prior results. A key aspect of our analysis is the use of a single function approximator to jointly estimate scores across noise levels, a critical feature of diffusion models in practice which enables generalization across timesteps. Our analysis introduces a novel martingale-based error decomposition and sharp variance bounds, enabling efficient learning from dependent data generated by Markov processes, which may be of independent interest. Building on these insights, we propose Bootstrapped Score Matching (BSM), a variance reduction technique that utilizes previously learned scores to improve accuracy at higher noise levels. These results provide crucial insights into the efficiency and effectiveness of diffusion models for generative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10354v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Kumar, Dheeraj Nagaraj, Purnamrita Sarkar</dc:creator>
    </item>
    <item>
      <title>Consistency of Oblique Decision Tree and its Boosting and Random Forest</title>
      <link>https://arxiv.org/abs/2211.12653</link>
      <description>arXiv:2211.12653v4 Announce Type: replace 
Abstract: Classification and Regression Tree (CART), Random Forest (RF) and Gradient Boosting Tree (GBT) are probably the most popular set of statistical learning methods. However, their statistical consistency can only be proved under very restrictive assumptions on the underlying regression function. As an extension to standard CART, the oblique decision tree (ODT), which uses linear combinations of predictors as partitioning variables, has received much attention. ODT tends to perform numerically better than CART and requires fewer partitions. In this paper, we show that ODT is consistent for very general regression functions as long as they are $L^2$ integrable. Then, we prove the consistency of the ODT-based random forest (ODRF), whether fully grown or not. Finally, we propose an ensemble of GBT for regression by borrowing the technique of orthogonal matching pursuit and study its consistency under very mild conditions on the tree structure. After refining existing computer packages according to the established theory, extensive experiments on real data sets show that both our ensemble boosting trees and ODRF have noticeable overall improvements over RF and other forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12653v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zhan, Yu Liu, Yingcun Xia</dc:creator>
    </item>
    <item>
      <title>Linear parametric model checking for functional time series</title>
      <link>https://arxiv.org/abs/2303.09644</link>
      <description>arXiv:2303.09644v5 Announce Type: replace 
Abstract: The presented methodology for testing the goodness-of-fit of an Autoregressive Hilbertian model (ARH(1) model) provides an infinite-dimensional formulation of the approach proposed in Koul and Stute (1999), based on empirical process marked by residuals. Applying a central and functional central limit result for Hilbert-valued martingale difference sequences, the Wiener-type limiting process of the H-valued empirical process indexed by an H-valued covariate is obtained. The performance of the proposed testing procedure is illustrated by simulations. Consistency of this asymptotically distributed free test is derived. Since, in practice, the autocorrelation and autocovariance operators of the ARH(1) process are unknown, the uniform asymptotic equivalence in probability in H-norm of the test statistics under totally and misspecified scenarios is established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09644v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>W. Gonz\'alez-Manteiga, M. D. Ruiz-Medina, M. Febrero-Bande</dc:creator>
    </item>
    <item>
      <title>Asymptotic Breakdown Point Analysis for a General Class of Minimum Divergence Estimators</title>
      <link>https://arxiv.org/abs/2304.07466</link>
      <description>arXiv:2304.07466v3 Announce Type: replace 
Abstract: Robust inference based on the minimization of statistical divergences has proved to be a useful alternative to classical techniques based on maximum likelihood and related methods. Basu et al. (1998) introduced the density power divergence (DPD) family as a measure of discrepancy between two probability density functions and used this family for robust estimation of the parameter for independent and identically distributed data. Ghosh et al. (2017) proposed a more general class of divergence measures, namely the S-divergence family and discussed its usefulness in robust parametric estimation through several asymptotic properties and some numerical illustrations. In this paper, we develop the results concerning the asymptotic breakdown point for the minimum S-divergence estimators (in particular the minimum DPD estimator) under general model setups. The primary result of this paper provides lower bounds to the asymptotic breakdown point of these estimators which are independent of the dimension of the data, in turn corroborating their usefulness in robust inference under high dimensional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07466v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhrajyoty Roy, Abir Sarkar, Abhik Ghosh, Ayanendranath Basu</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v3 Announce Type: replace 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v3</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Multivariate root-n-consistent smoothing parameter free matching estimators and estimators of inverse density weighted expectations</title>
      <link>https://arxiv.org/abs/2407.08494</link>
      <description>arXiv:2407.08494v2 Announce Type: replace 
Abstract: Expected values weighted by the inverse of a multivariate density or, equivalently, Lebesgue integrals of regression functions with multivariate regressors occur in various areas of applications, including estimating average treatment effects, nonparametric estimators in random coefficient regression models or deconvolution estimators in Berkson errors-in-variables models. The frequently used nearest-neighbor and matching estimators suffer from bias problems in multiple dimensions. By using polynomial least squares fits on each cell of the $K^{\text{th}}$-order Voronoi tessellation for sufficiently large $K$, we develop novel modifications of nearest-neighbor and matching estimators which again converge at the parametric $\sqrt n $-rate under mild smoothness assumptions on the unknown regression function and without any smoothness conditions on the unknown density of the covariates. We stress that in contrast to competing methods for correcting for the bias of matching estimators, our estimators do not involve nonparametric function estimators and in particular do not rely on sample-size dependent smoothing parameters. We complement the upper bounds with appropriate lower bounds derived from information-theoretic arguments, which show that some smoothness of the regression function is indeed required to achieve the parametric rate. Simulations illustrate the practical feasibility of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08494v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajo Holzmann, Alexander Meister</dc:creator>
    </item>
    <item>
      <title>Functional Sieve Bootstrap for the Partial Sum Process with Application to Change-Point Detection</title>
      <link>https://arxiv.org/abs/2408.05071</link>
      <description>arXiv:2408.05071v3 Announce Type: replace 
Abstract: This paper applies the functional sieve bootstrap (FSB) to estimate the distribution of the partial sum process for time series stemming from a weakly stationary functional process. Consistency of the FSB procedure under weak assumptions on the underlying functional process is established. This result allows for the application of the FSB procedure to testing for a change-point in the mean of a functional time series using the CUSUM-statistic. We show that the FSB asymptotically correctly estimates critical values of the CUSUM-based test under the null-hypothesis. Consistency of the FSB-based test under local alternatives also is proven. The finite sample performance of the procedure is studied via simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05071v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Efstathios Paparoditis, Lea Wegner, Martin Wendler</dc:creator>
    </item>
  </channel>
</rss>
