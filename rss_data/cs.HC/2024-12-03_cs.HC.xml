<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 02:51:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Can LLM "Self-report"?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots</title>
      <link>https://arxiv.org/abs/2412.00207</link>
      <description>arXiv:2412.00207v1 Announce Type: new 
Abstract: Personality design plays an important role in chatbot development. From rule-based chatbots to LLM-based chatbots, evaluating the effectiveness of personality design has become more challenging due to the increasingly open-ended interactions. A recent popular approach uses self-report questionnaires to assess LLM-based chatbots' personality traits. However, such an approach has raised serious validity concerns: chatbot's "self-report" personality may not align with human perception based on their interaction. Can LLM-based chatbots "self-report" their personality? We created 500 chatbots with distinct personality designs and evaluated the validity of self-reported personality scales in LLM-based chatbot's personality evaluation. Our findings indicate that the chatbot's answers on human personality scales exhibit weak correlations with both user perception and interaction quality, which raises both criterion and predictive validity concerns of such a method. Further analysis revealed the role of task context and interaction in the chatbot's personality design assessment. We discuss the design implications for building contextualized and interactive evaluation of the chatbot's personality design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00207v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Huiqi Zou, Pengda Wang, Zihan Yan, Tianjun Sun, Ziang Xiao</dc:creator>
    </item>
    <item>
      <title>WiReSens Toolkit: An Open-source Platform towards Accessible Wireless Tactile Sensing</title>
      <link>https://arxiv.org/abs/2412.00247</link>
      <description>arXiv:2412.00247v1 Announce Type: new 
Abstract: Tactile sensors present a powerful means of capturing, analyzing, and augmenting human-environment interactions. Accelerated by advancements in design and manufacturing, resistive matrix-based sensing has emerged as a promising method for developing scalable and robust tactile sensors. However, the development of portable, adaptive, and long lasting resistive tactile sensing systems remains a challenge. To address this, we introduce WiReSens Toolkit. Our platform provides open-source hardware and software libraries to configure multi-sender, power-efficient, and adaptive wireless tactile sensing systems in as fast as ten minutes. We demonstrate our platform's flexibility by using it to prototype several applications such as musical gloves, gait monitoring shoe soles, and IoT-enabled smart home systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00247v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Devin Murphy, Junyi Zhu, Paul Liang, Wojciech Matusik, Yiyue Luo</dc:creator>
    </item>
    <item>
      <title>Towards Fair Pay and Equal Work: Imposing View Time Limits in Crowdsourced Image Classification</title>
      <link>https://arxiv.org/abs/2412.00260</link>
      <description>arXiv:2412.00260v1 Announce Type: new 
Abstract: Crowdsourcing is a common approach to rapidly annotate large volumes of data in machine learning applications. Typically, crowd workers are compensated with a flat rate based on an estimated completion time to meet a target hourly wage. Unfortunately, prior work has shown that variability in completion times among crowd workers led to overpayment by 168% in one case, and underpayment by 16% in another. However, by setting a time limit for task completion, it is possible to manage the risk of overpaying or underpaying while still facilitating flat rate payments. In this paper, we present an analysis of the impact of a time limit on crowd worker performance and satisfaction. We conducted a human study with a maximum view time for a crowdsourced image classification task. We find that the impact on overall crowd worker performance diminishes as view time increases. Despite some images being challenging under time limits, a consensus algorithm remains effective at preserving data quality and filters images needing more time. Additionally, crowd workers' consistent performance throughout the time-limited task indicates sustained effort, and their psychometric questionnaire scores show they prefer shorter limits. Based on our findings, we recommend implementing task time limits as a practical approach to making compensation more equitable and predictable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00260v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gordon Lim, Stefan Larson, Yu Huang, Kevin Leach</dc:creator>
    </item>
    <item>
      <title>2-Factor Retrieval for Improved Human-AI Decision Making in Radiology</title>
      <link>https://arxiv.org/abs/2412.00372</link>
      <description>arXiv:2412.00372v1 Announce Type: new 
Abstract: Human-machine teaming in medical AI requires us to understand to what degree a trained clinician should weigh AI predictions. While previous work has shown the potential of AI assistance at improving clinical predictions, existing clinical decision support systems either provide no explainability of their predictions or use techniques like saliency and Shapley values, which do not allow for physician-based verification. To address this gap, this study compares previously used explainable AI techniques with a newly proposed technique termed '2-factor retrieval (2FR)', which is a combination of interface design and search retrieval that returns similarly labeled data without processing this data. This results in a 2-factor security blanket where: (a) correct images need to be retrieved by the AI; and (b) humans should associate the retrieved images with the current pathology under test. We find that when tested on chest X-ray diagnoses, 2FR leads to increases in clinician accuracy, with particular improvements when clinicians are radiologists and have low confidence in their decision. Our results highlight the importance of understanding how different modes of human-AI decision making may impact clinician accuracy in clinical decision support systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00372v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jim Solomon, Laleh Jalilian, Alexander Vilesov, Meryl Mathew, Tristan Grogan, Arash Bedayat, Achuta Kadambi</dc:creator>
    </item>
    <item>
      <title>Seismocardiography for Emotion Recognition: A Study on EmoWear with Insights from DEAP</title>
      <link>https://arxiv.org/abs/2412.00411</link>
      <description>arXiv:2412.00411v1 Announce Type: new 
Abstract: Emotions have a profound impact on our daily lives, influencing our thoughts, behaviors, and interactions, but also our physiological reactions. Recent advances in wearable technology have facilitated studying emotions through cardio-respiratory signals. Accelerometers offer a non-invasive, convenient, and cost-effective method for capturing heart- and pulmonary-induced vibrations on the chest wall, specifically Seismocardiography (SCG) and Accelerometry-Derived Respiration (ADR). Their affordability, wide availability, and ability to provide rich contextual data make accelerometers ideal for everyday use. While accelerometers have been used as part of broader modality fusions for Emotion Recognition (ER), their stand-alone potential via SCG and ADR remains unexplored. Bridging this gap could significantly help the embedding of ER into real-world applications. To address this gap, we introduce SCG as a novel modality for ER and evaluate its performance using the EmoWear dataset. First, we replicate the single-trial emotion classification pipeline from the DEAP dataset study, achieving similar results. Then we use our validated pipeline to train models that predict affective valence-arousal states using SCG and compare it against established cardiac signals, Electrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG is a viable modality for ER, achieving similar performance to ECG and BVP. By combining ADR with SCG, we achieved a working ER framework that only requires a single chest-worn accelerometer. These findings pave the way for integrating ER into real-world, enabling seamless affective computing in everyday life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00411v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Hasan Rahmani, Rafael Berkvens, Maarten Weyn</dc:creator>
    </item>
    <item>
      <title>Point n Move: Designing a Glove-Based Pointing Device</title>
      <link>https://arxiv.org/abs/2412.00501</link>
      <description>arXiv:2412.00501v1 Announce Type: new 
Abstract: In-person presentations commonly depend on projectors or screens, requiring input devices for slide transitions and laser pointing. This paper introduces a glove-based pointer device that integrates these functions, offering an alternative to conventional tools. The device leverages accelerometer and gyroscope technology to enhance precision and usability. We evaluated its performance by comparing it to the original CheerPod interface in hierarchical menu navigation tasks, involving participants aged 18 to 25. Results indicate task completion times ranging from 9 to 15 seconds with the proposed device, highlighting its efficiency and consistency. While the original CheerPod interface performed adequately, the glove-based pointer demonstrated advantages in reliability across tasks. These findings contribute to the design considerations for wearable input devices and suggest pathways for future improvements in presentation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00501v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of CHIRP 2024: Transforming HCI Research in the Philippines Workshop</arxiv:journal_reference>
      <dc:creator>Sealtiel B. Dy, Robert Joachim O. Encinas, Daphne Janelyn L. Go, Kyle Carlo C. Lasala, Bentley Andrew Y. Lu, Maria Monica Manlises, Jordan Aiko Deja</dc:creator>
    </item>
    <item>
      <title>How Fitts' Fits in 3D: A Tangible Twist on Spatial Tasks</title>
      <link>https://arxiv.org/abs/2412.00506</link>
      <description>arXiv:2412.00506v1 Announce Type: new 
Abstract: Expanding Fitts' Law into a 3D context, we analyze PointARs, a mixed reality system that teaches pointer skills through an object manipulation task. Nine distinct configurations, varying in object sizes and distances, were explored to evaluate task complexity using metrics such as completion time, error rate, and throughput. Our results support Fitts' Law, showing that increased distances generally increase task difficulty. However, contrary to its predictions, larger objects also led to higher complexity, possibly due to the system's limitations in tracking them. Based on these findings, we suggest using tangible cubes between 1.5" and 2" in size and limiting the distance between objects to 2" for optimal interaction in the system's 3D space. Future research should explore additional configurations and shapes to further validate Fitts' Law in the context of 3D object manipulation in systems like PointARs. This could help refine guidelines for designing mixed reality interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00506v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of CHIRP 2024: Transforming HCI Research in the Philippines Workshop</arxiv:journal_reference>
      <dc:creator>Faith Griffin, Kevin Abelgas, Kriz Royce Tahimic, Andrei Kevin Chua, Jordan Aiko Deja, Tyrone Justin Sta. Maria</dc:creator>
    </item>
    <item>
      <title>Alexa, I Wanna See You: Envisioning Smart Home Assistants for the Deaf and Hard-of-Hearing</title>
      <link>https://arxiv.org/abs/2412.00514</link>
      <description>arXiv:2412.00514v1 Announce Type: new 
Abstract: Smart Home Assistants (SHAs) have become ubiquitous in modern households, offering convenience and efficiency through its voice interface. However, for Deaf and Hard-of-Hearing (DHH) individuals, the reliance on auditory and textual feedback through a screen poses significant challenges. Existing solutions primarily focus on sign language input but overlook the need for seamless interaction and feedback modalities. This paper envisions SHAs designed specifically for DHH users, focusing on accessibility and inclusion. We discuss integrating augmented reality (AR) for visual feedback, support for multimodal input, including sign language and gestural commands, and context awareness through sound detection. Our vision highlights the importance of considering the diverse communication needs of the DHH community in developing SHA to ensure equitable access to smart home technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00514v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of CHIRP 2024: Transforming HCI Research in the Philippines Workshop</arxiv:journal_reference>
      <dc:creator>Tyrone Justin Sta. Maria, Jordan Aiko Deja</dc:creator>
    </item>
    <item>
      <title>Simon Says: Exploring the Importance of Notification Design Formats on User Engagement</title>
      <link>https://arxiv.org/abs/2412.00531</link>
      <description>arXiv:2412.00531v1 Announce Type: new 
Abstract: Push notifications are brief messages that users frequently encounter in their daily lives. However, the volume of notifications can lead to information overload, making it challenging for users to engage effectively. This study investigates how notification behavior and color influence user interaction and perception. To explore this, we developed an app prototype that tracks user interactions with notifications, categorizing them as accepted, dismissed, or ignored. After each interaction, users were asked to complete a survey regarding their perception of the notifications. The study focused on how different notification colors might affect the likelihood of acceptance and perceived importance. The results reveal that certain colors were more likely to be accepted and were perceived as more important compared to others, suggesting that both color and behavior play significant roles in shaping user engagement with notifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00531v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of CHIRP 2024: Transforming HCI Research in the Philippines Workshop</arxiv:journal_reference>
      <dc:creator>Hans Matthew Abello, Maxine Beatriz Badiola, Mark John Custer, Lorane Bernadeth Fausto, Patrick Josh Leonida, Denzel Bryan Yongco, Jordan Aiko Deja</dc:creator>
    </item>
    <item>
      <title>ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM</title>
      <link>https://arxiv.org/abs/2412.00627</link>
      <description>arXiv:2412.00627v1 Announce Type: new 
Abstract: Cooking meals can be difficult, causing many to use cookbooks and online recipes, which results in missing ingredients, nutritional hazards, unsatisfactory meals. Using Augmented Reality (AR) can address this issue, however, current AR cooking applications have poor user interfaces and limited accessibility. This paper proposes a prototype of an iOS application that integrates AR and Computer Vision (CV) into the cooking process. We leverage Google's Gemini Large Language Model (LLM) to identify ingredients based on the camera's field of vision, and generate recipe choices with their nutritional information. Additionally, this application uses Apple's ARKit to create an AR user interface compatible with iOS devices. Users can personalize their meal suggestions by inputting their dietary preferences and rating each meal. The application's effectiveness is evaluated through user experience surveys. This application contributes to the field of accessible cooking assistance technologies, aiming to reduce food wastage and improve the meal planning experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00627v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rithik Vir, Parsa Madinei</dc:creator>
    </item>
    <item>
      <title>Collective Creation of Intimacy: Exploring the Cosplay Commission Practice within the Otome Game Community in China</title>
      <link>https://arxiv.org/abs/2412.00630</link>
      <description>arXiv:2412.00630v1 Announce Type: new 
Abstract: Cosplay commission is a newly emergent form of commodified intimacy within the Otome game community in China. This paper presents an interview-based study to explore the motivations, practices, perceived benefits, and challenges experienced by participants in cosplay commissions. Our analysis reveals that these intimate interactions enable participants to co-create personalized support, functioning as mechanisms for self-exploration and emotional restoration. However, we also identify several notable challenges, including emotional vulnerability, dependence, and the blurring of boundaries between performative roles and genuine emotional connections. While digital platforms facilitate hybrid communication in cosplay commissions, they often lack adequate safeguards to ensure secure and meaningful engagement. This preliminary work provides insights into the dynamics of hybrid intimate interactions and their potential to foster personalized, meaningful experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00630v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihao Zhou, Haowei Xu, Lili Zhang, Shengdong Zhao</dc:creator>
    </item>
    <item>
      <title>SynthLens: Visual Analytics for Facilitating Multi-step Synthetic Route Design</title>
      <link>https://arxiv.org/abs/2412.00729</link>
      <description>arXiv:2412.00729v1 Announce Type: new 
Abstract: Designing synthetic routes for novel molecules is pivotal in various fields like medicine and chemistry. In this process, researchers need to explore a set of synthetic reactions to transform starting molecules into intermediates step by step until the target novel molecule is obtained. However, designing synthetic routes presents challenges for researchers. First, researchers need to make decisions among numerous possible synthetic reactions at each step, considering various criteria (e.g., yield, experimental duration, and the count of experimental steps) to construct the synthetic route. Second, they must consider the potential impact of one choice at each step on the overall synthetic route. To address these challenges, we proposed SynthLens, a visual analytics system to facilitate the iterative construction of synthetic routes by exploring multiple possibilities for synthetic reactions at each step of construction. Specifically, we have introduced a tree-form visualization in SynthLens to compare and evaluate all the explored routes at various exploration steps, considering both the exploration step and multiple criteria. Our system empowers researchers to consider their construction process comprehensively, guiding them toward promising exploration directions to complete the synthetic route. We validated the usability and effectiveness of SynthLens through a quantitative evaluation and expert interviews, highlighting its role in facilitating the design process of synthetic routes. Finally, we discussed the insights of SynthLens to inspire other multi-criteria decision-making scenarios with visual analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00729v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qipeng Wang, Rui Sheng, Shaolun Ruan, Xiaofu Jin, Chuhan Shi, Min Zhu</dc:creator>
    </item>
    <item>
      <title>MapIO: Embodied Interaction for the Accessibility of Tactile Maps Through Augmented Touch Exploration and Conversation</title>
      <link>https://arxiv.org/abs/2412.00946</link>
      <description>arXiv:2412.00946v1 Announce Type: new 
Abstract: For individuals who are blind or have low vision, tactile maps provide essential spatial information but are limited in the amount of data they can convey. Digitally augmented tactile maps enhance these capabilities with audio feedback, thereby combining the tactile feedback provided by the map with an audio description of the touched elements. In this context, we explore an embodied interaction paradigm to augment tactile maps with conversational interaction based on Large Language Models, thus enabling users to obtain answers to arbitrary questions regarding the map. We analyze the type of questions the users are interested in asking, engineer the Large Language Model's prompt to provide reliable answers, and study the resulting system with a set of 10 participants, evaluating how the users interact with the system, its usability, and user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00946v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Manzoni, Sergio Mascetti, Dragan Ahmetovic, Ryan Crabb, James M. Coughlan</dc:creator>
    </item>
    <item>
      <title>Generating AI Literacy MCQs: A Multi-Agent LLM Approach</title>
      <link>https://arxiv.org/abs/2412.00970</link>
      <description>arXiv:2412.00970v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is transforming society, making it crucial to prepare the next generation through AI literacy in K-12 education. However, scalable and reliable AI literacy materials and assessment resources are lacking. To address this gap, our study presents a novel approach to generating multiple-choice questions (MCQs) for AI literacy assessments. Our method utilizes large language models (LLMs) to automatically generate scalable, high-quality assessment questions. These questions align with user-provided learning objectives, grade levels, and Bloom's Taxonomy levels. We introduce an iterative workflow incorporating LLM-powered critique agents to ensure the generated questions meet pedagogical standards. In the preliminary evaluation, experts expressed strong interest in using the LLM-generated MCQs, indicating that this system could enrich existing AI literacy materials and provide a valuable addition to the toolkit of K-12 educators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00970v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3641555.3705189</arxiv:DOI>
      <dc:creator>Jiayi Wang, Ruiwei Xiao, Ying-Jui Tseng</dc:creator>
    </item>
    <item>
      <title>Towards Understanding the Impact of Guidance in Data Visualization Systems for Domain Experts</title>
      <link>https://arxiv.org/abs/2412.01024</link>
      <description>arXiv:2412.01024v2 Announce Type: new 
Abstract: Guided data visualization systems are highly useful for domain experts to highlight important trends in their large-scale and complex datasets. However, more work is needed to understand the impact of guidance on interpreting data visualizations as well as on the resulting use of visualizations when communicating insights. We conducted two user studies with domain experts and found that experts benefit from a guided coarse-to-fine structure when using data visualization systems, as this is the same structure in which they communicate findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01024v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sherry Qiu, Holly Rushmeier, Kim R. M. Blenman</dc:creator>
    </item>
    <item>
      <title>Friction jointing of distributed rigid capacitors to stretchable liquid metal coil for full-body wireless charging clothing</title>
      <link>https://arxiv.org/abs/2412.01134</link>
      <description>arXiv:2412.01134v1 Announce Type: new 
Abstract: For full-body wireless power transfer (WPT), a liquid metal (LM)-based meandered textile coil has been proposed. Multiple rigid capacitors must be inserted in a long coil for efficiency; however, the conventional adhesive jointing suffers from the fragile connection between a rubber tube filled with LM and the capacitor due to the poor adhesion of the rubbers. This paper presents a friction-based jointing, which covers the capacitor with a rigid capsule to enhance the frictional force between the tube and capsule. By experimentally optimizing the capsule design, the LM coil with capacitors showed 3.1 times higher stretch tolerance (31.8 N) and 3.5 times higher bending tolerance (25.9 N) than the adhesive jointing. Moreover, the WPT garment prototype shows excellent mechanical durability against repeated stretching and washing over 100 times. Our full-body meandered textile coil can enable wireless charging to wearable devices around the body for long-term continuous healthcare monitoring, activity recognition, and AR/VR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01134v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Sato, Shinto Watanabe, Ryo Takahashi, Wakako Yukita, Tomoyuki Yokota, Takao Someya, Yoshihito Kawahara, Eiji Iwase, Junya Kurumida</dc:creator>
    </item>
    <item>
      <title>AR-Facilitated Safety Inspection and Fall Hazard Detection on Construction Sites</title>
      <link>https://arxiv.org/abs/2412.01273</link>
      <description>arXiv:2412.01273v1 Announce Type: new 
Abstract: Together with industry experts, we are exploring the potential of head-mounted augmented reality to facilitate safety inspections on high-rise construction sites. A particular concern in the industry is inspecting perimeter safety screens on higher levels of construction sites, intended to prevent falls of people and objects. We aim to support workers performing this inspection task by tracking which parts of the safety screens have been inspected. We use machine learning to automatically detect gaps in the perimeter screens that require closer inspection and remediation and to automate reporting. This work-in-progress paper describes the problem, our early progress, concerns around worker privacy, and the possibilities to mitigate these.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01273v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiazhou Liu, Aravinda S. Rao, Fucai Ke, Tim Dwyer, Benjamin Tag, Pari Delir Haghighi</dc:creator>
    </item>
    <item>
      <title>Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models</title>
      <link>https://arxiv.org/abs/2412.01353</link>
      <description>arXiv:2412.01353v1 Announce Type: new 
Abstract: In recent times, more and more people are posting about their mental states across various social media platforms. Leveraging this data, AI-based systems can be developed that help in assessing the mental health of individuals, such as suicide risk. This paper is a study done on suicidal risk assessments using Reddit data leveraging Base language models to identify patterns from social media posts. We have demonstrated that using smaller language models, i.e., less than 500M parameters, can also be effective in contrast to LLMs with greater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on suicide risk prediction task that utilized both the labeled and unlabeled Reddit data and tackled class imbalance by data augmentation using GPT-2 model. Our Su-RoBERTa model attained a 69.84% weighted F1 score during the Final evaluation. This paper demonstrates the effectiveness of Base language models for the analysis of the risk factors related to mental health with an efficient computation pipeline</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01353v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chayan Tank, Shaina Mehta, Sarthak Pol, Vinayak Katoch, Avinash Anand, Raj Jaiswal, Rajiv Ratn Shah</dc:creator>
    </item>
    <item>
      <title>Dual Prototyping with Domain and Class Prototypes for Affective Brain-Computer Interface in Unseen Target Conditions</title>
      <link>https://arxiv.org/abs/2412.00082</link>
      <description>arXiv:2412.00082v1 Announce Type: cross 
Abstract: EEG signals have emerged as a powerful tool in affective brain-computer interfaces, playing a crucial role in emotion recognition. However, current deep transfer learning-based methods for EEG recognition face challenges due to the reliance of both source and target data in model learning, which significantly affect model performance and generalization. To overcome this limitation, we propose a novel framework (PL-DCP) and introduce the concepts of feature disentanglement and prototype inference. The dual prototyping mechanism incorporates both domain and class prototypes: domain prototypes capture individual variations across subjects, while class prototypes represent the ideal class distributions within their respective domains. Importantly, the proposed PL-DCP framework operates exclusively with source data during training, meaning that target data remains completely unseen throughout the entire process. To address label noise, we employ a pairwise learning strategy that encodes proximity relationships between sample pairs, effectively reducing the influence of mislabeled data. Experimental validation on the SEED and SEED-IV datasets demonstrates that PL-DCP, despite not utilizing target data during training, achieves performance comparable to deep transfer learning methods that require both source and target data. This highlights the potential of PL-DCP as an effective and robust approach for EEG-based emotion recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00082v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangli Li, Zhehao Zhou, Tuo Sun, Ping Tan, Li Zhang, Zhen Liang</dc:creator>
    </item>
    <item>
      <title>Towards the Ultimate Programming Language: Trust and Benevolence in the Age of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2412.00206</link>
      <description>arXiv:2412.00206v1 Announce Type: cross 
Abstract: This article explores the evolving role of programming languages in the context of artificial intelligence. It highlights the need for programming languages to ensure human understanding while eliminating unnecessary implementation details and suggests that future programs should be designed to recognize and actively support user interests. The vision includes a three-level process: using natural language for requirements, translating it into a precise system definition language, and finally optimizing the code for performance. The concept of an "Ultimate Programming Language" is introduced, emphasizing its role in maintaining human control over machines. Trust, reliability, and benevolence are identified as key elements that will enhance cooperation between humans and AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00206v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bartosz Sawicki, Micha{\l} \'Smia{\l}ek, Bart{\l}omiej Skowron</dc:creator>
    </item>
    <item>
      <title>Fine Tuning Large Language Models to Deliver CBT for Depression</title>
      <link>https://arxiv.org/abs/2412.00251</link>
      <description>arXiv:2412.00251v1 Announce Type: cross 
Abstract: Cognitive Behavioral Therapy (CBT) is a well-established, evidence-based treatment for Major Depressive Disorder. Unfortunately, there exist significant barriers to individuals accessing CBT, including cost, scarcity of therapists and stigma. This study explores the feasibility of fine-tuning small open weight large language models (LLMs) to deliver CBT for depression. Using 58 sets of synthetic CBT transcripts generated by the Nous Research fine-tune of Llama 3.1 405b, we fine-tuned three models: Mistral 7b v0.3, Qwen 2.5 7b, and Llama 3.1 8b. CBT fidelity was evaluated through a modified Cognitive Therapy Rating Scale (CTRS). All fine-tuned models were compared against each other, as well as their instruct-tuned variants. Simulated patient transcripts were generated for the purpose of evaluating model performance, with the instruct and CBT-tuned models acting as the therapist and DeepSeek-V2.5 acting as the patient. These simulated transcripts were evaluated on a modified CTRS by Gemini 1.5 Pro-002. Our findings demonstrated that the CBT-tuned models significantly outperformed their instruct-tuned counterparts, with an average improvement of 11.33 points (p &lt; 0.001) on total CTRS score. Llama 3.1 8b had the strongest performance (mean CTRS score 67.86 +/- 7.24), followed by Qwen 2.5 7b (64.28 +/- 9.55) and Mistral 7b v0.3 (64.17 +/- 9.79), with these differences between models being statistically significant. The CBT-tuned models were competent in implementing core CBT techniques and providing empathetic responses, however, there were limitations observed in agenda adherence, exploration depth and long-context coherence. This study establishes that CBT specific fine-tuning can effectively encode therapeutic competencies in small LLMs, though significant technical and ethical considerations must be resolved prior to clinical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00251v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Talha Tahir</dc:creator>
    </item>
    <item>
      <title>A Feedback Toolkit and Procedural Guidance for Teaching Thorough Testing</title>
      <link>https://arxiv.org/abs/2412.00417</link>
      <description>arXiv:2412.00417v1 Announce Type: cross 
Abstract: Correctness is one of the more important criteria of qualitative software. However, it is often taught in isolation and most students consider it only as an afterthought. They also do not receive sufficient feedback on code quality and tests unless specified in the assignment. To improve this, we developed a procedural guidance that guides students to an implementation with appropriate tests. Furthermore, we have developed a toolkit that students can use to independently get individual feedback on their solution and the adequateness of their tests. A key instrument is a test coverage analysis which allows for teachers to customize the feedback with constructive instructions specific to the current assignment to improve a student's test suite. In this paper, we outline the procedural guidance, explain the working of the feedback toolkit and present a method for using the toolkit in conjunction with the different steps of the procedural guidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00417v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steffen Dick, Christoph Bockisch, Harrie Passier, Lex Bijlsma, Ruurd Kuiper</dc:creator>
    </item>
    <item>
      <title>Benchmark Real-time Adaptation and Communication Capabilities of Embodied Agent in Collaborative Scenarios</title>
      <link>https://arxiv.org/abs/2412.00435</link>
      <description>arXiv:2412.00435v1 Announce Type: cross 
Abstract: Advancements in Large Language Models (LLMs) have opened transformative possibilities for human-robot interaction, especially in collaborative environments. However, Real-time human-AI collaboration requires agents to adapt to unseen human behaviors while maintaining effective communication dynamically. Existing benchmarks fall short in evaluating such adaptability for embodied agents, focusing mostly on the task performance of the agent itself. To address this gap, we propose a novel benchmark that assesses agents' reactive adaptability and instantaneous communication capabilities at every step. Based on this benchmark, we propose a Monitor-then-Adapt framework (MonTA), combining strong adaptability and communication with real-time execution. MonTA contains three key LLM modules, a lightweight \textit{Monitor} for monitoring the need for adaptation in high frequency, and two proficient \textit{Adapters} for subtask and path adaptation reasoning in low frequency. Our results demonstrate that MonTA outperforms other baseline agents on our proposed benchmark. Further user studies confirm the high reasonability adaptation plan and consistent language instruction provided by our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00435v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shipeng Liu, Boshen Zhang, Zhehui Huang</dc:creator>
    </item>
    <item>
      <title>Context-Based Echo State Networks with Prediction Confidence for Human-Robot Shared Control</title>
      <link>https://arxiv.org/abs/2412.00541</link>
      <description>arXiv:2412.00541v1 Announce Type: cross 
Abstract: In this paper, we propose a novel lightweight learning from demonstration (LfD) model based on reservoir computing that can learn and generate multiple movement trajectories with prediction intervals, which we call as Context-based Echo State Network with prediction confidence (CESN+). CESN+ can generate movement trajectories that may go beyond the initial LfD training based on a desired set of conditions while providing confidence on its generated output. To assess the abilities of CESN+, we first evaluate its performance against Conditional Neural Movement Primitives (CNMP), a comparable framework that uses a conditional neural process to generate movement primitives. Our findings indicate that CESN+ not only outperforms CNMP but is also faster to train and demonstrates impressive performance in generating trajectories for extrapolation cases. In human-robot shared control applications, the confidence of the machine generated trajectory is a key indicator of how to arbitrate control sharing. To show the usability of the CESN+ for human-robot adaptive shared control, we have designed a proof-of-concept human-robot shared control task and tested its efficacy in adapting the sharing weight between the human and the robot by comparing it to a fixed-weight control scheme. The simulation experiments show that with CESN+ based adaptive sharing the total human load in shared control can be significantly reduced. Overall, the developed CESN+ model is a strong lightweight LfD system with desirable properties such fast training and ability to extrapolate to the new task parameters while producing robust prediction intervals for its output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00541v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Negin Amirshirzad, Mehmet Arda Eren, Erhan Oztop</dc:creator>
    </item>
    <item>
      <title>VR-Doh: Hands-on 3D Modeling in Virtual Reality</title>
      <link>https://arxiv.org/abs/2412.00814</link>
      <description>arXiv:2412.00814v1 Announce Type: cross 
Abstract: We present VR-Doh, a hands-on 3D modeling system designed for creating and manipulating elastoplastic objects in virtual reality (VR). The system employs the Material Point Method (MPM) for simulating realistic large deformations and incorporates optimized Gaussian Splatting for seamless rendering. With direct, hand-based interactions, users can naturally sculpt, deform, and edit objects interactively. To achieve real-time performance, we developed localized simulation techniques, optimized collision handling, and separated appearance and physical representations, ensuring smooth and responsive user interaction. The system supports both freeform creation and precise adjustments, catering to diverse modeling tasks. A user study involving novice and experienced users highlights the system's intuitive design, immersive feedback, and creative potential. Compared to traditional geometry-based modeling tools, our approach offers improved accessibility and natural interaction in specific contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00814v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaofeng Luo, Zhitong Cui, Shijian Luo, Mengyu Chu, Minchen Li</dc:creator>
    </item>
    <item>
      <title>Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data</title>
      <link>https://arxiv.org/abs/2412.01078</link>
      <description>arXiv:2412.01078v2 Announce Type: cross 
Abstract: The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{https://huggingface.co/spaces/KE-Team/KE-Omni}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01078v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</dc:creator>
    </item>
    <item>
      <title>Federated Motor Imagery Classification for Privacy-Preserving Brain-Computer Interfaces</title>
      <link>https://arxiv.org/abs/2412.01079</link>
      <description>arXiv:2412.01079v1 Announce Type: cross 
Abstract: Training an accurate classifier for EEG-based brain-computer interface (BCI) requires EEG data from a large number of users, whereas protecting their data privacy is a critical consideration. Federated learning (FL) is a promising solution to this challenge. This paper proposes Federated classification with local Batch-specific batch normalization and Sharpness-aware minimization (FedBS) for privacy protection in EEG-based motor imagery (MI) classification. FedBS utilizes local batch-specific batch normalization to reduce data discrepancies among different clients, and sharpness-aware minimization optimizer in local training to improve model generalization. Experiments on three public MI datasets using three popular deep learning models demonstrated that FedBS outperformed six state-of-the-art FL approaches. Remarkably, it also outperformed centralized training, which does not consider privacy protection at all. In summary, FedBS protects user EEG data privacy, enabling multiple BCI users to participate in large-scale machine learning model training, which in turn improves the BCI decoding accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01079v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TNSRE.2024.3457504</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. on Neural Systems and Rehabilitation Engineering, 32:3442-3451, 2024</arxiv:journal_reference>
      <dc:creator>Tianwang Jia, Lubin Meng, Siyang Li, Jiajing Liu, Dongrui Wu</dc:creator>
    </item>
    <item>
      <title>Cross-Task Inconsistency Based Active Learning (CTIAL) for Emotion Recognition</title>
      <link>https://arxiv.org/abs/2412.01171</link>
      <description>arXiv:2412.01171v1 Announce Type: cross 
Abstract: Emotion recognition is a critical component of affective computing. Training accurate machine learning models for emotion recognition typically requires a large amount of labeled data. Due to the subtleness and complexity of emotions, multiple evaluators are usually needed for each affective sample to obtain its ground-truth label, which is expensive. To save the labeling cost, this paper proposes an inconsistency-based active learning approach for cross-task transfer between emotion classification and estimation. Affective norms are utilized as prior knowledge to connect the label spaces of categorical and dimensional emotions. Then, the prediction inconsistency on the two tasks for the unlabeled samples is used to guide sample selection in active learning for the target task. Experiments on within-corpus and cross-corpus transfers demonstrated that cross-task inconsistency could be a very valuable metric in active learning. To our knowledge, this is the first work that utilizes prior knowledge on affective norms and data in a different task to facilitate active learning for a new task, even the two tasks are from different datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01171v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TAFFC.2024.3366767</arxiv:DOI>
      <dc:creator>Yifan Xu, Xue Jiang, Dongrui Wu</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models with Reasoning and Acting Meet the Needs of Task-Oriented Dialogue?</title>
      <link>https://arxiv.org/abs/2412.01262</link>
      <description>arXiv:2412.01262v1 Announce Type: cross 
Abstract: Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. However, they underperform compared to previous approaches in task-oriented dialogue (TOD), wherein reasoning and accessing external information are crucial. Empowering LLMs with advanced prompting strategies such as reasoning and acting (ReAct) has shown promise in solving complex tasks traditionally requiring reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs performing TOD. We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs seem to underperform state-of-the-art approaches in simulation, human evaluation indicates higher user satisfaction rate compared to handcrafted systems despite having a lower success rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01262v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek, Lina M. Rojas-Barahona</dc:creator>
    </item>
    <item>
      <title>Misalignments in AI Perception: Quantitative Findings and Visual Mapping of How Experts and the Public Differ in Expectations and Risks, Benefits, and Value Judgments</title>
      <link>https://arxiv.org/abs/2412.01459</link>
      <description>arXiv:2412.01459v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is transforming diverse societal domains, raising critical questions about its risks and benefits and the misalignments between public expectations and academic visions. This study examines how the general public (N=1110) -- people using or being affected by AI -- and academic AI experts (N=119) -- people shaping AI development -- perceive AI's capabilities and impact across 71 scenarios, including sustainability, healthcare, job performance, societal divides, art, and warfare. Participants evaluated each scenario on four dimensions: expected probability, perceived risk and benefit, and overall sentiment (or value). The findings reveal significant quantitative differences: experts anticipate higher probabilities, perceive lower risks, report greater utility, and express more favorable sentiment toward AI compared to the non-experts. Notably, risk-benefit tradeoffs differ: the public assigns risk half the weight of benefits, while experts assign it only a third. Visual maps of these evaluations highlight areas of convergence and divergence, identifying potential sources of public concern. These insights offer actionable guidance for researchers and policymakers to align AI development with societal values, fostering public trust and informed governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01459v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Brauner, Felix Glawe, Gian Luca Liehner, Luisa Vervier, Martina Ziefle</dc:creator>
    </item>
    <item>
      <title>Handwriting-based Automated Assessment and Grading of Degree of Handedness: A Pilot Study</title>
      <link>https://arxiv.org/abs/2412.01587</link>
      <description>arXiv:2412.01587v1 Announce Type: cross 
Abstract: Hand preference and degree of handedness (DoH) are two different aspects of human behavior which are often confused to be one. DoH is a person's inherent capability of the brain; affected by nature and nurture. In this study, we used dominant and non-dominant handwriting traits to assess DoH for the first time, on 43 subjects of three categories- Unidextrous, Partially Unidextrous, and Ambidextrous. Features extracted from the segmented handwriting signals called strokes were used for DoH quantification. Davies Bouldin Index, Multilayer perceptron, and Convolutional Neural Network (CNN) were used for automated grading of DoH. The outcomes of these methods were compared with the widely used DoH assessment questionnaires from Edinburgh Inventory (EI). The CNN based automated grading outperformed other computational methods with an average classification accuracy of 95.06% under stratified 10-fold cross-validation. The leave-one-subject-out strategy on this CNN resulted in a test individual's DoH score which was converted into a 4-point score. Around 90% of the obtained scores from all the implemented computational methods were found to be in accordance with the EI scores under 95% confidence interval. Automated grading of degree of handedness using handwriting signals can provide more resolution to the Edinburgh Inventory scores. This could be used in multiple applications concerned with neuroscience, rehabilitation, physiology, psychometry, behavioral sciences, and forensics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01587v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Smriti Bala, Venugopalan Y. Vishnu, Deepak Joshi</dc:creator>
    </item>
    <item>
      <title>If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World</title>
      <link>https://arxiv.org/abs/2412.01617</link>
      <description>arXiv:2412.01617v1 Announce Type: cross 
Abstract: Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT, particularly those outside of its marketed use as task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22 times more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations for research and industry to address loneliness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01617v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adrian de Wynter</dc:creator>
    </item>
    <item>
      <title>FathomVerse: A community science dataset for ocean animal discovery</title>
      <link>https://arxiv.org/abs/2412.01701</link>
      <description>arXiv:2412.01701v1 Announce Type: cross 
Abstract: Can computer vision help us explore the ocean? The ultimate challenge for computer vision is to recognize any visual phenomena, more than only the objects and animals humans encounter in their terrestrial lives. Previous datasets have explored everyday objects and fine-grained categories humans see frequently. We present the FathomVerse v0 detection dataset to push the limits of our field by exploring animals that rarely come in contact with people in the deep sea. These animals present a novel vision challenge.
  The FathomVerse v0 dataset consists of 3843 images with 8092 bounding boxes from 12 distinct morphological groups recorded at two locations on the deep seafloor that are new to computer vision. It features visually perplexing scenarios such as an octopus intertwined with a sea star, and confounding categories like vampire squids and sea spiders. This dataset can push forward research on topics like fine-grained transfer learning, novel category discovery, species distribution modeling, and carbon cycle analysis, all of which are important to the care and husbandry of our planet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01701v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Genevieve Patterson, Joost Daniels, Benjamin Woodward, Kevin Barnard, Giovanna Sainz, Lonny Lundsten, Kakani Katija</dc:creator>
    </item>
    <item>
      <title>Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review</title>
      <link>https://arxiv.org/abs/2412.01708</link>
      <description>arXiv:2412.01708v1 Announce Type: cross 
Abstract: Scholarly peer review is a cornerstone of scientific advancement, but the system is under strain due to increasing manuscript submissions and the labor-intensive nature of the process. Recent advancements in large language models (LLMs) have led to their integration into peer review, with promising results such as substantial overlaps between LLM- and human-generated reviews. However, the unchecked adoption of LLMs poses significant risks to the integrity of the peer review system. In this study, we comprehensively analyze the vulnerabilities of LLM-generated reviews by focusing on manipulation and inherent flaws. Our experiments show that injecting covert deliberate content into manuscripts allows authors to explicitly manipulate LLM reviews, leading to inflated ratings and reduced alignment with human reviews. In a simulation, we find that manipulating 5% of the reviews could potentially cause 12% of the papers to lose their position in the top 30% rankings. Implicit manipulation, where authors strategically highlight minor limitations in their papers, further demonstrates LLMs' susceptibility compared to human reviewers, with a 4.5 times higher consistency with disclosed limitations. Additionally, LLMs exhibit inherent flaws, such as potentially assigning higher ratings to incomplete papers compared to full papers and favoring well-known authors in single-blind review process. These findings highlight the risks of over-reliance on LLMs in peer review, underscoring that we are not yet ready for widespread adoption and emphasizing the need for robust safeguards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01708v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Ye, Xianghe Pang, Jingyi Chai, Jiaao Chen, Zhenfei Yin, Zhen Xiang, Xiaowen Dong, Jing Shao, Siheng Chen</dc:creator>
    </item>
    <item>
      <title>WebAssembly enables low latency interoperable augmented and virtual reality software</title>
      <link>https://arxiv.org/abs/2110.07128</link>
      <description>arXiv:2110.07128v2 Announce Type: replace 
Abstract: There is a clear difference in runtime performance between native applications that use augmented/virtual reality (AR/VR) device-specific hardware and comparable web-based implementations. Here we show that WebAssembly (Wasm) offers a promising developer solution that can bring near-native low latency performance to web-based applications, enabling hardware-agnostic interoperability at scale through portable bytecode that runs on any WiFi or cellular data network-enabled AR/VR device. Many software application areas have begun to realize Wasm's potential as a key enabling technology, but it has yet to establish a robust presence in the AR/VR domain. When considering the limitations of current web-based AR/VR development technologies such as WebXR, which provides an existing application programming interface (API) that enables AR/VR capabilities for web-based programs, Wasm can resolve critical issues faced with just-in-time (JIT) compilation, slow run-times, large file sizes, and big data, among other challenges. Existing applications using Wasm-based WebXR are sparse but growing, and the potential for porting native applications to use this emerging framework will benefit the web-based AR/VR application space and bring it closer to its native counterparts in terms of performance. Taken together, this kind of standardized ``write-once-deploy-everywhere'' software framework for AR/VR applications has the potential to consolidate user experiences across different head-mounted displays and other embedded devices to ultimately create an interoperable AR/VR ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.07128v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Woo Jae Kim, Bohdan B. Khomtchouk</dc:creator>
    </item>
    <item>
      <title>Estimating Continuous Muscle Fatigue For Multi-Muscle Coordinated Exercise: A Pilot Study on Walking</title>
      <link>https://arxiv.org/abs/2303.17614</link>
      <description>arXiv:2303.17614v2 Announce Type: replace 
Abstract: Assessing the progression of muscle fatigue for daily exercises provides vital indicators for precise rehabilitation, personalized training dose, especially under the context of Metaverse. Assessing fatigue of multi-muscle coordination-involved daily exercises requires the neuromuscular features that represent the fatigue-induced characteristics of spatiotemporal adaptions of multiple muscles and the estimator that captures the time-evolving progression of fatigue. In this paper, we propose to depict fatigue by the features of muscle compensation and spinal module activation changes and estimate continuous fatigue by a physiological rationale model. First, we extract muscle synergy fractionation and the variance of spinal module spikings as features inspired by the prior of fatigue-induced neuromuscular adaptations. Second, we treat the features as observations and develop a Bayesian Gaussian process to capture the time-evolving progression. Third, we solve the issue of lacking supervision information by mathematically formulating the time-evolving characteristics of fatigue as the loss function. Finally, we adapt the metrics that follow the physiological principles of fatigue to quantitatively evaluate the performance. Our extensive experiments present a 0.99 similarity between days, a over 0.7 similarity with other views of fatigue and a nearly 1 weak monotonicity, which outperform other methods. This study would aim the objective assessment of muscle fatigue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17614v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chunzhi Yi, Xiaolei Sun, Chunyu Zhang, Wei Jin, Jianfei Zhu, Haiqi Zhu, Baichun Wei</dc:creator>
    </item>
    <item>
      <title>Development and User Experiences of a Novel Virtual Reality Task for Post-stroke Visuospatial Neglect: An Exploratory Pilot Study</title>
      <link>https://arxiv.org/abs/2312.12399</link>
      <description>arXiv:2312.12399v2 Announce Type: replace 
Abstract: Visuospatial Neglect (VSN) affects spatial awareness, leading to functional and motor challenges. This study explores virtual reality (VR) as a potential complementary tool for VSN rehabilitation, offering a novel environment that intends to support therapy outcomes. Specifically, we aim to explore the initial experiences of patients and physiotherapists engaging with the protocol. VSN occurs in approximately 30% of stroke survivors, often presenting as inattention to one side of space. While conventional therapies rely on repetitive motor tasks, VR has emerged as a promising alternative for targeted and patient-centered rehabilitation. However, evidence on the integration of audio-visual cues in VR for VSN is limited. A preliminary VR task integrating audio-visual cues was co-designed with two physiotherapists. The task was then tested with two VSN patients over 12 sessions. Preliminary findings suggest potential benefits in patient experience, with one patient reporting increased confidence in mobility. However, outcomes varied, and the results are exploratory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12399v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Danso, Patti Nijhuis, Alessandro Ansani, Martin Hartmann, Gulnara Minkkinen, Geoff Luck, Joshua S. Bamford, Sarah Faber, Kat Agres, Solange Glasser, Teppo S\"ark\"am\"o, Rebekah Rousi, Marc R. Thompson</dc:creator>
    </item>
    <item>
      <title>Inclusive Design of AI's Explanations: Just for Those Previously Left Out, or for Everyone?</title>
      <link>https://arxiv.org/abs/2404.13217</link>
      <description>arXiv:2404.13217v3 Announce Type: replace 
Abstract: Motivations: Explainable Artificial Intelligence (XAI) systems aim to improve users' understanding of AI, but XAI research shows many cases of different explanations serving some users well and being unhelpful to others. In non-AI systems, some software practitioners have used inclusive design approaches and sometimes their improvements turned out to be "curb-cut" improvements -- not only addressing the needs of underserved users, but also making the products better for everyone. So, if AI practitioners used inclusive design approaches, they too might create curb-cut improvements, i.e., better explanations for everyone. Objectives: To find out, we investigated the curb-cut effects of inclusivity-driven fixes on users' mental models of AI when using an XAI prototype. The prototype and fixes came from an AI team who had adopted an inclusive design approach (GenderMag) to improve their XAI prototype. Methods: We ran a between-subject study with 69 participants with no AI background. 34 participants used the original version of the XAI prototype and 35 used the version with the inclusivity fixes. We compared the two groups' mental model concepts scores, prediction accuracy, and inclusivity. Results: We found four main results. First, it revealed several curb-cut effects of the inclusivity fixes: overall increased engagement with explanations and better mental model concepts scores, which revealed fixes with curb-cut properties. However (second), the inclusivity fixes did not improve participants' prediction accuracy scores -- instead, it appears to have harmed them. This "curb-fence" effect (opposite of the curb-cut effect) revealed the AI explanations' double-edged impact. Third, the AI team's inclusivity fixes brought significant improvements for users whose problem-solving styles had previously been underserved. Further (fourth), the AI team's fixes reduced the gender gap by 45%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13217v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Montaser Hamid, Fatima Moussaoui, Jimena Noa Guevara, Andrew Anderson, Puja Agarwal, Jonathan Dodge, Margaret Burnett</dc:creator>
    </item>
    <item>
      <title>Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities</title>
      <link>https://arxiv.org/abs/2407.06125</link>
      <description>arXiv:2407.06125v2 Announce Type: replace 
Abstract: Depression has proven to be a significant public health issue, profoundly affecting the psychological well-being of individuals. If it remains undiagnosed, depression can lead to severe health issues, which can manifest physically and even lead to suicide. Generally, Diagnosing depression or any other mental disorder involves conducting semi-structured interviews alongside supplementary questionnaires, including variants of the Patient Health Questionnaire (PHQ) by Clinicians and mental health professionals. This approach places significant reliance on the experience and judgment of trained physicians, making the diagnosis susceptible to personal biases. Given that the underlying mechanisms causing depression are still being actively researched, physicians often face challenges in diagnosing and treating the condition, particularly in its early stages of clinical presentation. Recently, significant strides have been made in Artificial neural computing to solve problems involving text, image, and speech in various domains. Our analysis has aimed to leverage these state-of-the-art (SOTA) models in our experiments to achieve optimal outcomes leveraging multiple modalities. The experiments were performed on the Extended Distress Analysis Interview Corpus Wizard of Oz dataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC) 2019 Challenge. The proposed solutions demonstrate better results achieved by Proprietary and Open-source Large Language Models (LLMs), which achieved a Root Mean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC 2019 challenge baseline results and current SOTA regression analysis architectures. Additionally, the proposed solution achieved an accuracy of 71.43% in the classification task. The paper also includes a novel audio-visual multi-modal network that predicts PHQ-8 scores with an RMSE of 6.51.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06125v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chayan Tank, Sarthak Pol, Vinayak Katoch, Shaina Mehta, Avinash Anand, Rajiv Ratn Shah</dc:creator>
    </item>
    <item>
      <title>CREW: Facilitating Human-AI Teaming Research</title>
      <link>https://arxiv.org/abs/2408.00170</link>
      <description>arXiv:2408.00170v2 Announce Type: replace 
Abstract: With the increasing deployment of artificial intelligence (AI) technologies, the potential of humans working with AI agents has been growing at a great speed. Human-AI teaming is an important paradigm for studying various aspects when humans and AI agents work together. The unique aspect of Human-AI teaming research is the need to jointly study humans and AI agents, demanding multidisciplinary research efforts from machine learning to human-computer interaction, robotics, cognitive science, neuroscience, psychology, social science, and complex systems. However, existing platforms for Human-AI teaming research are limited, often supporting oversimplified scenarios and a single task, or specifically focusing on either human-teaming research or multi-agent AI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming research in real-time decision-making scenarios and engage collaborations from multiple scientific disciplines, with a strong emphasis on human involvement. It includes pre-built tasks for cognitive studies and Human-AI teaming with expandable potentials from our modular design. Following conventional cognitive neuroscience research, CREW also supports multimodal human physiological signal recording for behavior analysis. Moreover, CREW benchmarks real-time human-guided reinforcement learning agents using state-of-the-art algorithms and well-tuned baselines. With CREW, we were able to conduct 50 human subject studies within a week to verify the effectiveness of our benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00170v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingyu Zhang, Zhengran Ji, Boyuan Chen</dc:creator>
    </item>
    <item>
      <title>Game Design Prototype with GIMs: Fostering Neurodiverse Connections through Storytelling</title>
      <link>https://arxiv.org/abs/2408.13962</link>
      <description>arXiv:2408.13962v2 Announce Type: replace 
Abstract: This ongoing experimental project investigates the use of Generative Image Models (GIMs) in crafting a picture book creation game designed to nurture social connections among autistic children and their neurotypical peers within a neuro-affirming environment. Moving away from traditional methods that often seek to condition neurodivergent children to socialize in prescribed ways, this project strives to cultivate a space where children can engage with one another naturally and creatively through art and storytelling, free from the pressure to adhere to standard social norms. Beyond merely "story-choosing," the research highlights the potential of GIMs to facilitate "story-creating," fostering peer social connections in a creative and structured collaborative learning experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13962v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiqi Xiao</dc:creator>
    </item>
    <item>
      <title>Long-Term Ad Memorability: Understanding &amp; Generating Memorable Ads</title>
      <link>https://arxiv.org/abs/2309.00378</link>
      <description>arXiv:2309.00378v5 Announce Type: replace-cross 
Abstract: Despite the importance of long-term memory in marketing and brand building, until now, there has been no large-scale study on the memorability of ads. All previous memorability studies have been conducted on short-term recall on specific content types like action videos. On the other hand, long-term memorability is crucial for the advertising industry, and ads are almost always highly multimodal. Therefore, we release the first memorability dataset, LAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad types, we find many interesting insights into what makes an ad memorable, e.g., fast-moving ads are more memorable than those with slower scenes; people who use ad-blockers remember a lower number of ads than those who don't. Next, we present a model, Henry, to predict the memorability of a content. Henry achieves state-of-the-art performance across all prominent literature memorability datasets. It shows strong generalization performance with better results in 0-shot on unseen datasets. Finally, with the intent of memorable ad generation, we present a scalable method to build a high-quality memorable ad generation model by leveraging automatically annotated data. Our approach, SEED (Self rEwarding mEmorability Modeling), starts with a language model trained on LAMBDA as seed data and progressively trains an LLM to generate more memorable ads. We show that the generated advertisements have 44% higher memorability scores than the original ads. We release this large-scale ad dataset, UltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and UltraLAMBDA, are open-sourced at https://behavior-in-the-wild.github.io/memorability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00378v5</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harini SI, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy</dc:creator>
    </item>
    <item>
      <title>Corn Yield Prediction Model with Deep Neural Networks for Smallholder Farmer Decision Support System</title>
      <link>https://arxiv.org/abs/2401.03768</link>
      <description>arXiv:2401.03768v4 Announce Type: replace-cross 
Abstract: Crop yield prediction has been modeled on the assumption that there is no interaction between weather and soil variables. However, this paper argues that an interaction exists, and it can be finely modelled using the Kendall Correlation coefficient. Given the nonlinearity of the interaction between weather and soil variables, a deep neural network regressor (DNNR) is carefully designed with consideration to the depth, number of neurons of the hidden layers, and the hyperparameters with their optimizations. Additionally, a new metric, the average of absolute root squared error (ARSE) is proposed to combine the strengths of root mean square error (RMSE) and mean absolute error (MAE). With the ARSE metric, the proposed DNNR(s), optimised random forest regressor (RFR) and the extreme gradient boosting regressor (XGBR) achieved impressively small yield errors, 0.0172 t/ha, and 0.0243 t/ha, 0.0001 t/ha, and 0.001 t/ha, respectively. However, the DNNR(s), with changes to the explanatory variables to ensure generalizability to unforeseen data, DNNR(s) performed best. Further analysis reveals that a strong interaction does exist between weather and soil variables. Precisely, yield is observed to increase when precipitation is reduced and silt increased, and vice-versa. However, the degree of decrease or increase is not quantified in this paper. Contrary to existing yield models targeted towards agricultural policies and global food security, the goal of the proposed corn yield model is to empower the smallholder farmer to farm smartly and intelligently, thus the prediction model is integrated into a mobile application that includes education, and a farmer-to-market access module.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03768v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chollette C. Olisah, Lyndon Smith, Melvyn Smith, Morolake O. Lawrence, Osita Ojukwu</dc:creator>
    </item>
    <item>
      <title>Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment</title>
      <link>https://arxiv.org/abs/2406.06874</link>
      <description>arXiv:2406.06874v3 Announce Type: replace-cross 
Abstract: Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06874v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong</dc:creator>
    </item>
    <item>
      <title>Investigating the heterogenous effects of a massive content moderation intervention via Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2411.04037</link>
      <description>arXiv:2411.04037v3 Announce Type: replace-cross 
Abstract: In today's online environments, users encounter harm and abuse on a daily basis. Therefore, content moderation is crucial to ensure their safety and well-being. However, the effectiveness of many moderation interventions is still uncertain. Here, we apply a causal inference approach to shed light on the effectiveness of The Great Ban, a massive social media deplatforming intervention. We analyze 53M comments shared by nearly 34K users, providing in-depth results on both the intended and unintended consequences of the ban. Our causal analyses reveal that 15.6% of the moderated users abandoned the platform while the remaining ones decreased their overall toxicity by 4.1%. Nonetheless, a subset of those users increased their toxicity by 70% after the intervention. However, the increases in toxicity did not lead to marked increases in activity or engagement, meaning that the most toxic users had an overall limited impact. Our findings bring to light new insights on the effectiveness of deplatforming moderation interventions. Furthermore, they also contribute to informing future content moderation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04037v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Cima, Benedetta Tessa, Stefano Cresci, Amaury Trujillo, Marco Avvenuti</dc:creator>
    </item>
    <item>
      <title>ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition</title>
      <link>https://arxiv.org/abs/2411.07118</link>
      <description>arXiv:2411.07118v3 Announce Type: replace-cross 
Abstract: Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures, they are now utilized for gesture recognition. So, we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data, due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further, an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07118v3</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan</dc:creator>
    </item>
    <item>
      <title>"Give me the code" -- Log Analysis of First-Year CS Students' Interactions With GPT</title>
      <link>https://arxiv.org/abs/2411.17855</link>
      <description>arXiv:2411.17855v2 Announce Type: replace-cross 
Abstract: The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in computer science (CS) education is expected to be profound. Students now have the power to generate code solutions for a wide array of programming assignments. For first-year students, this may be particularly problematic since the foundational skills are still in development and an over-reliance on generative AI tools can hinder their ability to grasp essential programming concepts. This paper analyzes the prompts used by 69 freshmen undergraduate students to solve a certain programming problem within a project assignment, without giving them prior prompt training. We also present the rules of the exercise that motivated the prompts, designed to foster critical thinking skills during the interaction. Despite using unsophisticated prompting techniques, our findings suggest that the majority of students successfully leveraged GPT, incorporating the suggested solutions into their projects. Additionally, half of the students demonstrated the ability to exercise judgment in selecting from multiple GPT-generated solutions, showcasing the development of their critical thinking skills in evaluating AI-generated code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17855v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Alves, Bruno Pereira Cipriano</dc:creator>
    </item>
  </channel>
</rss>
