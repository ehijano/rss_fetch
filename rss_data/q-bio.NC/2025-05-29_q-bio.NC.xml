<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 01:44:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models</title>
      <link>https://arxiv.org/abs/2505.21507</link>
      <description>arXiv:2505.21507v1 Announce Type: new 
Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis of numerous neurological disorders. Due to the large volume of EEGs requiring interpretation and the specific expertise involved, artificial intelligence-based tools are being developed to assist in their visual analysis. In this paper, we compare two deep learning models (CNN-LSTM and Transformer-based) with BioSerenity-E1, a recently proposed foundation model, in the task of classifying entire EEG recordings as normal or abnormal. The three models were trained or finetuned on 2,500 EEG recordings and their performances were evaluated on two private and one public datasets: a large multicenter dataset annotated by a single specialist (dataset A composed of n = 4,480 recordings), a small multicenter dataset annotated by three specialists (dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus evaluation dataset (n = 276). On dataset A, the three models achieved at least 86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced accuracy. The models were then validated on TUAB evaluation dataset, whose corresponding training set was not used during training, where they achieved at least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results highlight the usefulness of leveraging pre-trained models for automatic EEG classification: enabling robust and efficient interpretation of EEG data with fewer resources and broader applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21507v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aurore Bussalb, Fran\c{c}ois Le Gac, Guillaume Jubien, Mohamed Rahmouni, Ruggero G. Bettinardi, Pedro Marinho R. de Oliveira, Phillipe Derambure, Nicolas Gaspard, Jacques Jonas, Louis Maillard, Laurent Vercueil, Herv\'e Vespignani, Philippe Laval, Laurent Koessler, Ulysse Gimenez</dc:creator>
    </item>
    <item>
      <title>Organizational Regularities in Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2505.22047</link>
      <description>arXiv:2505.22047v1 Announce Type: new 
Abstract: Previous work has shown that the dynamical regime of Recurrent Neural Networks (RNNs) - ranging from oscillatory to chaotic and fixpoint behavior - can be controlled by the global distribution of weights in connection matrices with statistically independent elements. However, it remains unclear how network dynamics respond to organizational regularities in the weight matrix, as often observed in biological neural networks. Here, we investigate three such regularities: (1) monopolar output weights per neuron, in accordance with Dale's principle, (2) reciprocal symmetry between neuron pairs, as in Hopfield networks, and (3) modular structure, where strongly connected blocks are embedded in a background of weaker connectivity. We construct weight matrices in which the strength of each regularity can be continuously tuned via control parameters, and analyze how key dynamical signatures of the RNN evolve as a function of these parameters. Moreover, using the RNN for actual information processing in a reservoir computing framework, we study how each regularity affects performance. We find that Dale monopolarity and modularity significantly enhance task accuracy, while Hopfield reciprocity tends to reduce it by promoting early saturation, limiting reservoir flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22047v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claus Metzner, Achim Schilling, Andreas Maier, Patrick Krauss</dc:creator>
    </item>
    <item>
      <title>Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test</title>
      <link>https://arxiv.org/abs/2505.22112</link>
      <description>arXiv:2505.22112v1 Announce Type: cross 
Abstract: Cognitive flexibility has been extensively studied in human cognition but remains relatively unexplored in the context of Visual Large Language Models (VLLMs). This study assesses the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card Sorting Test (WCST), a classic measure of set-shifting ability. Our results reveal that VLLMs achieve or surpass human-level set-shifting capabilities under chain-of-thought prompting with text-based inputs. However, their abilities are highly influenced by both input modality and prompting strategy. In addition, we find that through role-playing, VLLMs can simulate various functional deficits aligned with patients having impairments in cognitive flexibility, suggesting that VLLMs may possess a cognitive architecture, at least regarding the ability of set-shifting, similar to the brain. This study reveals the fact that VLLMs have already approached the human level on a key component underlying our higher cognition, and highlights the potential to use them to emulate complex brain processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22112v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangfu Hao, Frederic Alexandre, Shan Yu</dc:creator>
    </item>
    <item>
      <title>Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language</title>
      <link>https://arxiv.org/abs/2505.22146</link>
      <description>arXiv:2505.22146v1 Announce Type: cross 
Abstract: Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Ablation studies revealed that manipulation-related attributes (graspability, hand-relatedness, elongation) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22146v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu</dc:creator>
    </item>
    <item>
      <title>Synaptic shot-noise triggers fast and slow global oscillations in balanced neural networks</title>
      <link>https://arxiv.org/abs/2505.22373</link>
      <description>arXiv:2505.22373v1 Announce Type: cross 
Abstract: Neural dynamics is determined by the transmission of discrete synaptic pulses (synaptic shot-noise) among neurons. However, the neural responses are usually obtained within the diffusion approximation modeling synaptic inputs as continuous Gaussian noise. Here, we present a rigorous mean-field theory that encompasses synaptic shot-noise for sparse balanced inhibitory neural networks driven by an excitatory drive. Our theory predicts new dynamical regimes, in agreement with numerical simulations, which are not captured by the classical diffusion approximation. Notably, these regimes feature self-sustained global oscillations emerging at low connectivity (in-degree) via either continuous or hysteretic transitions and characterized by irregular neural activity, as expected for balanced dynamics. For sufficiently weak (strong) excitatory drive (inhibitory feedback) the transition line displays a peculiar re-entrant shape revealing the existence of global oscillations at low and high in-degrees, separated by an asynchronous regime at intermediate levels of connectivity. The mechanisms leading to the emergence of these global oscillations are distinct: drift-driven at high connectivity and cluster activation at low connectivity. The frequency of these two kinds of global oscillations can be varied from slow (around 1 Hz) to fast (around 100 Hz), without altering their microscopic and macroscopic features, by adjusting the excitatory drive and the synaptic inhibition strength in a prescribed way. Furthermore, the cluster-activated oscillations at low in-degrees could correspond to the gamma rhythms reported in mammalian cortex and hippocampus and attributed to ensembles of inhibitory neurons sharing few synaptic connections [G. Buzsaki and X.-J. Wang, Annual Review of Neuroscience 35, 203 (2012)].</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22373v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis S. Goldobin, Maria V. Ageeva, Matteo di Volo, Ferdinand Tixidre, Alessandro Torcini</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings</title>
      <link>https://arxiv.org/abs/2505.22563</link>
      <description>arXiv:2505.22563v1 Announce Type: cross 
Abstract: Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in LLMs align with the dynamic neural responses during human sentence comprehension. By comparing hierarchical embeddings from 14 publicly available LLMs with fMRI data collected from participants, who were exposed to a naturalistic narrative story, we constructed sentence-level neural prediction models to precisely identify the model layers most significantly correlated with brain region activations. Results show that improvements in model performance drive the evolution of representational architectures toward brain-like hierarchies, particularly achieving stronger functional and anatomical correspondence at higher semantic abstraction levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22563v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Lei, Xingyang Ge, Yi Zhang, Yiming Yang, Bolei Ma</dc:creator>
    </item>
    <item>
      <title>Symmetry's Edge in Cortical Dynamics: Multiscale Dynamics of Ensemble Excitation and Inhibition</title>
      <link>https://arxiv.org/abs/2306.11965</link>
      <description>arXiv:2306.11965v4 Announce Type: replace 
Abstract: Creating a quantitative theory for the cortex presents challenges and raises questions. What are the significant scales--micro, meso, or macroscopic? What are the interactions--pairwise, higher order, or mean-field? And what control parameters are relevant--noisy, dissipative, or emergent?
  We suggest an approach inspired by advances in understanding matter. This involves identifying invariances in neuron ensemble dynamics, searching for order parameters connecting key degrees of freedom and distinguishing macroscopic states, and pinpointing broken symmetries to uncover emergent laws when neurons interact and coordinate.
  Using multielectrode and multiscale neural recordings, we measure population-level ensemble Excitation/Inhibition (E/I) balance, differing from the input-level E/I balance of single neurons, to study collective behavior in large neural populations. We investigate a set of parameters that can assist us in differentiating between various functional system states (during wake/sleep cycle) and pinpointing broken symmetries that serve different information processing and memory functions. Furthermore, we identify pathological broken symmetries that result in states like seizures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11965v4</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nima Dehghani</dc:creator>
    </item>
    <item>
      <title>On the Within-class Variation Issue in Alzheimer's Disease Detection</title>
      <link>https://arxiv.org/abs/2409.16322</link>
      <description>arXiv:2409.16322v2 Announce Type: replace-cross 
Abstract: Alzheimer's Disease (AD) detection employs machine learning classification models to distinguish between individuals with AD and those without. Different from conventional classification tasks, we identify within-class variation as a critical challenge in AD detection: individuals with AD exhibit a spectrum of cognitive impairments. Therefore, simplistic binary AD classification may overlook two crucial aspects: within-class heterogeneity and instance-level imbalance. In this work, we found using a sample score estimator can generate sample-specific soft scores aligning with cognitive scores. We subsequently propose two simple yet effective methods: Soft Target Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two problems respectively. Based on the ADReSS and CU-MARVEL corpora, we demonstrated and analyzed the advantages of the proposed approaches in detection performance. These findings provide insights for developing robust and reliable AD detection models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16322v2</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawen Kang, Dongrui Han, Lingwei Meng, Jingyan Zhou, Jinchao Li, Xixin Wu, Helen Meng</dc:creator>
    </item>
    <item>
      <title>Efficient Logit-based Knowledge Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment</title>
      <link>https://arxiv.org/abs/2501.15925</link>
      <description>arXiv:2501.15925v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative to traditional Artificial Neural Networks (ANNs), prized for their potential energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer from accuracy degradation compared to ANNs and face deployment challenges due to fixed inference timesteps, which require retraining for adjustments, limiting operational flexibility. To address these issues, our work considers the spatio-temporal property inherent in SNNs, and proposes a novel distillation framework for deep SNNs that optimizes performance across full-range timesteps without specific retraining, enhancing both efficacy and deployment adaptability. We provide both theoretical analysis and empirical validations to illustrate that training guarantees the convergence of all implicit models across full-range timesteps. Experimental results on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance among distillation-based SNNs training methods. Our code is available at https://github.com/Intelli-Chip-Lab/snn\_temporal\_decoupling\_distillation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15925v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengting Yu, Xiaochen Zhao, Lei Liu, Shu Yang, Gaoang Wang, Erping Li, Aili Wang</dc:creator>
    </item>
  </channel>
</rss>
