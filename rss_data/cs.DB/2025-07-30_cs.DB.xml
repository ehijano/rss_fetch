<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Jul 2025 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Compact Answers to Temporal Path Queries</title>
      <link>https://arxiv.org/abs/2507.22143</link>
      <description>arXiv:2507.22143v1 Announce Type: new 
Abstract: We study path-based graph queries that, in addition to navigation through edges, also perform navigation through time. This allows asking questions about the dynamics of networks, like traffic movement, cause-effect relationships, or the spread of a disease. In this setting, a graph consists of triples annotated with validity intervals, and a query produces pairs of nodes where each pair is associated with a binary relation over time. For instance, such a pair could be two airports, and the temporal relation could map potential departure times to possible arrival times. An open question is how to represent such a relation in a compact form and maintain this property during query evaluation. We investigate four compact representations of answers to a such queries, which are based on alternative ways to encode sets of intervals. We discuss their respective advantages and drawbacks, in terms of conciseness, uniqueness, and computational cost. Notably, the most refined encoding guarantees that query answers over dense time can be finitely represented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22143v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Adnan, Diego Calvanese, Julien Corman, Anton Dign\"os, Werner Nutt, Ognjen Savkovi\'c</dc:creator>
    </item>
    <item>
      <title>Is SHACL Suitable for Data Quality Assessment?</title>
      <link>https://arxiv.org/abs/2507.22305</link>
      <description>arXiv:2507.22305v1 Announce Type: new 
Abstract: Knowledge graphs have been widely adopted in both enterprises, such as the Google Knowledge Graph, and open platforms like Wikidata to represent domain knowledge and support analysis with artificial intelligence. They model real-world information as nodes and edges. To embrace flexibility, knowledge graphs often lack enforced schemas (i.e., ontologies), leading to potential data quality issues, such as semantically overlapping nodes. Therefore, ensuring their quality is essential, as issues in the data can affect applications relying on them. To assess the quality of knowledge graphs, existing works either propose high-level frameworks comprising various data quality dimensions without concrete implementations, define tools that measure data quality with ad-hoc SPARQL (SPARQL Protocol and RDF Query Language) queries, or promote the usage of constraint languages, such as the Shapes Constraint Language (SHACL), to assess and improve the quality of the graph. Although the latter approaches claim to address data quality assessment, none of them comprehensively tries to cover all data quality dimensions. In this paper, we explore this gap by investigating the extent to which SHACL can be used to assess data quality in knowledge graphs. Specifically, we defined SHACL shapes for 69 data quality metrics proposed by Zaveri et al. [1] and implemented a prototype that automatically instantiates these shapes and computes the corresponding data quality measures from their validation results. All resources are provided for repeatability at https://github.com/caroocortes/SHACL-DQA-prototype/tree/main</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22305v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Cort\'es Lasalle, Lisa Ehrlinger, Lorena Etcheverry, Felix Naumann</dc:creator>
    </item>
    <item>
      <title>Scalability, Availability, Reproducibility and Extensibility in Islamic Database Systems</title>
      <link>https://arxiv.org/abs/2507.22384</link>
      <description>arXiv:2507.22384v1 Announce Type: new 
Abstract: With the widespread of software systems and applications that serve the Islamic knowledge domain, several concerns arise. Authenticity and accuracy of the databases that back up these systems are questionable. With the excitement that some software developers and amateur researchers may have, false statements and incorrect claims may be made around numerical signs or miracles in the Quran. Reproducibility of these claims may not be addressed by the people making such claims. Moreover, with the increase in the number of users, scalability and availability of these systems become a concern. In addition to all these concerns, extensibility is also another major issue. Properly designed systems can be extensible, reusable and built on top of one another, instead of each system being built from scratch every time a new framework is developed. In this paper, we introduce the QuranResearch.Org system and its vision for scalability, availability, reproducibility and extensibility to serve Islamic database systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22384v1</guid>
      <category>cs.DB</category>
      <category>cs.SE</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Journal on Islamic Applications in Computer Science and Technology, Vol. 9, Issue 3, September 2021, 14-20</arxiv:journal_reference>
      <dc:creator>Umar Siddiqui, Habiba Youssef, Adel Sabour, Mohamed Ali</dc:creator>
    </item>
    <item>
      <title>Systematic Evaluation of Knowledge Graph Repair with Large Language Models</title>
      <link>https://arxiv.org/abs/2507.22419</link>
      <description>arXiv:2507.22419v1 Announce Type: new 
Abstract: We present a systematic approach for evaluating the quality of knowledge graph repairs with respect to constraint violations defined in shapes constraint language (SHACL). Current evaluation methods rely on \emph{ad hoc} datasets, which limits the rigorous analysis of repair systems in more general settings. Our method addresses this gap by systematically generating violations using a novel mechanism, termed violation-inducing operations (VIOs). We use the proposed evaluation framework to assess a range of repair systems which we build using large language models. We analyze the performance of these systems across different prompting strategies. Results indicate that concise prompts containing both the relevant violated SHACL constraints and key contextual information from the knowledge graph yield the best performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22419v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tung-Wei Lin, Gabe Fierro, Han Li, Tianzhen Hong, Pierluigi Nuzzo, Alberto Sangiovanni-Vinentelli</dc:creator>
    </item>
    <item>
      <title>SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases</title>
      <link>https://arxiv.org/abs/2507.22701</link>
      <description>arXiv:2507.22701v1 Announce Type: new 
Abstract: The co-location of multiple database instances on resource constrained edge nodes creates significant cache contention, where traditional schemes are inefficient and unstable under dynamic workloads. To address this, we present SAM, an autonomic cache manager powered by our novel AURA algorithm. AURA makes stability a first-class design principle by resolving the exploitation-exploration dilemma: it achieves this by synthesizing two orthogonal factors, which we introduce as: the H-factor, representing a database's proven, historically stable efficiency (exploitation), and the V-factor, representing its empirically estimated marginal gain for future improvements (exploration). This dual-factor model, governed by an adaptive weight, enables SAM to achieve sustained high performance through strategic stability and robustness in volatile conditions.
  Extensive experiments against 14 diverse baselines demonstrate SAM's superiority. It achieves top-tier throughput while being uniquely resilient to complex workload shifts and cache pollution attacks. Furthermore, its decision latency is highly scalable, remaining nearly constant as the system grows to 120 databases. Crucially, SAM achieves superior decision stability -- maintaining consistent optimization directions despite noise, avoiding performance oscillations while ensuring predictable Quality of Service. These results prove that a principled, stability-aware design is essential for sustained high performance in real-world, large-scale systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22701v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zhang, Decheng Zuo, Yu Yan, Zhiyu Liang, Hongzhi Wang</dc:creator>
    </item>
    <item>
      <title>SourceSplice: Source Selection for Machine Learning Tasks</title>
      <link>https://arxiv.org/abs/2507.22186</link>
      <description>arXiv:2507.22186v1 Announce Type: cross 
Abstract: Data quality plays a pivotal role in the predictive performance of machine learning (ML) tasks - a challenge amplified by the deluge of data sources available in modern organizations.Prior work in data discovery largely focus on metadata matching, semantic similarity or identifying tables that should be joined to answer a particular query, but do not consider source quality for high performance of the downstream ML task.This paper addresses the problem of determining the best subset of data sources that must be combined to construct the underlying training dataset for a given ML task.We propose SourceGrasp and SourceSplice, frameworks designed to efficiently select a suitable subset of sources that maximizes the utility of the downstream ML model.Both the algorithms rely on the core idea that sources (or their combinations) contribute differently to the task utility, and must be judiciously chosen.While SourceGrasp utilizes a metaheuristic based on a greediness criterion and randomization, the SourceSplice framework presents a source selection mechanism inspired from gene splicing - a core concept used in protein synthesis.We empirically evaluate our algorithms on three real-world datasets and synthetic datasets and show that, with significantly fewer subset explorations, SourceSplice effectively identifies subsets of data sources leading to high task utility.We also conduct studies reporting the sensitivity of SourceSplice to the decision choices under several settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22186v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ambarish Singh, Romila Pradhan</dc:creator>
    </item>
    <item>
      <title>AgileDART: An Agile and Scalable Edge Stream Processing Engine</title>
      <link>https://arxiv.org/abs/2407.14953</link>
      <description>arXiv:2407.14953v3 Announce Type: replace 
Abstract: Edge applications generate a large influx of sensor data on massive scales, and these massive data streams must be processed shortly to derive actionable intelligence. However, traditional data processing systems are not well-suited for these edge applications as they often do not scale well with a large number of concurrent stream queries, do not support low-latency processing under limited edge computing resources, and do not adapt to the level of heterogeneity and dynamicity commonly present in edge computing environments. As such, we present AgileDart, an agile and scalable edge stream processing engine that enables fast stream processing of many concurrently running low-latency edge applications' queries at scale in dynamic, heterogeneous edge environments. The novelty of our work lies in a dynamic dataflow abstraction that leverages distributed hash table-based peer-to-peer overlay networks to autonomously place, chain, and scale stream operators to reduce query latencies, adapt to workload variations, and recover from failures and a bandit-based path planning model that re-plans the data shuffling paths to adapt to unreliable and heterogeneous edge networks. We show that AgileDart outperforms Storm and EdgeWise on query latency and significantly improves scalability and adaptability when processing many real-world edge stream applications' queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14953v3</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2025.3526143</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Mobile Computing, vol. 24, no. 5, pp. 4510 - 4528, 2025</arxiv:journal_reference>
      <dc:creator>Cheng-Wei Ching, Xin Chen, Chaeeun Kim, Tongze Wang, Dong Chen, Dilma Da Silva, Liting Hu</dc:creator>
    </item>
    <item>
      <title>SHARP: Shared State Reduction for Efficient Matching of Sequential Patterns</title>
      <link>https://arxiv.org/abs/2507.04872</link>
      <description>arXiv:2507.04872v2 Announce Type: replace 
Abstract: The detection of sequential patterns in data is a basic functionality of modern data processing systems for complex event processing (CEP), OLAP, and retrieval-augmented generation (RAG). In practice, pattern matching is challenging, since common applications rely on a large set of patterns that shall be evaluated with tight latency bounds. At the same time, matching needs to maintain state, i.e., intermediate results, that grows exponentially in the input size. Hence, systems turn to best-effort processing, striving for maximal recall under a latency bound. Existing techniques, however, consider each pattern in isolation, neglecting the optimization potential induced by state sharing in pattern matching.
  In this paper, we present SHARP, a library that employs state reduction to achieve efficient best-effort pattern matching. To this end, SHARP incorporates state sharing between patterns through a new abstraction, coined pattern-sharing degree (PSD). At runtime, this abstraction facilitates the categorization and indexing of partial pattern matches. Based thereon, once a latency bound is exceeded, SHARP realizes best-effort processing by selecting a subset of partial matches for further processing in constant time. In experiments with real-world data, SHARP achieves a recall of 97%, 96% and 73% for pattern matching in CEP, OLAP, and RAG applications, under a bound of 50% of the average processing latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04872v2</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cong Yu, Tuo Shi, Matthias Weidlich, Bo Zhao</dc:creator>
    </item>
    <item>
      <title>Properties for Paths in Graph Databases</title>
      <link>https://arxiv.org/abs/2507.19329</link>
      <description>arXiv:2507.19329v2 Announce Type: replace 
Abstract: This paper presents a formalism for defining properties of paths in graph databases, which can be used to restrict the number of solutions to navigational queries. In particular, our formalism allows us to define quantitative properties such as length or accumulated cost, which can be used as query filters. Furthermore, it enables the identification and removal of paths that may be considered ill-formed.
  The new formalism is defined in terms of an operational semantics for the query language that incorporates these new constructs, demonstrating its soundness and completeness by proving its compatibility with a simple logical semantics. We also analyze its expressive power, showing that path properties are more expressive than register automata. Finally, after discussing some complexity issues related to this new approach, we present an empirical analysis carried out using our prototype implementation of the graph database that serves as a running example throughout the paper. The results show that queries using path properties as filters outperform standard queries that do not use them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19329v2</guid>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Orejas, Elvira Pino, Renzo Angles, Edelmira Pasarella, Nikos Milonakis</dc:creator>
    </item>
    <item>
      <title>Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis</title>
      <link>https://arxiv.org/abs/2507.01053</link>
      <description>arXiv:2507.01053v2 Announce Type: replace-cross 
Abstract: As ever-larger clinical datasets become available, they have the potential to unlock unprecedented opportunities for medical research. Foremost among them is Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest open-source EHR database. However, the inherent complexity of these datasets, particularly the need for sophisticated querying skills and the need to understand the underlying clinical settings, often presents a significant barrier to their effective use. M3 lowers the technical barrier to understanding and querying MIMIC-IV data. With a single command it retrieves MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers converse with the database in plain English. Ask a clinical question in natural language; M3 uses a language model to translate it into SQL, executes the query against the MIMIC-IV dataset, and returns structured results alongside the underlying query for verifiability and reproducibility. Demonstrations show that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that once demanded hours of handcrafted SQL and relied on understanding the complexities of clinical workflows. By simplifying access, M3 invites the broader research community to mine clinical critical-care data and accelerates the translation of raw records into actionable insight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01053v2</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafi Al Attrach, Pedro Moreira, Rajna Fani, Renato Umeton, Leo Anthony Celi</dc:creator>
    </item>
  </channel>
</rss>
