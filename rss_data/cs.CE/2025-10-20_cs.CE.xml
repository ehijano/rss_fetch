<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 02:44:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Extending Temporal Disturbance Estimations For Magnetic Anomaly Navigation and Mapping</title>
      <link>https://arxiv.org/abs/2510.15113</link>
      <description>arXiv:2510.15113v1 Announce Type: new 
Abstract: Slow-moving vehicles relying on crustal magnetic anomaly navigation (MagNav) or vehicles revisiting the same location in a short time - such as those used for surveys in magnetic anomaly mapping - require fixed ground stations within 100 km of the vehicle's trajectory to measure and remove the geomagnetic disturbance field from magnetic readings. This approach is impractical due to the limited network of fixed-ground magnetometer stations, making long-range (several hundred kilometers long) aeromagnetic surveys for anomaly map-making infeasible. To address these challenges, we developed the Extended Reference Station Model (ERSM). ERSM applies a longitudinal correction and regression model to an extended reference ground magnetometer station (ERS) to produce an estimate of the local temporal disturbance field. ERSM is regression model-agnostic, so we implemented a linear regression, a k-nearest neighbors (kNN) regression, and a neural-network regression model to assess performance benefits. Our results show typical performance below 10nT root mean square error and median performance below 5nT for typical use with the kNN and neural-net model for farther distances and below 5nT performance using the linear regression model on stations with proximity. We also consider how space-weather events, water-body separation, and proximity to polar regions affect the model performance based on ERS selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15113v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anutam Srinivasan, Aaron Nielsen</dc:creator>
    </item>
    <item>
      <title>Toward Black Scholes for Prediction Markets: A Unified Kernel and Market Maker's Handbook</title>
      <link>https://arxiv.org/abs/2510.15205</link>
      <description>arXiv:2510.15205v1 Announce Type: new 
Abstract: Prediction markets, such as Polymarket, aggregate dispersed information into tradable probabilities, but they still lack a unifying stochastic kernel comparable to the one options gained from Black-Scholes. As these markets scale with institutional participation, exchange integrations, and higher volumes around elections and macro prints, market makers face belief volatility, jump, and cross-event risks without standardized tools for quoting or hedging. We propose such a foundation: a logit jump-diffusion with risk-neutral drift that treats the traded probability p_t as a Q-martingale and exposes belief volatility, jump intensity, and dependence as quotable risk factors. On top, we build a calibration pipeline that filters microstructure noise, separates diffusion from jumps using expectation-maximization, enforces the risk-neutral drift, and yields a stable belief-volatility surface. We then define a coherent derivative layer (variance, correlation, corridor, and first-passage instruments) analogous to volatility and correlation products in option markets. In controlled experiments on synthetic risk-neutral paths and real event data, the model reduces short-horizon belief-variance forecast error relative to diffusion-only and probability-space baselines, supporting both causal calibration and economic interpretability. Conceptually, the logit jump-diffusion kernel supplies an implied-volatility analogue for prediction markets: a tractable, tradable language for quoting, hedging, and transferring belief risk across venues such as Polymarket.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15205v1</guid>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaw Dalen</dc:creator>
    </item>
    <item>
      <title>SoK: Market Microstructure for Decentralized Prediction Markets (DePMs)</title>
      <link>https://arxiv.org/abs/2510.15612</link>
      <description>arXiv:2510.15612v1 Announce Type: new 
Abstract: Decentralized prediction markets (DePMs) allow open participation in event-based wagering without fully relying on centralized intermediaries. We review the history of DePMs which date back to 2011 and includes hundreds of proposals. Perhaps surprising, modern DePMs like Polymarket deviate materially from earlier designs like Truthcoin and Augur v1. We use our review to present a modular workflow comprising seven stages: underlying infrastructure, market topic, share structure and pricing, trading, market resolution, settlement, and archiving. For each module, we enumerate the design variants, analyzing trade-offs around decentralization, expressiveness, and manipulation resistance. We also identify open problems for researchers interested in this ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15612v1</guid>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>q-fin.TR</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nahid Rahman, Joseph Al-Chami, Jeremy Clark</dc:creator>
    </item>
    <item>
      <title>DeepAries: Adaptive Rebalancing Interval Selection for Enhanced Portfolio Selection</title>
      <link>https://arxiv.org/abs/2510.14985</link>
      <description>arXiv:2510.14985v1 Announce Type: cross 
Abstract: We propose DeepAries , a novel deep reinforcement learning framework for dynamic portfolio management that jointly optimizes the timing and allocation of rebalancing decisions. Unlike prior reinforcement learning methods that employ fixed rebalancing intervals regardless of market conditions, DeepAries adaptively selects optimal rebalancing intervals along with portfolio weights to reduce unnecessary transaction costs and maximize risk-adjusted returns. Our framework integrates a Transformer-based state encoder, which effectively captures complex long-term market dependencies, with Proximal Policy Optimization (PPO) to generate simultaneous discrete (rebalancing intervals) and continuous (asset allocations) actions. Extensive experiments on multiple real-world financial markets demonstrate that DeepAries significantly outperforms traditional fixed-frequency and full-rebalancing strategies in terms of risk-adjusted returns, transaction costs, and drawdowns. Additionally, we provide a live demo of DeepAries at https://deep-aries.github.io/, along with the source code and dataset at https://github.com/dmis-lab/DeepAries, illustrating DeepAries' capability to produce interpretable rebalancing and allocation decisions aligned with shifting market regimes. Overall, DeepAries introduces an innovative paradigm for adaptive and practical portfolio management by integrating both timing and allocation into a unified decision-making process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14985v1</guid>
      <category>q-fin.PM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinkyu Kim, Hyunjung Yi, Mogan Gim, Donghee Choi, Jaewoo Kang</dc:creator>
    </item>
    <item>
      <title>Deep learning reveals key predictors of thermal conductivity in covalent organic frameworks</title>
      <link>https://arxiv.org/abs/2409.06457</link>
      <description>arXiv:2409.06457v3 Announce Type: replace 
Abstract: The thermal conductivity of covalent organic frameworks (COFs), an emerging class of nanoporous polymeric materials, is crucial for many applications, yet the link between their structure and thermal properties remains poorly understood. Analysis of a dataset containing over 2,400 COFs reveals that conventional features such as density, pore size, void fraction, and surface area do not reliably predict thermal conductivity. To address this, an attention-based machine learning model was trained, accurately predicting thermal conductivities even for structures outside the training set. The attention mechanism was then utilized to investigate the model's success. The analysis identified dangling molecular branches as a key predictor of thermal conductivity, leading us to define the dangling mass ratio (DMR), a descriptor that quantifies the fraction of atomic mass in dangling branches relative to the total COF mass. Feature importance assessments on regression models confirm the significance of DMR in predicting thermal conductivity. These findings indicate that COFs with dangling functional groups exhibit lower thermal transfer capabilities. Molecular dynamics simulations support this observation, revealing significant mismatches in the vibrational density of states due to the presence of dangling branches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06457v3</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1039/D5DD00126A</arxiv:DOI>
      <dc:creator>Prakash Thakolkaran, Yiwen Zheng, Yaqi Guo, Aniruddh Vashisth, Siddhant Kumar</dc:creator>
    </item>
    <item>
      <title>Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams</title>
      <link>https://arxiv.org/abs/2506.04960</link>
      <description>arXiv:2506.04960v2 Announce Type: replace 
Abstract: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams based on a rotation-free isogeometric formulation. The procedure follows two steps: First, the elastic properties are identified from quasi-static displacements; then, the density is determined from modal data (low frequencies and mode shapes), given the previously obtained elastic properties. The identification relies on three independent discretizations: the isogeometric finite element mesh, a high-resolution grid of experimental measurements, and a material mesh composed of low-order Lagrange elements. The material mesh approximates the unknown material distributions, with its nodal values serving as design variables. The error between experiments and numerical model is expressed in a least squares manner. The objective is minimized using local optimization with the trust-region method, providing analytical derivatives to accelerate computations. Several numerical examples exhibiting large displacements are provided to test the proposed approach. To alleviate membrane locking, the B2M1 discretization is employed when necessary. Quasi-experimental data is generated using refined finite element models with random noise applied up to 4%. The method yields satisfactory results as long as a sufficient amount of experimental data is available, even for high measurement noise. Regularization is used to ensure a stable solution for dense material meshes. The density can be accurately reconstructed based on the previously identified elastic properties. The proposed framework can be straightforwardly extended to shells and 3D continua.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04960v2</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2025.118415</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering, Volume 448, Part A, 118415, 2026</arxiv:journal_reference>
      <dc:creator>Bart{\l}omiej {\L}azorczyk, Roger A. Sauer</dc:creator>
    </item>
    <item>
      <title>An Eulerian Data Assimilation Method for Two-Layer Quasi-Geostrophic Model in Physical Domain</title>
      <link>https://arxiv.org/abs/2509.14586</link>
      <description>arXiv:2509.14586v3 Announce Type: replace 
Abstract: Data assimilation (DA) integrates observational data with numerical models to improve the prediction of complex physical systems. However, traditional DA methods often struggle with nonlinear dynamics and multi-scale variability, particularly when implemented directly in the physical domain. To address these challenges, this work develops an Eulerian Data Assimilation (EuDA) framework with the Conditional Gaussian Nonlinear System (CGNS). The proposed approach enables the treatment of non-periodic systems and provides a more intuitive representation of localized and time-dependent phenomena. The work considers a physical domain inspired by sea-ice floe trajectories and ocean eddy recovery in the Arctic regions, where the model dynamics are modeled by a two-layer quasi-geostrophic (QG) system. The QG equations are numerically solved using forward-Euler time stepping and centered finite-difference schemes. CGNS provides a nonlinear filter as it offers an analytical and continuous formulation for filtering a nonlinear system. Model performance is assessed using normalized root mean square error (RMSE) and pattern correlation (Corr) of the posterior mean. The results show that both metrics improve monotonically with increasing timesteps, while RMSE converges to approximately 0.1 across all grid sizes and Corr increases from 0.64 to 0.92 as grid resolution becomes finer. Lastly, a coupled scenario with sea-ice particles advected by the two-layer QG flow under a linear drag force is examined, demonstrating the flexibility of the EuDA-CGNS framework in capturing coupled ice-ocean interactions. These findings demonstrate the effectiveness of exploiting the two-layer QG model in the physical domain to capture multiscale flow features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14586v3</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyeonggeun Yun, Quanling Deng</dc:creator>
    </item>
    <item>
      <title>Towards Symmetry-Aware Efficient Simulation of Quantum Systems and Beyond</title>
      <link>https://arxiv.org/abs/2303.11409</link>
      <description>arXiv:2303.11409v2 Announce Type: replace-cross 
Abstract: The efficient simulation of complex quantum systems remains a central challenge due to the exponential growth of Hilbert space with system size. Tensor network methods have long been established as powerful approximation schemes, and their efficiency can be further enhanced by incorporating physics-informed priors. A prominent example is symmetry: recent progress on $U(1)$-symmetric tensor networks, accelerated on GPUs and scaled to supercomputers, shows how conserved charges induce block-sparse structures that reduce computational cost and enable larger simulations. The same principle extends to general symmetries, inspiring equivariant neural networks in machine learning and guiding symmetry-preserving ansatze in variational quantum algorithms. Beyond symmetry, physics-informed design also includes strategies such as hybrid tensor networks and parallel sequential circuits, which pursue efficiency from complementary principles. This Perspective argues that physics-informed tensor networks, grounded in both symmetry and beyond-symmetry insights, provide unifying strategies for scalable approaches in quantum simulation, computation, and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11409v2</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <category>quant-ph</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Chen, Minzhao Liu, Changhun Oh, Liang Jiang, Yuri Alexeev, Junyu Liu</dc:creator>
    </item>
    <item>
      <title>Establishing trust in automated reasoning</title>
      <link>https://arxiv.org/abs/2309.12351</link>
      <description>arXiv:2309.12351v2 Announce Type: replace-cross 
Abstract: Since its beginnings in the 1940s, automated reasoning by computers has become a tool of ever growing importance in scientific research. So far, the rules underlying automated reasoning have mainly been formulated by humans, in the form of program source code. Rules derived from large amounts of data, via machine learning techniques, are a complementary approach currently under intense development. The question of why we should trust these systems, and the results obtained with their help, has been discussed by philosophers of science but has so far received little attention by practitioners. The present work focuses on independent reviewing, an important source of trust in science, and identifies the characteristics of automated reasoning systems that affect their reviewability. It also discusses possible steps towards increasing reviewability and trustworthiness via a combination of technical and social measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12351v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad Hinsen (SSOLEIL, CBM)</dc:creator>
    </item>
  </channel>
</rss>
