<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Dec 2024 02:54:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Real-time Adaptive Anisotropic Image-to-mesh Conversion for Vascular Flow Simulations</title>
      <link>https://arxiv.org/abs/2412.13222</link>
      <description>arXiv:2412.13222v1 Announce Type: cross 
Abstract: Presented is a path towards a fast and robust adaptive anisotropic mesh generation method that is designed to help streamline the discretization of complex vascular geometries within the Computational Fluid Dynamics (CFD) modeling process. The proposed method combines multiple software tools into a single pipeline to provide the following: (1) image-to-mesh conversion which satisfies quality, fidelity, and smoothness requirements, (2) the generation of a boundary layer grid over the high fidelity surface, (3) a parallel adaptive anisotropic meshing procedure which satisfies real-time requirements, and (4) robustness, which is satisfied by the pipeline's ability to process segmented images and CAD models. The proposed approach is tested with two brain aneurysm cases and is shown to satisfy all the aforementioned requirements. The next steps are to fully parallelize the remaining components of the pipeline to maximize potential performance and to test its integration within a CFD vascular flow simulation. Just as the parallel anisotropic adaptation procedure was tested within aerospace CFD simulations using CAD models, the method is expected to provide accurate results for CFD vascular flow simulations in real-time when executed on multicore cc-NUMA architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13222v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.DC</category>
      <category>cs.GR</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Garner, Fotis Drakopoulos, Chander Sadasivan, Nikos Chrisochoides</dc:creator>
    </item>
    <item>
      <title>Enabling Region-Specific Control via Lassos in Point-Based Colorization</title>
      <link>https://arxiv.org/abs/2412.13469</link>
      <description>arXiv:2412.13469v1 Announce Type: cross 
Abstract: Point-based interactive colorization techniques allow users to effortlessly colorize grayscale images using user-provided color hints. However, point-based methods often face challenges when different colors are given to semantically similar areas, leading to color intermingling and unsatisfactory results-an issue we refer to as color collapse. The fundamental cause of color collapse is the inadequacy of points for defining the boundaries for each color. To mitigate color collapse, we introduce a lasso tool that can control the scope of each color hint. Additionally, we design a framework that leverages the user-provided lassos to localize the attention masks. The experimental results show that using a single lasso is as effective as applying 4.18 individual color hints and can achieve the desired outcomes in 30% less time than using points alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13469v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanghyeon Lee, Jooyeol Yun, Jaegul Choo</dc:creator>
    </item>
    <item>
      <title>T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation</title>
      <link>https://arxiv.org/abs/2412.13486</link>
      <description>arXiv:2412.13486v1 Announce Type: cross 
Abstract: Scene generation is crucial to many computer graphics applications. Recent advances in generative AI have streamlined sketch-to-image workflows, easing the workload for artists and designers in creating scene concept art. However, these methods often struggle for complex scenes with multiple detailed objects, sometimes missing small or uncommon instances. In this paper, we propose a Training-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after reviewing the entire cross-attention mechanism. This scheme revitalizes the existing ControlNet model, enabling effective handling of multi-instance generations, involving prompt balance, characteristics prominence, and dense tuning. Specifically, this approach enhances keyword representation via the prompt balance module, reducing the risk of missing critical instances. It also includes a characteristics prominence module that highlights TopK indices in each channel, ensuring essential features are better represented based on token sketches. Additionally, it employs dense tuning to refine contour details in the attention map, compensating for instance-related regions. Experiments validate that our triplet tuning approach substantially improves the performance of existing sketch-to-image models. It consistently generates detailed, multi-instance 2D images, closely adhering to the input prompts and enhancing visual quality in complex multi-instance scenes. Code is available at https://github.com/chaos-sun/t3s2s.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13486v1</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>cs.GR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhenhong Sun, Yifu Wang, Yonhon Ng, Yunfei Duan, Daoyi Dong, Hongdong Li, Pan Ji</dc:creator>
    </item>
    <item>
      <title>DragScene: Interactive 3D Scene Editing with Single-view Drag Instructions</title>
      <link>https://arxiv.org/abs/2412.13552</link>
      <description>arXiv:2412.13552v1 Announce Type: cross 
Abstract: 3D editing has shown remarkable capability in editing scenes based on various instructions. However, existing methods struggle with achieving intuitive, localized editing, such as selectively making flowers blossom. Drag-style editing has shown exceptional capability to edit images with direct manipulation instead of ambiguous text commands. Nevertheless, extending drag-based editing to 3D scenes presents substantial challenges due to multi-view inconsistency. To this end, we introduce DragScene, a framework that integrates drag-style editing with diverse 3D representations. First, latent optimization is performed on a reference view to generate 2D edits based on user instructions. Subsequently, coarse 3D clues are reconstructed from the reference view using a point-based representation to capture the geometric details of the edits. The latent representation of the edited view is then mapped to these 3D clues, guiding the latent optimization of other views. This process ensures that edits are propagated seamlessly across multiple views, maintaining multi-view consistency. Finally, the target 3D scene is reconstructed from the edited multi-view images. Extensive experiments demonstrate that DragScene facilitates precise and flexible drag-style editing of 3D scenes, supporting broad applicability across diverse 3D representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13552v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenghao Gu, Zhenzhe Li, Zhengqi Zhang, Yunpeng Bai, Shuzhao Xie, Zhi Wang</dc:creator>
    </item>
    <item>
      <title>Real-Time Position-Aware View Synthesis from Single-View Input</title>
      <link>https://arxiv.org/abs/2412.14005</link>
      <description>arXiv:2412.14005v1 Announce Type: cross 
Abstract: Recent advancements in view synthesis have significantly enhanced immersive experiences across various computer graphics and multimedia applications, including telepresence, and entertainment. By enabling the generation of new perspectives from a single input view, view synthesis allows users to better perceive and interact with their environment. However, many state-of-the-art methods, while achieving high visual quality, face limitations in real-time performance, which makes them less suitable for live applications where low latency is critical. In this paper, we present a lightweight, position-aware network designed for real-time view synthesis from a single input image and a target camera pose. The proposed framework consists of a Position Aware Embedding, modeled with a multi-layer perceptron, which efficiently maps positional information from the target pose to generate high dimensional feature maps. These feature maps, along with the input image, are fed into a Rendering Network that merges features from dual encoder branches to resolve both high level semantics and low level details, producing a realistic new view of the scene. Experimental results demonstrate that our method achieves superior efficiency and visual quality compared to existing approaches, particularly in handling complex translational movements without explicit geometric operations like warping. This work marks a step toward enabling real-time view synthesis from a single image for live and interactive applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14005v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manu Gond, Emin Zerman, Sebastian Knorr, M{\aa}rten Sj\"ostr\"om</dc:creator>
    </item>
    <item>
      <title>AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era</title>
      <link>https://arxiv.org/abs/2412.10255</link>
      <description>arXiv:2412.10255v3 Announce Type: replace 
Abstract: Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation. Our evaluation benchmark will be publicly available at https://github.com/bilibili/Index-anisora.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10255v3</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Xinwen Zhang, Xingyu Zheng, Jixuan Xu, Yue Zhang, Jinlong Hou, Huyang Sun</dc:creator>
    </item>
    <item>
      <title>Visual Deformation Detection Using Soft Material Simulation for Pre-training of Condition Assessment Models</title>
      <link>https://arxiv.org/abs/2405.14877</link>
      <description>arXiv:2405.14877v2 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of geometric quality assurance in manufacturing, particularly when human assessment is required. It proposes using Blender, an open-source simulation tool, to create synthetic datasets for machine learning (ML) models. The process involves translating expert information into shape key parameters to simulate deformations, generating images for both deformed and non-deformed objects. The study explores the impact of discrepancies between real and simulated environments on ML model performance and investigates the effect of different simulation backgrounds on model sensitivity. Additionally, the study aims to enhance the model's robustness to camera positioning by generating datasets with a variety of randomized viewpoints. The entire process, from data synthesis to model training and testing, is implemented using a Python API interfacing with Blender. An experiment with a soda can object validates the accuracy of the proposed pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14877v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CASE59546.2024.10711635</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE 20th International Conference on Automation Science and Engineering (CASE) Page(s): 1839 - 1844</arxiv:journal_reference>
      <dc:creator>Joel Sol, Amir M. Soufi Enayati, Homayoun Najjaran</dc:creator>
    </item>
    <item>
      <title>Deep Learning and Machine Learning -- Object Detection and Semantic Segmentation: From Theory to Applications</title>
      <link>https://arxiv.org/abs/2410.15584</link>
      <description>arXiv:2410.15584v2 Announce Type: replace-cross 
Abstract: An in-depth exploration of object detection and semantic segmentation is provided, combining theoretical foundations with practical applications. State-of-the-art advancements in machine learning and deep learning are reviewed, focusing on convolutional neural networks (CNNs), YOLO architectures, and transformer-based approaches such as DETR. The integration of artificial intelligence (AI) techniques and large language models for enhancing object detection in complex environments is examined. Additionally, a comprehensive analysis of big data processing is presented, with emphasis on model optimization and performance evaluation metrics. By bridging the gap between traditional methods and modern deep learning frameworks, valuable insights are offered for researchers, data scientists, and engineers aiming to apply AI-driven methodologies to large-scale object detection tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15584v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jintao Ren, Ziqian Bi, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Xuanhe Pan, Jinlang Wang, Keyu Chen, Caitlyn Heqi Yin, Pohsun Feng, Yizhu Wen, Tianyang Wang, Silin Chen, Ming Li, Jiawei Xu, Ming Liu</dc:creator>
    </item>
    <item>
      <title>Thunderscapes: Simulating the Dynamics of Mesoscale Convective System</title>
      <link>https://arxiv.org/abs/2412.00703</link>
      <description>arXiv:2412.00703v2 Announce Type: replace-cross 
Abstract: A Mesoscale Convective System (MCS) is a collection of thunderstorms that function as a system, representing a widely discussed phenomenon in both the natural sciences and visual effects industries, and embodying the untamed forces of nature.In this paper, we present the first interactive, physically inspired mesoscale thunderstorms simulation model that integrates Grabowski-style cloud microphysics with atmospheric electrification processes. Our model simulates thunderclouds development and lightning flashes within a unified meteorological framework, providing a realistic and interactive approach for graphical applications. By incorporating key physical principles, it effectively links cloud formation, electrification, and lightning generation. The simulation also encompasses various thunderstorm types and their corresponding lightning activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00703v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.GR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianchen Hao</dc:creator>
    </item>
  </channel>
</rss>
