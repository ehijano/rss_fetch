<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mechanizing Operads with Event-B</title>
      <link>https://arxiv.org/abs/2512.16342</link>
      <description>arXiv:2512.16342v1 Announce Type: new 
Abstract: Rigorous modelling of natural and industrial systems still conveys various challenges related to abstractions, methods to proceed with and easy-to-use tools to build, compose and reason on models. Operads are mathematical structures that provide such abstractions to compose various objects and garanteeing well-formedness.
  Concrete implementations of operads will offer practical means to exploit operads and to use them for various technical applications. Going from the mathematical structures, we develop with Event-B a complete refinement chain that implements algebraic operads and their basic operations.
  The result of this work, can be used from the methodological point of view to handle similar implementations for symbolic computation questions, and also to reason on symbolic computation applications supported by operads structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16342v1</guid>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christian Attiogb\'e</dc:creator>
    </item>
    <item>
      <title>(Pointed) Univalence in Universe Category Models of Type Theory</title>
      <link>https://arxiv.org/abs/2512.16697</link>
      <description>arXiv:2512.16697v1 Announce Type: new 
Abstract: We provide a formulation of the univalence axiom in a universe category model of dependent type theory that is convenient to verify in homotopy-theoretic settings. We further develop a strengthening of the univalence axiom, called pointed univalence, that is both computationally desirable and semantically natural, and verify its closure under Artin-Wraith gluing and formation of inverse diagrams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16697v1</guid>
      <category>cs.LO</category>
      <category>math.CT</category>
      <category>math.LO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Kapulkin, Yufeng Li</dc:creator>
    </item>
    <item>
      <title>Towards Mass Spectrum Analysis with ASP</title>
      <link>https://arxiv.org/abs/2512.16780</link>
      <description>arXiv:2512.16780v1 Announce Type: new 
Abstract: We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implemen- tation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16780v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils K\"uchenmeister, Alex Ivliev, Markus Kr\"otzsch</dc:creator>
    </item>
    <item>
      <title>Checking the HAL Interface Specification Continuously, Right from the Start</title>
      <link>https://arxiv.org/abs/2512.16897</link>
      <description>arXiv:2512.16897v1 Announce Type: new 
Abstract: The correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16897v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Bentele, Onur Altinordu, Jan K\"orner, Andreas Podelski, Axel Sikora</dc:creator>
    </item>
    <item>
      <title>A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning</title>
      <link>https://arxiv.org/abs/2512.15816</link>
      <description>arXiv:2512.15816v1 Announce Type: cross 
Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15816v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daragh King, Vasileios Koutavas, Laura Kovacs</dc:creator>
    </item>
    <item>
      <title>Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems</title>
      <link>https://arxiv.org/abs/2512.16707</link>
      <description>arXiv:2512.16707v1 Announce Type: cross 
Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16707v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhisek Ganguly</dc:creator>
    </item>
    <item>
      <title>TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge</title>
      <link>https://arxiv.org/abs/2512.16855</link>
      <description>arXiv:2512.16855v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16855v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCAD66269.2025.11240962</arxiv:DOI>
      <dc:creator>Khurram Khalil, Khaza Anuarul Hoque</dc:creator>
    </item>
    <item>
      <title>Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of Constraints and Objectives</title>
      <link>https://arxiv.org/abs/2508.20978</link>
      <description>arXiv:2508.20978v4 Announce Type: replace-cross 
Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20978v4</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1613/jair.1.21105</arxiv:DOI>
      <dc:creator>Marianne Defresne, Romain Gambardella, Sophie Barbe, Thomas Schiex</dc:creator>
    </item>
  </channel>
</rss>
