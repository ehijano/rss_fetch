<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Bayesian Classification Trees Approach to Treatment Effect Variation with Noncompliance</title>
      <link>https://arxiv.org/abs/2408.07765</link>
      <description>arXiv:2408.07765v1 Announce Type: new 
Abstract: Estimating varying treatment effects in randomized trials with noncompliance is inherently challenging since variation comes from two separate sources: variation in the impact itself and variation in the compliance rate. In this setting, existing frequentist and flexible machine learning methods are highly sensitive to the weak instruments problem, in which the compliance rate is (locally) close to zero. Bayesian approaches, on the other hand, can naturally account for noncompliance via imputation. We propose a Bayesian machine learning approach that combines the best features of both approaches. Our main methodological contribution is to present a Bayesian Causal Forest model for binary response variables in scenarios with noncompliance by repeatedly imputing individuals' compliance types, allowing us to flexibly estimate varying treatment effects among compliers. Simulation studies demonstrate the usefulness of our approach when compliance and treatment effects are heterogeneous. We apply the method to detect and analyze heterogeneity in the treatment effects in the Illinois Workplace Wellness Study, which not only features heterogeneous and one-sided compliance but also several binary outcomes of interest. We demonstrate the methodology on three outcomes one year after intervention. We confirm a null effect on the presence of a chronic condition, discover meaningful heterogeneity in a "bad health" outcome that cancels out to null in classical partial effect estimates, and find substantial heterogeneity in individuals' perception of management prioritization of health and safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07765v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared D. Fisher, David W. Puelz, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Assessing the properties of the prediction interval in random-effects meta-analysis</title>
      <link>https://arxiv.org/abs/2408.08080</link>
      <description>arXiv:2408.08080v1 Announce Type: new 
Abstract: Random effects meta-analysis is a widely applied methodology to synthetize research findings of studies in a specific scientific question. Besides estimating the mean effect, an important aim of the meta-analysis is to summarize the heterogeneity, i.e. the variation in the underlying effects caused by the differences in study circumstances. The prediction interval is frequently used for this purpose: a 95% prediction interval contains the true effect of a similar new study in 95% of the cases when it is constructed, or in other words, it covers 95% of the true effects distribution on average. In this article, after providing a clear mathematical background, we present an extensive simulation investigating the performance of all frequentist prediction interval methods published to date. The work focuses on the distribution of the coverage probabilities and how these distributions change depending on the amount of heterogeneity and the number of involved studies. Although the single requirement that a prediction interval has to fulfill is to keep a nominal coverage probability on average, we demonstrate why the distribution of coverages cannot be disregarded, and that for small number of studies no reliable conclusion can be drawn from the prediction interval. We argue that assessing only the mean coverage can easily lead to misunderstanding and misinterpretation. The length of the intervals and the robustness of the methods concerning non-normality of the true effects are also investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08080v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peter Matrai, Tamas Koi, Zoltan Sipos, Nelli Farkas</dc:creator>
    </item>
    <item>
      <title>Simple Macroeconomic Forecast Distributions for the G7 Economies</title>
      <link>https://arxiv.org/abs/2408.08304</link>
      <description>arXiv:2408.08304v1 Announce Type: new 
Abstract: We present a simple method for predicting the distribution of output growth and inflation in the G7 economies. The method is based on point forecasts published by the International Monetary Fund (IMF), as well as robust statistics from the the empirical distribution of the IMF's past forecast errors while imposing coherence of prediction intervals across horizons. We show that the technique yields calibrated prediction intervals and performs similar to, or better than, more complex time series models in terms of statistical loss functions. We provide a simple website with graphical illustrations of our forecasts, as well as time-stamped data files that document their real time character.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08304v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Friederike Becker, Fabian Kr\"uger, Melanie Schienle</dc:creator>
    </item>
    <item>
      <title>Ranking and Combining Latent Structured Predictive Scores without Labeled Data</title>
      <link>https://arxiv.org/abs/2408.07796</link>
      <description>arXiv:2408.07796v1 Announce Type: cross 
Abstract: Combining multiple predictors obtained from distributed data sources to an accurate meta-learner is promising to achieve enhanced performance in lots of prediction problems. As the accuracy of each predictor is usually unknown, integrating the predictors to achieve better performance is challenging. Conventional ensemble learning methods assess the accuracy of predictors based on extensive labeled data. In practical applications, however, the acquisition of such labeled data can prove to be an arduous task. Furthermore, the predictors under consideration may exhibit high degrees of correlation, particularly when similar data sources or machine learning algorithms were employed during their model training. In response to these challenges, this paper introduces a novel structured unsupervised ensemble learning model (SUEL) to exploit the dependency between a set of predictors with continuous predictive scores, rank the predictors without labeled data and combine them to an ensembled score with weights. Two novel correlation-based decomposition algorithms are further proposed to estimate the SUEL model, constrained quadratic optimization (SUEL.CQO) and matrix-factorization-based (SUEL.MF) approaches. The efficacy of the proposed methods is rigorously assessed through both simulation studies and real-world application of risk genes discovery. The results compellingly demonstrate that the proposed methods can efficiently integrate the dependent predictors to an ensemble model without the need of ground truth data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07796v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Shiva Afshar, Yinghan Chen, Shizhong Han, Ying Lin</dc:creator>
    </item>
    <item>
      <title>Autonomous on-Demand Shuttles for First Mile-Last Mile Connectivity: Design, Optimization, and Impact Assessment</title>
      <link>https://arxiv.org/abs/2408.07872</link>
      <description>arXiv:2408.07872v1 Announce Type: cross 
Abstract: The First-Mile Last-Mile (FMLM) connectivity is crucial for improving public transit accessibility and efficiency, particularly in sprawling suburban regions where traditional fixed-route transit systems are often inadequate. Autonomous on-Demand Shuttles (AODS) hold a promising option for FMLM connections due to their cost-effectiveness and improved safety features, thereby enhancing user convenience and reducing reliance on personal vehicles. A critical issue in AODS service design is the optimization of travel paths, for which realistic traffic network assignment combined with optimal routing offers a viable solution. In this study, we have designed an AODS controller that integrates a mesoscopic simulation-based dynamic traffic assignment model with a greedy insertion heuristics approach to optimize the travel routes of the shuttles. The controller also considers the charging infrastructure/strategies and the impact of the shuttles on regular traffic flow for routes and fleet-size planning. The controller is implemented in Aimsun traffic simulator considering Lake Nona in Orlando, Florida as a case study. We show that, under the present demand based on 1% of total trips as transit riders, a fleet of 3 autonomous shuttles can serve about 80% of FMLM trip requests on-demand basis with an average waiting time below 4 minutes. Additional power sources have significant effect on service quality as the inactive waiting time for charging would increase the fleet size. We also show that low-speed autonomous shuttles would have negligible impact on regular vehicle flow, making them suitable for suburban areas. These findings have important implications for sustainable urban planning and public transit operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07872v1</guid>
      <category>cs.RO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sudipta Roy, Gabriel Dadashev, Lampros Yfantis, Bat-hen Nahmias-Biran, Samiul Hasan</dc:creator>
    </item>
    <item>
      <title>$statcheck$ is flawed by design and no valid spell checker for statistical results</title>
      <link>https://arxiv.org/abs/2408.07948</link>
      <description>arXiv:2408.07948v1 Announce Type: cross 
Abstract: The R package $statcheck$ is designed to extract statistical test results from text and check the consistency of the reported test statistics and corresponding p-values. Recently, it has also been featured as a spell checker for statistical results, aimed at improving reporting accuracy in scientific publications. In this study, I perform a check on $statcheck$ using a non-exhaustive list of 187 simple text strings with arbitrary statistical test results. These strings represent a wide range of textual representations of results including correctly manageable results, non-targeted test statistics, variable reporting styles, and common typos. Since $statcheck$'s detection heuristic is tied to a specific set of statistical test results that strictly adhere to the American Psychological Association (APA) reporting guidelines, it is unable to detect and check any reported result that even slightly deviates from this narrow style. In practice, $statcheck$ is unlikely to detect many statistical test results reported in the literature. I conclude that the capabilities and usefulness of the $statcheck$ software are very limited and that it should not be used to detect irregularities in results nor as a spell checker for statistical results. Future developments should aim to incorporate more flexible algorithms capable of handling a broader variety of reporting styles, such as those provided by $JATSdecoder$ and Large Language Models, which show promise in overcoming these limitations but they cannot replace the critical eye of a knowledgeable reader.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07948v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ingmar B\"oschen</dc:creator>
    </item>
    <item>
      <title>Integrated population model reveals human and environment driven changes in Baltic ringed seal (Pusa hispida botnica) demography and behavior</title>
      <link>https://arxiv.org/abs/2408.08069</link>
      <description>arXiv:2408.08069v1 Announce Type: cross 
Abstract: Integrated population models (IPMs) are a promising approach to test ecological theories and assess wildlife populations in dynamic and uncertain conditions. By combining multiple data sources into a single, unified model, they enable the parametrization of versatile, mechanistic models that can predict population dynamics in novel circumstances. Here, we present a Bayesian IPM for the ringed seal (Pusa hispida botnica) population inhabiting the Bothnian Bay in the Baltic Sea. Despite the availability of long-term monitoring data, traditional assessment methods have faltered due to dynamic environmental conditions, varying reproductive rates, and the recently re-introduced hunting, thus limiting the quality of information available to managers. We fit our model to census and various demographic, reproductive and harvest data from 1988 to 2023 to provide a comprehensive assessment of past population trends, and predict population response to alternative hunting scenarios. We estimated that 20,000 to 36,000 ringed seals inhabit the Bothnian Bay, and the population is increasing 3% to 6% per year. Reproductive rates have increased since 1988, leading to a substantial increase in the growth rate up until 2015. However, the re-introduction of hunting has since reduced the growth rate, and even minor quota increases are likely to reduce it further. Our results also support the hypothesis that a greater proportion of seals haul-out under lower ice cover circumstances, leading to higher aerial survey counts in such years. In general, our study demonstrates the value of IPMs for monitoring natural populations under changing environments, and supporting science-based management decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08069v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Ersalman, Mervi Kunnasranta, Markus Ahola, Anja M. Carlsson, Sara Persson, Britt-Marie B\"acklin, Inari Helle, Linnea Cervin, Jarno Vanhatalo</dc:creator>
    </item>
    <item>
      <title>Quantifying the informativity of emission lines to infer physical conditions in giant molecular clouds. I. Application to model predictions</title>
      <link>https://arxiv.org/abs/2408.08114</link>
      <description>arXiv:2408.08114v1 Announce Type: cross 
Abstract: Observations of ionic, atomic, or molecular lines are performed to improve our understanding of the interstellar medium (ISM). However, the potential of a line to constrain the physical conditions of the ISM is difficult to assess quantitatively, because of the complexity of the ISM physics. The situation is even more complex when trying to assess which combinations of lines are the most useful. Therefore, observation campaigns usually try to observe as many lines as possible for as much time as possible. We search for a quantitative statistical criterion to evaluate the constraining power of a (or combination of) tracer(s) with respect to physical conditions in order to improve our understanding of the statistical relationships between ISM tracers and physical conditions and helps observers to motivate their observation proposals. The best tracers are obtained by comparing the mutual information between a physical parameter and different sets of lines. We apply this method to simulations of radio molecular lines emitted by a photodissociation region similar to the Horsehead Nebula that would be observed at the IRAM 30m telescope. We search for the best lines to constrain the visual extinction $A_v^{tot}$ or the far UV illumination $G_0$. The most informative lines change with the physical regime (e.g., cloud extinction). Short integration time of the CO isotopologue $J=1-0$ lines already yields much information on the total column density most regimes. The best set of lines to constrain the visual extinction does not necessarily combine the most informative individual lines. Precise constraints on $G_0$ are more difficult to achieve with molecular lines. They require spectral lines emitted at the cloud surface (e.g., [CII] and [CI] lines). This approach allows one to better explore the knowledge provided by ISM codes, and to guide future observation campaigns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08114v1</guid>
      <category>astro-ph.GA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Einig, Pierre Palud, Antoine Roueff, J\'er\^ome Pety, Emeric Bron, Franck Le Petit, Maryvonne Gerin, Jocelyn Chanussot, Pierre Chainais, Pierre-Antoine Thouvenin, David Languignon, Ivana Be\v{s}li\'c, Simon Coud\'e, Helena Mazurek, Jan H. Orkisz, Miriam G. Santa-Maria, L\'eontine S\'egal, Antoine Zakardjian, S\'ebastien Bardeau, Karine Demyk, Victor de Souza Magalh\~es, Javier R. Goicoechea, Pierre Gratier, Viviana V. Guzm\'an, Annie Hughes, Fran\c{c}ois Levrier, Jacques Le Bourlot, Dariusz C. Lis, Harvey S. Liszt, Nicolas Peretto, Evelyne Roueff, Albrecht Sievers</dc:creator>
    </item>
    <item>
      <title>Analysing kinematic data from recreational runners using functional data analysis</title>
      <link>https://arxiv.org/abs/2408.08200</link>
      <description>arXiv:2408.08200v1 Announce Type: cross 
Abstract: We present a multivariate functional mixed effects model for kinematic data from a large number of recreational runners. The runners' sagittal plane hip and knee angles are modelled jointly as a bivariate function with random effects functions used to account for the dependence among measurements from either side of the body. The model is fitted by first applying multivariate functional principal component analysis (mv-FPCA) and then modelling the mv-FPCA scores using scalar linear mixed effects models. Simulation and bootstrap approaches are introduced to construct simultaneous confidence bands for the fixed effects functions, and covariance functions are reconstructed to summarise the variability structure in the data and thoroughly investigate the suitability of the proposed model. In our scientific application, we observe a statistically significant effect of running speed on both the hip and knee angles. We also observe strong within-subject correlations, reflecting the highly idiosyncratic nature of running technique. Our approach is more generally applicable to modelling multiple streams of smooth kinematic or kinetic data measured repeatedly for multiple subjects in complex experimental designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08200v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Gunning, Steven Golovkine, Andrew J. Simpkin, Aoife Burke, Sarah Dillon, Shane Gore, Kieran Moran, Siobhan O'Connor, Enda Whyte, Norma Bargary</dc:creator>
    </item>
    <item>
      <title>Peer-induced Fairness: A Causal Approach to Reveal Algorithmic Unfairness</title>
      <link>https://arxiv.org/abs/2408.02558</link>
      <description>arXiv:2408.02558v2 Announce Type: replace 
Abstract: This paper introduces a novel framework, "peer-induced fairness", to scientifically audit algorithmic fairness. It addresses a critical but often overlooked issue: distinguishing between adverse outcomes due to algorithmic discrimination and those resulting from individuals' insufficient capabilities. By utilizing counterfactual fairness and advanced causal inference techniques, such as the Single World Intervention Graph, this model-agnostic approach evaluates fairness at the individual level through peer comparisons and hypothesis testing. It also tackles challenges like data scarcity and imbalance, offering a flexible, plug-and-play self-audit tool for stakeholders and an external audit tool for regulators, while providing explainable feedback for those affected by unfavorable decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02558v2</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>q-fin.CP</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiqi Fang, Zexun Chen, Jake Ansell</dc:creator>
    </item>
    <item>
      <title>Extreme-value modelling of migratory bird arrival dates: Insights from citizen science data</title>
      <link>https://arxiv.org/abs/2312.01870</link>
      <description>arXiv:2312.01870v4 Announce Type: replace-cross 
Abstract: Citizen science mobilises many observers and gathers huge datasets but often without strict sampling protocols, resulting in observation biases due to heterogeneous sampling effort, which can lead to biased statistical inferences. We develop a spatiotemporal Bayesian hierarchical model for bias-corrected estimation of arrival dates of the first migratory bird individuals at a breeding site. Higher sampling effort could be correlated with earlier observed dates. We implement data fusion of two citizen-science datasets with fundamentally different protocols (BBS, eBird) and map posterior distributions of the latent process, which contains four spatial components with Gaussian process priors: species niche; sampling effort; position and scale parameters of annual first arrival date. The data layer includes four response variables: counts of observed eBird locations (Poisson); presence-absence at observed eBird locations (Binomial); BBS occurrence counts (Poisson); first arrival dates (Generalised Extreme-Value). We devise a Markov Chain Monte Carlo scheme and check by simulation that the latent process components are identifiable. We apply our model to several migratory bird species in the northeastern US for 2001--2021 and find that the sampling effort significantly modulates the observed first arrival date. We exploit this relationship to effectively bias-correct predictions of the true first arrivals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01870v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Koh, Thomas Opitz</dc:creator>
    </item>
  </channel>
</rss>
