<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Sep 2024 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Empowering Database Learning Through Remote Educational Escape Rooms</title>
      <link>https://arxiv.org/abs/2409.08284</link>
      <description>arXiv:2409.08284v1 Announce Type: new 
Abstract: Learning about databases is indispensable for individuals studying software engineering or computer science or those involved in the IT industry. We analyzed a remote educational escape room for teaching about databases in four different higher education courses in two consecutive academic years. We employed three instruments for evaluation: a pre- and post-test to assess the escape room's effectiveness for student learning, a questionnaire to gather students' perceptions, and a Web platform that unobtrusively records students' interactions and performance. We show novel evidence that educational escape rooms conducted remotely can be engaging as well as effective for teaching about databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08284v1</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MIC.2023.3333199</arxiv:DOI>
      <arxiv:journal_reference>IEEE Internet Computing Jan.-Feb. 2024, pp. 18-25, vol. 28</arxiv:journal_reference>
      <dc:creator>Enrique Barra, Sonsoles L\'opez-Pernas, Aldo Gordillo, Alejandro Pozo, Andres Mu\~noz-Arcentales, Javier Conde</dc:creator>
    </item>
    <item>
      <title>Payments Use Cases and Design Options for Interoperability and Funds Locking across Digital Pounds and Commercial Bank Money</title>
      <link>https://arxiv.org/abs/2409.08653</link>
      <description>arXiv:2409.08653v1 Announce Type: new 
Abstract: Central banks are actively exploring retail central bank digital currencies (CBDCs), with the Bank of England currently in the design phase for a potential UK retail CBDC, the digital pound. In a previous paper, we defined and explored the important concept of functional consistency (which is the principle that different forms of money have the same operational characteristics) and evaluated design options to support functional consistency across digital pounds and commercial bank money, based on a set of key capabilities. In this paper, we continue to analyse the design options for supporting functional consistency and, in order to perform a detailed analysis, we focus on three key capabilities: communication between digital pound ecosystem participants, funds locking, and interoperability across digital pounds and commercial bank money. We explore these key capabilities via three payments use cases: person-to-person push payment, merchant-initiated request to pay, and lock funds and pay on physical delivery. We then present and evaluate the suitability of design options to provide the specific capabilities for each use case and draw initial insights. We conclude that a financial market infrastructure (FMI) providing specific capabilities could simplify the experience of ecosystem participants, simplify the operating platforms for both the Bank of England and digital pound Payment Interface Providers (PIPs), and facilitate the creation of innovative services. We also identify potential next steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08653v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lee Braine, Shreepad Shukla, Piyush Agrawal, Shrirang Khedekar, Aishwarya Nair</dc:creator>
    </item>
    <item>
      <title>A Grading Rubric for AI Safety Frameworks</title>
      <link>https://arxiv.org/abs/2409.08751</link>
      <description>arXiv:2409.08751v1 Announce Type: new 
Abstract: Over the past year, artificial intelligence (AI) companies have been increasingly adopting AI safety frameworks. These frameworks outline how companies intend to keep the potential risks associated with developing and deploying frontier AI systems to an acceptable level. Major players like Anthropic, OpenAI, and Google DeepMind have already published their frameworks, while another 13 companies have signaled their intent to release similar frameworks by February 2025. Given their central role in AI companies' efforts to identify and address unacceptable risks from their systems, AI safety frameworks warrant significant scrutiny. To enable governments, academia, and civil society to pass judgment on these frameworks, this paper proposes a grading rubric. The rubric consists of seven evaluation criteria and 21 indicators that concretize the criteria. Each criterion can be graded on a scale from A (gold standard) to F (substandard). The paper also suggests three methods for applying the rubric: surveys, Delphi studies, and audits. The purpose of the grading rubric is to enable nuanced comparisons between frameworks, identify potential areas of improvement, and promote a race to the top in responsible AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08751v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jide Alaga, Jonas Schuett, Markus Anderljung</dc:creator>
    </item>
    <item>
      <title>Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance</title>
      <link>https://arxiv.org/abs/2409.08963</link>
      <description>arXiv:2409.08963v1 Announce Type: new 
Abstract: Ensuring content compliance with community guidelines is crucial for maintaining healthy online social environments. However, traditional human-based compliance checking struggles with scaling due to the increasing volume of user-generated content and a limited number of moderators. Recent advancements in Natural Language Understanding demonstrated by Large Language Models unlock new opportunities for automated content compliance verification. This work evaluates six AI-agents built on Open-LLMs for automated rule compliance checking in Decentralized Social Networks, a challenging environment due to heterogeneous community scopes and rules. Analyzing over 50,000 posts from hundreds of Mastodon servers, we find that AI-agents effectively detect non-compliant content, grasp linguistic subtleties, and adapt to diverse community contexts. Most agents also show high inter-rater reliability and consistency in score justification and suggestions for compliance. Human-based evaluation with domain experts confirmed the agents' reliability and usefulness, rendering them promising tools for semi-automated or human-in-the-loop content moderation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08963v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucio La Cava, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>Using Peer-Customers to Scalably Pair Student Teams with Customers for Hands-on Curriculum Final Projects</title>
      <link>https://arxiv.org/abs/2409.08299</link>
      <description>arXiv:2409.08299v1 Announce Type: cross 
Abstract: Peer-customer is a mechanism to pair student teams with customers in hands-on curriculum courses. Each student pitches a problem they want someone else in the class to solve for them. The use of peer-customers provides practical and scalable access for students to work with a customer on a real-world need for their final project. The peer-customer, despite being a student in the class, do not work on the project with the team. This dissociation forces a student team to practice customer needs assessment, testing, and surveying that can often be lacking in self-ideated final projects that do not have resources to curate external customers like in capstone courses. We prototyped the use of peer-customers in an introductory physical prototyping course focused on basic embedded systems design and python programming. In this paper, we present a practical guide on how best to use peer-customers, supported by key observations made during two separate offerings of the course with a total of N=64 students (N=29 Y1 and N=35 Y2).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08299v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Jay Wang</dc:creator>
    </item>
    <item>
      <title>Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue</title>
      <link>https://arxiv.org/abs/2409.08330</link>
      <description>arXiv:2409.08330v1 Announce Type: cross 
Abstract: Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08330v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johnathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, Dustin Wright, Abraham Israeli, Anders Giovanni M{\o}ller, Lechen Zhang, David Jurgens</dc:creator>
    </item>
    <item>
      <title>Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with LLM-based Agents</title>
      <link>https://arxiv.org/abs/2409.08717</link>
      <description>arXiv:2409.08717v1 Announce Type: cross 
Abstract: In the context where social media is increasingly becoming a significant platform for social movements and the formation of public opinion, accurately simulating and predicting the dynamics of user opinions is of great importance for understanding social phenomena, policy making, and guiding public opinion. However, existing simulation methods face challenges in capturing the complexity and dynamics of user behavior. Addressing this issue, this paper proposes an innovative simulation method for the dynamics of social media user opinions, the FDE-LLM algorithm, which incorporates opinion dynamics and epidemic model. This effectively constrains the actions and opinion evolution process of large language models (LLM), making them more aligned with the real cyber world. In particular, the FDE-LLM categorizes users into opinion leaders and followers. Opinion leaders are based on LLM role-playing and are constrained by the CA model, while opinion followers are integrated into a dynamic system that combines the CA model with the SIR model. This innovative design significantly improves the accuracy and efficiency of the simulation. Experiments were conducted on four real Weibo datasets and validated using the open-source model ChatGLM. The results show that, compared to traditional agent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion diffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08717v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junchi Yao, Hongjie Zhang, Jie Ou, Dingyi Zuo, Zheng Yang, Zhicheng Dong</dc:creator>
    </item>
    <item>
      <title>Affective Computing Has Changed: The Foundation Model Disruption</title>
      <link>https://arxiv.org/abs/2409.08907</link>
      <description>arXiv:2409.08907v1 Announce Type: cross 
Abstract: The dawn of Foundation Models has on the one hand revolutionised a wide range of research problems, and, on the other hand, democratised the access and use of AI-based tools by the general public. We even observe an incursion of these models into disciplines related to human psychology, such as the Affective Computing domain, suggesting their affective, emerging capabilities. In this work, we aim to raise awareness of the power of Foundation Models in the field of Affective Computing by synthetically generating and analysing multimodal affective data, focusing on vision, linguistics, and speech (acoustics). We also discuss some fundamental problems, such as ethical issues and regulatory aspects, related to the use of Foundation Models in this research area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08907v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bj\"orn Schuller, Adria Mallol-Ragolta, Alejandro Pe\~na Almansa, Iosif Tsangko, Mostafa M. Amin, Anastasia Semertzidou, Lukas Christ, Shahin Amiriparian</dc:creator>
    </item>
    <item>
      <title>National Treasure: The Call for e-Democracy and US Election Security</title>
      <link>https://arxiv.org/abs/2409.08952</link>
      <description>arXiv:2409.08952v1 Announce Type: cross 
Abstract: Faith in the US electoral system is at risk. This issue stems from trust or lack thereof. Poor leaders ranted and attempted to sew discord in the democratic process and even tried to influence election results. Historically, the US has relied on paper ballots to cast private votes. Votes are watered down by the Electoral College. Elections are contested due to voter IDs and proof of citizenship. Methods of voting are nonsensically complex. In the technology age, this can be solved with a Smartcard National ID backed by Public-Key Infrastructure (PKI). This could be a method to restore hope in democracy and move the country back towards elections under a Popular Vote. Numbers are empirical and immutable and can solve the issue of Election Security in a bipartisan way. NATO allies like Estonia have already broken ground in using technology for eDemocracy or (Internet-based) iVoting. Acknowledging cyber attacks will happen, this is an opportunity for DHS and DOD (CYBERCOM) to collaborate on domestic operations and protect critical election infrastructure. This idea will not fix malicious information operations or civil stupidity. However, this is the way forward to securing elections now and forever. The views expressed by this whitepaper are those of the author and do not reflect the official policy or position of Dakota State University, the N.H. Army National Guard, the U.S. Army, the Department of Defense, or the U.S. Government. Cleared for release by DOPSR on 13 SEP 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08952v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Dorian Wong</dc:creator>
    </item>
    <item>
      <title>E2MoCase: A Dataset for Emotional, Event and Moral Observations in News Articles on High-impact Legal Cases</title>
      <link>https://arxiv.org/abs/2409.09001</link>
      <description>arXiv:2409.09001v1 Announce Type: cross 
Abstract: The way media reports on legal cases can significantly shape public opinion, often embedding subtle biases that influence societal views on justice and morality. Analyzing these biases requires a holistic approach that captures the emotional tone, moral framing, and specific events within the narratives. In this work we introduce E2MoCase, a novel dataset designed to facilitate the integrated analysis of emotions, moral values, and events within legal narratives and media coverage. By leveraging advanced models for emotion detection, moral value identification, and event extraction, E2MoCase offers a multi-dimensional perspective on how legal cases are portrayed in news articles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09001v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Candida M. Greco, Lorenzo Zangari, Davide Picca, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives</title>
      <link>https://arxiv.org/abs/2402.01662</link>
      <description>arXiv:2402.01662v3 Announce Type: replace 
Abstract: As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on specific people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death; indeed, the past year has seen a boom in startups purporting to offer such services. We call these "generative ghosts," since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we reflect on the history of technologies for AI afterlives, including current early attempts by individual enthusiasts and by startup companies to create generative ghosts. We then introduce a novel design space detailing potential implementations of generative ghosts, and use this taxonomy to ground discussion of the practical and ethical implications of various approaches to designing generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to better understand the risk/benefit landscape of this novel technology so as to ultimately empower people who wish to create and interact with AI afterlives to do so in a safe and beneficial manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01662v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meredith Ringel Morris, Jed R. Brubaker</dc:creator>
    </item>
    <item>
      <title>The Cost of Arbitrariness for Individuals: Examining the Legal and Technical Challenges of Model Multiplicity</title>
      <link>https://arxiv.org/abs/2407.13070</link>
      <description>arXiv:2407.13070v2 Announce Type: replace 
Abstract: Model multiplicity, the phenomenon where multiple models achieve similar performance despite different underlying learned functions, introduces arbitrariness in model selection. While this arbitrariness may seem inconsequential in expectation, its impact on individuals can be severe. This paper explores various individual concerns stemming from multiplicity, including the effects of arbitrariness beyond final predictions, disparate arbitrariness for individuals belonging to protected groups, and the challenges associated with the arbitrariness of a single algorithmic system creating a monopoly across various contexts. It provides both an empirical examination of these concerns and a comprehensive analysis from the legal standpoint, addressing how these issues are perceived in the anti-discrimination law in Canada. We conclude the discussion with technical challenges in the current landscape of model multiplicity to meet legal requirements and the legal gap between current law and the implications of arbitrariness in model selection, highlighting relevant future research directions for both disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13070v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prakhar Ganesh, Ihsan Ibrahim Daldaban, Ignacio Cofone, Golnoosh Farnadi</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Fair Federated Learning with Small Samples</title>
      <link>https://arxiv.org/abs/2402.16158</link>
      <description>arXiv:2402.16158v2 Announce Type: replace-cross 
Abstract: As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for both fairness and accuracy, and our experimental results further provide robust empirical validation for our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16158v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qichuan Yin, Zexian Wang, Junzhou Huang, Huaxiu Yao, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning</title>
      <link>https://arxiv.org/abs/2403.10984</link>
      <description>arXiv:2403.10984v2 Announce Type: replace-cross 
Abstract: To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end tool for precise carbon footprint estimation in IoT-enabled DL, with deviations as low as 5\% for operational and 3.23\% for embodied carbon footprints compared to actual measurements across various DL models. Additionally, practical applications of \carb~are showcased through multiple user case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10984v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Chen, Shahzeen Attari, Gayle Buck, Lei Jiang</dc:creator>
    </item>
  </channel>
</rss>
