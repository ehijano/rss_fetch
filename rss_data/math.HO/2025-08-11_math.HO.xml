<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.HO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.HO</link>
    <description>math.HO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.HO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Aug 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bernard Bolzano: from Topological to Arithmetical Continuum and Back Again</title>
      <link>https://arxiv.org/abs/2508.06897</link>
      <description>arXiv:2508.06897v1 Announce Type: new 
Abstract: Although Bolzano's concept of the continuum has gradually evolved, the basis remained the same: the continuum as an infinite class of points arranged in such a way that the so-called \emph{Bolzano completeness} holds. Bolzano realized over time that the central role of a general comprehension of continuum plays in its arithmetic description and constructed his measurable numbers. Their interpretations in the standard and non-standard models of real numbers clarify their relationship and also suggest why Bolzano did not base his theory of functions on infinitesimal numbers. The three main theorems on measurable numbers are various forms of their completeness. I argue why the second one is indeed the \emph{Supremum Theorem} and that an important corollary of the third one is a proof of the \emph{Bolzano completeness}. Only when the notion of continuum was supported by measurable numbers could Bolzano, in his last book, \emph{Paradoxes of the Infinite}, confidently defend the general properties of the continuum and reject the paradoxes associated with them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06897v1</guid>
      <category>math.HO</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kate\v{r}ina Trlifajov\'a</dc:creator>
    </item>
    <item>
      <title>Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective</title>
      <link>https://arxiv.org/abs/2506.24006</link>
      <description>arXiv:2506.24006v2 Announce Type: replace-cross 
Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question of how they can be integrated into education. One hope is that they can support mathematics learning, including word-problem solving. Since LLMs can handle textual input with ease, they appear well-suited for solving mathematical word problems. Yet their real competence, whether they can make sense of the real-world context, and the implications for classrooms remain unclear. We conducted a scoping review from a mathematics-education perspective, including three parts: a technical overview, a systematic review of word problems used in research, and a state-of-the-art empirical evaluation of LLMs on mathematical word problems. First, in the technical overview, we contrast the conceptualization of word problems and their solution processes between LLMs and students. In computer-science research this is typically labeled mathematical reasoning, a term that does not align with usage in mathematics education. Second, our literature review of 213 studies shows that the most popular word-problem corpora are dominated by s-problems, which do not require a consideration of realities of their real-world context. Finally, our evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, o3, and GPT-5 on 287 word problems shows that most recent LLMs solve these s-problems with near-perfect accuracy, including a perfect score on 20 problems from PISA. LLMs still showed weaknesses in tackling problems where the real-world context is problematic or non-sensical. In sum, we argue based on all three aspects that LLMs have mastered a superficial solution process but do not make sense of word problems, which potentially limits their value as instructional tools in mathematics classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24006v2</guid>
      <category>cs.CL</category>
      <category>math.HO</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anselm R. Strohmaier, Wim Van Dooren, Kathrin Se{\ss}ler, Brian Greer, Lieven Verschaffel</dc:creator>
    </item>
  </channel>
</rss>
