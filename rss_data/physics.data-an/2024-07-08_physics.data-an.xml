<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Unbalanced optimal transport for stochastic particle tracking</title>
      <link>https://arxiv.org/abs/2407.04583</link>
      <description>arXiv:2407.04583v1 Announce Type: new 
Abstract: Non-invasive flow measurement techniques, such as particle tracking velocimetry, resolve 3D velocity fields by pairing tracer particle positions in successive time steps. These trajectories are crucial for evaluating physical quantities like vorticity, shear stress, pressure, and coherent structures. Traditional approaches deterministically reconstruct particle positions and extract particle tracks using tracking algorithms. However, reliable track estimation is challenging due to measurement noise caused by high particle density, particle image overlap, and falsely reconstructed 3D particle positions. To overcome this challenge, probabilistic approaches quantify the epistemic uncertainty in particle positions, typically using a Gaussian probability distribution. However, the standard deterministic tracking algorithms relying on nearest-neighbor search do not directly extend to the probabilistic setting. Moreover, such algorithms do not necessarily find globally consistent solutions robust to reconstruction errors. This paper aims to develop a globally consistent nearest-neighborhood algorithm that robustly extracts stochastic particle tracks from the reconstructed Gaussian particle distributions in all frames. Our tracking algorithm relies on the unbalanced optimal transport theory in the metric space of Gaussian measures. Specifically, we optimize a binary transport plan for efficiently moving the Gaussian distributions of reconstructed particle positions between time frames. We achieve this by computing the partial Wasserstein distance in the metric space of Gaussian measures. Our tracking algorithm is robust to position reconstruction errors since it automatically detects the number of particles that should be matched through hyperparameter optimization. Finally, we validate our method using an in vitro flow experiment using a 3D-printed cerebral aneurysm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04583v1</guid>
      <category>physics.data-an</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kairui Hao, Atharva Hans, Pavlos Vlachos, Ilias Bilionis</dc:creator>
    </item>
    <item>
      <title>Exotic and physics-informed support vector machines for high energy physics</title>
      <link>https://arxiv.org/abs/2407.03538</link>
      <description>arXiv:2407.03538v1 Announce Type: cross 
Abstract: In this article, we explore machine learning techniques using support vector machines with two novel approaches: exotic and physics-informed support vector machines. Exotic support vector machines employ unconventional techniques such as genetic algorithms and boosting. Physics-informed support vector machines integrate the physics dynamics of a given high-energy physics process in a straightforward manner. The goal is to efficiently distinguish signal and background events in high-energy physics collision data. To test our algorithms, we perform computational experiments with simulated Drell-Yan events in proton-proton collisions. Our results highlight the superiority of the physics-informed support vector machines, emphasizing their potential in high-energy physics and promoting the inclusion of physics information in machine learning algorithms for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03538v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Ramirez-Morales, A. Guti\'errez-Rodr\'iguez, T. Cisneros-P\'erez, H. Garcia-Tecocoatzi, A. D\'avila-Rivera</dc:creator>
    </item>
    <item>
      <title>On the performance of sequential Bayesian update for database of diverse tsunami scenarios</title>
      <link>https://arxiv.org/abs/2407.03631</link>
      <description>arXiv:2407.03631v1 Announce Type: cross 
Abstract: Although the sequential tsunami scenario detection framework was validated in our previous work, several tasks remain to be resolved from a practical point of view. This study aims to evaluate the performance of the previous tsunami scenario detection framework using a diverse database consisting of complex fault rupture patterns with heterogeneous slip distributions. Specifically, we compare the effectiveness of scenario superposition to that of the previous most likely scenario detection method. Additionally, how the length of the observation time window influences the accuracy of both methods is analyzed. We utilize an existing database comprising 1771 tsunami scenarios targeting the city Westport (WA, U.S.), which includes synthetic wave height records and inundation distributions as the result of fault rupture in the Cascadia subduction zone. The heterogeneous patterns of slips used in the database increase the diversity of the scenarios and thus make it a proper database for evaluating the performance of scenario superposition. To assess the performance, we consider various observation time windows shorter than 15 minutes and divide the database into five testing and learning sets. The evaluation accuracy of the maximum offshore wave, inundation depth, and its distribution is analyzed to examine the advantages of the scenario superposition method over the previous method. We introduce the dynamic time warping (DTW) method as an additional benchmark and compare its results to that of the Bayesian scenario detection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03631v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reika Nomura, Louise A. Hirao Vermare, Saneiki Fujita, Donsub Rim, Shuji Moriguchi, Randall J. LeVeque, Kenjiro Terada</dc:creator>
    </item>
    <item>
      <title>Learning Patterns from Biological Networks: A Compounded Burr Probability Model</title>
      <link>https://arxiv.org/abs/2407.04465</link>
      <description>arXiv:2407.04465v1 Announce Type: cross 
Abstract: Complex biological networks, comprising metabolic reactions, gene interactions, and protein interactions, often exhibit scale-free characteristics with power-law degree distributions. However, empirical studies have revealed discrepancies between observed biological network data and ideal power-law fits, highlighting the need for improved modeling approaches. To address this challenge, we propose a novel family of distributions, building upon the baseline Burr distribution. Specifically, we introduce the compounded Burr (CBurr) distribution, derived from a continuous probability distribution family, enabling flexible and efficient modeling of node degree distributions in biological networks. This study comprehensively investigates the general properties of the CBurr distribution, focusing on parameter estimation using the maximum likelihood method. Subsequently, we apply the CBurr distribution model to large-scale biological network data, aiming to evaluate its efficacy in fitting the entire range of node degree distributions, surpassing conventional power-law distributions and other benchmarks. Through extensive data analysis and graphical illustrations, we demonstrate that the CBurr distribution exhibits superior modeling capabilities compared to traditional power-law distributions. This novel distribution model holds great promise for accurately capturing the complex nature of biological networks and advancing our understanding of their underlying mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04465v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tanujit Chakraborty, Shraddha M. Naik, Swarup Chattopadhyay, Suchismita Das</dc:creator>
    </item>
    <item>
      <title>Efficient Materials Informatics between Rockets and Electrons</title>
      <link>https://arxiv.org/abs/2407.04648</link>
      <description>arXiv:2407.04648v1 Announce Type: cross 
Abstract: The true power of computational research typically can lay in either what it accomplishes or what it enables others to accomplish. In this work, both avenues are simultaneously embraced across several distinct efforts existing at three general scales of abstractions of what a material is - atomistic, physical, and design. At each, an efficient materials informatics infrastructure is being built from the ground up based on (1) the fundamental understanding of the underlying prior knowledge, including the data, (2) deployment routes that take advantage of it, and (3) pathways to extend it in an autonomous or semi-autonomous fashion, while heavily relying on artificial intelligence (AI) to guide well-established DFT-based ab initio and CALPHAD-based thermodynamic methods.
  The resulting multi-level discovery infrastructure is highly generalizable as it focuses on encoding problems to solve them easily rather than looking for an existing solution. To showcase it, this dissertation discusses the design of multi-alloy functionally graded materials (FGMs) incorporating ultra-high temperature refractory high entropy alloys (RHEAs) towards gas turbine and jet engine efficiency increase reducing CO2 emissions, as well as hypersonic vehicles. It leverages a new graph representation of underlying mathematical space using a newly developed algorithm based on combinatorics, not subject to many problems troubling the community. Underneath, property models and phase relations are learned from optimized samplings of the largest and highest quality dataset of HEA in the world, called ULTERA. At the atomistic level, a data ecosystem optimized for machine learning (ML) from over 4.5 million relaxed structures, called MPDD, is used to inform experimental observations and improve thermodynamic models by providing stability data enabled by a new efficient featurization framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04648v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adam M. Krajewski</dc:creator>
    </item>
    <item>
      <title>Visualization for physics analysis improvement and applications in BESIII</title>
      <link>https://arxiv.org/abs/2404.07951</link>
      <description>arXiv:2404.07951v2 Announce Type: replace 
Abstract: Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07951v2</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11467-024-1422-7</arxiv:DOI>
      <arxiv:journal_reference>Front. Phys. 19, 64201 (2024)</arxiv:journal_reference>
      <dc:creator>Zhi-Jun Li, Ming-Kuan Yuan, Yun-Xuan Song, Yan-Gu Li, Jing-Shu Li, Sheng-Sen Sun, Xiao-Long Wang, Zheng-Yun You, Ya-Jun Mao</dc:creator>
    </item>
    <item>
      <title>Approximate solutions of a general stochastic velocity-jump process subject to discrete-time noisy observations</title>
      <link>https://arxiv.org/abs/2406.19787</link>
      <description>arXiv:2406.19787v2 Announce Type: replace 
Abstract: Advances in experimental techniques allow the collection of high-space-and-time resolution data that track individual motile entities over time. This poses the question of how to use these data to efficiently and effectively calibrate motion models. However, typical mathematical models often overlook the inherent aspects of data collection, such as the discreteness and the experimental noise of the measured locations. In this paper, we focus on velocity-jump models suitable to describe single-agent motion in one spatial dimension, characterised by successive Markovian transitions between a finite network of $n$ states, each with a specified velocity and a fixed rate of switching to every other state. Since the problem of finding the exact distributions of discrete-time noisy data is generally intractable, we derive a series of approximations for the data distributions and compare them to in-silico data generated by the models using four example network structures. These comparisons suggest that the approximations are accurate given sufficiently infrequent state switching, or equivalently, a sufficiently high data collection frequency. Moreover, for infrequent switching, the PDFs comparisons highlight the importance of accounting for the correlation between subsequent measured locations, due to the likely permanence in the state visited in the previous measurement. The approximate distributions computed can be used for fast parameter inference and model selection between a range of velocity-jump models using single-agent tracking data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19787v2</guid>
      <category>physics.data-an</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arianna Ceccarelli, Alexander P. Browning, Ruth E. Baker</dc:creator>
    </item>
    <item>
      <title>Inference through innovation processes tested in the authorship attribution task</title>
      <link>https://arxiv.org/abs/2306.05186</link>
      <description>arXiv:2306.05186v3 Announce Type: replace-cross 
Abstract: Urn models for innovation capture fundamental empirical laws shared by several real-world processes. The so-called urn model with triggering includes, as particular cases, the urn representation of the two-parameter Poisson-Dirichlet process and the Dirichlet process, seminal in Bayesian non-parametric inference. In this work, we leverage this connection to introduce a general approach for quantifying closeness between symbolic sequences and test it within the framework of the authorship attribution problem. The method demonstrates high accuracy when compared to other related methods in different scenarios, featuring a substantial gain in computational efficiency and theoretical transparency. Beyond the practical convenience, this work demonstrates how the recently established connection between urn models and non-parametric Bayesian inference can pave the way for designing more efficient inference methods. In particular, the hybrid approach that we propose allows us to relax the exchangeability hypothesis, which can be particularly relevant for systems exhibiting complex correlation patterns and non-stationary dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05186v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulio Tani Raffaelli, Margherita Lalli, Francesca Tria</dc:creator>
    </item>
    <item>
      <title>Mixed Noise and Posterior Estimation with Conditional DeepGEM</title>
      <link>https://arxiv.org/abs/2402.02964</link>
      <description>arXiv:2402.02964v2 Announce Type: replace-cross 
Abstract: Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02964v2</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-2153/ad5926</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning: Science and Technology, Volume 5, Number 3, 2024</arxiv:journal_reference>
      <dc:creator>Paul Hagemann, Johannes Hertrich, Maren Casfor, Sebastian Heidenreich, Gabriele Steidl</dc:creator>
    </item>
  </channel>
</rss>
