<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 05:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Invariant Feature Extraction Through Conditional Independence and the Optimal Transport Barycenter Problem: the Gaussian case</title>
      <link>https://arxiv.org/abs/2512.20914</link>
      <description>arXiv:2512.20914v1 Announce Type: new 
Abstract: A methodology is developed to extract $d$ invariant features $W=f(X)$ that predict a response variable $Y$ without being confounded by variables $Z$ that may influence both $X$ and $Y$.
  The methodology's main ingredient is the penalization of any statistical dependence between $W$ and $Z$ conditioned on $Y$, replaced by the more readily implementable plain independence between $W$ and the random variable $Z_Y = T(Z,Y)$ that solves the [Monge] Optimal Transport Barycenter Problem for $Z\mid Y$. In the Gaussian case considered in this article, the two statements are equivalent.
  When the true confounders $Z$ are unknown, other measurable contextual variables $S$ can be used as surrogates, a replacement that involves no relaxation in the Gaussian case if the covariance matrix $\Sigma_{ZS}$ has full range. The resulting linear feature extractor adopts a closed form in terms of the first $d$ eigenvectors of a known matrix. The procedure extends with little change to more general, non-Gaussian / non-linear cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20914v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Bounos, Pablo Groisman, Mariela Sued, Esteban Tabak</dc:creator>
    </item>
    <item>
      <title>Statistical and computational challenges in ranking</title>
      <link>https://arxiv.org/abs/2512.21111</link>
      <description>arXiv:2512.21111v1 Announce Type: new 
Abstract: We consider the problem of ranking $n$ experts according to their abilities, based on the correctness of their answers to $d$ questions. This is modeled by the so-called crowd-sourcing model, where the answer of expert $i$ on question $k$ is modeled by a random entry, parametrized by $M_{i,k}$ which is increasing linearly with the expected quality of the answer. To enable the unambiguous ranking of the experts by ability, several assumptions on $M$ are available in the literature. We consider here the general isotonic crowd-sourcing model, where $M$ is assumed to be isotonic up to an unknown permutation $\pi^*$ of the experts - namely, $M_{\pi^{*-1}(i),k} \geq M_{\pi^{*-1}(i+1),k}$ for any $i\in [n-1], k \in [d]$. Then, ranking experts amounts to constructing an estimator of $\pi^*$. In particular, we investigate here the existence of statistically optimal and computationally efficient procedures and we describe recent results that disprove the existence of computational-statistical gaps for this problem. To provide insights on the key ideas, we start by discussing simpler and yet related sub-problems, namely sub-matrix detection and estimation. This corresponds to specific instances of the ranking problem where the matrix $M$ is constrained to be of the form $\lambda \mathbf 1\{S\times T\}$ where $S\subset [n], T\subset [d]$. This model has been extensively studied. We provide an overview of the results and proof techniques for this problem with a particular emphasis on the computational lower bounds based on low-degree polynomial methods. Then, we build upon this instrumental sub-problem to discuss existing results and algorithmic ideas for the general ranking problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21111v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Carpentier, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>Closed-form empirical Bernstein confidence sequences for scalars and matrices</title>
      <link>https://arxiv.org/abs/2512.21300</link>
      <description>arXiv:2512.21300v1 Announce Type: new 
Abstract: We derive a new closed-form variance-adaptive confidence sequence (CS) for estimating the average conditional mean of a sequence of bounded random variables. Empirically, it yields the tightest closed-form CS we have found for tracking time-varying means, across sample sizes up to $\approx 10^6$. When the observations happen to have the same conditional mean, our CS is asymptotically tighter than the recent closed-form CS of Waudby-Smith and Ramdas [38]. It also has other desirable properties: it is centered at the unweighted sample mean and has limiting width (multiplied by $\sqrt{t/\log t}$) independent of the significance level. We extend our results to provide a CS with the same properties for random matrices with bounded eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21300v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Subgroup Discovery with the Cox Model</title>
      <link>https://arxiv.org/abs/2512.20762</link>
      <description>arXiv:2512.20762v1 Announce Type: cross 
Abstract: We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.
  Subgroup discovery methods generally require a "quality function" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.
  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20762v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Izzo, Iain Melvin</dc:creator>
    </item>
    <item>
      <title>Optimal Algorithms for Nonlinear Estimation with Convex Models</title>
      <link>https://arxiv.org/abs/2512.20826</link>
      <description>arXiv:2512.20826v1 Announce Type: cross 
Abstract: A linear functional of an object from a convex symmetric set can be optimally estimated, in a worst-case sense, by a linear functional of observations made on the object. This well-known fact is extended here to a nonlinear setting: other simple functionals of the object can be optimally estimated by functionals of the observations that share a similar simple structure. This is established for the maximum of several linear functionals and even for the $\ell$th largest among them. Proving the latter requires an unusual refinement of the analytical Hahn--Banach theorem. The existence results are accompanied by practical recipes relying on convex optimization to construct the desired functionals, thereby justifying the term of estimation algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20826v1</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Foucart</dc:creator>
    </item>
    <item>
      <title>Testing degree heterogeneity in directed networks</title>
      <link>https://arxiv.org/abs/2502.09865</link>
      <description>arXiv:2502.09865v2 Announce Type: replace 
Abstract: In this study, we focus on the likelihood ratio tests in the $p_0$ model for testing degree heterogeneity in directed networks, which is an exponential family distribution on directed graphs with the bi-degree sequence as the naturally sufficient statistic. For testing the homogeneous null hypotheses $H_0: \alpha_1 = \cdots = \alpha_r$, we establish Wilks-type results in both increasing-dimensional and fixed-dimensional settings. For increasing dimensions, the normalized log-likelihood ratio statistic $[2\{\ell(\widehat{\mathbf{\theta}})-\ell(\widehat{\mathbf{\theta}}^0)\}-r]/(2r)^{1/2}$ converges in distribution to a standard normal distribution. For fixed dimensions, $2\{\ell(\widehat{\mathbf{\theta}})-\ell(\widehat{\mathbf{\theta}}^0)\}$ converges in distribution to a chi-square distribution with $r-1$ degrees of freedom as $n\rightarrow \infty$, independent of the nuisance parameters. Additionally, we present a Wilks-type theorem for the specified null $H_0: \alpha_i=\alpha_i^0$, $i=1,\ldots, r$ in high-dimensional settings, where the normalized log-likelihood ratio statistic also converges in distribution to a standard normal distribution. These results extend the work of \cite{yan2025likelihood} to directed graphs in a highly non-trivial way, where we need to analyze much more expansion terms in the fourth-order asymptotic expansions of the likelihood function and develop new approximate inverse matrices under the null restricted parameter spaces for approximating the inverse of the Fisher information matrices in the $p_0$ model. Simulation studies and real data analyses are presented to verify our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09865v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Pan, Qiuping Wang, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Don't Pass@k: A Bayesian Framework for Large Language Model Evaluation</title>
      <link>https://arxiv.org/abs/2510.04265</link>
      <description>arXiv:2510.04265v2 Announce Type: replace-cross 
Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://github.com/mohsenhariri/scorio</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04265v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Hariri, Amirhossein Samandar, Michael Hinczewski, Vipin Chaudhary</dc:creator>
    </item>
    <item>
      <title>Divergence-based Robust Generalised Bayesian Inference for Directional Data via von Mises-Fisher models</title>
      <link>https://arxiv.org/abs/2512.05668</link>
      <description>arXiv:2512.05668v2 Announce Type: replace-cross 
Abstract: This paper focusses on robust estimation of location and concentration parameters of the von Mises-Fisher distribution in the Bayesian framework. The von Mises-Fisher (or Langevin) distribution has played a central role in directional statistics. Directional data have been investigated for many decades, and more recently, they have gained increasing attention in diverse areas such as bioinformatics and text data analysis. Although outliers can significantly affect the estimation results even for directional data, the treatment of outliers remains an unresolved and challenging problem. In the frequentist framework, numerous studies have developed robust estimation methods for directional data with outliers, but, in contrast, only a few robust estimation methods have been proposed in the Bayesian framework. In this paper, we propose Bayesian inference based on the density power divergence and the $\gamma$-divergence and establish their asymptotic properties and robustness. In addition, the Bayesian approach naturally provides a way to assess estimation uncertainty through the posterior distribution, which is particularly useful for small samples. Furthermore, to carry out the posterior computation, we develop the posterior computation algorithm based on the weighted Bayesian bootstrap for estimating parameters. The effectiveness of the proposed methods is demonstrated through simulation studies. Using two real datasets, we further show that the proposed method provides reliable and robust estimation even in the presence of outliers or data contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05668v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoyuki Nakagawa, Yasuhito Tsuruta, Sho Kazari, Kouji Tahata</dc:creator>
    </item>
  </channel>
</rss>
