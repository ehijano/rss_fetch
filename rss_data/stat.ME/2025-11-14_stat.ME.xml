<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:02:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Coalescent Inference for Epidemics with Latent Periods</title>
      <link>https://arxiv.org/abs/2511.09686</link>
      <description>arXiv:2511.09686v1 Announce Type: new 
Abstract: Coalescent models are used to study the transmission dynamics of rapidly evolving pathogens from molecular sequence data obtained from infected individuals. However coalescent parameters, such as effective population size, offer limited interpretability for transmission dynamics. In this work, we derive a coalescent model for exposed-infected population dynamics that allows us to infer the number of infected individuals and the effective reproduction number over time from the sample genealogy. The model can be interpreted as a two-deme model in which coalescence is restricted to individuals from different demes (exposed and infected). We propose a new data-augmentation framework with Phase-type distribution for Bayesian inference of epidemiological parameters. We study the performance of our approach on simulations and apply it to re-analyze the 2014 Ebola outbreak in Liberia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09686v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac H. Goldstein, Julia A. Palacios</dc:creator>
    </item>
    <item>
      <title>State Space Modeling of Mortgage Default Rates under Natural Hazard Shocks</title>
      <link>https://arxiv.org/abs/2511.09698</link>
      <description>arXiv:2511.09698v1 Announce Type: new 
Abstract: Mortgage default rates, on the one hand, serve as a measure of economic health to support decision-making by insurance companies, and on the other hand, is a key risk factor in the asset-liability management (ALM) practice, as mortgage related assets constitute a significant proportion of insurers' investment portfolios. This paper studies the relationship between economic losses due to natural hazards and mortgage default rates. The topic is greatly relevant to the insurance industry, as excessive insurance losses from natural hazards can lead to a surge in mortgage defaults, creating compounded challenges for insurers. To this end, we apply a state-space modeling approach to decouple the effect of natural hazard losses on mortgage default rates after controlling for other economic determinants through the inclusion of latent variables. Moreover, we consider a sliced variant of the classical SSM to capture the subtle relationship that only emerges when natural hazard losses are sufficiently high. Our model verifies the significance of this relationship and provides insights into how natural hazard losses manifest as increased mortgage default rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09698v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel J. Eschker, Antik Chakraborty, Melanie Gall, Peter Jevtic, Jianxi Su</dc:creator>
    </item>
    <item>
      <title>Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport</title>
      <link>https://arxiv.org/abs/2511.09759</link>
      <description>arXiv:2511.09759v1 Announce Type: new 
Abstract: We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09759v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Borna Bateni, Yubai Yuan, Qi Xu, Annie Qu</dc:creator>
    </item>
    <item>
      <title>Modelos Empiricos de Pos-Dupla Selecao por LASSO: Discussoes para Estudos do Transporte Aereo</title>
      <link>https://arxiv.org/abs/2511.09767</link>
      <description>arXiv:2511.09767v1 Announce Type: new 
Abstract: This paper presents and discusses forms of estimation by regularized regression and model selection using the LASSO method - Least Absolute Shrinkage and Selection Operator. LASSO is recognized as one of the main supervised learning methods applied to high-dimensional econometrics, allowing work with large volumes of data and multiple correlated controls. Conceptual issues related to the consequences of high dimensionality in modern econometrics and the principle of sparsity, which underpins regularization procedures, are addressed. The study examines the main post-double selection and post-regularization models, including variations applied to instrumental variable models. A brief description of the lassopack routine package, its syntaxes, and examples of HD, HDS (High-Dimension Sparse), and IV-HDS models, with combinations involving fixed effects estimators, is also presented. Finally, the potential application of the approach in research focused on air transport is discussed, with emphasis on an empirical study on the operational efficiency of airlines and aircraft fuel consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09767v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.17594515</arxiv:DOI>
      <arxiv:journal_reference>Communications in Airline Economics Research, 201717804h, 2021</arxiv:journal_reference>
      <dc:creator>Alessandro V. M. Oliveira</dc:creator>
    </item>
    <item>
      <title>Multiple Treatments Causal Effects Estimation with Task Embeddings and Balanced Representation Learning</title>
      <link>https://arxiv.org/abs/2511.09814</link>
      <description>arXiv:2511.09814v1 Announce Type: new 
Abstract: The simultaneous application of multiple treatments is increasingly common in many fields, such as healthcare and marketing. In such scenarios, it is important to estimate the single treatment effects and the interaction treatment effects that arise from treatment combinations. Previous studies have proposed using independent outcome networks with subnetworks for interactions, or combining task embedding networks that capture treatment similarity with variational autoencoders. However, these methods suffer from the lack of parameter sharing among related treatments, or the estimation of unnecessary latent variables reduces the accuracy of causal effect estimation. To address these issues, we propose a novel deep learning framework that incorporates a task embedding network and a representation learning network with the balancing penalty. The task embedding network enables parameter sharing across related treatment patterns because it encodes elements common to single effects and contributions specific to interaction effects. The representation learning network with the balancing penalty learns representations nonparametrically from observed covariates while reducing distances in representation distributions across different treatment patterns. This process mitigates selection bias and avoids model misspecification. Simulation studies demonstrate that the proposed method outperforms existing baselines, and application to real-world marketing datasets confirms the practical implications and utility of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09814v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Murakami, Takumi Hattori, Kohsuke Kubota</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit Test for Generalized Functional Linear Models via Projection Averaging</title>
      <link>https://arxiv.org/abs/2511.09886</link>
      <description>arXiv:2511.09886v1 Announce Type: new 
Abstract: Assessing model adequacy is a crucial step in regression analysis, ensuring the validity of statistical inferences. For Generalized Functional Linear Models (GFLMs), which are widely used for modeling relationships between scalar responses and functional predictors, there is a recognized need for formal goodness-of-fit testing procedures. Current literature on this specific topic remains limited. This paper introduces a novel goodness-of-fit test for GFLMs. The test statistic is formulated as a U-statistic derived from a Cram\'er-von-Mises metric integrated over all one-dimensional projections of the functional predictor. This projection averaging strategy is designed to effectively mitigate the curse of dimensionality. We establish the asymptotic normality of the test statistic under the null hypothesis and prove the consistency under the alternatives. As the asymptotic variance of the limiting null distribution can be complex for practical use, we also propose practical bootstrap resampling methods for both continuous and discrete responses to compute p-values. Simulation studies confirm that the proposed test demonstrates good power performance across various settings, showing advantages over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09886v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feifei Chen, Kaiming Zhang, Yanni Zhang, Hua Liang</dc:creator>
    </item>
    <item>
      <title>A Clustering Approach for Basket Trials Based on Treatment Response Trajectories</title>
      <link>https://arxiv.org/abs/2511.09890</link>
      <description>arXiv:2511.09890v1 Announce Type: new 
Abstract: Heterogeneity in efficacy is sometimes observed across baskets in basket trials. In this study, we propose a model-free clustering framework that groups baskets based on transition probabilities derived from the trajectories of treatment response, rather than relying solely on a single efficacy endpoint such as the objective response rate. The number of clusters is not predetermined but is automatically determined in a data-driven manner based on the similarity structure among baskets. After clustering, baskets within the same cluster are analyzed using a hierarchical Bayesian model. This framework aims to improve the estimation precision of efficacy endpoints and enhance statistical power while maintaining the type~I error rate at the nominal level. The performance of the proposed method was evaluated through simulation studies. The results demonstrated that the proposed method can accurately identify cluster structures in heterogeneous settings and, even under such conditions, maintain the type~I error rate at the nominal level while improving statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09890v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Kojima, Keisuke Hanada, Atsuya Sato</dc:creator>
    </item>
    <item>
      <title>Leader-Follower Identification Methodology for Non-Lane Disciplined Heterogeneous Traffic Using Steady State Features</title>
      <link>https://arxiv.org/abs/2511.09946</link>
      <description>arXiv:2511.09946v1 Announce Type: new 
Abstract: Road traffic in developing countries, such as India, features a heterogeneous mix of vehicles operating under weak lane discipline (HWLD), encompassing both motorised and non-motorised modes with diverse sizes and manoeuvrability. These conditions lead to complex driver interactions, complicating the reliable identification of vehicle-following (VF) behaviour and leader-follower (LF) pairs. Traditional identification methods based on fixed thresholds for longitudinal and lateral proximity often misclassify non-following instances as valid LF pairs, degrading model performance. This study presents a refined and adaptive method for LF identification in HWLD traffic. It employs vehicle-type- and speed-specific desirable gap thresholds derived from the fundamental density-speed diagram to eliminate false-positive pairs. Additionally, Mexican Hat Wavelet Transform (MWT) is employed to analyse LV and SV speed profiles, verifying LV-SV interaction for LF pair identification. The three-stage filtering includes: (i) speed-gap consistency, (ii) approach/diverge detection via relative velocity sign changes and gap range, and (iii) wavelet-based speed correlation using MWT to confirm LV influence on SV. The framework effectively filters out LF pairs associated with overtaking, tailgating, and inconsistent gap dynamics, retaining only those with consistent VF behaviour and improving model accuracy. Analysis across thirteen LF combinations shows that VF dynamics depend on both SV and LV types. Symmetric pairs (e.g., CAR-CAR, AUTO-CAR) exhibit higher predictability and lower errors, while asymmetric pairs with heavy vehicles or two-wheelers show greater variability. The framework offers a robust foundation for traffic modelling and behaviour analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09946v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Eldhose, Bhargava Rama Chilukuri, Chandrasekharan Rajendran</dc:creator>
    </item>
    <item>
      <title>Addressing zero-inflated and mis-measured functional predictors in scalar-on-function regression model</title>
      <link>https://arxiv.org/abs/2511.09972</link>
      <description>arXiv:2511.09972v1 Announce Type: new 
Abstract: Wearable devices are often used in clinical and epidemiological studies to monitor physical activity behavior and its influence on health outcomes. These devices are worn over multiple days to record activity patterns, such as step counts recorded at the minute level, resulting in multi-level, longitudinal, high-dimensional, or functional data. When monitoring patterns of step counts over multiple days, devices may record excess zeros during periods of sedentary behavior or non-wear times. Additionally, it has been demonstrated that the accuracy of wearable devices in monitoring true physical activity patterns depends on the intensity of the activities and wear times. While work on adjusting for biases due to measurement errors in functional data is a growing field, relatively less work has been done to study the occurrence of excess zeros along with measurement errors and their combined influence on estimation and inference in multi-level scalar-on-function regression models. We propose semi-continuous modeling approaches to adjust for biases due to zero inflation and measurement errors in scalar-on-function regression models. We provide theoretical justifications for our proposed methods and, through extensive simulations, we demonstrated their finite sample properties. Finally, the developed methods are applied to a school-based intervention study examining the association between school day physical activity with age- and sex-adjusted body mass index among elementary school-aged children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09972v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyang Ji, Lan Xue, Ufuk Beyaztas, Roger S. Zoh, Jeff Goldsmith, Mark E. Benden, Carmen D. Tekwe</dc:creator>
    </item>
    <item>
      <title>Outlier-robust copula regression for bivariate continuous proportions: an application to cushion plant vitality</title>
      <link>https://arxiv.org/abs/2511.10016</link>
      <description>arXiv:2511.10016v1 Announce Type: new 
Abstract: Continuous proportions measured on the same experimental unit often pose two challenges: interior outliers that inflate variance beyond the beta ceiling and residual dependence that invalidates independent-margin models. We introduce a Bayesian copula modeling approach that combines rectangular-beta margins, which temper interior outliers by reallocating mass from the peak to a uniform component, with a single-parameter copula to capture concordance. Gaussian, Gumbel, and Clayton copula families are fitted, and log marginal likelihoods are obtained via bridge sampling to guide model selection. Applied to a 13-year survey (2003-2016) of Azorella selago cushion plants on sub-Antarctic Marion Island, the copula models outperform independence baselines in explaining percent dead stem cover. Accounting for between-year dependence uncovers a positive west-slope effect and weakens the cushion size effect. Simulation results show negligible bias and near-nominal 95% highest posterior density coverage across a range of tail weight and dependence scenarios, confirming good frequentist properties. The method integrates readily with JAGS and provides a robust default for paired proportion data in ecology and other disciplines where bounded outcomes and occasional outliers coincide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10016v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Divan A. Burger, Janet van Niekerk, Peter C. le Roux, Morgan J. Raath-Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Masking criteria for selecting an imputation model</title>
      <link>https://arxiv.org/abs/2511.10048</link>
      <description>arXiv:2511.10048v1 Announce Type: new 
Abstract: The masking-one-out (MOO) procedure, masking an observed entry and comparing it versus its imputed values, is a very common procedure for comparing imputation models. We study the optimum of this procedure and generalize it to a missing data assumption and establish the corresponding semi-parametric efficiency theory. However, MOO is a measure of prediction accuracy, which is not ideal for evaluating an imputation model. To address this issue, we introduce three modified MOO criteria, based on rank transformation, energy distance, and likelihood principle, that allow us to select an imputation model that properly account for the stochastic nature of data. The likelihood approach further enables an elegant framework of learning an imputation model from the data and we derive its statistical and computational learning theories as well as consistency of BIC model selection. We also show how MOO is related to the missing-at-random assumption. Finally, we introduce the prediction-imputation diagram, a two-dimensional diagram visually comparing both the prediction and imputation utilities for various imputation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10048v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanjiao Yang, Daniel Suen, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>A tutorial for propensity score weighting methods under violations of the positivity assumption</title>
      <link>https://arxiv.org/abs/2511.10077</link>
      <description>arXiv:2511.10077v1 Announce Type: new 
Abstract: Violations of the positivity assumption can render conventional causal estimands unidentifiable, including the average treatment effect (ATE), the average treatment effect on the treated (ATT), and the average treatment effect on the controls (ATC). Shifting the inferential focus to their alternative counterparts -- the weighted ATE (WATE), the weighted ATT (WATT), and the weighted ATC (WATC) -- offers valuable insights into treatment effects while preserving internal validity. In this tutorial, we provide a comprehensive review of recent advances in propensity score (PS) weighting methods, along with practical guidance on how to select a primary target estimand (while other estimands serve as supplementary analyses), implement the corresponding PS-weighted estimators, and conduct post-weighting diagnostic assessments. The tutorial is accompanied by a user-friendly R package, ChiPS. We demonstrate the pertinence of various estimators through extensive simulation studies. We illustrate the flow of the tutorial on two real-world case studies: (i) Effect of smoking on blood lead level using data from the 2007-2008 National Health and Nutrition Examination Survey (NHANES); and (ii) Impact of history of sex work on HIV status among transgender women in South Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10077v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Yuan Wang, Ying Gao, Tonia Poteat, Roland A. Matsouaka</dc:creator>
    </item>
    <item>
      <title>Zeroes and Extrema of Functions via Random Measures</title>
      <link>https://arxiv.org/abs/2511.10293</link>
      <description>arXiv:2511.10293v1 Announce Type: new 
Abstract: We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10293v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios Christou Micheas</dc:creator>
    </item>
    <item>
      <title>Modelling toroidal and cylindrical data via the trivariate wrapped Cauchy copula with non-uniform marginals</title>
      <link>https://arxiv.org/abs/2511.10336</link>
      <description>arXiv:2511.10336v1 Announce Type: new 
Abstract: In this paper, we propose a new flexible family of distributions for data that consist of three angles, two angles and one linear component, or one angle and two linear components. To achieve this, we equip the recently proposed trivariate wrapped Cauchy copula with non-uniform marginals and develop a parameter estimation procedure. We compare our model to its main competitors for analyzing trivariate data and provide some evidence of its advantages. We illustrate our new model using toroidal data from protein bioinformatics of conformational angles, and cylindrical data from climate science related to buoy in the Adriatic Sea. The paper is motivated by these real trivariate datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10336v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophia Loizidou, Christophe Ley, Shogo Kato, Kanti V. Mardia</dc:creator>
    </item>
    <item>
      <title>Diagnostics for Semiparametric Accelerated Failure Time Models with R Package afttest</title>
      <link>https://arxiv.org/abs/2511.09823</link>
      <description>arXiv:2511.09823v1 Announce Type: cross 
Abstract: The semiparametric accelerated failure time (AFT) model is a useful alternative to the widely used Cox proportional hazard model, which directly links the logarithm of the failure time to the covariates, yielding more interpretable regression coefficients. However, diagnostic procedures for the semiparametric AFT model have received relatively little attention. This paper introduces afttest, an R package that implements recently developed diagnostic tools for the semiparametric AFT model. The package supports diagnostic procedures for models fitted with either rank-based or least-squares methods. It provides functions to assess model assumptions, including the overall adequacy, the link function, and functional form of each covariate. The test statistics are of Kolmogorov-type suprema of transformed aggregated martingale residual processes. The p-values are obtained by approximating the null distribution with an efficient multiplier bootstrap procedure. Additionally, the package offers graphical tools to compare the observed stochastic processes with a number of approximated realizations. Applications of the package to the well-known Mayo clinic primary biliary cirrhosis study are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09823v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Woojung Bae, Dongrak Choi, Jun Yan, Sangwook Kang</dc:creator>
    </item>
    <item>
      <title>Bridging Constraints and Stochasticity: A Fully First-Order Method for Stochastic Bilevel Optimization with Linear Constraints</title>
      <link>https://arxiv.org/abs/2511.09845</link>
      <description>arXiv:2511.09845v1 Announce Type: cross 
Abstract: This work provides the first finite-time convergence guarantees for linearly constrained stochastic bilevel optimization using only first-order methods, requiring solely gradient information without any Hessian computations or second-order derivatives. We address the unprecedented challenge of simultaneously handling linear constraints, stochastic noise, and finite-time analysis in bilevel optimization, a combination that has remained theoretically intractable until now. While existing approaches either require second-order information, handle only unconstrained stochastic problems, or provide merely asymptotic convergence results, our method achieves finite-time guarantees using gradient-based techniques alone. We develop a novel framework that constructs hypergradient approximations via smoothed penalty functions, using approximate primal and dual solutions to overcome the fundamental challenges posed by the interaction between linear constraints and stochastic noise. Our theoretical analysis provides explicit finite-time bounds on the bias and variance of the hypergradient estimator, demonstrating how approximation errors interact with stochastic perturbations. We prove that our first-order algorithm converges to $(\delta, \epsilon)$-Goldstein stationary points using $\Theta(\delta^{-1}\epsilon^{-5})$ stochastic gradient evaluations, establishing the first finite-time complexity result for this challenging problem class and representing a significant theoretical breakthrough in constrained stochastic bilevel optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09845v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cac Phan</dc:creator>
    </item>
    <item>
      <title>Model-oriented Graph Distances via Partially Ordered Sets</title>
      <link>https://arxiv.org/abs/2511.10625</link>
      <description>arXiv:2511.10625v1 Announce Type: cross 
Abstract: A well-defined distance on the parameter space is key to evaluating estimators, ensuring consistency, and building confidence sets. While there are typically standard distances to adopt in a continuous space, this is not the case for combinatorial parameters such as graphs that represent statistical models. Existing proposals like the structural Hamming distance are defined on the graphs rather than the models they represent and can hence lead to undesirable behaviors. We propose a model-oriented framework for defining the distance between graphs that is applicable across many different graph classes. Our approach treats each graph as a statistical model and organizes the graphs in a partially ordered set based on model inclusion. This induces a neighborhood structure, from which we define the model-oriented distance as the length of a shortest path through neighbors, yielding a metric in the space of graphs. We apply this framework to both probabilistic graphical models (e.g., undirected graphs and completed partially directed acyclic graphs) and causal graphical models (e.g., directed acyclic graphs and maximally oriented partially directed acyclic graphs). We analyze the theoretical and empirical behaviors of model-oriented distances. Algorithmic tools are also developed for computing and bounding these distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10625v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armeen Taeb, F. Richard Guo, Leonard Henckel</dc:creator>
    </item>
    <item>
      <title>Prediction intervals for random-effects meta-analysis: a confidence distribution approach</title>
      <link>https://arxiv.org/abs/1804.01054</link>
      <description>arXiv:1804.01054v5 Announce Type: replace 
Abstract: Prediction intervals are commonly used in meta-analysis with random-effects models. One widely used method, the Higgins-Thompson-Spiegelhalter prediction interval, replaces the heterogeneity parameter with its point estimate, but its validity strongly depends on a large sample approximation. This is a weakness in meta-analyses with few studies. We propose an alternative based on bootstrap and show by simulations that its coverage is close to the nominal level, unlike the Higgins-Thompson-Spiegelhalter method and its extensions. The proposed method was applied in three meta-analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:1804.01054v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/0962280218773520</arxiv:DOI>
      <arxiv:journal_reference>Statistical Methods in Medical Research 2019; 28(6): 1689-1702</arxiv:journal_reference>
      <dc:creator>Kengo Nagashima, Hisashi Noma, Toshi A. Furukawa</dc:creator>
    </item>
    <item>
      <title>Group Spike and Slab Variational Bayes</title>
      <link>https://arxiv.org/abs/2309.10378</link>
      <description>arXiv:2309.10378v3 Announce Type: replace 
Abstract: We introduce Group Spike-and-slab Variational Bayes (GSVB), a scalable method for group sparse regression. A fast co-ordinate ascent variational inference (CAVI) algorithm is developed for several common model families including Gaussian, Binomial and Poisson. Theoretical guarantees for our proposed approach are provided by deriving contraction rates for the variational posterior in grouped linear regression. Through extensive numerical studies, we demonstrate that GSVB provides state-of-the-art performance, offering a computationally inexpensive substitute to MCMC, whilst performing comparably or better than existing MAP methods. Additionally, we analyze three real world datasets wherein we highlight the practical utility of our method, demonstrating that GSVB provides parsimonious models with excellent predictive performance, variable selection and uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10378v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1542</arxiv:DOI>
      <dc:creator>Michael Komodromos, Marina Evangelou, Sarah Filippi, Kolyan Ray</dc:creator>
    </item>
    <item>
      <title>Proximal Causal Inference for Conditional Separable Effects</title>
      <link>https://arxiv.org/abs/2402.11020</link>
      <description>arXiv:2402.11020v4 Announce Type: replace 
Abstract: Scientists regularly pose questions about treatment effects on outcomes conditional on a post-treatment event. However, causal inference in such settings requires care, even in perfectly executed randomized experiments. Recently, the conditional separable effect (CSE) was proposed as an interventionist estimand that corresponds to scientifically meaningful questions in these settings. However, existing results for the CSE require no unmeasured confounding between the outcome and post-treatment event, an assumption frequently violated in practice. In this work, we address this concern by developing new identification and estimation results for the CSE that allow for unmeasured confounding. We establish nonparametric identification of the CSE in observational and experimental settings with time-varying confounders, provided that certain proxy variables for hidden common causes of the post-treatment event and outcome are available. For inference, we characterize an influence function for the CSE under a semiparametric model where nuisance functions are a priori unrestricted. Using modern machine learning methods, we construct nonparametric nuisance function estimators and establish convergence rates that improve upon existing results. Moreover, we develop a consistent, asymptotically linear, and locally semiparametric efficient estimator of the CSE. We illustrate our framework with simulation studies and a real-world cancer therapy trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11020v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chan Park, Mats Stensrud, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Generalizing the Finkelstein-Schoenfeld Test to Incorporate Multiple Alternating Thresholds</title>
      <link>https://arxiv.org/abs/2407.18341</link>
      <description>arXiv:2407.18341v5 Announce Type: replace 
Abstract: Composite endpoints consisting of both terminal and non-terminal events, such as death and hospitalization, are frequently used in cardiovascular clinical trials. The Finkelstein-Schoenfeld (FS) test provides a way to employ a hierarchical structure to combine fatal and non-fatal events by giving death information an absolute priority, which may limit the contribution of clinically meaningful non-fatal events. To provide a more flexible alternative, we propose the Finkelstein-Schoenfeld with Multiple Thresholds (FS-MT) test, which extends the standard FS test by incorporating multiple thresholds applied sequentially and alternating across endpoints. A weighted adaptive approach is also developed to help determine the thresholds in FS-MT. The proposed approach retains the statistical properties of the FS test while allowing more flexible use of information from lower-priority events. We evaluate the operating characteristics of the proposed test through simulations that vary the follow-up time, the correlation between events, and the treatment effect sizes. A case study based on the Digitalis Investigation Group clinical trial data is presented to further illustrate our proposed method. An R package ``FSMT'' that implements the proposed methodology has been developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18341v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Tassos Kyriakides, Scott Hummel, Fan Li, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>An Online Meta-Level Adaptive-Design Framework with Targeted Learning Inference: Applications to Evaluating and Utilizing Surrogate Outcomes in Adaptive Designs</title>
      <link>https://arxiv.org/abs/2408.02667</link>
      <description>arXiv:2408.02667v5 Announce Type: replace 
Abstract: Adaptive designs are increasingly used in clinical trials and online experiments to improve participant outcomes by dynamically updating treatment allocation based on accumulating data. However, in practice, experimenters often consider multiple candidate designs, each with distinct trade-offs, while only one can be implemented at a time, leaving benefits and costs of alternative designs unobserved and unquantified. To address this, we propose a novel meta-level adaptive design framework that enables real-time, data-driven evaluation and selection among candidate adaptive designs. Specifically, we define a new class of causal estimands to evaluate adaptive designs, estimate them with Targeted Maximum Likelihood Estimation framework, which yields an asymptotically normal estimator accommodating dependence in adaptive-design data without parametric assumptions, and support online design selection. We further apply this framework to a motivating example where multiple surrogates of a long-term primary outcome are considered for updating randomization probabilities in adaptive experiments. Unlike existing surrogate evaluation methods, our approach comprehensively quantifies the utility of surrogates to accelerate detection of heterogeneous treatment effects, expedite updates to treatment randomization and improve participant outcomes, facilitating dynamic selection among surrogate-guided adaptive designs. Overall, our framework provides a unified tool for evaluating opportunities and costs of various adaptive designs and guiding real-time decision-making in adaptive experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02667v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Aaron Hudson, Maya Petersen, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Easy Conditioning far beyond Gaussian</title>
      <link>https://arxiv.org/abs/2409.16003</link>
      <description>arXiv:2409.16003v4 Announce Type: replace 
Abstract: We address the challenge of conditioning multivariate densities, extending analytical conditioning results far beyond the Gaussian case. We review and discuss families of multivariate distributions that do enjoy analytical conditioning, also providing a few counter-examples. Proving that transdimensional stability under conditioning extends to mixtures and transformations, we demonstrate that a broader class of multivariate distributions inherit easy conditioning properties. Building on this insight, we developed a generative method to estimate conditional distributions from data by first fitting a flexible joint distribution using copulas and then performing analytical conditioning in a latent space. We specifically apply this methodology to Gaussian Mixture Copula Models (GMCM) and examine various fitting strategies. Through simulations and real-world data experiments, we showcase the efficacy of our method in tasks involving conditional density estimation and data imputation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16003v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Faul, David Ginsbourger, Ben Spycher</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Computation with Statistical Distances for Model Selection</title>
      <link>https://arxiv.org/abs/2410.21603</link>
      <description>arXiv:2410.21603v4 Announce Type: replace 
Abstract: Model selection is a key task in statistics, playing a critical role across various scientific disciplines. While no model can fully capture the complexities of a real-world data-generating process, identifying the model that best approximates it can provide valuable insights. Bayesian statistics offers a flexible framework for model selection by updating prior beliefs as new data becomes available, allowing for ongoing refinement of candidate models. This is typically achieved by calculating posterior probabilities, which quantify the support for each model given the observed data. However, in cases where likelihood functions are intractable, exact computation of these posterior probabilities becomes infeasible. Approximate Bayesian computation (ABC) has emerged as a likelihood-free method and it is traditionally used with summary statistics to reduce data dimensionality, however this often results in information loss difficult to quantify, particularly in model selection contexts. Recent advancements propose the use of full data approaches based on statistical distances, offering a promising alternative that bypasses the need for handcrafted summary statistics and can yield posterior approximations that more closely reflect the true posterior under suitable conditions. Despite these developments, full data ABC approaches have not yet been widely applied to model selection problems. This paper seeks to address this gap by investigating the performance of ABC with statistical distances in model selection. Through simulation studies and an application to toad movement models, this work explores whether full data approaches can overcome the limitations of summary statistic-based ABC for model choice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21603v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Grazian</dc:creator>
    </item>
    <item>
      <title>Monotone Missing Data: A Blessing and a Curse</title>
      <link>https://arxiv.org/abs/2411.03848</link>
      <description>arXiv:2411.03848v2 Announce Type: replace 
Abstract: Monotone missingness is commonly encountered in practice when a missing measurement compels another measurement to be missing. Because of the simpler missing data pattern, monotone missing data is often viewed as beneficial from the perspective of practical data analysis. However, in graphical missing data models, monotonicity has implications for the identifiability of the full law, i.e., the joint distribution of actual variables and response indicators. In the general nonmonotone case, the full law is known to be nonparametrically identifiable if and only if specific graphical structures are not present. We show that while monotonicity may enable the identification of the full law despite some of these structures, it also prevents the identification in certain cases that are identifiable without monotonicity. The results emphasize the importance of proper treatment of monotone missingness in the analysis of incomplete data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03848v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research, 2025, https://openreview.net/forum?id=kVthdlAVks</arxiv:journal_reference>
      <dc:creator>Santtu Tikka, Juha Karvanen</dc:creator>
    </item>
    <item>
      <title>Spatial scale-aware tail dependence modeling for high-dimensional spatial extremes</title>
      <link>https://arxiv.org/abs/2412.07957</link>
      <description>arXiv:2412.07957v3 Announce Type: replace 
Abstract: Extreme events over large spatial domains may exhibit highly heterogeneous tail dependence characteristics, yet most existing spatial extremes models yield only one dependence class over the entire spatial domain. To accurately characterize "data-level dependence'' in analysis of extreme events, we propose a mixture model that achieves flexible dependence properties and allows high-dimensional inference for extremes of spatial processes. We modify the popular random scale construction that multiplies a Gaussian random field by a single radial variable; we allow the radial variable to vary smoothly across space and add non-stationarity to the Gaussian process. As the level of extremeness increases, this single model exhibits both asymptotic independence at long ranges and either asymptotic dependence or independence at short ranges. We make joint inference on the dependence model and a marginal model using a copula approach within a Bayesian hierarchical model. Three different simulation scenarios show close to nominal frequentist coverage rates. Lastly, we apply the model to a dataset of extreme summertime precipitation over the central United States. We find that the joint tail of precipitation exhibits non-stationary dependence structure that cannot be captured by limiting extreme value models or current state-of-the-art sub-asymptotic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07957v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muyang Shi, Likun Zhang, Mark D. Risser, Benjamin A. Shaby</dc:creator>
    </item>
    <item>
      <title>Automatic Debiased Machine Learning for Smooth Functionals of Nonparametric M-Estimands</title>
      <link>https://arxiv.org/abs/2501.11868</link>
      <description>arXiv:2501.11868v2 Announce Type: replace 
Abstract: We develop a unified framework for automatic debiased machine learning (autoDML) to simplify inference for a broad class of statistical parameters. It applies to any smooth functional of a nonparametric \emph{M-estimand}, defined as the minimizer of a population risk over an infinite-dimensional linear space. Examples of M-estimands include counterfactual regression, quantile, and survival functions, as well as conditional average treatment effects. Rather than requiring manual derivation of influence functions, the framework automates the construction of debiased estimators using three components: the gradient and Hessian of the loss function and a linear approximation of the target functional. Estimation reduces to solving two risk minimization problems -- one for the M-estimand and one for a Riesz representer. The framework accommodates Neyman-orthogonal loss functions depending on nuisance parameters and extends to vector-valued M-estimands through joint risk minimization. For functionals of M-estimands, we characterize the efficient influence function and construct efficient autoDML estimators via one-step correction, targeted minimum loss estimation, and sieve-based plug-in methods. Under quadratic risk, these estimators exhibit double robustness for linear functionals. We further show they are insensitive to mild misspecification of the M-estimand model, incurring only second-order bias. We illustrate the method by estimating long-term survival probabilities under a semiparametric beta-geometric model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11868v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Aurelien Bibaut, Nathan Kallus, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>A robust contaminated discrete Weibull regression model for outlier-prone count data</title>
      <link>https://arxiv.org/abs/2504.09536</link>
      <description>arXiv:2504.09536v2 Announce Type: replace 
Abstract: Count data often exhibit overdispersion driven by heavy tails or excess zeros, making standard models (e.g., Poisson, negative binomial) insufficient for handling outlying observations. We propose a novel contaminated discrete Weibull (cDW) framework that augments a baseline discrete Weibull (DW) distribution with a heavier-tail subcomponent. This mixture retains a single shifted-median parameter for a unified regression link while selectively assigning extreme outcomes to the heavier-tail subdistribution. The cDW distribution accommodates strictly positive data by setting the truncation limit c=1 as well as full-range counts with c=0. We develop a Bayesian regression formulation and describe posterior inference using Markov chain Monte Carlo sampling. In an application to hospital length-of-stay data (with c=1, meaning the minimum possible stay is 1), the cDW model more effectively captures extreme stays and preserves the median-based link. Simulation-based residual checks, leave-one-out cross-validation, and a Kullback-Leibler outlier assessment confirm that the cDW model provides a more robust fit than the single-component DW model, reducing the influence of outliers and improving predictive accuracy. A simulation study further demonstrates the cDW model's robustness in the presence of heavy contamination. We also discuss how a hurdle scheme can accommodate datasets with many zeros while preventing the spurious inflation of zeros in situations without genuine zero inflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09536v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Divan A. Burger, Janet van Niekerk, Emmanuel Lesaffre</dc:creator>
    </item>
    <item>
      <title>While-alive regression analysis of composite survival endpoints</title>
      <link>https://arxiv.org/abs/2504.21710</link>
      <description>arXiv:2504.21710v3 Announce Type: replace 
Abstract: Composite endpoints, which combine two or more distinct outcomes, are frequently used in clinical trials to enhance the event rate and improve the statistical power. In the recent literature, the while-alive cumulative frequency measure offers a strong alternative to define composite survival outcomes, by relating the average event rate to the survival time. Although non-parametric methods have been proposed for two-sample comparisons between cumulative frequency measures in clinical trials, limited attention has been given to regression methods that directly address time-varying effects in while-alive measures for composite survival outcomes. Motivated by an individually randomized trial (HF-ACTION) and a cluster randomized trial (STRIDE), we address this gap by developing a regression framework for while-alive measures for composite survival outcomes that include a terminal component event. Our regression approach uses splines to model time-varying association between covariates and a while-alive loss rate of all component events, and can be applied to both independent and clustered data. We derive the asymptotic properties of the regression estimator in each setting and evaluate its performance through simulations. Finally, we apply our regression method to analyze data from the HF-ACTION individually randomized trial and the STRIDE cluster randomized trial. The proposed methods are implemented in the WAreg R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21710v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Hajime Uno, Fan Li</dc:creator>
    </item>
    <item>
      <title>Prediction of linear fractional stable motions using codifference, with application to non-Gaussian rough volatility</title>
      <link>https://arxiv.org/abs/2507.15437</link>
      <description>arXiv:2507.15437v2 Announce Type: replace 
Abstract: The linear fractional stable motion (LFSM) extends the fractional Brownian motion (fBm) by considering $\alpha$-stable increments. We propose a method to forecast future increments of the LFSM from past discrete-time observations, using the conditional expectation when $\alpha&gt;1$ or a semimetric projection otherwise. It relies on the codifference, which describes the serial dependence of the process, instead of the covariance. Indeed, covariance is commonly used for predicting an fBm but it is infinite when $\alpha&lt;2$. Some theoretical properties of the method and of its accuracy are studied and both a simulation study and an application to real data confirm the relevance of the approach. The LFSM-based method outperforms the fBm, when forecasting high-frequency FX rates. It also shows a promising performance in the forecast of time series of volatilities, decomposing properly, in the fractal dynamic of rough volatilities, the contribution of the kurtosis of the increments and the contribution of their serial dependence. Moreover, the analysis of hit ratios suggests that, beside independence, persistence, and antipersistence, a fourth regime of serial dependence exists for fractional processes, characterized by a selective memory controlled by a few large increments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15437v2</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Karl Sawaya, Thomas Valade</dc:creator>
    </item>
    <item>
      <title>New sampling approaches for Shrinkage Inverse-Wishart distribution</title>
      <link>https://arxiv.org/abs/2511.03044</link>
      <description>arXiv:2511.03044v2 Announce Type: replace 
Abstract: In this paper, we propose new sampling approaches for the Shrinkage Inverse-Wishart (SIW) distribution, a generalized family of the Inverse-Wishart distribution originally proposed by Berger et al. (2020, Annals of Statistics). It offers a flexible prior for covariance matrices and remains conjugate to the Gaussian likelihood, similar to the classical Inverse-Wishart. Despite these advantages, sampling from SIW remains challenging. The existing algorithm relies on a nested Gibbs sampler, which is slow and lacks rigorous theoretical analysis of its convergence. We propose a new algorithm based on the Sampling Importance Resampling (SIR) method, which is significantly faster and comes with theoretical guarantees on convergence rates. A known issue with SIR methods is the large discrepancy in importance weights, which occurs when the proposal distribution has thinner tails than the target. In the case of SIW, certain parameter settings can lead to such discrepancies, reducing the robustness of the output samples. To sample from such SIW distributions, we robustify the proposed algorithm by including a clipping step to the SIR framework which transforms large importance weights. We provide theoretical results on the convergence behavior in terms of the clipping size, and discuss strategies for choosing this parameter via simulation studies. The robustified version retains the computational efficiency of the original algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03044v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiye Jiang</dc:creator>
    </item>
    <item>
      <title>Asymmetric Space-Time Covariance Functions via Hierarchical Mixtures</title>
      <link>https://arxiv.org/abs/2511.07959</link>
      <description>arXiv:2511.07959v2 Announce Type: replace 
Abstract: This work is focused on constructing space-time covariance functions through a hierarchical mixture approach that can serve as building blocks for capturing complex dependency structures. This hierarchical mixture approach provides a unified modeling framework that not only constructs a new class of asymmetric space-time covariance functions with closed-form expressions, but also provides corresponding space-time process representations, which further unify constructions for many existing space-time covariance models. This hierarchical mixture framework decomposes the complexity of model specification at different levels of hierarchy, for which parsimonious covariance models can be specified with simple mixing measures to yield flexible properties and closed-form derivation. A characterization theorem is provided for the hierarchical mixture approach on how the mixing measures determine the statistical properties of covariance functions. Several new covariance models resulting from this hierarchical mixture approach are discussed in terms of their practical usefulness. A theorem is also provided to construct a general class of valid asymmetric space-time covariance functions with arbitrary and possibly different degrees of smoothness in space and in time and flexible long-range dependence. The proposed covariance class also bridges a theoretical gap in using the Lagrangian reference framework. The superior performance of several new parsimonious covariance models over existing models is verified with the well-known Irish wind data and the U.S. air temperature data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07959v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pulong Ma</dc:creator>
    </item>
    <item>
      <title>Modeling Spatio-Temporal Transport: From Rigid Advection to Realistic Dynamics</title>
      <link>https://arxiv.org/abs/2303.02756</link>
      <description>arXiv:2303.02756v3 Announce Type: replace-cross 
Abstract: Stochastic models for spatio-temporal transport face a critical trade-off between physical realism and interpretability. The advection model with a single constant velocity is interpretable but physically limited by its perfect correlation over time. This work aims to bridge the gap between this simple framework and its physically realistic extensions. Our guiding principle is to introduce a spatial correlation structure that vanishes over time. To achieve this, we present two distinct approaches. The first constructs complex velocity structures, either through superpositions of advection components or by allowing the velocity to vary locally. The second is a spectral technique that replaces the singular spectrum of rigid advection with a more flexible form, introducing temporal decorrelation controlled by parameters. We accompany these models with efficient simulation algorithms and demonstrate their success in replicating complex dynamics, such as tropical cyclones and the solutions of partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02756v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Laura Battagliola, Sofia Charlotta Olhede</dc:creator>
    </item>
    <item>
      <title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
      <link>https://arxiv.org/abs/2501.02409</link>
      <description>arXiv:2501.02409v5 Announce Type: replace-cross 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02409v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-bio.MN</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaikang Lin, Sei Chang, Aaron Zweig, Minseo Kang, Elham Azizi, David A. Knowles</dc:creator>
    </item>
    <item>
      <title>Semiparametric Double Reinforcement Learning with Applications to Long-Term Causal Inference</title>
      <link>https://arxiv.org/abs/2501.06926</link>
      <description>arXiv:2501.06926v4 Announce Type: replace-cross 
Abstract: Double Reinforcement Learning (DRL) enables efficient inference for policy values in nonparametric Markov decision processes (MDPs), but existing methods face two major obstacles: (1) they require stringent intertemporal overlap conditions on state trajectories, and (2) they rely on estimating high-dimensional occupancy density ratios. Motivated by problems in long-term causal inference, we extend DRL to a semiparametric setting and develop doubly robust, automatic estimators for general linear functionals of the Q-function in infinite-horizon, time-homogeneous MDPs. By imposing structure on the Q-function, we relax the overlap conditions required by nonparametric methods and obtain efficiency gains. The second obstacle--density-ratio estimation--typically requires computationally expensive and unstable min-max optimization. To address both challenges, we introduce superefficient nonparametric estimators whose limiting variance falls below the generalized Cramer-Rao bound. These estimators treat the Q-function as a one-dimensional summary of the state-action process, reducing high-dimensional overlap requirements to a single-dimensional condition. The procedure is simple to implement: estimate and calibrate the Q-function using fitted Q-iteration, then plug the result into the target functional, thereby avoiding density-ratio estimation altogether.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06926v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, David Hubbard, Allen Tran, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Why do zeroes happen? A model-based approach for demand classification</title>
      <link>https://arxiv.org/abs/2504.05894</link>
      <description>arXiv:2504.05894v2 Announce Type: replace-cross 
Abstract: Effective demand forecasting is critical for inventory management, production planning, and decision making across industries. Selecting the appropriate model and suitable features to efficiently capture patterns in the data is one of the main challenges in demand forecasting. In reality, this becomes even more complicated when the recorded sales have zeroes, which can happen naturally or due to some anomalies, such as stockouts and recording errors. Mistreating the zeroes can lead to the application of inappropriate forecasting methods, and thus leading to poor decision making. Furthermore, the demand itself can have different fundamental characteristics, and being able to distinguish one type from another might bring substantial benefits in terms of accuracy and thus decision making. We propose a two-stage model-based classification framework that in the first step, identifies artificially occurring zeroes, and in the second, classifies demand to one of the possible types: regular/intermittent, intermittent smooth/lumpy, fractional/count. The framework relies on statistical modelling and information criteria. We argue that different types of demand need different features, and show empirically that they tend to increase the accuracy of the forecasting methods and reduce inventory costs compared to those applied directly to the dataset without the generated features and the two-stage framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05894v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ivan Svetunkov, Anna Sroginis</dc:creator>
    </item>
  </channel>
</rss>
