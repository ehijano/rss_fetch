<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Back to the Continuous Attractor</title>
      <link>https://arxiv.org/abs/2408.00109</link>
      <description>arXiv:2408.00109v1 Announce Type: new 
Abstract: Continuous attractors offer a unique class of solutions for storing continuous-valued variables in recurrent system states for indefinitely long time intervals. Unfortunately, continuous attractors suffer from severe structural instability in general--they are destroyed by most infinitesimal changes of the dynamical law that defines them. This fragility limits their utility especially in biological systems as their recurrent dynamics are subject to constant perturbations. We observe that the bifurcations from continuous attractors in theoretical neuroscience models display various structurally stable forms. Although their asymptotic behaviors to maintain memory are categorically distinct, their finite-time behaviors are similar. We build on the persistent manifold theory to explain the commonalities between bifurcations from and approximations of continuous attractors. Fast-slow decomposition analysis uncovers the persistent manifold that survives the seemingly destructive bifurcation. Moreover, recurrent neural networks trained on analog memory tasks display approximate continuous attractors with predicted slow manifold structures. Therefore, continuous attractors are functionally robust and remain useful as a universal analogy for understanding analog memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00109v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Abel S\'agodi, Guillermo Mart\'in-S\'anchez, Piotr Sok\'o\l, Il Memming Park</dc:creator>
    </item>
    <item>
      <title>Investigating Brain Connectivity and Regional Statistics from EEG for early stage Parkinson's Classification</title>
      <link>https://arxiv.org/abs/2408.00711</link>
      <description>arXiv:2408.00711v1 Announce Type: new 
Abstract: We evaluate the effectiveness of combining brain connectivity metrics with signal statistics for early stage Parkinson's Disease (PD) classification using electroencephalogram data (EEG). The data is from 5 arousal states - wakeful and four sleep stages (N1, N2, N3 and REM). Our pipeline uses an Ada Boost model for classification on a challenging early stage PD classification task with with only 30 participants (11 PD , 19 Healthy Control). Evaluating 9 brain connectivity metrics we find the best connectivity metric to be different for each arousal state with Phase Lag Index achieving the highest individual classification accuracy of 86\% on N1 data. Further to this our pipeline using regional signal statistics achieves an accuracy of 78\%, using brain connectivity only achieves an accuracy of 86\% whereas combining the two achieves a best accuracy of 91\%. This best performance is achieved on N1 data using Phase Lag Index (PLI) combined with statistics derived from the frequency characteristics of the EEG signal. This model also achieves a recall of 80 \% and precision of 96\%. Furthermore we find that on data from each arousal state, combining PLI with regional signal statistics improves classification accuracy versus using signal statistics or brain connectivity alone. Thus we conclude that combining brain connectivity statistics with regional EEG statistics is optimal for classifier performance on early stage Parkinson's. Additionally, we find outperformance of N1 EEG for classification of Parkinson's and expect this could be due to disrupted N1 sleep in PD. This should be explored in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00711v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amarpal Sahota, Amber Roguski, Matthew W Jones, Zahraa S. Abdallah, Raul Santos-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Explainable Emotion Decoding for Human and Computer Vision</title>
      <link>https://arxiv.org/abs/2408.00493</link>
      <description>arXiv:2408.00493v1 Announce Type: cross 
Abstract: Modern Machine Learning (ML) has significantly advanced various research fields, but the opaque nature of ML models hinders their adoption in several domains. Explainable AI (XAI) addresses this challenge by providing additional information to help users understand the internal decision-making process of ML models. In the field of neuroscience, enriching a ML model for brain decoding with attribution-based XAI techniques means being able to highlight which brain areas correlate with the task at hand, thus offering valuable insights to domain experts. In this paper, we analyze human and Computer Vision (CV) systems in parallel, training and explaining two ML models based respectively on functional Magnetic Resonance Imaging (fMRI) and movie frames. We do so by leveraging the "StudyForrest" dataset, which includes functional Magnetic Resonance Imaging (fMRI) scans of subjects watching the "Forrest Gump" movie, emotion annotations, and eye-tracking data. For human vision the ML task is to link fMRI data with emotional annotations, and the explanations highlight the brain regions strongly correlated with the label. On the other hand, for computer vision, the input data is movie frames, and the explanations are pixel-level heatmaps. We cross-analyzed our results, linking human attention (obtained through eye-tracking) with XAI saliency on CV models and brain region activations. We show how a parallel analysis of human and computer vision can provide useful information for both the neuroscience community (allocation theory) and the ML community (biological plausibility of convolutional models).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00493v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessio Borriero, Martina Milazzo, Matteo Diano, Davide Orsenigo, Maria Chiara Villa, Chiara Di Fazio, Marco Tamietto, Alan Perotti</dc:creator>
    </item>
    <item>
      <title>Form Follows Function: A Different Approach to Neuron Connectivity</title>
      <link>https://arxiv.org/abs/2306.03337</link>
      <description>arXiv:2306.03337v4 Announce Type: replace 
Abstract: A different method of discovering how neurons are connected to process information is presented here: Design a simple logic circuit that can perform a single, biologically advantageous function. Engineering concepts can be helpful in choosing the function and in designing the logic circuit. Several implementations of the method are reviewed to demonstrate how a biologically advantageous function can be chosen, how one simple network can generate major phenomena that are widely considered unrelated, and how one network design can lead to others that explain entirely different aspects of the brain. These results show that the method can benefit neuromorphic engineering as well as neuroscience, and that some brain functions can be carried out remarkably simply, at least in principle if not in the details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03337v4</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lane Yoder</dc:creator>
    </item>
    <item>
      <title>An introduction to reinforcement learning for neuroscience</title>
      <link>https://arxiv.org/abs/2311.07315</link>
      <description>arXiv:2311.07315v2 Announce Type: replace 
Abstract: Reinforcement learning has a rich history in neuroscience, from early work on dopamine as a reward prediction error signal for temporal difference learning (Schultz et al., 1997) to recent work suggesting that dopamine could implement a form of 'distributional reinforcement learning' popularized in deep learning (Dabney et al., 2020). Throughout this literature, there has been a tight link between theoretical advances in reinforcement learning and neuroscientific experiments and findings. As a result, the theories describing our experimental data have become increasingly complex and difficult to navigate. In this review, we cover the basic theory underlying classical work in reinforcement learning and build up to an introductory overview of methods in modern deep reinforcement learning that have found applications in systems neuroscience. We start with an overview of the reinforcement learning problem and classical temporal difference algorithms, followed by a discussion of 'model-free' and 'model-based' reinforcement learning together with methods such as DYNA and successor representations that fall in between these two extremes. Throughout these sections, we highlight the close parallels between such machine learning methods and related work in both experimental and theoretical neuroscience. We then provide an introduction to deep reinforcement learning with examples of how these methods have been used to model different learning phenomena in systems neuroscience, such as meta-reinforcement learning (Wang et al., 2018) and distributional reinforcement learning (Dabney et al., 2020). Code that implements the methods discussed in this work and generates the figures is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07315v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristopher T. Jensen</dc:creator>
    </item>
    <item>
      <title>Information dynamics of $in\; silico$ EEG Brain Waves: Insights into oscillations and functions</title>
      <link>https://arxiv.org/abs/2311.13977</link>
      <description>arXiv:2311.13977v2 Announce Type: replace 
Abstract: The relation between EEG rhythms, brain functions, and behavioral correlates is well-established. Some mechanisms underlying rhythm generation are understood, enabling the replication of brain rhythms $in\; silico$. This allows to explore relations between neural oscillations and specific neuronal circuits, helping to decipher the functional properties of brain waves. Integrated information Decomposition ($\Phi$-ID) framework relates dynamical regimes with informational properties, providing deeper insights into neuronal dynamic functions. Here, we investigate wave emergence in an excitatory/inhibitory (E/I) balanced network of IF neurons with short-term synaptic plasticity producing a diverse range of EEG-like rhythms, from low $\delta$ waves to high-frequency oscillations. Through $\Phi$-ID, we analyze the network's information dynamics elucidating the system's suitability for robust information transfer, storage, and parallel operation. Our study identifies also regimes that may resemble pathological states due to poor informational properties and high randomness. We found that $in\; silico$ $\beta$ and $\delta$ waves are associated with maximum information transfer in inhibitory and excitatory neuron populations, and the coexistence of excitatory $\theta$, $\alpha$, and $\beta$ waves associated to information storage. Also, high-frequency oscillations can exhibit either high or poor informational properties, shedding light on discussions regarding physiological versus pathological high-frequency oscillations. Our study demonstrates that dynamical regimes with similar oscillations may exhibit different information dynamics. Finally, our findings suggest that the use of information dynamics in both model and experimental data analysis, could help discriminate between oscillations associated with cognitive functions and those linked to neuronal disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13977v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gustavo Menesse, Joaquin J. Torres</dc:creator>
    </item>
    <item>
      <title>Inter-individual and inter-site neural code conversion without shared stimuli</title>
      <link>https://arxiv.org/abs/2403.11517</link>
      <description>arXiv:2403.11517v2 Announce Type: replace 
Abstract: Inter-individual variability in fine-grained functional brain organization poses challenges for scalable data analysis and modeling. Functional alignment techniques can help mitigate these individual differences but typically require paired brain data with the same stimuli between individuals, which is often unavailable. We present a neural code conversion method that overcomes this constraint by optimizing conversion parameters based on the discrepancy between the stimulus contents represented by original and converted brain activity patterns. This approach, combined with hierarchical features of deep neural networks (DNNs) as latent content representations, achieves conversion accuracy comparable to methods using shared stimuli. The converted brain activity from a source subject can be accurately decoded using the target's pre-trained decoders, producing high-quality visual image reconstructions that rival within-individual decoding, even with data across different sites and limited training samples. Our approach offers a promising framework for scalable neural data analysis and modeling and a foundation for brain-to-brain communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11517v2</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haibao Wang, Jun Kai Ho, Fan L. Cheng, Shuntaro C. Aoki, Yusuke Muraki, Misato Tanaka, Yukiyasu Kamitani</dc:creator>
    </item>
    <item>
      <title>Analyzing the Brain's Dynamic Response to Targeted Stimulation using Generative Modeling</title>
      <link>https://arxiv.org/abs/2407.19737</link>
      <description>arXiv:2407.19737v2 Announce Type: replace 
Abstract: Generative models of brain activity have been instrumental in testing hypothesized mechanisms underlying brain dynamics against experimental datasets. Beyond capturing the key mechanisms underlying spontaneous brain dynamics, these models hold an exciting potential for understanding the mechanisms underlying the dynamics evoked by targeted brain-stimulation techniques. This paper delves into this emerging application, using concepts from dynamical systems theory to argue that the stimulus-evoked dynamics in such experiments may be shaped by new types of mechanisms distinct from those that dominate spontaneous dynamics. We review and discuss: (i) the targeted experimental techniques across spatial scales that can both perturb the brain to novel states and resolve its relaxation trajectory back to spontaneous dynamics; and (ii) how we can understand these dynamics in terms of mechanisms using physiological, phenomenological, and data-driven models. A tight integration of targeted stimulation experiments with generative quantitative modeling provides an important opportunity to uncover novel mechanisms of brain dynamics that are difficult to detect in spontaneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19737v2</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rishikesan Maran, Eli J. M\"uller, Ben D. Fulcher</dc:creator>
    </item>
    <item>
      <title>Localization of Brain Activity from EEG/MEG Using MV-PURE Framework</title>
      <link>https://arxiv.org/abs/1809.03930</link>
      <description>arXiv:1809.03930v2 Announce Type: replace-cross 
Abstract: We consider the problem of localization of sources of brain electrical activity from electroencephalographic (EEG) and magnetoencephalographic (MEG) measurements using spatial filtering techniques. We propose novel reduced-rank activity indices based on the minimum-variance pseudo-unbiased reduced-rank estimation (MV-PURE) framework. The main results of this paper establish the key unbiasedness property of the proposed indices and their higher spatial resolution compared with full-rank indices in challenging task of localizing closely positioned and possibly highly correlated sources, especially in low signal-to-noise regime. Numerical examples are provided to illustrate the practical applicability of the proposed activity indices using both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.03930v2</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.bspc.2020.102243</arxiv:DOI>
      <dc:creator>Tomasz Piotrowski, Jan Nikadon, Alexander Moiseev</dc:creator>
    </item>
  </channel>
</rss>
