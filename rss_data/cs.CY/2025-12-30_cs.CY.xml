<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Dec 2025 12:49:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability</title>
      <link>https://arxiv.org/abs/2512.22594</link>
      <description>arXiv:2512.22594v1 Announce Type: new 
Abstract: Urban food production is becoming an increasingly significant topic in the context of climate change and food security. Conducting research on this subject is becoming an essential element of urban development, deepening knowledge regarding the benefits, challenges, and potential for the development of urban agriculture as an alternative form of food production. Responding to this need, this monograph presents the results of a project study developing innovative socio-technological solutions for sustainable food production and consumption. The idea behind this unique project was to install twenty hydroponic cabinets in the corridors of the selected block of flats, where residents would grow edible plants. The presented research aimed to understand the people who joined this unique initiative. The qualitative study employed purposive sampling and in-depth interviews conducted in two waves. The study comprised 42 participants drawn from two communities of residents in {\L}\'od\'z and Warsaw, Poland. The findings outline the reasons that motivate urban residents to implement sustainable food production solutions, their farming experiences and the educational activities that led to their decision to join an innovative urban food production project. The results obtained will be relevant for those involved in the urban education process, including city authorities, urban educators, pro-environmental associations, and grassroots activists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22594v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ewa Duda, Adamina Korwin-Szymanowska</dc:creator>
    </item>
    <item>
      <title>Ungraded Assignments in Introductory Computing: A Report</title>
      <link>https://arxiv.org/abs/2512.23004</link>
      <description>arXiv:2512.23004v1 Announce Type: new 
Abstract: This experience report explores the effects of ungraded assignments on the learning experience of students in an introductory computing course. Our study examines the impact of ungraded assignments on student engagement, understanding, and overall academic performance. We developed and administered new ungraded assignments for a required course in the first year of the Computer Engineering curriculum called ECE 120 Introduction to Computing. To assess the effectiveness of our ungraded assignments, we employed a mixed-methods approach, including surveys, interviews, and performance analysis. Our analysis shows a positive relationship between participation in ungraded assignments and overall course performance, suggesting these assignments may appeal to high-achieving students and/or support better outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23004v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yehya Sleiman Tellawi, Abhishek K. Umrawal</dc:creator>
    </item>
    <item>
      <title>Inteligencia Artificial y Empleo: perspectiva Territorial y de G\'enero</title>
      <link>https://arxiv.org/abs/2512.23059</link>
      <description>arXiv:2512.23059v1 Announce Type: new 
Abstract: The diffusion of artificial intelligence, particularly generative models, is expected to transform labor markets in uneven ways across sectors, territories, and social groups. This paper proposes a methodological framework to estimate the potential exposure of employment to AI using sector based data, addressing the limitations of occupation centered approaches in the Spanish context. By constructing an AI CNAE incidence matrix and applying it to provincial employment data for the period 2021 to 2023, we provide a territorial and gender disaggregated assessment of AI exposure across Spain. The results reveal stable structural patterns, with higher exposure in metropolitan and service oriented regions and a consistent gender gap, as female employment exhibits higher exposure in all territories. Rather than predicting job displacement, the framework offers a structural perspective on where AI is most likely to reshape work and skill demands, supporting evidence based policy and strategic planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23059v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoni Mestre, Xavier Naya, Manoli Albert, Vicente Pelechano</dc:creator>
    </item>
    <item>
      <title>Identifying Barriers Hindering the Acceptance of Generative AI as a Work Associate, measured with the new AGAWA scale</title>
      <link>https://arxiv.org/abs/2512.23373</link>
      <description>arXiv:2512.23373v1 Announce Type: new 
Abstract: The attitudes of today's students toward generative AI (GenAI) will significantly influence its adoption in the workplace in the years to come, carrying both economic and social implications. It is therefore crucial to study this phenomenon now and identify obstacles for the successful implementation of GenAI in the workplace, using tools that keep pace with its rapid evolution. For this purpose, we propose the AGAWA scale, which measures attitudes toward an artificial agent utilising GenAI and perceived as a coworker. It is partially based on the TAM and UTAUT models of technology acceptance, taking into account issues that are particularly important in the context of the AI revolution, namely acceptance of its presence and social influence (e.g., as an assistant or even a supervisor), and above all, resolution of moral dilemmas. The advantage of the AGAWA scale is that it takes little time to complete and analyze, as it contains only four items. In the context of such cooperation, we investigated the importance of three factors: concerns about interaction with GenAI, its human-like characteristics, and a sense of human uniqueness, or even superiority over GenAI. An observed manifestation of the attitude towards this technology is the actual need to get help from it. The results showed that positive attitudes toward GenAI as a coworker were strongly associated with all three factors (negative correlation), and those factors were also related to each other (positive correlation). This confirmed the relationship between affective and moral dimensions of trust towards AI and attitudes towards generative AI at the workplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23373v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Sikorski, Albert {\L}ukasik, Jacek Matulewski, Arkadiusz Gut</dc:creator>
    </item>
    <item>
      <title>Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education</title>
      <link>https://arxiv.org/abs/2512.23587</link>
      <description>arXiv:2512.23587v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) presents a significant challenge to academic integrity within computing education. As educators seek reliable detection methods, this paper evaluates the capacity of three prominent LLMs (GPT-4, Claude, and Gemini) to identify AI-generated text in computing-specific contexts. We test their performance under both standard and 'deceptive' prompt conditions, where the models were instructed to evade detection. Our findings reveal a significant instability: while default AI-generated text was easily identified, all models struggled to correctly classify human-written work (with error rates up to 32%). Furthermore, the models were highly susceptible to deceptive prompts, with Gemini's output completely fooling GPT-4. Given that simple prompt alterations significantly degrade detection efficacy, our results demonstrate that these LLMs are currently too unreliable for making high-stakes academic misconduct judgments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23587v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Burger, Karmece Talley, Christina Trotter</dc:creator>
    </item>
    <item>
      <title>AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms</title>
      <link>https://arxiv.org/abs/2512.23633</link>
      <description>arXiv:2512.23633v1 Announce Type: new 
Abstract: One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23633v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> LearnLM Team,  Eedi,  :, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Braz\~ao</dc:creator>
    </item>
    <item>
      <title>Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models</title>
      <link>https://arxiv.org/abs/2512.22145</link>
      <description>arXiv:2512.22145v1 Announce Type: cross 
Abstract: Large Language Models are versatile general-task solvers, and their capabilities can truly assist people with scholarly peer review as \textit{pre-review} agents, if not as fully autonomous \textit{peer-review} agents. While incredibly beneficial, automating academic peer-review, as a concept, raises concerns surrounding safety, research integrity, and the validity of the academic peer-review process. The majority of the studies performing a systematic evaluation of frontier LLMs generating reviews across science disciplines miss the mark on addressing the alignment/misalignment of reviews along with the utility of LLM generated reviews when compared against publication outcomes such as \textbf{Citations}, \textbf{Hit-papers}, \textbf{Novelty}, and \textbf{Disruption}. This paper presents an experimental study in which we gathered ground-truth reviewer ratings from OpenReview and used various frontier open-weight LLMs to generate reviews of papers to gauge the safety and reliability of incorporating LLMs into the scientific review pipeline. Our findings demonstrate the utility of frontier open-weight LLMs as pre-review screening agents despite highlighting fundamental misalignment risks when deployed as autonomous reviewers. Our results show that all models exhibit weak correlation with human peer reviewers (0.15), with systematic overestimation bias of 3-5 points and uniformly high confidence scores (8.0-9.0/10) despite prediction errors. However, we also observed that LLM reviews correlate more strongly with post-publication metrics than with human scores, suggesting potential utility as pre-review screening tools. Our findings highlight the potential and address the pitfalls of automating peer reviews with language models. We open-sourced our dataset $D_{LMRSD}$ to help the research community expand the safety framework of automating scientific reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22145v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi</dc:creator>
    </item>
    <item>
      <title>AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History</title>
      <link>https://arxiv.org/abs/2512.22196</link>
      <description>arXiv:2512.22196v1 Announce Type: cross 
Abstract: Digital-humanities work on semantic shift often alternates between handcrafted close readings and opaque embedding machinery. We present a reproducible expert-system style pipeline that quantifies and visualises lexical drift in the Old Bailey Corpus (1720--1913), coupling interpretable trajectories with legally meaningful axes. We bin proceedings by decade with dynamic merging for low-resource slices, train skip-gram embeddings, align spaces through orthogonal Procrustes, and measure both geometric displacement and neighborhood turnover. Three visual analytics outputs, which are drift magnitudes, semantic trajectories, and movement along a mercy-versus-retribution axis, expose how justice, crime, poverty, and insanity evolve with penal reforms, transportation debates, and Victorian moral politics. The pipeline is implemented as auditable scripts so results can be reproduced in other historical corpora.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22196v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qizhi Wang</dc:creator>
    </item>
    <item>
      <title>Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening</title>
      <link>https://arxiv.org/abs/2512.22242</link>
      <description>arXiv:2512.22242v1 Announce Type: cross 
Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p &lt; .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22242v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>eess.IV</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.59275/j.melba.2025-1e7f</arxiv:DOI>
      <arxiv:journal_reference>Machine.Learning.for.Biomedical.Imaging. 3 (2025)</arxiv:journal_reference>
      <dc:creator>Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf</dc:creator>
    </item>
    <item>
      <title>Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities</title>
      <link>https://arxiv.org/abs/2512.22348</link>
      <description>arXiv:2512.22348v1 Announce Type: cross 
Abstract: Deplatforming, the permanent banning of entire communities, is a primary tool for content moderation on mainstream platforms. While prior research examines effects on banned communities or source platform health, the impact on alternative platforms that absorb displaced users remains understudied. We analyze four major Reddit ban waves (2015--2020) and their effects on generalist communities on Voat, asking how post-ban arrivals reshape community structure and through what mechanisms transformation occurs. Combining network analysis, toxicity detection, and dynamic reputation modeling, we identify two distinct regimes of migration impact: (1) Hostile Takeover (2015--2018), where post-ban arrival cohorts formed parallel social structures that bypassed existing community cores through sheer volume, and (2) Toxic Equilibrium (2018--2020), where the flattening of existing user hierarchy enabled newcomers to integrate into the now-dominant toxic community. Crucially, community transformation occurred through peripheral dynamics rather than hub capture: fewer than 5% of newcomers achieved central positions in most months, yet toxicity doubled. Migration structure also shaped outcomes: loosely organized communities dispersed into generalist spaces, while ideologically cohesive groups concentrated in dedicated enclaves. These findings suggest that receiving platforms face a narrow intervention window during the hostile takeover phase, after which toxic norms become self-sustaining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22348v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksandar Toma\v{s}evi\'c, Ana Vrani\'c, Aleksandra Alori\'c, Marija Mitrovi\'c Dankulov</dc:creator>
    </item>
    <item>
      <title>Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy</title>
      <link>https://arxiv.org/abs/2512.22462</link>
      <description>arXiv:2512.22462v1 Announce Type: cross 
Abstract: As large language models (LLMs) are embedded into mental health technologies, they are often framed either as tools assisting therapists or autonomous therapeutic systems. Such perspectives overlook their potential to mediate relational complexities in therapy, particularly for systemically marginalized clients. Drawing on in-depth interviews with 12 therapists and 12 marginalized clients in China, including LGBTQ+ individuals or those from other marginalized backgrounds, we identify enduring relational challenges: difficulties building trust amid institutional barriers, the burden clients carry in educating therapists about marginalized identities, and challenges sustaining authentic self-disclosure across therapy and daily life. We argue that addressing these challenges requires AI systems capable of actively mediating underlying knowledge gaps, power asymmetries, and contextual disconnects. To this end, we propose the Dynamic Boundary Mediation Framework, which reconceptualizes LLM-enhanced systems as adaptive boundary objects that shift mediating roles across therapeutic stages. The framework delineates three forms of mediation: Epistemic (reducing knowledge asymmetries), Relational (rebalancing power dynamics), and Contextual (bridging therapy-life discontinuities). This framework offers a pathway toward designing relationally accountable AI systems that center the lived realities of marginalized users and more effectively support therapeutic relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22462v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiatao Quan (The Hong Kong Polytechnic University), Ziyue Li (University of Washington), Tian Qi Zhu (University of Washington), Yuxuan Li (University of Washington), Baoying Wang (University of Washington), Wanda Pratt (University of Washington), Nan Gao (Nankai University)</dc:creator>
    </item>
    <item>
      <title>Mitigating Social Desirability Bias in Random Silicon Sampling</title>
      <link>https://arxiv.org/abs/2512.22725</link>
      <description>arXiv:2512.22725v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22725v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sashank Chapala, Maksym Mironov, Songgaojun Deng</dc:creator>
    </item>
    <item>
      <title>The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective</title>
      <link>https://arxiv.org/abs/2512.23429</link>
      <description>arXiv:2512.23429v1 Announce Type: cross 
Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23429v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.joi.2025.101766</arxiv:DOI>
      <arxiv:journal_reference>Journal of Informetrics, 2026</arxiv:journal_reference>
      <dc:creator>Yi Zhao, Yongjun Zhu, Donghun Kim, Yuzhuo Wang, Heng Zhang, Chao Lu, Chengzhi Zhang</dc:creator>
    </item>
    <item>
      <title>Prioritization First, Principles Second: An Adaptive Interpretation of Helpful, Honest, and Harmless Principles</title>
      <link>https://arxiv.org/abs/2502.06059</link>
      <description>arXiv:2502.06059v5 Announce Type: replace 
Abstract: The Helpful, Honest, and Harmless (HHH) principle is a foundational framework for aligning AI systems with human values. However, existing interpretations of the HHH principle often overlook contextual variability and conflicting requirements across applications. In this paper, we argue for an adaptive interpretation of the HHH principle and propose a reference framework for its adaptation to diverse scenarios. We first examine the principle's foundational significance and identify ambiguities and conflicts through case studies of its dimensions. To address these challenges, we introduce the concept of priority order, which provides a structured approach for balancing trade-offs among helpfulness, honesty, and harmlessness. Further, we explore the interrelationships between these dimensions, demonstrating how harmlessness and helpfulness can be jointly enhanced and analyzing their interdependencies in high-risk evaluations. Building on these insights, we propose a reference framework that integrates context definition, value prioritization, risk assessment, and benchmarking standards to guide the adaptive application of the HHH principle. This work offers practical insights for improving AI alignment, ensuring that HHH principles remain both ethically grounded and operationally effective in real-world AI deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06059v5</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Huang, Chujie Gao, Yujun Zhou, Kehan Guo, Xiangqi Wang, Or Cohen-Sasson, Max Lamparth, Xiangliang Zhang</dc:creator>
    </item>
    <item>
      <title>The Malaysian Election Corpus (MECo): Federal and State-Level Election Results from 1955 to 2025</title>
      <link>https://arxiv.org/abs/2505.06564</link>
      <description>arXiv:2505.06564v3 Announce Type: replace 
Abstract: Empirical research and public knowledge on Malaysia's elections have long been constrained by a lack of high-quality open data, particularly in the absence of a Freedom of Information framework. This paper introduces the Malaysian Election Corpus (MECo), an open-access panel database covering all federal and state general elections since 1955, as well as by-elections since 2008. MECo includes candidate- and constituency-level data for 9,704 electoral contests across seven decades, standardised with unique identifiers for candidates, parties, and coalitions. The database also provides summary statistics for each contest (electorate size, voter turnout, majority size, rejected ballots, unreturned ballots), key demographic data for candidates (age, gender, ethnicity), and lineage data for political parties. MECo is the most well-curated open database on Malaysian elections to date, and will unlock new opportunities for research, data journalism, and civic engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06564v3</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41597-025-06502-7</arxiv:DOI>
      <dc:creator>Thevesh Thevananthan</dc:creator>
    </item>
    <item>
      <title>The Tragedy of Productivity: A Unified Framework for Diagnosing Coordination Failures in Labor Markets and AI Governance</title>
      <link>https://arxiv.org/abs/2512.05995</link>
      <description>arXiv:2512.05995v3 Announce Type: replace 
Abstract: Despite productivity increasing eightfold since Keynes's 1930 prediction of 15-hour workweeks, workers globally still work roughly double these hours. Separately, AI development accelerates despite existential risk warnings from leading researchers. We demonstrate these failures share identical game-theoretic structure: coordination failures where individually rational choices produce collectively suboptimal outcomes.
  We synthesize five necessary and sufficient conditions characterizing such coordination failures as structural tragedies: N-player structure, binary choices with negative externalities, dominance where defection yields higher payoffs, Pareto-inefficiency where cooperation dominates mutual defection, and enforcement difficulty from structural barriers. We validate this framework across canonical cases and extend it through condition intensities, introducing a Tragedy Index revealing governance of transformative AI breakthroughs faces orders-of-magnitude greater coordination difficulty than climate change or nuclear weapons.
  Applied to productivity competition, we prove firms face coordination failure preventing productivity gains from translating to worker welfare. European evidence shows that even under favorable conditions, productivity-welfare decoupling persists. Applied to AI governance, we demonstrate development faces the same structure but with amplified intensity across eight dimensions compared to successful arms control, making coordination structurally more difficult than for nuclear weapons. The Russia-Ukraine drone war validates this: both sides escalated from dozens to thousands of drones monthly within two years despite prior governance dialogue.
  The analysis is diagnostic rather than prescriptive, identifying structural barriers to coordination rather than proposing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05995v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Dasdan</dc:creator>
    </item>
    <item>
      <title>ChatGPT-4 and other LLMs in the Turing Test: A Critical Analysis</title>
      <link>https://arxiv.org/abs/2503.06551</link>
      <description>arXiv:2503.06551v4 Announce Type: replace-cross 
Abstract: This paper critically examines the recent publication "ChatGPT-4 in the Turing Test" by Restrepo Echavarr\'ia (2025), challenging its central claims regarding the absence of minimally serious test implementations and the conclusion that ChatGPT-4 fails the Turing Test. The analysis reveals that the criticisms based on rigid criteria and limited experimental data are not fully justified. More importantly, the paper makes several constructive contributions that enrich our understanding of Turing Test implementations. It demonstrates that two distinct formats, the three-player and two-player tests, are both valid, each with unique methodological implications. The work distinguishes between absolute criteria for passing the test--the machine's probability of incorrect identification equals or exceeds the human's probability of correct identification--and relative criteria--which measure how closely a machine's performance approximates that of a human--, offering a more nuanced evaluation framework. Furthermore, the paper clarifies the probabilistic underpinnings of both test types by modeling them as Bernoulli experiments--correlated in the three-player version and uncorrelated in the two-player version. This formalization allows for a rigorous separation between the theoretical criteria for passing the test, defined in probabilistic terms, and the experimental data that require robust statistical methods for proper interpretation. In doing so, the paper not only refutes key aspects of the criticized study but also lays a solid foundation for future research on objective measures of how closely an AI's behavior aligns with, or deviates from, that of a human being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06551v4</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Giunti</dc:creator>
    </item>
    <item>
      <title>Beyond Context: Large Language Models Failure to Grasp Users Intent</title>
      <link>https://arxiv.org/abs/2512.21110</link>
      <description>arXiv:2512.21110v2 Announce Type: replace-cross 
Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21110v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos</dc:creator>
    </item>
  </channel>
</rss>
