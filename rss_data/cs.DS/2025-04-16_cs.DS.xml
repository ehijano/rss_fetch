<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 05:06:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Trie Measure, Revisited</title>
      <link>https://arxiv.org/abs/2504.10703</link>
      <description>arXiv:2504.10703v1 Announce Type: new 
Abstract: In this paper, we study the following problem: given $n$ subsets $S_1, \dots, S_n$ of an integer universe $U = \{0,\dots, u-1\}$, having total cardinality $N = \sum_{i=1}^n |S_i|$, find a prefix-free encoding $enc : U \rightarrow \{0,1\}^+$ minimizing the so-called trie measure, i.e., the total number of edges in the $n$ binary tries $\mathcal T_1, \dots, \mathcal T_n$, where $\mathcal T_i$ is the trie packing the encoded integers $\{enc(x):x\in S_i\}$. We first observe that this problem is equivalent to that of merging $u$ sets with the cheapest sequence of binary unions, a problem which in [Ghosh et al., ICDCS 2015] is shown to be NP-hard. Motivated by the hardness of the general problem, we focus on particular families of prefix-free encodings. We start by studying the fixed-length shifted encoding of [Gupta et al., Theoretical Computer Science 2007]. Given a parameter $0\le a &lt; u$, this encoding sends each $x \in U$ to $(x + a) \mod u$, interpreted as a bit-string of $\log u$ bits. We develop the first efficient algorithms that find the value of $a$ minimizing the trie measure when this encoding is used. Our two algorithms run in $O(u + N\log u)$ and $O(N\log^2 u)$ time, respectively. We proceed by studying ordered encodings (a.k.a. monotone or alphabetic), and describe an algorithm finding the optimal such encoding in $O(N+u^3)$ time. Within the same running time, we show how to compute the best shifted ordered encoding, provably no worse than both the optimal shifted and optimal ordered encodings. We provide implementations of our algorithms and discuss how these encodings perform in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10703v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jarno N. Alanko, Ruben Becker, Davide Cenzato, Travis Gagie, Sung-Hwan Kim, Bojana Kodric, Nicola Prezza</dc:creator>
    </item>
    <item>
      <title>Robust Gittins for Stochastic Scheduling</title>
      <link>https://arxiv.org/abs/2504.10743</link>
      <description>arXiv:2504.10743v1 Announce Type: new 
Abstract: A common theme in stochastic optimization problems is that, theoretically, stochastic algorithms need to "know" relatively rich information about the underlying distributions. This is at odds with most applications, where distributions are rough predictions based on historical data. Thus, commonly, stochastic algorithms are making decisions using imperfect predicted distributions, while trying to optimize over some unknown true distributions. We consider the fundamental problem of scheduling stochastic jobs preemptively on a single machine to minimize expected mean completion time in the setting where the scheduler is only given imperfect predicted job size distributions. If the predicted distributions are perfect, then it is known that this problem can be solved optimally by the Gittins index policy. The goal of our work is to design a scheduling policy that is robust in the sense that it produces nearly optimal schedules even if there are modest discrepancies between the predicted distributions and the underlying real distributions. Our main contributions are:
  (1) We show that the standard Gittins index policy is not robust in this sense. If the true distributions are perturbed by even an arbitrarily small amount, then running the Gittins index policy using the perturbed distributions can lead to an unbounded increase in mean completion time.
  (2) We explain how to modify the Gittins index policy to make it robust, that is, to produce nearly optimal schedules, where the approximation depends on a new measure of error between the true and predicted distributions that we define.
  Looking forward, the approach we develop here can be applied more broadly to many other stochastic optimization problems to better understand the impact of mispredictions, and lead to the development of new algorithms that are robust against such mispredictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10743v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Moseley, Heather Newman, Kirk Pruhs, Rudy Zhou</dc:creator>
    </item>
    <item>
      <title>An Improved Fully Dynamic Algorithm for Counting 4-Cycles in General Graphs using Fast Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2504.10748</link>
      <description>arXiv:2504.10748v1 Announce Type: new 
Abstract: We study subgraph counting over fully dynamic graphs, which undergo edge insertions and deletions. Counting subgraphs is a fundamental problem in graph theory with numerous applications across various fields, including database theory, social network analysis, and computational biology.
  Maintaining the number of triangles in fully dynamic graphs is very well studied and has an upper bound of O(m^{1/2}) for the update time by Kara, Ngo, Nikolic, Olteanu, and Zhang (TODS 20). There is also a conditional lower bound of approximately Omega(m^{1/2}) for the update time by Henzinger, Krinninger, Nanongkai, and Saranurak (STOC 15) under the OMv conjecture implying that O(m^{1/2}) is the ``right answer'' for the update time of counting triangles. More recently, Hanauer, Henzinger, and Hua (SAND 22) studied the problem of maintaining the number of 4-cycles in fully dynamic graphs and designed an algorithm with O(m^{2/3}) update time which is a natural generalization of the approach for counting triangles. Thus, it seems natural that O(m^{2/3}) might be the correct answer for the complexity of the update time for counting 4-cycles.
  In this work, we present an improved algorithm for maintaining the number of 4-cycles in fully dynamic graphs. Our algorithm achieves a worst-case update time of O(m^{2/3-eps}) for some constant eps&gt;0. Our approach crucially uses fast matrix multiplication and leverages recent developments therein to get an improved runtime. Using the current best value of the matrix multiplication exponent omega=2.371339 we get eps=0.009811 and if we assume the best possible exponent i.e. omega=2 then we get eps=1/24. The lower bound for the update time is Omega(m^{1/2}), so there is still a big gap between the best-known upper and lower bounds. The key message of our paper is demonstrating that O(m^{2/3}) is not the correct answer for the complexity of the update time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10748v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Vihan Shah</dc:creator>
    </item>
    <item>
      <title>Tighter Bounds on Non-clairvoyant Parallel Machine Scheduling with Prediction to Minimize Makespan</title>
      <link>https://arxiv.org/abs/2504.10945</link>
      <description>arXiv:2504.10945v1 Announce Type: new 
Abstract: This paper investigates the non-clairvoyant parallel machine scheduling problem with prediction, with the objective of minimizing the makespan. Improved lower bounds for the problem and competitive ratios of online algorithms with respect to the prediction error are presented for both the non-preemptive and preemptive cases on m identical machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10945v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianqi Chen, Zhiyi Tan</dc:creator>
    </item>
    <item>
      <title>BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs</title>
      <link>https://arxiv.org/abs/2504.10948</link>
      <description>arXiv:2504.10948v1 Announce Type: new 
Abstract: Subgraph counting the task of determining the number of instances of a query pattern within a large graph lies at the heart of many critical applications, from analyzing financial networks and transportation systems to understanding biological interactions. Despite decades of work yielding efficient algorithmic (AL) solutions and, more recently, machine learning (ML) approaches, a clear comparative understanding is elusive. This gap stems from the absence of a unified evaluation framework, standardized datasets, and accessible ground truths, all of which hinder systematic analysis and fair benchmarking. To overcome these barriers, we introduce BEACON: a comprehensive benchmark designed to rigorously evaluate both AL and ML-based subgraph counting methods. BEACON provides a standardized dataset with verified ground truths, an integrated evaluation environment, and a public leaderboard, enabling reproducible and transparent comparisons across diverse approaches. Our extensive experiments reveal that while AL methods excel in efficiently counting subgraphs on very large graphs, they struggle with complex patterns (e.g., those exceeding six nodes). In contrast, ML methods are capable of handling larger patterns but demand massive graph data inputs and often yield suboptimal accuracy on small, dense graphs. These insights not only highlight the unique strengths and limitations of each approach but also pave the way for future advancements in subgraph counting techniques. Overall, BEACON represents a significant step towards unifying and accelerating research in subgraph counting, encouraging innovative solutions and fostering a deeper understanding of the trade-offs between algorithmic and machine learning paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10948v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.SI</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Matin Najafi, Xianju Zhu, Chrysanthi Kosyfaki, Laks V. S. Lakshmanan, Reynold Cheng</dc:creator>
    </item>
    <item>
      <title>Covering Approximate Shortest Paths with DAGs</title>
      <link>https://arxiv.org/abs/2504.11256</link>
      <description>arXiv:2504.11256v1 Announce Type: new 
Abstract: We define and study analogs of probabilistic tree embedding and tree cover for directed graphs. We define the notion of a DAG cover of a general directed graph $G$: a small collection $D_1,\dots D_g$ of DAGs so that for all pairs of vertices $s,t$, some DAG $D_i$ provides low distortion for $dist(s,t)$; i.e. $ dist_G(s, t) \le \min_{i \in [g]} dist_{D_i}(s, t) \leq \alpha \cdot dist_G(s, t)$, where $\alpha$ is the distortion.
  As a trivial upper bound, there is a DAG cover with $n$ DAGs and $\alpha=1$ by taking the shortest-paths tree from each vertex. When each DAG is restricted to be a subgraph of $G$, there is a matching lower bound (via a directed cycle) that $n$ DAGs are necessary, even to preserve reachability. Thus, we allow the DAGs to include a limited number of additional edges not in the original graph.
  When $n^2$ additional edges are allowed, there is a simple upper bound of two DAGs and $\alpha=1$. Our first result is an almost-matching lower bound that even for $n^{2-o(1)}$ additional edges, at least $n^{1-o(1)}$ DAGs are needed, even to preserve reachability. However, the story is different when the number of additional edges is $\tilde{O}(m)$, a natural setting where the sparsity of the DAG collection nearly matches the original graph. Our main upper bound is that there is a near-linear time algorithm to construct a DAG cover with $\tilde{O}(m)$ additional edges, polylogarithmic distortion, and only $O(\log n)$ DAGs. This is similar to known results for undirected graphs: the well-known FRT probabilistic tree embedding implies a tree cover where both the number of trees and the distortion are logarithmic. Our algorithm also extends to a certain probabilistic embedding guarantee. Lastly, we complement our upper bound with a lower bound showing that achieving a DAG cover with no distortion and $\tilde{O}(m)$ additional edges requires a polynomial number of DAGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11256v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Gary Hoppenworth, Nicole Wein</dc:creator>
    </item>
    <item>
      <title>Breaking a Long-Standing Barrier: 2-$\varepsilon$ Approximation for Steiner Forest</title>
      <link>https://arxiv.org/abs/2504.11398</link>
      <description>arXiv:2504.11398v1 Announce Type: new 
Abstract: The Steiner Forest problem, also known as the Generalized Steiner Tree problem, is a fundamental optimization problem on edge-weighted graphs where, given a set of vertex pairs, the goal is to select a minimum-cost subgraph such that each pair is connected. This problem generalizes the Steiner Tree problem, first introduced in 1811, for which the best known approximation factor is 1.39 [Byrka, Grandoni, Rothvo{\ss}, and Sanit\`a, 2010] (Best Paper award, STOC 2010).
  The celebrated work of [Agrawal, Klein, and Ravi, 1989] (30-Year Test-of-Time award, STOC 2023), along with refinements by [Goemans and Williamson, 1992] (SICOMP'95), established a 2-approximation for Steiner Forest over 35 years ago. Jain's (FOCS'98) pioneering iterative rounding techniques later extended these results to higher connectivity settings. Despite the long-standing importance of this problem, breaking the approximation factor of 2 has remained a major challenge, raising suspicions that achieving a better factor -- similar to Vertex Cover -- might indeed be hard. Notably, fundamental works, including those by Gupta and Kumar (STOC'15) and Gro{\ss} et al. (ITCS'18), introduced 96- and 69-approximation algorithms, possibly with the hope of paving the way for a breakthrough in achieving a constant-factor approximation below 2 for the Steiner Forest problem.
  In this paper, we break the approximation barrier of 2 by designing a novel deterministic algorithm that achieves a $2 - 10^{-11}$ approximation for this fundamental problem. As a key component of our approach, we also introduce a novel dual-based local search algorithm for the Steiner Tree problem with an approximation guarantee of $1.943$, which is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11398v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Ahmadi, Iman Gholami, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, Mohammad Mahdavi</dc:creator>
    </item>
    <item>
      <title>Optimal Hardness of Online Algorithms for Large Independent Sets</title>
      <link>https://arxiv.org/abs/2504.11450</link>
      <description>arXiv:2504.11450v1 Announce Type: new 
Abstract: We study the algorithmic problem of finding a large independent set in an Erd\"{o}s-R\'{e}nyi random graph $\mathbb{G}(n,p)$. For constant $p$ and $b=1/(1-p)$, the largest independent set has size $2\log_b n$, while a simple greedy algorithm revealing vertices sequentially and making decisions based only on previously seen vertices finds an independent set of size $\log_b n$. In his seminal 1976 paper, Karp challenged to either improve this guarantee or establish its hardness. Decades later, this problem remains open, one of the most prominent algorithmic problems in the theory of random graphs.
  In this paper, we establish that a broad class of online algorithms fails to find an independent set of size $(1+\epsilon)\log_b n$ any constant $\epsilon&gt;0$ w.h.p. This class includes Karp's algorithm as a special case, and extends it by allowing the algorithm to query exceptional edges not yet 'seen' by the algorithm. Our lower bound holds for all $p\in [d/n,1-n^{-1/d}]$, where $d$ is a large constant. In the dense regime (constant $p$), we further prove that our result is asymptotically tight with respect to the number of exceptional edges queried, by designing an online algorithm which beats the half-optimality threshold when the number of exceptional edges slightly exceeds our bound.
  Our result provides evidence for the algorithmic hardness of Karp's problem by supporting the conjectured optimality of the aforementioned greedy algorithm and establishing it within the class of online algorithms. Our proof relies on a refined analysis of the geometric structure of tuples of large independent sets, establishing a variant of the Overlap Gap Property (OGP) commonly used as a barrier for classes of algorithms. While OGP has predominantly served as a barrier to stable algorithms, online algorithms are not stable and our application of OGP-based techniques to online setting is novel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11450v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Gamarnik, Eren C. K{\i}z{\i}lda\u{g}, Lutz Warnke</dc:creator>
    </item>
    <item>
      <title>Balanced TSP partitioning</title>
      <link>https://arxiv.org/abs/2504.10657</link>
      <description>arXiv:2504.10657v1 Announce Type: cross 
Abstract: The traveling salesman problem (TSP) famously asks for a shortest tour that a salesperson can take to visit a given set of cities in any order. In this paper, we ask how much faster $k \ge 2$ salespeople can visit the cities if they divide the task among themselves. We show that, in the two-dimensional Euclidean setting, two salespeople can always achieve a speedup of at least $\frac12 + \frac1\pi \approx 0.818$, for any given input, and there are inputs where they cannot do better. We also give (non-matching) upper and lower bounds for $k \geq 3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10657v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Aram Berendsohn, Hwi Kim, L\'aszl\'o Kozma</dc:creator>
    </item>
    <item>
      <title>Improved approximation algorithms for the EPR Hamiltonian</title>
      <link>https://arxiv.org/abs/2504.10712</link>
      <description>arXiv:2504.10712v1 Announce Type: cross 
Abstract: The EPR Hamiltonian is a family of 2-local quantum Hamiltonians introduced by King (arXiv:2209.02589). We introduce a polynomial time $\frac{1+\sqrt{5}}{4}\approx 0.809$-approximation algorithm for the problem of computing the ground energy of the EPR Hamiltonian, improving upon the previous state of the art of $0.72$ (arXiv:2410.15544). As a special case, this also implies a $\frac{1+\sqrt{5}}{4}$-approximation for Quantum Max Cut on bipartite instances, improving upon the approximation ratio of $3/4$ that one can infer in a relatively straightforward manner from the work of Lee and Parekh (arXiv:2401.03616).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10712v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Ju, Ansh Nagda</dc:creator>
    </item>
    <item>
      <title>Mildly-Interacting Fermionic Unitaries are Efficiently Learnable</title>
      <link>https://arxiv.org/abs/2504.11318</link>
      <description>arXiv:2504.11318v1 Announce Type: cross 
Abstract: Recent work has shown that one can efficiently learn fermionic Gaussian unitaries, also commonly known as nearest-neighbor matchcircuits or non-interacting fermionic unitaries. However, one could ask a similar question about unitaries that are near Gaussian: for example, unitaries prepared with a small number of non-Gaussian circuit elements. These operators find significance in quantum chemistry and many-body physics, yet no algorithm exists to learn them.
  We give the first such result by devising an algorithm which makes queries to a $n$-mode fermionic unitary $U$ prepared by at most $O(t)$ non-Gaussian gates and returns a circuit approximating $U$ to diamond distance $\varepsilon$ in time $\textrm{poly}(n,2^t,1/\varepsilon)$. This resolves a central open question of Mele and Herasymenko under the strongest distance metric. In fact, our algorithm is much more general: we define a property of unitary Gaussianity known as unitary Gaussian dimension and show that our algorithm can learn $n$-mode unitaries of Gaussian dimension at least $2n - O(t)$ in time $\textrm{poly}(n,2^t,1/\varepsilon)$. Indeed, this class subsumes unitaries prepared by at most $O(t)$ non-Gaussian gates but also includes several unitaries that require up to $2^{O(t)}$ non-Gaussian gates to construct.
  In addition, we give a $\textrm{poly}(n,1/\varepsilon)$-time algorithm to distinguish whether an $n$-mode unitary is of Gaussian dimension at least $k$ or $\varepsilon$-far from all such unitaries in Frobenius distance, promised that one is the case. Along the way, we prove structural results about near-Gaussian fermionic unitaries that are likely to be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11318v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishnu Iyer</dc:creator>
    </item>
    <item>
      <title>Clustering with Non-adaptive Subset Queries</title>
      <link>https://arxiv.org/abs/2409.10908</link>
      <description>arXiv:2409.10908v2 Announce Type: replace 
Abstract: Recovering the underlying clustering of a set $U$ of $n$ points by asking pair-wise same-cluster queries has garnered significant interest in the last decade. Given a query $S \subset U$, $|S|=2$, the oracle returns yes if the points are in the same cluster and no otherwise. For adaptive algorithms with pair-wise queries, the number of required queries is known to be $\Theta(nk)$, where $k$ is the number of clusters. However, non-adaptive schemes require $\Omega(n^2)$ queries, which matches the trivial $O(n^2)$ upper bound attained by querying every pair of points.
  To break the quadratic barrier for non-adaptive queries, we study a generalization of this problem to subset queries for $|S|&gt;2$, where the oracle returns the number of clusters intersecting $S$. Allowing for subset queries of unbounded size, $O(n)$ queries is possible with an adaptive scheme (Chakrabarty-Liao, 2024). However, the realm of non-adaptive algorithms is completely unknown.
  In this paper, we give the first non-adaptive algorithms for clustering with subset queries. Our main result is a non-adaptive algorithm making $O(n \log k \cdot (\log k + \log\log n)^2)$ queries, which improves to $O(n \log \log n)$ when $k$ is a constant. We also consider algorithms with a restricted query size of at most $s$. In this setting we prove that $\Omega(\max(n^2/s^2,n))$ queries are necessary and obtain algorithms making $\tilde{O}(n^2k/s^2)$ queries for any $s \leq \sqrt{n}$ and $\tilde{O}(n^2/s)$ queries for any $s \leq n$. We also consider the natural special case when the clusters are balanced, obtaining non-adaptive algorithms which make $O(n \log k) + \tilde{O}(k)$ and $O(n\log^2 k)$ queries. Finally, allowing two rounds of adaptivity, we give an algorithm making $O(n \log k)$ queries in the general case and $O(n \log \log k)$ queries when the clusters are balanced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10908v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadley Black, Euiwoong Lee, Arya Mazumdar, Barna Saha</dc:creator>
    </item>
    <item>
      <title>Multicut Problems in Embedded Graphs: The Dependency of Complexity on the Demand Pattern</title>
      <link>https://arxiv.org/abs/2312.11086</link>
      <description>arXiv:2312.11086v2 Announce Type: replace-cross 
Abstract: The Multicut problem asks for a minimum cut separating certain pairs of vertices: formally, given a graph $G$ and demand graph $H$ on a set $T\subseteq V(G)$ of terminals, the task is to find a minimum-weight set $C$ of edges of $G$ such that whenever two vertices of $T$ are adjacent in $H$, they are in different components of $G\setminus C$. Colin de Verdi\`{e}re [Algorithmica, 2017] showed that Multicut with $t$ terminals on a graph $G$ of genus $g$ can be solved in time $f(t,g)n^{O(\sqrt{g^2+gt+t})}$. Cohen-Addad et al. [JACM, 2021] proved a matching lower bound showing that the exponent of $n$ is essentially best possible (for every fixed value of $t$ and $g$), even in the special case of Multiway Cut, where the demand graph $H$ is a complete graph.
  However, this lower bound tells us nothing about other special cases of Multicut such as Group 3-Terminal Cut (where three groups of terminals need to be separated from each other). We show that if the demand pattern is, in some sense, close to being a complete bipartite graph, then Multicut can be solved faster than $f(t,g)n^{O(\sqrt{g^2+gt+t})}$, and furthermore this is the only property that allows such an improvement. Formally, for a class $\mathcal{H}$ of graphs, Multicut$(\mathcal{H})$ is the special case where the demand graph $H$ is in $\mathcal{H}$. For every fixed class $\mathcal{H}$ (satisfying some mild closure property), fixed $g$, and fixed $t$, our main result gives tight upper and lower bounds on the exponent of $n$ in algorithms solving Multicut$(\mathcal{H})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11086v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Focke, Florian H\"orsch, Shaohua Li, D\'aniel Marx</dc:creator>
    </item>
    <item>
      <title>Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations</title>
      <link>https://arxiv.org/abs/2411.02942</link>
      <description>arXiv:2411.02942v2 Announce Type: replace-cross 
Abstract: We study the problem of assigning items to agents so as to maximize the \emph{weighted} Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an $O(nw_{\max})$-approximation due to Garg, Husic, Li, Vega, and Vondrak~\cite{GHL23}, where $w_{\max}$ is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.
  We give the first such algorithm for the problem, thus solving the open problem in the affirmative. Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li~\cite{FL24} for the additive valuation case. Our rounding algorithm is similar to that of Li \cite{Li25} developed for the unrelated machine scheduling problem to minimize weighted completion time. Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items. So, every agent gets precisely 1 fractional large item in the configuration LP solution. With the rounding algorithm in \cite{Li25}, we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02942v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuda Feng, Yang Hu, Shi Li, Ruilong Zhang</dc:creator>
    </item>
    <item>
      <title>OPMOS: Ordered Parallel Algorithm for Multi-Objective Shortest-Paths</title>
      <link>https://arxiv.org/abs/2411.16667</link>
      <description>arXiv:2411.16667v2 Announce Type: replace-cross 
Abstract: The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. The literature explores multi-objective A*-style algorithmic approaches to solving the NP-hard MOS problem. These approaches use consistent heuristics to compute an exact set of solutions for the goal node. A generalized MOS algorithm maintains a "frontier" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node. The algorithm becomes computationally intractable at a higher number of objectives due to a rapid increase in the search space for non-dominated paths and the significant increase in Pareto-optimal solutions. While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism to accelerate the MOS problem. The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency. The proposed parallel algorithm (OPMOS) unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS. Experimental evaluation using the NVIDIA GH200 Superchip's 72-core Arm-based CPU shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16667v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3721145.3725781</arxiv:DOI>
      <dc:creator>Leo Gold, Adam Bienkowski, David Sidoti, Krishna Pattipati, Omer Khan</dc:creator>
    </item>
    <item>
      <title>Diversity-Fair Online Selection</title>
      <link>https://arxiv.org/abs/2504.10389</link>
      <description>arXiv:2504.10389v2 Announce Type: replace-cross 
Abstract: Online selection problems frequently arise in applications such as crowdsourcing and employee recruitment. Existing research typically focuses on candidates with a single attribute. However, crowdsourcing tasks often require contributions from individuals across various demographics. Further motivated by the dynamic nature of crowdsourcing and hiring, we study the diversity-fair online selection problem, in which a recruiter must make real-time decisions to foster workforce diversity across many dimensions. We propose two scenarios for this problem. The fixed-capacity scenario, suited for short-term hiring for crowdsourced workers, provides the recruiter with a fixed capacity to fill temporary job vacancies. In contrast, in the unknown-capacity scenario, recruiters optimize diversity across recruitment seasons with increasing capacities, reflecting that the firm honors diversity consideration in a long-term employee acquisition strategy. By modeling the diversity over $d$ dimensions as a max-min fairness objective, we show that no policy can surpass a competitive ratio of $O(1/d^{1/3})$ for either scenario, indicating that any achievable result inevitably decays by some polynomial factor in $d$. To this end, we develop bilevel hierarchical randomized policies that ensure compliance with the capacity constraint. For the fixed-capacity scenario, leveraging marginal information about the arriving population allows us to achieve a competitive ratio of $1/(4\sqrt{d} \lceil \log_2 d \rceil)$. For the unknown-capacity scenario, we establish a competitive ratio of $\Omega(1/d^{3/4})$ under mild boundedness conditions. In both bilevel hierarchical policies, the higher level determines ex-ante selection probabilities and then informs the lower level's randomized selection that ensures no loss in efficiency. Both policies prioritize core diversity and then adjust for underrepresented dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10389v2</guid>
      <category>econ.TH</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Hu, Yanzhi Li, Tongwen Wu</dc:creator>
    </item>
  </channel>
</rss>
