<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Aug 2024 04:04:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Treatment Allocation Strategies for A/B Testing in Partially Observable Time Series Experiments</title>
      <link>https://arxiv.org/abs/2408.05342</link>
      <description>arXiv:2408.05342v1 Announce Type: new 
Abstract: Time series experiments, in which experimental units receive a sequence of treatments over time, are frequently employed in many technological companies to evaluate the performance of a newly developed policy, product, or treatment relative to a baseline control. Many existing A/B testing solutions assume a fully observable experimental environment that satisfies the Markov condition, which often does not hold in practice. This paper studies the optimal design for A/B testing in partially observable environments. We introduce a controlled (vector) autoregressive moving average model to capture partial observability. We introduce a small signal asymptotic framework to simplify the analysis of asymptotic mean squared errors of average treatment effect estimators under various designs. We develop two algorithms to estimate the optimal design: one utilizing constrained optimization and the other employing reinforcement learning. We demonstrate the superior performance of our designs using a dispatch simulator and two real datasets from a ride-sharing company.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05342v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Sun, Linglong Kong, Hongtu Zhu, Chengchun Shi</dc:creator>
    </item>
    <item>
      <title>Change-Point Detection in Time Series Using Mixed Integer Programming</title>
      <link>https://arxiv.org/abs/2408.05665</link>
      <description>arXiv:2408.05665v1 Announce Type: new 
Abstract: We use cutting-edge mixed integer optimization (MIO) methods to develop a framework for detection and estimation of structural breaks in time series regression models. The framework is constructed based on the least squares problem subject to a penalty on the number of breakpoints. We restate the $l_0$-penalized regression problem as a quadratic programming problem with integer- and real-valued arguments and show that MIO is capable of finding provably optimal solutions using a well-known optimization solver. Compared to the popular $l_1$-penalized regression (LASSO) and other classical methods, the MIO framework permits simultaneous estimation of the number and location of structural breaks as well as regression coefficients, while accommodating the option of specifying a given or minimal number of breaks. We derive the asymptotic properties of the estimator and demonstrate its effectiveness through extensive numerical experiments, confirming a more accurate estimation of multiple breaks as compared to popular non-MIO alternatives. Two empirical examples demonstrate usefulness of the framework in applications from business and economic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05665v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artem Prokhorov, Peter Radchenko, Alexander Semenov, Anton Skrobotov</dc:creator>
    </item>
    <item>
      <title>Bank Cost Efficiency and Credit Market Structure Under a Volatile Exchange Rate</title>
      <link>https://arxiv.org/abs/2408.05688</link>
      <description>arXiv:2408.05688v1 Announce Type: new 
Abstract: We study the impact of exchange rate volatility on cost efficiency and market structure in a cross-section of banks that have non-trivial exposures to foreign currency (FX) operations. We use unique data on quarterly revaluations of FX assets and liabilities (Revals) that Russian banks were reporting between 2004 Q1 and 2020 Q2. {\it First}, we document that Revals constitute the largest part of the banks' total costs, 26.5\% on average, with considerable variation across banks. {\it Second}, we find that stochastic estimates of cost efficiency are both severely downward biased -- by 30\% on average -- and generally not rank preserving when Revals are ignored, except for the tails, as our nonparametric copulas reveal. To ensure generalizability to other emerging market economies, we suggest a two-stage approach that does not rely on Revals but is able to shrink the downward bias in cost efficiency estimates by two-thirds. {\it Third}, we show that Revals are triggered by the mismatch in the banks' FX operations, which, in turn, is driven by household FX deposits and the instability of Ruble's exchange rate. {\it Fourth}, we find that the failure to account for Revals leads to the erroneous conclusion that the credit market is inefficient, which is driven by the upper quartile of the banks' distribution by total assets. Revals have considerable negative implications for financial stability which can be attenuated by the cross-border diversification of bank assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05688v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jbankfin.2024.107285</arxiv:DOI>
      <dc:creator>Mikhail Mamonov, Christopher Parmeter, Artem Prokhorov</dc:creator>
    </item>
    <item>
      <title>Correcting invalid regression discontinuity designs with multiple time period data</title>
      <link>https://arxiv.org/abs/2408.05847</link>
      <description>arXiv:2408.05847v1 Announce Type: new 
Abstract: A common approach to Regression Discontinuity (RD) designs relies on a continuity assumption of the mean potential outcomes at the cutoff defining the RD design. In practice, this assumption is often implausible when changes other than the intervention of interest occur at the cutoff (e.g., other policies are implemented at the same cutoff). When the continuity assumption is implausible, researchers often retreat to ad-hoc analyses that are not supported by any theory and yield results with unclear causal interpretation. These analyses seek to exploit additional data where either all units are treated or all units are untreated (regardless of their running variable value). For example, when data from multiple time periods are available. We first derive the bias of RD designs when the continuity assumption does not hold. We then present a theoretical foundation for analyses using multiple time periods by the means of a general identification framework incorporating data from additional time periods to overcome the bias. We discuss this framework under various RD designs, and also extend our work to carry-over effects and time-varying running variables. We develop local linear regression estimators, bias correction procedures, and standard errors that are robust to bias-correction for the multiple period setup. The approach is illustrated using an application that studied the effect of new fiscal laws on debt of Italian municipalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05847v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dor Leventer, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>Method-of-Moments Inference for GLMs and Doubly Robust Functionals under Proportional Asymptotics</title>
      <link>https://arxiv.org/abs/2408.06103</link>
      <description>arXiv:2408.06103v1 Announce Type: cross 
Abstract: In this paper, we consider the estimation of regression coefficients and signal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models (GLMs), and explore their implications in inferring popular estimands such as average treatment effects in high-dimensional observational studies. Under the ``proportional asymptotic'' regime and Gaussian covariates with known (population) covariance $\Sigma$, we derive Consistent and Asymptotically Normal (CAN) estimators of our targets of inference through a Method-of-Moments type of estimators that bypasses estimation of high dimensional nuisance functions and hyperparameter tuning altogether. Additionally, under non-Gaussian covariates, we demonstrate universality of our results under certain additional assumptions on the regression coefficients and $\Sigma$. We also demonstrate that knowing $\Sigma$ is not essential to our proposed methodology when the sample covariance matrix estimator is invertible. Finally, we complement our theoretical results with numerical experiments and comparisons with existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06103v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Chen, Lin Liu, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Fused LASSO as Non-Crossing Quantile Regression</title>
      <link>https://arxiv.org/abs/2403.14036</link>
      <description>arXiv:2403.14036v2 Announce Type: replace 
Abstract: Quantile crossing has been an ever-present thorn in the side of quantile regression. This has spurred research into obtaining densities and coefficients that obey the quantile monotonicity property. While important contributions, these papers do not provide insight into how exactly these constraints influence the estimated coefficients. This paper extends non-crossing constraints and shows that by varying a single hyperparameter ($\alpha$) one can obtain commonly used quantile estimators. Namely, we obtain the quantile regression estimator of Koenker and Bassett (1978) when $\alpha=0$, the non crossing quantile regression estimator of Bondell et al. (2010) when $\alpha=1$, and the composite quantile regression estimator of Koenker (1984) and Zou and Yuan (2008) when $\alpha\rightarrow\infty$. As such, we show that non-crossing constraints are simply a special type of fused-shrinkage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14036v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tibor Szendrei, Arnab Bhattacharjee, Mark E. Schaffer</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Causal Effects in High-Frequency Event Studies</title>
      <link>https://arxiv.org/abs/2406.15667</link>
      <description>arXiv:2406.15667v3 Announce Type: replace 
Abstract: We provide precise conditions for nonparametric identification of causal effects by high-frequency event study regressions, which have been used widely in the recent macroeconomics, financial economics and political economy literatures. The high-frequency event study method regresses changes in an outcome variable on a measure of unexpected changes in a policy variable in a narrow time window around an event or a policy announcement (e.g., a 30-minute window around an FOMC announcement). We show that, contrary to popular belief, the narrow size of the window is not sufficient for identification. Rather, the population regression coefficient identifies a causal estimand when (i) the effect of the policy shock on the outcome does not depend on the other shocks (separability) and (ii) the surprise component of the news or event dominates all other shocks that are present in the event window (relative exogeneity). Technically, the latter condition requires the policy shock to have infinite variance in the event window. Under these conditions, we establish the causal meaning of the event study estimand corresponding to the regression coefficient and the consistency and asymptotic normality of the event study estimator. Notably, this standard linear regression estimator is robust to general forms of nonlinearity. We apply our results to Nakamura and Steinsson's (2018a) analysis of the real economic effects of monetary policy, providing a simple empirical procedure to analyze the extent to which the standard event study estimator adequately estimates causal effects of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15667v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Alessandro Casini, Adam McCloskey</dc:creator>
    </item>
    <item>
      <title>Robust Identification in Randomized Experiments with Noncompliance</title>
      <link>https://arxiv.org/abs/2408.03530</link>
      <description>arXiv:2408.03530v2 Announce Type: replace 
Abstract: This paper considers a robust identification of causal parameters in a randomized experiment setting with noncompliance where the standard local average treatment effect assumptions could be violated. Following Li, K\'edagni, and Mourifi\'e (2024), we propose a misspecification robust bound for a real-valued vector of various causal parameters. We discuss identification under two sets of weaker assumptions: random assignment and exclusion restriction (without monotonicity), and random assignment and monotonicity (without exclusion restriction). We introduce two causal parameters: the local average treatment-controlled direct effect (LATCDE), and the local average instrument-controlled direct effect (LAICDE). Under the random assignment and monotonicity assumptions, we derive sharp bounds on the local average treatment-controlled direct effects for the always-takers and never-takers, respectively, and the total average controlled direct effect for the compliers. Additionally, we show that the intent-to-treat effect can be expressed as a convex weighted average of these three effects. Finally, we apply our method on the proximity to college instrument and find that growing up near a four-year college increases the wage of never-takers (who represent more than 70% of the population) by a range of 4.15% to 27.07%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03530v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Cui, D\'esir\'e K\'edagni, Huan Wu</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning in High-frequency Market Making</title>
      <link>https://arxiv.org/abs/2407.21025</link>
      <description>arXiv:2407.21025v2 Announce Type: replace-cross 
Abstract: This paper establishes a new and comprehensive theoretical analysis for the application of reinforcement learning (RL) in high-frequency market making. We bridge the modern RL theory and the continuous-time statistical models in high-frequency financial economics. Different with most existing literature on methodological research about developing various RL methods for market making problem, our work is a pilot to provide the theoretical analysis. We target the effects of sampling frequency, and find an interesting tradeoff between error and complexity of RL algorithm when tweaking the values of the time increment $\Delta$ $-$ as $\Delta$ becomes smaller, the error will be smaller but the complexity will be larger. We also study the two-player case under the general-sum game framework and establish the convergence of Nash equilibrium to the continuous-time game equilibrium as $\Delta\rightarrow0$. The Nash Q-learning algorithm, which is an online multi-agent RL method, is applied to solve the equilibrium. Our theories are not only useful for practitioners to choose the sampling frequency, but also very general and applicable to other high-frequency financial decision making problems, e.g., optimal executions, as long as the time-discretization of a continuous-time markov decision process is adopted. Monte Carlo simulation evidence support all of our theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21025v2</guid>
      <category>q-fin.TR</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Zheng, Zihan Ding</dc:creator>
    </item>
  </channel>
</rss>
