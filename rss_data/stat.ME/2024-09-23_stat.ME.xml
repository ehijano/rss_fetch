<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 04:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Supervised low-rank approximation of high-dimensional multivariate functional data via tensor decomposition</title>
      <link>https://arxiv.org/abs/2409.13819</link>
      <description>arXiv:2409.13819v1 Announce Type: new 
Abstract: Motivated by the challenges of analyzing high-dimensional ($p \gg n$) sequencing data from longitudinal microbiome studies, where samples are collected at multiple time points from each subject, we propose supervised functional tensor singular value decomposition (SupFTSVD), a novel dimensionality reduction method that leverages auxiliary information in the dimensionality reduction of high-dimensional functional tensors. Although multivariate functional principal component analysis is a natural choice for dimensionality reduction of multivariate functional data, it becomes computationally burdensome in high-dimensional settings. Low-rank tensor decomposition is a feasible alternative and has gained popularity in recent literature, but existing methods in this realm are often incapable of simultaneously utilizing the temporal structure of the data and subject-level auxiliary information. SupFTSVD overcomes these limitations by generating low-rank representations of high-dimensional functional tensors while incorporating subject-level auxiliary information and accounting for the functional nature of the data. Moreover, SupFTSVD produces low-dimensional representations of subjects, features, and time, as well as subject-specific trajectories, providing valuable insights into the biological significance of variations within the data. In simulation studies, we demonstrate that our method achieves notable improvement in tensor approximation accuracy and loading estimation by utilizing auxiliary information. Finally, we applied SupFTSVD to two longitudinal microbiome studies where biologically meaningful patterns in the data were revealed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13819v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Samsul Alam, Ana-Maria Staicu, Pixu Shi</dc:creator>
    </item>
    <item>
      <title>BRcal: An R Package to Boldness-Recalibrate Probability Predictions</title>
      <link>https://arxiv.org/abs/2409.13858</link>
      <description>arXiv:2409.13858v1 Announce Type: new 
Abstract: When probability predictions are too cautious for decision making, boldness-recalibration enables responsible emboldening while maintaining the probability of calibration required by the user. We introduce BRcal, an R package implementing boldness-recalibration and supporting methodology as recently proposed. The BRcal package provides direct control of the calibration-boldness tradeoff and visualizes how different calibration levels change individual predictions. We describe the implementation details in BRcal related to non-linear optimization of boldness with a non-linear inequality constraint on calibration. Package functionality is demonstrated via a real world case study involving housing foreclosure predictions. The BRcal package is available on the Comprehensive R Archive Network (CRAN) (https://cran.r-project.org/web/packages/BRcal/index.html) and on Github (https://github.com/apguthrie/BRcal).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13858v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeline P. Guthrie, Christopher T. Franck</dc:creator>
    </item>
    <item>
      <title>Jointly modeling time-to-event and longitudinal data with individual-specific change points: a case study in modeling tumor burden</title>
      <link>https://arxiv.org/abs/2409.13873</link>
      <description>arXiv:2409.13873v1 Announce Type: new 
Abstract: In oncology clinical trials, tumor burden (TB) stands as a crucial longitudinal biomarker, reflecting the toll a tumor takes on a patient's prognosis. With certain treatments, the disease's natural progression shows the tumor burden initially receding before rising once more. Biologically, the point of change may be different between individuals and must have occurred between the baseline measurement and progression time of the patient, implying a random effects model obeying a bound constraint. However, in practice, patients may drop out of the study due to progression or death, presenting a non-ignorable missing data problem. In this paper, we introduce a novel joint model that combines time-to-event data and longitudinal data, where the latter is parameterized by a random change point augmented by random pre-slope and post-slope dynamics. Importantly, the model is equipped to incorporate covariates across for the longitudinal and survival models, adding significant flexibility. Adopting a Bayesian approach, we propose an efficient Hamiltonian Monte Carlo algorithm for parameter inference. We demonstrate the superiority of our approach compared to a longitudinal-only model via simulations and apply our method to a data set in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13873v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan M. Alt, Yixiang Qu, Emily Damone, Jing-ou Liu, Chenguang Wang, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Chauhan Weighted Trajectory Analysis of combined efficacy and safety outcomes for risk-benefit analysis</title>
      <link>https://arxiv.org/abs/2409.13946</link>
      <description>arXiv:2409.13946v1 Announce Type: new 
Abstract: Analyzing and effectively communicating the efficacy and toxicity of treatment is the basis of risk benefit analysis (RBA). More efficient and objective tools are needed. We apply Chauhan Weighted Trajectory Analysis (CWTA) to perform RBA with superior objectivity, power, and clarity.
  We used CWTA to perform 1000-fold simulations of RCTs using ordinal endpoints for both treatment efficacy and toxicity. RCTs were simulated with 1:1 allocation at defined sample sizes and hazard ratios. We studied the simplest case of 3 levels each of toxicity and efficacy and the general case of the advanced cancer trial, with efficacy graded by five RECIST 1.1 health statuses and toxicity by the six-point CTCAE scale (6 x 5 matrix). The latter model was applied to a real-world dose escalation phase I trial in advanced cancer.
  Simulations in both the 3 x 3 and the 6 x 5 advanced cancer matrix confirmed that drugs with both superior efficacy and toxicity profiles synergize for greater statistical power with CWTA-RBA. The CWTA-RBA 6 x 5 matrix reduced sample size requirements over CWTA efficacy-only analysis. Application to the dose finding phase I clinical trial provided objective, statistically significant validation for the selected dose.
  CWTA-RBA, by incorporating both drug efficacy and toxicity, provides a single test statistic and plot that analyzes and effectively communicates therapeutic risks and benefits. CWTA-RBA requires fewer patients than CWTA efficacy-only analysis when the experimental drug is both more effective and less toxic. CWTA-RBA facilitates the objective and efficient assessment of new therapies throughout the drug development pathway. Furthermore, several advantages over competing tests in communicating risk-benefit will assist regulatory review, clinical adoption, and understanding of therapeutic risks and benefits by clinicians and patients alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13946v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Utkarsh Chauhan, Daylen Mackey, John R Mackey</dc:creator>
    </item>
    <item>
      <title>Functional Factor Modeling of Brain Connectivity</title>
      <link>https://arxiv.org/abs/2409.13963</link>
      <description>arXiv:2409.13963v1 Announce Type: new 
Abstract: Many fMRI analyses examine functional connectivity, or statistical dependencies among remote brain regions. Yet popular methods for studying whole-brain functional connectivity often yield results that are difficult to interpret. Factor analysis offers a natural framework in which to study such dependencies, particularly given its emphasis on interpretability. However, multivariate factor models break down when applied to functional and spatiotemporal data, like fMRI. We present a factor model for discretely-observed multidimensional functional data that is well-suited to the study of functional connectivity. Unlike classical factor models which decompose a multivariate observation into a "common" term that captures covariance between observed variables and an uncorrelated "idiosyncratic" term that captures variance unique to each observed variable, our model decomposes a functional observation into two uncorrelated components: a "global" term that captures long-range dependencies and a "local" term that captures short-range dependencies. We show that if the global covariance is smooth with finite rank and the local covariance is banded with potentially infinite rank, then this decomposition is identifiable. Under these conditions, recovery of the global covariance amounts to rank-constrained matrix completion, which we exploit to formulate consistent loading estimators. We study these estimators, and their more interpretable post-processed counterparts, through simulations, then use our approach to uncover a rich covariance structure in a collection of resting-state fMRI scans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13963v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyle Stanley, Nicole Lazar, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Batch Predictive Inference</title>
      <link>https://arxiv.org/abs/2409.13990</link>
      <description>arXiv:2409.13990v1 Announce Type: new 
Abstract: Constructing prediction sets with coverage guarantees for unobserved outcomes is a core problem in modern statistics. Methods for predictive inference have been developed for a wide range of settings, but usually only consider test data points one at a time. Here we study the problem of distribution-free predictive inference for a batch of multiple test points, aiming to construct prediction sets for functions -- such as the mean or median -- of any number of unobserved test datapoints. This setting includes constructing simultaneous prediction sets with a high probability of coverage, and selecting datapoints satisfying a specified condition while controlling the number of false claims.
  For the general task of predictive inference on a function of a batch of test points, we introduce a methodology called batch predictive inference (batch PI), and provide a distribution-free coverage guarantee under exchangeability of the calibration and test data. Batch PI requires the quantiles of a rank ordering function defined on certain subsets of ranks. While computing these quantiles is NP-hard in general, we show that it can be done efficiently in many cases of interest, most notably for batch score functions with a compositional structure -- which includes examples of interest such as the mean -- via a dynamic programming algorithm that we develop. Batch PI has advantages over naive approaches (such as partitioning the calibration data or directly extending conformal prediction) in many settings, as it can deliver informative prediction sets even using small calibration sample sizes. We illustrate that our procedures provide informative inference across the use cases mentioned above, through experiments on both simulated data and a drug-target interaction dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13990v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Refitted cross-validation estimation for high-dimensional subsamples from low-dimension full data</title>
      <link>https://arxiv.org/abs/2409.14032</link>
      <description>arXiv:2409.14032v1 Announce Type: new 
Abstract: The technique of subsampling has been extensively employed to address the challenges posed by limited computing resources and meet the needs for expedite data analysis. Various subsampling methods have been developed to meet the challenges characterized by a large sample size with a small number of parameters. However, direct applications of these subsampling methods may not be suitable when the dimension is also high and available computing facilities at hand are only able to analyze a subsample of size similar or even smaller than the dimension. In this case, although there is no high-dimensional problem in the full data, the subsample may have a sample size smaller or smaller than the number of parameters, making it a high-dimensional problem. We call this scenario the high-dimensional subsample from low-dimension full data problem. In this paper, we tackle this problem by proposing a novel subsampling-based approach that combines penalty-based dimension reduction and refitted cross-validation. The asymptotic normality of the refitted cross-validation subsample estimator is established, which plays a crucial role in statistical inference. The proposed method demonstrates appealing performance in numerical experiments on simulated data and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14032v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haixiang Zhang, HaiYing Wang</dc:creator>
    </item>
    <item>
      <title>Adaptive radar detection of subspace-based distributed target in power heterogeneous clutter</title>
      <link>https://arxiv.org/abs/2409.14049</link>
      <description>arXiv:2409.14049v1 Announce Type: new 
Abstract: This paper investigates the problem of adaptive detection of distributed targets in power heterogeneous clutter. In the considered scenario, all the data share the identical structure of clutter covariance matrix, but with varying and unknown power mismatches. To address this problem, we iteratively estimate all the unknowns, including the coordinate matrix of the target, the clutter covariance matrix, and the corresponding power mismatches, and propose three detectors based on the generalized likelihood ratio test (GLRT), Rao and the Wald tests. The results from simulated and real data both illustrate that the detectors based on GLRT and Rao test have higher probabilities of detection (PDs) than the existing competitors. Among them, the Rao test-based detector exhibits the best overall detection performance. We also analyze the impact of the target extended dimensions, the signal subspace dimensions, and the number of training samples on the detection performance. Furthermore, simulation experiments also demonstrate that the proposed detectors have a constant false alarm rate (CFAR) property for the structure of clutter covariance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14049v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daipeng Xiao, Weijian Liu, Jun Liu, Lingyan Dai, Xueli Fang, Jianjun Ge</dc:creator>
    </item>
    <item>
      <title>Skew-symmetric approximations of posterior distributions</title>
      <link>https://arxiv.org/abs/2409.14167</link>
      <description>arXiv:2409.14167v1 Announce Type: new 
Abstract: Routinely-implemented deterministic approximations of posterior distributions from, e.g., Laplace method, variational Bayes and expectation-propagation, generally rely on symmetric approximating densities, often taken to be Gaussian. This choice facilitates optimization and inference, but typically affects the quality of the overall approximation. In fact, even in basic parametric models, the posterior distribution often displays asymmetries that yield bias and reduced accuracy when considering symmetric approximations. Recent research has moved towards more flexible approximating densities that incorporate skewness. However, current solutions are model-specific, lack general supporting theory, increase the computational complexity of the optimization problem, and do not provide a broadly-applicable solution to include skewness in any symmetric approximation. This article addresses such a gap by introducing a general and provably-optimal strategy to perturb any off-the-shelf symmetric approximation of a generic posterior distribution. Crucially, this novel perturbation is derived without additional optimization steps, and yields a similarly-tractable approximation within the class of skew-symmetric densities that provably enhances the finite-sample accuracy of the original symmetric approximation, and, under suitable assumptions, improves its convergence rate to the exact posterior by at least a $\sqrt{n}$ factor, in asymptotic regimes. These advancements are illustrated in numerical studies focusing on skewed perturbations of state-of-the-art Gaussian approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14167v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pozza, Daniele Durante, Botond Szabo</dc:creator>
    </item>
    <item>
      <title>On the asymptotic distributions of some test statistics for two-way contingency tables</title>
      <link>https://arxiv.org/abs/2409.14255</link>
      <description>arXiv:2409.14255v1 Announce Type: new 
Abstract: Pearson's Chi-square test is a widely used tool for analyzing categorical data, yet its statistical power has remained theoretically underexplored. Due to the difficulties in obtaining its power function in the usual manner, Cochran (1952) suggested the derivation of its Pitman limiting power, which is later implemented by Mitra (1958) and Meng &amp; Chapman (1966). Nonetheless, this approach is suboptimal for practical power calculations under fixed alternatives. In this work, we solve this long-standing problem by establishing the asymptotic normality of the Chi-square statistic under fixed alternatives and deriving an explicit formula for its variance. For finite samples, we suggest a second-order expansion based on the multivariate delta method to improve the approximations. As a further contribution, we obtain the power functions of two distance covariance tests. We apply our findings to study the statistical power of these tests under different simulation settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14255v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Zhang</dc:creator>
    </item>
    <item>
      <title>POI-SIMEX for Conditionally Poisson Distributed Biomarkers from Tissue Histology</title>
      <link>https://arxiv.org/abs/2409.14256</link>
      <description>arXiv:2409.14256v1 Announce Type: new 
Abstract: Covariate measurement error in regression analysis is an important issue that has been studied extensively under the classical additive and the Berkson error models. Here, we consider cases where covariates are derived from tumor tissue histology, and in particular tissue microarrays. In such settings, biomarkers are evaluated from tissue cores that are subsampled from a larger tissue section so that these biomarkers are only estimates of the true cell densities. The resulting measurement error is non-negligible but is seldom accounted for in the analysis of cancer studies involving tissue microarrays. To adjust for this type of measurement error, we assume that these discrete-valued biomarkers are conditionally Poisson distributed, based on a Poisson process model governing the spatial locations of marker-positive cells. Existing methods for addressing conditional Poisson surrogates, particularly in the absence of internal validation data, are limited. We extend the simulation extrapolation (SIMEX) algorithm to accommodate the conditional Poisson case (POI-SIMEX), where measurement errors are non-Gaussian with heteroscedastic variance. The proposed estimator is shown to be strongly consistent in a linear regression model under the assumption of a conditional Poisson distribution for the observed biomarker. Simulation studies evaluate the performance of POI-SIMEX, comparing it with the naive method and an alternative corrected likelihood approach in linear regression and survival analysis contexts. POI-SIMEX is then applied to a study of high-grade serous cancer, examining the association between survival and the presence of triple-positive biomarker (CD3+CD8+FOXP3+ cells)</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14256v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aijun Yang, Phineas T. Hamilton, Brad H. Nelson, Julian J. Lum, Mary Lesperance, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Potential root mean square error skill score</title>
      <link>https://arxiv.org/abs/2409.14263</link>
      <description>arXiv:2409.14263v1 Announce Type: new 
Abstract: Consistency, in a narrow sense, denotes the alignment between the forecast-optimization strategy and the verification directive. The current recommended deterministic solar forecast verification practice is to report the skill score based on root mean square error (RMSE), which would violate the notion of consistency if the forecasts are optimized under another strategy such as minimizing the mean absolute error (MAE). This paper overcomes such difficulty by proposing a so-called "potential RMSE skill score," which depends only on: (1) the crosscorrelation between forecasts and observations, and (2) the autocorrelation of observations. While greatly simplifying the calculation, the new skill score does not discriminate inconsistent forecasts as much, e.g., even MAE-optimized forecasts can attain a high RMSE skill score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14263v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0187044</arxiv:DOI>
      <arxiv:journal_reference>Journal of Renewable Sustainable Energy 16, 016501 (2024)</arxiv:journal_reference>
      <dc:creator>Martin J\'anos Mayer, Dazhi Yang</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Tensor Classification with CP Low-Rank Discriminant Structure</title>
      <link>https://arxiv.org/abs/2409.14397</link>
      <description>arXiv:2409.14397v1 Announce Type: new 
Abstract: Tensor classification has become increasingly crucial in statistics and machine learning, with applications spanning neuroimaging, computer vision, and recommendation systems. However, the high dimensionality of tensors presents significant challenges in both theory and practice. To address these challenges, we introduce a novel data-driven classification framework based on linear discriminant analysis (LDA) that exploits the CP low-rank structure in the discriminant tensor. Our approach includes an advanced iterative projection algorithm for tensor LDA and incorporates a novel initialization scheme called Randomized Composite PCA (\textsc{rc-PCA}). \textsc{rc-PCA}, potentially of independent interest beyond tensor classification, relaxes the incoherence and eigen-ratio assumptions of existing algorithms and provides a warm start close to the global optimum. We establish global convergence guarantees for the tensor estimation algorithm using \textsc{rc-PCA} and develop new perturbation analyses for noise with cross-correlation, extending beyond the traditional i.i.d. assumption. This theoretical advancement has potential applications across various fields dealing with correlated data and allows us to derive statistical upper bounds on tensor estimation errors. Additionally, we confirm the rate-optimality of our classifier by establishing minimax optimal misclassification rates across a wide class of parameter spaces. Extensive simulations and real-world applications validate our method's superior performance.
  Keywords: Tensor classification; Linear discriminant analysis; Tensor iterative projection; CP low-rank; High-dimensional data; Minimax optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14397v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elynn Chen, Yuefeng Han, Jiayu Li</dc:creator>
    </item>
    <item>
      <title>Scalable Expectation Propagation for Mixed-Effects Regression</title>
      <link>https://arxiv.org/abs/2409.14646</link>
      <description>arXiv:2409.14646v1 Announce Type: new 
Abstract: Mixed-effects regression models represent a useful subclass of regression models for grouped data; the introduction of random effects allows for the correlation between observations within each group to be conveniently captured when inferring the fixed effects. At a time where such regression models are being fit to increasingly large datasets with many groups, it is ideal if (a) the time it takes to make the inferences scales linearly with the number of groups and (b) the inference workload can be distributed across multiple computational nodes in a numerically stable way, if the dataset cannot be stored in one location. Current Bayesian inference approaches for mixed-effects regression models do not seem to account for both challenges simultaneously. To address this, we develop an expectation propagation (EP) framework in this setting that is both scalable and numerically stable when distributed for the case where there is only one grouping factor. The main technical innovations lie in the sparse reparameterisation of the EP algorithm, and a moment propagation (MP) based refinement for multivariate random effect factor approximations. Experiments are conducted to show that this EP framework achieves linear scaling, while having comparable accuracy to other scalable approximate Bayesian inference (ABI) approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14646v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackson Zhou, John Ormerod, Clara Grazian</dc:creator>
    </item>
    <item>
      <title>Consistent Order Determination of Markov Decision Process</title>
      <link>https://arxiv.org/abs/2409.14684</link>
      <description>arXiv:2409.14684v1 Announce Type: new 
Abstract: The Markov assumption in Markov Decision Processes (MDPs) is fundamental in reinforcement learning, influencing both theoretical research and practical applications. Existing methods that rely on the Bellman equation benefit tremendously from this assumption for policy evaluation and inference. Testing the Markov assumption or selecting the appropriate order is important for further analysis. Existing tests primarily utilize sequential hypothesis testing methodology, increasing the tested order if the previously-tested one is rejected. However, This methodology cumulates type-I and type-II errors in sequential testing procedures that cause inconsistent order estimation, even with large sample sizes. To tackle this challenge, we develop a procedure that consistently distinguishes the true order from others. We first propose a novel estimator that equivalently represents any order Markov assumption. Based on this estimator, we thus construct a signal function and an associated signal statistic to achieve estimation consistency. Additionally, the curve pattern of the signal statistic facilitates easy visualization, assisting the order determination process in practice. Numerical studies validate the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14684v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuyun Ye, Lixing Zhu, Ruoqing Zhu</dc:creator>
    </item>
    <item>
      <title>Analysis of Stepped-Wedge Cluster Randomized Trials when treatment effect varies by exposure time or calendar time</title>
      <link>https://arxiv.org/abs/2409.14706</link>
      <description>arXiv:2409.14706v1 Announce Type: new 
Abstract: Stepped-wedge cluster randomized trials (SW-CRTs) are traditionally analyzed with models that assume an immediate and sustained treatment effect. Previous work has shown that making such an assumption in the analysis of SW-CRTs when the true underlying treatment effect varies by exposure time can produce severely misleading estimates. Alternatively, the true underlying treatment effect might vary by calendar time. Comparatively less work has examined treatment effect structure misspecification in this setting. Here, we evaluate the behavior of the mixed effects model-based immediate treatment effect, exposure time-averaged treatment effect, and calendar time-averaged treatment effect estimators in different scenarios where they are misspecified for the true underlying treatment effect structure. We prove that the immediate treatment effect estimator can be relatively robust to bias when estimating a true underlying calendar time-averaged treatment effect estimand. However, when there is a true underlying calendar (exposure) time-varying treatment effect, misspecifying an analysis with an exposure (calendar) time-averaged treatment effect estimator can yield severely misleading estimates and even converge to a value of the opposite sign of the true calendar (exposure) time-averaged treatment effect estimand. Researchers should carefully consider how the treatment effect may vary as a function of exposure time and/or calendar time in the analysis of SW-CRTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14706v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth M. Lee, Elizabeth L. Turner, Avi Kenny</dc:creator>
    </item>
    <item>
      <title>Clinical research and methodology What usage and what hierarchical order for secondary endpoints?</title>
      <link>https://arxiv.org/abs/2409.14770</link>
      <description>arXiv:2409.14770v1 Announce Type: new 
Abstract: In a randomised clinical trial, when the result of the primary endpoint shows a significant benefit, the secondary endpoints are scrutinised to identify additional effects of the treatment. However, this approach entails a risk of concluding that there is a benefit for one of these endpoints when such benefit does not exist (inflation of type I error risk). There are mainly two methods used to control the risk of drawing erroneous conclusions for secondary endpoints. The first method consists of distributing the risk over several co-primary endpoints, so as to maintain an overall risk of 5%. The second is the hierarchical test procedure, which consists of first establishing a hierarchy of the endpoints, then evaluating each endpoint in succession according to this hierarchy while the endpoints continue to show statistical significance. This simple method makes it possible to show the additional advantages of treatments and to identify the factors that differentiate them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14770v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.therap.2015.11.001</arxiv:DOI>
      <arxiv:journal_reference>Therapies, 2016, 71 (1), pp.27-34</arxiv:journal_reference>
      <dc:creator>Silvy Laporte, Marine Divin\'e, Dani\`ele Girault, Pierre Boutouyrie, Olivier Chassany, Michel Cucherat, Herv\'e de Trogoff, Sophie Dubois, Cecile Fouret, Natalie Hoog-Labouret, Pascale Jolliet, Patrick Mismetti, Rapha\"el Porcher, C\'ecile Rey-Coquais, Eric Vicaut</dc:creator>
    </item>
    <item>
      <title>Rescaled Bayes factors: a class of e-variables</title>
      <link>https://arxiv.org/abs/2409.14806</link>
      <description>arXiv:2409.14806v1 Announce Type: new 
Abstract: A class of e-variables is introduced and analyzed. Some examples are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14806v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thorsten Dickhaus</dc:creator>
    </item>
    <item>
      <title>Early and Late Buzzards: Comparing Different Approaches for Quantile-based Multiple Testing in Heavy-Tailed Wildlife Research Data</title>
      <link>https://arxiv.org/abs/2409.14926</link>
      <description>arXiv:2409.14926v1 Announce Type: new 
Abstract: In medical, ecological and psychological research, there is a need for methods to handle multiple testing, for example to consider group comparisons with more than two groups. Typical approaches that deal with multiple testing are mean or variance based which can be less effective in the context of heavy-tailed and skewed data. Here, the median is the preferred measure of location and the interquartile range (IQR) is an adequate alternative to the variance. Therefore, it may be fruitful to formulate research questions of interest in terms of the median or the IQR. For this reason, we compare different inference approaches for two-sided and non-inferiority hypotheses formulated in terms of medians or IQRs in an extensive simulation study. We consider multiple contrast testing procedures combined with a bootstrap method as well as testing procedures with Bonferroni correction. As an example of a multiple testing problem based on heavy-tailed data we analyse an ecological trait variation in early and late breeding in a medium-sized bird of prey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14926v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marl\'ene Baumeister, Merle Munko, Kai-Philipp Gladow, Marc Ditzhaus, Nayden Chakarov, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>Risk Estimate under a Nonstationary Autoregressive Model for Data-Driven Reproduction Number Estimation</title>
      <link>https://arxiv.org/abs/2409.14937</link>
      <description>arXiv:2409.14937v1 Announce Type: new 
Abstract: COVID-19 pandemic has brought to the fore epidemiological models which, though describing a rich variety of behaviors, have previously received little attention in the signal processing literature. During the pandemic, several works successfully leveraged state-of-the-art signal processing strategies to robustly infer epidemiological indicators despite the low quality of COVID-19 data. In the present work, a novel nonstationary autoregressive model is introduced, encompassing, but not reducing to, one of the most popular models for the propagation of viral epidemics. Using a variational framework, penalized likelihood estimators of the parameters of this new model are designed. In practice, the main bottleneck is that the estimation accuracy strongly depends on hyperparameters tuning. Without available ground truth, hyperparameters are selected by minimizing specifically designed data-driven oracles, used as proxy for the estimation error. Focusing on the nonstationary autoregressive Poisson model, the Stein's Unbiased Risk Estimate formalism is generalized to construct asymptotically unbiased risk estimators based on the derivation of an original autoregressive counterpart of Stein's lemma. The accuracy of these oracles and of the resulting estimates are assessed through intensive Monte Carlo simulations on synthetic data. Then, elaborating on recent epidemiological models, a novel weekly scaled Poisson model is proposed, enabling to better account for intrinsic variability of the contamination while being robust to reporting errors. Finally, the overall data-driven procedure is particularized to the estimation of COVID-19 reproduction number and exemplified on real COVID-19 infection counts in different countries and at different stages of the pandemic, demonstrating its ability to yield consistent estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14937v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barbara Pascal, Samuel Vaiter</dc:creator>
    </item>
    <item>
      <title>Adaptive weight selection for time-to-event data under non-proportional hazards</title>
      <link>https://arxiv.org/abs/2409.15145</link>
      <description>arXiv:2409.15145v1 Announce Type: new 
Abstract: When planning a clinical trial for a time-to-event endpoint, we require an estimated effect size and need to consider the type of effect. Usually, an effect of proportional hazards is assumed with the hazard ratio as the corresponding effect measure. Thus, the standard procedure for survival data is generally based on a single-stage log-rank test. Knowing that the assumption of proportional hazards is often violated and sufficient knowledge to derive reasonable effect sizes is usually unavailable, such an approach is relatively rigid. We introduce a more flexible procedure by combining two methods designed to be more robust in case we have little to no prior knowledge. First, we employ a more flexible adaptive multi-stage design instead of a single-stage design. Second, we apply combination-type tests in the first stage of our suggested procedure to benefit from their robustness under uncertainty about the deviation pattern. We can then use the data collected during this period to choose a more specific single-weighted log-rank test for the subsequent stages. In this step, we employ Royston-Parmar spline models to extrapolate the survival curves to make a reasonable decision. Based on a real-world data example, we show that our approach can save a trial that would otherwise end with an inconclusive result. Additionally, our simulation studies demonstrate a sufficient power performance while maintaining more flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15145v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Fabian Danzer, Ina Dormuth</dc:creator>
    </item>
    <item>
      <title>Grid Point Approximation for Distributed Nonparametric Smoothing and Prediction</title>
      <link>https://arxiv.org/abs/2409.14079</link>
      <description>arXiv:2409.14079v1 Announce Type: cross 
Abstract: Kernel smoothing is a widely used nonparametric method in modern statistical analysis. The problem of efficiently conducting kernel smoothing for a massive dataset on a distributed system is a problem of great importance. In this work, we find that the popularly used one-shot type estimator is highly inefficient for prediction purposes. To this end, we propose a novel grid point approximation (GPA) method, which has the following advantages. First, the resulting GPA estimator is as statistically efficient as the global estimator under mild conditions. Second, it requires no communication and is extremely efficient in terms of computation for prediction. Third, it is applicable to the case where the data are not randomly distributed across different machines. To select a suitable bandwidth, two novel bandwidth selectors are further developed and theoretically supported. Extensive numerical studies are conducted to corroborate our theoretical findings. Two real data examples are also provided to demonstrate the usefulness of our GPA method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14079v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Gao, Rui Pan, Feng Li, Riquan Zhang, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Mining Causality: AI-Assisted Search for Instrumental Variables</title>
      <link>https://arxiv.org/abs/2409.14202</link>
      <description>arXiv:2409.14202v1 Announce Type: cross 
Abstract: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity (especially exclusion restrictions) is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can accelerate this process exponentially and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We argue that multi-step prompting is useful and role-playing prompts are suitable for mimicking the endogenous decisions of economic agents. We apply our method to three well-known examples in economics: returns to schooling, production functions, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14202v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v1 Announce Type: cross 
Abstract: Integration of probabilistic and non-probabilistic samples for the estimation of finite population totals (or means) has recently received considerable attention in the field of survey sampling; yet, to the best of our knowledge, this framework has not been extended to cumulative distribution function (CDF) estimation. To address this gap, we propose a novel CDF estimator that integrates data from probability samples with data from (potentially large) nonprobability samples. Assuming that a set of shared covariates are observed in both samples, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some regularity conditions, we show that our CDF estimator is both design-consistent for the finite population CDF and asymptotically normally distributed. Additionally, we define and study a quantile estimator based on the proposed CDF estimator. Furthermore, we use both the bootstrap and asymptotic formulae to estimate their respective sampling variances. Our empirical results show that the proposed CDF estimator is robust to model misspecification under ignorability, and robust to ignorability under model misspecification. When both assumptions are violated, our residual-based CDF estimator still outperforms its 'plug-in' mass imputation and naive siblings, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood (North Carolina A&amp;T State University), Sayed Mostafa (North Carolina A&amp;T State University)</dc:creator>
    </item>
    <item>
      <title>Optimal sequencing depth for single-cell RNA-sequencing in Wasserstein space</title>
      <link>https://arxiv.org/abs/2409.14326</link>
      <description>arXiv:2409.14326v1 Announce Type: cross 
Abstract: How many samples should one collect for an empirical distribution to be as close as possible to the true population? This question is not trivial in the context of single-cell RNA-sequencing. With limited sequencing depth, profiling more cells comes at the cost of fewer reads per cell. Therefore, one must strike a balance between the number of cells sampled and the accuracy of each measured gene expression profile. In this paper, we analyze an empirical distribution of cells and obtain upper and lower bounds on the Wasserstein distance to the true population. Our analysis holds for general, non-parametric distributions of cells, and is validated by simulation experiments on a real single-cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14326v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Sharvaj Kubal, Geoffrey Schiebinger</dc:creator>
    </item>
    <item>
      <title>Testing Causal Models with Hidden Variables in Polynomial Delay via Conditional Independencies</title>
      <link>https://arxiv.org/abs/2409.14593</link>
      <description>arXiv:2409.14593v1 Announce Type: cross 
Abstract: Testing a hypothesized causal model against observational data is a key prerequisite for many causal inference tasks. A natural approach is to test whether the conditional independence relations (CIs) assumed in the model hold in the data. While a model can assume exponentially many CIs (with respect to the number of variables), testing all of them is both impractical and unnecessary. Causal graphs, which encode these CIs in polynomial space, give rise to local Markov properties that enable model testing with a significantly smaller subset of CIs. Model testing based on local properties requires an algorithm to list the relevant CIs. However, existing algorithms for realistic settings with hidden variables and non-parametric distributions can take exponential time to produce even a single CI constraint. In this paper, we introduce the c-component local Markov property (C-LMP) for causal graphs with hidden variables. Since C-LMP can still invoke an exponential number of CIs, we develop a polynomial delay algorithm to list these CIs in poly-time intervals. To our knowledge, this is the first algorithm that enables poly-delay testing of CIs in causal graphs with hidden variables against arbitrary data distributions. Experiments on real-world and synthetic data demonstrate the practicality of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14593v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunchai Jeong, Adiba Ejaz, Jin Tian, Elias Bareinboim</dc:creator>
    </item>
    <item>
      <title>Non-linear dependence and Granger causality: A vine copula approach</title>
      <link>https://arxiv.org/abs/2409.15070</link>
      <description>arXiv:2409.15070v1 Announce Type: cross 
Abstract: Inspired by Jang et al. (2022), we propose a Granger causality-in-the-mean test for bivariate $k-$Markov stationary processes based on a recently introduced class of non-linear models, i.e., vine copula models. By means of a simulation study, we show that the proposed test improves on the statistical properties of the original test in Jang et al. (2022), constituting an excellent tool for testing Granger causality in the presence of non-linear dependence structures. Finally, we apply our test to study the pairwise relationships between energy consumption, GDP and investment in the U.S. and, notably, we find that Granger-causality runs two ways between GDP and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15070v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto Fuentes M., Irene Crimaldi, Armando Rungi</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Knockoff Generators for Feature Selection Under Complex Data Structure</title>
      <link>https://arxiv.org/abs/2111.06985</link>
      <description>arXiv:2111.06985v2 Announce Type: replace 
Abstract: The recent proliferation of high-dimensional data, such as electronic health records and genetics data, offers new opportunities to find novel predictors of outcomes. Presented with a large set of candidate features, interest often lies in selecting the ones most likely to be predictive of an outcome for further study. Controlling the false discovery rate (FDR) at a specified level is often desired in evaluating these variables. Knockoff filtering is an innovative strategy for conducting FDR-controlled feature selection. This paper proposes a nonparametric Bayesian model for generating high-quality knockoff copies that can improve the accuracy of predictive feature identification for variables arising from complex distributions, which can be skewed, highly dispersed and/or a mixture of distributions. This paper provides a detailed description for generating knockoff copies from a GDPM model via MCMC posterior sampling. Additionally, we provide a theoretical guarantee on the robustness of the knockoff procedure. Through simulations, the method is shown to identify important features with accurate FDR control and improved power over the popular second-order Gaussian knockoff generator. Furthermore, the model is compared with finite Gaussian mixture knockoff generator in FDR and power. The proposed technique is applied for detecting genes predictive of survival in ovarian cancer patients using data from The Cancer Genome Atlas (TCGA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.06985v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael J. Martens, Anjishnu Banerjee, Xinran Qi, Yushu Shi</dc:creator>
    </item>
    <item>
      <title>Fighting Noise with Noise: Causal Inference with Many Candidate Instruments</title>
      <link>https://arxiv.org/abs/2203.09330</link>
      <description>arXiv:2203.09330v3 Announce Type: replace 
Abstract: Instrumental variable methods provide useful tools for inferring causal effects in the presence of unmeasured confounding. To apply these methods with large-scale data sets, a major challenge is to find valid instruments from a possibly large candidate set. In practice, most of the candidate instruments are often not relevant for studying a particular exposure of interest. Moreover, not all relevant candidate instruments are valid as they may directly influence the outcome of interest. In this article, we propose a data-driven method for causal inference with many candidate instruments that addresses these two challenges simultaneously. A key component of our proposal involves using pseudo variables, known to be irrelevant, to remove variables from the original set that exhibit spurious correlations with the exposure. Synthetic data analyses show that the proposed method performs favourably compared to existing methods. We apply our method to a Mendelian randomization study estimating the effect of obesity on health-related quality of life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09330v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyi Zhang, Linbo Wang, Stanislav Volgushev, Dehan Kong</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Continuous Multiple Time Point Interventions</title>
      <link>https://arxiv.org/abs/2305.06645</link>
      <description>arXiv:2305.06645v5 Announce Type: replace 
Abstract: There are limited options to estimate the treatment effects of variables which are continuous and measured at multiple time points, particularly if the true dose-response curve should be estimated as closely as possible. However, these situations may be of relevance: in pharmacology, one may be interested in how outcomes of people living with -- and treated for -- HIV, such as viral failure, would vary for time-varying interventions such as different drug concentration trajectories. A challenge for doing causal inference with continuous interventions is that the positivity assumption is typically violated. To address positivity violations, we develop projection functions, which reweigh and redefine the estimand of interest based on functions of the conditional support for the respective interventions. With these functions, we obtain the desired dose-response curve in areas of enough support, and otherwise a meaningful estimand that does not require the positivity assumption. We develop $g$-computation type plug-in estimators for this case. Those are contrasted with g-computation estimators which are applied to continuous interventions without specifically addressing positivity violations, which we propose to be presented with diagnostics. The ideas are illustrated with longitudinal data from HIV positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children $&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard g-computation approach is appropriate, and in which it leads to bias and how the proposed weighted estimation approach then recovers the alternative estimand of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06645v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Schomaker, Helen McIlleron, Paolo Denti, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Distributional Welfare</title>
      <link>https://arxiv.org/abs/2311.15878</link>
      <description>arXiv:2311.15878v3 Announce Type: replace 
Abstract: In this paper, we explore optimal treatment allocation policies that target distributional welfare. Most literature on treatment choice has considered utilitarian welfare based on the conditional average treatment effect (ATE). While average welfare is intuitive, it may yield undesirable allocations especially when individuals are heterogeneous (e.g., with outliers) - the very reason individualized treatments were introduced in the first place. This observation motivates us to propose an optimal policy that allocates the treatment based on the conditional quantile of individual treatment effects (QoTE). Depending on the choice of the quantile probability, this criterion can accommodate a policymaker who is either prudent or negligent. The challenge of identifying the QoTE lies in its requirement for knowledge of the joint distribution of the counterfactual outcomes, which is generally hard to recover even with experimental data. Therefore, we introduce minimax policies that are robust to model uncertainty. A range of identifying assumptions can be used to yield more informative policies. For both stochastic and deterministic policies, we establish the asymptotic bound on the regret of implementing the proposed policies. In simulations and two empirical applications, we compare optimal decisions based on the QoTE with decisions based on other criteria. The framework can be generalized to any setting where welfare is defined as a functional of the joint distribution of the potential outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15878v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Counterfactual Slopes and Their Applications in Social Stratification</title>
      <link>https://arxiv.org/abs/2401.07000</link>
      <description>arXiv:2401.07000v3 Announce Type: replace 
Abstract: This paper addresses two prominent theses in social stratification research, the great equalizer thesis and Mare's (1980) school transition thesis. Both theses describe the role of an intermediate educational transition in the association between socioeconomic status and an outcome variable. However, the descriptive regularities of the two theses may be driven by differential selection into the intermediate transition, which prevents the two theses from having substantive interpretations. We propose a set of novel counterfactual slope estimands, which capture these theses under hypothetical interventions that eliminate the differential selection. We thereby construct selection-free tests for these theses. Compared with the existing literature, we are the first to explicitly provide nonparametric causal estimands, which enable us to conduct more principled analysis. We are also the first to develop flexible, efficient, and robust estimators for the two theses based on efficient influence functions. We apply our framework to a nationally representative dataset in the United States and re-evaluate the two theses. Findings from our selection-free tests suggest that the descriptive regularities are misleading for the substantive interpretation of the great equalizer thesis, but not for that of the school transition thesis. Additionally, the counterfactual slopes also provide a new framework for evaluating the inequality impacts of policy interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07000v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Flexible Models for Simple Longitudinal Data</title>
      <link>https://arxiv.org/abs/2401.11827</link>
      <description>arXiv:2401.11827v2 Announce Type: replace 
Abstract: We propose a new method for modelling simple longitudinal data. We aim to do this in a flexible manner (without restrictive assumptions about the shapes of individual trajectories), while exploiting structural similarities between the trajectories. Hierarchical models (such as linear mixed models, generalised additive mixed models and hierarchical generalised additive models) are commonly used to model longitudinal data, but fail to meet one or other of these requirements: either they make restrictive assumptions about the shape of individual trajectories, or fail to exploit structural similarities between trajectories. Functional principal components analysis promises to fulfil both requirements, and methods for functional principal components analysis have been developed for longitudinal data. However, we find that existing methods sometimes give poor-quality estimates of individual trajectories, particularly when the number of observations on each individual is small. We develop a new approach, which we call hierarchical modelling with functional principal components. Inference is conducted based on the full likelihood of all unknown quantities, with a penalty term to control the balance between fit to the data and smoothness of the trajectories. We run simulation studies to demonstrate that the new method substantially improves the quality of inference relative to existing methods across a range of examples, and apply the method to data on changes in body composition in adolescent girls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11827v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen Ogden</dc:creator>
    </item>
    <item>
      <title>Rapid Bayesian identification of sparse nonlinear dynamics from scarce and noisy data</title>
      <link>https://arxiv.org/abs/2402.15357</link>
      <description>arXiv:2402.15357v2 Announce Type: replace 
Abstract: We propose a fast probabilistic framework for identifying differential equations governing the dynamics of observed data. We recast the SINDy method within a Bayesian framework and use Gaussian approximations for the prior and likelihood to speed up computation. The resulting method, Bayesian-SINDy, not only quantifies uncertainty in the parameters estimated but also is more robust when learning the correct model from limited and noisy data. Using both synthetic and real-life examples such as Lynx-Hare population dynamics, we demonstrate the effectiveness of the new framework in learning correct model equations and compare its computational and data efficiency with existing methods. Because Bayesian-SINDy can quickly assimilate data and is robust against noise, it is particularly suitable for biological data and real-time system identification in control. Its probabilistic framework also enables the calculation of information entropy, laying the foundation for an active learning strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15357v2</guid>
      <category>stat.ME</category>
      <category>nlin.CD</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lloyd Fung, Urban Fasel, Matthew P. Juniper</dc:creator>
    </item>
    <item>
      <title>Data-adaptive structural change-point detection via isolation</title>
      <link>https://arxiv.org/abs/2404.19344</link>
      <description>arXiv:2404.19344v2 Announce Type: replace 
Abstract: In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. The novelty of the proposed algorithm comes from the data-adaptive nature of the methodology. At each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity. The methodology can be applied to both univariate and multivariate signals. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors and in many cases significantly less computationally expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19344v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Anastasiou, Sophia Loizidou</dc:creator>
    </item>
    <item>
      <title>An Autoregressive Model for Time Series of Random Objects</title>
      <link>https://arxiv.org/abs/2405.03778</link>
      <description>arXiv:2405.03778v2 Announce Type: replace 
Abstract: Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. The absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr\'echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic for the hypothesis of absence of serial correlation and establish its asymptotic normality. Finally, we use a permutation-based procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03778v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthieu Bult\'e, Helle S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Graphical tools for detection and control of selection bias with multiple exposures and samples</title>
      <link>https://arxiv.org/abs/2407.20027</link>
      <description>arXiv:2407.20027v2 Announce Type: replace 
Abstract: Among recent developments in definitions and analysis of selection bias is the potential outcomes approach of Kenah (Epidemiology, 2023), which allows non-parametric analysis using single-world intervention graphs, linking selection of study participants to identification of causal effects. Mohan &amp; Pearl (JASA, 2021) provide a framework for missing data via directed acyclic graphs augmented with nodes indicating missingness for each sometimes-missing variable, which allows for analysis of more general missing data problems but cannot easily encode scenarios in which different groups of variables are observed in specific subsamples. We give an alternative formulation of the potential outcomes framework based on conditional separable effects and indicators for selection into subsamples. This is practical for problems between the single-sample scenarios considered by Kenah and the variable-wise missingness considered by Mohan &amp; Pearl. This simplifies identification conditions and admits generalizations to scenarios with multiple, potentially nested or overlapping study samples, as well as multiple or time-dependent exposures. We give examples of identifiability arguments for case-cohort studies, multiple or time-dependent exposures, and direct effects of selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20027v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick M. Schnell, Eben Kenah</dc:creator>
    </item>
    <item>
      <title>Interpretability Indices and Soft Constraints for Factor Models</title>
      <link>https://arxiv.org/abs/2409.11525</link>
      <description>arXiv:2409.11525v3 Announce Type: replace 
Abstract: Factor analysis is a way to characterize the relationships between many (observable) variables in terms of a smaller number of unobservable random variables which are called factors. However, the application of factor models and its success can be subjective or difficult to gauge, since infinitely many factor models that produce the same correlation matrix can be fit given sample data. Thus, there is a need to operationalize a criterion that measures how meaningful or "interpretable" a factor model is in order to select the best among many factor models. While there are already techniques that aim to measure and enhance interpretability, new indices, as well as rotation methods via mathematical optimization based on them, are proposed to measure interpretability. The proposed methods directly incorporate semantics with the help of natural language processing and are generalized to incorporate any "prior information". Moreover, the indices allow for complete or partial specification of relationships at a pairwise level. Aside from these, two other main benefits of the proposed methods are that they do not require the estimation of factor scores, which avoids the factor score indeterminacy problem, and that no additional explanatory variables are necessary. The implementation of the proposed methods is written in Python 3 and is made available together with several helper functions through the package interpretablefa on the Python Package Index. The methods' application is demonstrated here using data on the Experiences in Close Relationships Scale, obtained from the Open-Source Psychometrics Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11525v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Philip Tuazon, Gia Mizrane Abubo, Joemari Olea</dc:creator>
    </item>
    <item>
      <title>Yurinskii's Coupling for Martingales</title>
      <link>https://arxiv.org/abs/2210.00362</link>
      <description>arXiv:2210.00362v3 Announce Type: replace-cross 
Abstract: Yurinskii's coupling is a popular theoretical tool for non-asymptotic distributional analysis in mathematical statistics and applied probability, offering a Gaussian strong approximation with an explicit error bound under easily verifiable conditions. Originally stated in $\ell^2$-norm for sums of independent random vectors, it has recently been extended both to the $\ell^p$-norm, for $1 \leq p \leq \infty$, and to vector-valued martingales in $\ell^2$-norm, under some strong conditions. We present as our main result a Yurinskii coupling for approximate martingales in $\ell^p$-norm, under substantially weaker conditions than those previously imposed. Our formulation further allows for the coupling variable to follow a more general Gaussian mixture distribution, and we provide a novel third-order coupling method which gives tighter approximations in certain settings. We specialize our main result to mixingales, martingales, and independent data, and derive uniform Gaussian mixture strong approximations for martingale empirical processes. Applications to nonparametric partitioning-based and local polynomial regression procedures are provided, alongside central limit theorems for high-dimensional martingale vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00362v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ricardo P. Masini, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Spectrum-Aware Debiasing: A Modern Inference Framework with Applications to Principal Components Regression</title>
      <link>https://arxiv.org/abs/2309.07810</link>
      <description>arXiv:2309.07810v4 Announce Type: replace-cross 
Abstract: Debiasing is a fundamental concept in high-dimensional statistics. While degrees-of-freedom adjustment is the state-of-the-art technique in high-dimensional linear regression, it is limited to i.i.d. samples and sub-Gaussian covariates. These constraints hinder its broader practical use. Here, we introduce Spectrum-Aware Debiasing--a novel method for high-dimensional regression. Our approach applies to problems with structured dependencies, heavy tails, and low-rank structures. Our method achieves debiasing through a rescaled gradient descent step, deriving the rescaling factor using spectral information of the sample covariance matrix. The spectrum-based approach enables accurate debiasing in much broader contexts. We study the common modern regime where the number of features and samples scale proportionally. We establish asymptotic normality of our proposed estimator (suitably centered and scaled) under various convergence notions when the covariates are right-rotationally invariant. Such designs have garnered recent attention due to their crucial role in compressed sensing. Furthermore, we devise a consistent estimator for its asymptotic variance.
  Our work has two notable by-products: first, we use Spectrum-Aware Debiasing to correct bias in principal components regression (PCR), providing the first debiased PCR estimator in high dimensions. Second, we introduce a principled test for checking alignment between the signal and the eigenvectors of the sample covariance matrix. This test is independently valuable for statistical methods developed using approximate message passing, leave-one-out, or convex Gaussian min-max theorems. We demonstrate our method through simulated and real data experiments. Technically, we connect approximate message passing algorithms with debiasing and provide the first proof of the Cauchy property of vector approximate message passing (V-AMP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07810v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>ALAAMEE: Open-source software for fitting autologistic actor attribute models</title>
      <link>https://arxiv.org/abs/2404.03116</link>
      <description>arXiv:2404.03116v2 Announce Type: replace-cross 
Abstract: The autologistic actor attribute model (ALAAM) is a model for social influence, derived from the more widely known exponential-family random graph model (ERGM). ALAAMs can be used to estimate parameters corresponding to multiple forms of social contagion associated with network structure and actor covariates. This work introduces ALAAMEE, open-source Python software for estimation, simulation, and goodness-of-fit testing for ALAAM models. ALAAMEE implements both the stochastic approximation and equilibrium expectation (EE) algorithms for ALAAM parameter estimation, including estimation from snowball sampled network data. It implements data structures and statistics for undirected, directed, and bipartite networks. We use a simulation study to assess the accuracy of the EE algorithm for ALAAM parameter estimation and statistical inference, and demonstrate the use of ALAAMEE with empirical examples using both small (fewer than 100 nodes) and large (more than 10 000 nodes) networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03116v2</guid>
      <category>stat.CO</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stivala, Peng Wang, Alessandro Lomi</dc:creator>
    </item>
    <item>
      <title>Length Optimization in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2406.18814</link>
      <description>arXiv:2406.18814v2 Announce Type: replace-cross 
Abstract: Conditional validity and length efficiency are two crucial aspects of conformal prediction (CP). Achieving conditional validity ensures accurate uncertainty quantification for data subpopulations, while proper length efficiency ensures that the prediction sets remain informative and non-trivial. Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature. In this paper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel framework that constructs prediction sets with (near-) optimal length while ensuring conditional validity under various classes of covariate shifts, including the key cases of marginal and group-conditional coverage. In the infinite sample regime, we provide strong duality results which indicate that CPL achieves conditional validity and length optimality. In the finite sample regime, we show that CPL constructs conditionally valid prediction sets. Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and large language model-based multiple choice question answering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18814v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayan Kiyani, George Pappas, Hamed Hassani</dc:creator>
    </item>
    <item>
      <title>Low dimensional representation of multi-patient flow cytometry datasets using optimal transport for minimal residual disease detection in leukemia</title>
      <link>https://arxiv.org/abs/2407.17329</link>
      <description>arXiv:2407.17329v2 Announce Type: replace-cross 
Abstract: Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid Leukemia (AML), a type of cancer that affects the blood and bone marrow, is essential in the prognosis and follow-up of AML patients. As traditional cytological analysis cannot detect leukemia cells below 5\%, the analysis of flow cytometry dataset is expected to provide more reliable results. In this paper, we explore statistical learning methods based on optimal transport (OT) to achieve a relevant low-dimensional representation of multi-patient flow cytometry measurements (FCM) datasets considered as high-dimensional probability distributions. Using the framework of OT, we justify the use of the K-means algorithm for dimensionality reduction of multiple large-scale point clouds through mean measure quantization by merging all the data into a single point cloud. After this quantization step, the visualization of the intra and inter-patients FCM variability is carried out by embedding low-dimensional quantized probability measures into a linear space using either Wasserstein Principal Component Analysis (PCA) through linearized OT or log-ratio PCA of compositional data. Using a publicly available FCM dataset and a FCM dataset from Bordeaux University Hospital, we demonstrate the benefits of our approach over the popular kernel mean embedding technique for statistical learning from multiple high-dimensional probability distributions. We also highlight the usefulness of our methodology for low-dimensional projection and clustering patient measurements according to their level of MRD in AML from FCM. In particular, our OT-based approach allows a relevant and informative two-dimensional representation of the results of the FlowSom algorithm, a state-of-the-art method for the detection of MRD in AML using multi-patient FCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17329v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erell Gachon, J\'er\'emie Bigot, Elsa Cazelles, Audrey Bidet, Jean-Philippe Vial, Pierre-Yves Dumas, Aguirre Mimoun</dc:creator>
    </item>
  </channel>
</rss>
