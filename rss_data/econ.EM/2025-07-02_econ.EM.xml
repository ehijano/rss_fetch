<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jul 2025 01:39:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Extrapolation in Regression Discontinuity Design Using Comonotonicity</title>
      <link>https://arxiv.org/abs/2507.00289</link>
      <description>arXiv:2507.00289v1 Announce Type: new 
Abstract: We present a novel approach for extrapolating causal effects away from the margin between treatment and non-treatment in sharp regression discontinuity designs with multiple covariates. Our methods apply both to settings in which treatment is a function of multiple observables and settings in which treatment is determined based on a single running variable. Our key identifying assumption is that conditional average treated and untreated potential outcomes are comonotonic: covariate values associated with higher average untreated potential outcomes are also associated with higher average treated potential outcomes. We provide an estimation method based on local linear regression. Our estimands are weighted average causal effects, even if comonotonicity fails. We apply our methods to evaluate counterfactual mandatory summer school policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00289v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Deaner, Soonwoo Kwon</dc:creator>
    </item>
    <item>
      <title>Robust Inference when Nuisance Parameters may be Partially Identified with Applications to Synthetic Controls</title>
      <link>https://arxiv.org/abs/2507.00307</link>
      <description>arXiv:2507.00307v2 Announce Type: new 
Abstract: When conducting inference for the average treatment effect on the treated with a Synthetic Control Estimator, the vector of control weights is a nuisance parameter which is often constrained, high-dimensional, and may be only partially identified even when the average treatment effect on the treated is point-identified. All three of these features of a nuisance parameter can lead to failure of asymptotic normality for the estimate of the parameter of interest when using standard methods. I provide a new method yielding asymptotic normality for an estimate of the parameter of interest, even when all three of these complications are present. This is accomplished by first estimating the nuisance parameter using a regularization penalty to achieve a form of identification, and then estimating the parameter of interest using moment conditions that have been orthogonalized with respect to the nuisance parameter. I present high-level sufficient conditions for the estimator and verify these conditions in an example involving Synthetic Controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00307v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Fry</dc:creator>
    </item>
    <item>
      <title>Plausible GMM: A Quasi-Bayesian Approach</title>
      <link>https://arxiv.org/abs/2507.00555</link>
      <description>arXiv:2507.00555v1 Announce Type: new 
Abstract: Structural estimation in economics often makes use of models formulated in terms of moment conditions. While these moment conditions are generally well-motivated, it is often unknown whether the moment restrictions hold exactly. We consider a framework where researchers model their belief about the potential degree of misspecification via a prior distribution and adopt a quasi-Bayesian approach for performing inference on structural parameters. We provide quasi-posterior concentration results, verify that quasi-posteriors can be used to obtain approximately optimal Bayesian decision rules under the maintained prior structure over misspecification, and provide a form of frequentist coverage results. We illustrate the approach through empirical examples where we obtain informative inference for structural objects allowing for substantial relaxations of the requirement that moment conditions hold exactly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00555v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Christian B. Hansen, Lingwei Kong, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Comparing Misspecified Models with Big Data: A Variational Bayesian Perspective</title>
      <link>https://arxiv.org/abs/2507.00763</link>
      <description>arXiv:2507.00763v1 Announce Type: new 
Abstract: Optimal data detection in massive multiple-input multiple-output (MIMO) systems often requires prohibitively high computational complexity. A variety of detection algorithms have been proposed in the literature, offering different trade-offs between complexity and detection performance. In recent years, Variational Bayes (VB) has emerged as a widely used method for addressing statistical inference in the context of massive data. This study focuses on misspecified models and examines the risk functions associated with predictive distributions derived from variational posterior distributions. These risk functions, defined as the expectation of the Kullback-Leibler (KL) divergence between the true data-generating density and the variational predictive distributions, provide a framework for assessing predictive performance. We propose two novel information criteria for predictive model comparison based on these risk functions. Under certain regularity conditions, we demonstrate that the proposed information criteria are asymptotically unbiased estimators of their respective risk functions. Through comprehensive numerical simulations and empirical applications in economics and finance, we demonstrate the effectiveness of these information criteria in comparing misspecified models in the context of massive data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00763v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Li, Sushanta K. Mallick, Tao Zeng, Junxing Zhang</dc:creator>
    </item>
    <item>
      <title>Randomization Inference with Sample Attrition</title>
      <link>https://arxiv.org/abs/2507.00795</link>
      <description>arXiv:2507.00795v1 Announce Type: new 
Abstract: Although appealing, randomization inference for treatment effects can suffer from severe size distortion due to sample attrition. We propose new, computationally efficient methods for randomization inference that remain valid under a range of potentially informative missingness mechanisms. We begin by constructing valid p-values for testing sharp null hypotheses, using the worst-case p-value from the Fisher randomization test over all possible imputations of missing outcomes. Leveraging distribution-free test statistics, this worst-case p-value admits a closed-form solution, connecting naturally to bounds in the partial identification literature. Our test statistics incorporate both potential outcomes and missingness indicators, allowing us to exploit structural assumptions-such as monotone missingness-for increased power. We further extend our framework to test non-sharp null hypotheses concerning quantiles of individual treatment effects. The methods are illustrated through simulations and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00795v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinran Li, Peizan Sheng, Zeyang Yu</dc:creator>
    </item>
    <item>
      <title>Coarse Personalization</title>
      <link>https://arxiv.org/abs/2204.05793</link>
      <description>arXiv:2204.05793v4 Announce Type: replace 
Abstract: With advances in estimating heterogeneous treatment effects, firms can personalize and target individuals at a granular level. However, feasibility constraints limit full personalization. In practice, firms choose segments of individuals and assign a treatment to each segment to maximize profits: We call this the coarse personalization problem. We propose a two-step solution that simultaneously makes segmentation and targeting decisions. First, the firm personalizes by estimating conditional average treatment effects. Second, the firm discretizes using treatment effects to choose which treatments to offer and their segments. We show that a combination of available machine learning tools for estimating heterogeneous treatment effects and a novel application of optimal transport methods provides a viable and efficient solution. With data from a large-scale field experiment in promotions management, we find our methodology outperforms extant approaches that segment on consumer characteristics, consumer preferences, or those that only search over a prespecified grid. Using our procedure, the firm recoups over $99.5\%$ of its expected incremental profits under full personalization while offering only five segments. We conclude by discussing how coarse personalization arises in other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05793v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Walter W. Zhang, Sanjog Misra</dc:creator>
    </item>
    <item>
      <title>Individual Shrinkage for Random Effects</title>
      <link>https://arxiv.org/abs/2308.01596</link>
      <description>arXiv:2308.01596v3 Announce Type: replace 
Abstract: This paper develops a novel approach to random effects estimation and individual-level forecasting in micropanels, targeting individual accuracy rather than aggregate performance. The conventional shrinkage methods used in the literature, such as the James-Stein estimator and Empirical Bayes, target aggregate performance and can lead to inaccurate decisions at the individual level. We propose a class of shrinkage estimators with individual weights (IW) that leverage an individual's own past history, instead of the cross-sectional dimension. This approach overcomes the "tyranny of the majority" inherent in existing methods, while relying on weaker assumptions. A key contribution is addressing the challenge of obtaining feasible weights from short time-series data and under parameter heterogeneity. We discuss the theoretical optimality of IW and recommend using feasible weights determined through a Minimax Regret analysis in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01596v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raffaella Giacomini, Sokbae Lee, Silvia Sarpietro</dc:creator>
    </item>
    <item>
      <title>Threshold Regression in Heterogeneous Panel Data with Interactive Fixed Effects</title>
      <link>https://arxiv.org/abs/2308.04057</link>
      <description>arXiv:2308.04057v2 Announce Type: replace 
Abstract: This paper introduces unit-specific heterogeneity in panel data threshold regression. We develop a comprehensive asymptotic theory for models with heterogeneous thresholds, heterogeneous slope coefficients, and interactive fixed effects. Our estimation methodology employs the Common Correlated Effects approach, which is able to handle heterogeneous coefficients while maintaining computational simplicity. We also propose a semi-homogeneous model with heterogeneous slopes but a common threshold, revealing novel mean group estimator convergence rates due to the interaction of heterogeneity with the shrinking threshold assumption. Tests for linearity are provided, and also a modified information criterion which can choose between the fully heterogeneous and the semi-homogeneous models. Monte Carlo simulations demonstrate the good performance of the new methods in small samples. The new theory is applied to examine the Feldstein-Horioka puzzle and it is found that threshold nonlinearity with respect to trade openness exists only in a small subset of countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04057v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Barassi (University of Birmingham), Yiannis Karavias (Brunel University of London), Chongxian Zhu (University of Birmingham)</dc:creator>
    </item>
    <item>
      <title>Vulnerability Webs: Systemic Risk in Software Networks</title>
      <link>https://arxiv.org/abs/2402.13375</link>
      <description>arXiv:2402.13375v3 Announce Type: replace 
Abstract: Software development relies on code reuse to minimize costs, creating vulnerability risks through dependencies with substantial economic impact, as seen in the Crowdstrike and HeartBleed incidents. We analyze 52,897 dependencies across 16,102 Python repositories using a strategic network formation model incorporating observable and unobservable heterogeneity. Through variational approximation of conditional distributions, we demonstrate that dependency creation generates negative externalities. Vulnerability propagation, modeled as a contagion process, shows that popular protection heuristics are ineffective. AI-assisted coding, on the other hand, offers an effective alternative by enabling dependency replacement with in-house code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13375v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cornelius Fritz, Co-Pierre Georg, Angelo Mele, Michael Schweinberger</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Interaction Models for a Resource Allocator's Decisions and Local Agents' Multiple Activities</title>
      <link>https://arxiv.org/abs/2411.13810</link>
      <description>arXiv:2411.13810v2 Announce Type: replace 
Abstract: This paper introduces a novel spatial interaction model to explore the decision-making processes of a resource allocator and local agents, with central and local governments serving as empirical representations. The model captures two key features: (i) resource allocations from the allocator to local agents and the resulting strategic interactions, and (ii) local agents' multiple activities and their interactions. We develop a network game for the micro-foundations of these processes. In this game, local agents engage in multiple activities, while the allocator distributes resources by monitoring the externalities arising from their interactions. The game's unique Nash equilibrium establishes our econometric framework. To estimate the agent payoff parameters, we employ the quasi-maximum likelihood (QML) estimation method and examine the asymptotic properties of the QML estimator to ensure robust statistical inference. Empirically, we study interactions among U.S. states in public welfare and housing and community development expenditures, focusing on how federal grants influence these expenditures and the interdependencies among state governments. Our findings reveal significant spillovers across the states' two expenditures. Additionally, we detect positive effects of federal grants on both types of expenditures, inducing a responsive grant scheme based on states' decisions. Last, we compare state expenditures and social welfare through counterfactual simulations under two scenarios: (i) responsive intervention by monitoring states' decisions and (ii) autonomous transfers. We find that responsive intervention enhances social welfare by leading to an increase in the states' two expenditures. However, due to the heavy reliance on autonomous transfers, the magnitude of these improvements remains relatively small compared to the share of federal grants in total state revenues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13810v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanbat Jeong</dc:creator>
    </item>
    <item>
      <title>A Binary IV Model for Persuasion: Profiling Persuasion Types among Compliers</title>
      <link>https://arxiv.org/abs/2411.16906</link>
      <description>arXiv:2411.16906v2 Announce Type: replace 
Abstract: In an empirical study of persuasion, researchers often use a binary instrument to encourage individuals to consume information and take some action. We show that, with a binary Imbens-Angrist instrumental variable model and the monotone treatment response assumption, it is possible to identify the joint distribution of potential outcomes among compliers. This is necessary to identify the percentage of mobilised voters and their statistical characteristic defined by the moments of the joint distribution of treatment and covariates. Specifically, we develop a method that enables researchers to identify the statistical characteristic of persuasion types: always-voters, never-voters, and mobilised voters among compliers. These findings extend the kappa weighting results in Abadie (2003). We also provide a sharp test for the two sets of identification assumptions. The test boils down to testing whether there exists a nonnegative solution to a possibly under-determined system of linear equations with known coefficients. An application based on Green et al. (2003) is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16906v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zeyang Yu</dc:creator>
    </item>
  </channel>
</rss>
