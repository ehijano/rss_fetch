<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Feb 2025 16:52:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accelerator-assisted Floating-point ASIP for Communication and Positioning in Massive MIMO Systems</title>
      <link>https://arxiv.org/abs/2502.09785</link>
      <description>arXiv:2502.09785v1 Announce Type: new 
Abstract: This paper presents an implementation of a floating-point-capable application-specific instruction set processor (ASIP) for both communication and positioning tasks using the massive multiple-input multiple-output (MIMO) technology. The ASIP is geared with vector processing capabilities in the form of single instruction multiple data (SIMD). A dual-pronged accelerator composition assists the processor to tame the heavier mathematical workloads. A standalone systolic array accelerator accompanies the processor to aid with matrix multiplications. A parallel vector memory subsystem provides functionalities to both the processor and the systolic array. Additionally, A convolutional neural network (CNN) module accelerator, which is paired with its own separate vector memory, works hand in glove with the processor to take on the positioning task. The processor is synthesized in 22 nm fully depleted silicon-on-insulator (FD-SOI) technology running at a clock frequency of 800 MHz. The system achieves a maximum detection throughput of 2.1 Gb/s in a 128x16 massive MIMO system for the user equipment (UE) speed of 50km/h. The localization throughput settles at around 390 positionings/s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09785v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Attari, Ove Edfors, Liang Liu</dc:creator>
    </item>
    <item>
      <title>INF^2: High-Throughput Generative Inference of Large Language Models using Near-Storage Processing</title>
      <link>https://arxiv.org/abs/2502.09921</link>
      <description>arXiv:2502.09921v1 Announce Type: new 
Abstract: The growing memory and computational demands of large language models (LLMs) for generative inference present significant challenges for practical deployment. One promising solution to address these challenges is offloading-based batched inference, which leverages host memory and disk as an extended memory hierarchy for GPUs. While the approach cost-effectively enables LLM inference, its performance is limited by substantial I/O overhead, primarily due to the large key-value (KV) cache sizes, which increase with batch size and LLM context window length.
  In this paper, we introduce INFerence-INFinity (INF^2), a framework that boosts generative inference throughput using computational storage devices (CSDs). The core of INF^2 is attention-near storage, which offloads memory-intensive self-attention operations to near-storage accelerators, significantly reducing traffic through the system interconnect. We also propose delayed KV cache writeback to hide storage write latency by delaying newly generated KV cache writes until the cache reaches sufficient size in system memory. Additionally, we introduce cooperative X-cache, a technique designed to further trade off the remaining memory capacity for storage bandwidth. Our methods effectively minimize idle time for computation, improving the overall throughput.
  To demonstrate the effectiveness of our approach, \thiswork has been implemented on PyTorch and evaluated on a real system. Our experiments show that INF^2 achieves up to 3.46$\times$ throughput improvement compared to state-of-the-art baselines. We will open-source INF^2 to facilitate broader adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09921v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hongsun Jang, Siung Noh, Changmin Shin, Jaewon Jung, Jaeyong Song, Jinho Lee</dc:creator>
    </item>
    <item>
      <title>Strassen Multisystolic Array Hardware Architectures</title>
      <link>https://arxiv.org/abs/2502.10063</link>
      <description>arXiv:2502.10063v1 Announce Type: new 
Abstract: While Strassen's matrix multiplication algorithm reduces the complexity of naive matrix multiplication, general-purpose hardware is not suitable for achieving the algorithm's promised theoretical speedups. This leaves the question of if it could be better exploited in custom hardware architectures designed specifically for executing the algorithm. However, there is limited prior work on this and it is not immediately clear how to derive such architectures or if they can ultimately lead to real improvements. We bridge this gap, presenting and evaluating new systolic array architectures that efficiently translate the theoretical complexity reductions of Strassen's algorithm directly into hardware resource savings. Furthermore, the architectures are multisystolic array designs that can multiply smaller matrices with higher utilization than single-systolic array designs. The proposed designs implemented on FPGA reduce DSP requirements by a factor of $1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require overall similar soft logic resources when instantiated to support matrix sizes down to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We evaluate the proposed designs both in isolation and in an end-to-end machine learning accelerator compared to baseline designs and prior works, achieving state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10063v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVLSI.2025.3530785</arxiv:DOI>
      <dc:creator>Trevor E. Pogue, Nicola Nicolici</dc:creator>
    </item>
    <item>
      <title>Modeling and Simulating Emerging Memory Technologies: A Tutorial</title>
      <link>https://arxiv.org/abs/2502.10167</link>
      <description>arXiv:2502.10167v1 Announce Type: new 
Abstract: Non-volatile Memory (NVM) technologies present a promising alternative to traditional volatile memories such as SRAM and DRAM. Due to the limited availability of real NVM devices, simulators play a crucial role in architectural exploration and hardware-software co-design. This tutorial presents a simulation toolchain through four detailed case studies, showcasing its applicability to various domains of system design, including hybrid main-memory and cache, compute-in-memory, and wear-leveling design. These case studies provide the reader with practical insights on customizing the toolchain for their specific research needs. The source code is open-sourced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10167v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yun-Chih Chen, Tristan Seidl, Nils H\"olscher, Christian Hakert, Minh Duy Truong, Jian-Jia Chen, Jo\~ao Paulo C. de Lima, Asif Ali Khan, Jeronimo Castrillon, Ali Nezhadi, Lokesh Siddhu, Hassan Nassar, Mahta Mayahinia, Mehdi Baradaran Tahoori, J\"org Henkel, Nils Wilbert, Stefan Wildermann, J\"urgen Teich</dc:creator>
    </item>
    <item>
      <title>EmbBERT-Q: Breaking Memory Barriers in Embedded NLP</title>
      <link>https://arxiv.org/abs/2502.10001</link>
      <description>arXiv:2502.10001v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computational demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25x reduction in size with respect to SotA models. By combining architectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with respect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at https://github.com/RiccardoBravin/tiny-LLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10001v1</guid>
      <category>cs.CL</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri</dc:creator>
    </item>
    <item>
      <title>A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS ACAM for Energy-Efficient Inference</title>
      <link>https://arxiv.org/abs/2502.10089</link>
      <description>arXiv:2502.10089v1 Announce Type: cross 
Abstract: In recent years, the development of smart edge computing systems to process information locally is on the rise. Many near-sensor machine learning (ML) approaches have been implemented to introduce accurate and energy efficient template matching operations in resource-constrained edge sensing systems, such as wearables. To introduce novel solutions that can be viable for extreme edge cases, hybrid solutions combining conventional and emerging technologies have started to be proposed. Deep Neural Networks (DNN) optimised for edge application alongside new approaches of computing (both device and architecture -wise) could be a strong candidate in implementing edge ML solutions that aim at competitive accuracy classification while using a fraction of the power of conventional ML solutions. In this work, we are proposing a hybrid software-hardware edge classifier aimed at the extreme edge near-sensor systems. The classifier consists of two parts: (i) an optimised digital tinyML network, working as a front-end feature extractor, and (ii) a back-end RRAM-CMOS analogue content addressable memory (ACAM), working as a final stage template matching system. The combined hybrid system exhibits a competitive trade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$ and $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with 78.06$\mu$J for the original teacher model, representing a 792-fold reduction, making it a viable solution for extreme edge applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10089v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kieran Woodward, Eiman Kanjo, Georgios Papandroulidakis, Shady Agwa, Themis Prodromakis</dc:creator>
    </item>
    <item>
      <title>Translating Common Security Assertions Across Processor Designs: A RISC-V Case Study</title>
      <link>https://arxiv.org/abs/2502.10194</link>
      <description>arXiv:2502.10194v1 Announce Type: cross 
Abstract: RISC-V is gaining popularity for its adaptability and cost-effectiveness in processor design. With the increasing adoption of RISC-V, the importance of implementing robust security verification has grown significantly. In the state of the art, various approaches have been developed to strengthen the security verification process. Among these methods, assertion-based security verification has proven to be a promising approach for ensuring that security features are effectively met. To this end, some approaches manually define security assertions for processor designs; however, these manual methods require significant time, cost, and human expertise. Consequently, recent approaches focus on translating pre-defined security assertions from one design to another. Nonetheless, these methods are not primarily centered on processor security, particularly RISC-V. Furthermore, many of these approaches have not been validated against real-world attacks, such as hardware Trojans. In this work, we introduce a methodology for translating security assertions across processors with different architectures, using RISC-V as a case study. Our approach reduces time and cost compared to developing security assertions manually from the outset. Our methodology was applied to five critical security modules with assertion translation achieving nearly 100% success across all modules. These results validate the efficacy of our approach and highlight its potential for enhancing security verification in modern processor designs. The effectiveness of the translated assertions was rigorously tested against hardware Trojans defined by large language models (LLMs), demonstrating their reliability in detecting security breaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10194v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sharjeel Imtiaz, Uljana Reinsalu, Tara Ghasempouri</dc:creator>
    </item>
    <item>
      <title>Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS</title>
      <link>https://arxiv.org/abs/2501.13238</link>
      <description>arXiv:2501.13238v2 Announce Type: replace 
Abstract: This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary search (BS) locking, designed to cover a broad frequency range from 533 MHz to 4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a linear to a logarithmic function, completing in B+1 cycles, where B represents the digital-to-analog (DAC) resolution controlling the voltage-controlled delay line (VCDL). At the start of the BS process, large step sizes can cause significant bias overshoots, potentially leading to clock failure conditions (i.e., clocks fail to propagate through the VCDL). To address this issue, a toggle detector is introduced to monitor clock activity and adjust the binary search controller. Upon detecting a stalled clock, the controller reverts the DAC code to the previous working code and resumes the BS with a reduced step size. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a locking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at 4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with a static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter of 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit (FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\cdot ns^2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13238v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicol\'as Wainstein, Eran Avitay, Eugene Avner</dc:creator>
    </item>
    <item>
      <title>The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit Quality Estimation</title>
      <link>https://arxiv.org/abs/2411.00843</link>
      <description>arXiv:2411.00843v2 Announce Type: replace-cross 
Abstract: Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. This work proposes augmenting LLMs with predictor networks trained to estimate circuit quality directly from HDL code. To enhance performance, the model is regularized using embeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT) graphs, thereby incorporating lower-level circuit insights. The proposed method demonstrates superior performance compared to existing graph-based RTL-level estimation techniques on the established benchmark OpenABCD, while providing instant feedback on HDL code quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00843v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Reza Moravej, Saurabh Bodhe, Zhanguang Zhang, Didier Chetelat, Dimitrios Tsaras, Yingxue Zhang, Hui-Ling Zhen, Jianye Hao, Mingxuan Yuan</dc:creator>
    </item>
    <item>
      <title>Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks</title>
      <link>https://arxiv.org/abs/2411.07902</link>
      <description>arXiv:2411.07902v2 Announce Type: replace-cross 
Abstract: Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by generating an ensemble of predictive distributions. However, inference via ensembling is resource-intensive, requiring additional entropy sources to generate stochasticity which increases resource consumption. We introduce Bayes2IMC, an in-memory computing (IMC) architecture designed for binary Bayesian neural networks that leverage nanoscale device stochasticity to generate desired distributions. Our novel approach utilizes Phase-Change Memory (PCM) to harness inherent noise characteristics, enabling the creation of a binary neural network. This design eliminates the necessity for a pre-neuron Analog-to-Digital Converter (ADC), significantly improving power and area efficiency. We also develop a hardware-software co-optimized correction method applied solely on the logits in the final layer to reduce device-induced accuracy variations across deployments on hardware. Additionally, we devise a simple compensation technique that ensures no drop in classification accuracy despite conductance drift of PCM. We validate the effectiveness of our approach on the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy metrics comparable to ideal software implementations as well as results reported in the literature using other technologies. Finally, we present a complete core architecture and compare its projected power, performance, and area efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6 \times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6 \times$ improvement in power efficiency (in GOPS/W). In addition, the projected hardware performance of Bayes2IMC surpasses that of most of the BNN architectures based on memristive devices reported in the literature, and achieves up to $20\%$ higher power efficiency compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07902v2</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prabodh Katti, Clement Ruah, Osvaldo Simeone, Bashir M. Al-Hashimi, Bipin Rajendran</dc:creator>
    </item>
    <item>
      <title>Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</title>
      <link>https://arxiv.org/abs/2502.06309</link>
      <description>arXiv:2502.06309v2 Announce Type: replace-cross 
Abstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. Among all the physical properties of resistive elements, the response to the pulses directly affects the training dynamics. This paper first provides a theoretical foundation for gradient-based training on AIMC hardware and studies the impact of response functions. We demonstrate that noisy update and asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty term on the objective. To overcome the issue, Tiki-Taka, a residual learning algorithm, converges exactly to a critical point by optimizing a main array and a residual array bilevelly. The conclusion is supported by simulations validating our theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06309v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>math.OC</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxian Wu, Quan Xiao, Tayfun Gokmen, Omobayode Fagbohungbe, Tianyi Chen</dc:creator>
    </item>
  </channel>
</rss>
