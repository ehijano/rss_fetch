<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives</title>
      <link>https://arxiv.org/abs/2509.23026</link>
      <description>arXiv:2509.23026v1 Announce Type: new 
Abstract: In practical multi-agent systems, agents often have diverse objectives, which makes the system more complex, as each agent's performance across multiple criteria depends on the joint actions of all agents, creating intricate strategic trade-offs. To address this, we introduce the Multi-Objective Markov Game (MOMG), a framework for multi-agent reinforcement learning with multiple objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary solution concept, where no agent can unilaterally improve one objective without sacrificing performance on another. We prove existence of PNE, and establish an equivalence between the PNE and the set of Nash Equilibria of MOMG's corresponding linearly scalarized games, enabling solutions of MOMG by transferring to a standard single-objective Markov game. However, we note that computing a PNE is theoretically and computationally challenging, thus we propose and study weaker but more tractable solution concepts. Building on these foundations, we develop online learning algorithm that identify a single solution to MOMGs. Furthermore, we propose a two-phase, preference-free algorithm that decouples exploration from planning. Our algorithm enables computation of a PNE for any given preference profile without collecting new samples, providing an efficient methodological characterization of the entire Pareto-Nash front.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23026v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Wang</dc:creator>
    </item>
    <item>
      <title>Situational Awareness for Safe and Robust Multi-Agent Interactions Under Uncertainty</title>
      <link>https://arxiv.org/abs/2509.23425</link>
      <description>arXiv:2509.23425v1 Announce Type: new 
Abstract: Multi-agent systems are prevalent in a wide range of domains including power systems, vehicular networks, and robotics. Two important problems to solve in these types of systems are how the intentions of non-coordinating agents can be determined to predict future behavior and how the agents can achieve their objectives under resource constraints without significantly sacrificing performance. To study this, we develop a model where an autonomous agent observes the environment within a safety radius of observation, determines the state of a surrounding agent of interest (within the observation radius), estimates future actions to be taken, and acts in an optimal way. In the absence of observations, agents are able to utilize an estimation algorithm to predict the future actions of other agents based on historical trajectory. The use of the proposed estimation algorithm introduces uncertainty, which is managed via risk analysis. The proposed approach in this study is validated using two different learning-based decision making frameworks: reinforcement learning and game theoretic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23425v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Alcorn, Eman Hammad</dc:creator>
    </item>
    <item>
      <title>PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features</title>
      <link>https://arxiv.org/abs/2509.24046</link>
      <description>arXiv:2509.24046v1 Announce Type: new 
Abstract: High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24046v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingyao Li, Haolun Wu, Zhenkun Li, Jiabei Hu, Yu Wang, Xiaoshan Huang, Wenyue Hua, Wenqian Wang</dc:creator>
    </item>
    <item>
      <title>CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems</title>
      <link>https://arxiv.org/abs/2509.24088</link>
      <description>arXiv:2509.24088v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24088v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Yu, Moyan Li, Shaoyuan Xu, Jinmiao Fu, Xinhai Hou, Fan Lai, Bryan Wang</dc:creator>
    </item>
    <item>
      <title>MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2509.24323</link>
      <description>arXiv:2509.24323v1 Announce Type: new 
Abstract: The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24323v1</guid>
      <category>cs.MA</category>
      <category>cs.CL</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Wang, Guibin Zhang, ManKit Ye, Xinyu Deng, Dongxia Wang, Xiaobin Hu, Jinyang Guo, Yang Liu, Yufei Guo</dc:creator>
    </item>
    <item>
      <title>MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</title>
      <link>https://arxiv.org/abs/2509.25034</link>
      <description>arXiv:2509.25034v1 Announce Type: new 
Abstract: As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25034v1</guid>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heming Fu, Guojun Xiong, Jian Li, Shan Lin</dc:creator>
    </item>
    <item>
      <title>Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2509.23144</link>
      <description>arXiv:2509.23144v1 Announce Type: cross 
Abstract: Information-processing systems coordinating across multiple agents and objectives face fundamental thermodynamic constraints. We show that solutions with maximum utility to act as coordination focal points have much higher selection pressure for being findable across agents rather than accuracy. We derive that the information-theoretic minimum description length of coordination protocols to precision $\varepsilon$ scales as $L(P)\geq NK\log_2 K+N^2d^2\log (1/\varepsilon)$ for $N$ agents with $d$ potentially conflicting objectives and internal model complexity $K$. This scaling forces progressive simplification, with coordination dynamics changing the environment itself and shifting optimization across hierarchical levels. Moving from established focal points requires re-coordination, creating persistent metastable states and hysteresis until significant environmental shifts trigger phase transitions through spontaneous symmetry breaking. We operationally define coordination temperature to predict critical phenomena and estimate coordination work costs, identifying measurable signatures across systems from neural networks to restaurant bills to bureaucracies. Extending the topological version of Arrow's theorem on the impossibility of consistent preference aggregation, we find it recursively binds whenever preferences are combined. This potentially explains the indefinite cycling in multi-objective gradient descent and alignment faking in Large Language Models trained with reinforcement learning with human feedback. We term this framework Thermodynamic Coordination Theory (TCT), which demonstrates that coordination requires radical information loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23144v1</guid>
      <category>cs.AI</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.MA</category>
      <category>nlin.AO</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Atma Anand</dc:creator>
    </item>
    <item>
      <title>Grouped Satisficing Paths in Pure Strategy Games: a Topological Perspective</title>
      <link>https://arxiv.org/abs/2509.23157</link>
      <description>arXiv:2509.23157v1 Announce Type: cross 
Abstract: In game theory and multi-agent reinforcement learning (MARL), each agent selects a strategy, interacts with the environment and other agents, and subsequently updates its strategy based on the received payoff. This process generates a sequence of joint strategies $(s^t)_{t \geq 0}$, where $s^t$ represents the strategy profile of all agents at time step $t$. A widely adopted principle in MARL algorithms is "win-stay, lose-shift", which dictates that an agent retains its current strategy if it achieves the best response. This principle exhibits a fixed-point property when the joint strategy has become an equilibrium. The sequence of joint strategies under this principle is referred to as a satisficing path, a concept first introduced in [40] and explored in the context of $N$-player games in [39]. A fundamental question arises regarding this principle: Under what conditions does every initial joint strategy $s$ admit a finite-length satisficing path $(s^t)_{0 \leq t \leq T}$ where $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient condition for such a property, and demonstrates that any finite-state Markov game, as well as any $N$-player game, guarantees the existence of a finite-length satisficing path from an arbitrary initial strategy to some equilibrium. These results provide a stronger theoretical foundation for the design of MARL algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23157v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanqing Fu, Chao Huang, Chenrun Wang, Zhuping Wang</dc:creator>
    </item>
    <item>
      <title>SIMPOL Model for Solving Continuous-Time Heterogeneous Agent Problems</title>
      <link>https://arxiv.org/abs/2509.23557</link>
      <description>arXiv:2509.23557v1 Announce Type: cross 
Abstract: This paper presents SIMPOL (Simplified Policy Iteration), a modular numerical framework for solving continuous-time heterogeneous agent models. The core economic problem, the optimization of consumption and savings under idiosyncratic uncertainty, is formulated as a coupled system of partial differential equations: a Hamilton-Jacobi-Bellman (HJB) equation for the agent's optimal policy and a Fokker-Planck-Kolmogorov (FPK) equation for the stationary wealth distribution. SIMPOL addresses this system using Howard's policy iteration with an *upwind* finite difference scheme that guarantees stability. A distinctive contribution is a novel consumption policy post-processing module that imposes regularity through smoothing and a projection onto an economically plausible slope band, improving convergence and model behavior. The robustness and accuracy of SIMPOL are validated through a set of integrated diagnostics, including verification of contraction in the Wasserstein-2 metric and comparison with the analytical solution of the Merton model in the no-volatility case. The framework is shown to be not only computationally efficient but also to produce solutions consistent with economic and mathematical theory, offering a reliable tool for research in quantitative macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23557v1</guid>
      <category>q-fin.CP</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Alonzo Fern\'andez Salguero</dc:creator>
    </item>
    <item>
      <title>Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse</title>
      <link>https://arxiv.org/abs/2509.23778</link>
      <description>arXiv:2509.23778v1 Announce Type: cross 
Abstract: Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of Multi-Agent Path Finding (MAPF), where agents are required to sequentially complete tasks with fixed-location pickup and delivery demands. Although learning-based methods have made progress in MAPD, they often perform poorly in warehouse-like environments with narrow pathways and long corridors when relying only on local observations for distributed decision-making. Communication learning can alleviate the lack of global information but introduce high computational complexity due to point-to-point communication. To address this challenge, we formulate MAPF as a sequence modeling problem and prove that path-finding policies under sequence modeling possess order-invariant optimality, ensuring its effectiveness in MAPD. Building on this, we propose the Sequential Pathfinder (SePar), which leverages the Transformer paradigm to achieve implicit information exchange, reducing decision-making complexity from exponential to linear while maintaining efficiency and global awareness. Experiments demonstrate that SePar consistently outperforms existing learning-based methods across various MAPF tasks and their variants, and generalizes well to unseen environments. Furthermore, we highlight the necessity of integrating imitation learning in complex maps like warehouses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23778v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyuan Zhang, Chaoran Li, Shao Zhang, Ying Wen</dc:creator>
    </item>
    <item>
      <title>FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents</title>
      <link>https://arxiv.org/abs/2509.23803</link>
      <description>arXiv:2509.23803v1 Announce Type: cross 
Abstract: Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23803v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pramit Saha, Joshua Strong, Divyanshu Mishra, Cheng Ouyang, J. Alison Noble</dc:creator>
    </item>
    <item>
      <title>GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning</title>
      <link>https://arxiv.org/abs/2509.24031</link>
      <description>arXiv:2509.24031v1 Announce Type: cross 
Abstract: Foundation models have driven remarkable progress in text, vision, and video understanding, and are now poised to unlock similar breakthroughs in trajectory modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a foundation model for large-scale mobility data that captures patterns of normalcy in human movement. Unlike prior approaches that flatten trajectories into coordinate streams, GPS-MTM decomposes mobility into two complementary modalities: states (point-of-interest categories) and actions (agent transitions). Leveraging a bi-directional Transformer with a self-supervised masked modeling objective, the model reconstructs missing segments across modalities, enabling it to learn rich semantic correlations without manual labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and Geolife, GPS-MTM consistently outperforms on downstream tasks such as trajectory infilling and next-stop prediction. Its advantages are most pronounced in dynamic tasks (inverse and forward dynamics), where contextual reasoning is critical. These results establish GPS-MTM as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning. Code is released for further reference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24031v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Umang Garg, Bowen Zhang, Anantanjit Subrahmanya, Chandrakanth Gudavalli, BS Manjunath</dc:creator>
    </item>
    <item>
      <title>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</title>
      <link>https://arxiv.org/abs/2509.24063</link>
      <description>arXiv:2509.24063v1 Announce Type: cross 
Abstract: Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24063v1</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <category>cs.PF</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Breitwieser, Ahmad Hesam, Abdullah Giray Ya\u{g}l{\i}k\c{c}{\i}, Mohammad Sadrosadati, Fons Rademakers, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Prompting Robot Teams with Natural Language</title>
      <link>https://arxiv.org/abs/2509.24575</link>
      <description>arXiv:2509.24575v1 Announce Type: cross 
Abstract: This paper presents a framework towards prompting multi-robot teams with high-level tasks using natural language expressions. Our objective is to use the reasoning capabilities demonstrated by recent language models in understanding and decomposing human expressions of intent, and repurpose these for multi-robot collaboration and decision-making. The key challenge is that an individual's behavior in a collective can be hard to specify and interpret, and must continuously adapt to actions from others. This necessitates a framework that possesses the representational capacity required by the logic and semantics of a task, and yet supports decentralized and interactive real-time operation. We solve this dilemma by recognizing that a task can be represented as a deterministic finite automaton (DFA), and that recurrent neural networks (RNNs) can encode numerous automata. This allows us to distill the logic and sequential decompositions of sub-tasks obtained from a language model into an RNN, and align its internal states with the semantics of a given task. By training a graph neural network (GNN) control policy that is conditioned on the hidden states of the RNN and the language embeddings, our method enables robots to execute task-relevant actions in a decentralized manner. We present evaluations of this single light-weight interpretable model on various simulated and real-world multi-robot tasks that require sequential and collaborative behavior by the team -- sites.google.com/view/prompting-teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24575v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Pfitzer, Eduardo Sebasti\'an, Ajay Shankar, Amanda Prorok</dc:creator>
    </item>
    <item>
      <title>AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2509.24826</link>
      <description>arXiv:2509.24826v1 Announce Type: cross 
Abstract: Large language models (LLMs) are being increasingly used for planning in orchestrated multi-agent systems. However, existing LLM-based approaches often fall short of human expectations and, critically, lack effective mechanisms for users to inspect, understand, and control their behaviors. These limitations call for enhanced transparency, controllability, and human oversight. To address this, we introduce AIPOM, a system supporting human-in-the-loop planning through conversational and graph-based interfaces. AIPOM enables users to transparently inspect, refine, and collaboratively guide LLM-generated plans, significantly enhancing user control and trust in multi-agent workflows. Our code and demo video are available at https://github.com/megagonlabs/aipom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24826v1</guid>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hannah Kim, Kushan Mitra, Chen Shen, Dan Zhang, Estevam Hruschka</dc:creator>
    </item>
    <item>
      <title>Safety-Critical Input-Constrained Nonlinear Intercept Guidance in Multiple Engagement Zones</title>
      <link>https://arxiv.org/abs/2509.25053</link>
      <description>arXiv:2509.25053v1 Announce Type: cross 
Abstract: This paper presents an input-constrained nonlinear guidance law to address the problem of intercepting a stationary target in contested environments with multiple defending agents. Contrary to prior approaches that rely on explicit knowledge of defender strategies or utilize conservative safety conditions based on a defender's range, our work characterizes defender threats geometrically through engagement zones that delineate inevitable interception regions. Outside these engagement zones, the interceptor remains invulnerable. The proposed guidance law switches between a repulsive safety maneuver near these zones and a pursuit maneuver outside their influence. To deal with multiple engagement zones, we employ a smooth minimum function (log-sum-exponent approximation) that aggregates threats from all the zones while prioritizing the most critical threats. Input saturation is modeled and embedded in the non-holonomic vehicle dynamics so the controller respects actuator limits while maintaining stability. Numerical simulations with several defenders demonstrate the proposed method's ability to avoid engagement zones and achieve interception across diverse initial conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25053v1</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Praveen Kumar Ranjan, Abhinav Sinha, Yongcan Cao</dc:creator>
    </item>
    <item>
      <title>Curriculum Imitation Learning of Distributed Multi-Robot Policies</title>
      <link>https://arxiv.org/abs/2509.25097</link>
      <description>arXiv:2509.25097v1 Announce Type: cross 
Abstract: Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25097v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jes\'us Roche, Eduardo Sebasti\'an, Eduardo Montijano</dc:creator>
    </item>
    <item>
      <title>HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis</title>
      <link>https://arxiv.org/abs/2509.25112</link>
      <description>arXiv:2509.25112v1 Announce Type: cross 
Abstract: Heatwaves pose complex cascading risks across interconnected climate, social, and economic systems, but knowledge fragmentation in scientific literature hinders comprehensive understanding of these risk pathways. We introduce HeDA (Heatwave Discovery Agent), an intelligent multi-agent system designed for automated scientific discovery through knowledge graph construction and multi-layer risk propagation analysis. HeDA processes over 10,247 academic papers to construct a comprehensive knowledge graph with 23,156 nodes and 89,472 relationships, employing novel multi-layer risk propagation analysis to systematically identify overlooked risk transmission pathways. Our system achieves 78.9% accuracy on complex question-answering tasks, outperforming state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA successfully discovered five previously unidentified high-impact risk chains, such as the pathway where a heatwave leads to a water demand surge, resulting in industrial water restrictions and ultimately causing small business disruption, which were validated through historical case studies and domain expert review. This work presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25112v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiquan Wang, Tin-Yeh Huang, Qingyun Gao, Jialin Zhang</dc:creator>
    </item>
    <item>
      <title>Asymmetric Information Enhanced Mapping Framework for Multirobot Exploration based on Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2404.18089</link>
      <description>arXiv:2404.18089v3 Announce Type: replace 
Abstract: Despite the great development of multirobot technologies, efficiently and collaboratively exploring an unknown environment is still a big challenge. In this paper, we propose AIM-Mapping, a Asymmetric InforMation Enhanced Mapping framework. The framework fully utilizes the privilege information in the training process to help construct the environment representation as well as the supervised signal in an asymmetric actor-critic training framework. Specifically, privilege information is used to evaluate the exploration performance through an asymmetric feature representation module and a mutual information evaluation module. The decision-making network uses the trained feature encoder to extract structure information from the environment and combines it with a topological map constructed based on geometric distance. Utilizing this kind of topological map representation, we employ topological graph matching to assign corresponding boundary points to each robot as long-term goal points. We conduct experiments in real-world-like scenarios using the Gibson simulation environments. It validates that the proposed method, when compared to existing methods, achieves great performance improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18089v3</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiyu Cheng, Junhui Fan, Xiaolei Li, Paul L. Rosin, Yibin Li, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Cost-Aware Opinion Dynamics in Multi-Agents Systems under Malicious Agent Influence</title>
      <link>https://arxiv.org/abs/2412.01524</link>
      <description>arXiv:2412.01524v3 Announce Type: replace 
Abstract: In many MASs, links to malicious agents cannot be severed immediately. Under these conditions, averaging-only consensus mechanisms typically lack sufficient resistance, leaving the system vulnerable to harmful deviations. To address this challenge, this brief leverages the Boomerang Effect from sociology, which drives normal agents to firmly reject malicious inputs, although this strategy may appear overly cautious. Thus, this brief emphasizes the necessity of acknowledging the resulting trade-off between cost and convergence speed in practice. To address this, the additional costs induced by Boomerang-style fusion is analyzed and a cost aware evolution rate adjustment mechanism is proposed. Multi-robot simulations demonstrate that this mechanism suppresses excess costs while maintaining resilience to extremist disruptions and ensuring stable convergence, enabling MAS to efficiently develop in a ethical order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01524v3</guid>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>math.OC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhan Suo, Kaiyuan Chen, Yuanqing Xia, Xudong Zhao, Shuo Wang, Runqi Chai</dc:creator>
    </item>
    <item>
      <title>Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions and Online Opponent Modeling</title>
      <link>https://arxiv.org/abs/2504.21164</link>
      <description>arXiv:2504.21164v2 Announce Type: replace 
Abstract: While multi-agent reinforcement learning (MARL) has been proven effective across both collaborative and competitive tasks, existing algorithms often struggle to scale to large populations of agents. Recent advancements in mean-field (MF) theory provide scalable solutions by approximating population interactions as a continuum, yet most existing frameworks focus exclusively on either fully cooperative or purely competitive settings. To bridge this gap, we introduce MF-MAPPO, a mean-field extension of PPO designed for zero-sum team games that integrate intra-team cooperation with inter-team competition. MF-MAPPO employs a shared actor and a minimally informed critic per team and is trained directly on finite-population simulators, thereby enabling deployment to realistic scenarios with thousands of agents. We further show that MF-MAPPO naturally extends to partially observable settings through a simple gradient-regularized training scheme. Our evaluation utilizes large-scale benchmark scenarios using our own testing simulation platform for MF team games (MFEnv), including offense-defense battlefield tasks as well as variants of population-based rock-paper-scissors games that admit analytical solutions, for benchmarking. Across these benchmarks, MF-MAPPO outperforms existing methods and exhibits complex, heterogeneous behaviors, demonstrating the effectiveness of combining mean-field theory and MARL techniques at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21164v2</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras</dc:creator>
    </item>
    <item>
      <title>When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</title>
      <link>https://arxiv.org/abs/2506.09434</link>
      <description>arXiv:2506.09434v2 Announce Type: replace 
Abstract: The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09434v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Amir, Matteo Bettini, Amanda Prorok</dc:creator>
    </item>
    <item>
      <title>Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</title>
      <link>https://arxiv.org/abs/2508.02912</link>
      <description>arXiv:2508.02912v2 Announce Type: replace 
Abstract: Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02912v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh</dc:creator>
    </item>
    <item>
      <title>Learning to Speak on Behalf of a Group: Medium Access Control for Sending a Shared Message</title>
      <link>https://arxiv.org/abs/2202.00401</link>
      <description>arXiv:2202.00401v2 Announce Type: replace-cross 
Abstract: The rapid development of Internet of Things (IoT) technologies has not only enabled new applications, but also presented new challenges for reliable communication with limited resources. In this work, we define a novel problem that can arise in these scenarios, in which a set of sensors need to communicate a joint observation. This observation is shared by a random subset of the nodes, which need to propagate it to the rest of the network, but coordination is complex: as signaling constraints require the use of random access schemes over shared channels, sensors need to implicitly coordinate, so that at least one transmission gets through without collisions. Unlike the majority of existing medium access schemes, the goal is to make sure that the shared message gets through, regardless of the sender. We analyze this coordination problem theoretically and provide low-complexity solutions. While a clustering-based approach is near-optimal if the sensors have prior knowledge, we provide a distributed multi-armed bandit (MAB) solution for the more general case and validate it by simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.00401v2</guid>
      <category>cs.NI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaan ul Haque, Siddharth Chandak, Federico Chiariotti, Deniz Gunduz, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2405.14314</link>
      <description>arXiv:2405.14314v4 Announce Type: replace-cross 
Abstract: Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://embodied-read.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14314v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li</dc:creator>
    </item>
    <item>
      <title>Second-Order Algorithms for Finding Local Nash Equilibria in Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2406.03565</link>
      <description>arXiv:2406.03565v3 Announce Type: replace-cross 
Abstract: Zero-sum games arise in a wide variety of problems, including robust optimization and adversarial learning. However, algorithms deployed for finding a local Nash equilibrium in these games often converge to non-Nash stationary points. This highlights a key challenge: for any algorithm, the stability properties of its underlying dynamical system can cause non-Nash points to be potential attractors. To overcome this challenge, algorithms must account for subtleties involving the curvatures of players' costs. To this end, we leverage dynamical system theory and develop a second-order algorithm for finding a local Nash equilibrium in the smooth, possibly nonconvex-nonconcave, zero-sum game setting. First, we prove that this novel method guarantees convergence to only local Nash equilibria with an asymptotic local \textit{linear} convergence rate. We then interpret a version of this method as a modified Gauss-Newton algorithm with local \textit{superlinear} convergence to the neighborhood of a point that satisfies first-order local Nash equilibrium conditions. In comparison, current related state-of-the-art methods with similar guarantees do not offer convergence rates in the nonconvex-nonconcave setting. Furthermore, we show that this approach naturally generalizes to settings with convex and potentially coupled constraints while retaining earlier guarantees of convergence to only local (generalized) Nash equilibria. Code for our experiments can be found at https://github.com/CLeARoboticsLab/ZeroSumGameSolve.jl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03565v3</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kushagra Gupta, Xinjie Liu, Ross Allen, Ufuk Topcu, David Fridovich-Keil</dc:creator>
    </item>
    <item>
      <title>SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation</title>
      <link>https://arxiv.org/abs/2505.09081</link>
      <description>arXiv:2505.09081v2 Announce Type: replace-cross 
Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09081v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Koley</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Desire Alignment for Embodied Agent-Human Adaptation</title>
      <link>https://arxiv.org/abs/2505.22503</link>
      <description>arXiv:2505.22503v2 Announce Type: replace-cross 
Abstract: While embodied agents have made significant progress in performing complex physical tasks, real-world applications demand more than pure task execution. The agents must collaborate with unfamiliar agents and human users, whose goals are often vague and implicit. In such settings, interpreting ambiguous instructions and uncovering underlying desires is essential for effective assistance. Therefore, fast and accurate desire alignment becomes a critical capability for embodied agents. In this work, we first develop a home assistance simulation environment HA-Desire that integrates an LLM-driven proxy human user exhibiting realistic value-driven goal selection and communication. The ego agent must interact with this proxy user to infer and adapt to the user's latent desires. To achieve this, we present a novel framework FAMER for fast desire alignment, which introduces a desire-based mental reasoning mechanism to identify user intent and filter desire-irrelevant actions. We further design a reflection-based communication module that reduces redundant inquiries, and incorporate goal-relevant information extraction with memory persistence to improve information reuse and reduce unnecessary exploration. Extensive experiments demonstrate that our framework significantly enhances both task execution and communication efficiency, enabling embodied agents to quickly adapt to user-specific desires in complex embodied environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22503v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanfei Wang, Xinju Huang, Fangwei Zhong, Yaodong Yang, Yizhou Wang, Yuanpei Chen, Hao Dong</dc:creator>
    </item>
    <item>
      <title>StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework</title>
      <link>https://arxiv.org/abs/2506.14159</link>
      <description>arXiv:2506.14159v2 Announce Type: replace-cross 
Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14159v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shayan Talaei, Meijin Li, Kanu Grover, James Kent Hippler, Diyi Yang, Amin Saberi</dc:creator>
    </item>
    <item>
      <title>Moving Out: Physically-grounded Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2507.18623</link>
      <description>arXiv:2507.18623v3 Announce Type: replace-cross 
Abstract: The ability to adapt to physical actions and constraints in an environment is crucial for embodied agents (e.g., robots) to effectively collaborate with humans. Such physically grounded human-AI collaboration must account for the increased complexity of the continuous state-action space and constrained dynamics caused by physical constraints. In this paper, we introduce Moving Out, a new human-AI collaboration benchmark that resembles a wide range of collaboration modes affected by physical attributes and constraints, such as moving heavy items together and maintaining consistent actions to move a big item around a corner. Using Moving Out, we designed two tasks and collected human-human interaction data to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes. To address the challenges in physical environments, we propose a novel method, BASS (Behavior Augmentation, Simulation, and Selection), to enhance the diversity of agents and their understanding of the outcome of actions. Our experiments show that BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration. The project page is available at https://live-robotics-uva.github.io/movingout_ai/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18623v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuhui Kang, Sung-Wook Lee, Haolin Liu, Yuyan Wang, Yen-Ling Kuo</dc:creator>
    </item>
    <item>
      <title>Co-Evolving Complexity: An Adversarial Framework for Automatic MARL Curricula</title>
      <link>https://arxiv.org/abs/2509.03771</link>
      <description>arXiv:2509.03771v2 Announce Type: replace-cross 
Abstract: The advancement of general-purpose intelligent agents is intrinsically linked to the environments in which they are trained. While scaling models and datasets has yielded remarkable capabilities, scaling the complexity, diversity, and interactivity of environments remains a crucial bottleneck. Hand-crafted environments are finite and often contain implicit biases, limiting the potential for agents to develop truly generalizable and robust skills. In this work, we propose a paradigm for generating a boundless and adaptive curriculum of challenges by framing the environment generation process as an adversarial game. We introduce a system where a team of cooperative multi-agent defenders learns to survive against a procedurally generative attacker. The attacker agent learns to produce increasingly challenging configurations of enemy units, dynamically creating novel worlds tailored to exploit the defenders' current weaknesses. Concurrently, the defender team learns cooperative strategies to overcome these generated threats. This co-evolutionary dynamic creates a self-scaling environment where complexity arises organically from the adversarial interaction, providing an effectively infinite stream of novel and relevant training data. We demonstrate that with minimal training, this approach leads to the emergence of complex, intelligent behaviors, such as flanking and shielding by the attacker, and focus-fire and spreading by the defenders. Our findings suggest that adversarial co-evolution is a powerful mechanism for automatically scaling environmental complexity, driving agents towards greater robustness and strategic depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03771v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen Hill</dc:creator>
    </item>
    <item>
      <title>Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</title>
      <link>https://arxiv.org/abs/2509.04731</link>
      <description>arXiv:2509.04731v2 Announce Type: replace-cross 
Abstract: Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04731v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen Hill</dc:creator>
    </item>
    <item>
      <title>Code2MCP: Transforming Code Repositories into MCP Services</title>
      <link>https://arxiv.org/abs/2509.05941</link>
      <description>arXiv:2509.05941v2 Announce Type: replace-cross 
Abstract: The Model Context Protocol (MCP) aims to create a standard for how Large Language Models use tools. However, most current research focuses on selecting tools from an existing pool. A more fundamental, yet largely overlooked, problem is how to populate this pool by converting the vast number of existing software projects into MCP-compatible services. To bridge this gap, we introduce Code2MCP, an agent-based framework that automatically transforms a GitHub repository into a functional MCP service with minimal human intervention. Code2MCP employs a multi-agent workflow for code analysis, environment setup, tool function design, and service generation, enhanced by a self-correcting loop to ensure reliability. We demonstrate that Code2MCP successfully transforms open-source computing libraries in scientific fields such as bioinformatics, mathematics, and fluid dynamics that are not available in existing MCP servers. By providing a novel automated pathway to unlock GitHub, the world's largest code repository, for the MCP ecosystem, Code2MCP serves as a catalyst to significantly accelerate the protocol's adoption and practical application. The code is public at https://github.com/DEFENSE-SEU/Code2MCP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05941v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoqian Ouyang, Ling Yue, Shimin Di, Libin Zheng, Linan Yue, Shaowu Pan, Jian Yin, Min-Ling Zhang</dc:creator>
    </item>
  </channel>
</rss>
