<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2024 04:01:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Staleness Factor Model and Volatility Estimation</title>
      <link>https://arxiv.org/abs/2410.07607</link>
      <description>arXiv:2410.07607v1 Announce Type: new 
Abstract: In this paper, we introduce a novel nonstationary price staleness factor model allowing for market friction pervasive across assets and possible input covariates. With large panel high-frequency data, we give the maximum likelihood estimators of the regressing coefficients, and the factors and their loading parameters, which recovers the time-varying price stale probability and an integrated functional of the price staleness over two assets. The asymptotic results are obtained when both the dimension $d$ and the sampling frequency $n$ diverge simultaneously. With the local principal component analysis (LPCA) method, we find that the efficient price co-volatilities (systematic and idiosyncratic), are biased downward due to the presence of staleness. Bias corrected estimators of the systematic and idiosyncratic covolatities (spot and integrated) are provided and proved to be consistent. Interestingly, beside their dependence on the dimensionality $d$, the integrated estimates converge with a factor of $n^{-1/2}$ though the local PCA estimates converge with a factor of $n^{-1/4}$, validating the aggregation efficiency after nonlinear nonstationary factor analysis. But the bias correction degrade the convergence rates of the estimated (spot or integrated) systematic covolatilies. Numerical experiments justify our theoretical findings. Empirically, we observe that the staleness correction indeed leads to higher in-sample systematic volatility estimates, but a reduced out-of-sample portfolio risk almost uniformly in tested gross exposure levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07607v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin-Bing Kong, Bin Wu, Wuyi Ye</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation of Parameters in Degree Corrected Mixed Membership Models</title>
      <link>https://arxiv.org/abs/2410.07621</link>
      <description>arXiv:2410.07621v1 Announce Type: new 
Abstract: With the rise of big data, networks have pervaded many aspects of our daily lives, with applications ranging from the social to natural sciences. Understanding the latent structure of the network is thus an important question. In this paper, we model the network using a Degree-Corrected Mixed Membership (DCMM) model, in which every node $i$ has an affinity parameter $\theta_i$, measuring the degree of connectivity, and an intrinsic membership probability vector $\pi_i = (\pi_1, \cdots \pi_K)$, measuring its belonging to one of $K$ communities, and a probability matrix $P$ that describes the average connectivity between two communities. Our central question is to determine the optimal estimation rates for the probability matrix and degree parameters $P$ and $\Theta$ of the DCMM, an often overlooked question in the literature. By providing new lower bounds, we show that simple extensions of existing estimators in the literature indeed achieve the optimal rate. Simulations lend further support to our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07621v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Jiang, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>A multivariate spatial regression model using signatures</title>
      <link>https://arxiv.org/abs/2410.07899</link>
      <description>arXiv:2410.07899v1 Announce Type: new 
Abstract: We propose a spatial autoregressive model for a multivariate response variable and functional covariates. The approach is based on the notion of signature, which represents a function as an infinite series of its iterated integrals and presents the advantage of being applicable to a wide range of processes. We have provided theoretical guarantees for the choice of the signature truncation order, and we have shown in a simulation study that this approach outperforms existing approaches in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07899v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Fr\'event, Issa-Mbenard Dabo</dc:creator>
    </item>
    <item>
      <title>On the Lower Confidence Band for the Optimal Welfare</title>
      <link>https://arxiv.org/abs/2410.07443</link>
      <description>arXiv:2410.07443v1 Announce Type: cross 
Abstract: This article addresses the question of reporting a lower confidence band (LCB) for optimal welfare in policy learning problems. A straightforward procedure inverts a one-sided t-test based on an efficient estimator of the optimal welfare. We argue that in an empirically relevant class of data-generating processes, a LCB corresponding to suboptimal welfare may exceed the straightforward LCB, with the average difference of order N-{1/2}. We relate this result to a lack of uniformity in the so-called margin assumption, commonly imposed in policy learning and debiased inference. We advocate for using uniformly valid asymptotic approximations and show how existing methods for inference in moment inequality models can be used to construct valid and tight LCBs for the optimal welfare. We illustrate our findings in the context of the National JTPA study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07443v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Ponomarev, Vira Semenova</dc:creator>
    </item>
    <item>
      <title>Representation-Enhanced Neural Knowledge Integration with Application to Large-Scale Medical Ontology Learning</title>
      <link>https://arxiv.org/abs/2410.07454</link>
      <description>arXiv:2410.07454v1 Announce Type: cross 
Abstract: A large-scale knowledge graph enhances reproducibility in biomedical data discovery by providing a standardized, integrated framework that ensures consistent interpretation across diverse datasets. It improves generalizability by connecting data from various sources, enabling broader applicability of findings across different populations and conditions. Generating reliable knowledge graph, leveraging multi-source information from existing literature, however, is challenging especially with a large number of node sizes and heterogeneous relations. In this paper, we propose a general theoretically guaranteed statistical framework, called RENKI, to enable simultaneous learning of multiple relation types. RENKI generalizes various network models widely used in statistics and computer science. The proposed framework incorporates representation learning output into initial entity embedding of a neural network that approximates the score function for the knowledge graph and continuously trains the model to fit observed facts. We prove nonasymptotic bounds for in-sample and out-of-sample weighted MSEs in relation to the pseudo-dimension of the knowledge graph function class. Additionally, we provide pseudo-dimensions for score functions based on multilayer neural networks with ReLU activation function, in the scenarios when the embedding parameters either fixed or trainable. Finally, we complement our theoretical results with numerical studies and apply the method to learn a comprehensive medical knowledge graph combining a pretrained language model representation with knowledge graph links observed in several medical ontologies. The experiments justify our theoretical findings and demonstrate the effect of weighting in the presence of heterogeneous relations and the benefit of incorporating representation learning in nonparametric models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07454v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqi Liu, Tianxi Cai, Xiaoou Li</dc:creator>
    </item>
    <item>
      <title>Language all the way down: Linguistic structures in statistics education</title>
      <link>https://arxiv.org/abs/2410.07569</link>
      <description>arXiv:2410.07569v1 Announce Type: cross 
Abstract: The ability to read, write, and speak mathematics is critical to students becoming comfortable with statistical models and skills. Faster development of those skills may act as encouragement to further engage with the discipline. Vocabulary has been the focus of scholarship in existing literature on the linguistics of mathematics and statistics but there are structures that go beyond the content of words and symbols. Here I introduce ideas for grammar and discourse features through a sequence of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07569v1</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tess O'Brien</dc:creator>
    </item>
    <item>
      <title>Theoretical limits of descending $\ell_0$ sparse-regression ML algorithms</title>
      <link>https://arxiv.org/abs/2410.07651</link>
      <description>arXiv:2410.07651v1 Announce Type: cross 
Abstract: We study the theoretical limits of the $\ell_0$ (quasi) norm based optimization algorithms when employed for solving classical compressed sensing or sparse regression problems. Considering standard contexts with deterministic signals and statistical systems, we utilize \emph{Fully lifted random duality theory} (Fl RDT) and develop a generic analytical program for studying performance of the \emph{maximum-likelihood} (ML) decoding. The key ML performance parameter, the residual \emph{root mean square error} ($\textbf{RMSE}$), is uncovered to exhibit the so-called \emph{phase-transition} (PT) phenomenon. The associated aPT curve, which separates the regions of systems dimensions where \emph{an} $\ell_0$ based algorithm succeeds or fails in achieving small (comparable to the noise) ML optimal $\textbf{RMSE}$ is precisely determined as well. In parallel, we uncover the existence of another dPT curve which does the same separation but for practically feasible \emph{descending} $\ell_0$ ($d\ell_0$) algorithms. Concrete implementation and practical relevance of the Fl RDT typically rely on the ability to conduct a sizeable set of the underlying numerical evaluations which reveal that for the ML decoding the Fl RDT converges astonishingly fast with corrections in the estimated quantities not exceeding $\sim 0.1\%$ already on the third level of lifting. Analytical results are supplemented by a sizeable set of numerical experiments where we implement a simple variant of $d\ell_0$ and demonstrate that its practical performance very accurately matches the theoretical predictions. Completely surprisingly, a remarkably precise agreement between the simulations and the theory is observed for fairly small dimensions of the order of 100.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07651v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihailo Stojnic</dc:creator>
    </item>
    <item>
      <title>Breaking the curse of dimensionality in structured density estimation</title>
      <link>https://arxiv.org/abs/2410.07685</link>
      <description>arXiv:2410.07685v1 Announce Type: cross 
Abstract: We consider the problem of estimating a structured multivariate density, subject to Markov conditions implied by an undirected graph. In the worst case, without Markovian assumptions, this problem suffers from the curse of dimensionality. Our main result shows how the curse of dimensionality can be avoided or greatly alleviated under the Markov property, and applies to arbitrary graphs. While existing results along these lines focus on sparsity or manifold assumptions, we introduce a new graphical quantity called "graph resilience" and show how it controls the sample complexity. Surprisingly, although one might expect the sample complexity of this problem to scale with local graph parameters such as the degree, this turns out not to be the case. Through explicit examples, we compute uniform deviation bounds and illustrate how the curse of dimensionality in density estimation can thus be circumvented. Notable examples where the rate improves substantially include sequential, hierarchical, and spatial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07685v1</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert A. Vandermeulen, Wai Ming Tai, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Equivalence of Approximate Message Passing and Low-Degree Polynomials in Rank-One Matrix Estimation</title>
      <link>https://arxiv.org/abs/2212.06996</link>
      <description>arXiv:2212.06996v2 Announce Type: replace 
Abstract: We consider the problem of estimating an unknown parameter vector ${\boldsymbol \theta}\in{\mathbb R}^n$, given noisy observations ${\boldsymbol Y} = {\boldsymbol \theta}{\boldsymbol \theta}^{\top}/\sqrt{n}+{\boldsymbol Z}$ of the rank-one matrix ${\boldsymbol \theta}{\boldsymbol \theta}^{\top}$, where ${\boldsymbol Z}$ has independent Gaussian entries. When information is available about the distribution of the entries of ${\boldsymbol \theta}$, spectral methods are known to be strictly sub-optimal. Past work characterized the asymptotics of the accuracy achieved by the optimal estimator. However, no polynomial-time estimator is known that achieves this accuracy.
  It has been conjectured that this statistical-computation gap is fundamental, and moreover that the optimal accuracy achievable by polynomial-time estimators coincides with the accuracy achieved by certain approximate message passing (AMP) algorithms. We provide evidence towards this conjecture by proving that no estimator in the (broader) class of constant-degree polynomials can surpass AMP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06996v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Montanari, Alexander S. Wein</dc:creator>
    </item>
    <item>
      <title>On the probability of linear separability through intrinsic volumes</title>
      <link>https://arxiv.org/abs/2404.12889</link>
      <description>arXiv:2404.12889v2 Announce Type: replace 
Abstract: A dataset with two labels is linearly separable if it can be split into its two classes with a hyperplane. This inflicts a curse on some statistical tools (such as logistic regression) but forms a blessing for others (e.g. support vector machines). Recently, the following question has regained interest: What is the probability that the data are linearly separable?
  We provide a formula for the probability of linear separability for Gaussian features and labels depending only on one marginal of the features (as in generalized linear models). In this setting, we derive an upper bound that complements the recent result by Hayakawa, Lyons, and Oberhauser [2023], and a sharp upper bound for sign-flip noise.
  To prove our results, we exploit that this probability can be expressed as a sum of the intrinsic volumes of a polyhedral cone of the form $\text{span}\{v\}\oplus[0,\infty)^n$, as shown in Cand\`es and Sur [2020]. After providing the inequality description for this cone, and an algorithm to project onto it, we calculate its intrinsic volumes. In doing so, we encounter Youden's demon problem, for which we provide a formula following Kabluchko and Zaporozhets [2020]. The key insight of this work is the following: The number of correctly labeled observations in the data affects the structure of this polyhedral cone, allowing the translation of insights from geometry into statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12889v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Kuchelmeister</dc:creator>
    </item>
    <item>
      <title>With random regressors, least squares inference is robust to correlated errors with unknown correlation structure</title>
      <link>https://arxiv.org/abs/2410.05567</link>
      <description>arXiv:2410.05567v2 Announce Type: replace 
Abstract: Linear regression is arguably the most widely used statistical method. With fixed regressors and correlated errors, the conventional wisdom is to modify the variance-covariance estimator to accommodate the known correlation structure of the errors. We depart from the literature by showing that with random regressors, linear regression inference is robust to correlated errors with unknown correlation structure. The existing theoretical analyses for linear regression are no longer valid because even the asymptotic normality of the least-squares coefficients breaks down in this regime. We first prove the asymptotic normality of the t statistics by establishing their Berry-Esseen bounds based on a novel probabilistic analysis of self-normalized statistics. We then study the local power of the corresponding t tests and show that, perhaps surprisingly, error correlation can even enhance power in the regime of weak signals. Overall, our results show that linear regression is applicable more broadly than the conventional theory suggests, and further demonstrate the value of randomization to ensure robustness of inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05567v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zifeng Zhang, Peng Ding, Wen Zhou, Haonan Wang</dc:creator>
    </item>
    <item>
      <title>General Covariance-Based Conditions for Central Limit Theorems with Dependent Triangular Arrays</title>
      <link>https://arxiv.org/abs/2308.12506</link>
      <description>arXiv:2308.12506v4 Announce Type: replace-cross 
Abstract: We present a general central limit theorem with simple, easy-to-check covariance-based sufficient conditions for triangular arrays of random vectors when all variables could be interdependent. The result is constructed from Stein's method, but the conditions are distinct from related work. We show that these covariance conditions nest standard assumptions studied in the literature such as $M$-dependence, mixing random fields, non-mixing autoregressive processes, and dependency graphs, which themselves need not imply each other. This permits researchers to work with high-level but intuitive conditions based on overall correlation instead of more complicated and restrictive conditions such as strong mixing in random fields that may not have any obvious micro-foundation. As examples of the implications, we show how the theorem implies asymptotic normality in estimating: treatment effects with spillovers in more settings than previously admitted, covariance matrices, processes with global dependencies such as epidemic spread and information diffusion, and spatial process with Mat\'{e}rn dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12506v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Matthew O. Jackson, Tyler H. McCormick, Vydhourie Thiyageswaran</dc:creator>
    </item>
    <item>
      <title>Matrix perturbation bounds via contour bootstrapping</title>
      <link>https://arxiv.org/abs/2407.05230</link>
      <description>arXiv:2407.05230v3 Announce Type: replace-cross 
Abstract: Matrix perturbation bounds play an essential role in the design and analysis of spectral algorithms. In this paper, we introduce a new method to deduce matrix perturbation bounds, which we call "contour bootstrapping". As applications, we work out several new bounds for eigensubspace computation and low rank approximation. Next, we use these bounds to study utility problems in the area of differential privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05230v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
    <item>
      <title>Sharp bounds on aggregate expert error</title>
      <link>https://arxiv.org/abs/2407.16642</link>
      <description>arXiv:2407.16642v3 Announce Type: replace-cross 
Abstract: We revisit the classic problem of aggregating binary advice from conditionally independent experts, also known as the Naive Bayes setting. Our quantity of interest is the error probability of the optimal decision rule. In the case of symmetric errors (sensitivity = specificity), reasonably tight bounds on the optimal error probability are known. In the general asymmetric case, we are not aware of any nontrivial estimates on this quantity. Our contribution consists of sharp upper and lower bounds on the optimal error probability in the general case, which recover and sharpen the best known results in the symmetric special case. Since this turns out to be equivalent to estimating the total variation distance between two product distributions, our results also have bearing on this important and challenging problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16642v3</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryeh Kontorovich</dc:creator>
    </item>
  </channel>
</rss>
