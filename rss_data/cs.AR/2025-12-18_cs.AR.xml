<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 02:45:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Implementation and Analysis of Thermometer Encoding in DWN FPGA Accelerators</title>
      <link>https://arxiv.org/abs/2512.15251</link>
      <description>arXiv:2512.15251v1 Announce Type: new 
Abstract: Fully parallel neural network accelerators on field-programmable gate arrays (FPGAs) offer high throughput for latency-critical applications but face hardware resource constraints. Weightless neural networks (WNNs) efficiently replace arithmetic with logic-based inference. Differential weightless neural networks (DWN) further optimize resource usage by learning connections between encoders and LUT layers via gradient-based training. However, DWNs rely on thermometer encoding, and the associated hardware cost has not been fully evaluated. We present a DWN hardware generator that includes thermometer encoding explicitly. Experiments on the Jet Substructure Classification (JSC) task show that encoding can increase LUT usage by up to 3.20$\times$, dominating costs in small networks and highlighting the need for encoding-aware hardware design in DWN accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15251v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Mecik, Martin Kumm</dc:creator>
    </item>
    <item>
      <title>FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption</title>
      <link>https://arxiv.org/abs/2512.15515</link>
      <description>arXiv:2512.15515v1 Announce Type: new 
Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15515v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihan Xu, Rajgopal Kannan, Viktor K. Prasanna</dc:creator>
    </item>
    <item>
      <title>A High-level Synthesis Toolchain for the Julia Language</title>
      <link>https://arxiv.org/abs/2512.15679</link>
      <description>arXiv:2512.15679v1 Announce Type: cross 
Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15679v1</guid>
      <category>cs.SE</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benedict Short, Ian McInerney, John Wickerson</dc:creator>
    </item>
    <item>
      <title>Pipeline Stage Resolved Timing Characterization of FPGA and ASIC Implementations of a RISC V Processor</title>
      <link>https://arxiv.org/abs/2512.13866</link>
      <description>arXiv:2512.13866v2 Announce Type: replace-cross 
Abstract: This paper presents a pipeline stage resolved timing characterization of a 32-bit RISC V processor implemented on a 20 nm FPGA and a 7 nm FinFET ASIC platform. A unified analysis framework is introduced that decomposes timing paths into logic, routing, and clocking components and maps them to well-defined pipeline stage transitions. This approach enables systematic comparison of timing behavior across heterogeneous implementation technologies at a microarchitectural level. Using static timing analysis and statistical characterization, the study shows that although both implementations exhibit dominant critical paths in the EX to MEM pipeline transition, their underlying timing mechanisms differ fundamentally. FPGA timing is dominated by routing parasitics and placement dependent variability, resulting in wide slack distributions and sensitivity to routing topology. In contrast, ASIC timing is governed primarily by combinational logic depth and predictable parametric variation across process, voltage, and temperature corners, yielding narrow and stable timing distributions. The results provide quantitative insight into the structural origins of timing divergence between programmable and custom fabrics and demonstrate the effectiveness of pipeline stage resolved analysis for identifying platform specific bottlenecks. Based on these findings, the paper derives design implications for achieving predictable timing closure in processor architectures targeting both FPGA and ASIC implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13866v2</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mostafa Darvishi</dc:creator>
    </item>
  </channel>
</rss>
