<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scaling Reproducibility: An AI-Assisted Workflow for Large-Scale Reanalysis</title>
      <link>https://arxiv.org/abs/2602.16733</link>
      <description>arXiv:2602.16733v1 Announce Type: new 
Abstract: Reproducibility is central to research credibility, yet large-scale reanalysis of empricial data remains costly because replication packages vary widely in structure, software environment, and documentation. We develop and evaluate an agentic AI workflow that addresses this execution bottleneck while preserving scientific rigor. The system separates scientific reasoning from computational execution: researchers design fixed diagnostic templates, and the workflow automates the acquisition, harmonization, and execution of replication materials using pre-specified, version-controlled code. A structured knowledge layer records resolved failure patterns, enabling adaptation across heterogeneous studies while keeping each pipeline version transparent and stable. We evaluate this workflow on 92 instrumental variable (IV) studies, including 67 with manually verified reproducible 2SLS estimates and 25 newly published IV studies under identical criteria. For each paper, we analyze up to three two-stage least squares (2SLS) specifications, totaling 215. Across the 92 papers, the system achieves 87% end-to-end success overall. Conditional on accessible data and code, reproducibility is 100% at both the paper and specification levels. The framework substantially lowers the cost of executing established empirical protocols and can be adapted in empirical settings where analytic templates and norms of transparency are well established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16733v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiqing Xu, Leo Yang Yang</dc:creator>
    </item>
    <item>
      <title>Generative modeling for the bootstrap</title>
      <link>https://arxiv.org/abs/2602.17052</link>
      <description>arXiv:2602.17052v1 Announce Type: cross 
Abstract: Generative modeling builds on and substantially advances the classical idea of simulating synthetic data from observed samples. This paper shows that this principle is not only natural but also theoretically well-founded for bootstrap inference: it yields statistically valid confidence intervals that apply simultaneously to both regular and irregular estimators, including settings in which Efron's bootstrap fails. In this sense, the generative modeling-based bootstrap can be viewed as a modern version of the smoothed bootstrap: it could mitigate the curse of dimensionality and remain effective in challenging regimes where estimators may lack root-$n$ consistency or a Gaussian limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17052v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leon Tran, Ting Ye, Peng Ding, Fang Han</dc:creator>
    </item>
    <item>
      <title>genriesz: A Python Package for Automatic Debiased Machine Learning with Generalized Riesz Regression</title>
      <link>https://arxiv.org/abs/2602.17543</link>
      <description>arXiv:2602.17543v1 Announce Type: cross 
Abstract: Efficient estimation of causal and structural parameters can be automated using the Riesz representation theorem and debiased machine learning (DML). We present genriesz, an open-source Python package that implements automatic DML and generalized Riesz regression, a unified framework for estimating Riesz representers by minimizing empirical Bregman divergences. This framework includes covariate balancing, nearest-neighbor matching, calibrated estimation, and density ratio estimation as special cases. A key design principle of the package is automatic regressor balancing (ARB): given a Bregman generator $g$ and a representer model class, genriesz} automatically constructs a compatible link function so that the generalized Riesz regression estimator satisfies balancing (moment-matching) optimality conditions in a user-chosen basis. The package provides a modulr interface for specifying (i) the target linear functional via a black-box evaluation oracle, (ii) the representer model via basis functions (polynomial, RKHS approximations, random forest leaf encodings, neural embeddings, and a nearest-neighbor catchment basis), and (iii) the Bregman generator, with optional user-supplied derivatives. It returns regression adjustment (RA), Riesz weighting (RW), augmented Riesz weighting (ARW), and TMLE-style estimators with cross-fitting, confidence intervals, and $p$-values. We highlight representative workflows for estimation problems such as the average treatment effect (ATE), ATE on treated (ATT), and average marginal effect estimation. The Python package is available at https://github.com/MasaKat0/genriesz and on PyPI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17543v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Group-Heterogeneous Changes-in-Changes and Distributional Synthetic Controls</title>
      <link>https://arxiv.org/abs/2307.15313</link>
      <description>arXiv:2307.15313v2 Announce Type: replace 
Abstract: We develop new changes-in-changes (CIC) and distributional synthetic controls (DSC) types of methods when there exists group-level heterogeneity. For CIC, we allow individuals to belong to heterogeneous groups, extending Athey and Imbens (2006) by finding appropriate control groups that share similar group-level unobserved characteristics to the treatment groups. For DSC, we show that the synthetic control units are not necessarily from the same period as in Gunsilius (2023); they may come from different periods in which they have comparable group-level heterogeneity to the treatment group. Implementation of these new methods is briefly discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15313v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songnian Chen, Junlong Feng</dc:creator>
    </item>
    <item>
      <title>Universal Factor Models</title>
      <link>https://arxiv.org/abs/2501.15761</link>
      <description>arXiv:2501.15761v3 Announce Type: replace 
Abstract: We propose a new factor analysis framework and estimators of the factors and loadings that are robust to certain weak factors in a large $N$ and large $T$ setting. Our framework, by simultaneously considering all quantile levels of the outcome variable, induces standard mean and quantile factor models, but the factors can have an arbitrarily weak influence on the outcome's mean or quantile at most quantile levels. Our method estimates the factor space at the $\sqrt{N}$-rate as long as each factor is strong at some unknown quantile level, and achieves $\sqrt{N}$- and $\sqrt{T}$-asymptotic normality for the factors and loadings based on a novel sample splitting approach that handles incidental nuisance parameters. We also develop a weak-factor-robust estimator of the number of factors and consistent selectors of factors of any tolerated level of influence on the outcome's mean or quantiles. Monte Carlo simulations demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15761v3</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songnian Chen, Junlong Feng</dc:creator>
    </item>
    <item>
      <title>A Unifying Framework for Robust and Efficient Inference with Unstructured Data</title>
      <link>https://arxiv.org/abs/2505.00282</link>
      <description>arXiv:2505.00282v3 Announce Type: replace 
Abstract: To analyze unstructured data (text, images, audio, video), economists typically first extract low-dimensional structured features with a neural network. Neural networks do not make generically unbiased predictions, and biases will propagate to estimators that use their predictions. While structured variables extracted from unstructured data have traditionally been treated as proxies - implicitly accepting arbitrary measurement error - this poses various challenges in an era where constantly evolving AI can cheaply extract data. Researcher degrees of freedom (e.g., the choice of neural network architecture, training data or prompts, and numerous implementation details) raise concerns about p-hacking and how to best show robustness, the frequent deprecation of proprietary neural networks complicates reproducibility, and researchers need a principled way to determine how accurate predictions need to be before making costly investments to improve them. To address these challenges, this study develops MAR-S (Missing At Random Structured Data), a semiparametric missing data framework that enables unbiased, efficient, and robust inference with unstructured data, by correcting for neural network prediction error with a validation sample. MAR-S synthesizes and extends existing methods for debiased inference using machine learning predictions and connects them to familiar problems such as causal inference, highlighting valuable parallels. We develop robust and efficient estimators for both descriptive and causal estimands and address inference with aggregated and transformed neural network predictions, a common scenario outside the existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00282v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Carlson, Melissa Dell</dc:creator>
    </item>
    <item>
      <title>Analytic inference with two-way clustering</title>
      <link>https://arxiv.org/abs/2506.20749</link>
      <description>arXiv:2506.20749v2 Announce Type: replace 
Abstract: This paper studies analytic inference along two dimensions of clustering. In such setups, the commonly used approach has two drawbacks. First, the corresponding variance estimator is not necessarily positive. Second, inference is invalid in non-Gaussian regimes, namely when the estimator of the parameter of interest is not asymptotically Gaussian. We consider a simple fix that addresses both issues. In Gaussian regimes, the corresponding tests are asymptotically exact and equivalent to usual ones. Otherwise, the new tests are asymptotically conservative. We also establish their uniform validity over a certain class of data generating processes. Independently of our tests, we highlight potential issues with multiple testing and nonlinear estimators under two-way clustering. Finally, we compare our approach with existing ones through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20749v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurent Davezies, Xavier D'Haultf{\oe}uille, Yannick Guyonvarch</dc:creator>
    </item>
  </channel>
</rss>
