<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 03:02:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LSD of the Commutator of two data Matrices</title>
      <link>https://arxiv.org/abs/2503.00014</link>
      <description>arXiv:2503.00014v1 Announce Type: new 
Abstract: We study the spectral properties of a class of random matrices of the form $S_n^{-} = n^{-1}(X_1 X_2^* - X_2 X_1^*)$ where $X_k = \Sigma_k^{1/2}Z_k$, $Z_k$'s are independent $p\times n$ complex-valued random matrices, and $\Sigma_k$ are $p\times p$ positive semi-definite matrices that commute and are independent of the $Z_k$'s for $k=1,2$. We assume that $Z_k$'s have independent entries with zero mean and unit variance. The skew-symmetric/skew-Hermitian matrix $S_n^{-}$ will be referred to as a random commutator matrix associated with the samples $X_1$ and $X_2$. We show that, when the dimension $p$ and sample size $n$ increase simultaneously, so that $p/n \to c \in (0,\infty)$, there exists a limiting spectral distribution (LSD) for $S_n^{-}$, supported on the imaginary axis, under the assumptions that the joint spectral distribution of $\Sigma_1, \Sigma_2$ converges weakly and the entries of $Z_k$'s have moments of sufficiently high order. This nonrandom LSD can be described through its Stieltjes transform, which satisfies a system of Mar\v{c}enko-Pastur-type functional equations. Moreover, we show that the companion matrix $S_n^{+} = n^{-1}(X_1X_2^* + X_2X_1^*)$, under identical assumptions, has an LSD supported on the real line, which can be similarly characterized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00014v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javed Hazarika, Debashis Paul</dc:creator>
    </item>
    <item>
      <title>Aspects of a Generalized Theory of Sparsity based Inference in Linear Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.00178</link>
      <description>arXiv:2503.00178v1 Announce Type: new 
Abstract: Linear inverse problems are ubiquitous in various science and engineering disciplines. Of particular importance in the past few decades, is the incorporation of sparsity based priors, in particular $\ell_1$ priors, into linear inverse problems, which led to the flowering of fields of compressive sensing (CS) and sparsity based signal processing. More recently, methods based on a Compound Gaussian (CG) prior have been investigated and demonstrate improved results over CS in practice. This paper is the first attempt to identify and elucidate the fundamental structures underlying the success of CG methods by studying CG in the context of a broader framework of generalized-sparsity-based-inference. After defining our notion of generalized sparsity we introduce a weak null space property and proceed to generalize two well-known methods in CS, basis pursuit and iteratively reweighted least squares (IRLS). We show how a subset of CG-induced regularizers fits into this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00178v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan O'Dowd, Raghu G. Raj, Hrushikesh N. Mhaskar</dc:creator>
    </item>
    <item>
      <title>A Few Observations on Sample-Conditional Coverage in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2503.00220</link>
      <description>arXiv:2503.00220v1 Announce Type: new 
Abstract: We revisit the problem of constructing predictive confidence sets for which we wish to obtain some type of conditional validity. We provide new arguments showing how ``split conformal'' methods achieve near desired coverage levels with high probability, a guarantee conditional on the validation data rather than marginal over it. In addition, we directly consider (approximate) conditional coverage, where, e.g., conditional on a covariate $X$ belonging to some group of interest, we would like a guarantee that a predictive set covers the true outcome $Y$. We show that the natural method of performing quantile regression on a held-out (validation) dataset yields minimax optimal guarantees of coverage here. Complementing these positive results, we also provide experimental evidence that interesting work remains to be done to develop computationally efficient but valid predictive inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00220v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John C. Duchi</dc:creator>
    </item>
    <item>
      <title>Geometric Ergodicity of a Gibbs Algorithm for a Normal Model With a Horseshoe Prior</title>
      <link>https://arxiv.org/abs/2503.00538</link>
      <description>arXiv:2503.00538v1 Announce Type: new 
Abstract: In this paper, we consider a two-stage Gibbs sampler for a normal linear regression model with a horseshoe prior. Under some assumptions, we show that it produces a geometrically ergodic Markov chain. In particular, we prove geometric ergodicity under some three-parameter beta global prior which does not have a finite $(p / 5)$-th negative moment, where $p$ is the number of regression coefficients. This is in contrast to the case of a known general result which is applicable if the global parameter has a finite approximately $(p / 2)$-th negative moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00538v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference</title>
      <link>https://arxiv.org/abs/2503.01708</link>
      <description>arXiv:2503.01708v1 Announce Type: new 
Abstract: We develop a pseudo-likelihood theory for rank one matrix estimation problems in the high dimensional limit. We prove a variational principle for the limiting pseudo-maximum likelihood which also characterizes the performance of the corresponding pseudo-maximum likelihood estimator. We show that this variational principle is universal and depends only on four parameters determined by the corresponding null model. Through this universality, we introduce a notion of equivalence for estimation problems of this type and, in particular, show that a broad class of estimation tasks, including community detection, sparse submatrix detection, and non-linear spiked matrix models, are equivalent to spiked matrix models. As an application, we obtain a complete description of the performance of the least-squares (or ``best rank one'') estimator for any rank one matrix estimation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01708v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Curtis Grant, Aukosh Jagannath, Justin Ko</dc:creator>
    </item>
    <item>
      <title>Uniform Limit Theory for Network Data</title>
      <link>https://arxiv.org/abs/2503.00290</link>
      <description>arXiv:2503.00290v1 Announce Type: cross 
Abstract: I present a novel uniform law of large numbers (ULLN) for network-dependent data. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive suite of limit theorems and a robust variance estimator for network-dependent processes, their analysis focuses on pointwise convergence. On the other hand, uniform convergence is essential for nonlinear estimators such as M and GMM estimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I establish the ULLN under network dependence and demonstrate its utility by proving the consistency of both M and GMM estimators. A byproduct of this work is a novel maximal inequality for network data, which may prove useful for future research beyond the scope of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00290v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>Semi-Parametric Batched Global Multi-Armed Bandits with Covariates</title>
      <link>https://arxiv.org/abs/2503.00565</link>
      <description>arXiv:2503.00565v1 Announce Type: cross 
Abstract: The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related rather than independent. We propose a novel semi-parametric framework for batched bandits with covariates and a shared parameter across arms, leveraging the single-index regression (SIR) model to capture relationships between arm rewards while balancing interpretability and flexibility. Our algorithm, Batched single-Index Dynamic binning and Successive arm elimination (BIDS), employs a batched successive arm elimination strategy with a dynamic binning mechanism guided by the single-index direction. We consider two settings: one where a pilot direction is available and another where the direction is estimated from data, deriving theoretical regret bounds for both cases. When a pilot direction is available with sufficient accuracy, our approach achieves minimax-optimal rates (with $d = 1$) for nonparametric batched bandits, circumventing the curse of dimensionality. Extensive experiments on simulated and real-world datasets demonstrate the effectiveness of our algorithm compared to the nonparametric batched bandit method introduced by \cite{jiang2024batched}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00565v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Hyebin Song</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory of Eigenvectors for Latent Embeddings with Generalized Laplacian Matrices</title>
      <link>https://arxiv.org/abs/2503.00640</link>
      <description>arXiv:2503.00640v1 Announce Type: cross 
Abstract: Laplacian matrices are commonly employed in many real applications, encoding the underlying latent structural information such as graphs and manifolds. The use of the normalization terms naturally gives rise to random matrices with dependency. It is well-known that dependency is a major bottleneck of new random matrix theory (RMT) developments. To this end, in this paper, we formally introduce a class of generalized (and regularized) Laplacian matrices, which contains the Laplacian matrix and the random adjacency matrix as a specific case, and suggest the new framework of the asymptotic theory of eigenvectors for latent embeddings with generalized Laplacian matrices (ATE-GL). Our new theory is empowered by the tool of generalized quadratic vector equation for dealing with RMT under dependency, and delicate high-order asymptotic expansions of the empirical spiked eigenvectors and eigenvalues based on local laws. The asymptotic normalities established for both spiked eigenvectors and eigenvalues will enable us to conduct precise inference and uncertainty quantification for applications involving the generalized Laplacian matrices with flexibility. We discuss some applications of the suggested ATE-GL framework and showcase its validity through some numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00640v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Yingying Fan, Jinchi Lv, Fan Yang, Diwen Yu</dc:creator>
    </item>
    <item>
      <title>Powerful rank verification for multivariate Gaussian data with any covariance structure</title>
      <link>https://arxiv.org/abs/2503.01065</link>
      <description>arXiv:2503.01065v1 Announce Type: cross 
Abstract: Upon observing $n$-dimensional multivariate Gaussian data, when can we infer that the largest $K$ observations came from the largest $K$ means? When $K=1$ and the covariance is isotropic, \cite{Gutmann} argue that this inference is justified when the two-sided difference-of-means test comparing the largest and second largest observation rejects. Leveraging tools from selective inference, we provide a generalization of their procedure that applies for both any $K$ and any covariance structure. We show that our procedure draws the desired inference whenever the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference rejects, and sometimes even when this test fails to reject. Using this insight, we argue that our procedure renders existing simultaneous inference approaches inadmissible when $n &gt; 2$. When the observations are independent (with possibly unequal variances) or equicorrelated, our procedure corresponds exactly to running the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01065v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anav Sood</dc:creator>
    </item>
    <item>
      <title>A direct extension of Azadkia &amp; Chatterjee's rank correlation to multi-response vectors</title>
      <link>https://arxiv.org/abs/2212.01621</link>
      <description>arXiv:2212.01621v4 Announce Type: replace 
Abstract: Recently, Chatterjee (2023) recognized the lack of a direct generalization of his rank correlation $\xi$ in Azadkia and Chatterjee (2021) to a multi-dimensional response vector. As a natural solution to this problem, we here propose an extension of $\xi$ that is applicable to a set of $q \geq 1$ response variables, where our approach builds upon converting the original vector-valued problem into a univariate problem and then applying the rank correlation $\xi$ to it. Our novel measure $T$ quantifies the scale-invariant extent of functional dependence of a response vector $\mathbf{Y} = (Y_1,\dots,Y_q)$ on predictor variables $\mathbf{X} = (X_1, \dots,X_p)$, characterizes independence of $\mathbf{X}$ and $\mathbf{Y}$ as well as perfect dependence of $\mathbf{Y}$ on $\mathbf{X}$ and hence fulfills all the characteristics of a measure of predictability. Aiming at maximum interpretability, we provide various invariance results for $T$ as well as a closed-form expression in multivariate normal models. Building upon the graph-based estimator for $\xi$ in Azadkia and Chatterjee (2021), we obtain a non-parametric, strongly consistent estimator for $T$ and show -- as a main contribution -- its asymptotic normality. Based on this estimator, we develop a model-free and rank-based feature ranking and forward feature selection for multiple-outcome data that works without any tuning parameters. Simulation results and real case studies illustrate $T$'s broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01621v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Sebastian Fuchs</dc:creator>
    </item>
    <item>
      <title>Semi-parametric inference based on adaptively collected data</title>
      <link>https://arxiv.org/abs/2303.02534</link>
      <description>arXiv:2303.02534v2 Announce Type: replace 
Abstract: Many standard estimators, when applied to adaptively collected data, fail to be asymptotically normal, thereby complicating the construction of confidence intervals. We address this challenge in a semi-parametric context: estimating the parameter vector of a generalized linear regression model contaminated by a non-parametric nuisance component. We construct suitably weighted estimating equations that account for adaptivity in data collection, and provide conditions under which the associated estimates are asymptotically normal. Our results characterize the degree of "explorability" required for asymptotic normality to hold. For the simpler problem of estimating a linear functional, we provide similar guarantees under much weaker assumptions. We illustrate our general theory with concrete consequences for various problems, including standard linear bandits and sparse generalized bandits, and compare with other methods via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02534v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Koulik Khamaru, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Estimation of on- and off-time distributions in a dynamic Erd\H{o}s-R\'enyi random graph</title>
      <link>https://arxiv.org/abs/2401.14531</link>
      <description>arXiv:2401.14531v4 Announce Type: replace 
Abstract: In this paper we consider a dynamic Erd\H{o}s-R\'enyi graph in which edges, according to an alternating renewal process, change from present to absent and vice versa. The objective is to estimate the on- and off-time distributions while only observing the aggregate number of edges. This inverse problem is dealt with, in a parametric context, by setting up an estimator based on the method of moments. We provide conditions under which the estimator is asymptotically normal, and we point out how the corresponding covariance matrix can be identified. It is also demonstrated how to adapt the estimation procedure if alternative subgraph counts are observed, such as the number of wedges or triangles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14531v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Mandjes, Jiesen Wang</dc:creator>
    </item>
    <item>
      <title>Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation</title>
      <link>https://arxiv.org/abs/2401.15262</link>
      <description>arXiv:2401.15262v2 Announce Type: replace 
Abstract: Adversarial training has been proposed to protect machine learning models against adversarial attacks. This paper focuses on adversarial training under $\ell_\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the asymptotic distribution of the adversarial training estimator under $\ell_\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -- adaptive adversarial training, which could further improve the performance of adversarial training under $\ell_\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic variable-selection consistency and unbiasedness. Numerical experiments are conducted to show the sparsity-recovery ability of adversarial training under $\ell_\infty$-perturbation and to compare the empirical performance between classic adversarial training and adaptive adversarial training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15262v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Xie, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Testing Elliptical Models in High Dimensions</title>
      <link>https://arxiv.org/abs/2408.05514</link>
      <description>arXiv:2408.05514v2 Announce Type: replace 
Abstract: Due to the broad applications of elliptical models, there is a long line of research on goodness-of-fit tests for empirically validating them. However, the existing literature on this topic is generally confined to low-dimensional settings, and to the best of our knowledge, there are no established goodness-of-fit tests for elliptical models that are supported by theoretical guarantees in high dimensions. In this paper, we propose a new goodness-of-fit test for this problem, and our main result shows that the test is asymptotically valid when the dimension and sample size diverge proportionally. Remarkably, it also turns out that the asymptotic validity of the test requires no assumptions on the population covariance matrix. With regard to numerical performance, we confirm that the empirical level of the test is close to the nominal level across a range of conditions, and that the test is able to reliably detect non-elliptical distributions. Moreover, when the proposed test is specialized to the problem of testing normality in high dimensions, we show that it compares favorably with a state-of-the-art method, and hence, this way of using the proposed test is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05514v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyao Wang, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>Linear cost and exponentially convergent approximation of Gaussian Mat\'ern processes on intervals</title>
      <link>https://arxiv.org/abs/2410.13000</link>
      <description>arXiv:2410.13000v2 Announce Type: replace 
Abstract: The computational cost for inference and prediction of statistical models based on Gaussian processes with Mat\'ern covariance functions scales cubicly with the number of observations, limiting their applicability to large data sets. The cost can be reduced in certain special cases, but there are currently no generally applicable exact methods with linear cost. Several approximate methods have been introduced to reduce the cost, but most of these lack theoretical guarantees for the accuracy. We consider Gaussian processes on bounded intervals with Mat\'ern covariance functions and for the first time develop a generally applicable method with linear cost and with a covariance error that decreases exponentially fast in the order $m$ of the proposed approximation. The method is based on an optimal rational approximation of the spectral density and results in an approximation that can be represented as a sum of $m$ independent Gaussian Markov processes, which facilitates easy usage in general software for statistical inference, enabling its efficient implementation in general statistical inference software packages. Besides the theoretical justifications, we demonstrate the accuracy empirically through carefully designed simulation studies which show that the method outperforms all state-of-the-art alternatives in terms of accuracy for a fixed computational cost in statistical tasks such as Gaussian process regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13000v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Vaibhav Mehandiratta, Alexandre B. Simas</dc:creator>
    </item>
    <item>
      <title>Bayesian calculus and predictive characterizations of extended feature allocation models</title>
      <link>https://arxiv.org/abs/2502.10257</link>
      <description>arXiv:2502.10257v2 Announce Type: replace 
Abstract: We introduce and study a unified Bayesian framework for extended feature allocations which flexibly captures interactions -- such as repulsion or attraction -- among features and their associated weights. We provide a complete Bayesian analysis of the proposed model and specialize our general theory to noteworthy classes of priors. This includes a novel prior based on determinantal point processes, for which we show promising results in a spatial statistics application. Within the general class of extended feature allocations, we further characterize those priors that yield predictive probabilities of discovering new features depending either solely on the sample size or on both the sample size and the distinct number of observed features. These predictive characterizations, known as "sufficientness" postulates, have been extensively studied in the literature on species sampling models starting from the seminal contribution of the English philosopher W.E. Johnson for the Dirichlet distribution. Within the feature allocation setting, existing predictive characterizations are limited to very specific examples; in contrast, our results are general, providing practical guidance for prior selection. Additionally, our approach, based on Palm calculus, is analytical in nature and yields a novel characterization of the Poisson point process through its reduced Palm kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10257v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Federico Camerlenghi, Lorenzo Ghilotti</dc:creator>
    </item>
    <item>
      <title>On a class of high dimensional linear regression methods with debiasing and thresholding</title>
      <link>https://arxiv.org/abs/2502.17261</link>
      <description>arXiv:2502.17261v2 Announce Type: replace 
Abstract: In this paper, we introduce a unified framework, inspired by classical regularization theory, for designing and analyzing a broad class of linear regression approaches. Our framework encompasses traditional methods like least squares regression and Ridge regression, as well as innovative techniques, including seven novel regression methods such as Landweber and Showalter regressions. Within this framework, we further propose a class of debiased and thresholded regression methods to promote feature selection, particularly in terms of sparsity. These methods may offer advantages over conventional regression techniques, including Lasso, due to their ease of computation via a closed-form expression. Theoretically, we establish consistency results and Gaussian approximation theorems for this new class of regularization methods. Extensive numerical simulations further demonstrate that the debiased and thresholded counterparts of linear regression methods exhibit favorable finite sample performance and may be preferable in certain settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17261v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Ao Wang, Yunyi Zhang, Ye Zhang</dc:creator>
    </item>
    <item>
      <title>On the Glivenko-Cantelli theorem for real-valued empirical functions of stationary $\alpha$-mixing and $\beta$-mixing sequences</title>
      <link>https://arxiv.org/abs/2502.20206</link>
      <description>arXiv:2502.20206v3 Announce Type: replace 
Abstract: In this paper we extend the classical Glivenko-Cantelli theorem to real-valued empirical functions under dependence structures characterised by $\alpha$-mixing and $\beta$-mixing conditions. We investigate sufficient conditions ensuring that families of real-valued functions exhibit the Glivenko-Cantelli (GC) property in these dependence settings. Our analysis focuses on function classes satisfying uniform entropy conditions, and we establish conditions on mixing coefficients to obtain GC theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20206v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ousmane Coulibaly, Harouna Sangar\'e</dc:creator>
    </item>
    <item>
      <title>Optimal Decision Rules Under Partial Identification</title>
      <link>https://arxiv.org/abs/2111.04926</link>
      <description>arXiv:2111.04926v4 Announce Type: replace-cross 
Abstract: I consider a class of statistical decision problems in which the policymaker must decide between two policies to maximize social welfare (e.g., the population mean of an outcome) based on a finite sample. The framework introduced in this paper allows for various types of restrictions on the structural parameter (e.g., the smoothness of a conditional mean potential outcome function) and accommodates settings with partial identification of social welfare. As the main theoretical result, I derive a finite-sample optimal decision rule under the minimax regret criterion. This rule has a simple form, yet achieves optimality among all decision rules; no ad hoc restrictions are imposed on the class of decision rules. I apply my results to the problem of whether to change an eligibility cutoff in a regression discontinuity setup, and illustrate them in an empirical application to a school construction program in Burkina Faso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04926v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Identifying Drift, Diffusion, and Causal Structure from Temporal Snapshots</title>
      <link>https://arxiv.org/abs/2410.22729</link>
      <description>arXiv:2410.22729v2 Announce Type: replace-cross 
Abstract: Stochastic differential equations (SDEs) are a fundamental tool for modelling dynamic processes, including gene regulatory networks (GRNs), contaminant transport, financial markets, and image generation. However, learning the underlying SDE from data is a challenging task, especially if individual trajectories are not observable. Motivated by burgeoning research in single-cell datasets, we present the first comprehensive approach for jointly identifying the drift and diffusion of an SDE from its temporal marginals. Assuming linear drift and additive diffusion, we prove that these parameters are identifiable from marginals if and only if the initial distribution lacks any generalized rotational symmetries. We further prove that the causal graph of any SDE with additive diffusion can be recovered from the SDE parameters. To complement this theory, we adapt entropy-regularized optimal transport to handle anisotropic diffusion, and introduce APPEX (Alternating Projection Parameter Estimation from $X_0$), an iterative algorithm designed to estimate the drift, diffusion, and causal graph of an additive noise SDE, solely from temporal marginals. We show that APPEX iteratively decreases Kullback-Leibler divergence to the true solution, and demonstrate its effectiveness on simulated data from linear additive noise SDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22729v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Guan, Joseph Janssen, Hossein Rahmani, Andrew Warren, Stephen Zhang, Elina Robeva, Geoffrey Schiebinger</dc:creator>
    </item>
  </channel>
</rss>
