<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:52:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating Coverage in Streams via a Modified CVM Method</title>
      <link>https://arxiv.org/abs/2504.04567</link>
      <description>arXiv:2504.04567v1 Announce Type: new 
Abstract: When individuals in a population can be classified in classes or categories, the coverage of a sample, $C$, is defined as the probability that a randomly selected individual from the population belongs to a class represented in the sample. Estimating coverage is challenging because $C$ is not a fixed population parameter, but a property of the sample, and the task becomes more complex when the number of classes is unknown. Furthermore, this problem has not been addressed in scenarios where data arrive as a stream, under the constraint that only $n$ elements can be stored at a time. In this paper, we propose a simple and efficient method to estimate $C$ in streaming settings, based on a straightforward modification of the CVM algorithm, which is commonly used to estimate the number of distinct elements in a data stream.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04567v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Hernandez-Suarez</dc:creator>
    </item>
    <item>
      <title>MaxTDA: Robust Statistical Inference for Maximal Persistence in Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2504.03897</link>
      <description>arXiv:2504.03897v1 Announce Type: cross 
Abstract: Persistent homology is an area within topological data analysis (TDA) that can uncover different dimensional holes (connected components, loops, voids, etc.) in data. The holes are characterized, in part, by how long they persist across different scales. Noisy data can result in many additional holes that are not true topological signal. Various robust TDA techniques have been proposed to reduce the number of noisy holes, however, these robust methods have a tendency to also reduce the topological signal. This work introduces Maximal TDA (MaxTDA), a statistical framework addressing a limitation in TDA wherein robust inference techniques systematically underestimate the persistence of significant homological features. MaxTDA combines kernel density estimation with level-set thresholding via rejection sampling to generate consistent estimators for the maximal persistence features that minimizes bias while maintaining robustness to noise and outliers. We establish the consistency of the sampling procedure and the stability of the maximal persistence estimator. The framework also enables statistical inference on topological features through rejection bands, constructed from quantiles that bound the estimator's deviation probability. MaxTDA is particularly valuable in applications where precise quantification of statistically significant topological features is essential for revealing underlying structural properties in complex datasets. Numerical simulations across varied datasets, including an example from exoplanet astronomy, highlight the effectiveness of MaxTDA in recovering true topological signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03897v1</guid>
      <category>stat.ME</category>
      <category>math.AT</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sixtus Dakurah, Jessi Cisewski-Kehe</dc:creator>
    </item>
    <item>
      <title>nonprobsvy -- An R package for modern methods for non-probability surveys</title>
      <link>https://arxiv.org/abs/2504.04255</link>
      <description>arXiv:2504.04255v1 Announce Type: cross 
Abstract: The following paper presents {nonprobsvy} -- an {R} package for inference based on non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. In the package, we assume the existence of either population-level data or probability-based population information and leverage the \pkg{survey} package for inference. The package implements both analytical and bootstrap variance estimation for the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at scientists and researchers who would like to use non-probability samples (e.g.big data, opt-in web panels, social media) to accurately estimate population characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04255v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Chrostowski, Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>BlockingPy: approximate nearest neighbours for blocking of records for entity resolution</title>
      <link>https://arxiv.org/abs/2504.04266</link>
      <description>arXiv:2504.04266v1 Announce Type: cross 
Abstract: Entity resolution (probabilistic record linkage, deduplication) is a key step in scientific analysis and data science pipelines involving multiple data sources. The objective of entity resolution is to link records without identifiers that refer to the same entity (e.g., person, company). However, without identifiers, researchers need to specify which records to compare in order to calculate matching probability and reduce computational complexity. One solution is to deterministically block records based on some common variables, such as names, dates of birth or sex. However, this approach assumes that these variables are free of errors and completely observed, which is often not the case. To address this challenge, we have developed a Python package, BlockingPy, which utilises blocking via modern approximate nearest neighbour search and graph algorithms to significantly reduce the number of comparisons. In this paper, we present the design of the package, its functionalities and two case studies related to official statistics. We believe the presented software will be useful for researchers (i.e., social scientists, economists or statisticians) interested in linking data from various sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04266v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tymoteusz Strojny, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Efficient Rejection Sampling in the Entropy-Optimal Range</title>
      <link>https://arxiv.org/abs/2504.04267</link>
      <description>arXiv:2504.04267v1 Announce Type: cross 
Abstract: The problem of generating a random variate $X$ from a finite discrete probability distribution $P$ using an entropy source of independent unbiased coin flips is considered. The Knuth and Yao complexity theory of nonuniform random number generation furnishes a family of "entropy-optimal" sampling algorithms that consume between $H(P)$ and $H(P)+2$ coin flips per generated output, where $H$ is the Shannon entropy function. However, the space complexity of entropy-optimal samplers scales exponentially with the number of bits required to encode $P$. This article introduces a family of efficient rejection samplers and characterizes their entropy, space, and time complexity. Within this family is a distinguished sampling algorithm that requires linearithmic space and preprocessing time, and whose expected entropy cost always falls in the entropy-optimal range $[H(P), H(P)+2)$. No previous sampler for discrete probability distributions is known to achieve these characteristics. Numerical experiments demonstrate performance improvements in runtime and entropy of the proposed algorithm compared to the celebrated alias method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04267v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas L. Draper, Feras A. Saad</dc:creator>
    </item>
    <item>
      <title>How Untested Modeling Assumptions Influence the U.S. EPA's Estimates of Population-Level Ozone Exposure Risk</title>
      <link>https://arxiv.org/abs/2504.04591</link>
      <description>arXiv:2504.04591v1 Announce Type: cross 
Abstract: In recent reviews of the National Ambient Air Quality Standards (NAAQS) for ozone, the U.S. Environmental Protection Agency (U.S. EPA) has presented estimates of the health risks associated with ozone exposure. One way in which the U.S. EPA calculates population-level ozone risk estimates is through a simulation model that calculates ozone exposures and the resulting lung function decrements for a simulated population. This simulation model includes several random error terms to capture inter- and intra-individual variability in responsiveness to ozone exposure. In this manuscript we undertake a sensitivity analysis examining the influence of untested assumptions about these error terms. We show that ad hoc bounds imposed on the error terms and the frequency of redrawing the intra-individual error terms have a strong influence on the population-level ozone exposure risk reported by the U.S. EPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04591v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Garrett Glasgow, Anne E. Smith</dc:creator>
    </item>
    <item>
      <title>Robustifying Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2504.04733</link>
      <description>arXiv:2504.04733v1 Announce Type: cross 
Abstract: Approximate Bayesian computation (ABC) is one of the most popular "likelihood-free" methods. These methods have been applied in a wide range of fields by providing solutions to intractable likelihood problems in which exact Bayesian approaches are either infeasible or computationally costly. However, the performance of ABC can be unreliable when dealing with model misspecification. To circumvent the poor behavior of ABC in these settings, we propose a novel ABC approach that is robust to model misspecification. This new method can deliver more accurate statistical inference under model misspecification than alternatives and also enables the detection of summary statistics that are incompatible with the assumed data-generating process. We demonstrate the effectiveness of our approach through several simulated examples, where it delivers more accurate point estimates and uncertainty quantification over standard ABC approaches when the model is misspecified. Additionally, we apply our approach to an empirical example, further showcasing its advantages over alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04733v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaya Weerasinghe, David T. Frazier, Ruben Loaiza-Maya, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Eigenvalue-Based Randomness Test for Residual Diagnostics in Panel Data Models</title>
      <link>https://arxiv.org/abs/2504.05297</link>
      <description>arXiv:2504.05297v1 Announce Type: cross 
Abstract: This paper introduces the Eigenvalue-Based Randomness (EBR) test - a novel approach rooted in the Tracy-Widom law from random matrix theory - and applies it to the context of residual analysis in panel data models. Unlike traditional methods, which target specific issues like cross-sectional dependence or autocorrelation, the EBR test simultaneously examines multiple assumptions by analyzing the largest eigenvalue of a symmetrized residual matrix. Monte Carlo simulations demonstrate that the EBR test is particularly robust in detecting not only standard violations such as autocorrelation and linear cross-sectional dependence (CSD) but also more intricate non-linear and non-monotonic dependencies, making it a comprehensive and highly flexible tool for enhancing the reliability of panel data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05297v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcell T. Kurbucz, Betsab\'e P\'erez Garrido, Antal Jakov\'ac</dc:creator>
    </item>
    <item>
      <title>Linear-Cost Vecchia Approximation of Multivariate Normal Probabilities</title>
      <link>https://arxiv.org/abs/2311.09426</link>
      <description>arXiv:2311.09426v3 Announce Type: replace 
Abstract: Multivariate normal (MVN) probabilities arise in myriad applications, but they are analytically intractable and need to be evaluated via Monte-Carlo-based numerical integration. For the state-of-the-art minimax exponential tilting (MET) method, we show that the complexity of each of its components can be greatly reduced through an integrand parameterization that utilizes the sparse inverse Cholesky factor produced by the Vecchia approximation, whose approximation error is often negligible relative to the Monte-Carlo error. Based on this idea, we derive algorithms that can estimate MVN probabilities and sample from truncated MVN distributions in linear time (and that are easily parallelizable) at the same convergence or acceptance rate as MET, whose complexity is cubic in the dimension of the MVN probability. We showcase the advantages of our methods relative to existing approaches using several simulated examples. We also analyze a groundwater-contamination dataset with over twenty thousand censored measurements to demonstrate the scalability of our method for partially censored Gaussian-process models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09426v3</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Cao, Matthias Katzfuss</dc:creator>
    </item>
    <item>
      <title>AutoStep: Locally adaptive involutive MCMC</title>
      <link>https://arxiv.org/abs/2410.18929</link>
      <description>arXiv:2410.18929v2 Announce Type: replace 
Abstract: Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using a deterministic involutive proposal with a step size parameter. Selecting an appropriate step size is often a challenging task in practice; and for complex multiscale targets, there may not be one choice of step size that works well globally. In this work, we address this problem with a novel class of involutive MCMC methods -- AutoStep MCMC -- that selects an appropriate step size at each iteration adapted to the local geometry of the target distribution. We prove that AutoStep MCMC is $\pi$-invariant and has other desirable properties under mild assumptions on the target distribution $\pi$ and involutive proposal. Empirical results examine the effect of various step size selection design choices, and show that AutoStep MCMC is competitive with state-of-the-art methods in terms of effective sample size per unit cost on a range of challenging target distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18929v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiange Liu, Nikola Surjanovic, Miguel Biron-Lattes, Alexandre Bouchard-C\^ot\'e, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>CDsampling: An R Package for Constrained D-Optimal Sampling in Paid Research Studies</title>
      <link>https://arxiv.org/abs/2410.20606</link>
      <description>arXiv:2410.20606v3 Announce Type: replace 
Abstract: In the context of paid research studies and clinical trials, budget considerations often require patient sampling from available populations which comes with inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, the package revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20606v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Huang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Sampling with time-changed Markov processes</title>
      <link>https://arxiv.org/abs/2501.15155</link>
      <description>arXiv:2501.15155v2 Announce Type: replace 
Abstract: We study time-changed Markov processes to speed up the convergence of Markov chain Monte Carlo (MCMC) algorithms. The time-changed process is defined by adjusting the speed of time of a base process via a user-chosen, state-dependent function. We explore the properties of such transformations and apply this idea to several Markov processes from the MCMC literature, such as Langevin diffusions and piecewise deterministic Markov processes, obtaining novel modifications of classical algorithms and also re-discovering known MCMC algorithms. We prove theoretical properties of the time-changed process under suitable conditions on the base process, focusing on connecting the stationary distributions and qualitative convergence properties such as geometric and uniform ergodicity, as well as a functional central limit theorem. We also provide a comparison with the framework of space transformations, clarifying the similarities between the approaches. Throughout the paper we give various visualisations and numerical simulations on simple tasks to gain intuition on the method and its performance. Finally, we provide numerical simulations to gain intuition on the method and its performance on benchmark problems. Our results indicate a performance improvement in the context of multimodal distributions and rare event simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15155v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bertazzi, Giorgos Vasdekis</dc:creator>
    </item>
    <item>
      <title>Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions</title>
      <link>https://arxiv.org/abs/2108.11328</link>
      <description>arXiv:2108.11328v5 Announce Type: replace-cross 
Abstract: In this paper, we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application, which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition (Erdman and Bates, 2016) organized more than ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with a small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study our estimator's computational and statistical aspects and discuss variants incorporating strong hierarchical interactions. Our algorithms (open-sourced on GitHub) extend the computational frontiers of existing algorithms for sparse additive models to be able to handle datasets relevant to the application we consider. We discuss and interpret findings from our model on the US Census Planning Database. In addition to being useful from an interpretability standpoint, our models lead to predictions comparable to popular black-box machine learning methods based on gradient boosting and feedforward neural networks - suggesting that it is possible to have models that have the best of both worlds: good model accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.11328v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1929</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Applied Statistics 2025, Vol. 19, No. 1, 94-120</arxiv:journal_reference>
      <dc:creator>Shibal Ibrahim, Peter Radchenko, Emanuel Ben-David, Rahul Mazumder</dc:creator>
    </item>
    <item>
      <title>Embrace rejection: Kernel matrix approximation by accelerated randomly pivoted Cholesky</title>
      <link>https://arxiv.org/abs/2410.03969</link>
      <description>arXiv:2410.03969v3 Announce Type: replace-cross 
Abstract: Randomly pivoted Cholesky (RPCholesky) is an algorithm for constructing a low-rank approximation of a positive-semidefinite matrix using a small number of columns. This paper develops an accelerated version of RPCholesky that employs block matrix computations and rejection sampling to efficiently simulate the execution of the original algorithm. For the task of approximating a kernel matrix, the accelerated algorithm can run over $40\times$ faster. The paper contains implementation details, theoretical guarantees, experiments on benchmark data sets, and an application to computational chemistry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03969v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan N. Epperly, Joel A. Tropp, Robert J. Webber</dc:creator>
    </item>
    <item>
      <title>CMHSU: An R Statistical Software Package to Detect Mental Health Status, Substance Use Status, and their Concurrent Status in the North American Healthcare Administrative Databases</title>
      <link>https://arxiv.org/abs/2501.06435</link>
      <description>arXiv:2501.06435v4 Announce Type: replace-cross 
Abstract: The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU, available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example using a simulated real-world dataset is presented to examine various analytical aspects, including three key dimensions of MHSU detection based on the DDDM framework, as well as temporal analysis to demonstrate the package's application for healthcare policymakers. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06435v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Soltanifar, Chel Hee Lee</dc:creator>
    </item>
  </channel>
</rss>
