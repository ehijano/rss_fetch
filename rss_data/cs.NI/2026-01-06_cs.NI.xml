<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jan 2026 02:32:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving the Graph Challenge Reference Implementation</title>
      <link>https://arxiv.org/abs/2601.00974</link>
      <description>arXiv:2601.00974v1 Announce Type: new 
Abstract: The MIT/IEEE/Amazon Graph Challenge provides a venue for individuals and teams to showcase new innovations in large-scale graph and sparse data analysis. The Anonymized Network Sensing Graph Challenge processes over 100 billion network packets to construct privacy-preserving traffic matrices, with a GraphBLAS reference implementation demonstrating how hypersparse matrices can be applied to this problem. This work presents a refactoring and benchmarking of a section of the reference code to improve clarity, adaptability, and performance. The original Python implementation spanning approximately 1000 lines across 3 files has been streamlined to 325 lines across two focused modules, achieving a 67% reduction in code size while maintaining full functionality. Using pMatlab and pPython distributed array programming libraries, the addition of parallel maps allowed for parallel benchmarking of the data. Scalable performance is demonstrated for large-scale summation and analysis of traffic matrices. The resulting implementation increases the potential impact of the Graph Challenge by providing a clear and efficient foundation for participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00974v1</guid>
      <category>cs.NI</category>
      <category>cs.DM</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Inna Voloshchuk, Hayden Jananthan, Chansup Byun, Jeremy Kepner</dc:creator>
    </item>
    <item>
      <title>Decision-Aware Semantic State Synchronization in Compute-First Networking</title>
      <link>https://arxiv.org/abs/2601.01086</link>
      <description>arXiv:2601.01086v1 Announce Type: new 
Abstract: In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01086v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianpeng Qi, Chao Liu, Chengrui Wang, Rui Wang, Junyu Dong, Yanwei Yu</dc:creator>
    </item>
    <item>
      <title>Utility Maximization in Wireless Backhaul Networks with Service Guarantees</title>
      <link>https://arxiv.org/abs/2601.01630</link>
      <description>arXiv:2601.01630v1 Announce Type: new 
Abstract: We consider the problem of maximizing utility in wireless backhaul networks, where utility is a function of satisfied service level agreements (SLAs), defined in terms of end-to-end packet delays and instantaneous throughput. We model backhaul networks as a tree topology and show that SLAs can be satisfied by constructing link schedules with bounded inter-scheduling times, an NP-complete problem known as pinwheel scheduling. For symmetric tree topologies, we show that simple round-robin schedules can be optimal under certain conditions. In the general case, we develop a mixed-integer program that optimizes over the set of admission decisions and pinwheel schedules. We develop a novel pinwheel scheduling algorithm, which significantly expands the set of schedules that can be found in polynomial time over the state of the art. Using conditions from this algorithm, we develop a scalable, distributed approach to solve the utility-maximization problem, with complexity that is linear in the depth of the tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01630v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Jones, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Revisiting the Interface between Error and Erasure Correction in Wireless Standards</title>
      <link>https://arxiv.org/abs/2601.01645</link>
      <description>arXiv:2601.01645v1 Announce Type: new 
Abstract: Modern 5G communication systems implement a combination of error correction and feedback-based erasure correction (HARQ/ARQ) as reliability mechanisms, which can introduce substantial delay and resource inefficiency. We propose forward erasure correction using network coding as a more delay-efficient alternative. We present a mathematical characterization of network delay for existing reliability mechanisms and network coding. Through simulations in a network slicing environment, we demonstrate that network coding not only improves the in-order delivery delay and goodput for the applications utilizing the slice, but also benefits other applications sharing the network by reducing resource utilization for the coded slice. Our analysis and characterization point towards ideas that require attention in the 6G standardization process. These findings highlight the need for greater modularity in protocol stack design that enables the integration of novel technologies in future wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01645v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vipindev Adat Vasudevan, Homa Esfahanizadeh, Benjamin D. Kim, Laura Landon, Alejandro Cohen, Muriel M\'edard</dc:creator>
    </item>
    <item>
      <title>Enhanced Open-Source NWDAF for Event-Driven Analytics in 5G Networks</title>
      <link>https://arxiv.org/abs/2601.01838</link>
      <description>arXiv:2601.01838v1 Announce Type: new 
Abstract: The network data analytics function (NWDAF) has been introduced in the fifth-generation (5G) core standards to enable event-driven analytics and support intelligent network automation. However, existing implementations remain largely proprietary, and open-source alternatives lack comprehensive support for end-to-end event subscription and notification. In this paper, we present an open source NWDAF framework integrated into an existing Free5GC implementation, which serves as an open-source 5G core implementation. Our implementation extends the session management function to support standardized event exposure interfaces and introduces custom-built notification mechanisms into the SMF and the access and mobility management function for seamless data delivery. The NWDAF subscribes to events and generates analytics on user equipment (UE) behavior, session lifecycle, and handover dynamics. We validate our system through a two-week deployment involving four virtual next-generation NodeBs (gNBs) and multiple virtual UEs with dynamic mobility patterns. To demonstrate predictive capabilities, we incorporate a mobility-aware module that achieves 80.65\% accuracy in forecasting the next gNB handover cell. The framework supports reliable UE registration, state tracking, and cross-cell handovers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01838v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henok Daniel, Omar Alhussein, Jie Liang, Cheng Li, Ernesto Damiani</dc:creator>
    </item>
    <item>
      <title>Near-Field Multi-Cell ISCAP with Extremely Large-Scale Antenna Array</title>
      <link>https://arxiv.org/abs/2601.01968</link>
      <description>arXiv:2601.01968v1 Announce Type: new 
Abstract: This paper investigates a coordinated multi-cell integrated sensing, communication, and powering (ISCAP) system operating in the electromagnetic near field, where each base station (BS) employs an extremely large-scale antenna array (ELAA) to simultaneously support downlink communication, wireless power transfer (WPT), and environmental sensing. Three categories of communication users (CUs) with different interference cancellation capabilities are considered, and sensing is enabled through a distributed multiple-input multiple-output (MIMO) radar architecture. To address the resulting design challenges, a robust optimization framework is proposed by optimizing the beamforming strategy to maximize the worst-case detection probability over a prescribed sensing region, subject to per-user signal-to-interference-plus-noise ratio (SINR) constraints and energy harvesting requirements at energy receivers (ERs), while explicitly capturing the uncertainty in ER locations. By leveraging semidefinite relaxation (SDR), the original non-convex problem is reformulated as a convex semidefinite program with a provably tight relaxation. Furthermore, a low-complexity maximum ratio transmission (MRT)-based suboptimal scheme is developed, yielding a closed-form solution in the asymptotic regime as the number of antenna elements approaches infinity. Extensive numerical results reveal the fundamental trade-offs among sensing accuracy, communication reliability, and WPT efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01968v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Guo, Yilong Chen, Zixiang Ren, Derrick Wing Kwan Ng, Jie Xu</dc:creator>
    </item>
    <item>
      <title>AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI</title>
      <link>https://arxiv.org/abs/2601.02021</link>
      <description>arXiv:2601.02021v1 Announce Type: new 
Abstract: The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02021v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Runze Zheng, Yuqing Zheng, Zhengyi Cheng, Long Luo, Haoxiang Luo, Gang Sun, Hongfang Yu, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Enabling Deep Reinforcement Learning Research for Energy Saving in Open RAN</title>
      <link>https://arxiv.org/abs/2601.02240</link>
      <description>arXiv:2601.02240v2 Announce Type: new 
Abstract: The growing performance demands and higher deployment densities of next-generation wireless systems emphasize the importance of adopting strategies to manage the energy efficiency of mobile networks. In this demo, we showcase a framework that enables research on Deep Reinforcement Learning (DRL) techniques for improving the energy efficiency of intelligent and programmable Open Radio Access Network (RAN) systems. Using the open-source simulator ns-O-RAN and the reinforcement learning environment Gymnasium, the framework enables to train and evaluate DRL agents that dynamically control the activation and deactivation of cells in a 5G network. We show how to collect data for training and evaluate the impact of DRL on energy efficiency in a realistic 5G network scenario, including users' mobility and handovers, a full protocol stack, and 3rd Generation Partnership Project (3GPP)-compliant channel models. The tool will be open-sourced and a tutorial for energy efficiency testing in ns-O-RAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02240v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CCNC54725.2025.10975928</arxiv:DOI>
      <dc:creator>Matteo Bordin, Andrea Lacava, Michele Polese, Francesca Cuomo, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL</title>
      <link>https://arxiv.org/abs/2601.01209</link>
      <description>arXiv:2601.01209v1 Announce Type: cross 
Abstract: Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01209v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Tan, Yicheng Feng, Yu Zhou, Yimin Jiang, Yibo Zhu, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Automated SBOM-Driven Vulnerability Triage for IoT Firmware: A Lightweight Pipeline for Risk Prioritization</title>
      <link>https://arxiv.org/abs/2601.01308</link>
      <description>arXiv:2601.01308v1 Announce Type: cross 
Abstract: The proliferation of Internet of Things (IoT) devices has introduced significant security challenges, primarily due to the opacity of firmware components and the complexity of supply chain dependencies. IoT firmware frequently relies on outdated, third-party libraries embedded within monolithic binary blobs, making vulnerability management difficult. While Software Bill of Materials (SBOM) standards have matured, generating actionable intelligence from raw firmware dumps remains a manual and error-prone process. This paper presents a lightweight, automated pipeline designed to extract file systems from Linux-based IoT firmware, generate a comprehensive SBOM, map identified components to known vulnerabilities, and apply a multi-factor triage scoring model. The proposed system focuses on risk prioritization by integrating signals from the Common Vulnerability Scoring System (CVSS), Exploit Prediction Scoring System (EPSS), and the CISA Known Exploited Vulnerabilities (KEV) catalog. Unlike conventional scanners that produce high volumes of uncontextualized alerts, this approach emphasizes triage by calculating a localized risk score for each finding. We describe the architecture, the normalization challenges of embedded Linux, and a scoring methodology intended to reduce alert fatigue. The study outlines a planned evaluation strategy to validate the extraction success rate and triage efficacy using a dataset of public vendor firmware, offering a reproducibility framework for future research in firmware security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01308v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdurrahman Tolay</dc:creator>
    </item>
    <item>
      <title>Benchmarking Quantum Data Center Architectures: A Performance and Scalability Perspective</title>
      <link>https://arxiv.org/abs/2601.01353</link>
      <description>arXiv:2601.01353v1 Announce Type: cross 
Abstract: Scalable distributed quantum computing (DQC) has motivated the design of multiple quantum data-center (QDC) architectures that overcome the limitations of single quantum processors through modular interconnection. While these architectures adopt fundamentally different design philosophies, their relative performance under realistic quantum hardware constraints remains poorly understood.
  In this paper, we present a systematic benchmarking study of four representative QDC architectures-QFly, BCube, Clos, and Fat-Tree-quantifying their impact on distributed quantum circuit execution latency, resource contention, and scalability. Focusing on quantum-specific effects absent from classical data-center evaluations, we analyze how optical-loss-induced Einstein-Podolsky-Rosen (EPR) pair generation delays, coherence-limited entanglement retry windows, and contention from teleportation-based non-local gates shape end-to-end execution performance. Across diverse circuit workloads, we evaluate how architectural properties such as path diversity and path length, and shared BSM (Bell State Measurement) resources interact with optical-switch insertion loss and reconfiguration delay. Our results show that distributed quantum performance is jointly shaped by topology, scheduling policies, and physical-layer parameters, and that these factors interact in nontrivial ways. Together, these insights provide quantitative guidance for the design of scalable and high-performance quantum data-center architectures for DQC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01353v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahrooz Pouryousef, Eneet Kaur, Hassan Shapourian, Don Towsley, Ramana Kompella, Reza Nejabati</dc:creator>
    </item>
    <item>
      <title>Rethinking Secure Semantic Communications in the Age of Generative and Agentic AI: Threats and Opportunities</title>
      <link>https://arxiv.org/abs/2601.01791</link>
      <description>arXiv:2601.01791v2 Announce Type: cross 
Abstract: Semantic communication (SemCom) improves communication efficiency by transmitting task-relevant information instead of raw bits and is expected to be a key technology for 6G networks. Recent advances in generative AI (GenAI) further enhance SemCom by enabling robust semantic encoding and decoding under limited channel conditions. However, these efficiency gains also introduce new security and privacy vulnerabilities. Due to the broadcast nature of wireless channels, eavesdroppers can also use powerful GenAI-based semantic decoders to recover private information from intercepted signals. Moreover, rapid advances in agentic AI enable eavesdroppers to perform long-term and adaptive inference through the integration of memory, external knowledge, and reasoning capabilities. This allows eavesdroppers to further infer user private behavior and intent beyond the transmitted content. Motivated by these emerging challenges, this paper comprehensively rethinks the security and privacy of SemCom systems in the age of generative and agentic AI. We first present a systematic taxonomy of eavesdropping threat models in SemCom systems. Then, we provide insights into how GenAI and agentic AI can enhance eavesdropping threats. Meanwhile, we also highlight potential opportunities for leveraging GenAI and agentic AI to design privacy-preserving SemCom systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01791v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shunpu Tang, Yuanyuan Jia, Zijiu Yang, Qianqian Yang, Ruichen Zhang, Jun Du, Jihong Park, Zhiguo Shi, Jiming Chen</dc:creator>
    </item>
    <item>
      <title>Beyond Redundancy: Toward Agile Resilience in Optical Networks to Overcome Unpredictable Disasters</title>
      <link>https://arxiv.org/abs/2509.24038</link>
      <description>arXiv:2509.24038v2 Announce Type: replace 
Abstract: Resilience in optical networks has traditionally relied on redundancy and pre-planned recovery strategies, both of which assume a certain level of disaster predictability. However, recent environmental changes such as climate shifts, the evolution of communication services, and rising geopolitical risks have increased the unpredictability of disasters, reducing the effectiveness of conventional resilience approaches. To address this unpredictability, this paper introduces the concept of agile resilience, which emphasizes dynamic adaptability across multiple operators and layers. We identify key requirements and challenges, and present enabling technologies for the realization of agile resilience. Using a field-deployed transmission system, we demonstrate rapid system characterization, optical path provisioning, and database migration within six hours. These results validate the effectiveness of the proposed enabling technologies and confirm the feasibility of agile resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24038v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Mano, Hideki Nishizawa, Takeo Sasai, Soichiroh Usui, Dmitrii Briantcev, Devika Dass, Brandt Bashaw, Eoin Kenny, Marco Ruffini, Yoshiaki Sone, Koichi Takasugi, Daniel Kilper</dc:creator>
    </item>
    <item>
      <title>User-Centric Comparison of 5G NTN and DVB-S2/RCS2 Using OpenAirInterface and OpenSAND</title>
      <link>https://arxiv.org/abs/2509.26013</link>
      <description>arXiv:2509.26013v2 Announce Type: replace 
Abstract: The integration of satellite networks into next-generation mobile communication systems has gained considerable momentum with the advent of 5G Non-Terrestrial Networks (5G-NTN). Since established technologies like DVB-S2/RCS2 are already widely used for satellite broadband, a detailed comparison with emerging 5G NTN solutions is necessary to understand their relative merits and guide deployment decisions. This paper presents a user-centric, end-to-end evaluation of these technologies under realistic traffic conditions, showing how differences in architecture and protocols impact application-layer performance. Utilizing the 6G Sandbox platform, we employ OpenAirInterface to emulate 5G NTN and OpenSAND for DVB-S2/RCS2, replicating transparent payload GEO satellite scenarios under uniform downlink conditions. A range of real-world applications, such as web browsing, file downloads, and video streaming, are tested across both systems and systematically analyzed. While the emulation lacks real-time capability, it reveals key strengths and limitations of each approach, helping identify suitable deployment scenarios for 5G NTN and DVB-S2/RCS2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26013v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FNWF66845.2025.11317602</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE Future Networks World Forum (FNWF)</arxiv:journal_reference>
      <dc:creator>Sumit Kumar, Juan Carlos Estrada-Jimenez, Ion Turcanu</dc:creator>
    </item>
    <item>
      <title>Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks</title>
      <link>https://arxiv.org/abs/2510.19322</link>
      <description>arXiv:2510.19322v2 Announce Type: replace 
Abstract: Collective communication (CC) is critical for scaling distributed machine learning (DML). The predictable traffic patterns of DML present a great oppotunity for applying optical network technologies. Optical networks with reconfigurable topologies promise high bandwidth and low latency for collective communications. However, existing approaches face inherent limitations: static topologies are inefficient for dynamic communication patterns within CC algorithm, while frequent topology reconfiguration matching every step of the algorithm incurs significant overhead.
  In this paper, we propose SWOT, a demand-aware optical network framework that employs ``intra-collective reconfiguration'' to dynamically align network resources with CC traffic patterns. SWOT hides reconfiguration latency by overlapping it with data transmission through three key techniques: Heterogeneous Message Splitting, Asynchronous Overlapping, and Topology Bypassing. Extensive simulations demonstrate that SWOT reduces communication completion time up to 89.7% across diverse CC algorithm compared to static baselines, demonstrating strong robustness to varying optical resources and reconfiguration delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19322v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu</dc:creator>
    </item>
    <item>
      <title>Diffusion Model-Enhanced Environment Reconstruction in ISAC</title>
      <link>https://arxiv.org/abs/2511.19044</link>
      <description>arXiv:2511.19044v2 Announce Type: replace 
Abstract: Recently, environment reconstruction (ER) in integrated sensing and communication (ISAC) systems has emerged as a promising approach for achieving high-resolution environmental perception. However, the initial results obtained from ISAC systems are coarse and often unsatisfactory due to the high sparsity of the point clouds and significant noise variance. To address this problem, we propose a noise-sparsity-aware diffusion model (NSADM) post-processing framework. Leveraging the powerful data recovery capabilities of diffusion models, the proposed scheme exploits spatial features and the additive nature of noise to enhance point cloud density and denoise the initial input. Simulation results demonstrate that the proposed method significantly outperforms existing model-based and deep learning-based approaches in terms of Chamfer distance and root mean square error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19044v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nguyen Duc Minh Quang, Chang Liu, Shuangyang Li, Hoai-Nam Vu, Derrick Wing Kwan Ng, Wei Xiang</dc:creator>
    </item>
    <item>
      <title>Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks</title>
      <link>https://arxiv.org/abs/2512.11667</link>
      <description>arXiv:2512.11667v2 Announce Type: replace 
Abstract: Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\%, reduces communication resource usage by 75\%, and achieves up to 35\% cost savings, while maintaining an average optimality gap of 5\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11667v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriel Almeida, Jo\~ao Paulo Esper, Cleverson Nahum, Aldebaro Klautau, Kleber Vieira Cardoso</dc:creator>
    </item>
    <item>
      <title>Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN</title>
      <link>https://arxiv.org/abs/2512.01596</link>
      <description>arXiv:2512.01596v2 Announce Type: replace-cross 
Abstract: Securing the near-real-time (near-RT) control operations in Open Radio Access Networks (Open RAN) is increasingly critical, yet remains insufficiently addressed, as new runtime threats target the control loop while the system is operational. In this paper, we propose a multi-layer defence framework designed to enhance the security of near-RT RAN Intelligent Controller (RIC) operations. We classify operational-time threats into three categories, message-level, data-level, and control logic-level, and design and implement a dedicated detection and mitigation component for each: a signature-based E2 message inspection module performing structural and semantic validation of signalling exchanges, a telemetry poisoning detector based on temporal anomaly scoring using an LSTM network, and a runtime xApp attestation mechanism based on execution-time hash challenge-response. The framework is evaluated on an O-RAN testbed comprising FlexRIC and a commercial RAN emulator, demonstrating effective detection rates, low latency overheads, and practical integration feasibility. Results indicate that the proposed safeguards can operate within near-RT time constraints while significantly improving protection against runtime attacks, introducing less than 80 ms overhead for a network with 500 User Equipment (UEs). Overall, this work lays the foundation for deployable, layered, and policy-driven runtime security architectures for the near-RT RIC control loop in Open RAN, and provides an extensible framework into which future mitigation policies and threat-specific modules can be integrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01596v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamed Alimohammadi, Samara Mayhoub, Sotiris Chatzimiltis, Mohammad Shojafar, Muhammad Nasir Mumtaz Bhutta</dc:creator>
    </item>
    <item>
      <title>{\mu}ACP: A Formal Calculus for Expressive, Resource-Constrained Agent Communication</title>
      <link>https://arxiv.org/abs/2601.00219</link>
      <description>arXiv:2601.00219v2 Announce Type: replace-cross 
Abstract: Agent communication remains a foundational problem in multi-agent systems: protocols such as FIPA-ACL guarantee semantic richness but are intractable for constrained environments, while lightweight IoT protocols achieve efficiency at the expense of expressiveness. This paper presents $\mu$ACP, a formal calculus for expressive agent communication under explicit resource bounds. We formalize the Resource-Constrained Agent Communication (RCAC) model, prove that a minimal four-verb basis \textit{\{PING, TELL, ASK, OBSERVE\}} is suffices to encode finite-state FIPA protocols, and establish tight information-theoretic bounds on message complexity. We further show that $\mu$ACP can implement standard consensus under partial synchrony and crash faults, yielding a constructive coordination framework for edge-native agents. Formal verification in TLA$^{+}$ (model checking) and Coq (mechanized invariants) establishes safety and boundedness, and supports liveness under modeled assumptions. Large-scale system simulations confirm ACP achieves a median end-to-end message latency of 34 ms (95th percentile 104 ms) at scale, outperforming prior agent and IoT protocols under severe resource constraints. The main contribution is a unified calculus that reconciles semantic expressiveness with provable efficiency, providing a rigorous foundation for the next generation of resource-constrained multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00219v2</guid>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Mallick, Indraveni Chebolu</dc:creator>
    </item>
  </channel>
</rss>
