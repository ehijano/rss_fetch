<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 02:55:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Least squares variational inference</title>
      <link>https://arxiv.org/abs/2502.18475</link>
      <description>arXiv:2502.18475v1 Announce Type: new 
Abstract: Variational inference consists in finding the best approximation of a target distribution within a certain family, where `best' means (typically) smallest Kullback-Leiber divergence. We show that, when the approximation family is exponential, the best approximation is the solution of a fixed-point equation. We introduce LSVI (Least-Squares Variational Inference), a Monte Carlo variant of the corresponding fixed-point recursion, where each iteration boils down to ordinary least squares regression and does not require computing gradients. We show that LSVI is equivalent to stochastic mirror descent; we use this insight to derive convergence guarantees. We introduce various ideas to improve LSVI further when the approximation family is Gaussian, leading to a $O(d^3)$ complexity in the dimension $d$ of the target in the full-covariance case, and a $O(d)$ complexity in the mean-field case. We show that LSVI outperforms state-of-the-art methods in a range of examples, while remaining gradient-free, that is, it does not require computing gradients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18475v1</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yvann Le Fay, Nicolas Chopin, Simon Barthelm\'e</dc:creator>
    </item>
    <item>
      <title>PyTorchFire: A GPU-Accelerated Wildfire Simulator with Differentiable Cellular Automata</title>
      <link>https://arxiv.org/abs/2502.18738</link>
      <description>arXiv:2502.18738v1 Announce Type: cross 
Abstract: Accurate and rapid prediction of wildfire trends is crucial for effective management and mitigation. However, the stochastic nature of fire propagation poses significant challenges in developing reliable simulators. In this paper, we introduce PyTorchFire, an open-access, PyTorch-based software that leverages GPU acceleration. With our redesigned differentiable wildfire Cellular Automata (CA) model, we achieve millisecond-level computational efficiency, significantly outperforming traditional CPU-based wildfire simulators on real-world-scale fires at high resolution. Real-time parameter calibration is made possible through gradient descent on our model, aligning simulations closely with observed wildfire behavior both temporally and spatially, thereby enhancing the realism of the simulations. Our PyTorchFire simulator, combined with real-world environmental data, demonstrates superior generalizability compared to supervised learning surrogate models. Its ability to predict and calibrate wildfire behavior in real-time ensures accuracy, stability, and efficiency. PyTorchFire has the potential to revolutionize wildfire simulation, serving as a powerful tool for wildfire prediction and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18738v1</guid>
      <category>cs.CE</category>
      <category>nlin.CG</category>
      <category>physics.comp-ph</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zeyu Xia, Sibo Cheng</dc:creator>
    </item>
    <item>
      <title>Validating uncertainty propagation approaches for two-stage Bayesian spatial models using simulation-based calibration</title>
      <link>https://arxiv.org/abs/2502.18962</link>
      <description>arXiv:2502.18962v1 Announce Type: cross 
Abstract: This work tackles the problem of uncertainty propagation in two-stage Bayesian models, with a focus on spatial applications. A two-stage modeling framework has the advantage of being more computationally efficient than a fully Bayesian approach when the first-stage model is already complex in itself, and avoids the potential problem of unwanted feedback effects. Two ways of doing two-stage modeling are the crude plug-in method and the posterior sampling method. The former ignores the uncertainty in the first-stage model, while the latter can be computationally expensive. This paper validates the two aforementioned approaches and proposes a new approach to do uncertainty propagation, which we call the $\mathbf{Q}$ uncertainty method, implemented using the Integrated Nested Laplace Approximation (INLA). We validate the different approaches using the simulation-based calibration method, which tests the self-consistency property of Bayesian models. Results show that the crude plug-in method underestimates the true posterior uncertainty in the second-stage model parameters, while the resampling approach and the proposed method are correct. We illustrate the approaches in a real life data application which aims to link relative humidity and Dengue cases in the Philippines for August 2018.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18962v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Jun Villejo, Sara Martino, Janine Illian, William Ryan, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>Fast Conservative Monte Carlo Confidence Intervals</title>
      <link>https://arxiv.org/abs/2405.05238</link>
      <description>arXiv:2405.05238v3 Announce Type: replace 
Abstract: Extant "fast" algorithms for Monte Carlo confidence sets are limited to univariate shift parameters for the one-sample and two-sample problems using the sample mean as the test statistic; moreover, some do not converge reliably and most do not produce conservative confidence sets. We outline general methods for constructing confidence sets for real-valued and multidimensional parameters by inverting Monte Carlo tests using any test statistic and a broad range of randomization schemes. The method exploits two facts that, to our knowledge, had not been combined: (i) there are Monte Carlo tests that are conservative despite relying on simulation, and (ii) since the coverage probability of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number $N$ of Monte Carlo replicates increases. We present open-source Python and R implementations of new algorithms to compute conservative confidence sets for real-valued parameters from Monte Carlo tests, for test statistics and randomization schemes that yield $P$-values that are monotone or weakly unimodal in the parameter, with the data and Monte Carlo sample held fixed. In this case, the new method finds conservative confidence sets for real-valued parameters in $O(n)$ time, where $n$ is the number of data. The values of some test statistics for different simulations and parameter values have a simple relationship that makes more savings possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05238v3</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amanda K. Glazer, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>High-accuracy sampling from constrained spaces with the Metropolis-adjusted Preconditioned Langevin Algorithm</title>
      <link>https://arxiv.org/abs/2412.18701</link>
      <description>arXiv:2412.18701v3 Announce Type: replace 
Abstract: In this work, we propose a first-order sampling method called the Metropolis-adjusted Preconditioned Langevin Algorithm for approximate sampling from a target distribution whose support is a proper convex subset of $\mathbb{R}^{d}$. Our proposed method is the result of applying a Metropolis-Hastings filter to the Markov chain formed by a single step of the preconditioned Langevin algorithm with a metric $\mathscr{G}$, and is motivated by the natural gradient descent algorithm for optimisation. We derive non-asymptotic upper bounds for the mixing time of this method for sampling from target distributions whose potentials are bounded relative to $\mathscr{G}$, and for exponential distributions restricted to the support. Our analysis suggests that if $\mathscr{G}$ satisfies stronger notions of self-concordance introduced in Kook and Vempala (2024), then these mixing time upper bounds have a strictly better dependence on the dimension than when is merely self-concordant. We also provide numerical experiments that demonstrates the practicality of our proposed method. Our method is a high-accuracy sampler due to the polylogarithmic dependence on the error tolerance in our mixing time upper bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18701v3</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishwak Srinivasan, Andre Wibisono, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference of Reproduction Number from Epidemiological and Genetic Data Using Particle MCMC</title>
      <link>https://arxiv.org/abs/2311.09838</link>
      <description>arXiv:2311.09838v2 Announce Type: replace-cross 
Abstract: Inference of the reproduction number through time is of vital importance during an epidemic outbreak. Typically, epidemiologists tackle this using observed prevalence or incidence data. However, prevalence and incidence data alone is often noisy or partial. Models can also have identifiability issues with determining whether a large amount of a small epidemic or a small amount of a large epidemic has been observed. Sequencing data however is becoming more abundant, so approaches which can incorporate genetic data are an active area of research. We propose using particle MCMC methods to infer the time-varying reproduction number from a combination of prevalence data reported at a set of discrete times and a dated phylogeny reconstructed from sequences. We validate our approach on simulated epidemics with a variety of scenarios. We then apply the method to real data sets of HIV-1 in North Carolina, USA and tuberculosis in Buenos Aires, Argentina. The models and algorithms are implemented in an open source R package called EpiSky which is available at https://github.com/alicia-gill/EpiSky.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09838v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicia Gill, Jere Koskela, Xavier Didelot, Richard G. Everitt</dc:creator>
    </item>
  </channel>
</rss>
