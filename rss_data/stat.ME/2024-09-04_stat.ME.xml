<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Sep 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Moment-type estimators for a weighted exponential family</title>
      <link>https://arxiv.org/abs/2409.02204</link>
      <description>arXiv:2409.02204v1 Announce Type: new 
Abstract: In this paper, we propose and study closed-form moment type estimators for a weighted exponential family. We also develop a bias-reduced version of these proposed closed-form estimators using bootstrap techniques. The estimators are evaluated using Monte Carlo simulation. This shows favourable results for the proposed bootstrap bias-reduced estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02204v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Estimand-based Inference in Presence of Long-Term Survivors</title>
      <link>https://arxiv.org/abs/2409.02209</link>
      <description>arXiv:2409.02209v1 Announce Type: new 
Abstract: In this article, we develop nonparametric inference methods for comparing survival data across two samples, which are beneficial for clinical trials of novel cancer therapies where long-term survival is a critical outcome. These therapies, including immunotherapies or other advanced treatments, aim to establish durable effects. They often exhibit distinct survival patterns such as crossing or delayed separation and potentially leveling-off at the tails of survival curves, clearly violating the proportional hazards assumption and rendering the hazard ratio inappropriate for measuring treatment effects. The proposed methodology utilizes the mixture cure framework to separately analyze the cure rates of long-term survivors and the survival functions of susceptible individuals. We evaluate a nonparametric estimator for the susceptible survival function in the one-sample setting. Under sufficient follow-up, it is expressed as a location-scale-shift variant of the Kaplan-Meier (KM) estimator. It retains several desirable features of the KM estimator, including inverse-probability-censoring weighting, product-limit estimation, self-consistency, and nonparametric efficiency. In scenarios of insufficient follow-up, it can easily be adapted by incorporating a suitable cure rate estimator. In the two-sample setting, besides using the difference in cure rates to measure the long-term effect, we propose a graphical estimand to compare the relative treatment effects on susceptible subgroups. This process, inspired by Kendall's tau, compares the order of survival times among susceptible individuals. The proposed methods' large-sample properties are derived for further inference, and the finite-sample properties are examined through extensive simulation studies. The proposed methodology is applied to analyze the digitized data from the CheckMate 067 immunotherapy clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02209v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Cheng Tai, Weijing Wang, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Generalized implementation of invariant coordinate selection with positive semi-definite scatter matrices</title>
      <link>https://arxiv.org/abs/2409.02258</link>
      <description>arXiv:2409.02258v1 Announce Type: new 
Abstract: Invariant coordinate selection (ICS) is an unsupervised multivariate data transformation useful in many contexts such as outlier detection or clustering. It is based on the simultaneous diagonalization of two affine equivariant and positive definite scatter matrices. Its classical implementation relies on a non-symmetric eigenvalue problem (EVP) by diagonalizing one scatter relatively to the other. In case of collinearity, at least one of the scatter matrices is singular and the problem cannot be solved. To address this limitation, three approaches are proposed based on: a Moore-Penrose pseudo inverse (GINV), a dimension reduction (DR), and a generalized singular value decomposition (GSVD). Their properties are investigated theoretically and in different empirical applications. Overall, the extension based on GSVD seems the most promising even if it restricts the choice of scatter matrices that can be expressed as cross-products. In practice, some of the approaches also look suitable in the context of data in high dimension low sample size (HDLSS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02258v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aurore Archimbaud</dc:creator>
    </item>
    <item>
      <title>Simulation-calibration testing for inference in Lasso regressions</title>
      <link>https://arxiv.org/abs/2409.02269</link>
      <description>arXiv:2409.02269v1 Announce Type: new 
Abstract: We propose a test of the significance of a variable appearing on the Lasso path and use it in a procedure for selecting one of the models of the Lasso path, controlling the Family-Wise Error Rate. Our null hypothesis depends on a set A of already selected variables and states that it contains all the active variables. We focus on the regularization parameter value from which a first variable outside A is selected. As the test statistic, we use this quantity's conditional p-value, which we define conditional on the non-penalized estimated coefficients of the model restricted to A. We estimate this by simulating outcome vectors and then calibrating them on the observed outcome's estimated coefficients. We adapt the calibration heuristically to the case of generalized linear models in which it turns into an iterative stochastic procedure. We prove that the test controls the risk of selecting a false positive in linear models, both under the null hypothesis and, under a correlation condition, when A does not contain all active variables. We assess the performance of our procedure through extensive simulation studies. We also illustrate it in the detection of exposures associated with drug-induced liver injuries in the French pharmacovigilance database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02269v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthieu Pluntz, Cyril Dalmasso, Pascale Tubert-Bitter, Ismail Ahmed</dc:creator>
    </item>
    <item>
      <title>A parameterization of anisotropic Gaussian fields with penalized complexity priors</title>
      <link>https://arxiv.org/abs/2409.02331</link>
      <description>arXiv:2409.02331v1 Announce Type: new 
Abstract: Gaussian random fields (GFs) are fundamental tools in spatial modeling and can be represented flexibly and efficiently as solutions to stochastic partial differential equations (SPDEs). The SPDEs depend on specific parameters, which enforce various field behaviors and can be estimated using Bayesian inference. However, the likelihood typically only provides limited insights into the covariance structure under in-fill asymptotics. In response, it is essential to leverage priors to achieve appropriate, meaningful covariance structures in the posterior. This study introduces a smooth, invertible parameterization of the correlation length and diffusion matrix of an anisotropic GF and constructs penalized complexity (PC) priors for the model when the parameters are constant in space. The formulated prior is weakly informative, effectively penalizing complexity by pushing the correlation range toward infinity and the anisotropy to zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02331v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Llamazares-Elias, Jonas Latz, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>A Principal Square Response Forward Regression Method for Dimension Reduction</title>
      <link>https://arxiv.org/abs/2409.02372</link>
      <description>arXiv:2409.02372v1 Announce Type: new 
Abstract: Dimension reduction techniques, such as Sufficient Dimension Reduction (SDR), are indispensable for analyzing high-dimensional datasets. This paper introduces a novel SDR method named Principal Square Response Forward Regression (PSRFR) for estimating the central subspace of the response variable Y, given the vector of predictor variables $\bm{X}$. We provide a computational algorithm for implementing PSRFR and establish its consistency and asymptotic properties. Monte Carlo simulations are conducted to assess the performance, efficiency, and robustness of the proposed method. Notably, PSRFR exhibits commendable performance in scenarios where the variance of each component becomes increasingly dissimilar, particularly when the predictor variables follow an elliptical distribution. Furthermore, we illustrate and validate the effectiveness of PSRFR using a real-world dataset concerning wine quality. Our findings underscore the utility and reliability of the PSRFR method in practical applications of dimension reduction for high-dimensional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02372v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Li, Yunhao Wang, Wei Gao, Hon Keung Tony Ng</dc:creator>
    </item>
    <item>
      <title>High-dimensional Bayesian Model for Disease-Specific Gene Detection in Spatial Transcriptomics</title>
      <link>https://arxiv.org/abs/2409.02397</link>
      <description>arXiv:2409.02397v1 Announce Type: new 
Abstract: Identifying disease-indicative genes is critical for deciphering disease mechanisms and has attracted significant interest in biomedical research. Spatial transcriptomics offers unprecedented insights for the detection of disease-specific genes by enabling within-tissue contrasts. However, this new technology poses challenges for conventional statistical models developed for RNA-sequencing, as these models often neglect the spatial organization of tissue spots. In this article, we propose a Bayesian shrinkage model to characterize the relationship between high-dimensional gene expressions and the disease status of each tissue spot, incorporating spatial correlation among these spots through autoregressive terms. Our model adopts a hierarchical structure to facilitate the analysis of multiple correlated samples and is further extended to accommodate the missing data within tissues. To ensure the model's applicability to datasets of varying sizes, we carry out two computational frameworks for Bayesian parameter estimation, tailored to both small and large sample scenarios. Simulation studies are conducted to evaluate the performance of the proposed model. The proposed model is applied to analyze the data arising from a HER2-positive breast cancer study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02397v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qicheng Zhao, Qihuang Zhang</dc:creator>
    </item>
    <item>
      <title>Fitting an Equation to Data Impartially</title>
      <link>https://arxiv.org/abs/2409.02573</link>
      <description>arXiv:2409.02573v1 Announce Type: new 
Abstract: We consider the problem of fitting a relationship (e.g. a potential scientific law) to data involving multiple variables. Ordinary (least squares) regression is not suitable for this because the estimated relationship will differ according to which variable is chosen as being dependent, and the dependent variable is unrealistically assumed to be the only variable which has any measurement error (noise). We present a very general method for estimating a linear functional relationship between multiple noisy variables, which are treated impartially, i.e. no distinction between dependent and independent variables. The data are not assumed to follow any distribution, but all variables are treated as being equally reliable. Our approach extends the geometric mean functional relationship to multiple dimensions. This is especially useful with variables measured in different units, as it is naturally scale-invariant, whereas orthogonal regression is not. This is because our approach is not based on minimizing distances, but on the symmetric concept of correlation. The estimated coefficients are easily obtained from the covariances or correlations, and correspond to geometric means of associated least squares coefficients. The ease of calculation will hopefully allow widespread application of impartial fitting to estimate relationships in a neutral way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02573v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math11183957</arxiv:DOI>
      <arxiv:journal_reference>Mathematics, 11(18), 3957 (2023)</arxiv:journal_reference>
      <dc:creator>Chris Tofallis</dc:creator>
    </item>
    <item>
      <title>A family of toroidal diffusions with exact likelihood inference</title>
      <link>https://arxiv.org/abs/2409.02705</link>
      <description>arXiv:2409.02705v1 Announce Type: new 
Abstract: We provide a class of diffusion processes for continuous time-varying multivariate angular data with explicit transition probability densities, enabling exact likelihood inference. The presented diffusions are time-reversible and can be constructed for any pre-specified stationary distribution on the torus, including highly-multimodal mixtures. We give results on asymptotic likelihood theory allowing one-sample inference and tests of linear hypotheses for $k$ groups of diffusions, including homogeneity. We show that exact and direct diffusion bridge simulation is possible too. A class of circular jump processes with similar properties is also proposed. Several numerical experiments illustrate the methodology for the circular and two-dimensional torus cases. The new family of diffusions is applied (i) to test several homogeneity hypotheses on the movement of ants and (ii) to simulate bridges between the three-dimensional backbones of two related proteins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02705v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eduardo Garc\'ia-Portugu\'es, Michael S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Cost-Effectiveness Analysis for Disease Prevention -- A Case Study on Colorectal Cancer Screening</title>
      <link>https://arxiv.org/abs/2409.02888</link>
      <description>arXiv:2409.02888v1 Announce Type: new 
Abstract: Cancer Screening has been widely recognized as an effective strategy for preventing the disease. Despite its effectiveness, determining when to start screening is complicated, because starting too early increases the number of screenings over lifetime and thus costs but starting too late may miss the cancer that could have been prevented. Therefore, to make an informed recommendation on the age to start screening, it is necessary to conduct cost-effectiveness analysis to assess the gain in life years relative to the cost of screenings. As more large-scale observational studies become accessible, there is growing interest in evaluating cost-effectiveness based on empirical evidence. In this paper, we propose a unified measure for evaluating cost-effectiveness and a causal analysis for the continuous intervention of screening initiation age, under the multi-state modeling with semi-competing risks. Extensive simulation results show that the proposed estimators perform well in realistic scenarios. We perform a cost-effectiveness analysis of the colorectal cancer screening, utilizing data from the large-scale Women's Health Initiative. Our analysis reveals that initiating screening at age 50 years yields the highest quality-adjusted life years with an acceptable incremental cost-effectiveness ratio compared to no screening, providing real-world evidence in support of screening recommendation for colorectal cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02888v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Xiong, Kwun C G Chan, Malka Gorfine, Li Hsu</dc:creator>
    </item>
    <item>
      <title>Discovering Candidate Genes Regulated by GWAS Signals in Cis and Trans</title>
      <link>https://arxiv.org/abs/2409.02116</link>
      <description>arXiv:2409.02116v1 Announce Type: cross 
Abstract: Understanding the genetic underpinnings of complex traits and diseases has been greatly advanced by genome-wide association studies (GWAS). However, a significant portion of trait heritability remains unexplained, known as ``missing heritability". Most GWAS loci reside in non-coding regions, posing challenges in understanding their functional impact. Integrating GWAS with functional genomic data, such as expression quantitative trait loci (eQTLs), can bridge this gap. This study introduces a novel approach to discover candidate genes regulated by GWAS signals in both cis and trans. Unlike existing eQTL studies that focus solely on cis-eQTLs or consider cis- and trans-QTLs separately, we utilize adaptive statistical metrics that can reflect both the strong, sparse effects of cis-eQTLs and the weak, dense effects of trans-eQTLs. Consequently, candidate genes regulated by the joint effects can be prioritized. We demonstrate the efficiency of our method through theoretical and numerical analyses and apply it to adipose eQTL data from the METabolic Syndrome in Men (METSIM) study, uncovering genes playing important roles in the regulatory networks influencing cardiometabolic traits. Our findings offer new insights into the genetic regulation of complex traits and present a practical framework for identifying key regulatory genes based on joint eQTL effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02116v1</guid>
      <category>q-bio.GN</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Xinge Jessie Jeng</dc:creator>
    </item>
    <item>
      <title>Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling</title>
      <link>https://arxiv.org/abs/2409.02135</link>
      <description>arXiv:2409.02135v1 Announce Type: cross 
Abstract: Learning-based methods have gained attention as general-purpose solvers because they can automatically learn problem-specific heuristics, reducing the need for manually crafted heuristics. However, these methods often face challenges with scalability. To address these issues, the improved Sampling algorithm for Combinatorial Optimization (iSCO) using discrete Langevin dynamics has been proposed, demonstrating better performance than several learning-based solvers. This study proposes a different approach that integrates gradient-based update through continuous relaxation, combined with Quasi-Quantum Annealing (QQA). QQA smoothly transitions the objective function from a simple convex form, where half-integral solutions dominate, to the original objective function, where the variables are restricted to 0 or 1. Furthermore, we incorporate parallel run communication leveraging GPUs, enhancing exploration capabilities and accelerating convergence. Numerical experiments demonstrate that our approach is a competitive general-purpose solver, achieving comparable performance to iSCO across various benchmark problems. Notably, our method exhibits superior trade-offs between speed and solution quality for large-scale instances compared to iSCO, commercial solvers, and specialized algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02135v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Ichikawa, Yamato Arai</dc:creator>
    </item>
    <item>
      <title>Distribution Regression Difference-In-Differences</title>
      <link>https://arxiv.org/abs/2409.02311</link>
      <description>arXiv:2409.02311v1 Announce Type: cross 
Abstract: We provide a simple distribution regression estimator for treatment effects in the difference-in-differences (DiD) design. Our procedure is particularly useful when the treatment effect differs across the distribution of the outcome variable. Our proposed estimator easily incorporates covariates and, importantly, can be extended to settings where the treatment potentially affects the joint distribution of multiple outcomes. Our key identifying restriction is that the counterfactual distribution of the treated in the untreated state has no interaction effect between treatment and time. This assumption results in a parallel trend assumption on a transformation of the distribution. We highlight the relationship between our procedure and assumptions with the changes-in-changes approach of Athey and Imbens (2006). We also reexamine two existing empirical examples which highlight the utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02311v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning at Scale to Predict Causal Impact of Customer Actions</title>
      <link>https://arxiv.org/abs/2409.02332</link>
      <description>arXiv:2409.02332v1 Announce Type: cross 
Abstract: Causal Impact (CI) of customer actions are broadly used across the industry to inform both short- and long-term investment decisions of various types. In this paper, we apply the double machine learning (DML) methodology to estimate the CI values across 100s of customer actions of business interest and 100s of millions of customers. We operationalize DML through a causal ML library based on Spark with a flexible, JSON-driven model configuration approach to estimate CI at scale (i.e., across hundred of actions and millions of customers). We outline the DML methodology and implementation, and associated benefits over the traditional potential outcomes based CI model. We show population-level as well as customer-level CI values along with confidence intervals. The validation metrics show a 2.2% gain over the baseline methods and a 2.5X gain in the computational time. Our contribution is to advance the scalable application of CI, while also providing an interface that allows faster experimentation, cross-platform support, ability to onboard new use cases, and improves accessibility of underlying code for partner teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02332v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-43427-3_31</arxiv:DOI>
      <arxiv:journal_reference>Lecture Notes in Computer Science, vol 14174. (2023) Springer, Cham</arxiv:journal_reference>
      <dc:creator>Sushant More, Priya Kotwal, Sujith Chappidi, Dinesh Mandalapu, Chris Khawand</dc:creator>
    </item>
    <item>
      <title>Hypothesizing Missing Causal Variables with LLMs</title>
      <link>https://arxiv.org/abs/2409.02604</link>
      <description>arXiv:2409.02604v1 Announce Type: cross 
Abstract: Scientific discovery is a catalyst for human intellectual advances, driven by the cycle of hypothesis generation, experimental design, data evaluation, and iterative assumption refinement. This process, while crucial, is expensive and heavily dependent on the domain knowledge of scientists to generate hypotheses and navigate the scientific cycle. Central to this is causality, the ability to establish the relationship between the cause and the effect. Motivated by the scientific discovery process, in this work, we formulate a novel task where the input is a partial causal graph with missing variables, and the output is a hypothesis about the missing variables to complete the partial graph. We design a benchmark with varying difficulty levels and knowledge assumptions about the causal graph. With the growing interest in using Large Language Models (LLMs) to assist in scientific discovery, we benchmark open-source and closed models on our testbed. We show the strong ability of LLMs to hypothesize the mediation variables between a cause and its effect. In contrast, they underperform in hypothesizing the cause and effect variables themselves. We also observe surprising results where some of the open-source models outperform the closed GPT-4 model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02604v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ivaxi Sheth, Sahar Abdelnabi, Mario Fritz</dc:creator>
    </item>
    <item>
      <title>Few-shot Multi-Task Learning of Linear Invariant Features with Meta Subspace Pursuit</title>
      <link>https://arxiv.org/abs/2409.02708</link>
      <description>arXiv:2409.02708v1 Announce Type: cross 
Abstract: Data scarcity poses a serious threat to modern machine learning and artificial intelligence, as their practical success typically relies on the availability of big datasets. One effective strategy to mitigate the issue of insufficient data is to first harness information from other data sources possessing certain similarities in the study design stage, and then employ the multi-task or meta learning framework in the analysis stage. In this paper, we focus on multi-task (or multi-source) linear models whose coefficients across tasks share an invariant low-rank component, a popular structural assumption considered in the recent multi-task or meta learning literature. Under this assumption, we propose a new algorithm, called Meta Subspace Pursuit (abbreviated as Meta-SP), that provably learns this invariant subspace shared by different tasks. Under this stylized setup for multi-task or meta learning, we establish both the algorithmic and statistical guarantees of the proposed method. Extensive numerical experiments are conducted, comparing Meta-SP against several competing methods, including popular, off-the-shelf model-agnostic meta learning algorithms such as ANIL. These experiments demonstrate that Meta-SP achieves superior performance over the competing methods in various aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02708v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaozhi Zhang, Lin Liu, Xiaoqun Zhang</dc:creator>
    </item>
    <item>
      <title>A variational inference framework for inverse problems</title>
      <link>https://arxiv.org/abs/2103.05909</link>
      <description>arXiv:2103.05909v4 Announce Type: replace 
Abstract: A framework is presented for fitting inverse problem models via variational Bayes approximations. This methodology guarantees flexibility to statistical model specification for a broad range of applications, good accuracy and reduced model fitting times. The message passing and factor graph fragment approach to variational Bayes that is also described facilitates streamlined implementation of approximate inference algorithms and allows for supple inclusion of numerous response distributions and penalizations into the inverse problem model. Models for one- and two-dimensional response variables are examined and an infrastructure is laid down where efficient algorithm updates based on nullifying weak interactions between variables can also be derived for inverse problems in higher dimensions. An image processing application and a simulation exercise motivated by biomedical problems reveal the computational advantage offered by efficient implementation of variational Bayes over Markov chain Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.05909v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Maestrini, Robert G. Aykroyd, Matt P. Wand</dc:creator>
    </item>
    <item>
      <title>A stableness of resistance model for nonresponse adjustment with callback data</title>
      <link>https://arxiv.org/abs/2112.02822</link>
      <description>arXiv:2112.02822v4 Announce Type: replace 
Abstract: Nonresponse arises frequently in surveys and follow-ups are routinely made to increase the response rate. In order to monitor the follow-up process, callback data have been used in social sciences and survey studies for decades. In modern surveys, the availability of callback data is increasing because the response rate is decreasing and follow-ups are essential to collect maximum information. Although callback data are helpful to reduce the bias in surveys, such data have not been widely used in statistical analysis until recently. We propose a stableness of resistance assumption for nonresponse adjustment with callback data. We establish the identification and the semiparametric efficiency theory under this assumption, and propose a suite of semiparametric estimation methods including doubly robust estimators, which generalize existing parametric approaches for callback data analysis. We apply the approach to a Consumer Expenditure Survey dataset. The results suggest an association between nonresponse and high housing expenditures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.02822v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Miao, Xinyu Li, Ping Zhang, Baoluo Sun</dc:creator>
    </item>
    <item>
      <title>Safe Policy Learning under Regression Discontinuity Designs with Multiple Cutoffs</title>
      <link>https://arxiv.org/abs/2208.13323</link>
      <description>arXiv:2208.13323v4 Announce Type: replace 
Abstract: The regression discontinuity (RD) design is widely used for program evaluation with observational data. The primary focus of the existing literature has been the estimation of the local average treatment effect at the existing treatment cutoff. In contrast, we consider policy learning under the RD design. Because the treatment assignment mechanism is deterministic, learning better treatment cutoffs requires extrapolation. We develop a robust optimization approach to finding optimal treatment cutoffs that improve upon the existing ones. We first decompose the expected utility into point-identifiable and unidentifiable components. We then propose an efficient doubly-robust estimator for the identifiable parts. To account for the unidentifiable components, we leverage the existence of multiple cutoffs that are common under the RD design. Specifically, we assume that the heterogeneity in the conditional expectations of potential outcomes across different groups vary smoothly along the running variable. Under this assumption, we minimize the worst case utility loss relative to the status quo policy. The resulting new treatment cutoffs have a safety guarantee that they will not yield a worse overall outcome than the existing cutoffs. Finally, we establish the asymptotic regret bounds for the learned policy using semi-parametric efficiency theory. We apply the proposed methodology to empirical and simulated data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13323v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhang, Eli Ben-Michael, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Rank-transformed subsampling: inference for multiple data splitting and exchangeable p-values</title>
      <link>https://arxiv.org/abs/2301.02739</link>
      <description>arXiv:2301.02739v3 Announce Type: replace 
Abstract: Many testing problems are readily amenable to randomised tests such as those employing data splitting. However despite their usefulness in principle, randomised tests have obvious drawbacks. Firstly, two analyses of the same dataset may lead to different results. Secondly, the test typically loses power because it does not fully utilise the entire sample. As a remedy to these drawbacks, we study how to combine the test statistics or p-values resulting from multiple random realisations such as through random data splits. We develop rank-transformed subsampling as a general method for delivering large sample inference about the combined statistic or p-value under mild assumptions. We apply our methodology to a wide range of problems, including testing unimodality in high-dimensional data, testing goodness-of-fit of parametric quantile regression models, testing no direct effect in a sequentially randomised trial and calibrating cross-fit double machine learning confidence intervals. In contrast to existing p-value aggregation schemes that can be highly conservative, our method enjoys type-I error control that asymptotically approaches the nominal level. Moreover, compared to using the ordinary subsampling, we show that our rank transform can remove the first-order bias in approximating the null under alternatives and greatly improve power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02739v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkae091</arxiv:DOI>
      <dc:creator>F. Richard Guo, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Price Experimentation and Interference</title>
      <link>https://arxiv.org/abs/2310.17165</link>
      <description>arXiv:2310.17165v3 Announce Type: replace 
Abstract: In this paper, we examine biases arising in A/B tests where firms modify a continuous parameter, such as price, to estimate the global treatment effect of a given performance metric, such as profit. These biases emerge in canonical experimental estimators due to interference among market participants. We employ structural modeling and differential calculus to derive intuitive characterizations of these biases. We then specialize our general model to a standard revenue management pricing problem. This setting highlights a key pitfall in the use of A/B pricing experiments to guide profit maximization: notably, the canonical estimator for the expected change in profits can have the {\em wrong sign}. In other words, following the guidance of canonical estimators may lead firms to move prices in the wrong direction, inadvertently decreasing profits relative to the status quo. We introduce a novel debiasing technique for such experiments, that only requires that the firm equally split experimental units between treatment and control. We apply these results to a two-sided market model and show how this "change of sign" regime depends on model parameters such as market imbalance, as well as the price markup. We also extend our revenue management model to an alternative specification in which firms set a fixed sales fee across heterogeneous products, and show that all our results extend to this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17165v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wassim Dhaouadi, Ramesh Johari, Orrie B. Page, Gabriel Y. Weintraub</dc:creator>
    </item>
    <item>
      <title>Optimal combination of composite likelihoods using approximate Bayesian computation with application to state-space models</title>
      <link>https://arxiv.org/abs/2404.02313</link>
      <description>arXiv:2404.02313v2 Announce Type: replace 
Abstract: Composite likelihood provides approximate inference when the full likelihood is intractable and sub-likelihood functions of marginal events can be evaluated relatively easily. It has been successfully applied for many complex models. However, its wider application is limited by two issues. First, weight selection of marginal likelihood can have a significant impact on the information efficiency and is currently an open question. Second, calibrated Bayesian inference with composite likelihood requires curvature adjustment which is difficult for dependent data. This work shows that approximate Bayesian computation (ABC) can properly address these two issues by using multiple composite score functions as summary statistics. First, the summary-based posterior distribution gives the optimal Godambe information among a wide class of estimators defined by linear combinations of estimating functions. Second, to make ABC computationally feasible for models where marginal likelihoods have no closed form, a novel approach is proposed to estimate all simulated marginal scores using a Monte Carlo sample with size N. Sufficient conditions are given for the additional noise to be negligible with N fixed as the data size n goes to infinity, and the computational cost is O(n). Third, asymptotic properties of ABC with summary statistics having heterogeneous convergence rates is derived, and an adaptive scheme to choose the component composite scores is proposed. Numerical studies show that the new method significantly outperforms the existing Bayesian composite likelihood methods, and the efficiency of adaptively combined composite scores well approximates the efficiency of particle MCMC using the full likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02313v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wentao Li, Rosabeth White, Dennis Prangle</dc:creator>
    </item>
    <item>
      <title>Preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed</title>
      <link>https://arxiv.org/abs/2408.12347</link>
      <description>arXiv:2408.12347v4 Announce Type: replace 
Abstract: One justification for preregistering research hypotheses, methods, and analyses is that it improves the transparent evaluation of the severity of hypothesis tests. In this article, I consider two cases in which preregistration does not improve this evaluation. First, I argue that, although preregistration can facilitate the transparent evaluation of severity in Mayo's error statistical philosophy of science, it does not facilitate this evaluation in Popper's theory-centric approach. To illustrate, I show that associated concerns about Type I error rate inflation are only relevant in the error statistical approach and not in a theory-centric approach. Second, I argue that a preregistered test procedure that allows deviations in its implementation does not provide a more transparent evaluation of Mayoian severity than a non-preregistered procedure. In particular, I argue that sample-based validity-enhancing deviations cause an unknown inflation of the test procedure's Type I (familywise) error rate and, consequently, an unknown reduction in its capability to license inferences severely. I conclude that preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12347v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Rubin</dc:creator>
    </item>
    <item>
      <title>Group Difference in Differences can Identify Effect Heterogeneity in Non-Canonical Settings</title>
      <link>https://arxiv.org/abs/2408.16039</link>
      <description>arXiv:2408.16039v2 Announce Type: replace 
Abstract: Consider a very general setting in which data on an outcome of interest is collected in two `groups' at two time periods, with certain group-periods deemed `treated' and others `untreated'. A special case is the canonical Difference-in-Differences (DiD) setting in which one group is treated only in the second period while the other is treated in neither period. Then it is well known that under a parallel trends assumption across the two groups the classic DiD formula (subtracting the average change in the outcome across periods in the treated group by the average change in the outcome across periods in the untreated group) identifies the average treatment effect on the treated in the second period. But other relations between group, period, and treatment are possible. For example, the groups might be demographic (or other baseline covariate) categories with all units in both groups treated in the second period and none treated in the first, i.e. a pre-post design. Or one group might be treated in both periods while the other is treated in neither. In these non-canonical settings (lacking a control group or a pre-period), some researchers still compute DiD estimators, while others avoid causal inference altogether. In this paper, we will elucidate the group-period-treatment scenarios and corresponding parallel trends assumptions under which a DiD formula identifies meaningful causal estimands and what those causal estimands are. We find that in non-canonical settings, under a group parallel trends assumption the DiD formula identifies effect heterogeneity in the treated across groups or across time periods (depending on the setting).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16039v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Laura Hatfield</dc:creator>
    </item>
    <item>
      <title>Weighted Regression with Sybil Networks</title>
      <link>https://arxiv.org/abs/2408.17426</link>
      <description>arXiv:2408.17426v2 Announce Type: replace 
Abstract: In many online domains, Sybil networks -- or cases where a single user assumes multiple identities -- is a pervasive feature. This complicates experiments, as off-the-shelf regression estimators at least assume known network topologies (if not fully independent observations) when Sybil network topologies in practice are often unknown. The literature has exclusively focused on techniques to detect Sybil networks, leading many experimenters to subsequently exclude suspected networks entirely before estimating treatment effects. I present a more efficient solution in the presence of these suspected Sybil networks: a weighted regression framework that applies weights based on the probabilities that sets of observations are controlled by single actors. I show in the paper that the MSE-minimizing solution is to set the weight matrix equal to the inverse of the expected network topology. I demonstrate the methodology on simulated data, and then I apply the technique to a competition with suspected Sybil networks run on the Sui blockchain and show reductions in the standard error of the estimate by 6 - 24%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17426v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nihar Shah</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v2 Announce Type: replace 
Abstract: Joint modeling of different data sources in decision-making processes is crucial for understanding decision dynamics in consumer behavior models. Sequential Sampling Models (SSMs), grounded in neuro-cognitive principles, provide a systematic approach to combining information from multi-source data, such as those based on response times and choice outcomes. However, parameter estimation of SSMs is challenging due to the complexity of joint likelihood functions. Likelihood-Free inference (LFI) approaches enable Bayesian inference in complex models with intractable likelihoods, like SSMs, and only require the ability to simulate synthetic data from the model. Extending a popular approach to simulation efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs calibrated using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a discrepancy for each data source. Multi-objective Bayesian Optimization is then used to ensure simulation efficient approximation of the SSM likelihood. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling both the detection of conflicting information and a deeper understanding of the importance of different data sources in estimating individual SSM parameters. We illustrate the advantages of our approach in comparison with the use of a single discrepancy in a simple synthetic data example and an SSM example with real-world data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles. Although we focus on applications to SSMs, our approach applies to the Likelihood-Free calibration of other models using multi-source data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>Ab initio uncertainty quantification in scattering analysis of microscopy</title>
      <link>https://arxiv.org/abs/2309.02468</link>
      <description>arXiv:2309.02468v4 Announce Type: replace-cross 
Abstract: Estimating parameters from data is a fundamental problem, customarily done by minimizing a loss function between a model and observed statistics. In scattering-based analysis, researchers often employ their domain expertise to select a specific range of wave vectors for analysis, a choice that can vary depending on the specific case. We introduce another paradigm that defines a probabilistic generative model from the beginning of data processing and propagates the uncertainty for parameter estimation, termed the ab initio uncertainty quantification (AIUQ). As an illustrative example, we demonstrate this approach with differential dynamic microscopy (DDM) that extracts dynamical information through Fourier analysis at a selected range of wave vectors. We first show that the conventional way of estimation in DDM is equivalent to fitting a temporal variogram in the reciprocal space using a latent factor model. Then we derive the maximum marginal likelihood estimator, which optimally weighs the information at all wave vectors, therefore eliminating the need to select the range of wave vectors. Furthermore, we substantially reduce the computational cost by utilizing the generalized Schur algorithm for Toeplitz covariances without approximation. Simulated studies validate that AIUQ improves estimation accuracy and enables model selection with automated analysis. The utility of AIUQ is also demonstrated by three distinct sets of experiments: first in an isotropic Newtonian fluid, pushing limits of optically dense systems compared to multiple particle tracking; next in a system undergoing a sol-gel transition, automating the determination of gelling points and critical exponent; and lastly, in discerning anisotropic diffusive behavior of colloids in a liquid crystal. These outcomes collectively underscore AIUQ's versatility to capture system dynamics in an efficient and automated manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02468v4</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.soft</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.110.034601</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 110, 034601 (2024)</arxiv:journal_reference>
      <dc:creator>Mengyang Gu, Yue He, Xubo Liu, Yimin Luo</dc:creator>
    </item>
    <item>
      <title>Separating States in Astronomical Sources Using Hidden Markov Models: With a Case Study of Flaring and Quiescence on EV Lac</title>
      <link>https://arxiv.org/abs/2405.06540</link>
      <description>arXiv:2405.06540v2 Announce Type: replace-cross 
Abstract: We present a new method to distinguish between different states (e.g., high and low, quiescent and flaring) in astronomical sources with count data. The method models the underlying physical process as latent variables following a continuous-space Markov chain that determines the expected Poisson counts in observed light curves in multiple passbands. For the underlying state process, we consider several autoregressive processes, yielding continuous-space hidden Markov models of varying complexity. Under these models, we can infer the state that the object is in at any given time. The continuous state predictions from these models are then dichotomized with the help of a finite mixture model to produce state classifications. We apply these techniques to X-ray data from the active dMe flare star EV Lac, splitting the data into quiescent and flaring states. We find that a first-order vector autoregressive process efficiently separates flaring from quiescence: flaring occurs over 30-40% of the observation durations, a well-defined persistent quiescent state can be identified, and the flaring state is characterized by higher plasma temperatures and emission measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06540v2</guid>
      <category>astro-ph.SR</category>
      <category>astro-ph.HE</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Zimmerman, David A. van Dyk, Vinay L. Kashyap, Aneta Siemiginowska</dc:creator>
    </item>
    <item>
      <title>A Systematic Bias of Machine Learning Regression Models and Its Correction: an Application to Imaging-based Brain Age Prediction</title>
      <link>https://arxiv.org/abs/2405.15950</link>
      <description>arXiv:2405.15950v2 Announce Type: replace-cross 
Abstract: Machine learning models for continuous outcomes often yield systematically biased predictions, particularly for values that largely deviate from the mean. Specifically, predictions for large-valued outcomes tend to be negatively biased (underestimating actual values), while those for small-valued outcomes are positively biased (overestimating actual values). We refer to this linear central tendency warped bias as the "systematic bias of machine learning regression". In this paper, we first demonstrate that this systematic prediction bias persists across various machine learning regression models, and then delve into its theoretical underpinnings. To address this issue, we propose a general constrained optimization approach designed to correct this bias and develop computationally efficient implementation algorithms. Simulation results indicate that our correction method effectively eliminates the bias from the predicted outcomes. We apply the proposed approach to the prediction of brain age using neuroimaging data. In comparison to competing machine learning regression models, our method effectively addresses the longstanding issue of "systematic bias of machine learning regression" in neuroimaging-based brain age calculation, yielding unbiased predictions of brain age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15950v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hwiyoung Lee, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Large-scale Multiple Testing of Cross-covariance Functions with Applications to Functional Network Models</title>
      <link>https://arxiv.org/abs/2407.19399</link>
      <description>arXiv:2407.19399v2 Announce Type: replace-cross 
Abstract: The estimation of functional networks through functional covariance and graphical models have recently attracted increasing attention in settings with high dimensional functional data, where the number of functional variables p is comparable to, and maybe larger than, the number of subjects. However, the existing methods all depend on regularization techniques, which make it unclear how the involved tuning parameters are related to the number of false edges. In this paper, we first reframe the functional covariance model estimation as a tuning-free problem of simultaneously testing p(p-1)/2 hypotheses for cross-covariance functions, and introduce a novel multiple testing procedure. We then explore the multiple testing procedure under a general error-contamination framework and establish that our procedure can control false discoveries asymptotically. Additionally, we demonstrate that our proposed methods for two concrete examples: the functional covariance model for discretely observed functional data and, importantly, the more challenging functional graphical model, can be seamlessly integrated into the general error-contamination framework, and, with verifiable conditions, achieve theoretical guarantees on effective false discovery control. Finally, we showcase the superiority of our proposals through extensive simulations and brain connectivity analysis of two neuroimaging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19399v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Fang, Qing Jiang, Xinghao Qiao</dc:creator>
    </item>
    <item>
      <title>EnsLoss: Stochastic Calibrated Loss Ensembles for Preventing Overfitting in Classification</title>
      <link>https://arxiv.org/abs/2409.00908</link>
      <description>arXiv:2409.00908v2 Announce Type: replace-cross 
Abstract: Empirical risk minimization (ERM) with a computationally feasible surrogate loss is a widely accepted approach for classification. Notably, the convexity and calibration (CC) properties of a loss function ensure consistency of ERM in maximizing accuracy, thereby offering a wide range of options for surrogate losses. In this article, we propose a novel ensemble method, namely EnsLoss, which extends the ensemble learning concept to combine loss functions within the ERM framework. A key feature of our method is the consideration on preserving the "legitimacy" of the combined losses, i.e., ensuring the CC properties. Specifically, we first transform the CC conditions of losses into loss-derivatives, thereby bypassing the need for explicit loss functions and directly generating calibrated loss-derivatives. Therefore, inspired by Dropout, EnsLoss enables loss ensembles through one training process with doubly stochastic gradient descent (i.e., random batch samples and random calibrated loss-derivatives). We theoretically establish the statistical consistency of our approach and provide insights into its benefits. The numerical effectiveness of EnsLoss compared to fixed loss methods is demonstrated through experiments on a broad range of 14 OpenML tabular datasets and 46 image datasets with various deep learning architectures. Python repository and source code are available on GitHub at https://github.com/statmlben/ensloss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00908v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ben Dai</dc:creator>
    </item>
  </channel>
</rss>
