<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Jan 2026 05:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Diffusion Large Language Models for Black-Box Optimization</title>
      <link>https://arxiv.org/abs/2601.14446</link>
      <description>arXiv:2601.14446v1 Announce Type: new 
Abstract: Offline black-box optimization (BBO) aims to find optimal designs based solely on an offline dataset of designs and their labels. Such scenarios frequently arise in domains like DNA sequence design and robotics, where only a few labeled data points are available. Traditional methods typically rely on task-specific proxy or generative models, overlooking the in-context learning capabilities of pre-trained large language models (LLMs). Recent efforts have adapted autoregressive LLMs to BBO by framing task descriptions and offline datasets as natural language prompts, enabling direct design generation. However, these designs often contain bidirectional dependencies, which left-to-right models struggle to capture. In this paper, we explore diffusion LLMs for BBO, leveraging their bidirectional modeling and iterative refinement capabilities. This motivates our in-context denoising module: we condition the diffusion LLM on the task description and the offline dataset, both formatted in natural language, and prompt it to denoise masked designs into improved candidates. To guide the generation toward high-performing designs, we introduce masked diffusion tree search, which casts the denoising process as a step-wise Monte Carlo Tree Search that dynamically balances exploration and exploitation. Each node represents a partially masked design, each denoising step is an action, and candidates are evaluated via expected improvement under a Gaussian Process trained on the offline dataset. Our method, dLLM, achieves state-of-the-art results in few-shot settings on design-bench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14446v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ye Yuan (Sam),  Can (Sam),  Chen, Zipeng Sun, Dinghuai Zhang, Christopher Pal, Xue Liu</dc:creator>
    </item>
    <item>
      <title>Operationalising DAO Sustainability KPIs: A Multi-Chain Dashboard for Governance Analytics</title>
      <link>https://arxiv.org/abs/2601.14927</link>
      <description>arXiv:2601.14927v1 Announce Type: new 
Abstract: We present DAO Portal, a production-grade analytics pipeline and interactive dashboard for assessing the sustainability of Decentralised Autonomous Organisations (DAOs) through Key Performance Indicators (KPIs) derived from on-chain governance and token events. Building on our previous work, which defined and validated a multidimensional KPI framework for DAO sustainability, this paper moves from theory to practice by operationalising that framework in software infrastructure designed for finance and FinTech contexts. The system ingests governance and treasury data from major EVM networks, harmonises the outputs, and computes sustainability scores across four dimensions: participation, accumulated funds, voting efficiency, and decentralisation. A composite 0 to 12 score is then derived using transparent thresholds that are applied client-side in the browser.
  Using a curated snapshot of more than 50 active DAOs covering 6,930 proposals and 317,317 unique voting addresses, we show how the platform surfaces recurring patterns such as persistently low participation and concentration of proposal activity. These results demonstrate how DAO Portal supports the diagnosis of governance risks and the comparison of design choices across DAOs. To promote reproducibility and adoption, we release source code, data schema, and dashboard implementation. By turning governance traces into measurable and explainable KPIs, DAO Portal provides auditable evidence of DAO sustainability and contributes software engineering infrastructure for financial applications where treasuries and decision-making rights involve significant assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14927v1</guid>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Silvio Meneguzzo, Claudio Schifanella, Valentina Gatteschi, Giuseppe Destefanis</dc:creator>
    </item>
    <item>
      <title>Unsupervised Material Fingerprinting: Ultra-fast hyperelastic model discovery from full-field experimental measurements</title>
      <link>https://arxiv.org/abs/2601.14965</link>
      <description>arXiv:2601.14965v1 Announce Type: new 
Abstract: Material Fingerprinting is a lookup table-based strategy to discover material models from experimental measurements, which completely avoids the need to solve an optimization problem. In an offline phase, a comprehensive database of simulated material responses, so-called material fingerprints, is generated for a predefined experimental setup. This database can then be used repeatedly in the online phase to discover material models corresponding to experimentally measured observations. To this end, the experimentally measured fingerprint is compared with all fingerprints in the database to identify the closest match. The primary advantage of this strategy is that it does not require solving a continuous optimization problem. This avoids the associated computational costs as well as issues of ill-posedness caused by local minima in non-convex optimization landscapes. Material Fingerprinting has been successfully demonstrated for supervised datasets consisting of stress-strain pairs, as well as for unsupervised datasets involving full-field displacements and net reaction forces. However, to date, there is no experimental validation for the latter approach which is the objective of this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14965v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Flaschel, Miguel Angel Moreno-Mateos, Simon Wiesheier, Paul Steinmann, Ellen Kuhl</dc:creator>
    </item>
    <item>
      <title>The Limits of Lognormal: Assessing Cryptocurrency Volatility and VaR using Geometric Brownian Motion</title>
      <link>https://arxiv.org/abs/2601.14272</link>
      <description>arXiv:2601.14272v1 Announce Type: cross 
Abstract: The integration of cryptocurrencies into institutional portfolios necessitates the adoption of robust risk modeling frameworks. This study is a part of a series of subsequent works to fine-tune model risk analysis for cryptocurrencies. Through this first research work, we establish a foundational benchmark by applying the traditional industry-standard Geometric Brownian Motion (GBM) model. Popularly used for non-crypto financial assets, GBM assumes Lognormal return distributions for a multi-asset cryptocurrency portfolio (XRP, SOL, ADA). This work utilizes Maximum Likelihood Estimation and a correlated Monte Carlo Simulation incorporating the Cholesky decomposition of historical covariance. We present our stock portfolio model as a Minimum Variance Portfolio (MVP). We observe the model's structural shift within the heavy-tailed, non-Gaussian cryptocurrency environment. The results reveal limitations of the Lognormal assumption: the calculated Value-at-Risk at the 5% confidence level over the one-year horizon. For baselining our results, we also present a holistic comparative analysis with an equity portfolio (AAPL, TSLA, NVDA), demonstrating a significantly lower failure rate. This performance provides conclusive evidence that the GBM model is fundamentally the perfect benchmark for our subsequent works. Results from this novel work will be an indicator for the success criteria in our future model for crypto risk management, rigorously motivating the development and application of advanced models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14272v1</guid>
      <category>q-fin.RM</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ekleen Kaur</dc:creator>
    </item>
    <item>
      <title>DeepInflation: an AI agent for research and model discovery of inflation</title>
      <link>https://arxiv.org/abs/2601.14288</link>
      <description>arXiv:2601.14288v1 Announce Type: cross 
Abstract: We present \textbf{DeepInflation}, an AI agent designed for research and model discovery in inflationary cosmology. Built upon a multi-agent architecture, \textbf{DeepInflation} integrates Large Language Models (LLMs) with a symbolic regression (SR) engine and a retrieval-augmented generation (RAG) knowledge base. This framework enables the agent to automatically explore and verify the vast landscape of inflationary potentials while grounding its outputs in established theoretical literature. We demonstrate that \textbf{DeepInflation} can successfully discover simple and viable single-field slow-roll inflationary potentials consistent with the latest observations (here ACT DR6 results as example) or any given $n_s$ and $r$, and provide accurate theoretical context for obscure inflationary scenarios. \textbf{DeepInflation} serves as a prototype for a new generation of autonomous scientific discovery engines in cosmology, which enables researchers and non-experts alike to explore the inflationary landscape using natural language. This agent is available at https://github.com/pengzy-cosmo/DeepInflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14288v1</guid>
      <category>astro-ph.CO</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>gr-qc</category>
      <category>hep-th</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ze-Yu Peng, Hao-Shi Yuan, Qi Lai, Jun-Qian Jiang, Gen Ye, Jun Zhang, Yun-Song Piao</dc:creator>
    </item>
    <item>
      <title>Case-Guided Sequential Assay Planning in Drug Discovery</title>
      <link>https://arxiv.org/abs/2601.14710</link>
      <description>arXiv:2601.14710v1 Announce Type: cross 
Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14710v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianchi Chen, Jan Bima, Sean L. Wu, Otto Ritter, Bingjia Yang, Xiang Yu</dc:creator>
    </item>
    <item>
      <title>Efficient Parameter Calibration of Numerical Weather Prediction Models via Evolutionary Sequential Transfer Optimization</title>
      <link>https://arxiv.org/abs/2601.08663</link>
      <description>arXiv:2601.08663v2 Announce Type: replace 
Abstract: The configuration of physical parameterization schemes in Numerical Weather Prediction (NWP) models plays a critical role in determining the accuracy of the forecast. However, existing parameter calibration methods typically treat each calibration task as an isolated optimization problem. This approach suffers from prohibitive computational costs and necessitates performing iterative searches from scratch for each task, leading to low efficiency in sequential calibration scenarios. To address this issue, we propose the SEquential Evolutionary Transfer Optimization (SEETO) algorithm driven by the representations of the meteorological state. First, to accurately measure the physical similarity between calibration tasks, a meteorological state representation extractor is introduced to map high-dimensional meteorological fields into latent representations. Second, given the similarity in the latent space, a bi-level adaptive knowledge transfer mechanism is designed. At the solution level, superior populations from similar historical tasks are reused to achieve a "warm start" for optimization. At the model level, an ensemble surrogate model based on source task data is constructed to assist the search, employing an adaptive weighting mechanism to dynamically balance the contributions of source domain knowledge and target domain data. Extensive experiments across 10 distinct calibration tasks, which span varying source-target similarities, highlight SEETO's superior efficiency. Under a strict budget of 20 expensive evaluations, SEETO achieves a 6% average improvement in Hypervolume (HV) over two state-of-the-art baselines. Notably, to match SEETO's performance at this stage, the comparison algorithms would require an average of 64% and 28% additional evaluations, respectively. This presents a new paradigm for the efficient and accurate automated calibration of NWP model parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08663v2</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heping Fang, Bingdong Li, Peng Yang</dc:creator>
    </item>
    <item>
      <title>Development and Experimental Validation of Novel Evaluation Criteria for Turbulent Two-Phase VOF Simulations in High-Pressure Die Casting</title>
      <link>https://arxiv.org/abs/2601.05701</link>
      <description>arXiv:2601.05701v2 Announce Type: replace-cross 
Abstract: Air entrapment during mold filling critically affects porosity and overall casting quality in High Pressure Die Casting. This study assesses the feasibility of applying the vof method within OpenFOAM to simulate compressible, turbulent mold filling in a thin-walled geometry. Three-dimensional simulations with the "compressibleInterFoam" solver were carried out under ambient initial cavity conditions, using both laminar flow and the k-e turbulence model. The free surface dynamics were examined across a range of inlet velocities to evaluate their influence on interface morphology, cavity pressurization, and gas entrapment. To quantify these effects, three evaluation criteria were introduced: the TIFSA as a measure of oxidation risk, the TMVF as an indicator of filling continuity and air entrapment, and the TIVF as a proxy for surface loading. Results show that turbulence modeling accelerates pressurization and limits the persistence of entrapped gas, with velocity governing the balance between smooth filling, turbulent breakup, and exposure duration. Comparison with experimental casting trials, including CT based porosity analysis and photogrammetric surface evaluation, validated that the model captures key defect mechanisms and provides quantitative guidance for process optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05701v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>physics.app-ph</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mehran Shazedeh, Fabian Teichmann, Sebastian M\"uller</dc:creator>
    </item>
  </channel>
</rss>
