<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Potential-energy gating for robust state estimation in bistable stochastic systems</title>
      <link>https://arxiv.org/abs/2602.11712</link>
      <description>arXiv:2602.11712v1 Announce Type: cross 
Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p &lt; 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11712v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luigi Simeone</dc:creator>
    </item>
    <item>
      <title>Learnability Window in Gated Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2512.05790</link>
      <description>arXiv:2512.05790v5 Announce Type: replace-cross 
Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the effective learning rates $\mu_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ scales as $N(\ell)\propto f(\ell)^{-\kappa_\alpha}$, where $f(\ell)=\|\mu_{t,\ell}\|_1$ is the effective learning rate envelope and $\kappa_\alpha=\alpha/(\alpha-1)$ is the concentration exponent governing empirical averages. This yields an explicit characterization of $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory shows that the time-scale spectra induced by the effective learning rates are the dominant determinants of learnability: broader or more heterogeneous spectra slow the decay of $f(\ell)$, enlarging the learnability window, while heavy-tailed noise uniformly compresses $\mathcal{H}_N$ by slowing statistical concentration to $N^{-1/\kappa_{\alpha}}$. By integrating gate-induced time-scale geometry with gradient noise and sample complexity, the framework identifies effective learning rates as the primary objects that determine whether, when, and over what horizons recurrent networks can learn long-range temporal dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05790v5</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Livi</dc:creator>
    </item>
    <item>
      <title>The search for the gust-wing interaction "textbook"</title>
      <link>https://arxiv.org/abs/2602.10968</link>
      <description>arXiv:2602.10968v2 Announce Type: replace-cross 
Abstract: We address whether complex physical relations can be investigated through the synergy of automated high-volume experiments and the reduction of large datasets to a concise, representative subset of canonical examples -- a "textbook". To this end, we consider the unsteady aerodynamics of wing-gust interactions, which is characterized by its rich, high-dimensional physics. We take advantage of a purpose-built gust generator to systematically produce over 1,000 distinct random gust events and to measure the unsteady loads induced on a delta wing. We then employ a data summarization procedure to identify representative subsets of increasing size from the large-scale database, which then serve as training data for a machine-learning model of the aerodynamic loads from sparse pressure measurements. An appropriately selected "textbook" of a few events can achieve predictive accuracy comparable to random training sets up to two orders of magnitude larger, capturing the intrinsic diversity of the full-scale data and enhancing modeling efficiency and interpretability. Our methodology evidences the potential of distilling the essential information contained in large amounts of experimental observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10968v2</guid>
      <category>physics.flu-dyn</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Olivucci, David E. Rival</dc:creator>
    </item>
  </channel>
</rss>
