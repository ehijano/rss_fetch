<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2024 02:58:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact distribution-free tests of spherical symmetry applicable to high dimensional data</title>
      <link>https://arxiv.org/abs/2412.05608</link>
      <description>arXiv:2412.05608v1 Announce Type: new 
Abstract: We develop some graph-based tests for spherical symmetry of a multivariate distribution using a method based on data augmentation. These tests are constructed using a new notion of signs and ranks that are computed along a path obtained by optimizing an objective function based on pairwise dissimilarities among the observations in the augmented data set. The resulting tests based on these signs and ranks have the exact distribution-free property, and irrespective of the dimension of the data, the null distributions of the test statistics remain the same. These tests can be conveniently used for high-dimensional data, even when the dimension is much larger than the sample size. Under appropriate regularity conditions, we prove the consistency of these tests in high dimensional asymptotic regime, where the dimension grows to infinity while the sample size may or may not grow with the dimension. We also propose a generalization of our methods to take care of the situations, where the center of symmetry is not specified by the null hypothesis. Several simulated data sets and a real data set are analyzed to demonstrate the utility of the proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05608v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Improved estimation of the positive powers ordered restricted standard deviation of two normal populations</title>
      <link>https://arxiv.org/abs/2412.05620</link>
      <description>arXiv:2412.05620v1 Announce Type: new 
Abstract: The present manuscript is concerned with component-wise estimation of the positive power of ordered restricted standard deviation of two normal populations with certain restrictions on the means. We propose several improved estimators under a general scale invariant bowl-shaped loss function. Also, we proposed a class of improved estimators. It has been shown that the boundary estimator of this class is a generalized Bayes. As an application, the improved estimators are obtained with respect to quadratic loss, entropy loss, and a symmetric loss function. We have conducted extensive Monte Carlo simulations to study and compare the risk performance of the proposed estimators. Finally, a real life data analysis is given to illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05620v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Somnath Mondal, Lakshmi Kanta Patra</dc:creator>
    </item>
    <item>
      <title>Large-sample analysis of cost functionals for inference under the coalescent</title>
      <link>https://arxiv.org/abs/2412.06004</link>
      <description>arXiv:2412.06004v1 Announce Type: new 
Abstract: The coalescent is a foundational model of latent genealogical trees under neutral evolution, but suffers from intractable sampling probabilities. Methods for approximating these sampling probabilities either introduce bias or fail to scale to large sample sizes. We show that a class of cost functionals of the coalescent with recurrent mutation and a finite number of alleles converge to tractable processes in the infinite-sample limit. A particular choice of costs yields insight about importance sampling methods, which are a classical tool for coalescent sampling probability approximation. These insights reveal that the behaviour of coalescent importance sampling algorithms differs markedly from standard sequential importance samplers, with or without resampling. We conduct a simulation study to verify that our asymptotics are accurate for algorithms with finite (and moderate) sample sizes. Our results also facilitate the a priori optimisation of computational resource allocation for coalescent sequential importance sampling. We do not observe the same behaviour for importance sampling methods under the infinite sites model of mutation, which is regarded as a good and more tractable approximation of finite alleles mutation in most respects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06004v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Favero, Jere Koskela</dc:creator>
    </item>
    <item>
      <title>UCB algorithms for multi-armed bandits: Precise regret and adaptive inference</title>
      <link>https://arxiv.org/abs/2412.06126</link>
      <description>arXiv:2412.06126v1 Announce Type: new 
Abstract: Upper Confidence Bound (UCB) algorithms are a widely-used class of sequential algorithms for the $K$-armed bandit problem. Despite extensive research over the past decades aimed at understanding their asymptotic and (near) minimax optimality properties, a precise understanding of their regret behavior remains elusive. This gap has not only hindered the evaluation of their actual algorithmic efficiency, but also limited further developments in statistical inference in sequential data collection.
  This paper bridges these two fundamental aspects--precise regret analysis and adaptive statistical inference--through a deterministic characterization of the number of arm pulls for an UCB index algorithm [Lai87, Agr95, ACBF02]. Our resulting precise regret formula not only accurately captures the actual behavior of the UCB algorithm for finite time horizons and individual problem instances, but also provides significant new insights into the regimes in which the existing theory remains informative. In particular, we show that the classical Lai-Robbins regret formula is exact if and only if the sub-optimality gaps exceed the order $\sigma\sqrt{K\log T/T}$. We also show that its maximal regret deviates from the minimax regret by a logarithmic factor, and therefore settling its strict minimax optimality in the negative.
  The deterministic characterization of the number of arm pulls for the UCB algorithm also has major implications in adaptive statistical inference. Building on the seminal work of [Lai82], we show that the UCB algorithm satisfies certain stability properties that lead to quantitative central limit theorems in two settings including the empirical means of unknown rewards in the bandit setting. These results have an important practical implication: conventional confidence sets designed for i.i.d. data remain valid even when data are collected sequentially.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06126v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Koulik Khamaru, Cun-Hui Zhang</dc:creator>
    </item>
    <item>
      <title>Diffusion on the circle and a stochastic correlation model</title>
      <link>https://arxiv.org/abs/2412.06343</link>
      <description>arXiv:2412.06343v1 Announce Type: new 
Abstract: We propose analytically tractable SDE models for correlation in financial markets. We study diffusions on the circle, namely the Brownian motion on the circle and the von Mises process, and consider these as models for correlation. The von Mises process was proposed in Kent (1975) as a probabilistic justification for the von Mises distribution which is widely used in Circular statistics. The transition density of the von Mises process has been unknown, we identify an approximate analytic transition density for the von Mises process. We discuss the estimation of these diffusion models and a stochastic correlation model in finance. We illustrate the application of the proposed model on real-data of equity-currency pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06343v1</guid>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Majumdar, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals As Analogues to Profile Likelihood Ratio Confidence Intervals for Modes of Unimodal Distributions</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v2 Announce Type: new 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, it is sometimes criticized for being transformation invariant.
  We make the case that the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). First we provide background on the HPD interval as well as the Likelihood Ratio Test statistic and its inversion to generate asymptotically-correct CIs. Our main result is to show that the HPD interval has similar desirable properties as the profile LRCI, such as transformation invariance with respect to the mode for monotonic functions. We then discuss an application of the main result, an example case which compares the profile LRCI for the binomial probability parameter p with the Bayesian HPD interval for the beta distribution density function, both of which are used to estimate population proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
    <item>
      <title>Optimal estimation in private distributed functional data analysis</title>
      <link>https://arxiv.org/abs/2412.06582</link>
      <description>arXiv:2412.06582v1 Announce Type: new 
Abstract: We systematically investigate the preservation of differential privacy in functional data analysis, beginning with functional mean estimation and extending to varying coefficient model estimation. Our work introduces a distributed learning framework involving multiple servers, each responsible for collecting several sparsely observed functions. This hierarchical setup introduces a mixed notion of privacy. Within each function, user-level differential privacy is applied to $m$ discrete observations. At the server level, central differential privacy is deployed to account for the centralised nature of data collection. Across servers, only private information is exchanged, adhering to federated differential privacy constraints. To address this complex hierarchy, we employ minimax theory to reveal several fundamental phenomena: from sparse to dense functional data analysis, from user-level to central and federated differential privacy costs, and the intricate interplay between different regimes of functional data analysis and privacy preservation.
  To the best of our knowledge, this is the first study to rigorously examine functional data estimation under multiple privacy constraints. Our theoretical findings are complemented by efficient private algorithms and extensive numerical evidence, providing a comprehensive exploration of this challenging problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06582v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gengyu Xue, Zhenhua Lin, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Solving a global optimal problem requires only two-armed slot machine</title>
      <link>https://arxiv.org/abs/2412.05604</link>
      <description>arXiv:2412.05604v1 Announce Type: cross 
Abstract: For a general purpose optimization problem over a finite rectangle region, this paper pioneers a unified slot machine framework for global optimization by transforming the search for global optimizer(s) to the optimal strategy formulation of a bandit process in infinite policy sets and proves that two-armed bandit is enough. By leveraging the strategic bandit process-driven optimization framework, we introduce a new {\bf S}trategic {\bf M}onte {\bf C}arlo {\bf O}ptimization (SMCO) algorithm that coordinate-wisely generates points from multiple paired distributions and can be implemented parallel for high-dimensional continuous functions. Our SMCO algorithm, equipped with tree search that broadens the optimal policy search space of slot machine for attaining the global optimizer(s) of a multi-modal function, facilitates fast learning via trial and error. We provide a strategic law of large numbers for nonlinear expectations in bandit settings, and establish that our SMCO algorithm converges to global optimizer(s) almost surely. Unlike the standard gradient descent ascent (GDA) that uses a one-leg walk to climb the mountain and is sensitive to starting points and step sizes, our SMCO algorithm takes a two-leg walk to the peak by using the two-sided sampling from the paired distributions and is not sensitive to initial point selection or step size constraints. Numerical studies demonstrate that the new SMCO algorithm outperforms GDA, particle swarm optimization and simulated annealing in both convergence accuracy and speed. Our SMCO algorithm should be extremely useful for finding optimal tuning parameters in many large scale complex optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05604v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Zengjing Chen, Xiaodong Yan, Guodong Zhang, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors</title>
      <link>https://arxiv.org/abs/2412.05673</link>
      <description>arXiv:2412.05673v1 Announce Type: cross 
Abstract: This paper presents a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, achieving a balance between quadratic and absolute linear loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data effectively. The generalized Bayesian approach constructs a working likelihood utilizing the SPH loss that facilitates efficient and stable estimation while providing rigorous estimation uncertainty quantification for all model parameters. Notably, this allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients -- e.g., ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings -- the framework ensures principled inference. We establish rigorous theoretical guarantees for the accurate estimation of underlying model parameters and the correct selection of predictor variables under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superiority of our approach compared to traditional quadratic and absolute linear loss-based Bayesian regression methods, highlighting its flexibility and robustness in high-dimensional and challenging data contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05673v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Kshitij Khare, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Convolution Mode Regression</title>
      <link>https://arxiv.org/abs/2412.05736</link>
      <description>arXiv:2412.05736v1 Announce Type: cross 
Abstract: For highly skewed or fat-tailed distributions, mean or median-based methods often fail to capture the central tendencies in the data. Despite being a viable alternative, estimating the conditional mode given certain covariates (or mode regression) presents significant challenges. Nonparametric approaches suffer from the "curse of dimensionality", while semiparametric strategies often lead to non-convex optimization problems. In order to avoid these issues, we propose a novel mode regression estimator that relies on an intermediate step of inverting the conditional quantile density. In contrast to existing approaches, we employ a convolution-type smoothed variant of the quantile regression. Our estimator converges uniformly over the design points of the covariates and, unlike previous quantile-based mode regressions, is uniform with respect to the smoothing bandwidth. Additionally, the Convolution Mode Regression is dimension-free, carries no issues regarding optimization and preliminary simulations suggest the estimator is normally distributed in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05736v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduardo Schirmer Finn, Eduardo Horta</dc:creator>
    </item>
    <item>
      <title>Partial identification of principal causal effects under violations of principal ignorability</title>
      <link>https://arxiv.org/abs/2412.06628</link>
      <description>arXiv:2412.06628v1 Announce Type: cross 
Abstract: Principal stratification is a general framework for studying causal mechanisms involving post-treatment variables. When estimating principal causal effects, the principal ignorability assumption is commonly invoked, which we study in detail in this manuscript. Our first key contribution is studying a commonly used strategy of using parametric models to jointly model the outcome and principal strata without requiring the principal ignorability assumption. We show that even if the joint distribution of principal strata is known, this strategy necessarily leads to only partial identification of causal effects, even under very simple and correctly specified outcome models. While principal ignorability can lead to point identification in this setting, we discuss alternative, weaker assumptions and show how they lead to more informative partial identification regions. An additional contribution is that we provide theoretical support to strategies used in the literature for identifying association parameters that govern the joint distribution of principal strata. We prove that this is possible, but only if the principal ignorability assumption is violated. Additionally, due to partial identifiability of causal effects even when these association parameters are known, we show that these association parameters are only identifiable under strong parametric constraints. Lastly, we extend these results to more flexible semiparametric and nonparametric Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06628v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxuan Wu, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Increasing Domain Infill Asymptotics for Stochastic Differential Equations Driven by Fractional Brownian Motion</title>
      <link>https://arxiv.org/abs/2005.09577</link>
      <description>arXiv:2005.09577v4 Announce Type: replace 
Abstract: Although statistical inference in stochastic differential equations (SDEs) driven by Wiener process has received significant attention in the literature, inference in those driven by fractional Brownian motion seem to have seen much less development in comparison, despite their importance in modeling long range dependence. In this article, we consider both classical and Bayesian inference in such fractional Brownian motion based SDEs. In particular, we consider asymptotic inference for two parameters in this regard; a multiplicative parameter associated with the drift function, and the so-called "Hurst parameter" of the fractional Brownian motion, when the time domain tends to infinity. For unknown Hurst parameter, the likelihood does not lend itself amenable to the popular Girsanov form, rendering usual asymptotic development difficult. As such, we develop increasing domain infill asymptotic theory, by discretizing the SDE. In this setup, we establish consistency and asymptotic normality of the maximum likelihood estimators, as well as consistency and asymptotic normality of the Bayesian posterior distributions. However, classical or Bayesian asymptotic normality with respect to the Hurst parameter could not be established. We supplement our theoretical investigations with simulation studies in a non-asymptotic setup, prescribing suitable methodologies for classical and Bayesian analyses of SDEs driven by fractional Brownian motion. Applications to a real, close price data, along with comparison with standard SDE driven by Wiener process, is also considered. As expected, it turned out that our Bayesian fractional SDE triumphed over the other model and methods, in both simulated and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2005.09577v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trisha Maitra, Sourabh Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Consistency of MLE for partially observed diffusions, with application in market microstructure modeling</title>
      <link>https://arxiv.org/abs/2201.07656</link>
      <description>arXiv:2201.07656v3 Announce Type: replace 
Abstract: This paper presents a tractable sufficient condition for the consistency of maximum likelihood estimators (MLEs) in partially observed diffusion models, stated in terms of stationary distribution of the associated fully observed diffusion, under the assumption that the set of unknown parameter values is finite. This sufficient condition is then verified in the context of a latent price model of market microstructure, yielding consistency of maximum likelihood estimators of the unknown parameters in this model. Finally, we compute the latter estimators using historical financial data taken from the NASDAQ exchange.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.07656v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.TR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergey Nadtochiy, Yuan Yin</dc:creator>
    </item>
    <item>
      <title>Residual permutation test for regression coefficient testing</title>
      <link>https://arxiv.org/abs/2211.16182</link>
      <description>arXiv:2211.16182v3 Announce Type: replace 
Abstract: We consider the problem of testing whether a single coefficient is equal to zero in linear models when the dimension of covariates $p$ can be up to a constant fraction of sample size $n$. In this regime, an important topic is to propose tests with finite-population valid size control without requiring the noise to follow strong distributional assumptions. In this paper, we propose a new method, called residual permutation test (RPT), which is constructed by projecting the regression residuals onto the space orthogonal to the union of the column spaces of the original and permuted design matrices. RPT can be proved to achieve finite-population size validity under fixed design with just exchangeable noises, whenever $p &lt; n / 2$. Moreover, RPT is shown to be asymptotically powerful for heavy tailed noises with bounded $(1+t)$-th order moment when the true coefficient is at least of order $n^{-t/(1+t)}$ for $t \in [0,1]$. We further proved that this signal size requirement is essentially rate-optimal in the minimax sense. Numerical studies confirm that RPT performs well in a wide range of simulation settings with normal and heavy-tailed noise distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16182v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyue Wen, Tengyao Wang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>A minimax optimal approach to high-dimensional double sparse linear regression</title>
      <link>https://arxiv.org/abs/2305.04182</link>
      <description>arXiv:2305.04182v2 Announce Type: replace 
Abstract: In this paper, we focus our attention on the high-dimensional double sparse linear regression, that is, a combination of element-wise and group-wise sparsity. To address this problem, we propose an IHT-style (iterative hard thresholding) procedure that dynamically updates the threshold at each step. We establish the matching upper and lower bounds for parameter estimation, showing the optimality of our proposal in the minimax sense. More importantly, we introduce a fully adaptive optimal procedure designed to address unknown sparsity and noise levels. Our adaptive procedure demonstrates optimal statistical accuracy with fast convergence. Additionally, we elucidate the significance of the element-wise sparsity level $s_0$ as the trade-off between IHT and group IHT, underscoring the superior performance of our method over both. Leveraging the beta-min condition, we establish that our IHT-style procedure can attain the oracle estimation rate and achieve almost full recovery of the true support set at both the element level and group level. Finally, we demonstrate the superiority of our method by comparing it with several state-of-the-art algorithms on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04182v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of machine learning research, 2024</arxiv:journal_reference>
      <dc:creator>Yanhang Zhang, Zhifan Li, Shixiang Liu, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>Stability of Sequential Lateration and of Stress Minimization in the Presence of Noise</title>
      <link>https://arxiv.org/abs/2310.10900</link>
      <description>arXiv:2310.10900v3 Announce Type: replace 
Abstract: Sequential lateration is a class of methods for multidimensional scaling where a suitable subset of nodes is first embedded by some method, e.g., a clique embedded by classical scaling, and then the remaining nodes are recursively embedded by lateration. A graph is a lateration graph when it can be embedded by such a procedure. We provide a stability result for a particular variant of sequential lateration. We do so in a setting where the dissimilarities represent noisy Euclidean distances between nodes in a geometric lateration graph. We then deduce, as a corollary, a perturbation bound for stress minimization. To argue that our setting applies broadly, we show that a (large) random geometric graph is a lateration graph with high probability under mild conditions, extending a previous result of Aspnes et al (2006).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10900v3</guid>
      <category>math.ST</category>
      <category>cs.NI</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ery Arias-Castro, Siddharth Vishwanath</dc:creator>
    </item>
    <item>
      <title>Nonparametric density estimation for the small jumps of L\'evy processes</title>
      <link>https://arxiv.org/abs/2404.09725</link>
      <description>arXiv:2404.09725v2 Announce Type: replace 
Abstract: We consider the problem of estimating the density of the process associated with the small jumps of a pure jump L\'evy process, possibly of infinite variation, from discrete observations of one trajectory. The interest of such a question lies on the observation that even when the L\'evy measure is known, the density of the increments of the small jumps of the process cannot be computed in closed-form. We discuss results both from low and high frequency observations. In a low frequency setting, assuming the L\'evy density associated with the jumps larger than $\varepsilon\in (0,1]$ in absolute value is known, a spectral estimator relying on the convolution structure of the problem achieves a parametric rate of convergence with respect to the integrated $L_2$ loss, up to a logarithmic factor. In a high frequency setting, we remove the assumption on the knowledge of the L\'evy measure of the large jumps and show that the rate of convergence depends both on the sampling scheme and on the behaviour of the L\'evy measure in a neighborhood of zero. We show that the rate we find is minimax up to a logarithmic factor. An adaptive penalized procedure is studied to select the cutoff parameter. These results are extended to encompass the case where a Brownian component is present in the L\'evy process. Furthermore, we illustrate numerically the performances of our procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09725v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Duval, Taher Jalal, Ester Mariucci</dc:creator>
    </item>
    <item>
      <title>Towards a unified theory for testing statistical hypothesis: Multinormal mean with nuisance covariance matrix</title>
      <link>https://arxiv.org/abs/2411.12532</link>
      <description>arXiv:2411.12532v3 Announce Type: replace 
Abstract: Under a multinormal distribution with an arbitrary unknown covariance matrix, the main purpose of this paper is to propose a framework to achieve the goal of reconciliation of Bayesian, frequentist, and Fisher's reporting $p$-values, Neyman-Pearson's optimal theory and Wald's decision theory for the problems of testing mean against restricted alternatives (closed convex cones). To proceed, the tests constructed via the likelihood ratio (LR) and the union-intersection (UI) principles are studied. For the problems of testing against restricted alternatives, first, we show that the LRT and the UIT are not the proper Bayes tests, however, they are shown to be the integrated LRT and the integrated UIT, respectively. For the problem of testing against the positive orthant space alternative, both the null distributions of the LRT and the UIT depend on the unknown nuisance covariance matrix. Hence we have difficulty adopting Fisher's approach to reporting $p$-values. On the other hand, according to the definition of the level of significance, both the LRT and the UIT are shown to be power-dominated by the corresponding LRT and UIT for testing against the half-space alternative, respectively. Hence, both the LRT and the UIT are $\alpha$-inadmissible, these results are against the common statistical sense. Neither Fisher's approach of reporting $p$-values alone nor Neyman-Pearson's optimal theory for power function alone is a satisfactory criterion for evaluating the performance of tests. Wald's decision theory via $d$-admissibility may shed light on resolving these challenging issues of imposing the balance between type 1 error and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12532v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming-Tien Tsai</dc:creator>
    </item>
    <item>
      <title>Identifiability implies consistency of MLE in partially observed diffusions on a torus</title>
      <link>https://arxiv.org/abs/2412.03380</link>
      <description>arXiv:2412.03380v2 Announce Type: replace 
Abstract: In this paper, we consider a general partially observed diffusion model with periodic coefficients and with non-degenerate diffusion component. The coefficients of such a model depend on an unknown (static and deterministic) parameter which needs to be estimated based on the observed component of the diffusion process. We show that, under a minimal assumption of identifiability, and given enough regularity of the diffusion coefficients, a maximum likelihood estimator of the unknown parameter converges to the true parameter value as the sample size grows to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03380v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Ekren, Sergey Nadtochiy</dc:creator>
    </item>
    <item>
      <title>Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound</title>
      <link>https://arxiv.org/abs/2202.05560</link>
      <description>arXiv:2202.05560v3 Announce Type: replace-cross 
Abstract: Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of $M$ error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.05560v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2024</arxiv:journal_reference>
      <dc:creator>Reuben Adams, John Shawe-Taylor, Benjamin Guedj</dc:creator>
    </item>
    <item>
      <title>Breakdown points of Fermat-Weber problems under gauge distances</title>
      <link>https://arxiv.org/abs/2306.13424</link>
      <description>arXiv:2306.13424v2 Announce Type: replace-cross 
Abstract: We compute the robustness of Fermat-Weber points with respect to any finite gauge. We show a breakdown point of $1/(1+\sigma)$ where $\sigma$ is the asymmetry measure of the gauge. We obtain quantitative results indicating how far a corrupted Fermat-Weber point can lie from the true value in terms of the original sample and the size of the corrupted part. If the distance from the true value depends only on the original sample, then we call the gauge `uniformly robust.' We show that polyhedral gauges are uniformly robust, but locally strictly convex norms are not, while in dimension 2 any uniform robust gauge is polyhedral.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13424v2</guid>
      <category>math.MG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Com\u{a}neci, Frank Plastria</dc:creator>
    </item>
    <item>
      <title>Improved Sample Complexity Bounds for Diffusion Model Training</title>
      <link>https://arxiv.org/abs/2311.13745</link>
      <description>arXiv:2311.13745v3 Announce Type: replace-cross 
Abstract: Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works~\cite{chen2022,chen2022improved,benton2023linear} have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the \emph{sample complexity} of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work~\cite{BMR20} showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an \emph{exponential improvement} in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13745v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Aditya Parulekar, Eric Price, Zhiyang Xun</dc:creator>
    </item>
    <item>
      <title>Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models</title>
      <link>https://arxiv.org/abs/2402.09236</link>
      <description>arXiv:2402.09236v2 Announce Type: replace-cross 
Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09236v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Sch\"olkopf, Pradeep Ravikumar</dc:creator>
    </item>
    <item>
      <title>Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability</title>
      <link>https://arxiv.org/abs/2410.05117</link>
      <description>arXiv:2410.05117v2 Announce Type: replace-cross 
Abstract: We develop a unifying framework for information-theoretic lower bound in statistical estimation and interactive decision making. Classical lower bound techniques -- such as Fano's method, Le Cam's method, and Assouad's lemma -- are central to the study of minimax risk in statistical estimation, yet are insufficient to provide tight lower bounds for \emph{interactive decision making} algorithms that collect data interactively (e.g., algorithms for bandits and reinforcement learning). Recent work of Foster et al. (2021, 2023) provides minimax lower bounds for interactive decision making using seemingly different analysis techniques from the classical methods. These results -- which are proven using a complexity measure known as the \emph{Decision-Estimation Coefficient} (DEC) -- capture difficulties unique to interactive learning, yet do not recover the tightest known lower bounds for passive estimation. We propose a unified view of these distinct methodologies through a new lower bound approach called \emph{interactive Fano method}. As an application, we introduce a novel complexity measure, the \emph{Fractional Covering Number}, which facilitates the new lower bounds for interactive decision making that extend the DEC methodology by incorporating the complexity of estimation. Using the fractional covering number, we (i) provide a unified characterization of learnability for \emph{any} stochastic bandit problem, (ii) close the remaining gap between the upper and lower bounds in Foster et al. (2021, 2023) (up to polynomial factors) for any interactive decision making problem in which the underlying model class is convex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05117v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Dylan J. Foster, Yanjun Han, Jian Qian, Alexander Rakhlin, Yunbei Xu</dc:creator>
    </item>
    <item>
      <title>Sparse Hanson-Wright Inequalities with Applications</title>
      <link>https://arxiv.org/abs/2410.15652</link>
      <description>arXiv:2410.15652v2 Announce Type: replace-cross 
Abstract: We derive Hanson-Wright inequalities for the quadratic form of a random vector with sparse independent components. Specifically, we consider cases where the components of the random vector are sparse $\alpha$-subexponential random variables with $\alpha&gt;0$. Our proof relies on a novel combinatorial approach to estimate the moments of the random quadratic form. In addition, we obtain a new Bernstein-type inequality for the sum of independent sparse $\alpha$-subexponential random variables. We present two applications with the sparse Hanson-Wright inequality: (1) Local law and complete eigenvector delocalization for sparse $\alpha$-subexponential Hermitian random matrices; (2) Concentration of the Euclidean norm for the linear transformation of a sparse $\alpha$-subexponential random vector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15652v2</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyun He, Ke Wang, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Sharp Matrix Empirical Bernstein Inequalities</title>
      <link>https://arxiv.org/abs/2411.09516</link>
      <description>arXiv:2411.09516v2 Announce Type: replace-cross 
Abstract: We present two sharp empirical Bernstein inequalities for symmetric random matrices with bounded eigenvalues. By sharp, we mean that both inequalities adapt to the unknown variance in a tight manner: the deviation captured by the first-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernstein inequality exactly, including constants, the latter requiring knowledge of the variance. Our first inequality holds for the sample mean of independent matrices, and our second inequality holds for a mean estimator under martingale dependence at stopping times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09516v2</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Dense ReLU Neural Networks for Temporal-spatial Model</title>
      <link>https://arxiv.org/abs/2411.09961</link>
      <description>arXiv:2411.09961v4 Announce Type: replace-cross 
Abstract: In this paper, we focus on fully connected deep neural networks utilizing the Rectified Linear Unit (ReLU) activation function for nonparametric estimation. We derive non-asymptotic bounds that lead to convergence rates, addressing both temporal and spatial dependence in the observed measurements. By accounting for dependencies across time and space, our models better reflect the complexities of real-world data, enhancing both predictive performance and theoretical robustness. We also tackle the curse of dimensionality by modeling the data on a manifold, exploring the intrinsic dimensionality of high-dimensional data. We broaden existing theoretical findings of temporal-spatial analysis by applying them to neural networks in more general contexts and demonstrate that our proof techniques are effective for models with short-range dependence. Our empirical simulations across various synthetic response functions underscore the superior performance of our method, outperforming established approaches in the existing literature. These findings provide valuable insights into the strong capabilities of dense neural networks for temporal-spatial modeling across a broad range of function classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09961v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhang, Carlos Misael Madrid Padilla, Xiaokai Luo, Daren Wang, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
  </channel>
</rss>
