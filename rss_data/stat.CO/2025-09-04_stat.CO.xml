<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 04:02:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Gaussian process surrogate with physical law-corrected prior for multi-coupled PDEs defined on irregular geometry</title>
      <link>https://arxiv.org/abs/2509.02617</link>
      <description>arXiv:2509.02617v1 Announce Type: cross 
Abstract: Parametric partial differential equations (PDEs) are fundamental mathematical tools for modeling complex physical systems, yet their numerical evaluation across parameter spaces remains computationally intensive when using conventional high-fidelity solvers. To address this challenge, we propose a novel physical law-corrected prior Gaussian process (LC-prior GP) surrogate modeling framework that effectively integrates data-driven learning with underlying physical constraints to flexibly handle multi-coupled variables defined on complex geometries. The proposed approach leverages proper orthogonal decomposition (POD) to parameterize high-dimensional PDE solutions via their dominant modes and associated coefficients, thereby enabling efficient Gaussian process (GP) surrogate modeling within a reduced-dimensional coefficient space. A key contribution lies in the incorporation of physical laws together with a limited number of parameter samples to correct the GP posterior mean, thus avoiding reliance on computationally expensive numerical solvers. Furthermore, interpolation functions are constructed to describe the mapping from the full parameter space to the physics-based correction term. This mapping is subsequently backpropagated to constrain the original GP surrogate, yielding a more physically consistent conditional prior. To handle irregular geometries, the radial basis function-finite difference (RBF-FD) method is incorporated during training set computation, with its inherent differentiation matrices providing both computational efficiency and numerical accuracy for physical constraint optimization. The effectiveness of the proposed method is demonstrated through numerical experiments involving a reaction-diffusion model, miscible flooding models, and Navier-Stokes equations with multi-physics coupling defined on irregular domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02617v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pucheng Tang, Hongqiao Wang, Wenzhou Lin, Qian Chen, Heng Yong</dc:creator>
    </item>
    <item>
      <title>Robust Survival Estimation under Interval Censoring: Expectation-Maximization and Bayesian Accelerated Failure Time Assessment via Simulation and Application</title>
      <link>https://arxiv.org/abs/2509.02634</link>
      <description>arXiv:2509.02634v1 Announce Type: cross 
Abstract: Interval censoring occurs when event times are only known to fall between scheduled assessments, a common design in clinical trials, epidemiology, and reliability studies. Standard right-censoring methods, such as Kaplan-Meier and Cox regression, are not directly applicable and can produce biased results. This study compares three complementary approaches for interval-censored survival data. First, the Turnbull nonparametric maximum likelihood estimator (NPMLE) via the EM algorithm recovers the survival distribution without strong assumptions. Second, Weibull and log-normal accelerated failure time (AFT) models with interval likelihoods provide smooth, covariate-adjusted survival curves and interpretable time-ratio effects. Third, Bayesian AFT models extend these tools by quantifying posterior uncertainty, incorporating prior information, and enabling interval-aware model comparisons via PSIS-LOO cross-validation. Simulations across generating distributions, censoring intensities, sample sizes, and covariate structures evaluated the integrated squared error (ISE) for curve recovery, integrated Brier score (IBS) for prediction, and coverage for uncertainty calibration. Results show that the EM achieves the lowest ISE for distribution recovery, AFT models improve predictive performance when families are correctly specified, and Bayesian AFT offers calibrated uncertainty and principled model selection. An application to the ovarian cancer dataset, restructured into interval-censored form, demonstrates the workflow in practice: the EM algorithm reveals the baseline shape, parametric AFT provides covariate-adjusted predictions, and Bayesian AFT validates model adequacy through posterior predictive checks. Together, these methods form a tiered strategy: EM for shape discovery, AFT for covariate-driven prediction, and Bayesian AFT for complete uncertainty quantification and model comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02634v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>J. T. Korley</dc:creator>
    </item>
    <item>
      <title>Inference on covariance structure in high-dimensional multi-view data</title>
      <link>https://arxiv.org/abs/2509.02772</link>
      <description>arXiv:2509.02772v1 Announce Type: cross 
Abstract: This article focuses on covariance estimation for multi-view data. Popular approaches rely on factor-analytic decompositions that have shared and view-specific latent factors. Posterior computation is conducted via expensive and brittle Markov chain Monte Carlo (MCMC) sampling or variational approximations that underestimate uncertainty and lack theoretical guarantees. Our proposed methodology employs spectral decompositions to estimate and align latent factors that are active in at least one view. Conditionally on these factors, we choose jointly conjugate prior distributions for factor loadings and residual variances. The resulting posterior is a simple product of normal-inverse gamma distributions for each variable, bypassing MCMC and facilitating posterior computation. We prove favorable increasing-dimension asymptotic properties, including posterior contraction and central limit theorems for point estimators. We show excellent performance in simulations, including accurate uncertainty quantification, and apply the methodology to integrate four high-dimensional views from a multi-omics dataset of cancer cell samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02772v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mauri, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction</title>
      <link>https://arxiv.org/abs/2509.02826</link>
      <description>arXiv:2509.02826v1 Announce Type: cross 
Abstract: Obesity is a critical global health issue driven by dietary, physiological, and environmental factors, and is strongly associated with chronic diseases such as diabetes, cardiovascular disorders, and cancer. Machine learning has emerged as a promising approach for early obesity risk prediction, yet a comparative evaluation of ensemble techniques -- particularly hybrid majority voting and ensemble stacking -- remains limited. This study aims to compare hybrid majority voting and ensemble stacking methods for obesity risk prediction, identifying which approach delivers higher accuracy and efficiency. The analysis seeks to highlight the complementary strengths of these ensemble techniques in guiding better predictive model selection for healthcare applications. Two datasets were utilized to evaluate three ensemble models: Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer Perceptron as meta-classifier). A pool of nine Machine Learning (ML) algorithms, evaluated across a total of 50 hyperparameter configurations, was analyzed to identify the top three models to serve as base learners for the ensemble methods. Preprocessing steps involved dataset balancing, and outlier detection, and model performance was evaluated using Accuracy and F1-Score. On Dataset-1, weighted hard voting and stacking achieved nearly identical performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard voting. On Dataset-2, stacking demonstrated superior results (Accuracy: 0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707, F1: 0.981675) and weighted hard voting, which showed the lowest performance. The findings confirm that ensemble stacking provides stronger predictive capability, particularly for complex data distributions, while hybrid majority voting remains a robust alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02826v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Towhidul Islam, Md Sumon Ali</dc:creator>
    </item>
    <item>
      <title>Learning from geometry-aware near-misses to real-time COR: A spatiotemporal grouped random GEV framework</title>
      <link>https://arxiv.org/abs/2509.02871</link>
      <description>arXiv:2509.02871v1 Announce Type: cross 
Abstract: Real-time prediction of corridor-level crash occurrence risk (COR) remains challenging, as existing near-miss based extreme value models oversimplify collision geometry, exclude vehicle-infrastructure (V-I) interactions, and inadequately capture spatial heterogeneity in vehicle dynamics. This study introduces a geometry-aware two-dimensional time-to-collision (2D-TTC) indicator within a Hierarchical Bayesian spatiotemporal grouped random parameter (HBSGRP) framework using a non-stationary univariate generalized extreme value (UGEV) model to estimate short-term COR in urban corridors. High-resolution trajectories from the Argoverse-2 dataset, covering 28 locations along Miami's Biscayne Boulevard, were analyzed to extract extreme V-V and V-I near misses. The model incorporates dynamic variables and roadway features as covariates, with partial pooling across locations to address unobserved heterogeneity. Results show that the HBSGRP-UGEV framework outperforms fixed-parameter alternatives, reducing DIC by up to 7.5% for V-V and 3.1% for V-I near-misses. Predictive validation using ROC-AUC confirms strong performance: 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections. Model interpretation reveals that relative speed and distance dominate V-V risks at intersections and segments, with deceleration critical in segments, while V-I risks are driven by speed, boundary proximity, and steering/heading adjustments. These findings highlight the value of a statistically rigorous, geometry-sensitive, and spatially adaptive modeling approach for proactive corridor-level safety management, supporting real-time interventions and long-term design strategies aligned with Vision Zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02871v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Group-averaged Markov chains: mixing improvement</title>
      <link>https://arxiv.org/abs/2509.02996</link>
      <description>arXiv:2509.02996v1 Announce Type: cross 
Abstract: For Markov kernels $P$ on a general state space $\mathcal{X}$, we introduce a new class of averaged Markov kernels $P_{da}(G,\nu)$ of $P$ induced by a group $G$ that acts on $\mathcal{X}$ and a probability measure $\nu$ on $G \times G$. Notable special cases are the group-orbit average $\overline{P}$, left-average $P_{la}$, right-average $P_{ra}$ and the independent-double-average $(P_{la})_{ra}$. For $\pi$-stationary $P$ in which $\pi$ is invariant with respect to $G$, we show that in general $P_{da}$ enjoys favorable convergence properties than $P$ based on metrics such as spectral gap or asymptotic variance, and within the family of $P_{da}$ the most preferable kernel is in general $(P_{la})_{ra}$. We demonstrate that $P_{la}, P_{ra}, (P_{la})_{ra}$ are comparable in terms of mixing times, which supports the use of $P_{la}, P_{ra}$ in practice as computationally cheaper alternatives over $(P_{la})_{ra}$. These averaged kernels also admit natural geometric interpretations: they emerge as unique projections of $P$ onto specific $G$-invariant structures under the Kullback-Leibler divergence or the Hilbert-Schmidt norm and satisfy Pythagorean identities. On the other hand, in the general case if $\pi$ is not invariant with respect to $G$, we propose and study a technique that we call state-dependent averaging of Markov kernels which generalizes the earlier results to this setting. As examples and applications, this averaging perspective not only allows us to recast state-of-the-art Markov chain samplers such as Hamiltonian Monte Carlo or piecewise-deterministic Markov processes as specific cases of $P_{da}$, but also enables improvements to existing samplers such as Metropolis-Hastings, achieving rapid mixing in some toy models or when $\pi$ is the discrete uniform distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02996v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.GR</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Youjia Wang</dc:creator>
    </item>
    <item>
      <title>Ensemble Control Variates</title>
      <link>https://arxiv.org/abs/2509.01091</link>
      <description>arXiv:2509.01091v2 Announce Type: replace 
Abstract: Control variates have become an increasingly popular variance-reduction technique in Bayesian inference. Many broadly applicable control variates are based on the Langevin-Stein operator, which leverages gradient information from any gradient-based sampler to produce variance-reduced estimators of expectations. These control variates typically require optimising over a function $u(\theta)$ within a user-defined functional class $G$, such as the space of $Q$th-order polynomials or a reproducing kernel Hilbert space. We propose using averaging-based ensemble learning to construct Stein-based control variates. While the proposed framework is broadly applicable, we focus on ensembles constructed from zero-variance control variates (ZVCV), a popular parametric approach based on solving a linear approximation problem that can easily be over-parameterised in medium-to-high dimensional settings. A common remedy is to use regularised ZVCV via penalised regression, but these methods can be prohibitively slow. We introduce ensemble ZVCV methods based on ensembles of OLS estimators and evaluate the proposed methods against established methods in the literature in a simulation study. Our results show that ensemble ZVCV methods are competitive with regularised ZVCV methods in terms of statistical efficiency, but are substantially faster. This work opens a new direction for constructing broadly applicable control variate techniques via ensemble learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01091v2</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long M. Nguyen, Christopher Drovandi, Leah F. South</dc:creator>
    </item>
    <item>
      <title>Adaptive greedy forward variable selection for linear regression models with incomplete data using multiple imputation</title>
      <link>https://arxiv.org/abs/2210.10967</link>
      <description>arXiv:2210.10967v3 Announce Type: replace-cross 
Abstract: Variable selection is crucial for sparse modeling in this age of big data. Missing values are common in data, and make variable selection more complicated. The approach of multiple imputation (MI) results in multiply imputed datasets for missing values, and has been widely applied in various variable selection procedures. However, directly performing variable selection on the whole MI data or bootstrapped MI data may not be worthy in terms of computation cost. To fast identify the active variables in the linear regression model, we propose the adaptive grafting procedure with three pooling rules on MI data. The proposed methods proceed iteratively, which starts from finding the active variables based on the complete case subset and then expand the working data matrix with both the number of active variables and available observations. A comprehensive simulation study shows the selection accuracy in different aspects and computational efficiency of the proposed methods. Two real-life examples illustrate the strength of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10967v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yong-Shiuan Lee</dc:creator>
    </item>
    <item>
      <title>Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers</title>
      <link>https://arxiv.org/abs/2405.06464</link>
      <description>arXiv:2405.06464v4 Announce Type: replace-cross 
Abstract: Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax).
  Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBT's time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart.
  We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06464v4</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andra\v{z} Jelin\v{c}i\v{c}, James Foster, Patrick Kidger</dc:creator>
    </item>
    <item>
      <title>An integrated method for clustering and association network inference</title>
      <link>https://arxiv.org/abs/2503.22467</link>
      <description>arXiv:2503.22467v2 Announce Type: replace-cross 
Abstract: High dimensional Gaussian graphical models provide a rigorous framework to describe a network of statistical dependencies between entities, such as genes in genomic regulation studies or species in ecology. Penalized methods, including the standard Graphical-Lasso, are well-known approaches to infer the parameters of these models. As the number of variables in the model (of entities in the network) grow, the network inference and interpretation become more complex. The Normal-Block model is introduced, a new model that clusters variables and consider a network at the cluster level. Normal-Block both adds structure to the network and reduces its size. The approach builds on Graphical-Lasso to add a penalty on the network's edges and limit the detection of spurious dependencies. A zero-inflated version of the model is also proposed to account for real-world data properties. For the inference procedure, two approaches are introduced, a straightforward method based on state-of-the-art approaches and an original, more rigorous method that simultaneously infers the clustering of variables and the association network between clusters, using a penalized variational Expectation-Maximization approach. An implementation of the model in R, in a package called \textbf{normalblockr}, is available on github\footnote{https://github.com/jeannetous/normalblockr}. The results of the models in terms of clustering and network inference are presented, using both simulated data and various types of real-world data (proteomics and words occurrences on webpages).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22467v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanne Tous, Julien Chiquet</dc:creator>
    </item>
  </channel>
</rss>
