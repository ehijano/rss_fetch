<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 01:49:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Zipf's law in the distribution of Brazilian firm size</title>
      <link>https://arxiv.org/abs/2409.09470</link>
      <description>arXiv:2409.09470v2 Announce Type: new 
Abstract: Zipf's law states that the probability of a variable being larger than $s$ is roughly inversely proportional to $s$. In this paper, we evaluate Zipf's law for the distribution of firm size by the number of employees in Brazil. We use publicly available binned annual data from the Central Register of Enterprises (CEMPRE), which is held by the Brazilian Institute of Geography and Statistics (IBGE) and covers all formal organizations. Remarkably, we find that Zipf's law provides a very good, although not perfect, approximation to data for each year between 1996 and 2020 at the economy-wide level and also for agriculture, industry, and services alone. However, a lognormal distribution also performs well and even outperforms Zipf's law in certain cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09470v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiago Trafane Oliveira Santos (Central Bank of Brazil, Bras\'ilia, Brazil. Department of %Economics, University of Brasilia, Brazil), Daniel Oliveira Cajueiro (Department of Economics, University of Brasilia, Brazil. National Institute of Science and Technology for Complex Systems)</dc:creator>
    </item>
    <item>
      <title>Forensically useful mid-term and short-term temperature reconstruction for quasi-indoor death scenes</title>
      <link>https://arxiv.org/abs/2409.09516</link>
      <description>arXiv:2409.09516v1 Announce Type: new 
Abstract: Accurate reconstruction of ambient temperature at death scenes is crucial for estimating the postmortem interval (PMI) in forensic science. Typically, this is done by correcting weather station temperatures using measurements from the scene, often through linear regression. While recent attempts to use alternative algorithms like GAM have improved accuracy, they usually require additional variables such as humidity, making them impractical. This study presents two methods for accurate temperature reconstruction using only temperature data. The first, a concurrent regression model, is known in mathematics and is applied here for mid-term reconstructions (several days of measurements). The second, a new method based on Fourier expansion, is designed for short-term reconstructions (only a few hours of measurements). Both models were tested in quasi-indoor conditions, using data from six different environments. The concurrent regression model provided nearly perfect reconstructions for periods longer than six days, while the short-term model achieved similar accuracy after just 4-5 hours of measurements. These findings demonstrate that reliable temperature corrections for PMI estimation can be made with significantly reduced measurement periods, enhancing the practicality of the method in forensic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09516v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>J\k{e}drzej Wydra, {\L}ukasz Smaga, Szymon Matuszewski</dc:creator>
    </item>
    <item>
      <title>A Convolutional Neural Network-based Ensemble Post-processing with Data Augmentation for Tropical Cyclone Precipitation Forecasts</title>
      <link>https://arxiv.org/abs/2409.09607</link>
      <description>arXiv:2409.09607v1 Announce Type: new 
Abstract: Heavy precipitation from tropical cyclones (TCs) may result in disasters, such as floods and landslides, leading to substantial economic damage and loss of life. Prediction of TC precipitation based on ensemble post-processing procedures using machine learning (ML) approaches has received considerable attention for its flexibility in modeling and its computational power in managing complex models. However, when applying ML techniques to TC precipitation for a specific area, the available observation data are typically insufficient for comprehensive training, validation, and testing of the ML model, primarily due to the rapid movement of TCs. We propose to use the convolutional neural network (CNN) as a deep ML model to leverage the spatial information of precipitation. The proposed model has three distinct features that differentiate it from traditional CNNs applied in meteorology. First, it utilizes data augmentation to alleviate challenges posed by the small sample size. Second, it contains geographical and dynamic variables to account for area-specific features and the relative distance between the study area and the moving TC. Third, it applies unequal weights to accommodate the temporal structure in the training data when calculating the objective function. The proposed CNN-all model is then illustrated with the TC Soudelor's impact on Taiwan. Soudelor was the strongest TC of the 2015 Pacific typhoon season. The results show that the inclusion of augmented data and dynamic variables improves the prediction of heavy precipitation. The proposed CNN-all outperforms traditional CNN models, based on the continuous probability skill score (CRPSS), probability plots, and reliability diagram. The proposed model has the potential to be utilized in a wide range of meteorological studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09607v1</guid>
      <category>stat.AP</category>
      <category>physics.geo-ph</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sing-Wen Chen (Institute of Health Data Analytics and Statistics, College of Public Health, National Taiwan University, Taiwan), Joyce Juang (Central Weather Administration, Taiwan), Charlotte Wang (Institute of Health Data Analytics and Statistics, College of Public Health, National Taiwan University, Taiwan, Master Program of Public Health, College of Public Health, National Taiwan University, Taiwan), Hui-Ling Chang (Central Weather Administration, Taiwan), Jing-Shan Hong (Central Weather Administration, Taiwan), Chuhsing Kate Hsiao (Institute of Health Data Analytics and Statistics, College of Public Health, National Taiwan University, Taiwan, Master Program of Public Health, College of Public Health, National Taiwan University, Taiwan)</dc:creator>
    </item>
    <item>
      <title>Spatial occupancy models for data collected on stream networks</title>
      <link>https://arxiv.org/abs/2409.10017</link>
      <description>arXiv:2409.10017v1 Announce Type: new 
Abstract: To effectively monitor biodiversity in streams and rivers, we need to quantify species distribution accurately. Occupancy models are useful for distinguishing between the non-detection of a species and its actual absence. While these models can account for spatial autocorrelation, they are not suited for streams and rivers due to their unique network spatial structure. Here, I propose spatial occupancy models specifically designed for data collected on stream and river networks. I present the statistical developments and illustrate their application using data on a semi-aquatic mammal. Overall, spatial stream network occupancy models offer a robust method for assessing biodiversity in freshwater ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10017v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Gimenez</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causality in Brain Networks: With Application to Motor Imagery vs Execution</title>
      <link>https://arxiv.org/abs/2409.10374</link>
      <description>arXiv:2409.10374v1 Announce Type: new 
Abstract: One fundamental challenge of data-driven analysis in neuroscience is modeling causal interactions and exploring the connectivity of nodes in a brain network. Various statistical methods, relying on various perspectives and employing different data modalities, are being developed to examine and comprehend the underlying causal structures inherent to brain dynamics. This study introduces a novel statistical approach, TAR4C, to dissect causal interactions in multichannel EEG recordings. TAR4C uses the threshold autoregressive model to describe the causal interaction between nodes or clusters of nodes in a brain network. The perspective involves testing whether one node, which may represent a brain region, can control the dynamics of the other. The node that has such an impact on the other is called a threshold variable and can be classified as a causative because its functionality is the leading source operating as an instantaneous switching mechanism that regulates the time-varying autoregressive structure of the other. This statistical concept is commonly referred to as threshold non-linearity. Once threshold non-linearity has been verified between a pair of nodes, the subsequent essential facet of TAR modeling is to assess the predictive ability of the causal node for the current activity on the other and represent causal interactions in autoregressive terms. This predictive ability is what underlies Granger causality. The TAR4C approach can discover non-linear and time-dependent causal interactions without negating the G-causality perspective. The efficacy of the proposed approach is exemplified by analyzing the EEG signals recorded during the motor movement/imagery experiment. The similarities and differences between the causal interactions manifesting during the execution and the imagery of a given motor movement are demonstrated by analyzing EEG recordings from multiple subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10374v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Leadership and Engagement Dynamics in Legislative Twitter Networks: Statistical Analysis and Modeling</title>
      <link>https://arxiv.org/abs/2409.10475</link>
      <description>arXiv:2409.10475v1 Announce Type: new 
Abstract: In this manuscript, we analyze the interaction network on Twitter among members of the 117th U.S. Congress to assess the visibility of political leaders and explore how systemic properties and node attributes influence the formation of legislative connections. We employ descriptive social network statistical methods, the exponential random graph model (ERGM), and the stochastic block model (SBM) to evaluate the relative impact of network systemic properties, as well as institutional and personal traits, on the generation of online relationships among legislators. Our findings reveal that legislative networks on social media platforms like Twitter tend to reinforce the leadership of dominant political actors rather than diminishing their influence. However, we identify that these leadership roles can manifest in various forms. Additionally, we highlight that online connections within legislative networks are influenced by both the systemic properties of the network and institutional characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10475v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carolina Luque, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections</title>
      <link>https://arxiv.org/abs/2409.09045</link>
      <description>arXiv:2409.09045v1 Announce Type: cross 
Abstract: Large language models (LLMs) are perceived by some as having the potential to revolutionize social science research, considering their training data includes information on human attitudes and behavior. If these attitudes are reflected in LLM output, LLM-generated "synthetic samples" could be used as a viable and efficient alternative to surveys of real humans. However, LLM-synthetic samples might exhibit coverage bias due to training data and fine-tuning processes being unrepresentative of diverse linguistic, social, political, and digital contexts. In this study, we examine to what extent LLM-based predictions of public opinion exhibit context-dependent biases by predicting voting behavior in the 2024 European Parliament elections using a state-of-the-art LLM. We prompt GPT-4-Turbo with anonymized individual-level background information, varying prompt content and language, ask the LLM to predict each person's voting behavior, and compare the weighted aggregates to the real election results. Our findings emphasize the limited applicability of LLM-synthetic samples to public opinion prediction. We show that (1) the LLM-based prediction of future voting behavior largely fails, (2) prediction accuracy is unequally distributed across national and linguistic contexts, and (3) improving LLM predictions requires detailed attitudinal information about individuals for prompting. In investigating the contextual differences of LLM-based predictions of public opinion, our research contributes to the understanding and mitigation of biases and inequalities in the development of LLMs and their applications in computational social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09045v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leah von der Heyde, Anna-Carolina Haensch, Alexander Wenz</dc:creator>
    </item>
    <item>
      <title>Identifying Factors to Help Improve Existing Decomposition-Based PMI Estimation Methods</title>
      <link>https://arxiv.org/abs/2409.09056</link>
      <description>arXiv:2409.09056v1 Announce Type: cross 
Abstract: Accurately assessing the postmortem interval (PMI) is an important task in forensic science. Some of the existing techniques use regression models that use a decomposition score to predict the PMI or accumulated degree days (ADD), however, the provided formulas are based on very small samples and the accuracy is low. With the advent of Big Data, much larger samples can be used to improve PMI estimation methods. We, therefore, aim to investigate ways to improve PMI prediction accuracy by (a) using a much larger sample size, (b) employing more advanced linear models, and (c) enhancing models with factors known to affect the human decay process. Specifically, this study involved the curation of a sample of 249 human subjects from a large-scale decomposition dataset, followed by evaluating pre-existing PMI/ADD formulas and fitting increasingly sophisticated models to estimate the PMI/ADD. Results showed that including the total decomposition score (TDS), demographic factors (age, biological sex, and BMI), and weather-related factors (season of discovery, temperature history, and humidity history) increased the accuracy of the PMI/ADD models. Furthermore, the best performing PMI estimation model using the TDS, demographic, and weather-related features as predictors resulted in an adjusted R-squared of 0.34 and an RMSE of 0.95. It had a 7% lower RMSE than a model using only the TDS to predict the PMI and a 48% lower RMSE than the pre-existing PMI formula. The best ADD estimation model, also using the TDS, demographic, and weather-related features as predictors, resulted in an adjusted R-squared of 0.52 and an RMSE of 0.89. It had an 11% lower RMSE than the model using only the TDS to predict the ADD and a 52% lower RMSE than the pre-existing ADD formula. This work demonstrates the need (and way) to incorporate demographic and environmental factors into PMI/ADD estimation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09056v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna-Maria Nau, Phillip Ditto, Dawnie Wolfe Steadman, Audris Mockus</dc:creator>
    </item>
    <item>
      <title>Exploring Dimensionality Reduction of SDSS Spectral Abundances</title>
      <link>https://arxiv.org/abs/2409.09227</link>
      <description>arXiv:2409.09227v1 Announce Type: cross 
Abstract: High-resolution stellar spectra offer valuable insights into atmospheric parameters and chemical compositions. However, their inherent complexity and high-dimensionality present challenges in fully utilizing the information they contain. In this study, we utilize data from the Apache Point Observatory Galactic Evolution Experiment (APOGEE) within the Sloan Digital Sky Survey IV (SDSS-IV) to explore latent representations of chemical abundances by applying five dimensionality reduction techniques: PCA, t-SNE, UMAP, Autoencoder, and VAE. Through this exploration, we evaluate the preservation of information and compare reconstructed outputs with the original 19 chemical abundance data. Our findings reveal a performance ranking of PCA &lt; UMAP &lt; t-SNE &lt; VAE &lt; Autoencoder, through comparing their explained variance under optimized MSE. The performance of non-linear (Autoencoder and VAE) algorithms has approximately 10\% improvement compared to linear (PCA) algorithm. This difference can be referred to as the "non-linearity gap." Future work should focus on incorporating measurement errors into extension VAEs, thereby enhancing the reliability and interpretability of chemical abundance exploration in astronomical spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09227v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qianyu Fan, Joshua S. Speagle</dc:creator>
    </item>
    <item>
      <title>Simulation of Public Cash Transfer Programs on US Entrepreneurs' Financing Constraint</title>
      <link>https://arxiv.org/abs/2409.09955</link>
      <description>arXiv:2409.09955v1 Announce Type: cross 
Abstract: In this paper, I conduct a policy exercise about how much the introduction of a cash transfer program as large as a Norwegian-sized lottery sector to the United States would affect startups. The key results are that public cash transfer programs (like lottery) do not increase much the number of new startups, but increase the size of startups, and only modestly increase aggregate productivity and output. The most important factor for entrepreneurs to start new businesses is their ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09955v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liukun Wu</dc:creator>
    </item>
    <item>
      <title>TCDformer-based Momentum Transfer Model for Long-term Sports Prediction</title>
      <link>https://arxiv.org/abs/2409.10176</link>
      <description>arXiv:2409.10176v1 Announce Type: cross 
Abstract: Accurate sports prediction is a crucial skill for professional coaches, which can assist in developing effective training strategies and scientific competition tactics. Traditional methods often use complex mathematical statistical techniques to boost predictability, but this often is limited by dataset scale and has difficulty handling long-term predictions with variable distributions, notably underperforming when predicting point-set-game multi-level matches. To deal with this challenge, this paper proposes TM2, a TCDformer-based Momentum Transfer Model for long-term sports prediction, which encompasses a momentum encoding module and a prediction module based on momentum transfer. TM2 initially encodes momentum in large-scale unstructured time series using the local linear scaling approximation (LLSA) module. Then it decomposes the reconstructed time series with momentum transfer into trend and seasonal components. The final prediction results are derived from the additive combination of a multilayer perceptron (MLP) for predicting trend components and wavelet attention mechanisms for seasonal components. Comprehensive experimental results show that on the 2023 Wimbledon men's tournament datasets, TM2 significantly surpasses existing sports prediction models in terms of performance, reducing MSE by 61.64% and MAE by 63.64%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10176v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Liu, Jiacheng Gu, Xiyuan Huang, Junjie Shi, Tongtong Feng, Ning He</dc:creator>
    </item>
    <item>
      <title>A point process approach for the classification of noisy calcium imaging data</title>
      <link>https://arxiv.org/abs/2409.10409</link>
      <description>arXiv:2409.10409v1 Announce Type: cross 
Abstract: We study noisy calcium imaging data, with a focus on the classification of spike traces. As raw traces obscure the true temporal structure of neuron's activity, we performed a tuned filtering of the calcium concentration using two methods: a biophysical model and a kernel mapping. The former characterizes spike trains related to a particular triggering event, while the latter filters out the signal and refines the selection of the underlying neuronal response. Transitioning from traditional time series analysis to point process theory, the study explores spike-time distance metrics and point pattern prototypes to describe repeated observations. We assume that the analyzed neuron's firing events, i.e. spike occurrences, are temporal point process events. In particular, the study aims to categorize 47 point patterns by depth, assuming the similarity of spike occurrences within specific depth categories. The results highlight the pivotal roles of depth and stimuli in discerning diverse temporal structures of neuron firing events, confirming the point process approach based on prototype analysis is largely useful in the classification of spike traces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10409v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Burzacchi, Nicoletta D'Angelo, David Payares-Garcia, Jorge Mateu</dc:creator>
    </item>
    <item>
      <title>Multidimensional Deconvolution with Profiling</title>
      <link>https://arxiv.org/abs/2409.10421</link>
      <description>arXiv:2409.10421v1 Announce Type: cross 
Abstract: In many experimental contexts, it is necessary to statistically remove the impact of instrumental effects in order to physically interpret measurements. This task has been extensively studied in particle physics, where the deconvolution task is called unfolding. A number of recent methods have shown how to perform high-dimensional, unbinned unfolding using machine learning. However, one of the assumptions in all of these methods is that the detector response is accurately modeled in the Monte Carlo simulation. In practice, the detector response depends on a number of nuisance parameters that can be constrained with data. We propose a new algorithm called Profile OmniFold (POF), which works in a similar iterative manner as the OmniFold (OF) algorithm while being able to simultaneously profile the nuisance parameters. We illustrate the method with a Gaussian example as a proof of concept highlighting its promising capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10421v1</guid>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Federated Epidemic Surveillance</title>
      <link>https://arxiv.org/abs/2307.02616</link>
      <description>arXiv:2307.02616v2 Announce Type: replace 
Abstract: Epidemic surveillance is a challenging task, especially when crucial data is fragmented across institutions and data custodians are unable or unwilling to share it. This study aims to explore the feasibility of a simple federated surveillance approach. The idea is to conduct hypothesis tests for a rise in counts behind each custodian's firewall and then combine p-values from these tests using techniques from meta-analysis. We propose a hypothesis testing framework to identify surges in epidemic-related data streams and conduct experiments on real and semi-synthetic data to assess the power of different p-value combination methods to detect surges without needing to combine the underlying counts. Our findings show that relatively simple combination methods achieve a high degree of fidelity and suggest that infectious disease outbreaks can be detected without needing to share even aggregate data across institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02616v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Lyu, Roni Rosenfeld, Bryan Wilder</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of Street Vendors in New York City</title>
      <link>https://arxiv.org/abs/2406.00527</link>
      <description>arXiv:2406.00527v4 Announce Type: replace 
Abstract: We estimate the number of street vendors in New York City. First, we summarize the process by which vendors receive licenses and permits to operate legally in New York City. Second, we describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Third, we review ratio estimation and provide a theoretical justification based on the theory of point processes. Fourth, we use ratio estimation to calculate the total number of vendors, finding approximately 23,000 street vendors operate in New York City (20,500 mobile food vendors and 2,400 general merchandise vendors) with one third located in just six ZIP Codes (11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan). Finally, we evaluate the accuracy of the ratio estimator when the distribution of vendors is explained by a Poisson or Yule process, and we discuss several policy implications. In particular, our estimates suggest the American Community Survey misses the majority of New York City street vendors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00527v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>A Bayesian framework to evaluate evidence in cases of alleged cheating with secret codes in sports</title>
      <link>https://arxiv.org/abs/2409.08172</link>
      <description>arXiv:2409.08172v2 Announce Type: replace 
Abstract: We present a Bayesian framework to analyze a case of alleged cheating in the mind sport contract bridge. We explain why a Bayesian approach is called for, and not a frequentistic one. We argue that such a Bayesian framework can and should also be used in other sports for cases of alleged cheating by means of illegal signalling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08172v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aafko Boonstra, Ronald Meester</dc:creator>
    </item>
    <item>
      <title>Mathematical modelling, selection and hierarchical inference to determine the minimal dose in IFN$\alpha$ therapy against Myeloproliferative Neoplasms</title>
      <link>https://arxiv.org/abs/2112.10688</link>
      <description>arXiv:2112.10688v2 Announce Type: replace-cross 
Abstract: Myeloproliferative Neoplasms (MPN) are blood cancers that appear after acquiring a driver mutation in a hematopoietic stem cell. These hematological malignancies result in the overproduction of mature blood cells and, if not treated, induce a risk of cardiovascular events and thrombosis. Pegylated IFN$\alpha$ is commonly used to treat MPN, but no clear guidelines exist concerning the dose prescribed to patients. We applied a model selection procedure and ran a hierarchical Bayesian inference method to decipher how dose variations impact the response to the therapy. We inferred that IFN$\alpha$ acts on mutated stem cells by inducing their differentiation into progenitor cells; the higher the dose, the higher the effect. We found that the treatment can induce long-term remission when a sufficient (patient-dependent) dose is reached. We determined this minimal dose for individuals in a cohort of patients and estimated the most suitable starting dose to give to a new patient to increase the chances of being cured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.10688v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/imammb/dqae006</arxiv:DOI>
      <arxiv:journal_reference>Mathematical Medicine and Biology: A Journal of the IMA, Volume 41, Issue 2, June 2024, Pages 110-134</arxiv:journal_reference>
      <dc:creator>Gurvan Hermange, William Vainchenker, Isabelle Plo, Paul-Henry Courn\`ede</dc:creator>
    </item>
    <item>
      <title>Assessing biomedical knowledge robustness in large language models by query-efficient sampling attacks</title>
      <link>https://arxiv.org/abs/2402.10527</link>
      <description>arXiv:2402.10527v2 Announce Type: replace-cross 
Abstract: The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. Understanding model vulnerabilities in high-stakes and knowledge-intensive tasks is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples (i.e. adversarial entities) in natural language processing tasks raises questions about their potential impact on the knowledge robustness of pre-trained and finetuned LLMs in high-stakes and specialized domains. We examined the use of type-consistent entity substitution as a template for collecting adversarial entities for billion-parameter LLMs with biomedical knowledge. To this end, we developed an embedding-space attack based on powerscaled distance-weighted sampling to assess the robustness of their biomedical knowledge with a low query budget and controllable coverage. Our method has favorable query efficiency and scaling over alternative approaches based on random sampling and blackbox gradient-guided search, which we demonstrated for adversarial distractor generation in biomedical question answering. Subsequent failure mode analysis uncovered two regimes of adversarial entities on the attack surface with distinct characteristics and we showed that entity substitution attacks can manipulate token-wise Shapley value explanations, which become deceptive in this setting. Our approach complements standard evaluations for high-capacity models and the results highlight the brittleness of domain knowledge in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10527v2</guid>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Patrick Xian, Alex J. Lee, Satvik Lolla, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</dc:creator>
    </item>
    <item>
      <title>Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.13821</link>
      <description>arXiv:2405.13821v3 Announce Type: replace-cross 
Abstract: In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be "normalized" to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13821v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony Sikorski, Daniel McKenzie, Douglas Nychka</dc:creator>
    </item>
    <item>
      <title>Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers</title>
      <link>https://arxiv.org/abs/2406.01380</link>
      <description>arXiv:2406.01380v2 Announce Type: replace-cross 
Abstract: Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01380v2</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiqi Liu, Wenhan Cao, Chang Liu, Tianyi Zhang, Shengbo Eben Li</dc:creator>
    </item>
    <item>
      <title>Computationally efficient and statistically accurate conditional independence testing with spaCRT</title>
      <link>https://arxiv.org/abs/2407.08911</link>
      <description>arXiv:2407.08911v2 Announce Type: replace-cross 
Abstract: We introduce the saddlepoint approximation-based conditional randomization test (spaCRT), a novel conditional independence test that effectively balances statistical accuracy and computational efficiency, inspired by applications to single-cell CRISPR screens. Resampling-based methods like the distilled conditional randomization test (dCRT) offer statistical precision but at a high computational cost. The spaCRT leverages a saddlepoint approximation to the resampling distribution of the dCRT test statistic, achieving very similar finite-sample statistical performance with significantly reduced computational demands. We prove that the spaCRT $p$-value approximates the dCRT $p$-value with vanishing relative error, and that these two tests are asymptotically equivalent. Through extensive simulations and real data analysis, we demonstrate that the spaCRT controls Type-I error and maintains high power, outperforming other asymptotic and resampling-based tests. Our method is particularly well-suited for large-scale single-cell CRISPR screen analyses, facilitating the efficient and accurate assessment of perturbation-gene associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08911v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Graphical Structural Learning of rs-fMRI data in Heavy Smokers</title>
      <link>https://arxiv.org/abs/2409.08395</link>
      <description>arXiv:2409.08395v2 Announce Type: replace-cross 
Abstract: Recent studies revealed structural and functional brain changes in heavy smokers. However, the specific changes in topological brain connections are not well understood. We used Gaussian Undirected Graphs with the graphical lasso algorithm on rs-fMRI data from smokers and non-smokers to identify significant changes in brain connections. Our results indicate high stability in the estimated graphs and identify several brain regions significantly affected by smoking, providing valuable insights for future clinical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08395v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiru Gong, Qimin Zhang, Huili Zheng, Zheyan Liu, Shaohan Chen</dc:creator>
    </item>
  </channel>
</rss>
