<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 02:39:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Estimation of the intercept parameter in integrated Galton-Watson processes</title>
      <link>https://arxiv.org/abs/2601.06317</link>
      <description>arXiv:2601.06317v1 Announce Type: new 
Abstract: We study estimation of the intercept parameter in an integrated Galton-Watson process, a basic building-block for many count-valued time series models. In this unit root setting, the ordinary least squares estimator is inconsistent, whereas an existing weighted least squares (WLS) estimator is consistent only in the case where the process is transient, a condition that depends on the unknown intercept parameter . We propose an alternative WLS estimator based on the new weight function of $1/t$, and show that it is consistent regardless of whether the process is transient or null recurrent, with a convergence rate of $\sqrt{\ln n}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06317v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Lu</dc:creator>
    </item>
    <item>
      <title>Reduction and classification of higher-order Markov chains for categorical data</title>
      <link>https://arxiv.org/abs/2601.06674</link>
      <description>arXiv:2601.06674v1 Announce Type: new 
Abstract: Categorical time series models are powerful tools for understanding natural phenomena. Most available models can be formulated as special cases of $m$-th order Markov chains, for $m\geq 1$. Despite their broad applicability, theoretical research has largely focused on first-order Markov chains, mainly because many properties of higher-order chains can be analyzed by reducing them to first-order chains on an enlarged alphabet. However, the resulting first-order representation is sparse and possesses a highly structured transition kernel, a feature that has not been fully exploited.
  In this work, we study finite-alphabet Markov chains with arbitrary memory length and introduce a new reduction framework for their structural classification. We define the skeleton of a transition kernel, an object that captures the intrinsic pattern of transition probability constraints in a higher-order Markov chain.
  We show that the class structure of a binary matrix associated with the skeleton completely determines the recurrent classes and their periods in the original chain. We also provide an explicit algorithm for efficiently extracting the skeleton, which in many cases yields substantial computational savings. Applications include simple criteria for irreducibility and essential irreducibility of higher-order Markov chains and a concrete illustration based on a 10th-order Markov chain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06674v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Gallesco, Caio Teodore Genovese Huss Oliveira, Daniel Yasumasa Takahashi</dc:creator>
    </item>
    <item>
      <title>Diffusion Models with Heavy-Tailed Targets: Score Estimation and Sampling Guarantees</title>
      <link>https://arxiv.org/abs/2601.06715</link>
      <description>arXiv:2601.06715v1 Announce Type: new 
Abstract: Score-based diffusion models have become a powerful framework for generative modeling, with score estimation as a central statistical bottleneck. Existing guarantees for score estimation largely focus on light-tailed targets or rely on restrictive assumptions such as compact support, which are often violated by heavy-tailed data in practice. In this work, we study conventional (Gaussian) score-based diffusion models when the target distribution is heavy-tailed and belongs to a Sobolev class with smoothness parameter $\beta&gt;0$. We consider both exponential and polynomial tail decay, indexed by a tail parameter $\gamma$. Using kernel density estimation, we derive sharp minimax rates for score estimation, revealing a qualitative dichotomy: under exponential tails, the rate matches the light-tailed case up to polylogarithmic factors, whereas under polynomial tails the rate depends explicitly on $\gamma$. We further provide sampling guarantees for the associated continuous reverse dynamics. In total variation, the generated distribution converges at the minimax optimal rate $n^{-\beta/(2\beta+d)}$ under exponential tails (up to logarithmic factors), and at a $\gamma$-dependent rate under polynomial tails. Whether the latter sampling rate is minimax optimal remains an open question. These results characterize the statistical limits of score estimation and the resulting sampling accuracy for heavy-tailed targets, extending diffusion theory beyond the light-tailed setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06715v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Yu, Lu Yu</dc:creator>
    </item>
    <item>
      <title>A Note on NBUE and NWBUE Classes of Life Distributions</title>
      <link>https://arxiv.org/abs/2601.06760</link>
      <description>arXiv:2601.06760v1 Announce Type: new 
Abstract: Non-monotonic ageing notions are looked upon as an extension of the corresponding monotonic ageing notions in this work. In particular, the New Better than Used in Expectation (NBUE) and the corresponding non-monotonic analogue New Worse then Better than Used in Expectation (NWBUE) classes of life distributions is considered. Some additional results for the NBUE class are obtained. While many properties of the NBUE class carry over in an analogous way to the NWBUE class, it is shown by means of counterexamples that the moment bounds do not. Some corrective results with respect to popular notions of the NWBUE class are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06760v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>M. Z. Anis</dc:creator>
    </item>
    <item>
      <title>Wasserstein Concentration of Empirical Measures for Dependent Data via the Method of Moments</title>
      <link>https://arxiv.org/abs/2601.07228</link>
      <description>arXiv:2601.07228v1 Announce Type: new 
Abstract: We establish a general concentration result for the 1-Wasserstein distance between the empirical measure of a sequence of random variables and its expectation. Unlike standard results that rely on independence (e.g., Sanov's theorem) or specific mixing conditions, our result requires only two conditions: (1) control over the variance of the empirical moments, and (2) a flexible tail condition we term $\Psi_{r_n}$-sub-Gaussianity. This approach allows for significant dependencies between variables, provided their algebraic moments behave predictably. The proof uses the method of moments combined with a polynomial approximation of Lipschitz functions via Jackson kernels, allowing us to translate moment concentration into topological concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07228v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arash A. Amini, Luciano Vinas</dc:creator>
    </item>
    <item>
      <title>Gold standard process Markovian poisoning: a semiparametric approach</title>
      <link>https://arxiv.org/abs/2601.07503</link>
      <description>arXiv:2601.07503v1 Announce Type: new 
Abstract: We consider in this paper a stochastic process that mixes in time, according to a nonobserved stationary Markov selection process, two separate sources of randomness: i) a stationary process which distribution is accessible (gold standard); ii) a pure i.i.d. sequence which distribution is unknown (poisoning process). In this framework we propose to estimate, with two different approaches, the transition of the hidden Markov selection process along with the distribution, not supposed to belong to any parametric family, of the unknown i.i.d. sequence, under minimal (identifiability, stationarity and dependence in time) conditions. We show that both estimators provide consistent estimations of the Euclidean transition parameter, and also prove that one of them, which is $\sqrt$ n-consistent, allows to establish a functional central limit theorem about the unknown poisoning sequence cumulative distribution function. The numerical performances of our estimators are illustrated through various challenging examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07503v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claire Lacour (LAMA), Pierre Vandekerkhove (LAMA)</dc:creator>
    </item>
    <item>
      <title>Comparing three learn-then-test paradigms in a multivariate normal means problem</title>
      <link>https://arxiv.org/abs/2601.07764</link>
      <description>arXiv:2601.07764v1 Announce Type: new 
Abstract: Many modern procedures use the data to learn a structure and then leverage it to test many hypotheses. If the entire data is used at both stages, analytical or computational corrections for selection bias are required to ensure validity (post-learning adjustment). Alternatively, one can learn and/or test on masked versions of the data to avoid selection bias, either via information splitting or null augmentation}. Choosing among these three learn-then-test paradigms, and how much masking to employ for the latter two, are critical decisions impacting power that currently lack theoretical guidance. In a multivariate normal means model, we derive asymptotic power formulas for prototypical methods from each paradigm -- variants of sample splitting, conformal-style null augmentation, and resampling-based post-learning adjustment -- quantifying the power losses incurred by masking at each stage. For these paradigm representatives, we find that post-learning adjustment is most powerful, followed by null augmentation, and then information splitting. Moreover, null augmentation can be nearly as powerful as post-learning adjustment, while avoiding its challenges: the power of the former approaches that of the latter if the number of nulls used for augmentation is a vanishing fraction of the number of hypotheses. We also prove for a tractable proxy that the optimal number of nulls scales as the square root of the number of hypotheses, challenging existing heuristics. Finally, we characterize optimal tuning for information splitting by identifying an optimal split fraction and tying it to the difficulty of the learning problem. These results establish a theoretical foundation for key decisions in the deployment of learn-then-test methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07764v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Junu Lee, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Inference-Time Alignment for Diffusion Models via Doob's Matching</title>
      <link>https://arxiv.org/abs/2601.06514</link>
      <description>arXiv:2601.06514v1 Announce Type: cross 
Abstract: Inference-time alignment for diffusion models aims to adapt a pre-trained diffusion model toward a target distribution without retraining the base score network, thereby preserving the generative capacity of the base model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce Doob's matching, a novel framework for guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-penalized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06514v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Chenguang Duan, Yuling Jiao, Yi Xu, Jerry Zhijian Yang</dc:creator>
    </item>
    <item>
      <title>Censored Graphical Horseshoe: Bayesian sparse precision matrix estimation with censored and missing data</title>
      <link>https://arxiv.org/abs/2601.06671</link>
      <description>arXiv:2601.06671v1 Announce Type: cross 
Abstract: Gaussian graphical models provide a powerful framework for studying conditional dependencies in multivariate data, with widespread applications spanning biomedical, environmental sciences, and other data-rich scientific domains. While the Graphical Horseshoe (GHS) method has emerged as a state-of-the-art Bayesian method for sparse precision matrix estimation, existing approaches assume fully observed data and thus fail in the presence of censoring or missingness, which are pervasive in real-world studies. In this paper, we develop the Censored Graphical Horseshoe (CGHS), a novel Bayesian framework that extends the GHS to censored and arbitrarily missing Gaussian data. By introducing a latent-variable representation, CGHS accommodates incomplete observations while retaining the adaptive global-local shrinkage properties of the Horseshoe prior. We derive efficient Gibbs samplers for posterior computation and establish new theoretical results on posterior behavior under censoring and missingness, filling a gap not addressed by frequentist Lasso-based methods. Through extensive simulations, we demonstrate that CGHS consistently improves estimation accuracy compared to penalized likelihood approaches. Our methods are implemented in the package GHScenmis available on Github: https://github.com/tienmt/ghscenmis .</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06671v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai, Sayantan Banerjee</dc:creator>
    </item>
    <item>
      <title>The Sample Complexity of Lossless Data Compression</title>
      <link>https://arxiv.org/abs/2601.06688</link>
      <description>arXiv:2601.06688v1 Announce Type: cross 
Abstract: A new framework is introduced for examining and evaluating the fundamental limits of lossless data compression, that emphasizes genuinely non-asymptotic results. The {\em sample complexity} of compressing a given source is defined as the smallest blocklength at which it is possible to compress that source at a specified rate and to within a specified excess-rate probability. This formulation parallels corresponding developments in statistics and computer science, and it facilitates the use of existing results on the sample complexity of various hypothesis testing problems. For arbitrary sources, the sample complexity of general variable-length compressors is shown to be tightly coupled with the sample complexity of prefix-free codes and fixed-length codes. For memoryless sources, it is shown that the sample complexity is characterized not by the source entropy, but by its R\'{e}nyi entropy of order~$1/2$. Nonasymptotic bounds on the sample complexity are obtained, with explicit constants. Generalizations to Markov sources are established, showing that the sample complexity is determined by the source's R\'{e}nyi entropy rate of order~$1/2$. Finally, bounds on the sample complexity of universal data compression are developed for arbitrary families of memoryless sources. There, the sample complexity is characterized by the minimum R\'{e}nyi divergence of order~$1/2$ between elements of the family and the uniform distribution. The connection of this problem with identity testing and with the associated separation rates is explored and discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06688v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Terence Viaud, Ioannis Kontoyiannis</dc:creator>
    </item>
    <item>
      <title>Extensions of the solidarity principle of the spectral gap for Gibbs samplers to their blocked and collapsed variants</title>
      <link>https://arxiv.org/abs/2601.06745</link>
      <description>arXiv:2601.06745v1 Announce Type: cross 
Abstract: Connections of a spectral nature are formed between Gibbs samplers and their blocked and collapsed variants. The solidarity principle of the spectral gap for full Gibbs samplers is generalized to different cycles and mixtures of Gibbs steps. This generalized solidarity principle is employed to establish that every cycle and mixture of Gibbs steps, which includes blocked Gibbs samplers and collapsed Gibbs samplers, inherits a spectral gap from a full Gibbs sampler. Exact relations between the spectra corresponding to blocked and collapsed variants of a Gibbs sampler are also established. An example is given to show that a blocked or collapsed Gibbs sampler does not in general inherit geometric ergodicity or a spectral gap from another blocked or collapsed Gibbs sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06745v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xavier Mak, James P. Hobert</dc:creator>
    </item>
    <item>
      <title>Robust Mean Estimation under Quantization</title>
      <link>https://arxiv.org/abs/2601.07074</link>
      <description>arXiv:2601.07074v1 Announce Type: cross 
Abstract: We consider the problem of mean estimation under quantization and adversarial corruption. We construct multivariate robust estimators that are optimal up to logarithmic factors in two different settings. The first is a one-bit setting, where each bit depends only on a single sample, and the second is a partial quantization setting, in which the estimator may use a small fraction of unquantized data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07074v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pedro Abdalla, Junren Chen</dc:creator>
    </item>
    <item>
      <title>Optimal Transport under Group Fairness Constraints</title>
      <link>https://arxiv.org/abs/2601.07144</link>
      <description>arXiv:2601.07144v1 Announce Type: cross 
Abstract: Ensuring fairness in matching algorithms is a key challenge in allocating scarce resources and positions. Focusing on Optimal Transport (OT), we introduce a novel notion of group fairness requiring that the probability of matching two individuals from any two given groups in the OT plan satisfies a predefined target. We first propose \texttt{FairSinkhorn}, a modified Sinkhorn algorithm to compute perfectly fair transport plans efficiently. Since exact fairness can significantly degrade matching quality in practice, we then develop two relaxation strategies. The first one involves solving a penalised OT problem, for which we derive novel finite-sample complexity guarantees. This result is of independent interest as it can be generalized to arbitrary convex penalties. Our second strategy leverages bilevel optimization to learn a ground cost that induces a fair OT solution, and we establish a bound guaranteeing that the learned cost yields fair matchings on unseen data. Finally, we present empirical results that illustrate the trade-offs between fairness and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07144v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linus Bleistein, Mathieu Dagr\'eou, Francisco Andrade, Thomas Boudou, Aur\'elien Bellet</dc:creator>
    </item>
    <item>
      <title>Approximate FKG inequalities for phase-bound spin systems</title>
      <link>https://arxiv.org/abs/2601.07169</link>
      <description>arXiv:2601.07169v1 Announce Type: cross 
Abstract: The FKG inequality is an invaluable tool in monotone spin systems satisfying the FKG lattice condition, which provides positive correlations for all coordinate-wise increasing functions of spins. However, the FKG lattice condition is somewhat brittle and is not preserved when confining a spin system to a particular phase. For instance, consider the Curie-Weiss model, which is a model of a ferromagnet with two phases at low temperature corresponding to positive and negative overall magnetization. It is not a priori clear if each phase internally has positive correlations for increasing functions, or if the positive correlations in the model arise primarily from the global choice of positive or negative magnetization.
  In this article, we show that the individual phases do indeed satisfy an approximate form of the FKG inequality in a class of generalized higher-order Curie-Weiss models (including the standard Curie-Weiss model as a special case), as well as in ferromagnetic exponential random graph models (ERGMs). To cover both of these settings, we present a general result which allows for the derivation of such approximate FKG inequalities in a straightforward manner from inputs related to metastable mixing; we expect that this general result will be widely applicable. In addition, we derive some consequences of the approximate FKG inequality, including a version of a useful covariance inequality originally due to Newman as well as Bulinski and Shabanovich. We use this to extend the proof of the central limit theorem for ERGMs within a phase at low temperatures, due to the second author, to the non-forest phase-coexistence regime, answering a question posed by Bianchi, Collet, and Magnanini for the edge-triangle model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07169v1</guid>
      <category>math.PR</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyaki Mukherjee, Vilas Winstein</dc:creator>
    </item>
    <item>
      <title>Multi-environment Invariance Learning with Missing Data</title>
      <link>https://arxiv.org/abs/2601.07247</link>
      <description>arXiv:2601.07247v1 Announce Type: cross 
Abstract: Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07247v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jia</dc:creator>
    </item>
    <item>
      <title>Variational Approximations for Robust Bayesian Inference via Rho-Posteriors</title>
      <link>https://arxiv.org/abs/2601.07325</link>
      <description>arXiv:2601.07325v1 Announce Type: cross 
Abstract: The $\rho$-posterior framework provides universal Bayesian estimation with explicit contamination rates and optimal convergence guarantees, but has remained computationally difficult due to an optimization over reference distributions that precludes intractable posterior computation. We develop a PAC-Bayesian framework that recovers these theoretical guarantees through temperature-dependent Gibbs posteriors, deriving finite-sample oracle inequalities with explicit rates and introducing tractable variational approximations that inherit the robustness properties of exact $\rho$-posteriors. Numerical experiments demonstrate that this approach achieves theoretical contamination rates while remaining computationally feasible, providing the first practical implementation of $\rho$-posterior inference with rigorous finite-sample guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07325v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>EL Mahdi Khribch, Pierre Alquier</dc:creator>
    </item>
    <item>
      <title>Characterization of multi-way binary tables with uniform margins and fixed correlations</title>
      <link>https://arxiv.org/abs/2601.07369</link>
      <description>arXiv:2601.07369v1 Announce Type: cross 
Abstract: In many applications involving binary variables, only pairwise dependence measures, such as correlations, are available. However, for multi-way tables involving more than two variables, these quantities do not uniquely determine the joint distribution, but instead define a family of admissible distributions that share the same pairwise dependence while potentially differing in higher-order interactions. In this paper, we introduce a geometric framework to describe the entire feasible set of such joint distributions with uniform margins. We show that this admissible set forms a convex polytope, analyze its symmetry properties, and characterize its extreme rays. These extremal distributions provide fundamental insights into how higher-order dependence structures may vary while preserving the prescribed pairwise information. Unlike traditional methods for table generation, which return a single table, our framework makes it possible to explore and understand the full admissible space of dependence structures, enabling more flexible choices for modeling and simulation. We illustrate the usefulness of our theoretical results through examples and a real case study on rater agreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07369v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Fontana, Elisa Perrone, Fabio Rapallo</dc:creator>
    </item>
    <item>
      <title>Riesz Representer Fitting under Bregman Divergence: A Unified Framework for Debiased Machine Learning</title>
      <link>https://arxiv.org/abs/2601.07752</link>
      <description>arXiv:2601.07752v1 Announce Type: cross 
Abstract: Estimating the Riesz representer is a central problem in debiased machine learning for causal and structural parameter estimation. Various methods for Riesz representer estimation have been proposed, including Riesz regression and covariate balancing. This study unifies these methods within a single framework. Our framework fits a Riesz representer model to the true Riesz representer under a Bregman divergence, which includes the squared loss and the Kullback--Leibler (KL) divergence as special cases. We show that the squared loss corresponds to Riesz regression, and the KL divergence corresponds to tailored loss minimization, where the dual solutions correspond to stable balancing weights and entropy balancing weights, respectively, under specific model specifications. We refer to our method as generalized Riesz regression, and we refer to the associated duality as automatic covariate balancing. Our framework also generalizes density ratio fitting under a Bregman divergence to Riesz representer estimation, and it includes various applications beyond density ratio estimation. We also provide a convergence analysis for both cases where the model class is a reproducing kernel Hilbert space (RKHS) and where it is a neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07752v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>A Complete Decomposition of Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2601.07834</link>
      <description>arXiv:2601.07834v1 Announce Type: cross 
Abstract: We show that any stochastic differential equation with prescribed time-dependent marginal distributions admits a decomposition into three components: a unique scalar field governing marginal evolution, a symmetric positive-semidefinite diffusion matrix field and a skew-symmetric matrix field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07834v1</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Duffield</dc:creator>
    </item>
    <item>
      <title>Directional testing for one-way MANOVA in divergent dimensions</title>
      <link>https://arxiv.org/abs/2403.07679</link>
      <description>arXiv:2403.07679v4 Announce Type: replace 
Abstract: Testing the equality of mean vectors across $g$ different groups plays an important role in many scientific fields. In regular frameworks, likelihood-based statistics under the normality assumption offer a general solution to this task. However, the accuracy of standard asymptotic results is not reliable when the dimension $p$ of the data is large relative to the sample size $n_i$ of each group. We propose here an exact directional test for the equality of $g$ normal mean vectors with identical unknown covariance matrix in a high dimensional setting, provided that $\sum_{i=1}^g n_i \ge p+g+1$. In the case of two groups ($g=2$), the directional test coincides with the Hotelling's $T^2$ test. In the more general situation where the $g$ independent groups may have different unknown covariance matrices, although exactness does not hold, simulation studies show that the directional test is more accurate than most commonly used likelihood{-}based solutions, at least in a moderate dimensional setting in which $p=O(n_i^\tau)$, $\tau \in (0,1)$. Robustness of the directional approach and its competitors under deviation from the assumption of multivariate normality is also numerically investigated. Our proposal is here applied to data on blood characteristics of male athletes and to microarray data storing gene expressions in patients with breast tumors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07679v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caizhu Huang, Claudia Di Caterina, Nicola Sartori</dc:creator>
    </item>
    <item>
      <title>Improved performance guarantees for Tukey's median</title>
      <link>https://arxiv.org/abs/2410.00219</link>
      <description>arXiv:2410.00219v3 Announce Type: replace 
Abstract: Is there a natural way to order data in dimension greater than one? The approach based on the notion of data depth, often associated with John Tukey, is among the most popular. Tukey's depth has found applications in robust statistics, graph theory, and the study of elections and social choice. We present improved performance guarantees for empirical Tukey's median, a deepest point associated with a given sample, when the data-generating distribution is elliptically symmetric and possibly anisotropic. Some of our results remain valid in the wider class of affine equivariant estimators. As a corollary of our bounds, we show that the typical diameter of the set of all empirical Tukey's medians scales like $o(n^{-1/2})$ where $n$ is the sample size. Moreover, when the data follow the bivariate normal distribution, we prove that with high probability, the diameter is of order $O(n^{-3/4}\log^{1/2}(n))$. On the technical side, we show how affine equivariance can be leveraged to improve concentration bounds; moreover, we develop sharp strong approximation results for empirical processes indexed by halfspaces that could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00219v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislav Minsker, Yinan Shen</dc:creator>
    </item>
    <item>
      <title>Fixed-strength spherical designs</title>
      <link>https://arxiv.org/abs/2502.06002</link>
      <description>arXiv:2502.06002v3 Announce Type: replace 
Abstract: A spherical $t$-design is a finite subset $X$ of the unit sphere such that every polynomial of degree at most $t$ has the same average over $X$ as it does over the entire sphere. Determining the minimum possible size of spherical designs, especially in a fixed dimension as $t \to \infty$, has been an important research topic for several decades. This paper presents results on the complementary asymptotic regime, where $t$ is fixed and the dimension tends to infinity. The main results in this paper are (1) a construction of smaller spherical designs via an explicit connection to Gaussian designs and (2) the exact order of magnitude of minimal-size signed $t$-designs, which is significantly smaller than predicted by a typical degrees-of-freedom heuristic. We also establish a method to ``project'' spherical designs between dimensions, prove a variety of results on approximate designs, and construct new $t$-wise independent subsets of $\{1,2,\dots,q\}^d$ which may be of independent interest. To achieve these results, we combine techniques from algebra, geometry, probability, representation theory, and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06002v3</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>math.MG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Travis Dillon</dc:creator>
    </item>
    <item>
      <title>Two statistical problems for multivariate mixture distributions</title>
      <link>https://arxiv.org/abs/2503.12147</link>
      <description>arXiv:2503.12147v4 Announce Type: replace 
Abstract: We address two important statistical problems: that of estimating for mixtures of multivariate normal distributions and mixtures of $t$-distributions based of univariate projections, and that of measuring the agreement between two different random partitions. The results are based on an earlier work of the authors, where it was shown that mixtures of multivariate Gaussian or $t$-distributions can be distinguished by projecting them onto a certain predetermined finite set of lines, the number of lines depending only on the total number of distributions involved and on the ambient dimension. We also compare our proposal with robust versions of the expectation-maximization method EM. In each case, we present algorithms for effecting the task, and compare them with existing methods by carrying out some simulati</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12147v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Fraiman, Leonardo Moreno, Thomas Ransford</dc:creator>
    </item>
    <item>
      <title>Calibration Bands for Mean Estimates within the Exponential Dispersion Family</title>
      <link>https://arxiv.org/abs/2503.18896</link>
      <description>arXiv:2503.18896v2 Announce Type: replace 
Abstract: A statistical model is said to be calibrated if the resulting mean estimates perfectly match the true means of the underlying responses. Aiming for calibration is often not achievable in practice as one has to deal with finite samples of noisy observations. A weaker notion of calibration is auto-calibration. An auto-calibrated model satisfies that the expected value of the responses for a given mean estimate matches this estimate. Testing for autocalibration has only been considered recently in the literature and we propose a new approach based on calibration bands. Calibration bands denote a set of lower and upper bounds such that the probability that the true means lie simultaneously inside those bounds exceeds some given confidence level. Such bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions. Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli distribution. We use the same idea in order to extend the construction to the entire exponential dispersion family that contains for example the binomial, Poisson, negative binomial, gamma and normal distributions. Moreover, we show that the obtained calibration bands allow us to construct various tests for calibration and auto-calibration, respectively. As the construction of the bands does not rely on asymptotic results, we emphasize that our tests can be used for any sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18896v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Delong, Selim Gatti, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Asymptotically well-calibrated Bayesian $p$-value using the Kolmogorov-Smirnov statistic</title>
      <link>https://arxiv.org/abs/2504.14077</link>
      <description>arXiv:2504.14077v3 Announce Type: replace 
Abstract: The posterior predictive $p$-value (ppp) is widely used in Bayesian model evaluation. However, due to double use of the data, the ppp may not be a valid $p$-value even in large samples: The asymptotic null distribution of the ppp can be non-uniform unless the underlying test statistic satisfies certain well-calibration conditions. Such conditions have been studied in the literature for asymptotically normal test statistics. We extend this line of work by establishing well-calibration conditions for test statistics that are not necessarily asymptotically normal. In particular, we show that Kolmogorov-Smirnov (KS)-type test statistics satisfy these conditions, such that their ppps are asymptotically well-calibrated Bayesian $p$-values. KS-type statistics are versatile, omnibus, and sensitive to model misspecifications. They apply to i.i.d. real-valued data, as well as non-identically distributed observations under regression models. Numerical experiments demonstrate that such $p$-values are well behaved in finite samples and can effectively detect a wide range of alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14077v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueming Shen, Surya Tokdar</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v5 Announce Type: replace 
Abstract: We introduce a novel measure of dependence that captures the extent to which a random variable $Y$ is determined by a random vector $X$. The measure equals zero precisely when $Y$ and $X$ are independent, and it attains one exactly when $Y$ is almost surely a measurable function of $X$. We further extend this framework to define a measure of conditional dependence between $Y$ and $X$ given $Z$. We propose a simple and interpretable estimator with computational complexity comparable to classical correlation coefficients, including those of Pearson, Spearman, and Chatterjee. Leveraging this dependence measure, we develop a tuning-free, model-agnostic variable selection procedure and establish its consistency under appropriate sparsity conditions. Extensive experiments on synthetic and real datasets highlight the strong empirical performance of our methodology and demonstrate substantial gains over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v5</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>Riesz representers for the rest of us</title>
      <link>https://arxiv.org/abs/2507.19413</link>
      <description>arXiv:2507.19413v3 Announce Type: replace 
Abstract: The application of semiparametric efficient estimators, particularly those that leverage machine learning, is rapidly expanding within epidemiology and causal inference. This literature is increasingly invoking the Riesz representation theorem and Riesz regression. This paper aims to introduce the Riesz representation theorem to an epidemiologic audience, explaining what it is and why it's useful, using step-by-step worked examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19413v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas T. Williams, Oliver J. Hines, Kara E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Time-complexity of sampling from a multimodal distribution using sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2508.02763</link>
      <description>arXiv:2508.02763v2 Announce Type: replace 
Abstract: We study a sequential Monte Carlo algorithm to sample from the Gibbs measure with a non-convex energy function at a low temperature. We use the practical and popular geometric annealing schedule, and use a Langevin diffusion at each temperature level. The Langevin diffusion only needs to run for a time that is long enough to ensure local mixing within energy valleys, which is much shorter than the time required for global mixing. Our main result shows convergence of Monte Carlo estimators with time complexity that, approximately, scales like the fourth power of the inverse temperature, and the square of the inverse allowed error. We also study this algorithm in an illustrative model scenario where more explicit estimates can be given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02763v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Han, Gautam Iyer, Dejan Slep\v{c}ev</dc:creator>
    </item>
    <item>
      <title>Robust Confidence Intervals for a Binomial Proportion: Local Optimality and Adaptivity</title>
      <link>https://arxiv.org/abs/2509.05568</link>
      <description>arXiv:2509.05568v2 Announce Type: replace 
Abstract: This paper revisits the classical problem of interval estimation of a binomial proportion under Huber contamination. Our main result derives the rate of optimal interval length when the contamination proportion is unknown under a local minimax framework, where the performance of an interval is evaluated at each point in the parameter space. By comparing the rate with the optimal length of a confidence interval that is allowed to use the knowledge of contamination proportion, we characterize the exact adaptation cost due to the ignorance of data quality. Our construction of the confidence interval to achieve local length optimality builds on robust hypothesis testing with a new monotonization step, which guarantees valid coverage, boundary-respecting intervals, and an efficient algorithm for computing the endpoints. The general strategy of interval construction can be applied beyond the binomial setting, and leads to optimal interval estimation for Poisson data with contamination as well. We also investigate a closely related Erd\H{o}s--R\'{e}nyi model with node contamination. Though its optimal rate of parameter estimation agrees with that of the binomial setting, we show that adaptation to unknown contamination proportion is provably impossible for interval estimation in that setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05568v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjun Cho, Yuetian Luo, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Batch learning equals online learning in Bayesian supervised learning</title>
      <link>https://arxiv.org/abs/2510.16892</link>
      <description>arXiv:2510.16892v4 Announce Type: replace 
Abstract: In this paper we study Bayesian supervised learning models proposed by L\^e in \cite{Le2025}. We show the existence of Bayesian inversions on universal Bayesian supervised learning models $(\Pp (\Yy)^\Xx, \mu, \Id_{\Pp (\Yy) ^\Xx}, \Pp (\Yy)^\Xx)$ for arbitrary input space $\Xx$, Souslin label space $\Yy$, and prior probability measure $\mu \in \Pp (\Pp (\Yy) ^\Xx)$. Using functoriality of probabilistic morphisms, we prove that sequential and batch Bayesian inversions coincide in supervised learning models with conditionally independent (possibly non-i.i.d.) data \cite{Le2025}. This equivalence holds without domination or discreteness assumptions on sampling operators. We derive a recursive formula for posterior predictive distributions, which reduces to the Kalman filter in Gaussian process regression. For Polish label spaces $\mathcal{Y}$ and arbitrary input sets $\mathcal{X}$, we characterize probability measures on $\mathcal{P}(\mathcal{Y})^{\mathcal{X}}$ via projective systems, generalizing Orbanz \cite{Orbanz2011}. We revisit MacEachern's Dependent Dirichlet Processes (DDP) \cite{MacEachern2000} using copula-based constructions \cite{BJQ2012} and show how to compute posterior predictive distributions in universal Bayesian supervised models with DDP priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16892v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\^ong V\^an L\^e</dc:creator>
    </item>
    <item>
      <title>The Feldman-H\'ajek Dichotomy for Countable Gaussian Mixtures and their Asymptotic Separability in High Dimensions</title>
      <link>https://arxiv.org/abs/2601.03911</link>
      <description>arXiv:2601.03911v2 Announce Type: replace 
Abstract: This paper establishes the theoretical foundations for the asymptotic separability of Gaussian Mixture Models (GMMs) in high dimensions by extending the classical Feldman-H\'ajek theorem. We first prove that a countable mixture of Gaussian measures is a well-defined probability measure. Our primary result, the Gaussian Mixture Dichotomy Theorem, demonstrates that the mutual singularity of individual Gaussian components is a sufficient condition for the mutual singularity of the resulting mixtures. We provide a rigorous proof and further discuss the ``Mixed Case,'' where the presence of even a single equivalent pair of components leads to partial absolute continuity via the Lebesgue decomposition, thereby defining the theoretical limits of perfect classification in infinite-dimensional spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03911v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Umberto Michelucci</dc:creator>
    </item>
    <item>
      <title>Approximating Persistent Homology for Large Datasets</title>
      <link>https://arxiv.org/abs/2204.09155</link>
      <description>arXiv:2204.09155v3 Announce Type: replace-cross 
Abstract: Persistent homology is an important methodology in topological data analysis which adapts theory from algebraic topology to data settings. Computing persistent homology produces persistence diagrams, which have been successfully used in diverse domains. Despite its widespread use, persistent homology is simply impossible to compute when a dataset is very large. We study a statistical approach to the problem of computing persistent homology for massive datasets using a multiple subsampling framework and extend it to three summaries of persistent homology: H\"{o}lder continuous vectorizations of persistence diagrams; the alternative representation as persistence measures; and standard persistence diagrams. Specifically, we derive finite sample convergence rates for empirical means for persistent homology and practical guidance on interpreting and tuning parameters. We validate our approach through extensive experiments on both synthetic and real-world data. We demonstrate the performance of multiple subsampling in a permutation test to analyze the topological structure of Poincar\'{e} embeddings of large lexical databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.09155v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Cao, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy</title>
      <link>https://arxiv.org/abs/2302.09049</link>
      <description>arXiv:2302.09049v4 Announce Type: replace-cross 
Abstract: We construct multiperiodic processes -- a simple example of stationary ergodic (but not mixing) processes over natural numbers that enjoy the vanishing entropy rate under a mild condition. Multiperiodic processes are supported on randomly shifted deterministic sequences called multiperiodic sequences, which can be efficiently generated using an algorithm called the Infinite Clock. Under a suitable parameterization, multiperiodic sequences exhibit relative frequencies of particular numbers given by Zipf's law. Exactly in the same setting, the respective multiperiodic processes satisfy an asymptotic power-law growth of block entropy, called Hilberg's law. Hilberg's law is deemed to hold for statistical language models, in particular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09049v4</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz D\k{e}bowski</dc:creator>
    </item>
    <item>
      <title>Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2402.04691</link>
      <description>arXiv:2402.04691v4 Announce Type: replace-cross 
Abstract: This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04691v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Shi, Jia-Qi Yang</dc:creator>
    </item>
    <item>
      <title>Point processes with event time uncertainty</title>
      <link>https://arxiv.org/abs/2411.02694</link>
      <description>arXiv:2411.02694v2 Announce Type: replace-cross 
Abstract: Point processes are widely used statistical models for continuous-time discrete event data, such as medical records, crime reports, and social network interactions, to capture the influence of historical events on future occurrences. In many applications, however, event times are not observed exactly, motivating the need to incorporate time uncertainty into point process modeling. In this work, we introduce a framework for modeling time-uncertain self-exciting point processes, known as Hawkes processes, possibly defined over a network. We begin by formulating the model in continuous time under assumptions motivated by real-world scenarios. By imposing a time grid, we obtain a discrete-time model that facilitates inference and enables computation via first-order optimization methods such as gradient descent and variational inequality (VI). We establish a parameter recovery guarantee for VI inference with an $O(1/k)$ convergence rate using $k$ steps. Our framework accommodates non-stationary processes by representing the influence kernel as a matrix (or tensor on a network), while also encompassing stationary processes, such as the classical Hawkes process, as a special case. Empirically, we demonstrate that the proposed approach outperforms existing baselines on both simulated and real-world datasets, including the sepsis-associated derangement prediction challenge and the Atlanta Police Crime Dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02694v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiuyuan Cheng, Tingnan Gong, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Ordinary Least Squares as an Attention Mechanism</title>
      <link>https://arxiv.org/abs/2504.09663</link>
      <description>arXiv:2504.09663v2 Announce Type: replace-cross 
Abstract: I show that ordinary least squares (OLS) predictions can be rewritten as the output of a restricted attention module, akin to those forming the backbone of large language models. This connection offers an alternative perspective on attention beyond the conventional information retrieval framework, making it more accessible to researchers and analysts with a background in traditional statistics. It falls into place when OLS is framed as a similarity-based method in a transformed regressor space, distinct from the standard view based on partial correlations. In fact, the OLS solution can be recast as the outcome of an alternative problem: minimizing squared prediction errors by optimizing the embedding space in which training and test vectors are compared via inner products. Rather than estimating coefficients directly, we equivalently learn optimal encoding and decoding operations for predictors. From this vantage point, OLS maps naturally onto the query-key-value structure of attention mechanisms. Building on this foundation, I discuss key elements of Transformer-style attention and draw connections to classic ideas from time series econometrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09663v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Goulet Coulombe</dc:creator>
    </item>
    <item>
      <title>Learning Operators by Regularized Stochastic Gradient Descent with Operator-valued Kernels</title>
      <link>https://arxiv.org/abs/2504.18184</link>
      <description>arXiv:2504.18184v3 Announce Type: replace-cross 
Abstract: We consider a class of statistical inverse problems involving the estimation of a regression operator from a Polish space to a separable Hilbert space, where the target lies in a vector-valued reproducing kernel Hilbert space induced by an operator-valued kernel. To address the associated ill-posedness, we analyze regularized stochastic gradient descent (SGD) algorithms in both online and finite-horizon settings. The former uses polynomially decaying step sizes and regularization parameters, while the latter adopts fixed values. Under suitable structural and distributional assumptions, we establish dimension-independent bounds for prediction and estimation errors. The resulting convergence rates are near-optimal in expectation, and we also derive high-probability estimates that imply almost sure convergence. Our analysis introduces a general technique for obtaining high-probability guarantees in infinite-dimensional settings. Possible extensions to broader kernel classes and encoder-decoder structures are briefly discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18184v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Qi Yang, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Omnibus goodness-of-fit tests based on trigonometric moments</title>
      <link>https://arxiv.org/abs/2507.18591</link>
      <description>arXiv:2507.18591v2 Announce Type: replace-cross 
Abstract: We propose a new omnibus goodness-of-fit test based on trigonometric moments of probability-integral-transformed data. The test builds on the framework of the LK test introduced by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077-1084], but fully exploits the covariance structure of the associated trigonometric statistics. As a result, our test statistic converges under the null hypothesis to a $\chi_2^2$ distribution, even in the presence of nuisance parameters, yielding a well-calibrated rejection region. We derive the exact asymptotic covariance matrix required for normalization and propose a unified approach to computing the LK normalizing scalar. The applicability of both the proposed test and the LK test is substantially expanded by providing implementation details for 11 families of continuous distributions, covering most commonly used parametric models. Simulation studies demonstrate accurate empirical size, close to the nominal level, and strong power properties, yielding fully plug-and-play procedures. Further insight is provided by an analysis under local alternatives. The methodology is illustrated using surface temperature forecast errors from a numerical weather prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18591v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>A Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning</title>
      <link>https://arxiv.org/abs/2509.11070</link>
      <description>arXiv:2509.11070v3 Announce Type: replace-cross 
Abstract: We develop a stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces utilizing general Mercer operator-valued kernels. Our framework encompasses two key classes: (i) compact kernels, which admit discrete spectral decompositions, and (ii) diagonal kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and $T$ is a positive operator on the output space. This broad setting induces expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that generalize the classical $K=kI$ paradigm, thereby enabling rich structural modeling with rigorous theoretical guarantees. To address target operators lying outside the RKHS, we introduce vector-valued interpolation spaces to precisely quantify misspecification error. Within this framework, we establish dimension-free polynomial convergence rates, demonstrating that nonlinear operator learning can overcome the curse of dimensionality. The use of general operator-valued kernels further allows us to derive rates for intrinsically nonlinear operator learning, going beyond the linear-type behavior inherent in diagonal constructions of $K=kI$. Importantly, this framework accommodates a wide range of operator learning tasks, ranging from integral operators such as Fredholm operators to architectures based on encoder-decoder representations. Moreover, we validate its effectiveness through numerical experiments on the two-dimensional Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11070v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Qi Yang, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Predictive inference for time series: why is split conformal effective despite temporal dependence?</title>
      <link>https://arxiv.org/abs/2510.02471</link>
      <description>arXiv:2510.02471v2 Announce Type: replace-cross 
Abstract: We consider the problem of uncertainty quantification for prediction in a time series: if we use past data to forecast the next time point, can we provide valid prediction intervals around our forecasts? To avoid placing distributional assumptions on the data, in recent years the conformal prediction method has been a popular approach for predictive inference, since it provides distribution-free coverage for any iid or exchangeable data distribution. However, in the time series setting, the strong empirical performance of conformal prediction methods is not well understood, since even short-range temporal dependence is a strong violation of the exchangeability assumption. Using predictors with "memory" -- i.e., predictors that utilize past observations, such as autoregressive models -- further exacerbates this problem. In this work, we examine the theoretical properties of split conformal prediction in the time series setting, including the case where predictors may have memory. Our results bound the loss of coverage of these methods in terms of a new "switch coefficient", measuring the extent to which temporal dependence within the time series creates violations of exchangeability. Our characterization of the coverage probability is sharp over the class of stationary, $\beta$-mixing processes. Along the way, we introduce tools that may prove useful in analyzing other predictive inference methods for dependent data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02471v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rina Foygel Barber, Ashwin Pananjady</dc:creator>
    </item>
    <item>
      <title>Gradient descent for deep equilibrium single-index models</title>
      <link>https://arxiv.org/abs/2511.16976</link>
      <description>arXiv:2511.16976v2 Announce Type: replace-cross 
Abstract: Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16976v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Testing Monotonicity in a Finite Population</title>
      <link>https://arxiv.org/abs/2512.25032</link>
      <description>arXiv:2512.25032v2 Announce Type: replace-cross 
Abstract: We consider the extent to which we can learn from a completely randomized experiment whether all individuals have treatment effects that are weakly of the same sign, a condition we call monotonicity. From a classical sampling perspective, it is well-known that monotonicity is not falsifiable. By contrast, we show from the design-based perspective -- in which the units in the population are fixed and only treatment assignment is stochastic -- that the distribution of treatment effects in the finite population (and hence whether monotonicity holds) is formally identified. We argue, however, that the usual definition of identification is unnatural in the design-based setting because it imagines knowing the distribution of outcomes over different treatment assignments for the same units. We thus evaluate the informativeness of the data by the extent to which it enables frequentist testing and Bayesian updating. We show that frequentist tests can have nontrivial power against some alternatives, but power is generically limited. Likewise, we show that there exist (non-degenerate) Bayesian priors that never update about whether monotonicity holds. We conclude that, despite the formal identification result, the ability to learn about monotonicity from data in practice is severely limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25032v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Jonathan Roth, Jann Spiess</dc:creator>
    </item>
  </channel>
</rss>
