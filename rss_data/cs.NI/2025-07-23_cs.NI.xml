<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Sweet Danger of Sugar: Debunking Representation Learning for Encrypted Traffic Classification</title>
      <link>https://arxiv.org/abs/2507.16438</link>
      <description>arXiv:2507.16438v1 Announce Type: new 
Abstract: Recently we have witnessed the explosion of proposals that, inspired by Language Models like BERT, exploit Representation Learning models to create traffic representations. All of them promise astonishing performance in encrypted traffic classification (up to 98% accuracy). In this paper, with a networking expert mindset, we critically reassess their performance. Through extensive analysis, we demonstrate that the reported successes are heavily influenced by data preparation problems, which allow these models to find easy shortcuts - spurious correlation between features and labels - during fine-tuning that unrealistically boost their performance. When such shortcuts are not present - as in real scenarios - these models perform poorly. We also introduce Pcap-Encoder, an LM-based representation learning model that we specifically design to extract features from protocol headers. Pcap-Encoder appears to be the only model that provides an instrumental representation for traffic classification. Yet, its complexity questions its applicability in practical settings. Our findings reveal flaws in dataset preparation and model training, calling for a better and more conscious test design. We propose a correct evaluation methodology and stress the need for rigorous benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16438v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3718958.3750498</arxiv:DOI>
      <dc:creator>Yuqi Zhao, Giovanni Dettori, Matteo Boffa, Luca Vassio, Marco Mellia</dc:creator>
    </item>
    <item>
      <title>An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes</title>
      <link>https://arxiv.org/abs/2507.16594</link>
      <description>arXiv:2507.16594v1 Announce Type: new 
Abstract: Running deep learning inference directly on ultra-low-power edge/IoT nodes has been limited by the tight memory and compute budgets of microcontrollers. Split learning (SL) addresses this limitation in which it executes part of the inference process on the sensor and off-loads the remainder to a companion device. In the context of constrained devices and the related impact of low-power, over-the-air transport protocols, the performance of split learning remains largely unexplored. TO the best of our knowledge, this paper presents the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards, designed to benchmark the over-the-air performance of split learning TinyML in edge/IoT environments. We benchmark the performance of a MobileNetV2 image recognition model, which is quantized to 8-bit integers, partitioned, and delivered to the nodes via over-the-air updates. The intermediate activations are exchanged through different wireless communication methods: ESP-NOW, BLE, and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on identical hardware. Measurements show that splitting the model after block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in 3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s. ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery life further but increases latency beyond 10s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16594v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zied Jenhani, Mounir Bensalem, Jasenka Dizdarevi\'c, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds</title>
      <link>https://arxiv.org/abs/2507.16077</link>
      <description>arXiv:2507.16077v1 Announce Type: cross 
Abstract: Network Slicing (NS) realization requires AI-native orchestration architectures to efficiently and intelligently handle heterogeneous user requirements. To achieve this, network slicing is evolving towards a more user-centric digital transformation, focusing on architectures that incorporate native intelligence to enable self-managed connectivity in an integrated and isolated manner. However, these initiatives face the challenge of validating their results in production environments, particularly those utilizing ML-enabled orchestration, as they are often tested in local networks or laboratory simulations. This paper proposes a large-scale validation method using a network slicing prediction model to forecast latency using Deep Neural Networks (DNNs) and basic ML algorithms embedded within an NS architecture, evaluated in real large-scale production testbeds. It measures and compares the performance of different DNNs and ML algorithms, considering a distributed database application deployed as a network slice over two large-scale production testbeds. The investigation highlights how AI-based prediction models can enhance network slicing orchestration architectures and presents a seamless, production-ready validation method as an alternative to fully controlled simulations or laboratory setups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16077v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2025.107971</arxiv:DOI>
      <arxiv:journal_reference>FUTURE GENERATION COMPUTER SYSTEMS, Volume 174,2026,107971</arxiv:journal_reference>
      <dc:creator>Rodrigo Moreira, Rafael Pasquini, Joberto S. B. Martins, Tereza C. Carvalho, Fl\'avio de Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>Latent Space Alignment for AI-Native MIMO Semantic Communications</title>
      <link>https://arxiv.org/abs/2507.16680</link>
      <description>arXiv:2507.16680v1 Announce Type: cross 
Abstract: Semantic communications focus on prioritizing the understanding of the meaning behind transmitted data and ensuring the successful completion of tasks that motivate the exchange of information. However, when devices rely on different languages, logic, or internal representations, semantic mismatches may occur, potentially hindering mutual understanding. This paper introduces a novel approach to addressing latent space misalignment in semantic communications, exploiting multiple-input multiple-output (MIMO) communications. Specifically, our method learns a MIMO precoder/decoder pair that jointly performs latent space compression and semantic channel equalization, mitigating both semantic mismatches and physical channel impairments. We explore two solutions: (i) a linear model, optimized by solving a biconvex optimization problem via the alternating direction method of multipliers (ADMM); (ii) a neural network-based model, which learns semantic MIMO precoder/decoder under transmission power budget and complexity constraints. Numerical results demonstrate the effectiveness of the proposed approach in a goal-oriented semantic communication scenario, illustrating the main trade-offs between accuracy, communication burden, and complexity of the solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16680v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Edoardo Pandolfo, Simone Fiorellino, Emilio Calvanese Strinati, Paolo Di Lorenzo</dc:creator>
    </item>
    <item>
      <title>InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers</title>
      <link>https://arxiv.org/abs/2502.03885</link>
      <description>arXiv:2502.03885v4 Announce Type: replace 
Abstract: Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 take a middle-ground approach, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs).
  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that unifies connectivity and dynamic switching at the transceiver level} using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, InfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing the topology to adapt to variable-size rings. This design provides: i) datacenter-wide scalability without cost explosion; ii) fault resilience by isolating failures to a single node, and iii) full bandwidth utilization for fault-free GPUs. Key innovations include a Silicon Photonic (SiPh)-based low-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology co-designed with intra-/inter-node communication, and an HBD-DCN orchestration algorithm maximizing GPU utilization while minimizing cross-ToR datacenter network traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of the cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude lower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault ratios are under 7%, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs per Node).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03885v4</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenchen Shou, Guyue Liu, Hao Nie, Huaiyu Meng, Yu Zhou, Yimin Jiang, Wenqing Lv, Yelong Xu, Yuanwei Lu, Zhang Chen, Yanbo Yu, Yichen Shen, Yibo Zhu, Daxin Jiang</dc:creator>
    </item>
    <item>
      <title>Open Wireless Digital Twin: End-to-End 5G Mobility Emulation with OpenAirInterface and Sionna RT</title>
      <link>https://arxiv.org/abs/2503.12177</link>
      <description>arXiv:2503.12177v3 Announce Type: replace 
Abstract: This study presents an end-to-end wireless digital twin platform constructed using open-source software and open data to enhance the evaluation of mobile communication systems. The proposed open wireless digital twin (OWDT) integrates OpenAirInterface (OAI) for 5G NR protocol stack emulation and NVIDIA Sionna RT for high-resolution ray-tracing-based radio propagation modeling. This integration enables the realistic emulation of 5G wireless communication in mobility scenarios on a CPU-based Linux system, leveraging real-world building data to bridge the gap between theoretical simulations and real-world deployment. The platform also incorporates OAI FlexRIC, which is an implementation aligned with the O-RAN near-real-time RAN Intelligent Controller (near-RT RIC), to dynamically monitor the key performance indicators (KPIs). Through extensive evaluation in urban environments, this study demonstrated the validity of the emulation framework, revealing its capability to replicate real-world communication dynamics with high fidelity. The results underscore the potential of the OWDT to accelerate wireless system development, reduce experimental costs, and optimize network configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12177v3</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Iye, Masaya Sakamoto, Shohei Takaya, Eisaku Sato, Yuki Susukida, Yu Nagaoka, Kazuki Maruta, Jin Nakazato</dc:creator>
    </item>
    <item>
      <title>Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering</title>
      <link>https://arxiv.org/abs/2507.13179</link>
      <description>arXiv:2507.13179v2 Announce Type: replace 
Abstract: As 6G networks are developed and defined, offloading of XR applications is emerging as one of the strong new use cases. The reduced 6G latency coupled with edge processing infrastructure will for the first time provide a realistic offloading scenario in cellular networks where several computationally intensive functions, including rendering, can migrate from the user device and into the network. A key advantage of doing so is the lowering of the battery needs in the user devices and the possibility to design new devices with smaller form factors. However, offloading introduces increased delays compared to local execution, primarily due to network transmission latency and queuing delays at edge servers, especially under multi-user concurrency. Despite the computational power of edge platforms, the resulting motion-to-photon (MTP) latency negatively impacts user experience. To mitigate this, motion prediction has been proposed to offset delays. Existing approaches build on either deep learning or Kalman filtering. Deep learning techniques face scalability limitations at the resource-constrained edge, as their computational expense intensifies with increasing user concurrency, while Kalman filtering suffers from poor handling of complex movements and fragility to packet loss inherent in 6G's high-frequency radio interfaces. In this work, we introduce a context-aware error-state Kalman filter (ESKF) prediction framework, which forecasts the user's head motion trajectory to compensate for MTP latency in remote XR. By integrating a motion classifier that categorizes head motions based on their predictability, our algorithm demonstrates reduced prediction error across different motion classes. Our findings demonstrate that the optimized ESKF not only surpasses traditional Kalman filters in positional and orientational accuracy but also exhibits enhanced robustness and resilience to packet loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13179v2</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Zhong, Bj\"orn Landfeldt, G\"unter Alce, Hector A Caltenco</dc:creator>
    </item>
  </channel>
</rss>
