<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Aug 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improved thresholds for e-values</title>
      <link>https://arxiv.org/abs/2408.11307</link>
      <description>arXiv:2408.11307v1 Announce Type: new 
Abstract: The rejection threshold used for e-values and e-processes is by default set to $1/\alpha$ for a guaranteed type-I error control at $\alpha$, based on Markov's and Ville's inequalities. This threshold can be wasteful in practical applications. We discuss how this threshold can be improved under additional distributional assumptions on the e-values; some of these assumptions are naturally plausible and empirically observable, without knowing explicitly the form or model of the e-values. For small values of $\alpha$, the threshold can roughly be improved (divided) by a factor of $2$ for decreasing or unimodal densities, and by a factor of $e$ for decreasing or unimodal-symmetric densities of the log-transformed e-value. Moreover, we propose to use the supremum of comonotonic e-values, which is shown to preserve the type-I error guarantee. We also propose some preliminary methods to boost e-values in the e-BH procedure under some distributional assumptions while controlling the false discovery rate. Through a series of simulation studies, we demonstrate the effectiveness of our proposed methods in various testing scenarios, showing enhanced power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11307v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Blier-Wong, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Small Sample Behavior of Wasserstein Projections, Connections to Empirical Likelihood, and Other Applications</title>
      <link>https://arxiv.org/abs/2408.11753</link>
      <description>arXiv:2408.11753v1 Announce Type: new 
Abstract: The empirical Wasserstein projection (WP) distance quantifies the Wasserstein distance from the empirical distribution to a set of probability measures satisfying given expectation constraints. The WP is a powerful tool because it mitigates the curse of dimensionality inherent in the Wasserstein distance, making it valuable for various tasks, including constructing statistics for hypothesis testing, optimally selecting the ambiguity size in Wasserstein distributionally robust optimization, and studying algorithmic fairness. While the weak convergence analysis of the WP as the sample size $n$ grows is well understood, higher-order (i.e., sharp) asymptotics of WP remain unknown. In this paper, we study the second-order asymptotic expansion and the Edgeworth expansion of WP, both expressed as power series of $n^{-1/2}$. These expansions are essential to develop improved confidence level accuracy and a power expansion analysis for the WP-based tests for moment equations null against local alternative hypotheses. As a by-product, we obtain insightful criteria for comparing the power of the Empirical Likelihood and Hotelling's $T^2$ tests against the WP-based test. This insight provides the first comprehensive guideline for selecting the most powerful local test among WP-based, empirical-likelihood-based, and Hotelling's $T^2$ tests for a null. Furthermore, we introduce Bartlett-type corrections to improve the approximation to WP distance quantiles and, thus, improve the coverage in WP applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11753v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sirui Lin, Jose Blanchet, Peter Glynn, Viet Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Dickman type stochastic processes with short- and long- range dependence</title>
      <link>https://arxiv.org/abs/2408.11521</link>
      <description>arXiv:2408.11521v1 Announce Type: cross 
Abstract: We study properties of the (generalized) Dickman distribution with two parameters and the stationary solution of the Ornstein-Uhlenbeck stochastic differential equation driven by a Poisson process. In particular, we show that the marginal distribution of this solution is the Dickman distribution. Additionally, we investigate superpositions of Ornstein-Uhlenbeck processes which may have short- or long-range dependencies and marginal distribution of the form of the Dickman distribution. The numerical algorithm for simulation of these processes is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11521v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danijel Grahovac, Anastasiia Kovtun, Nikolai N. Leonenko, Andrey Pepelyshev</dc:creator>
    </item>
    <item>
      <title>Asymptotic Normality of Chatterjee's Rank Correlation</title>
      <link>https://arxiv.org/abs/2408.11547</link>
      <description>arXiv:2408.11547v1 Announce Type: cross 
Abstract: We prove that Chatterjee's rank correlation based on i.i.d. copies of a random vector $(X,Y)$ is asymptotically normal whenever $Y$ is not almost surely constant. No further conditions on the joint distribution of $X$ and $Y$ are required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11547v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Kroll</dc:creator>
    </item>
    <item>
      <title>Distance Correlation in Multiple Biased Sampling Models</title>
      <link>https://arxiv.org/abs/2408.11808</link>
      <description>arXiv:2408.11808v1 Announce Type: cross 
Abstract: Testing the independence between random vectors is a fundamental problem in statistics. Distance correlation, a recently popular dependence measure, is universally consistent for testing independence against all distributions with finite moments. However, when data are subject to selection bias or collected from multiple sources or schemes, spurious dependence may arise. This creates a need for methods that can effectively utilize data from different sources and correct these biases. In this paper, we study the estimation of distance covariance and distance correlation under multiple biased sampling models, which provide a natural framework for addressing these issues. Theoretical properties, including the strong consistency and asymptotic null distributions of the distance covariance and correlation estimators, and the rate at which the test statistic diverges under sequences of alternatives approaching the null, are established. A weighted permutation procedure is proposed to determine the critical value of the independence test. Simulation studies demonstrate that our approach improves both the estimation of distance correlation and the power of the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11808v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuwei Ke, Hok Kan Ling, Yanglei Song</dc:creator>
    </item>
    <item>
      <title>Joint Spectral Clustering in Multilayer Degree-Corrected Stochastic Blockmodels</title>
      <link>https://arxiv.org/abs/2212.05053</link>
      <description>arXiv:2212.05053v2 Announce Type: replace 
Abstract: Modern network datasets are often composed of multiple layers, either as different views, time-varying observations, or independent sample units, resulting in collections of networks over the same set of vertices but with potentially different connectivity patterns on each network. These data require models and methods that are flexible enough to capture local and global differences across the networks, while at the same time being parsimonious and tractable to yield computationally efficient and theoretically sound solutions that are capable of aggregating information across the networks. This paper considers the multilayer degree-corrected stochastic blockmodel, where a collection of networks share the same community structure, but degree-corrections and block connection probability matrices are permitted to be different. We establish the identifiability of this model and propose a spectral clustering algorithm for community detection in this setting. Our theoretical results demonstrate that the misclustering error rate of the algorithm improves exponentially with multiple network realizations, even in the presence of significant layer heterogeneity with respect to degree corrections, signal strength, and spectral properties of the block connection probability matrices. Simulation studies show that this approach improves on existing multilayer community detection methods in this challenging regime. Furthermore, in a case study of US airport data through January 2016 -- September 2021, we find that this methodology identifies meaningful community structure and trends in airport popularity influenced by pandemic impacts on travel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05053v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Zachary Lubberts, Jes\'us Arroyo</dc:creator>
    </item>
    <item>
      <title>Is Cross-Validation the Gold Standard to Evaluate Model Performance?</title>
      <link>https://arxiv.org/abs/2407.02754</link>
      <description>arXiv:2407.02754v2 Announce Type: replace 
Abstract: Cross-Validation (CV) is the default choice for evaluating the performance of machine learning models. Despite its wide usage, their statistical benefits have remained half-understood, especially in challenging nonparametric regimes. In this paper we fill in this gap and show that in fact, for a wide spectrum of models, CV does not statistically outperform the simple "plug-in" approach where one reuses training data for testing evaluation. Specifically, in terms of both the asymptotic bias and coverage accuracy of the associated interval for out-of-sample evaluation, $K$-fold CV provably cannot outperform plug-in regardless of the rate at which the parametric or nonparametric models converge. Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account. We obtain our theoretical comparisons via a novel higher-order Taylor analysis that allows us to derive necessary conditions for limit theorems of testing evaluations, which applies to model classes that are not amenable to previously known sufficient conditions. Our numerical results demonstrate that plug-in performs indeed no worse than CV across a wide range of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02754v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garud Iyengar, Henry Lam, Tianyu Wang</dc:creator>
    </item>
    <item>
      <title>Local Fr\'echet regression with circular predictors</title>
      <link>https://arxiv.org/abs/2408.10118</link>
      <description>arXiv:2408.10118v2 Announce Type: replace 
Abstract: Fr\'echet regression extends the principles of linear regression to accommodate responses valued in generic metric spaces. While this approach has primarily focused on exploring relationships between Euclidean predictors and non-Euclidean responses, our work introduces a novel statistical method for handling random objects with circular predictors. We concentrate on local constant and local linear Fr\'echet regression, providing rigorous proofs for the upper bounds of both bias and stochastic deviation of the estimators under mild conditions. This research lays the groundwork for broadening the application of Fr\'echet regression to scenarios involving non-Euclidean covariates, thereby expanding its utility in complex data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10118v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Jun Im, Jeong Min Jeon</dc:creator>
    </item>
    <item>
      <title>Fatigue detection via sequential testing of biomechanical data using martingale statistic</title>
      <link>https://arxiv.org/abs/2306.01566</link>
      <description>arXiv:2306.01566v2 Announce Type: replace-cross 
Abstract: Injuries to the knee joint are very common for long-distance and frequent runners, an issue which is often attributed to fatigue. We address the problem of fatigue detection from biomechanical data from different sources, consisting of lower extremity joint angles and ground reaction forces from running athletes with the goal of better understanding the impact of fatigue on the biomechanics of runners in general and on an individual level. This is done by sequentially testing for change in a datastream using a simple martingale test statistic. Time-uniform probabilistic martingale bounds are provided which are used as thresholds for the test statistic. Sharp bounds can be developed by a hybrid of a piece-wise linear- and a law of iterated logarithm- bound over all time regimes, where the probability of an early detection is controlled in a uniform way. If the underlying distribution of the data gradually changes over the course of a run, then a timely upcrossing of the martingale over these bounds is expected. The methods are developed for a setting when change sets in gradually in an incoming stream of data. Parameter selection for the bounds are based on simulations and methodological comparison is done with respect to existing advances. The algorithms presented here can be easily adapted to an online change-detection setting. Finally, we provide a detailed data analysis based on extensive measurements of several athletes and benchmark the fatigue detection results with the runners' individual feedback over the course of the data collection. Qualitative conclusions on the biomechanical profiles of the athletes can be made based on the shape of the martingale trajectories even in the absence of an upcrossing of the threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01566v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rupsa Basu, Katharina Proksch</dc:creator>
    </item>
    <item>
      <title>A Regression-Based Approach to the CO2 Airborne Fraction: Enhancing Statistical Precision and Tackling Zero Emissions</title>
      <link>https://arxiv.org/abs/2311.01053</link>
      <description>arXiv:2311.01053v4 Announce Type: replace-cross 
Abstract: The global fraction of anthropogenically emitted carbon dioxide (CO$_2$) that stays in the atmosphere, the CO$_2$ airborne fraction, has been fluctuating around a constant value over the period 1959 to 2022. The consensus estimate of the airborne fraction is around $44\%$; the remaining $56\%$ is absorbed by the oceanic and terrestrials biospheres. In this study, we show that the conventional estimator of the airborne fraction, based on a ratio of changes in atmospheric CO$_2$ concentrations and CO$_2$ emissions, suffers from a number of statistical deficiencies, such as non-existence of moments and a non-Gaussian limiting distribution. We propose an alternative regression-based estimator of the airborne fraction that does not suffer from these deficiencies. We show that the regression-based estimator has a Gaussian limiting distribution and reduces estimation uncertainty substantially. Our empirical analysis leads to an estimate of the airborne fraction over 1959--2022 of $47.0\%$ ($\pm 1.1\%$; $1 \sigma$), implying a higher, and better constrained, estimate than the current consensus. Using climate model output, we show that a regression-based approach provides sensible estimates of the airborne fraction, also in future scenarios where emissions are at or near zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01053v4</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman</dc:creator>
    </item>
    <item>
      <title>Inference on summaries of a model-agnostic longitudinal variable importance trajectory with application to suicide prevention</title>
      <link>https://arxiv.org/abs/2311.01638</link>
      <description>arXiv:2311.01638v2 Announce Type: replace-cross 
Abstract: Risk of suicide attempt varies over time. Understanding the importance of risk factors measured at a mental health visit can help clinicians evaluate future risk and provide appropriate care during the visit. In prediction settings where data are collected over time, such as in mental health care, it is often of interest to understand both the importance of variables for predicting the response at each time point and the importance summarized over the time series. Building on recent advances in estimation and inference for variable importance measures, we define summaries of variable importance trajectories and corresponding estimators. The same approaches for inference can be applied to these measures regardless of the choice of the algorithm(s) used to estimate the prediction function. We propose a nonparametric efficient estimation and inference procedure as well as a null hypothesis testing procedure that are valid even when complex machine learning tools are used for prediction. Through simulations, we demonstrate that our proposed procedures have good operating characteristics. We use these approaches to analyze electronic health records data from two large health systems to investigate the longitudinal importance of risk factors for suicide attempt to inform future suicide prevention research and clinical workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01638v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian D. Williamson, Erica E. M. Moodie, Gregory E. Simon, Rebecca C. Rossom, Susan M. Shortreed</dc:creator>
    </item>
    <item>
      <title>Gambling-Based Confidence Sequences for Bounded Random Vectors</title>
      <link>https://arxiv.org/abs/2402.03683</link>
      <description>arXiv:2402.03683v2 Announce Type: replace-cross 
Abstract: A confidence sequence (CS) is a sequence of confidence sets that contains a target parameter of an underlying stochastic process at any time step with high probability. This paper proposes a new approach to constructing CSs for means of bounded multivariate stochastic processes using a general gambling framework, extending the recently established coin toss framework for bounded random processes. The proposed gambling framework provides a general recipe for constructing CSs for categorical and probability-vector-valued observations, as well as for general bounded multidimensional observations through a simple reduction. This paper specifically explores the use of the mixture portfolio, akin to Cover's universal portfolio, in the proposed framework and investigates the properties of the resulting CSs. Simulations demonstrate the tightness of these confidence sequences compared to existing methods. When applied to the sampling without-replacement setting for finite categorical data, it is shown that the resulting CS based on a universal gambling strategy is provably tighter than that of the posterior-prior ratio martingale proposed by Waudby-Smith and Ramdas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03683v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Jon Ryu, Gregory W. Wornell</dc:creator>
    </item>
    <item>
      <title>Bayesian Learning of Relational Graph in Semiparametric High-dimensional Time Series</title>
      <link>https://arxiv.org/abs/2403.04915</link>
      <description>arXiv:2403.04915v3 Announce Type: replace-cross 
Abstract: Time series data arising in many applications nowadays are high-dimensional. A large number of parameters describe features of these time series. We propose a novel approach to modeling a high-dimensional time series through several independent univariate time series, which are then orthogonally rotated and sparsely linearly transformed. With this approach, any specified intrinsic relations among component time series given by a graphical structure can be maintained at all time snapshots. We call the resulting process an Orthogonally-rotated Univariate Time series (OUT). Key structural properties of time series such as stationarity and causality can be easily accommodated in the OUT model. For Bayesian inference, we put suitable prior distributions on the spectral densities of the independent latent times series, the orthogonal rotation matrix, and the common precision matrix of the component times series at every time point. A likelihood is constructed using the Whittle approximation for univariate latent time series. An efficient Markov Chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We study the convergence of the pseudo-posterior distribution based on the Whittle likelihood for the model's parameters upon developing a new general posterior convergence theorem for pseudo-posteriors. We find that the posterior contraction rate for independent observations essentially prevails in the OUT model under very mild conditions on the temporal dependence described in terms of the smoothness of the corresponding spectral densities. Through a simulation study, we compare the accuracy of estimating the parameters and identifying the graphical structure with other approaches. We apply the proposed methodology to analyze a dataset on different industrial components of the US gross domestic product between 2010 and 2019 and predict future observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04915v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>The Laplace asymptotic expansion in high dimensions</title>
      <link>https://arxiv.org/abs/2406.12706</link>
      <description>arXiv:2406.12706v2 Announce Type: replace-cross 
Abstract: We prove that the classical Laplace asymptotic expansion of $\int_{\mathbb R^d} g(x)e^{-nz(x)}dx$, $n\gg1$ extends to the high-dimensional regime in which $d$ may grow large with $n$. We formulate simple conditions on the growth of the derivatives of $g$ and $z$ near the minimizer of $z$ under which the terms of the expansion and the remainder are bounded as powers of $(\tau d/\sqrt n)^2$. The parameter $\tau$ controls the growth rates of the derivatives and can be potentially large, but we obtain a useful expansion whenever $\tau d/\sqrt n\ll 1$. Our result relies on a new and transparent proof of the Laplace expansion valid for any $d$. The proof leads to a new representation of the terms and remainder, which is crucial in obtaining tight control on the growth of these quantities with $d$. To demonstrate that our bounds are tight, we consider the case of a quartic $z$ and $g\equiv1$, showing that the $k$th term is precisely of the order $(d^2/n)^k$ for all $k$, and that our bounds on the terms are of the same order of magnitude. We also apply our results to derive a high-dimensional Laplace expansion for a particular function $z$ arising in the context of Bayesian inference, which is both random and depends on $n$. We show that with high probability, the terms of the expansion and the remainder are bounded in powers of $d^2/n$. In both of these examples, $\tau=1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12706v2</guid>
      <category>math.CA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anya Katsevich</dc:creator>
    </item>
    <item>
      <title>Censored and extreme losses: functional convergence and applications to tail goodness-of-fit</title>
      <link>https://arxiv.org/abs/2408.05862</link>
      <description>arXiv:2408.05862v2 Announce Type: replace-cross 
Abstract: This paper establishes the functional convergence of the Extreme Nelson--Aalen and Extreme Kaplan--Meier estimators, which are designed to capture the heavy-tailed behaviour of censored losses. The resulting limit representations can be used to obtain the distributions of pathwise functionals with respect to the so-called tail process. For instance, we may recover the convergence of a censored Hill estimator, and we further investigate two goodness-of-fit statistics for the tail of the loss distribution. Using the the latter limit theorems, we propose two rules for selecting a suitable number of order statistics, both based on test statistics derived from the functional convergence results. The effectiveness of these selection rules is investigated through simulations and an application to a real dataset comprised of French motor insurance claim sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05862v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christoffer {\O}hlenschl{\ae}ger</dc:creator>
    </item>
  </channel>
</rss>
