<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:01:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Bayesian Semiparametric Mixture Model for Clustering Zero-Inflated Microbiome Data</title>
      <link>https://arxiv.org/abs/2508.14184</link>
      <description>arXiv:2508.14184v1 Announce Type: cross 
Abstract: Microbiome research has immense potential for unlocking insights into human health and disease. A common goal in human microbiome research is identifying subgroups of individuals with similar microbial composition that may be linked to specific health states or environmental exposures. However, existing clustering methods are often not equipped to accommodate the complex structure of microbiome data and typically make limiting assumptions regarding the number of clusters in the data which can bias inference. Designed for zero-inflated multivariate compositional count data collected in microbiome research, we propose a novel Bayesian semiparametric mixture modeling framework that simultaneously learns the number of clusters in the data while performing cluster allocation. In simulation, we demonstrate the clustering performance of our method compared to distance- and model-based alternatives and the importance of accommodating zero-inflation when present in the data. We then apply the model to identify clusters in microbiome data collected in a study designed to investigate the relation between gut microbial composition and enteric diarrheal disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14184v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suppapat Korsurat, Matthew D. Koslovsky</dc:creator>
    </item>
    <item>
      <title>piCurve: an R package for modeling photosynthesis-irradiance curves</title>
      <link>https://arxiv.org/abs/2508.14321</link>
      <description>arXiv:2508.14321v1 Announce Type: cross 
Abstract: Photosynthesis-irradiance (PI) curves are foundational for quantifying primary production, parameterizing ecosystem and biogeochemical models, and interpreting physiological acclimation to light. Despite their broad use, researchers lack a unified, reproducible toolkit to fit, compare, and diagnose the many PI formulations that have accumulated over the last century. We introduce piCurve, an R package that standardizes the modeling of PI relationships, with a library of widely used light-limited, light-saturated, and photoinhibited formulations and a consistent statistical framework for estimation and comparison. With the total of 24 PI models, piCurve supports mean squared error (MSE) and maximum likelihood estimation (MLE), provides uncertainty quantification via information matrix (Hessian), and includes automated, data-informed initialization to improve convergence. Utilities classify PI data into light-limited, light-saturated, and photoinhibited regions, while plotting and 'tidy' helpers streamline workflow and reporting. Together, these features enable reproducible analyses and fair model comparisons, including for curves exhibiting a plateau followed by photoinhibition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14321v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad M. Amirian, Andrew J. Irwin</dc:creator>
    </item>
    <item>
      <title>Evaluation and Optimization of Leave-one-out Cross-validation for the Lasso</title>
      <link>https://arxiv.org/abs/2508.14368</link>
      <description>arXiv:2508.14368v1 Announce Type: cross 
Abstract: I develop an algorithm to produce the piecewise quadratic that computes leave-one-out cross-validation for the lasso as a function of its hyperparameter. The algorithm can be used to find exact hyperparameters that optimize leave-one-out cross-validation either globally or locally, and its practicality is demonstrated on real-world data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14368v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Burn</dc:creator>
    </item>
    <item>
      <title>Bridge Sampling Diagnostics</title>
      <link>https://arxiv.org/abs/2508.14487</link>
      <description>arXiv:2508.14487v1 Announce Type: cross 
Abstract: In Bayesian statistics, the marginal likelihood is used for model selection and averaging, yet it is often challenging to compute accurately for complex models. Approaches such as bridge sampling, while effective, may suffer from issues of high variability of the estimates. We present how to estimate Monte Carlo standard error (MCSE) for bridge sampling, and how to diagnose the reliability of MCSE estimates using Pareto-$\hat{k}$ and block reshuffling diagnostics without the need to repeatedly re-run full posterior inference. We demonstrate the behavior with increasingly more difficult simulated posteriors and many real posteriors from the posteriordb database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14487v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgio Micaletto, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>A Bayesian Survival Model for Time-Varying Coefficients and Unobserved Heterogeneity</title>
      <link>https://arxiv.org/abs/2206.11320</link>
      <description>arXiv:2206.11320v2 Announce Type: replace-cross 
Abstract: Dynamic survival models are a flexible tool for overcoming limitations of popular methods in the field of survival analysis. While this flexibility allows them to uncover more intricate relationships between covariates and the time-to-event, it also has them running the risk of overfitting. This paper proposes a solution to this issue based on state of the art global-local shrinkage priors and shows that they are able to effectively regularize the amount of time-variation observed in the parameters. Further, a novel approach to accounting for unobserved heterogeneity in the data through a dynamic factor model is introduced. An efficient MCMC sampler is developed and made available in an accompanying R package. Finally, the method is applied to a current data set of survival times of patients with adenocarcinoma of the gastroesophageal junction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11320v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus, Daniel Winkler, Gerd Jomrich</dc:creator>
    </item>
    <item>
      <title>Comparison of parallel SMC and MCMC for Bayesian deep learning</title>
      <link>https://arxiv.org/abs/2402.06173</link>
      <description>arXiv:2402.06173v3 Announce Type: replace-cross 
Abstract: This work systematically compares parallel implementations of consistent (asymptotically unbiased) Bayesian deep learning algorithms: sequential Monte Carlo sampler (SMC$_\parallel$) or Markov chain Monte Carlo (MCMC$_\parallel$). We provide a proof of convergence for SMC$_\parallel$ showing that it theoretically achieves the same level of convergence as a single monolithic SMC sampler, while the reduced communication lowers wall-clock time. It is well-known that the first samples from MCMC need to be discarded to eliminate initialization bias, and that the number of discarded samples must grow like the logarithm of the number of parallel chains to control that bias for MCMC$_\parallel$. A systematic empirical numerical study on MNIST, CIFAR, and IMDb, reveals that parallel implementations of both methods perform comparably to non-parallel implementations in terms of performance and total cost, and also comparably to each other. However, both methods still require a large wall-clock time, and suffer from catastrophic non-convergence if they aren't run for long enough.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06173v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinzhu Liang, Joseph M. Lukens, Sanjaya Lohani, Brian T. Kirby, Thomas A. Searles, Xin Qiu, Kody J. H. Law</dc:creator>
    </item>
    <item>
      <title>nonprobsvy -- An R package for modern methods for non-probability surveys</title>
      <link>https://arxiv.org/abs/2504.04255</link>
      <description>arXiv:2504.04255v2 Announce Type: replace-cross 
Abstract: The following paper presents nonprobsvy -- an R package for inference based on non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. In the package, we assume the existence of either population-level data or probability-based population information and leverage the survey package for inference. The package implements both analytical and bootstrap variance estimation for the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at scientists and researchers who would like to use non-probability samples (e.g.big data, opt-in web panels, social media) to accurately estimate population characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04255v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Chrostowski, Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Better bootstrap t confidence intervals for the mean</title>
      <link>https://arxiv.org/abs/2508.10083</link>
      <description>arXiv:2508.10083v2 Announce Type: replace-cross 
Abstract: This article explores combinations of weighted bootstraps, like the Bayesian bootstrap, with the bootstrap $t$ method for setting approximate confidence intervals for the mean of a random variable in small samples. For this problem the usual bootstrap $t$ has good coverage but provides intervals with long and highly variable lengths. Those intervals can have infinite length not just for tiny $n$, when the data have a discrete distribution. The BC$_a$ bootstrap produces shorter intervals but tends to severely under-cover the mean. Bootstrapping the studentized mean with weights from a Beta$(1/2,3/2)$ distribution is shown to attain second order accuracy. It never yields infinite length intervals and the mean square bootstrap $t$ statistic is finite when there are at least three distinct values in the data, or two distinct values appearing at least three times each. In a range of small sample settings, the beta bootstrap $t$ intervals have closer to nominal coverage than the BC$_a$ and shorter length than the multinomial bootstrap $t$. The paper includes a lengthy discussion of the difficulties in constructing a utility function to evaluate nonparametric approximate confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10083v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
  </channel>
</rss>
