<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 06:23:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Highly Multivariate High-dimensionality Spatial Stochastic Processes-A Mixed Conditional Approach</title>
      <link>https://arxiv.org/abs/2408.10396</link>
      <description>arXiv:2408.10396v1 Announce Type: new 
Abstract: We propose a hybrid mixed spatial graphical model framework and novel concepts, e.g., cross-Markov Random Field (cross-MRF), to comprehensively address all feature aspects of highly multivariate high-dimensionality (HMHD) spatial data class when constructing the desired joint variance and precision matrix (where both p and n are large). Specifically, the framework accommodates any customized conditional independence (CI) among any number of p variate fields at the first stage, alleviating dynamic memory burden. Meanwhile, it facilitates parallel generation of covariance and precision matrix, with the latter's generation order scaling only linearly in p. In the second stage, we demonstrate the multivariate Hammersley-Clifford theorem from a column-wise conditional perspective and unearth the existence of cross-MRF. The link of the mixed spatial graphical framework and the cross-MRF allows for a mixed conditional approach, resulting in the sparsest possible representation of the precision matrix via accommodating the doubly CI among both p and n, with the highest possible exact-zero-value percentage. We also explore the possibility of the co-existence of geostatistical and MRF modelling approaches in one unified framework, imparting a potential solution to an open problem. The derived theories are illustrated with 1D simulation and 2D real-world spatial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10396v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen, Gavin Shaddick</dc:creator>
    </item>
    <item>
      <title>Spatial Knockoff Bayesian Variable Selection in Genome-Wide Association Studies</title>
      <link>https://arxiv.org/abs/2408.10401</link>
      <description>arXiv:2408.10401v1 Announce Type: new 
Abstract: High-dimensional variable selection has emerged as one of the prevailing statistical challenges in the big data revolution. Many variable selection methods have been adapted for identifying single nucleotide polymorphisms (SNPs) linked to phenotypic variation in genome-wide association studies. We develop a Bayesian variable selection regression model for identifying SNPs linked to phenotypic variation. We modify our Bayesian variable selection regression models to control the false discovery rate of SNPs using a knockoff variable approach. We reduce spurious associations by regressing the phenotype of interest against a set of basis functions that account for the relatedness of individuals. Using a restricted regression approach, we simultaneously estimate the SNP-level effects while removing variation in the phenotype that can be explained by population structure. We also accommodate the spatial structure among causal SNPs by modeling their inclusion probabilities jointly with a reduced rank Gaussian process. In a simulation study, we demonstrate that our spatial Bayesian variable selection regression model controls the false discovery rate and increases power when the relevant SNPs are clustered. We conclude with an analysis of Arabidopsis thaliana flowering time, a polygenic trait that is confounded with population structure, and find the discoveries of our method cluster near described flowering time genes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10401v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin J. Van Ee, Diana Gamba, Jesse R. Lasky, Megan L. Vahsen, Mevin B. Hooten</dc:creator>
    </item>
    <item>
      <title>On a fundamental difference between Bayesian and frequentist approaches to robustness</title>
      <link>https://arxiv.org/abs/2408.10478</link>
      <description>arXiv:2408.10478v1 Announce Type: new 
Abstract: Heavy-tailed models are often used as a way to gain robustness against outliers in Bayesian analyses. On the other side, in frequentist analyses, M-estimators are often employed. In this paper, the two approaches are reconciled by considering M-estimators as maximum likelihood estimators of heavy-tailed models. We realize that, even from this perspective, there is a fundamental difference in that frequentists do not require these heavy-tailed models to be proper. It is shown what the difference between improper and proper heavy-tailed models can be in terms of estimation results through two real-data analyses based on linear regression. The findings of this paper make us ponder on the use of improper heavy-tailed data models in Bayesian analyses, an approach which is seen to fit within the generalized Bayesian framework of Bissiri et al. (2016) when combined with proper prior distributions yielding proper (generalized) posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10478v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Gagnon, Alain Desgagn\'e</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Covariate-Augmented Overdispersed Multi-Study Poisson Factor Model</title>
      <link>https://arxiv.org/abs/2408.10542</link>
      <description>arXiv:2408.10542v1 Announce Type: new 
Abstract: Factor analysis for high-dimensional data is a canonical problem in statistics and has a wide range of applications. However, there is currently no factor model tailored to effectively analyze high-dimensional count responses with corresponding covariates across multiple studies, such as the single-cell sequencing dataset from a case-control study. In this paper, we introduce factor models designed to jointly analyze multiple studies by extracting study-shared and specified factors. Our factor models account for heterogeneous noises and overdispersion among counts with augmented covariates. We propose an efficient and speedy variational estimation procedure for estimating model parameters, along with a novel criterion for selecting the optimal number of factors and the rank of regression coefficient matrix. The consistency and asymptotic normality of estimators are systematically investigated by connecting variational likelihood and profile M-estimation. Extensive simulations and an analysis of a single-cell sequencing dataset are conducted to demonstrate the effectiveness of the proposed multi-study Poisson factor model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10542v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wei Liu, Qingzhi Zhong</dc:creator>
    </item>
    <item>
      <title>Multi-Attribute Preferences: A Transfer Learning Approach</title>
      <link>https://arxiv.org/abs/2408.10558</link>
      <description>arXiv:2408.10558v1 Announce Type: new 
Abstract: This contribution introduces a novel statistical learning methodology based on the Bradley-Terry method for pairwise comparisons, where the novelty arises from the method's capacity to estimate the worth of objects for a primary attribute by incorporating data of secondary attributes. These attributes are properties on which objects are evaluated in a pairwise fashion by individuals. By assuming that the main interest of practitioners lies in the primary attribute, and the secondary attributes only serve to improve estimation of the parameters underlying the primary attribute, this paper utilises the well-known transfer learning framework. To wit, the proposed method first estimates a biased worth vector using data pertaining to both the primary attribute and the set of informative secondary attributes, which is followed by a debiasing step based on a penalised likelihood of the primary attribute. When the set of informative secondary attributes is unknown, we allow for their estimation by a data-driven algorithm. Theoretically, we show that, under mild conditions, the $\ell_\infty$ and $\ell_2$ rates are improved compared to fitting a Bradley-Terry model on just the data pertaining to the primary attribute. The favourable (comparative) performance under more general settings is shown by means of a simulation study. To illustrate the usage and interpretation of the method, an application of the proposed method is provided on consumer preference data pertaining to a cassava derived food product: eba. An R package containing the proposed methodology can be found on xhttps://CRAN.R-project.org/package=BTTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10558v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>A two-sample test based on averaged Wilcoxon rank sums over interpoint distances</title>
      <link>https://arxiv.org/abs/2408.10570</link>
      <description>arXiv:2408.10570v1 Announce Type: new 
Abstract: An important class of two-sample multivariate homogeneity tests is based on identifying differences between the distributions of interpoint distances. While generating distances from point clouds offers a straightforward and intuitive way for dimensionality reduction, it also introduces dependencies to the resulting distance samples. We propose a simple test based on Wilcoxon's rank sum statistic for which we prove asymptotic normality under the null hypothesis and fixed alternatives under mild conditions on the underlying distributions of the point clouds. Furthermore, we show consistency of the test and derive a variance approximation that allows to construct a computationally feasible, distribution-free test with good finite sample performance. The power and robustness of the test for high-dimensional data and low sample sizes is demonstrated by numerical simulations. Finally, we apply the proposed test to case-control testing on microarray data in genetic studies, which is considered a notorious case for a high number of variables and low sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10570v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annika Betken, Aljosa Marjanovic, Katharina Proksch</dc:creator>
    </item>
    <item>
      <title>Principal component analysis for max-stable distributions</title>
      <link>https://arxiv.org/abs/2408.10650</link>
      <description>arXiv:2408.10650v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is one of the most popular dimension reduction techniques in statistics and is especially powerful when a multivariate distribution is concentrated near a lower-dimensional subspace. Multivariate extreme value distributions have turned out to provide challenges for the application of PCA since their constraint support impedes the detection of lower-dimensional structures and heavy-tails can imply that second moments do not exist, thereby preventing the application of classical variance-based techniques for PCA. We adapt PCA to max-stable distributions using a regression setting and employ max-linear maps to project the random vector to a lower-dimensional space while preserving max-stability. We also provide a characterization of those distributions which allow for a perfect reconstruction from the lower-dimensional representation. Finally, we demonstrate how an optimal projection matrix can be consistently estimated and show viability in practice with a simulation study and application to a benchmark dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10650v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Reinbott, Anja Jan{\ss}en</dc:creator>
    </item>
    <item>
      <title>Neural Networks for Parameter Estimation in Geometrically Anisotropic Geostatistical Models</title>
      <link>https://arxiv.org/abs/2408.10915</link>
      <description>arXiv:2408.10915v1 Announce Type: new 
Abstract: This article presents a neural network approach for estimating the covariance function of spatial Gaussian random fields defined in a portion of the Euclidean plane. Our proposal builds upon recent contributions, expanding from the purely isotropic setting to encompass geometrically anisotropic correlation structures, i.e., random fields with correlation ranges that vary across different directions. We conduct experiments with both simulated and real data to assess the performance of the methodology and to provide guidelines to practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10915v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Villaz\'on, Alfredo Alegr\'ia, Xavier Emery</dc:creator>
    </item>
    <item>
      <title>DEEPEAST technique to enhance power in two-sample tests via the same-attraction function</title>
      <link>https://arxiv.org/abs/2408.11003</link>
      <description>arXiv:2408.11003v1 Announce Type: new 
Abstract: Data depth has emerged as an invaluable nonparametric measure for the ranking of multivariate samples. The main contribution of depth-based two-sample comparisons is the introduction of the Q statistic (Liu and Singh, 1993), a quality index. Unlike traditional methods, data depth does not require the assumption of normal distributions and adheres to four fundamental properties. Many existing two-sample homogeneity tests, which assess mean and/or scale changes in distributions often suffer from low statistical power or indeterminate asymptotic distributions. To overcome these challenges, we introduced a DEEPEAST (depth-explored same-attraction sample-to-sample central-outward ranking) technique for improving statistical power in two-sample tests via the same-attraction function. We proposed two novel and powerful depth-based test statistics: the sum test statistic and the product test statistic, which are rooted in Q statistics, share a "common attractor" and are applicable across all depth functions. We further proved the asymptotic distribution of these statistics for various depth functions. To assess the performance of power gain, we apply three depth functions: Mahalanobis depth (Liu and Singh, 1993), Spatial depth (Brown, 1958; Gower, 1974), and Projection depth (Liu, 1992). Through two-sample simulations, we have demonstrated that our sum and product statistics exhibit superior power performance, utilizing a strategic block permutation algorithm and compare favourably with popular methods in literature. Our tests are further validated through analysis on Raman spectral data, acquired from cellular and tissue samples, highlighting the effectiveness of the proposed tests highlighting the effective discrimination between health and cancerous samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11003v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiting Chen, Min Gao, Wei Lin, Andrew Jirasek, Kirsty Milligan, Xiaoping Shi</dc:creator>
    </item>
    <item>
      <title>Discriminant Analysis in stationary time series based on robust cepstral coefficients</title>
      <link>https://arxiv.org/abs/2408.11012</link>
      <description>arXiv:2408.11012v1 Announce Type: new 
Abstract: Time series analysis is crucial in fields like finance, economics, environmental science, and biomedical engineering, aiding in forecasting, pattern identification, and understanding underlying mechanisms. While traditional time-domain methods focus on trends and seasonality, they often miss periodicities better captured in the frequency domain. Analyzing time series in the frequency domain uncovers spectral properties, offering deeper insights into underlying processes, aiding in differentiating data-generating processes of various populations, and assisting in classification. Common approaches use smoothed estimators, such as the smoothed periodogram, to minimize bias by averaging spectra from individual replicates within a population. However, these methods struggle with spectral variability among replicates, and abrupt values can skew estimators, complicating discrimination and classification. There's a gap in the literature for methods that account for within-population spectral variability, separate white noise effects from autocorrelations, and employ robust estimators in the presence of outliers. This paper fills that gap by introducing a robust framework for classifying time series groups. The process involves transforming time series into the frequency domain using the Fourier Transform, computing the power spectrum, and using the inverse Fourier Transform to obtain the cepstrum. To enhance spectral estimates' robustness and consistency, we apply the multitaper periodogram and the M-periodogram. These features are then used in Linear Discriminant Analysis (LDA) to improve classification accuracy and interpretability, offering a powerful tool for precise temporal pattern distinction and resilience to data anomalies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11012v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan de Souza Matias, Valderio Anselmo Reisen</dc:creator>
    </item>
    <item>
      <title>Impossible temperatures are not as rare as you think</title>
      <link>https://arxiv.org/abs/2408.10251</link>
      <description>arXiv:2408.10251v1 Announce Type: cross 
Abstract: The last decade has seen numerous record-shattering heatwaves in all corners of the globe. In the aftermath of these devastating events, there is interest in identifying worst-case thresholds or upper bounds that quantify just how hot temperatures can become. Generalized Extreme Value theory provides a data-driven estimate of extreme thresholds; however, upper bounds may be exceeded by future events, which undermines attribution and planning for heatwave impacts. Here, we show how the occurrence and relative probability of observed events that exceed a priori upper bound estimates, so-called "impossible" temperatures, has changed over time. We find that many unprecedented events are actually within data-driven upper bounds, but only when using modern spatial statistical methods. Furthermore, there are clear connections between anthropogenic forcing and the "impossibility" of the most extreme temperatures. Robust understanding of heatwave thresholds provides critical information about future record-breaking events and how their extremity relates to historical measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10251v1</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Likun Zhang, Michael F. Wehner</dc:creator>
    </item>
    <item>
      <title>On the Approximability of Stationary Processes using the ARMA Model</title>
      <link>https://arxiv.org/abs/2408.10610</link>
      <description>arXiv:2408.10610v1 Announce Type: cross 
Abstract: We identify certain gaps in the literature on the approximability of stationary random variables using the Autoregressive Moving Average (ARMA) model. To quantify approximability, we propose that an ARMA model be viewed as an approximation of a stationary random variable. We map these stationary random variables to Hardy space functions, and formulate a new function approximation problem that corresponds to random variable approximation, and thus to ARMA. Based on this Hardy space formulation we identify a class of stationary processes where approximation guarantees are feasible. We also identify an idealized stationary random process for which we conjecture that a good ARMA approximation is not possible. Next, we provide a constructive proof that Pad\'e approximations do not always correspond to the best ARMA approximation. Finally, we note that the spectral methods adopted in this paper can be seen as a generalization of unit root methods for stationary processes even when an ARMA model is not defined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10610v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anand Ganesh, Babhrubahan Bose, Anand Rajagopalan</dc:creator>
    </item>
    <item>
      <title>Valid Inference After Causal Discovery</title>
      <link>https://arxiv.org/abs/2208.05949</link>
      <description>arXiv:2208.05949v3 Announce Type: replace 
Abstract: Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to "double dipping," invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while achieving more accurate causal discovery than data splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.05949v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paula Gradu, Tijana Zrnic, Yixin Wang, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Matrix Quantile Factor Model</title>
      <link>https://arxiv.org/abs/2208.08693</link>
      <description>arXiv:2208.08693v3 Announce Type: replace 
Abstract: This paper introduces a matrix quantile factor model for matrix-valued data with low-rank structure. We estimate the row and column factor spaces via minimizing the empirical check loss function with orthogonal rotation constraints. We show that the estimates converge at rate $(\min\{p_1p_2,p_2T,p_1T\})^{-1/2}$ in the average Frobenius norm, where $p_1$, $p_2$ and $T$ are the row dimensionality, column dimensionality and length of the matrix sequence, respectively. This rate is faster than that of the quantile estimates via ``flattening" the matrix model into a large vector model. To derive the central limit theorem, we introduce a novel augmented Lagrangian function, which is equivalent to the original constrained empirical check loss minimization problem. Via the equivalence, we prove that the Hessian matrix of the augmented Lagrangian function is locally positive definite, resulting in a locally convex penalized loss function around the true factors and their loadings. This easily leads to a feasible second-order expansion of the score function and readily established central limit theorems of the smoothed estimates of the loadings. We provide three consistent criteria to determine the pair of row and column factor numbers. Extensive simulation studies and an empirical study justify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.08693v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xin-Bing Kong, Yong-Xin Liu, Long Yu, Peng Zhao</dc:creator>
    </item>
    <item>
      <title>Improving Algorithms for Fantasy Basketball</title>
      <link>https://arxiv.org/abs/2307.02188</link>
      <description>arXiv:2307.02188v4 Announce Type: replace 
Abstract: Fantasy basketball has a rich underlying mathematical structure which makes optimal drafting strategy unclear. A central issue for category leagues is how to aggregate a player's statistics from all categories into a single number representing general value. It is shown that under a simplified model of fantasy basketball, a novel metric dubbed the "G-score" is appropriate for this purpose. The traditional metric used by analysts, "Z-score", is a special case of the G-score under the condition that future player performances are known exactly. The distinction between Z-score and G-score is particularly meaningful for head-to-head formats, because there is a large degree of uncertainty in player performance from one week to another. Simulated fantasy basketball seasons with head-to-head scoring provide evidence that G-scores do in fact outperform Z-scores in that context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02188v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Rosenof</dc:creator>
    </item>
    <item>
      <title>Maintaining the validity of inference from linear mixed models in stepped-wedge cluster randomized trials under misspecified random-effects structures</title>
      <link>https://arxiv.org/abs/2308.07248</link>
      <description>arXiv:2308.07248v4 Announce Type: replace 
Abstract: Linear mixed models are commonly used in analyzing stepped-wedge cluster randomized trials (SW-CRTs). A key consideration for analyzing a SW-CRT is accounting for the potentially complex correlation structure, which can be achieved by specifying a random effects structure. Common random effects structures for a SW-CRT include random intercept, random cluster-by-period, and discrete-time decay. Recently, more complex structures, such as the random intervention structure, have been proposed. In practice, specifying appropriate random effects can be challenging. Robust variance estimators (RVE) may be applied to linear mixed models to provide consistent estimators of standard errors of fixed effect parameters in the presence of random-effects misspecification. However, there has been no empirical investigation of RVE for SW-CRT. In this paper, we first review five RVEs (both standard and small-sample bias-corrected RVEs) that are available for linear mixed models. We then describe a comprehensive simulation study to examine the performance of these RVEs for SW-CRTs with a continuous outcome under different data generators. For each data generator, we investigate whether the use of a RVE with either the random intercept model or the random cluster-by-period model is sufficient to provide valid statistical inference for fixed effect parameters, when these working models are subject to misspecification. Our results indicate that the random intercept and random cluster-by-period models with RVEs performed similarly. The CR3 RVE estimator, coupled with the number of clusters minus two degrees of freedom correction, consistently gave the best coverage results, but could be slightly conservative when the number of clusters was below 16. We summarize the implications of our results for linear mixed model analysis of SW-CRTs in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07248v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongdong Ouyang, Monica Taljaard, Andrew B Forbes, Fan Li</dc:creator>
    </item>
    <item>
      <title>A New measure of income inequality</title>
      <link>https://arxiv.org/abs/2310.02273</link>
      <description>arXiv:2310.02273v2 Announce Type: replace 
Abstract: A new measure of income inequality that captures the heavy tail behavior of the income distribution is proposed. We discuss two different approaches to find the estimators of the proposed measure. We show that these estimators are consistent and have an asymptotically normal distribution. We also obtain a jackknife empirical likelihood (JEL) confidence interval of the income inequality measure. A Monte Carlo simulation study is conducted to evaluate the finite sample properties of the estimators and JEL-based confidence inerval. Finally, we use our measure to study the income inequality of three states in India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02273v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sudheesh K Kattumannil, Saparya Suresh</dc:creator>
    </item>
    <item>
      <title>Treatment bootstrapping: A new approach to quantify uncertainty of average treatment effect estimates</title>
      <link>https://arxiv.org/abs/2310.11683</link>
      <description>arXiv:2310.11683v5 Announce Type: replace 
Abstract: This paper proposes a new non-parametric bootstrap method to quantify the uncertainty of average treatment effect estimate for the treated from matching estimators. More specifically, it seeks to quantify the uncertainty associated with the average treatment effect estimate for the treated by bootstrapping the treatment group only and finding the counterpart control group by pair matching on estimated propensity score without replacement. We demonstrate the validity of this approach and compare it with existing bootstrap approaches through Monte Carlo simulation and analysis of a real world data set. The results indicate that the proposed approach constructs confidence intervals and standard errors that have 95 percent or above coverage rate and better precision compared with existing bootstrap approaches, while these measures also depend on percent treated in the sample data and the sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11683v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Li</dc:creator>
    </item>
    <item>
      <title>Response Style Characterization for Repeated Measures Using the Visual Analogue Scale</title>
      <link>https://arxiv.org/abs/2403.10136</link>
      <description>arXiv:2403.10136v2 Announce Type: replace 
Abstract: Self-report measures (e.g., Likert scales) are widely used to evaluate subjective health perceptions. Recently, the visual analog scale (VAS), a slider-based scale, has become popular owing to its ability to precisely and easily assess how people feel. These data can be influenced by the response style (RS), a user-dependent systematic tendency that occurs regardless of questionnaire instructions. Despite its importance, especially in between-individual analysis, little attention has been paid to handling the RS in the VAS (denoted as response profile (RP)), as it is mainly used for within-individual monitoring and is less affected by RP. However, VAS measurements often require repeated self-reports of the same questionnaire items, making it difficult to apply conventional methods on a Likert scale. In this study, we developed a novel RP characterization method for various types of repeatedly measured VAS data. This approach involves the modeling of RP as distributional parameters ${\theta}$ through a mixture of RS-like distributions, and addressing the issue of unbalanced data through bootstrap sampling for treating repeated measures. We assessed the effectiveness of the proposed method using simulated pseudo-data and an actual dataset from an empirical study. The assessment of parameter recovery showed that our method accurately estimated the RP parameter ${\theta}$, demonstrating its robustness. Moreover, applying our method to an actual VAS dataset revealed the presence of individual RP heterogeneity, even in repeated VAS measurements, similar to the findings of the Likert scale. Our proposed method enables RP heterogeneity-aware VAS data analysis, similar to Likert-scale data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10136v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3445410</arxiv:DOI>
      <dc:creator>Shunsuke Minusa, Tadayuki Matsumura, Kanako Esaki, Yang Shao, Chihiro Yoshimura, Hiroyuki Mizuno</dc:creator>
    </item>
    <item>
      <title>A Correlation-induced Finite Difference Estimator</title>
      <link>https://arxiv.org/abs/2405.05638</link>
      <description>arXiv:2405.05638v4 Announce Type: replace 
Abstract: Finite difference (FD) approximation is a classic approach to stochastic gradient estimation when only noisy function realizations are available. In this paper, we first provide a sample-driven method via the bootstrap technique to estimate the optimal perturbation, and then propose an efficient FD estimator based on correlated samples at the estimated optimal perturbation. Furthermore, theoretical analyses of both the perturbation estimator and the FD estimator reveal that, {\it surprisingly}, the correlation enables the proposed FD estimator to achieve a reduction in variance and, in some cases, a decrease in bias compared to the traditional optimal FD estimator. Numerical results confirm the efficiency of our estimators and align well with the theory presented, especially in scenarios with small sample sizes. Finally, we apply the estimator to solve derivative-free optimization (DFO) problems, and numerical studies show that DFO problems with 100 dimensions can be effectively solved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05638v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guo Liang, Guangwu Liu, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>NIRVAR: Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2407.13314</link>
      <description>arXiv:2407.13314v2 Announce Type: replace 
Abstract: High-dimensional panels of time series arise in many scientific disciplines such as neuroscience, finance, and macroeconomics. Often, co-movements within groups of the panel components occur. Extracting these groupings from the data provides a course-grained description of the complex system in question and can inform subsequent prediction tasks. We develop a novel methodology to model such a panel as a restricted vector autoregressive process, where the coefficient matrix is the weighted adjacency matrix of a stochastic block model. This network time series model, which we call the Network Informed Restricted Vector Autoregression (NIRVAR) model, yields a coefficient matrix that has a sparse block-diagonal structure. We propose an estimation procedure that embeds each panel component in a low-dimensional latent space and clusters the embedded points to recover the blocks of the coefficient matrix. Crucially, the method allows for network-based time series modelling when the underlying network is unobserved. We derive the bias, consistency and asymptotic normality of the NIRVAR estimator. Simulation studies suggest that the NIRVAR estimated embedded points are Gaussian distributed around the ground truth latent positions. On three applications to finance, macroeconomics, and transportation systems, NIRVAR outperforms competing models in terms of prediction and provides interpretable results regarding group recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13314v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Francesco Sanna Passino, Mihai Cucuringu, Alessandra Luati</dc:creator>
    </item>
    <item>
      <title>Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</title>
      <link>https://arxiv.org/abs/2305.00050</link>
      <description>arXiv:2305.00050v3 Announce Type: replace-cross 
Abstract: The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a "behavorial" study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date.
  That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00050v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emre K{\i}c{\i}man, Robert Ness, Amit Sharma, Chenhao Tan</dc:creator>
    </item>
  </channel>
</rss>
