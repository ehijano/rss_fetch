<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Turning to Online Forums for Legal Information: A Case Study of GDPR's Legitimate Interests</title>
      <link>https://arxiv.org/abs/2506.04260</link>
      <description>arXiv:2506.04260v1 Announce Type: new 
Abstract: Practitioners building online services and tools often turn to online forums such as Reddit, Law Stack Exchange, and Stack Overflow for legal guidance to ensure compliance with the GDPR. The legal information presented in these forums directly impact present-day industry practitioner's decisions. Online forums can serve as gateways that, depending on the accuracy and quality of the answers provided, may either support or undermine the protection of privacy and data protection fundamental rights. However, there is a need for deeper investigation into practitioners' decision-making processes and their understanding of legal compliance.
  Using GDPR's ``legitimate interests'' legal ground for processing personal data as a case study, we investigate how practitioners use online forums to identify common areas of confusion in applying legitimate interests in practice, and evaluate how legally sound online forum responses are. Our analysis found that applying the ``legitimate interests'' legal basis is complex for practitioners, with important implications for how the GDPR is implemented in practice. The legal analysis showed that crowdsourced legal information tends to be legally sound, though sometimes incomplete. We outline recommendations to improve the quality of online forums by ensuring that responses are more legally sound and comprehensive, enabling practitioners to apply legitimate interests effectively in practice and uphold the GDPR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04260v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Kyi, Cristiana Santos, Sushil Ammanaghatta Shivakumar, Franziska Roesner, Asia Biega</dc:creator>
    </item>
    <item>
      <title>Enduring Disparities in the Workplace: A Pilot Study in the AI Community</title>
      <link>https://arxiv.org/abs/2506.04305</link>
      <description>arXiv:2506.04305v1 Announce Type: new 
Abstract: In efforts toward achieving responsible artificial intelligence (AI), fostering a culture of workplace transparency, diversity, and inclusion can breed innovation, trust, and employee contentment. In AI and Machine Learning (ML), such environments correlate with higher standards of responsible development. Without transparency, disparities, microaggressions and misconduct will remain unaddressed, undermining the very structural inequities responsible AI aims to mitigate. While prior work investigates workplace transparency and disparities in broad domains (e.g. science and technology, law) for specific demographic subgroups, it lacks in-depth and intersectional conclusions and a focus on the AI/ML community. To address this, we conducted a pilot survey of 1260 AI/ML professionals both in industry and academia across different axes, probing aspects such as belonging, performance, workplace Diversity, Equity and Inclusion (DEI) initiatives, accessibility, performance and compensation, microaggressions, misconduct, growth, and well-being. Results indicate enduring disparities in workplace experiences for underrepresented and/or marginalized subgroups. In particular, we highlight that accessibility remains an important challenge for a positive work environment and that disabled employees have a worse workplace experience than their non-disabled colleagues. We further surface disparities for intersectional groups and discuss how the implementation of DEI initiatives may differ from their perceived impact on the workplace. This study is a first step towards increasing transparency and informing AI/ML practitioners and organizations with empirical results. We aim to foster equitable decision-making in the design and evaluation of organizational policies and provide data that may empower professionals to make more informed choices of prospective workplaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04305v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunusa Simipa Abdulsalam, Siobhan Mackenzie Hall, Ana Quintero-Ossa, William Agnew, Carla Muntean, Sarah Tan, Ashley Heady, Savannah Thais, Jessica Schrouff</dc:creator>
    </item>
    <item>
      <title>A Framework for Auditing Chatbots for Dialect-Based Quality-of-Service Harms</title>
      <link>https://arxiv.org/abs/2506.04419</link>
      <description>arXiv:2506.04419v1 Announce Type: new 
Abstract: Increasingly, individuals who engage in online activities are expected to interact with large language model (LLM)-based chatbots. Prior work has shown that LLMs can display dialect bias, which occurs when they produce harmful responses when prompted with text written in minoritized dialects. However, whether and how this bias propagates to systems built on top of LLMs, such as chatbots, is still unclear. We conduct a review of existing approaches for auditing LLMs for dialect bias and show that they cannot be straightforwardly adapted to audit LLM-based chatbots due to issues of substantive and ecological validity. To address this, we present a framework for auditing LLM-based chatbots for dialect bias by measuring the extent to which they produce quality-of-service harms, which occur when systems do not work equally well for different people. Our framework has three key characteristics that make it useful in practice. First, by leveraging dynamically generated instead of pre-existing text, our framework enables testing over any dialect, facilitates multi-turn conversations, and represents how users are likely to interact with chatbots in the real world. Second, by measuring quality-of-service harms, our framework aligns audit results with the real-world outcomes of chatbot use. Third, our framework requires only query access to an LLM-based chatbot, meaning that it can be leveraged equally effectively by internal auditors, external auditors, and even individual users in order to promote accountability. To demonstrate the efficacy of our framework, we conduct a case study audit of Amazon Rufus, a widely-used LLM-based chatbot in the customer service domain. Our results reveal that Rufus produces lower-quality responses to prompts written in minoritized English dialects, and that these quality-of-service harms are exacerbated by the presence of typos in prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04419v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Emma Harvey, Rene F. Kizilcec, Allison Koenecke</dc:creator>
    </item>
    <item>
      <title>Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by LLM-Based Systems</title>
      <link>https://arxiv.org/abs/2506.04482</link>
      <description>arXiv:2506.04482v1 Announce Type: new 
Abstract: The NLP research community has made publicly available numerous instruments for measuring representational harms caused by large language model (LLM)-based systems. These instruments have taken the form of datasets, metrics, tools, and more. In this paper, we examine the extent to which such instruments meet the needs of practitioners tasked with evaluating LLM-based systems. Via semi-structured interviews with 12 such practitioners, we find that practitioners are often unable to use publicly available instruments for measuring representational harms. We identify two types of challenges. In some cases, instruments are not useful because they do not meaningfully measure what practitioners seek to measure or are otherwise misaligned with practitioner needs. In other cases, instruments - even useful instruments - are not used by practitioners due to practical and institutional barriers impeding their uptake. Drawing on measurement theory and pragmatic measurement, we provide recommendations for addressing these challenges to better meet practitioner needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04482v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Harvey, Emily Sheng, Su Lin Blodgett, Alexandra Chouldechova, Jean Garcia-Gathright, Alexandra Olteanu, Hanna Wallach</dc:creator>
    </item>
    <item>
      <title>Skill-Driven Certification Pathways: Measuring Industry Training Impact on Graduate Employability</title>
      <link>https://arxiv.org/abs/2506.04588</link>
      <description>arXiv:2506.04588v1 Announce Type: new 
Abstract: Australia faces a critical technology skills shortage, requiring approximately $52,000$ new technology professionals annually by 2030, while confronting a widening gap between employer requirements and graduate capabilities. With only $1\%$ of technology graduates considered immediately work-ready, traditional educational pathways alone prove insufficient to meet industry demands. This research examines how industry certifications, such as Microsoft's AI-900 (Azure AI Fundamentals), can bridge this critical skills gap. We propose a novel, data-driven methodology that quantitatively measures skill alignment between educational offerings and job market requirements by analysing over 2.5 million job advertisements from Australia, the US, and the UK, mapping extracted skills to industry taxonomies using the Vectorised Skills Space Method. Our findings reveal that combining university degrees with targeted industry certifications significantly enhances employability for technology roles. The Bachelor of Computer Science with AI major combined with AI-900 certification achieved the highest absolute skill similarity score for Machine Learning Engineer positions. Surprisingly, the largest improvements when augmented with AI certifications are experiences by non-technical degrees--such as nursing nursing--with up to $9,296\%$ percentage improvements in alignment with Machine Learning Engineer roles. Our results challenge conventional assumptions about technology career pathways. They can provide actionable insights for educational institutions seeking evidence-based curriculum design, students requiring strategic certification guidance, and employers recognising potential in candidates from non-traditional backgrounds who have obtained relevant certifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04588v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anatoli Kovalev, Narelle Stefanac, Marian-Andrei Rizoiu</dc:creator>
    </item>
    <item>
      <title>Oversight Structures for Agentic AI in Public-Sector Organizations</title>
      <link>https://arxiv.org/abs/2506.04836</link>
      <description>arXiv:2506.04836v1 Announce Type: new 
Abstract: This paper finds that the introduction of agentic AI systems intensifies existing challenges to traditional public sector oversight mechanisms -- which rely on siloed compliance units and episodic approvals rather than continuous, integrated supervision. We identify five governance dimensions essential for responsible agent deployment: cross-departmental implementation, comprehensive evaluation, enhanced security protocols, operational visibility, and systematic auditing. We evaluate the capacity of existing oversight structures to meet these challenges, via a mixed-methods approach consisting of a literature review and interviews with civil servants in AI-related roles. We find that agent oversight poses intensified versions of three existing governance challenges: continuous oversight, deeper integration of governance and operational capabilities, and interdepartmental coordination. We propose approaches that both adapt institutional structures and design agent oversight compatible with public sector constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04836v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Schmitz, Jonathan Rystr{\o}m, Jan Batzner</dc:creator>
    </item>
    <item>
      <title>The Data Dilemma: Authors' Intentions and Recognition of Research Data in Educational Technology Research</title>
      <link>https://arxiv.org/abs/2506.04954</link>
      <description>arXiv:2506.04954v1 Announce Type: new 
Abstract: Educational Technology (EdTec) research is conducted by multiple disciplines, some of which annually meet at the DELFI conference. Due to the heterogeneity of involved researchers and communities, it is our goal to identify categories of research data overseen in the context of EdTec research. Therefore, we analyze the author's perspective provided via EasyChair where authors specified whether they had research data to share. We compared this information with an analysis of the submitted articles and the contained research data. We found that not all research data was recognized as such by the authors, especially software and qualitative data, indicating a prevailing lack of awareness, and other potential barriers. In addition, we analyze the 2024 DELFI proceedings to learn what kind of data was subject to research, and where it is published. This work has implications for training future generations of EdTec researchers. It further stresses the need for guidelines and recognition of research data publications (particularly software, and qualitative data).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04954v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandra Schulz, Natalie Kiesler</dc:creator>
    </item>
    <item>
      <title>Evaluating Prompt-Driven Chinese Large Language Models: The Influence of Persona Assignment on Stereotypes and Safeguards</title>
      <link>https://arxiv.org/abs/2506.04975</link>
      <description>arXiv:2506.04975v1 Announce Type: new 
Abstract: Recent research has highlighted that assigning specific personas to large language models (LLMs) can significantly increase harmful content generation. Yet, limited attention has been given to persona-driven toxicity in non-Western contexts, particularly in Chinese-based LLMs. In this paper, we perform a large-scale, systematic analysis of how persona assignment influences refusal behavior and response toxicity in Qwen, a widely-used Chinese language model. Utilizing fine-tuned BERT classifiers and regression analysis, our study reveals significant gender biases in refusal rates and demonstrates that certain negative personas can amplify toxicity toward Chinese social groups by up to 60-fold compared to the default model. To mitigate this toxicity, we propose an innovative multi-model feedback strategy, employing iterative interactions between Qwen and an external evaluator, which effectively reduces toxic outputs without costly model retraining. Our findings emphasize the necessity of culturally specific analyses for LLMs safety and offer a practical framework for evaluating and enhancing ethical alignment in LLM-generated content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04975v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geng Liu, Li Feng, Carlo Alberto Bono, Songbo Yang, Mengxiao Zhu, Francesco Pierri</dc:creator>
    </item>
    <item>
      <title>Early linguistic fingerprints of online users who engage with conspiracy communities</title>
      <link>https://arxiv.org/abs/2506.05086</link>
      <description>arXiv:2506.05086v1 Announce Type: new 
Abstract: Online social media platforms are often seen as catalysts for radicalization, as they provide spaces where extreme beliefs can take root and spread, sometimes leading to real-world consequences. Conspiracy theories represent a specific form of radicalization that is notoriously resistant to online moderation strategies. One explanation for this resilience is the presence of a "conspiratorial mindset", a cognitive framework that fundamentally shapes how conspiracy believers perceive reality. However, the role of this mindset in driving online user behavior remains poorly understood. In this study, we analyze the psycholinguistic patterns of Reddit users who become active in a prominent conspiracy community by examining their activity in mainstream communities, which allows us to isolate linguistic markers for the presence of a conspiratorial mindset. We find that conspiracy-engaged individuals exhibit distinct psycholinguistic fingerprints, setting them apart from the general user population. Crucially, this signal is already evident in their online activity prior to joining the conspiracy community, allowing us to predict their involvement years in advance. These findings suggest that individuals who adopt conspiracy beliefs do not radicalize through community involvement, but possess a pre-existing conspiratorial mindset, which predisposes them to seek out and join extreme communities. By challenging the view that online social media platforms actively radicalize users into conspiracy theory beliefs, our findings suggest that standard moderation strategies have limited impact on curbing radicalization, and highlight the need for more targeted, supportive interventions that encourage disengagement from extremist narratives. Ultimately, this work contributes to fostering safer online and offline environments for public discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05086v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Corso, Giuseppe Russo, Francesco Pierri, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>A Framework for Ethical Judgment of Smart City Applications</title>
      <link>https://arxiv.org/abs/2506.05172</link>
      <description>arXiv:2506.05172v1 Announce Type: new 
Abstract: As modern cities increasingly adopt a variety of sensors and Internet of Things (IoT) technologies to collect and analyze data about residents, environments, and public services, they are fostering greater interactions among smart city applications, residents, governments, and businesses. This trend makes it essential for regulators to focus on these interactions to manage smart city practices effectively and prevent unethical outcomes. To facilitate ethical analysis for smart city applications, this paper introduces a judgment framework that examines various scenarios where ethical issues may arise. Employing a multi-agent approach, the framework incorporates diverse social entities and applies logic-based ethical rules to identify potential violations. Through a rights-based analysis, we developed a set of 13 ethical principles and rules to guide ethical practices in smart cities. We utilized two specification languages, Prototype Verification System (PVS) and Alloy, to model our multi-agent system. Our analysis suggests that Alloy may be more efficient for formalizing smart cities and conducting ethical rule checks, particularly with the assistance of a human evaluator. Simulations of a real-world smart city application demonstrate that our ethical judgment framework effectively detects unethical outcomes and can be extended for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05172v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weichen Shi</dc:creator>
    </item>
    <item>
      <title>Intentionally Unintentional: GenAI Exceptionalism and the First Amendment</title>
      <link>https://arxiv.org/abs/2506.05211</link>
      <description>arXiv:2506.05211v1 Announce Type: new 
Abstract: This paper challenges the assumption that courts should grant First Amendment protections to outputs from large generative AI models, such as GPT-4 and Gemini. We argue that because these models lack intentionality, their outputs do not constitute speech as understood in the context of established legal precedent, so there can be no speech to protect. Furthermore, if the model outputs are not speech, users cannot claim a First Amendment speech right to receive the outputs. We also argue that extending First Amendment rights to AI models would not serve the fundamental purposes of free speech, such as promoting a marketplace of ideas, facilitating self-governance, or fostering self-expression. In fact, granting First Amendment protections to AI models would be detrimental to society because it would hinder the government's ability to regulate these powerful technologies effectively, potentially leading to the unchecked spread of misinformation and other harms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05211v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Atkinson, Jena D. Hwang, Jacob Morrison</dc:creator>
    </item>
    <item>
      <title>Unveiling Voices: A Co-Hashtag Analysis of TikTok Discourse on the 2023 Israel-Palestine Crisis</title>
      <link>https://arxiv.org/abs/2501.07182</link>
      <description>arXiv:2501.07182v2 Announce Type: cross 
Abstract: TikTok has gradually become one of the most pervasive social media platforms in our daily lives. While much can be said about the merits of platforms such as TikTok, there is a different kind of attention paid towards the political affect of social media today compared to its impact on other aspects of modern networked reality. I explored how users on TikTok discussed the crisis in Palestine that worsened in 2023. Using network analysis, I situate keywords representing the conflict and categorize them thematically based on a coding schema derived from politically and ideologically differentiable stances. I conclude that activism and propaganda are contending amongst themselves in the thriving space afforded by TikTok today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07182v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>cs.MM</category>
      <category>math.IT</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rozin Hasin</dc:creator>
    </item>
    <item>
      <title>JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning</title>
      <link>https://arxiv.org/abs/2504.17264</link>
      <description>arXiv:2504.17264v1 Announce Type: cross 
Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17264v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaolu Kang, Hongtian Cai, Xiangyang Ji, Jinzhe Li, Nanfei Gu</dc:creator>
    </item>
    <item>
      <title>Edge interventions can mitigate demographic and prestige disparities in the Computer Science coauthorship network</title>
      <link>https://arxiv.org/abs/2506.04435</link>
      <description>arXiv:2506.04435v1 Announce Type: cross 
Abstract: Social factors such as demographic traits and institutional prestige structure the creation and dissemination of ideas in academic publishing. One place these effects can be observed is in how central or peripheral a researcher is in the coauthorship network. Here we investigate inequities in network centrality in a hand-collected data set of 5,670 U.S.-based faculty employed in Ph.D.-granting Computer Science departments and their DBLP coauthorship connections. We introduce algorithms for combining name- and perception-based demographic labels by maximizing alignment with self-reported demographics from a survey of faculty from our census. We find that women and individuals with minoritized race identities are less central in the computer science coauthorship network, implying worse access to and ability to spread information. Centrality is also highly correlated with prestige, such that faculty in top-ranked departments are at the core and those in low-ranked departments are in the peripheries of the computer science coauthorship network. We show that these disparities can be mitigated using simulated edge interventions, interpreted as facilitated collaborations. Our intervention increases the centrality of target individuals, chosen independently of the network structure, by linking them with researchers from highly ranked institutions. When applied to scholars during their Ph.D., the intervention also improves the predicted rank of their placement institution in the academic job market. This work was guided by an ameliorative approach: uncovering social inequities in order to address them. By targeting scholars for intervention based on institutional prestige, we are able to improve their centrality in the coauthorship network that plays a key role in job placement and longer-term academic success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04435v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kate Barnes, Mia Ellis-Einhorn, Carolina Ch\'avez-Ruelas, Nayera Hasan, Mohammad Fanous, Blair D. Sullivan, Sorelle Friedler, Aaron Clauset</dc:creator>
    </item>
    <item>
      <title>What Drives Team Success? Large-Scale Evidence on the Role of the Team Player Effect</title>
      <link>https://arxiv.org/abs/2506.04475</link>
      <description>arXiv:2506.04475v1 Announce Type: cross 
Abstract: Effective teamwork is essential in structured, performance-driven environments, from professional organizations to high-stakes competitive settings. As tasks grow more complex, achieving high performance requires not only technical proficiency but also strong interpersonal skills that allow individuals to coordinate effectively within teams. While prior research has identified social skills and familiarity as key drivers of team success, their joint effects -- particularly in temporary teams -- remain underexplored due to data and methodological constraints. To address this gap, we analyze a large-scale panel dataset from the real-time strategy game Age of Empires II, where players are assigned quasi-randomly to temporary teams and must coordinate under dynamic, high-pressure conditions. We isolate individual contributions by comparing observed match outcomes with predictions based on task proficiency. Our findings confirm a robust 'team player effect': certain individuals consistently improve team outcomes beyond what their technical skills predict. This effect is significantly amplified by team familiarity -- teams with prior shared experience benefit more from the presence of such individuals. Moreover, the effect grows with team size, suggesting that social skills become increasingly valuable as coordination demands rise. Our results demonstrate that social skills and familiarity interact in a complementary, rather than additive, way. These findings contribute to the literature on team performance by documenting the strength and structure of the team player effect in a quasi-randomized, high-stakes setting, with implications for teamwork in organizations and labor markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04475v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nico Elbert, Alicia von Schenk, Fabian Kosse, Victor Klockmann, Nikolai Stein, Christoph Flath</dc:creator>
    </item>
    <item>
      <title>User Altruism in Recommendation Systems</title>
      <link>https://arxiv.org/abs/2506.04525</link>
      <description>arXiv:2506.04525v1 Announce Type: cross 
Abstract: Users of social media platforms based on recommendation systems (RecSys) (e.g. TikTok, X, YouTube) strategically interact with platform content to influence future recommendations. On some such platforms, users have been documented to form large-scale grassroots movements encouraging others to purposefully interact with algorithmically suppressed content in order to "boost" its recommendation; we term this behavior user altruism. To capture this behavior, we study a game between users and a RecSys, where users provide the RecSys (potentially manipulated) preferences over the contents available to them, and the RecSys -- limited by data and computation constraints -- creates a low-rank approximation preference matrix, and ultimately provides each user her (approximately) most-preferred item. We compare the users' social welfare under truthful preference reporting and under a class of strategies capturing user altruism. In our theoretical analysis, we provide sufficient conditions to ensure strict increases in user social welfare under user altruism, and provide an algorithm to find an effective altruistic strategy. Interestingly, we show that for commonly assumed recommender utility functions, effectively altruistic strategies also improve the utility of the RecSys! We show that our results are robust to several model misspecifications, thus strengthening our conclusions. Our theoretical analysis is complemented by empirical results of effective altruistic strategies on the GoodReads dataset, and an online survey on how real-world users behave altruistically in RecSys. Overall, our findings serve as a proof-of-concept of the reasons why traditional RecSys may incentivize users to form collectives and/or follow altruistic strategies when interacting with them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04525v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ekaterina Fedorova, Madeline Kitch, Chara Podimata</dc:creator>
    </item>
    <item>
      <title>Judicial Permission</title>
      <link>https://arxiv.org/abs/2506.04610</link>
      <description>arXiv:2506.04610v1 Announce Type: cross 
Abstract: This paper examines the significance of weak permissions in criminal trials (\emph{judicial permission}). It introduces a dialogue game model to systematically address judicial permissions, considering different standards of proof and argumentation semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04610v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guido Governatori, Antonino Rotolo</dc:creator>
    </item>
    <item>
      <title>Urania: Differentially Private Insights into AI Use</title>
      <link>https://arxiv.org/abs/2506.04681</link>
      <description>arXiv:2506.04681v1 Announce Type: cross 
Abstract: We introduce $Urania$, a novel framework for generating insights about LLM chatbot interactions with rigorous differential privacy (DP) guarantees. The framework employs a private clustering mechanism and innovative keyword extraction methods, including frequency-based, TF-IDF-based, and LLM-guided approaches. By leveraging DP tools such as clustering, partition selection, and histogram-based summarization, $Urania$ provides end-to-end privacy protection. Our evaluation assesses lexical and semantic content preservation, pair similarity, and LLM-based metrics, benchmarking against a non-private Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple empirical privacy evaluation that demonstrates the enhanced robustness of our DP pipeline. The results show the framework's ability to extract meaningful conversational insights while maintaining stringent user privacy, effectively balancing data utility with privacy preservation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04681v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daogao Liu, Edith Cohen, Badih Ghazi, Peter Kairouz, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Adam Sealfon, Da Yu, Chiyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Beyond the Desktop: XR-Driven Segmentation with Meta Quest 3 and MX Ink</title>
      <link>https://arxiv.org/abs/2506.04858</link>
      <description>arXiv:2506.04858v1 Announce Type: cross 
Abstract: Medical imaging segmentation is essential in clinical settings for diagnosing diseases, planning surgeries, and other procedures. However, manual annotation is a cumbersome and effortful task. To mitigate these aspects, this study implements and evaluates the usability and clinical applicability of an extended reality (XR)-based segmentation tool for anatomical CT scans, using the Meta Quest 3 headset and Logitech MX Ink stylus. We develop an immersive interface enabling real-time interaction with 2D and 3D medical imaging data in a customizable workspace designed to mitigate workflow fragmentation and cognitive demands inherent to conventional manual segmentation tools. The platform combines stylus-driven annotation, mirroring traditional pen-on-paper workflows, with instant 3D volumetric rendering. A user study with a public craniofacial CT dataset demonstrated the tool's foundational viability, achieving a System Usability Scale (SUS) score of 66, within the expected range for medical applications. Participants highlighted the system's intuitive controls (scoring 4.1/5 for self-descriptiveness on ISONORM metrics) and spatial interaction design, with qualitative feedback highlighting strengths in hybrid 2D/3D navigation and realistic stylus ergonomics. While users identified opportunities to enhance task-specific precision and error management, the platform's core workflow enabled dynamic slice adjustment, reducing cognitive load compared to desktop tools. Results position the XR-stylus paradigm as a promising foundation for immersive segmentation tools, with iterative refinements targeting haptic feedback calibration and workflow personalization to advance adoption in preoperative planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04858v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lisle Faray de Paiva, Gijs Luijten, Ana Sofia Ferreira Santos, Moon Kim, Behrus Puladi, Jens Kleesiek, Jan Egger</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap</title>
      <link>https://arxiv.org/abs/2506.05030</link>
      <description>arXiv:2506.05030v1 Announce Type: cross 
Abstract: Artificial intelligence promises to revolutionise medicine, yet its impact remains limited because of the pervasive translational gap. We posit that the prevailing technology-centric approaches underpin this challenge, rendering such systems fundamentally incompatible with clinical practice, specifically diagnostic reasoning and decision making. Instead, we propose a novel sociotechnical conceptualisation of data-driven support tools designed to complement doctors' cognitive and epistemic activities. Crucially, it prioritises real-world impact over superhuman performance on inconsequential benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05030v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41746-025-01725-9</arxiv:DOI>
      <dc:creator>Kacper Sokol, James Fackler, Julia E Vogt</dc:creator>
    </item>
    <item>
      <title>ChatWise: A Strategy-Guided Chatbot for Enhancing Cognitive Support in Older Adults</title>
      <link>https://arxiv.org/abs/2503.05740</link>
      <description>arXiv:2503.05740v2 Announce Type: replace 
Abstract: Cognitive health in older adults presents a growing challenge. Although conversational interventions show feasibility in improving cognitive wellness, human caregiver resources remain overloaded. AI-based chatbots have shown promise, yet existing work is often limited to implicit strategies or heavily depends on training and label resources. In response, we propose a strategy-guided AI chatbot named ChatWise that follows a dual-level conversation reasoning framework. It integrates macro-level strategy planning and micro-level utterance generation to enable engaging, multi-turn dialogue tailored to older adults. Empirical results show that ChatWise closely aligns with professional human caregiver behaviors in offline evaluation using real clinic data, and achieves positive user cognitive and emotional responses in interactive simulations with digital twins, which significantly outperforms AI baselines that follow implicit conversation generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05740v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengbang Yang, Junyuan Hong, Yijiang Pang, Jiayu Zhou, Zhuangdi Zhu</dc:creator>
    </item>
    <item>
      <title>Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)</title>
      <link>https://arxiv.org/abs/2505.21091</link>
      <description>arXiv:2505.21091v2 Announce Type: replace 
Abstract: System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21091v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715275.3732038</arxiv:DOI>
      <dc:creator>Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh</dc:creator>
    </item>
    <item>
      <title>Construction of Urban Greenland Resources Collaborative Management Platform</title>
      <link>https://arxiv.org/abs/2506.03830</link>
      <description>arXiv:2506.03830v2 Announce Type: replace 
Abstract: Nowadays, environmental protection has become a global consensus. At the same time, with the rapid development of science and technology, urbanisation has become a phenomenon that has become the norm. Therefore, the urban greening management system is an essential component in protecting the urban environment. The system utilises a transparent management process known as" monitoring - early warning - response - optimisation," which enhances the tracking of greening resources, streamlines maintenance scheduling, and encourages employee involvement in planning. Designed with a microservice architecture, the system can improve the utilisation of greening resources by 30%, increase citizen satisfaction by 20%, and support carbon neutrality objectives, ultimately making urban governance more intelligent and focused on the community. The Happy City Greening Management System effectively manages gardeners, trees, flowers, and green spaces. It comprises modules for gardener management, purchase and supplier management, tree and flower management, and maintenance planning. Its automation feature allows for real-time updates of greening data, thereby enhancing decision-making. The system is built using Java for the backend and MySQL for data storage, complemented by a user-friendly frontend designed with the Vue framework. Additionally, it leverages features from the Spring Boot framework to enhance maintainability and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03830v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyang Lyu, Xiaoqi Li, Zongwei Li</dc:creator>
    </item>
    <item>
      <title>From User Surveys to Telemetry-Driven AI Agents: Exploring the Potential of Personalized Productivity Solutions</title>
      <link>https://arxiv.org/abs/2401.08960</link>
      <description>arXiv:2401.08960v2 Announce Type: replace-cross 
Abstract: Information workers increasingly struggle with productivity challenges in modern workplaces, facing difficulties in managing time and effectively utilizing workplace analytics data for behavioral improvement. Despite the availability of productivity metrics through enterprise tools, workers often fail to translate this data into actionable insights. We present a comprehensive, user-centric approach to address these challenges through AI-based productivity agents tailored to users' needs. Utilizing a two-phase method, we first conducted a survey with 363 participants, exploring various aspects of productivity, communication style, agent approach, personality traits, personalization, and privacy. Drawing on the survey insights, we developed a GPT-4 powered personalized productivity agent that utilizes telemetry data gathered via Viva Insights from information workers to provide tailored assistance. We compared its performance with alternative productivity-assistive tools, such as dashboard and narrative, in a study involving 40 participants. Our findings highlight the importance of user-centric design, adaptability, and the balance between personalization and privacy in AI-assisted productivity tools. By building on these insights, our work provides important guidance for developing more effective productivity solutions, ultimately leading to optimized efficiency and user experiences for information workers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08960v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Subigya Nepal, Javier Hernandez, Talie Massachi, Kael Rowan, Judith Amores, Jina Suh, Gonzalo Ramos, Brian Houck, Shamsi T. Iqbal, Mary Czerwinski</dc:creator>
    </item>
    <item>
      <title>Not All Options Are Created Equal: Textual Option Weighting for Token-Efficient LLM-Based Knowledge Tracing</title>
      <link>https://arxiv.org/abs/2410.12872</link>
      <description>arXiv:2410.12872v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently emerged as promising tools for knowledge tracing (KT) due to their strong reasoning and generalization abilities. While recent LLM-based KT methods have proposed new prompt formats, they struggle to represent the full interaction histories of example learners within a single prompt during in-context learning (ICL), resulting in limited scalability and high computational cost under token constraints. In this work, we present \textit{LLM-based Option-weighted Knowledge Tracing (LOKT)}, a simple yet effective framework that encodes the interaction histories of example learners in context as \textit{textual categorical option weights (TCOW)}. TCOW are semantic labels (e.g., ``inadequate'') assigned to the options selected by learners when answering questions, enhancing the interpretability of LLMs. Experiments on multiple-choice datasets show that LOKT outperforms existing non-LLM and LLM-based KT models in both cold-start and warm-start settings. Moreover, LOKT enables scalable and cost-efficient inference, achieving strong performance even under strict token constraints. Our code is available at \href{https://anonymous.4open.science/r/LOKT_model-3233}{https://anonymous.4open.science/r/LOKT\_model-3233}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12872v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>JongWoo Kim, SeongYeub Chu, Bryan Wong, Mun Yi</dc:creator>
    </item>
    <item>
      <title>LLM Social Simulations Are a Promising Research Method</title>
      <link>https://arxiv.org/abs/2504.02234</link>
      <description>arXiv:2504.02234v2 Announce Type: replace-cross 
Abstract: Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02234v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein</dc:creator>
    </item>
    <item>
      <title>DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis</title>
      <link>https://arxiv.org/abs/2505.14971</link>
      <description>arXiv:2505.14971v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have revolutionized natural language processing (NLP) and expanded their applications across diverse domains. However, despite their impressive capabilities, LLMs have been shown to reflect and perpetuate harmful societal biases, including those based on ethnicity, gender, and religion. A critical and underexplored issue is the reinforcement of caste-based biases, particularly towards India's marginalized caste groups such as Dalits and Shudras. In this paper, we address this gap by proposing DECASTE, a novel, multi-dimensional framework designed to detect and assess both implicit and explicit caste biases in LLMs. Our approach evaluates caste fairness across four dimensions: socio-cultural, economic, educational, and political, using a range of customized prompting strategies. By benchmarking several state-of-the-art LLMs, we reveal that these models systematically reinforce caste biases, with significant disparities observed in the treatment of oppressed versus dominant caste groups. For example, bias scores are notably elevated when comparing Dalits and Shudras with dominant caste groups, reflecting societal prejudices that persist in model outputs. These results expose the subtle yet pervasive caste biases in LLMs and emphasize the need for more comprehensive and inclusive bias evaluation methodologies that assess the potential risks of deploying such models in real-world contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14971v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Prashanth Vijayaraghavan, Soroush Vosoughi, Lamogha Chiazor, Raya Horesh, Rogerio Abreu de Paula, Ehsan Degan, Vandana Mukherjee</dc:creator>
    </item>
    <item>
      <title>Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race</title>
      <link>https://arxiv.org/abs/2506.00253</link>
      <description>arXiv:2506.00253v2 Announce Type: replace-cross 
Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00253v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihao Sun, Chengzhi Mao, Valentin Hofmann, Xuechunzi Bai</dc:creator>
    </item>
    <item>
      <title>Catching Stray Balls: Football, fandom, and the impact on digital discourse</title>
      <link>https://arxiv.org/abs/2506.01642</link>
      <description>arXiv:2506.01642v2 Announce Type: replace-cross 
Abstract: This paper examines how emotional responses to football matches influence online discourse across digital spaces on Reddit. By analysing millions of posts from dozens of subreddits, it demonstrates that real-world events trigger sentiment shifts that move across communities. It shows that negative sentiment correlates with problematic language; match outcomes directly influence sentiment and posting habits; sentiment can transfer to unrelated communities; and offers insights into the content of this shifting discourse. These findings reveal how digital spaces function not as isolated environments, but as interconnected emotional ecosystems vulnerable to cross-domain contagion triggered by real-world events, contributing to our understanding of the propagation of online toxicity. While football is used as a case-study to computationally measure affective causes and movements, these patterns have implications for understanding online communities broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01642v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Hill</dc:creator>
    </item>
  </channel>
</rss>
