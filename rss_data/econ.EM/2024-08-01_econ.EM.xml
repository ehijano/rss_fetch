<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 01:44:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v1 Announce Type: new 
Abstract: When do linear regressions estimate causal effects in quasi-experiments? This paper provides a generic diagnostic that assesses whether a given linear regression specification on a given dataset admits a design-based interpretation. To do so, we define a notion of potential weights, which encode counterfactual decisions a given regression makes to unobserved potential outcomes. If the specification does admit such an interpretation, this diagnostic can find a vector of unit-level treatment assignment probabilities -- which we call an implicit design -- under which the regression estimates a causal effect. This diagnostic also finds the implicit causal effect estimand. Knowing the implicit design and estimand adds transparency, leads to further sanity checks, and opens the door to design-based statistical inference. When applied to regression specifications studied in the causal inference literature, our framework recovers and extends existing theoretical results. When applied to widely-used specifications not covered by existing causal inference literature, our framework generates new theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning in High-frequency Market Making</title>
      <link>https://arxiv.org/abs/2407.21025</link>
      <description>arXiv:2407.21025v1 Announce Type: cross 
Abstract: This paper establishes a new and comprehensive theoretical analysis for the application of reinforcement learning (RL) in high-frequency market making. We bridge the modern RL theory and the continuous-time statistical models in high-frequency financial economics. Different with most existing literature on methodological research about developing various RL methods for market making problem, our work is a pilot to provide the theoretical analysis. We target the effects of sampling frequency, and find an interesting tradeoff between error and complexity of RL algorithm when tweaking the values of the time increment $\Delta$ $-$ as $\Delta$ becomes smaller, the error will be smaller but the complexity will be larger. We also study the two-player case under the general-sum game framework and establish the convergence of Nash equilibrium to the continuous-time game equilibrium as $\Delta\rightarrow0$. The Nash Q-learning algorithm, which is an online multi-agent RL method, is applied to solve the equilibrium. Our theories are not only useful for practitioners to choose the sampling frequency, but also very general and applicable to other high-frequency financial decision making problems, e.g., optimal executions, as long as the time-discretization of a continuous-time markov decision process is adopted. Monte Carlo simulation evidence support all of our theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21025v1</guid>
      <category>q-fin.TR</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Zheng, Zihan Ding</dc:creator>
    </item>
    <item>
      <title>Adaptive, Rate-Optimal Hypothesis Testing in Nonparametric IV Models</title>
      <link>https://arxiv.org/abs/2006.09587</link>
      <description>arXiv:2006.09587v5 Announce Type: replace 
Abstract: We propose a new adaptive hypothesis test for inequality (e.g., monotonicity, convexity) and equality (e.g., parametric, semiparametric) restrictions on a structural function in a nonparametric instrumental variables (NPIV) model. Our test statistic is based on a modified leave-one-out sample analog of a quadratic distance between the restricted and unrestricted sieve two-stage least squares estimators. We provide computationally simple, data-driven choices of sieve tuning parameters and Bonferroni adjusted chi-squared critical values. Our test adapts to the unknown smoothness of alternative functions in the presence of unknown degree of endogeneity and unknown strength of the instruments. It attains the adaptive minimax rate of testing in $L^2$. That is, the sum of the supremum of type I error over the composite null and the supremum of type II error over nonparametric alternative models cannot be minimized by any other tests for NPIV models of unknown regularities. Confidence sets in $L^2$ are obtained by inverting the adaptive test. Simulations confirm that, across different strength of instruments and sample sizes, our adaptive test controls size and its finite-sample power greatly exceeds existing non-adaptive tests for monotonicity and parametric restrictions in NPIV models. Empirical applications to test for shape restrictions of differentiated products demand and of Engel curves are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.09587v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Xiaohong Chen</dc:creator>
    </item>
    <item>
      <title>Testing for Peer Effects without Specifying the Network Structure</title>
      <link>https://arxiv.org/abs/2306.09806</link>
      <description>arXiv:2306.09806v3 Announce Type: replace 
Abstract: This paper proposes an Anderson-Rubin (AR) test for the presence of peer effects in panel data without the need to specify the network structure. The unrestricted model of our test is a linear panel data model of social interactions with dyad-specific peer effect coefficients for all potential peers. The proposed AR test evaluates if these peer effect coefficients are all zero. As the number of peer effect coefficients increases with the sample size, so does the number of instrumental variables (IVs) employed to test the restrictions under the null, rendering Bekker's many-IV environment. By extending existing many-IV asymptotic results to panel data, we establish the asymptotic validity of the proposed AR test. Our Monte Carlo simulations show the robustness and superior performance of the proposed test compared to some existing tests with misspecified networks. We provide two applications to demonstrate its empirical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09806v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyunseok Jung, Xiaodong Liu</dc:creator>
    </item>
    <item>
      <title>Merger Analysis with Latent Price</title>
      <link>https://arxiv.org/abs/2404.07684</link>
      <description>arXiv:2404.07684v3 Announce Type: replace 
Abstract: Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that data on merging firms' revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, compensating marginal cost reductions, and first-order welfare effects. Revenue diversion ratios can be identified from consumer expenditure data via the Hotz-Miller inversion under the standard discrete-continuous demand assumption. Merger simulations are also feasible with CES demand if data on all firms' margins and revenues are available. I use the proposed framework to evaluate the Albertsons/Safeway merger (2015) and the Staples/Office Depot merger (2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07684v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul S. Koh</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Causal Effects in High-Frequency Event Studies</title>
      <link>https://arxiv.org/abs/2406.15667</link>
      <description>arXiv:2406.15667v2 Announce Type: replace 
Abstract: We provide precise conditions for nonparametric identification of causal effects by high-frequency event study regressions, which have been used widely in the recent macroeconomics, financial economics and political economy literatures. The high-frequency event study method regresses changes in an outcome variable on a measure of unexpected changes in a policy variable in a narrow time window around an event or a policy announcement (e.g., a 30-minute window around an FOMC announcement). We show that, contrary to popular belief, the narrow size of the window is not sufficient for identification. Rather, the population regression coefficient identifies a causal estimand when (i) the effect of the policy shock on the outcome does not depend on the other shocks (separability) and (ii) the surprise component of the news or event dominates all other shocks that are present in the event window (relative exogeneity). Technically, the latter condition requires the policy shock to have infinite variance in the event window. Under these conditions, we establish the causal meaning of the event study estimand corresponding to the regression coefficient and the consistency and asymptotic normality of the event study estimator. Notably, this standard linear regression estimator is robust to general forms of nonlinearity. We apply our results to Nakamura and Steinsson's (2018a) analysis of the real economic effects of monetary policy, providing a simple empirical procedure to analyze the extent to which the standard event study estimator adequately estimates causal effects of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15667v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casini, Adam McCloskey</dc:creator>
    </item>
  </channel>
</rss>
