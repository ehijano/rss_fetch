<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Jul 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Switch-Less Dragonfly on Wafers: A Scalable Interconnection Architecture based on Wafer-Scale Integration</title>
      <link>https://arxiv.org/abs/2407.10290</link>
      <description>arXiv:2407.10290v1 Announce Type: new 
Abstract: Existing high-performance computing (HPC) interconnection architectures are based on high-radix switches, which limits the injection/local performance and introduces latency/energy/cost overhead. The new wafer-scale packaging and high-speed wireline technologies provide high-density, low-latency, and high-bandwidth connectivity, thus promising to support direct-connected high-radix interconnection architecture.
  In this paper, we propose a wafer-based interconnection architecture called Switch-Less-Dragonfly-on-Wafers. By utilizing distributed high-bandwidth networks-on-chip-on-wafer, costly high-radix switches of the Dragonfly topology are eliminated while increasing the injection/local throughput and maintaining the global throughput. Based on the proposed architecture, we also introduce baseline and improved deadlock-free minimal/non-minimal routing algorithms with only one additional virtual channel. Extensive evaluations show that the Switch-Less-Dragonfly-on-Wafers outperforms the traditional switch-based Dragonfly in both cost and performance. Similar approaches can be applied to other switch-based direct topologies, thus promising to power future large-scale supercomputers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10290v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiao Feng, Kaisheng Ma</dc:creator>
    </item>
    <item>
      <title>Effective Design Verification -- Constrained Random with Python and Cocotb</title>
      <link>https://arxiv.org/abs/2407.10312</link>
      <description>arXiv:2407.10312v1 Announce Type: new 
Abstract: Being the most widely used language across the world due to its simplicity and with 35 keywords (v3.7), Python attracts both hardware and software engineers. Python-based verification environment leverages open-source libraries such as cocotb and cocotb-coverage that enables interfacing the tesbenches with any available simulator and facilitating constrained randomization, coverage respectively. These libraries significantly ease the development of testbenches and have the potential to reduce the setup cost. The goal of this paper is to assess the effectiveness of a Python-Cocotb verification setup with design IPs and compare its features and performance metrics with the current de-facto hardware verification language i.e., SystemVerilog.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10312v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deepak Narayan Gadde, Suruchi Kumari, Aman Kumar</dc:creator>
    </item>
    <item>
      <title>Towards Efficient Design Verification -- Constrained Random Verification using PyUVM</title>
      <link>https://arxiv.org/abs/2407.10317</link>
      <description>arXiv:2407.10317v1 Announce Type: new 
Abstract: Python, as a multi-paradigm language known for its ease of integration with other languages, has gained significant attention among verification engineers recently. A Python-based verification environment capitalizes on open-source frameworks such as PyUVM providing Python-based UVM 1.2 implementation and PyVSC facilitating constrained randomization and functional coverage. These libraries play a pivotal role in expediting test development and hold promise for reducing setup costs. The goal of this paper is to evaluate the effectiveness of PyUVM verification testbenches across various design IPs, aiming for a comprehensive comparison of their features and performance metrics with the established SystemVerilog-UVM methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10317v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deepak Narayan Gadde, Suruchi Kumari, Aman Kumar</dc:creator>
    </item>
    <item>
      <title>SOFA: A Compute-Memory Optimized Sparsity Accelerator via Cross-Stage Coordinated Tiling</title>
      <link>https://arxiv.org/abs/2407.10416</link>
      <description>arXiv:2407.10416v1 Announce Type: new 
Abstract: Benefiting from the self-attention mechanism, Transformer models have attained impressive contextual comprehension capabilities for lengthy texts. The requirements of high-throughput inference arise as the large language models (LLMs) become increasingly prevalent, which calls for large-scale token parallel processing (LTPP). However, existing dynamic sparse accelerators struggle to effectively handle LTPP, as they solely focus on separate stage optimization, and with most efforts confined to computational enhancements. By re-examining the end-to-end flow of dynamic sparse acceleration, we pinpoint an ever-overlooked opportunity that the LTPP can exploit the intrinsic coordination among stages to avoid excessive memory access and redundant computation. Motivated by our observation, we present SOFA, a cross-stage compute-memory efficient algorithm-hardware co-design, which is tailored to tackle the challenges posed by LTPP of Transformer inference effectively. We first propose a novel leading zero computing paradigm, which predicts attention sparsity by using log-based add-only operations to avoid the significant overhead of prediction. Then, a distributed sorting and a sorted updating FlashAttention mechanism are proposed with a cross-stage coordinated tiling principle, which enables fine-grained and lightweight coordination among stages, helping optimize memory access and latency. Further, we propose a SOFA accelerator to support these optimizations efficiently. Extensive experiments on 20 benchmarks show that SOFA achieves $9.5\times$ speed up and $71.5\times$ higher energy efficiency than Nvidia A100 GPU. Compared to 8 SOTA accelerators, SOFA achieves an average $15.8\times$ energy efficiency, $10.3\times$ area efficiency and $9.3\times$ speed up, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10416v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huizheng Wang, Jiahao Fang, Xinru Tang, Zhiheng Yue, Jinxi Li, Yubin Qin, Sihan Guan, Qize Yang, Yang Wang, Chao Li, Yang Hu, Shouyi Yin</dc:creator>
    </item>
    <item>
      <title>Assessing the Performance of Stateful Logic in 1-Selector-1-RRAM Crossbar Arrays</title>
      <link>https://arxiv.org/abs/2407.10466</link>
      <description>arXiv:2407.10466v1 Announce Type: new 
Abstract: Resistive Random Access Memory (RRAM) crossbar arrays are an attractive memory structure for emerging nonvolatile memory due to their high density and excellent scalability. Their ability to perform logic operations using RRAM devices makes them a critical component in non-von Neumann processing-in-memory architectures. Passive RRAM crossbar arrays (1-RRAM or 1R), however, suffer from a major issue of sneak path currents, leading to a lower readout margin and increasing write failures. To address this challenge, active RRAM arrays have been proposed, which incorporate a selector device in each memory cell (termed 1-selector-1-RRAM or 1S1R). The selector eliminates currents from unselected cells and therefore effectively mitigates the sneak path phenomenon. Yet, there is a need for a comprehensive analysis of 1S1R arrays, particularly concerning in-memory computation. In this paper, we introduce a 1S1R model tailored to a VO2-based selector and TiN/TiOx/HfOx/Pt RRAM device. We also present simulations of 1S1R arrays, incorporating all parasitic parameters, across a range of array sizes from $4\times4$ to $512\times512$. We evaluate the performance of Memristor-Aided Logic (MAGIC) gates in terms of switching delay, power consumption, and readout margin, and provide a comparative evaluation with passive 1R arrays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10466v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISCAS58744.2024.10558539</arxiv:DOI>
      <dc:creator>Arjun Tyagi, Shahar Kvatinsky</dc:creator>
    </item>
    <item>
      <title>Power-Area Efficient Serial IMPLY-based 4:2 Compressor Applied in Data-Intensive Applications</title>
      <link>https://arxiv.org/abs/2407.09980</link>
      <description>arXiv:2407.09980v1 Announce Type: cross 
Abstract: The data transfer between a processor and memory has become a design bottleneck in data-intensive applications. Processing-In-Memory (PIM) is a practical approach to overcome the memory wall bottleneck. The 4:2 compressor is suitable for implementing the processor's crucial arithmetic circuits, including multiplier. Some area-efficient memristive structures, like Material Implication (IMPLY) in serial architecture, are compatible with the crossbar array. This paper proposes a serial memristive IMPLY-based 4:2 compressor, which is applied to present new 4-bit and 8-bit multipliers. The proposed circuits are evaluated regarding latency, area, and energy consumption. Compared to the existing serial compressor, the proposed 4:2 compressor's algorithm improves the area, energy consumption, and speed by 36%, 17%, and 15%, respectively. The proposed 4-bit and 8-bit multipliers are improved by 7.3% and 10%, respectively, regarding the latency, and reduced energy consumption by up to 12%, compared to the serial multiplier based on a 4:2 compressor with XOR/MUX design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09980v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bahareh Bagheralmoosavi, Seyed Erfan Fatemieh, Mohammad Reza Reshadinezhad, Antonio Rubio</dc:creator>
    </item>
    <item>
      <title>MOAT: Securely Mitigating Rowhammer with Per-Row Activation Counters</title>
      <link>https://arxiv.org/abs/2407.09995</link>
      <description>arXiv:2407.09995v1 Announce Type: cross 
Abstract: The security vulnerabilities due to Rowhammer have worsened over the last decade, with existing in-DRAM solutions, such as TRR, getting broken with simple patterns. In response, the DDR5 specifications have been extended to support Per-Row Activation Counting (PRAC), with counters inlined with each row, and ALERT-Back-Off (ABO) to stop the memory controller if the DRAM needs more time to mitigate. Although PRAC+ABO represents a strong advance in Rowhammer protection, they are just a framework, and the actual security is dependent on the implementation.
  In this paper, we first show that a prior work, Panopticon (which formed the basis for PRAC+ABO), is insecure, as our Jailbreak pattern can cause 1150 activations on an attack row for Panopticon configured for a threshold of 128. We then propose MOAT, a provably secure design, which uses two internal thresholds: ETH, an "Eligibility Threshold" for mitigating a row, and ATH, an "ALERT Threshold" for initiating an ABO. As JEDEC specifications permit a few activations between consecutive ALERTs, we also study how an attacker can exploit such activations to inflict more activations than ATH on an attack row and thus increase the tolerated Rowhammer threshold. Our analysis shows that MOAT configured with ATH=64 can safely tolerate a Rowhammer threshold of 99. Finally, we also study performance attacks and denial-of-service due to ALERTs. Our evaluations, with SPEC and GAP workloads, show that MOAT with ATH=64 incurs an average slowdown of 0.28\% and 7 bytes of SRAM per bank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09995v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moinuddin Qureshi, Salman Qazi</dc:creator>
    </item>
    <item>
      <title>Accelerator-as-a-Service in Public Clouds: An Intra-Host Traffic Management View for Performance Isolation in the Wild</title>
      <link>https://arxiv.org/abs/2407.10098</link>
      <description>arXiv:2407.10098v1 Announce Type: cross 
Abstract: I/O devices in public clouds have integrated increasing numbers of hardware accelerators, e.g., AWS Nitro, Azure FPGA and Nvidia BlueField. However, such specialized compute (1) is not explicitly accessible to cloud users with performance guarantee, (2) cannot be leveraged simultaneously by both providers and users, unlike general-purpose compute (e.g., CPUs). Through ten observations, we present that the fundamental difficulty of democratizing accelerators is insufficient performance isolation support. The key obstacles to enforcing accelerator isolation are (1) too many unknown traffic patterns in public clouds and (2) too many possible contention sources in the datapath. In this work, instead of scheduling such complex traffic on-the-fly and augmenting isolation support on each system component, we propose to model traffic as network flows and proactively re-shape the traffic to avoid unpredictable contention. We discuss the implications of our findings on the design of future I/O management stacks and device interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10098v1</guid>
      <category>cs.OS</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiechen Zhao, Ran Shu, Katie Lim, Zewen Fan, Thomas Anderson, Mingyu Gao, Natalie Enright Jerger</dc:creator>
    </item>
    <item>
      <title>Towards Green AI: Current status and future research</title>
      <link>https://arxiv.org/abs/2407.10237</link>
      <description>arXiv:2407.10237v1 Announce Type: cross 
Abstract: The immense technological progress in artificial intelligence research and applications is increasingly drawing attention to the environmental sustainability of such systems, a field that has been termed Green AI. With this contribution we aim to broaden the discourse on Green AI by investigating the current status of approaches to both environmental assessment and ecodesign of AI systems. We propose a life-cycle-based system thinking approach that accounts for the four key elements of these software-hardware-systems: model, data, server, and cloud. We conduct an exemplary estimation of the carbon footprint of relevant compute hardware and highlight the need to further investigate methods for Green AI and ways to facilitate wide-spread adoption of its principles. We envision that AI could be leveraged to mitigate its own environmental challenges, which we denote as AI4greenAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10237v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Clemm, Lutz Stobbe, Kishan Wimalawarne, Jan Druschke</dc:creator>
    </item>
    <item>
      <title>FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries</title>
      <link>https://arxiv.org/abs/2407.10810</link>
      <description>arXiv:2407.10810v1 Announce Type: cross 
Abstract: Intelligence is key to advancing integrated circuit (IC) fabrication. Recent breakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleled abilities in understanding images and text, fostering intelligent fabrication. Leveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication large multimodal model for wafer defect knowledge query. FabGPT manifests expertise in conducting defect detection in Scanning Electron Microscope (SEM) images, performing root cause analysis, and providing expert question-answering (Q&amp;A) on fabrication processes. FabGPT matches enhanced multimodal features to automatically detect minute defects under complex wafer backgrounds and reduce the subjectivity of manual threshold settings. Besides, the proposed modulation module and interactive corpus training strategy embed wafer defect knowledge into the pre-trained model, effectively balancing Q&amp;A queries related to defect knowledge and original knowledge and mitigating the modality bias issues. Experiments on in-house fab data (SEM-WaD) show that our FabGPT achieves significant performance improvement in wafer defect detection and knowledge querying.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10810v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuqi Jiang, Xudong Lu, Qian Jin, Qi Sun, Hanming Wu, Cheng Zhuo</dc:creator>
    </item>
    <item>
      <title>DAG-aware Synthesis Orchestration</title>
      <link>https://arxiv.org/abs/2310.07846</link>
      <description>arXiv:2310.07846v2 Announce Type: replace 
Abstract: The key methodologies of modern logic synthesis techniques are conducted on multi-level technology-independent representations such as And-Inverter-Graphs (AIGs) of the digital logic via directed-acyclic-graph (DAGs) traversal based structural rewriting, resubstitution, and refactoring. Existing state-of-the-art DAG-aware logic synthesis algorithms are all designed to perform stand-alone optimizations during a single DAG traversal. However, we empirically identify and demonstrate that these algorithms are limited in quality-of-results and runtime complexity due to this design concept. This work proposes Synthesis Orchestration, which orchestrates stand-alone operations within the single traversal of AIG. Thus, orchestration method explores more optimization opportunities and results in better performance. Our experimental results are comprehensively conducted on all 104 designs collected from ISCAS'85/89/99, VTR, and EPFL benchmark suites, with consistent logic minimization improvements over rewriting, resubstitution, refactoring, leading to an average of 4% more node reduction with improved runtime efficiency for the single optimization. Moreover, we evaluate orchestration as a plug-in algorithm in resyn and resyn3 flows in ABC, which demonstrates consistent logic minimization improvements (3.8% and 10.9% more node reduction on average). The runtime analysis demonstrates the orchestration outperforms stand-alone algorithms in both AIG minimization and runtime efficiency. Finally, we integrate the orchestration into OpenROAD for end-to-end performance evaluation. Our results demonstrate the advantages of the orchestration optimization technique, even after technology mapping and post-routing in the design flow have been conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07846v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2024</arxiv:journal_reference>
      <dc:creator>Yingjie Li, Mingju Liu, Mark Ren, Alan Mishchenko, Cunxi Yu</dc:creator>
    </item>
  </channel>
</rss>
