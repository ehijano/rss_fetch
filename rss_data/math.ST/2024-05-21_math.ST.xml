<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On General Weighted Extropy of Percentile Ranked Set Sampling</title>
      <link>https://arxiv.org/abs/2405.11201</link>
      <description>arXiv:2405.11201v1 Announce Type: new 
Abstract: The extropy measure, first proposed by Lad, Sanfilippo, and Agro in their (2015) paper in Statistical Science, has attracted considerable attention in recent years. Our study introduces a fresh approach to representing weighted extropy in the framework of percentile ranked set sampling. Furthermore, we provide additional insights such as stochastic orders, characterizations, and bounds. Our findings illuminate the comparison between the weighted extropy of percentile ranked set sampling and its equivalent in simple random sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11201v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pradeep Kumar Sahu, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>On the consistent estimators of the population covariance matrix and its reparameterizations</title>
      <link>https://arxiv.org/abs/2405.11246</link>
      <description>arXiv:2405.11246v1 Announce Type: new 
Abstract: For the high-dimensional covariance estimation problem, when $\lim_{n\to \infty}p/n=c \in (0,1)$ the orthogonally equivariant estimator of the population covariance matrix proposed by Tsai and Tsai (2024b) enjoys some optimal properties. Under some regularity conditions, they showed that their novel estimators of eigenvalues are consistent for the eigenvalues of the population covariance matrix. In this note, first, we show that their novel estimator is consistent estimator of the population covariance matrix under a high-dimensional asymptotic setup. Moreover, we also show that the novel estimator is the MLE of population covariance matrix when $c \in (0, 1)$. The novel estimator is incorporated to establish the optimal decomposite $T_{T}^{2}-$test for a high-dimensional statistical hypothesis testing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11246v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming-Tien Tsai, Chia-Hsian Tsai</dc:creator>
    </item>
    <item>
      <title>The High-Dimensional Asymptotics of Principal Component Regression</title>
      <link>https://arxiv.org/abs/2405.11676</link>
      <description>arXiv:2405.11676v1 Announce Type: new 
Abstract: We study principal components regression (PCR) in an asymptotic high-dimensional regression setting, where the number of data points is proportional to the dimension. We derive exact limiting formulas for the estimation and prediction risks, which depend in a complicated manner on the eigenvalues of the population covariance, the alignment between the population PCs and the true signal, and the number of selected PCs. A key challenge in the high-dimensional setting stems from the fact that the sample covariance is an inconsistent estimate of its population counterpart, so that sample PCs may fail to fully capture potential latent low-dimensional structure in the data. We demonstrate this point through several case studies, including that of a spiked covariance model.
  To calculate the asymptotic prediction risk, we leverage tools from random matrix theory which to our knowledge have not seen much use to date in the statistics literature: multi-resolvent traces and their associated eigenvector overlap measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11676v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alden Green, Elad Romanov</dc:creator>
    </item>
    <item>
      <title>Modified Greenwood statistic and its application for statistical testing</title>
      <link>https://arxiv.org/abs/2405.11834</link>
      <description>arXiv:2405.11834v1 Announce Type: new 
Abstract: In this paper, we explore the modified Greenwood statistic, which, in contrast to the classical Greenwood statistic, is properly defined for random samples from any distribution. The classical Greenwood statistic, extensively examined in the existing literature, has found diverse and interesting applications across various domains. Furthermore, numerous modifications to the classical statistic have been proposed. The modified Greenwood statistic, as proposed and discussed in this paper, shares several key properties with its classical counterpart. Emphasizing its stochastic monotonicity within three broad classes of distributions - namely, generalized Pareto, $\alpha-$stable, and Student's t distributions - we advocate for the utilization of the modified Greenwood statistic in testing scenarios. Our exploration encompasses three distinct directions. In the first direction, we employ the modified Greenwood statistic for Gaussian distribution testing. Our empirical results compellingly illustrate that the proposed approach consistently outperforms alternative goodness-of-fit tests documented in the literature, particularly exhibiting superior efficacy for small sample sizes. The second considered problem involves testing the infinite-variance distribution of a given random sample. The last proposition suggests using the modified Greenwood statistic for testing of a given distribution. The presented simulation study strongly supports the efficiency of the proposed approach in the considered problems. Theoretical results and power simulation studies are further validated by real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11834v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katarzyna Skowronek, Marek Arendarczyk, Rados{\l}aw Zimroz, Agnieszka Wy{\l}oma\'nska</dc:creator>
    </item>
    <item>
      <title>Rate Optimality and Phase Transition for User-Level Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2405.11923</link>
      <description>arXiv:2405.11923v1 Announce Type: new 
Abstract: Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.
  In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.
  In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \gtrsim s \log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11923v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kent, Thomas B. Berrett, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Uniform Ergodicity of Parallel Tempering With Efficient Local Exploration</title>
      <link>https://arxiv.org/abs/2405.11384</link>
      <description>arXiv:2405.11384v1 Announce Type: cross 
Abstract: Non-reversible parallel tempering (NRPT) is an effective algorithm for sampling from target distributions with complex geometry, such as those arising from posterior distributions of weakly identifiable and high-dimensional Bayesian models. In this work we establish the uniform (geometric) ergodicity of NRPT under a model of efficient local exploration. The uniform ergodicity log rates are inversely proportional to an easily-estimable divergence, the global communication barrier (GCB), which was recently introduced in the literature. We obtain analogous ergodicity results for classical reversible parallel tempering, providing new evidence that NRPT dominates its reversible counterpart. Our results are based on an analysis of the hitting time of a continuous-time persistent random walk, which is also of independent interest. The rates that we obtain reflect real experiments well for distributions where global exploration is not possible without parallel tempering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11384v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikola Surjanovic, Saifuddin Syed, Alexandre Bouchard-C\^ot\'e, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Breuer-Major Theorems for Hilbert Space-Valued Random Variables</title>
      <link>https://arxiv.org/abs/2405.11452</link>
      <description>arXiv:2405.11452v1 Announce Type: cross 
Abstract: Let $\{X_k\}_{k \in \mathbb{Z}}$ be a stationary Gaussian process with values in a separable Hilbert space $\mathcal{H}_1$, and let $G:\mathcal{H}_1 \to \mathcal{H}_2$ be an operator acting on $X_k$. Under suitable conditions on the operator $G$ and the temporal and cross-sectional correlations of $\{X_k\}_{k \in \mathbb{Z}}$, we derive a central limit theorem (CLT) for the normalized partial sums of $\{G[X_k]\}_{k \in \mathbb{Z}}$. To prove a CLT for the Hilbert space-valued process $\{G[X_k]\}_{k \in \mathbb{Z}}$, we employ techniques from the recently developed infinite dimensional Malliavin-Stein framework. In addition, we provide quantitative and continuous time versions of the derived CLT. In a series of examples, we recover and strengthen limit theorems for a wide array of statistics relevant in functional data analysis, and present a novel limit theorem in the framework of neural operators as an application of our result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11452v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marie-Christine D\"uker, Pavlos Zoubouloglou</dc:creator>
    </item>
    <item>
      <title>On Generalized Transmuted Lifetime Distribution</title>
      <link>https://arxiv.org/abs/2405.11624</link>
      <description>arXiv:2405.11624v1 Announce Type: cross 
Abstract: This article presents a new class of generalized transmuted lifetime distributions which includes a large number of lifetime distributions as sub-family. Several important mathematical quantities such as density function, distribution function, quantile function, moments, moment generating function, stress-strength reliability function, order statistics, R\'enyi and q-entropy, residual and reversed residual life function, and cumulative information generating function are obtained. The methods of maximum likelihood, ordinary least square, weighted least square, Cram\'er-von Mises, Anderson Darling, and Right-tail Anderson Darling are considered to estimate the model parameters in a general way. Further, a well-organized Monte Carlo simulation experiments have been performed to observe the behavior of the estimators. Finally, two real data have also been analyzed to demonstrate the effectiveness of the proposed distribution in real-life modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11624v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alok Kumar Pandey, Alam Ali, Ashok Kumar Pathak</dc:creator>
    </item>
    <item>
      <title>Distribution-in-distribution-out Regression</title>
      <link>https://arxiv.org/abs/2405.11626</link>
      <description>arXiv:2405.11626v1 Announce Type: cross 
Abstract: Regression analysis with probability measures as input predictors and output response has recently drawn great attention. However, it is challenging to handle multiple input probability measures due to the non-flat Riemannian geometry of the Wasserstein space, hindering the definition of arithmetic operations, hence additive linear structure is not well-defined. In this work, a distribution-in-distribution-out regression model is proposed by introducing parallel transport to achieve provable commutativity and additivity of newly defined arithmetic operations in Wasserstein space. The appealing properties of the DIDO regression model can serve a foundation for model estimation, prediction, and inference. Specifically, the Fr\'echet least squares estimator is employed to obtain the best linear unbiased estimate, supported by the newly established Fr\'echet Gauss-Markov Theorem. Furthermore, we investigate a special case when predictors and response are all univariate Gaussian measures, leading to a simple close-form solution of linear model coefficients and $R^2$ metric. A simulation study and real case study in intraoperative cardiac output prediction are performed to evaluate the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11626v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoyu Chen (Zipan), Mengfan Fu (Zipan),  Yujing (Zipan),  Huang, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>Distributed Tensor Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2405.11681</link>
      <description>arXiv:2405.11681v1 Announce Type: cross 
Abstract: As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.
  We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.
  We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11681v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing, Yichen Zhang</dc:creator>
    </item>
    <item>
      <title>General bounds on the quality of Bayesian coresets</title>
      <link>https://arxiv.org/abs/2405.11780</link>
      <description>arXiv:2405.11780v1 Announce Type: cross 
Abstract: Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings -- i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11780v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Stability and statistical inference for semidiscrete optimal transport maps</title>
      <link>https://arxiv.org/abs/2303.10155</link>
      <description>arXiv:2303.10155v3 Announce Type: replace 
Abstract: We study statistical inference for the optimal transport (OT) map (also known as the Brenier map) from a known absolutely continuous reference distribution onto an unknown finitely discrete target distribution. We derive limit distributions for the $L^p$-error with arbitrary $p \in [1,\infty)$ and for linear functionals of the empirical OT map, together with their moment convergence. The former has a non-Gaussian limit, whose explicit density is derived, while the latter attains asymptotic normality. For both cases, we also establish consistency of the nonparametric bootstrap. The derivation of our limit theorems relies on new stability estimates of functionals of the OT map with respect to the dual potential vector, which may be of independent interest. We also discuss applications of our limit theorems to the construction of confidence sets for the OT map and inference for a maximum tail correlation. Finally, we show that, while the empirical OT map does not possess nontrivial weak limits in the $L^2$ space, it satisfies a central limit theorem in a dual H\"{o}lder space, and the Gaussian limit law attains the asymptotic efficiency bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10155v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Sadhu, Ziv Goldfeld, Kengo Kato</dc:creator>
    </item>
    <item>
      <title>Residual spectrum: Brain functional connectivity detection beyond coherence</title>
      <link>https://arxiv.org/abs/2305.19461</link>
      <description>arXiv:2305.19461v2 Announce Type: replace 
Abstract: Coherence is a widely used measure to assess linear relationships between time series. However, it fails to capture nonlinear dependencies. To overcome this limitation, this paper introduces the notion of residual spectral density as a higher-order extension of the squared coherence. The method is based on an orthogonal decomposition of time series regression models. We propose a test for the existence of the residual spectrum and derive its fundamental properties. A numerical study illustrates finite sample performance of the proposed method. An application of the method shows that the residual spectrum can effectively detect brain connectivity. Our study reveals a noteworthy contrast in connectivity patterns between schizophrenia patients and healthy individuals. Specifically, we observed that non-linear connectivity in schizophrenia patients surpasses that of healthy individuals, which stands in stark contrast to the established understanding that linear connectivity tends to be higher in healthy individuals. This finding sheds new light on the intricate dynamics of brain connectivity in schizophrenia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19461v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuichi Goto, Xuze Zhang, Benjamin Kedem, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>On the sample complexity of parameter estimation in logistic regression with normal design</title>
      <link>https://arxiv.org/abs/2307.04191</link>
      <description>arXiv:2307.04191v3 Announce Type: replace 
Abstract: The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04191v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Hsu, Arya Mazumdar</dc:creator>
    </item>
    <item>
      <title>Continuous Time Locally Stationary Wavelet Processes</title>
      <link>https://arxiv.org/abs/2310.12788</link>
      <description>arXiv:2310.12788v5 Announce Type: replace 
Abstract: This article introduces the class of continuous time locally stationary wavelet processes. Continuous time models enable us to properly provide scale-based time series models for irregularly-spaced observations for the first time, while also permitting a spectral representation of the process over a continuous range of scales. We derive results for both the theoretical setting, where we assume access to the entire process sample path, and a more practical one, which develops methods for estimating the quantities of interest from sampled time series. The latter estimates are accurately computable in reasonable time by solving the relevant linear integral equation using the iterative thresholding method due to Daubechies, Defrise and De Mol. Appropriate smoothing techniques are also developed and applied in this new setting. We exemplify our new methods by computing spectral and autocovariance estimates on irregularly-spaced heart rate data obtained from a recent sleep-state study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12788v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Antonio Palasciano, Marina I. Knight, Guy P. Nason</dc:creator>
    </item>
    <item>
      <title>Identifiability of total effects from abstractions of time series causal graphs</title>
      <link>https://arxiv.org/abs/2310.14691</link>
      <description>arXiv:2310.14691v5 Announce Type: replace 
Abstract: We study the problem of identifiability of the total effect of an intervention from observational time series in the situation, common in practice, where one only has access to abstractions of the true causal graph. We consider here two abstractions: the extended summary causal graph, which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations, and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and provide sufficient conditions for identifiability in summary causal graphs. We furthermore provide adjustment sets allowing to estimate the total effect whenever it is identifiable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14691v5</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles K. Assaad, Emilie Devijver, Eric Gaussier, Gregor G\"ossler, Anouar Meynaoui</dc:creator>
    </item>
    <item>
      <title>Assessment of the quality of a prediction</title>
      <link>https://arxiv.org/abs/2404.15764</link>
      <description>arXiv:2404.15764v3 Announce Type: replace 
Abstract: Shannon defined the mutual information between two variables. We illustrate why the true mutual information between a variable and the predictions made by a prediction algorithm is not a suitable measure of prediction quality, but the apparent Shannon mutual information (ASI) is; indeed it is the unique prediction quality measure with either of two very different lists of desirable properties, as previously shown by de Finetti and other authors. However, estimating the uncertainty of the ASI is a difficult problem, because of long and non-symmetric heavy tails to the distribution of the individual values of $j(x,y)=\log\frac{Q_y(x)}{P(x)}$ We propose a Bayesian modelling method for the distribution of $j(x,y)$, from the posterior distribution of which the uncertainty in the ASI can be inferred. This method is based on Dirichlet-based mixtures of skew-Student distributions. We illustrate its use on data from a Bayesian model for prediction of the recurrence time of prostate cancer. We believe that this approach is generally appropriate for most problems, where it is infeasible to derive the explicit distribution of the samples of $j(x,y)$, though the precise modelling parameters may need adjustment to suit particular cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15764v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roger Sewell, Elisabeth Crowe, Sharokh F. Shariat</dc:creator>
    </item>
    <item>
      <title>A sparsity test for multivariate Hawkes processes</title>
      <link>https://arxiv.org/abs/2405.08640</link>
      <description>arXiv:2405.08640v2 Announce Type: replace 
Abstract: Multivariate Hawkes processes (MHP) are a class of point processes in which events at different coordinates interact through mutual excitation. The weighted adjacency matrix of the MHP encodes the strength of the relations, and shares its support with the causal graph of interactions of the process. We consider the problem of testing for causal relationships across the dimensions of a marked MHP. The null hypothesis is that a joint group of adjacency coefficients are null, corresponding to the absence of interactions. The alternative is that they are positive, and the associated interactions do exist. To this end, we introduce a novel estimation procedure in the context of a large sample of independent event sequences. We construct the associated likelihood ratio test and derive the asymptotic distribution of the test statistic as a mixture of chi squared laws. We offer two applications on financial datasets to illustrate the performance of our method. In the first one, our test reveals a deviation from a static equilibrium in bidders' strategies on retail online auctions. In the second one, we uncover some factors at play in the dynamics of German intraday power prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08640v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Lotz</dc:creator>
    </item>
    <item>
      <title>The Instrumental Variable Model with Categorical Instrument, Treatment and Outcome</title>
      <link>https://arxiv.org/abs/2405.09510</link>
      <description>arXiv:2405.09510v2 Announce Type: replace 
Abstract: Instrumental variable models are central to the inference of causal effects in many settings. We consider the instrumental variable model with discrete variables where the instrument (Z), exposure (X) and outcome (Y) take Q, K, and M levels respectively. We assume that the instrument is randomized and that there is no direct effect of Z on Y so that Y(x,z) = Y(x). We first provide a simple characterization of the set of joint distributions of the potential outcomes P(Y(x=1), ..., Y(x=K)) compatible with a given observed distribution P(X, Y | Z). We then discuss the variation (in)dependence property of the marginal probability distribution of the potential outcomes P(Y(x=1)), ..., P(Y(x=K)) which has direct implications for partial identification of average causal effect contrasts such as E[Y(x=i) - Y(x=j)]. We also include simulation results on the volume of the observed distributions not compatible with the IV model as K and Q change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09510v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Song, K. C. Gary Chan, Thomas S. Richardson</dc:creator>
    </item>
    <item>
      <title>Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics, Directional Convergence, and Equilibria</title>
      <link>https://arxiv.org/abs/2212.02457</link>
      <description>arXiv:2212.02457v3 Announce Type: replace-cross 
Abstract: Covariate distribution shifts and adversarial perturbations present robustness challenges to the conventional statistical learning framework: mild shifts in the test covariate distribution can significantly affect the performance of the statistical model learned based on the training distribution. The model performance typically deteriorates when extrapolation happens: namely, covariates shift to a region where the training distribution is scarce, and naturally, the learned model has little information. For robustness and regularization considerations, adversarial perturbation techniques are proposed as a remedy; however, careful study needs to be carried out about what extrapolation region adversarial covariate shift will focus on, given a learned model. This paper precisely characterizes the extrapolation region, examining both regression and classification in an infinite-dimensional setting. We study the implications of adversarial covariate shifts to subsequent learning of the equilibrium -- the Bayes optimal model -- in a sequential game framework. We exploit the dynamics of the adversarial learning game and reveal the curious effects of the covariate shift to equilibrium learning and experimental design. In particular, we establish two directional convergence results that exhibit distinctive phenomena: (1) a blessing in regression, the adversarial covariate shifts in an exponential rate to an optimal experimental design for rapid subsequent learning; (2) a curse in classification, the adversarial covariate shifts in a subquadratic rate to the hardest experimental design trapping subsequent learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02457v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 25 (2024) 1-27</arxiv:journal_reference>
      <dc:creator>Tengyuan Liang</dc:creator>
    </item>
    <item>
      <title>Quasi Maximum Likelihood Estimation of High-Dimensional Factor Models: A Critical Review</title>
      <link>https://arxiv.org/abs/2303.11777</link>
      <description>arXiv:2303.11777v5 Announce Type: replace-cross 
Abstract: We review Quasi Maximum Likelihood estimation of factor models for high-dimensional panels of time series. We consider two cases: (1) estimation when no dynamic model for the factors is specified (Bai and Li, 2012, 2016); (2) estimation based on the Kalman smoother and the Expectation Maximization algorithm thus allowing to model explicitly the factor dynamics (Doz et al., 2012, Barigozzi and Luciani, 2019). Our interest is in approximate factor models, i.e., when we allow for the idiosyncratic components to be mildly cross-sectionally, as well as serially, correlated. Although such setting apparently makes estimation harder, we show, in fact, that factor models do not suffer of the {\it curse of dimensionality} problem, but instead they enjoy a {\it blessing of dimensionality} property. In particular, given an approximate factor structure, if the cross-sectional dimension of the data, $N$, grows to infinity, we show that: (i) identification of the model is still possible, (ii) the mis-specification error due to the use of an exact factor model log-likelihood vanishes. Moreover, if we let also the sample size, $T$, grow to infinity, we can also consistently estimate all parameters of the model and make inference. The same is true for estimation of the latent factors which can be carried out by weighted least-squares, linear projection, or Kalman filtering/smoothing. We also compare the approaches presented with: Principal Component analysis and the classical, fixed $N$, exact Maximum Likelihood approach. We conclude with a discussion on efficiency of the considered estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11777v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi</dc:creator>
    </item>
    <item>
      <title>Metric Entropy-Free Sample Complexity Bounds for Sample Average Approximation in Convex Stochastic Programming</title>
      <link>https://arxiv.org/abs/2401.00664</link>
      <description>arXiv:2401.00664v2 Announce Type: replace-cross 
Abstract: This paper studies the sample average approximation (SAA) in solving convex or strongly convex stochastic programming problems. Under some common regularity conditions, we show -- perhaps for the first time -- that the SAA's sample complexity can be completely free from any quantification of metric entropy (such as the logarithm of the covering number), leading to a significantly more efficient rate with dimensionality $d$ than most existing results. From the newly established complexity bounds, an important revelation is that the SAA and the canonical stochastic mirror descent (SMD) method, two mainstream solution approaches to SP, entail almost identical rates of sample efficiency, rectifying a long-standing theoretical discrepancy of the SAA from the SMD by the order of $O(d)$. Furthermore, this paper explores non-Lipschitzian scenarios where the SAA maintains provable efficacy, whereas corresponding results for the SMD remain unexplored, indicating the potential of the SAA's better applicability in some irregular settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00664v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongcheng Liu, Jindong Tong</dc:creator>
    </item>
    <item>
      <title>A Notion of Uniqueness for the Adversarial Bayes Classifier</title>
      <link>https://arxiv.org/abs/2404.16956</link>
      <description>arXiv:2404.16956v2 Announce Type: replace-cross 
Abstract: We propose a new notion of uniqueness for the adversarial Bayes classifier in the setting of binary classification. Analyzing this concept produces a simple procedure for computing all adversarial Bayes classifiers for a well-motivated family of one dimensional data distributions. This characterization is then leveraged to show that as the perturbation radius increases, certain the regularity of adversarial Bayes classifiers improves. Various examples demonstrate that the boundary of the adversarial Bayes classifier frequently lies near the boundary of the Bayes classifier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16956v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie S. Frank</dc:creator>
    </item>
    <item>
      <title>Decoding Geometric Properties in Non-Random Data from First Information-Theoretic Principles</title>
      <link>https://arxiv.org/abs/2405.07803</link>
      <description>arXiv:2405.07803v2 Announce Type: replace-cross 
Abstract: Based on the principles of information theory, measure theory, and theoretical computer science, we introduce a univariate signal deconvolution method with a wide range of applications to coding theory, particularly in zero-knowledge one-way communication channels, such as in deciphering messages from unknown generating sources about which no prior knowledge is available and to which no return message can be sent. Our multidimensional space reconstruction method from an arbitrary received signal is proven to be agnostic vis-a-vis the encoding-decoding scheme, computation model, programming language, formal theory, the computable (or semi-computable) method of approximation to algorithmic complexity, and any arbitrarily chosen (computable) probability measure of the events. The method derives from the principles of an approach to Artificial General Intelligence capable of building a general-purpose model of models independent of any arbitrarily assumed prior probability distribution. We argue that this optimal and universal method of decoding non-random data has applications to signal processing, causal deconvolution, topological and geometric properties encoding, cryptography, and bio- and technosignature detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07803v2</guid>
      <category>cs.IT</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hector Zenil, Felipe S. Abrah\~ao</dc:creator>
    </item>
  </channel>
</rss>
