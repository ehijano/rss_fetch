<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 02:04:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>HPR-Mul: An Area and Energy-Efficient High-Precision Redundancy Multiplier by Approximate Computing</title>
      <link>https://arxiv.org/abs/2410.20150</link>
      <description>arXiv:2410.20150v1 Announce Type: new 
Abstract: For critical applications that require a higher level of reliability, the Triple Modular Redundancy (TMR) scheme is usually employed to implement fault-tolerant arithmetic units. However, this method imposes a significant area and power/energy overhead. Also, the majority-based voter in the typical TMR designs is highly sensitive to soft errors and the design diversity of the triplicated module, which may result in an error for a small difference between the output of the TMR modules. However, a wide range of applications deployed in critical systems are inherently error-resilient, i.e., they can tolerate some inexact results at their output while having a given level of reliability. In this paper, we propose a High Precision Redundancy Multiplier (HPR-Mul) that relies on the principles of approximate computing to achieve higher energy efficiency and lower area, as well as resolve the aforementioned challenges of the typical TMR schemes, while retaining the required level of reliability. The HPR-Mul is composed of full precision (FP) and two reduced precision (RP) multipliers, along with a simple voter to determine the output. Unlike the state-of-the-art Reduced Precision Redundancy multipliers (RPR-Mul) that require a complex voter, the voter of the proposed HPR-Mul is designed based on mathematical formulas resulting in a simpler structure. Furthermore, we use the intermediate signals of the FP multiplier as the inputs of the RP multipliers, which significantly enhance the accuracy of the HPR-Mul. The efficiency of the proposed HPR-Mul is evaluated in a 15-nm FinFET technology, where the results show up to 70% and 69% lower power consumption and area, respectively, compared to the typical TMR-based multipliers. Also, the HPR-Mul outperforms the state-of-the-art RPR-Mul by achieving up to 84% higher soft error tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20150v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVLSI.2024.3445108</arxiv:DOI>
      <dc:creator>Jafar Vafaei, Omid Akbari</dc:creator>
    </item>
    <item>
      <title>Architectural Solutions for High-Speed Data Processing Demands of CERN LHC Detectors with FPGA and High-Level Synthesis</title>
      <link>https://arxiv.org/abs/2410.20430</link>
      <description>arXiv:2410.20430v1 Announce Type: new 
Abstract: The planned high-luminosity upgrade of the Large Hadron Collider (LHC) at CERN will bring much higher data rates that are far above the capabilities of currently installed software-based data processing systems. Therefore, new methods must be used to facilitate on-the-fly extraction of scientifically significant information from the immense flow of data produced by LHC particle detectors. This paper focuses on implementation of a tau lepton triggering algorithm in FPGA. Due to the algorithm's complexity and strict technical requirements, its implementation in FPGA fabric becomes a particularly challenging task. The paper presents a study of algorithm development with the help of High-Level Synthesis (HLS) technique that can generate hardware description from C++ code. Various architectural solutions and optimizations that were tried out during the design architecture exploration process are also discussed in the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20430v1</guid>
      <category>cs.AR</category>
      <category>physics.ins-det</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergei Devadze, Christine Elizabeth Nielsen, Dmitri Mihhailov, Peeter Ellervee</dc:creator>
    </item>
    <item>
      <title>SPICEPilot: Navigating SPICE Code Generation and Simulation with AI Guidance</title>
      <link>https://arxiv.org/abs/2410.20553</link>
      <description>arXiv:2410.20553v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown great potential in automating code generation; however, their ability to generate accurate circuit-level SPICE code remains limited due to a lack of hardware-specific knowledge. In this paper, we analyze and identify the typical limitations of existing LLMs in SPICE code generation. To address these limitations, we present SPICEPilot a novel Python-based dataset generated using PySpice, along with its accompanying framework. This marks a significant step forward in automating SPICE code generation across various circuit configurations. Our framework automates the creation of SPICE simulation scripts, introduces standardized benchmarking metrics to evaluate LLM's ability for circuit generation, and outlines a roadmap for integrating LLMs into the hardware design process. SPICEPilot is open-sourced under the permissive MIT license at https://github.com/ACADLab/SPICEPilot.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20553v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deepak Vungarala, Sakila Alam, Arnob Ghosh, Shaahin Angizi</dc:creator>
    </item>
    <item>
      <title>The maximum storage capacity of open-loop written RRAM is around 4 bits</title>
      <link>https://arxiv.org/abs/2410.20332</link>
      <description>arXiv:2410.20332v1 Announce Type: cross 
Abstract: There have been a plethora of research on multi-level memory devices, where the resistive random-access memory (RRAM) is a prominent example. Although it is easy to write an RRAM device into multiple (even quasi-continuous) states, it suffers from the inherent variations that should limit the storage capacity, especially in the open-loop writing scenario. There have been many experimental results in this regard, however, it lacks a comprehensive analysis of the valid multi-bit storage capability, especially in theoretical terms. The absence of such an insight usually results in misleading conclusions that either exaggerate or underestimate the storage capacity of RRAM devices. Here, by the concept of information theory, we present a model for evaluating the storage capacity of open-loop written RRAM. Based on the experimental results in the literature and the test results of our own devices, we have carefully examined the effects of number of pre-defined levels, conductance variation, and conductance range, on the storage capacity. The analysis leads to a conclusion that the maximum capacity of RRAM devices is around 4 bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20332v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yongxiang Li, Shiqing Wang, Zhong Sun</dc:creator>
    </item>
    <item>
      <title>Characterization of Noise using variants of Unitarity Randomized Benchmarking</title>
      <link>https://arxiv.org/abs/2410.20528</link>
      <description>arXiv:2410.20528v2 Announce Type: cross 
Abstract: Benchmarking of noise that is induced during the implementation of quantum gates is the main concern for practical quantum computers. Several protocols have been proposed that empirically calculate various metrics that quantify the error rates of the quantum gates chosen from a preferred gate set. Unitarity randomized benchmarking (URB) protocol is a method to estimate the coherence of noise induced by the quantum gates which is measured by the metric \textit{unitarity}. In this paper, we for the first time, implement the URB protocol in a quantum simulator with all the parameters and noise model are used from a real quantum device. The direct implementation of the URB protocol in a quantum device is not possible using current technologies, as it requires the preparation of mixed states. To overcome this challenge, we propose a modification of the URB protocol, namely the m-URB protocol, that enables us to practically implement it on any quantum device. We validate our m-URB protocol using two single-qubit noise channels -- (a) depolarising channel and (b) bit-flip channel. We further alter the m-URB protocol, namely, native gate URB or Ng-URB protocol, to study the noise in the native gates into which the quantum circuits are compiled in a quantum computer. Using our Ng-URB protocol, we can also detect the presence of cross-talk errors which are correlated errors caused due to non-local and entangling gates such as CNOT gate. For illustration, we simulate the noise of the native gates taking the noise parameter from two real IBM-Q processors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20528v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adarsh Chandrashekar, Soumya Das, Goutam Paul</dc:creator>
    </item>
    <item>
      <title>PyGim: An Efficient Graph Neural Network Library for Real Processing-In-Memory Architectures</title>
      <link>https://arxiv.org/abs/2402.16731</link>
      <description>arXiv:2402.16731v5 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML library that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively. We extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using emerging GNN models, and demonstrate that it outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource utilization than CPU and GPU systems. Our work provides useful recommendations for software, system and hardware designers. PyGim is publicly available at https://github.com/CMU-SAFARI/PyGim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16731v5</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko</dc:creator>
    </item>
    <item>
      <title>Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent Interconnects</title>
      <link>https://arxiv.org/abs/2409.08141</link>
      <description>arXiv:2409.08141v2 Announce Type: replace 
Abstract: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. Like others before us, we argue that the assumptions that led to this model are obsolete, and in many use-cases use of Programmed I/O (PIO), where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. However, unlike others to date, we push this idea further and show, in a real implementation, the gains in average and tail latency for fine-grained communication achievable using an open cache-coherence protocol which exposes cache transitions to a smart device. We show this using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions, comparing our use of coherence with both traditional DMA-style interaction and a highly-optimized implementation using PIO over PCI Express (PCIe).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08141v2</guid>
      <category>cs.AR</category>
      <category>cs.OS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe</dc:creator>
    </item>
    <item>
      <title>Towards CPU Performance Prediction: New Challenge Benchmark Dataset and Novel Approach</title>
      <link>https://arxiv.org/abs/2407.03385</link>
      <description>arXiv:2407.03385v3 Announce Type: replace-cross 
Abstract: The server central processing unit (CPU) market continues to exhibit robust demand due to the rising global need for computing power. Against this backdrop, CPU benchmark performance prediction is crucial for architecture designers. It offers profound insights for optimizing system designs and significantly reduces the time required for benchmark testing. However, the current research suffers from a lack of a unified, standard and a comprehensive dataset covering various CPU benchmark suites on real machines. Additionally, the traditional simulation-based methods suffer from slow simulation speeds. Furthermore, traditional machine learning approaches not only struggle to process complex features across various hardware configurations but also fall short in achieving sufficient accuracy.
  To bridge these gaps, we firstly perform a streamlined data preprocessing and reorganize our in-house datasets gathered from a variety CPU models of 4th Generation Intel Xeon Scalable Processors on various benchmark suites. We then propose Nova CPU Performance Predictor (NCPP), a deep learning model with attention mechanisms, specifically designed to predict CPU performance across various benchmarks. Our model effectively captures key hardware configurations affecting performance in across various benchmarks. Moreover, we compare eight mainstream machine learning methods, demonstrating the significant advantages of our model in terms of accuracy and explainability over existing approaches. Finally, our results provide new perspectives and practical strategies for hardware designers. To foster further research and collaboration, we \textit{\textbf{open-source}} the model \url{https://github.com/xiaoman-liu/NCPP}</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03385v3</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoman Liu</dc:creator>
    </item>
  </channel>
</rss>
