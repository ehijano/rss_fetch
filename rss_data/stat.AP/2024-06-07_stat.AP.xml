<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impact of aleatoric, stochastic and epistemic uncertainties on project cost contingency reserves</title>
      <link>https://arxiv.org/abs/2406.03500</link>
      <description>arXiv:2406.03500v1 Announce Type: new 
Abstract: In construction projects, contingency reserves have traditionally been estimated based on a percentage of the total project cost, which is arbitrary and, thus, unreliable in practical cases. Monte Carlo simulation provides a more reliable estimation. However, works on this topic have focused exclusively on the effects of aleatoric uncertainty, but ignored the impacts of other uncertainty types. In this paper, we present a method to quantitatively determine project cost contingency reserves based on Monte Carlo Simulation that considers the impact of not only aleatoric uncertainty, but also of the effects of other uncertainty kinds (stochastic, epistemic) on the total project cost. The proposed method has been validated with a real-case construction project in Spain. The obtained results demonstrate that the approach will be helpful for construction Project Managers because the obtained cost contingency reserves are consistent with the actual uncertainty type that affects the risks identified in their projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03500v1</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijpe.2022.108626</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Production Economics 253, 108626 2022</arxiv:journal_reference>
      <dc:creator>David Curto, Fernando Acebes, Jose M Gonzalez-Varona, David Poza</dc:creator>
    </item>
    <item>
      <title>Socio-demographic Determinants of Child Malnutrition Age 0-5 years in Bangladesh: NB and ZINB Approaches</title>
      <link>https://arxiv.org/abs/2406.03515</link>
      <description>arXiv:2406.03515v1 Announce Type: new 
Abstract: Although child malnutrition is improving over the world in the last couple of decades, still now it is concerning issue among the developing countries including Bangladesh. In general, malnutrition is a dichotomous response variable fitted with logistic regression model. But in this study, counting number of malnourished children in each household is defined as response variable. UNICEF with co-operating Bangladesh Bureau of Statistics (BBS) conducted Multiple Indicator Cluster Survey (MICS) covering 64000 households in Bangladesh by using two stage stratified sampling technique, where 21000 households have children age 0-5 years. We use bivariate analysis figuring out significant association between target and socio-demographic predictor variables. Then Negative binomial regression model is used over poisson regression model due to arising over-dispersion problem ($variance &gt; mean$). Zero inflated negative binomial model also is applied for the excess of zeros in the target variable. Considering standard error and significant level of individual factors NB model provides better result as compare to ZINB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03515v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Mehedi Hasan Bhuiyan (Department of Statistics,Data Science, University of Central Florida)</dc:creator>
    </item>
    <item>
      <title>Bayesian generalized method of moments applied to pseudo-observations in survival analysis</title>
      <link>https://arxiv.org/abs/2406.03821</link>
      <description>arXiv:2406.03821v1 Announce Type: new 
Abstract: Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task. We propose an alternative approach that does not need the specification of this function. Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly. GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function. We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations. We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials. Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates. For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods. Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments. This offers new insights on using pseudo-observations for Bayesian survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03821v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (U1018), Caroline Brard (DSAS), Emmanuel Lesaffre (DSAS), Guosheng Yin (DSAS), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>A likelihood-based sensitivity analysis for addressing publication bias in meta-analysis of diagnostic studies using exact likelihood</title>
      <link>https://arxiv.org/abs/2406.04095</link>
      <description>arXiv:2406.04095v1 Announce Type: new 
Abstract: Publication bias (PB) poses a significant threat to meta-analysis, as studies yielding notable results are more likely to be published in scientific journals. Sensitivity analysis provides a flexible method to address PB and to examine the impact of unpublished studies. A selection model based on t-statistics to sensitivity analysis is proposed by Copas. This t-statistics selection model is interpretable and enables the modeling of biased publication sampling across studies, as indicated by the asymmetry in the funnel-plot. In meta-analysis of diagnostic studies, the summary receiver operating characteristic curve is an essential tool for synthesizing the bivariate outcomes of sensitivity and specificity reported by individual studies. Previous studies address PB upon the bivariate normal model but these methods rely on the normal approximation for the empirical logit-transformed sensitivity and specificity, which is not suitable for sparse data scenarios. Compared to the bivariate normal model, the bivariate binomial model which replaces the normal approximation in the within-study model with the exact within-study model has better finite sample properties. In this study, we applied the Copas t-statistics selection model to the meta-analysis of diagnostic studies using the bivariate binomial model. To our knowledge, this is the first study to apply the Copas t-statistics selection model to the bivariate binomial model. We have evaluated our proposed method through several real-world meta-analyses of diagnostic studies and simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04095v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taojun Hu, Yi Zhou, Xiao-Hua Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>A Practical Analysis Procedure on Generalizing Comparative Effectiveness in the Randomized Clinical Trial to the Real-world Trialeligible Population</title>
      <link>https://arxiv.org/abs/2406.04107</link>
      <description>arXiv:2406.04107v1 Announce Type: new 
Abstract: When evaluating the effectiveness of a drug, a Randomized Controlled Trial (RCT) is often considered the gold standard due to its perfect randomization. While RCT assures strong internal validity, its restricted external validity poses challenges in extending treatment effects to the broader real-world population due to possible heterogeneity in covariates. In this paper, we introduce a procedure to generalize the RCT findings to the real-world trial-eligible population based on the adaption of existing statistical methods. We utilized the augmented inversed probability of sampling weighting (AIPSW) estimator for the estimation and omitted variable bias framework to assess the robustness of the estimate against the assumption violation caused by potentially unmeasured confounders. We analyzed an RCT comparing the effectiveness of lowering hypertension between Songling Xuemaikang Capsule (SXC), a traditional Chinese medicine (TCM), and Losartan as an illustration. The generalization results indicated that although SXC is less effective in lowering blood pressure than Losartan on week 2, week 4, and week 6, there is no statistically significant difference among the trial-eligible population at week 8, and the generalization is robust against potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04107v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan Jiang, Xin-xing Lai, Shu Yang, Ying Gao, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting</title>
      <link>https://arxiv.org/abs/2406.03808</link>
      <description>arXiv:2406.03808v1 Announce Type: cross 
Abstract: Photovoltaic (PV) power forecasting plays a crucial role in optimizing the operation and planning of PV systems, thereby enabling efficient energy management and grid integration. However, un certainties caused by fluctuating weather conditions and complex interactions between different variables pose significant challenges to accurate PV power forecasting. In this study, we propose PV-Client (Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting) to address these challenges and enhance PV power forecasting accuracy. PV-Client employs an ENhanced Transformer module to capture complex interactions of various features in PV systems, and utilizes a linear module to learn trend information in PV power. Diverging from conventional time series-based Transformer models that use cross-time Attention to learn dependencies between different time steps, the Enhanced Transformer module integrates cross-variable Attention to capture dependencies between PV power and weather factors. Furthermore, PV-Client streamlines the embedding and position encoding layers by replacing the Decoder module with a projection layer. Experimental results on three real-world PV power datasets affirm PV-Client's state-of-the-art (SOTA) performance in PV power forecasting. Specifically, PV-Client surpasses the second-best model GRU by 5.3% in MSE metrics and 0.9% in accuracy metrics at the Jingang Station. Similarly, PV-Client outperforms the second-best model SVR by 10.1% in MSE metrics and 0.2% in accuracy metrics at the Xinqingnian Station, and PV-Client exhibits superior performance compared to the second-best model SVR with enhancements of 3.4% in MSE metrics and 0.9% in accuracy metrics at the Hongxing Station.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03808v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Gao, Qinglong Cao, Yuntian Chen, Dongxiao Zhang</dc:creator>
    </item>
    <item>
      <title>A Noise-robust Multi-head Attention Mechanism for Formation Resistivity Prediction: Frequency Aware LSTM</title>
      <link>https://arxiv.org/abs/2406.03849</link>
      <description>arXiv:2406.03849v1 Announce Type: cross 
Abstract: The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage. However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy. To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL). The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster. The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features. Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement. FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models. In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL. The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03849v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongan Zhang, Junfeng Zhao, Jian Li, Xuanran Wang, Youzhuang Sun, Yuntian Chen, Dongxiao Zhang</dc:creator>
    </item>
    <item>
      <title>Enhanced variable selection for boosting sparser and less complex models in distributional copula regression</title>
      <link>https://arxiv.org/abs/2406.03900</link>
      <description>arXiv:2406.03900v1 Announce Type: cross 
Abstract: Structured additive distributional copula regression allows to model the joint distribution of multivariate outcomes by relating all distribution parameters to covariates. Estimation via statistical boosting enables accounting for high-dimensional data and incorporating data-driven variable selection, both of which are useful given the complexity of the model class. However, as known from univariate (distributional) regression, the standard boosting algorithm tends to select too many variables with minor importance, particularly in settings with large sample sizes, leading to complex models with difficult interpretation. To counteract this behavior and to avoid selecting base-learners with only a negligible impact, we combined the ideas of probing, stability selection and a new deselection approach with statistical boosting for distributional copula regression. In a simulation study and an application to the joint modelling of weight and length of newborns, we found that all proposed methods enhance variable selection by reducing the number of false positives. However, only stability selection and the deselection approach yielded similar predictive performance to classical boosting. Finally, the deselection approach is better scalable to larger datasets and led to a competitive predictive performance, which we further illustrated in a genomic cohort study from the UK Biobank by modelling the joint genetic predisposition for two phenotypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03900v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annika Str\"omer, Nadja Klein, Christian Staerk, Florian Faschingbauer, Hannah Klinkhammer, Andreas Mayr</dc:creator>
    </item>
    <item>
      <title>Comparing estimators of discriminative performance of time-to-event models</title>
      <link>https://arxiv.org/abs/2406.04167</link>
      <description>arXiv:2406.04167v1 Announce Type: cross 
Abstract: Predicting the timing and occurrence of events is a major focus of data science applications, especially in the context of biomedical research. Performance for models estimating these outcomes, often referred to as time-to-event or survival outcomes, is frequently summarized using measures of discrimination, in particular time-dependent AUC and concordance. Many estimators for these quantities have been proposed which can be broadly categorized as either semi-parametric estimators or non-parametric estimators. In this paper, we review various estimators' mathematical construction and compare the behavior of the two classes of estimators. Importantly, we identify a previously unknown feature of the class of semi-parametric estimators that can result in vastly over-optimistic out-of-sample estimation of discriminative performance in common applied tasks. Although these semi-parametric estimators are popular in practice, the phenomenon we identify here suggests this class of estimators may be inappropriate for use in model assessment and selection based on out-of-sample evaluation criteria. This is due to the semi-parametric estimators' bias in favor of models that are overfit when using out-of-sample prediction criteria (e.g., cross validation). Non-parametric estimators, which do not exhibit this behavior, are highly variable for local discrimination. We propose to address the high variability problem through penalized regression splines smoothing. The behavior of various estimators of time-dependent AUC and concordance are illustrated via a simulation study using two different mechanisms that produce over-optimistic out-of-sample estimates using semi-parametric estimators. Estimators are further compared using a case study using data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04167v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ying Jin, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>Bayesian sequential design of computer experiments for quantile set inversion</title>
      <link>https://arxiv.org/abs/2211.01008</link>
      <description>arXiv:2211.01008v4 Announce Type: replace-cross 
Abstract: We consider an unknown multivariate function representing a system-such as a complex numerical simulator-taking both deterministic and uncertain inputs. Our objective is to estimate the set of deterministic inputs leading to outputs whose probability (with respect to the distribution of the uncertain inputs) of belonging to a given set is less than a given threshold. This problem, which we call Quantile Set Inversion (QSI), occurs for instance in the context of robust (reliability-based) optimization problems, when looking for the set of solutions that satisfy the constraints with sufficiently large probability.   To solve the QSI problem we propose a Bayesian strategy, based on Gaussian process modeling and the Stepwise Uncertainty Reduction (SUR) principle, to sequentially choose the points at which the function should be evaluated to efficiently approximate the set of interest. We illustrate the performance and interest of the proposed SUR strategy through several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01008v4</guid>
      <category>stat.ML</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Ait Abdelmalek-Lomenech (L2S, RT-UQ), Julien Bect (L2S, RT-UQ), Vincent Chabridon (EDF R\&amp;D PRISME, RT-UQ), Emmanuel Vazquez (L2S, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>NFT Wash Trading: Direct vs. Indirect Estimation</title>
      <link>https://arxiv.org/abs/2311.18717</link>
      <description>arXiv:2311.18717v2 Announce Type: replace-cross 
Abstract: Recent studies estimate around 70% of traded value on off-chain crypto exchanges like Binance is wash trading. This paper turns to NFT markets, where the on-chain nature of transactions-a key tenet of Web3 innovation-enables more direct estimation methods to be applied. Focusing on three of the largest NFT marketplaces, we find 30-40% of NFT volume and 25-95% of traded value involve wash trading. We leverage this direct approach to critically evaluate recent indirect estimation methods suggested in the literature, revealing major differences in effectiveness, with some failing altogether. Trade-roundedness filters, as suggested in Cong et al. (2023), emerge as the most accurate indirect estimation method. In fact, we show how direct and indirect approaches can be closely aligned via hyper-parameter fine-tuning. Our findings underscore the crucial role of technological innovation in detecting and regulating financial misconduct in digital finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18717v2</guid>
      <category>econ.GN</category>
      <category>cs.CR</category>
      <category>cs.MA</category>
      <category>q-fin.EC</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brett Hemenway Falk, Gerry Tsoukalas, Niuniu Zhang</dc:creator>
    </item>
  </channel>
</rss>
