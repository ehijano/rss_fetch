<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Jan 2026 02:46:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multifaceted Scenario-Aware Hypergraph Learning for Next POI Recommendation</title>
      <link>https://arxiv.org/abs/2601.11610</link>
      <description>arXiv:2601.11610v1 Announce Type: new 
Abstract: Among the diverse services provided by Location-Based Social Networks (LBSNs), Next Point-of-Interest (POI) recommendation plays a crucial role in inferring user preferences from historical check-in trajectories. However, existing sequential and graph-based methods frequently neglect significant mobility variations across distinct contextual scenarios (e.g., tourists versus locals). This oversight results in suboptimal performance due to two fundamental limitations: the inability to capture scenario-specific features and the failure to resolve inherent inter-scenario conflicts. To overcome these limitations, we propose the Multifaceted Scenario-Aware Hypergraph Learning method (MSAHG), a framework that adopts a scenario-splitting paradigm for next POI recommendation.
  Our main contributions are:
  (1) Construction of scenario-specific, multi-view disentangled sub-hypergraphs to capture distinct mobility patterns;
  (2) A parameter-splitting mechanism to adaptively resolve conflicting optimization directions across scenarios while preserving generalization capability.
  Extensive experiments on three real-world datasets demonstrate that MSAHG consistently outperforms five state-of-the-art methods across diverse scenarios, confirming its effectiveness in multi-scenario POI recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11610v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Lin, Yongkang Li, Jie Xing, Zipei Fan</dc:creator>
    </item>
    <item>
      <title>Effective and Unsupervised Social Event Detection and Evolution via RAG and Structural Entropy</title>
      <link>https://arxiv.org/abs/2601.12035</link>
      <description>arXiv:2601.12035v1 Announce Type: new 
Abstract: With the growing scale of social media, social event detection and evolution modeling have attracted increasing attention. Graph neural networks (GNNs) and transformer-based pre-trained language models (PLMs) have become mainstream approaches in this area. However, existing methods still face three major challenges. First, the sheer volume of social media messages makes learning resource-intensive. Second, the fragmentation of social media messages often impedes the model's ability to capture a comprehensive view of the events. Third, the lack of structured temporal context has hindered the development of effective models for event evolution, limiting users' access to event information. To address these challenges, we propose a foundation model for unsupervised Social Event Detection and Evolution, namely RagSEDE. Specifically, RagSEDE introduces a representativeness- and diversity-driven sampling strategy to extract key messages from massive social streams, significantly reducing noise and computational overhead. It further establishes a novel paradigm based on Retrieval Augmented Generation (RAG) that enhances PLMs in detecting events while simultaneously constructing and maintaining an evolving event knowledge base. Finally, RagSEDE leverages structural information theory to dynamically model event evolution keywords for the first time. Extensive experiments on two public datasets demonstrate the superiority of RagSEDE in open-world social event detection and evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12035v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qitong Liu, Hao Peng, Zuchen Li, Xihang Meng, Ziyu Yang, Jiting Li, Li Sun, Philip S. Yu</dc:creator>
    </item>
    <item>
      <title>Constructing a Dataset to Support Agent-Based Modeling of Online Interactions: Users, Topics, and Interaction Networks</title>
      <link>https://arxiv.org/abs/2601.12628</link>
      <description>arXiv:2601.12628v1 Announce Type: new 
Abstract: Agent-based modeling (ABM) provides a powerful framework for exploring how individual behaviors and interactions give rise to collective social dynamics. However, most ABMs rely on handcrafted or parameterized agent rules that are not empirically grounded, thereby limiting their realism and validation against observed data. To address this gap, we constructed a large-scale, empirically grounded dataset from Reddit to support the development and evaluation of agent-based social simulations. The dataset includes 33 technology-focused, 14 climate-focused, and 7 COVID-related aggregated agents, encompassing around one million posts and comments. Using publicly available posts and comments, we define agent categories based on content and interaction patterns, derive inter-agent relationships from temporal commenting behaviors, and build a directed, weighted network that reflects empirically observed user connections. The resulting dataset enables researchers to calibrate and benchmark agent behavior, network structure, and information diffusion processes against real social dynamics. Our quantitative analysis reveals clear topic-dependent differences in how users interact. Climate discussions show dense, highly connected networks with sustained engagement, COVID-related interactions are sparse and mostly one-directional, and technology discussions are organized around a small number of central hubs. Manual qualitative analysis further shows that agent interactions follow realistic patterns of timing, similarity between users, and sentiment change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12628v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul Sittar, Miha Cesnovar, Alenka Gucek, Marko Grobelnik</dc:creator>
    </item>
    <item>
      <title>The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on Telegram</title>
      <link>https://arxiv.org/abs/2601.13294</link>
      <description>arXiv:2601.13294v1 Announce Type: new 
Abstract: Telegram has become one of the leading platforms for disseminating misinformational messages. However, many existing pipelines still classify each message's credibility based on the reputation of its associated domain names or its lexical features. Such methods work well on traditional long-form news articles published by well-known sources, but high-risk posts on Telegram are short and URL-sparse, leading to failures for link-based and standard TF-IDF models. To this end, we propose the TAG2CRED pipeline, a method designed for such short, convoluted messages. Our model will directly score each post based on the tags assigned to the text. We designed a concise label system that covers the dimensions of theme, claim type, call to action, and evidence. The fine-tuned large language model (LLM) assigns tags to messages and then maps these tags to calibrated risk scores in the [0,1] interval through L2-regularized logistic regression. We evaluated 87,936 Telegram messages associated with Media Bias/Fact Check (MBFC), using URL masking and domain disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167, outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at the same time, the number of features used in this model is much smaller, and the generalization ability on infrequent domains is stronger. The performance of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was 0.813 (Brier score 0.114). This indicates that style labels and lexical features may capture different but complementary dimensions of information risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13294v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yipeng Wang, Huy Gia Han Vu, Mohit Singhal</dc:creator>
    </item>
    <item>
      <title>The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing</title>
      <link>https://arxiv.org/abs/2601.13487</link>
      <description>arXiv:2601.13487v1 Announce Type: new 
Abstract: News consumption on social media has become ubiquitous, yet how different forms of engagement shape psychosocial outcomes remains unclear. To address this gap, we leveraged a large-scale dataset of ~26M posts and ~45M comments on the BlueSky platform, and conducted a quasi-experimental study, matching 81,345 Treated users exposed to News feeds with 83,711 Control users using stratified propensity score analysis. We examined psychosocial wellbeing, in terms of affective, behavioral, and cognitive outcomes. Our findings reveal that news engagement produces systematic trade-offs: increased depression, stress, and anxiety, yet decreased loneliness and increased social interaction on the platform. Regression models reveal that News feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting, with magnitude differences exceeding tenfold. These per-engagement effects accumulate with repeated exposure, showing significant psychosocial impacts. Our work extends theories of news effects beyond crisis-centric frameworks by demonstrating that routine consumption creates distinct psychological dynamics depending on engagement type, and bears implications for tools and interventions for mitigating the psychosocial costs of news consumption on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13487v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivia Pal, Agam Goyal, Eshwar Chandrasekharan, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>Modeling Perpetrators' Fate-to-Fate Contagion in Public Mass Shootings In The United States Using Bivariate Hawkes Processes</title>
      <link>https://arxiv.org/abs/2601.13501</link>
      <description>arXiv:2601.13501v1 Announce Type: new 
Abstract: This study examines how the fate of a perpetrator in a public mass shooting influences the fate of subsequent perpetrators. Using data from 1966 to 2024, we classify incidents according to whether the perpetrator died at the scene or survived the attack. Using a bivariate Hawkes process, we quantify the cross-excitation effect, which is the triggering effect that each event type exerts on the other, i.e., "die at the scene"$\rightarrow$ "live" and "live"$\rightarrow$ "die at the scene", as well as the self-excitation effects, i.e., "die at the scene"$\rightarrow$ "die at the scene" and "live"$\rightarrow$ "live". Our results show that the strongest spillover was from "live" incidents to "die at the scene", where we estimate that 0.34 (0.09, 0.80) of "die at the scene" incidents are triggered by a prior event in which the offender survived the attack. This pathway also exhibits the longest estimated contagion timescale: approximately 20 days. In contrast, the reverse influence, that is, "die at the scene"$\rightarrow$"live", is not statistically significant, with the lower bound of its 95% confidence interval nearly equal to zero. We also find that "die at the scene" events can only cause their own type, where 0.139 (0.01, 0.52) of such incidents are caused by previous "die at the scene" events, with the shortest contagion timescale of roughly 20 hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13501v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Youness Diouane, James Silver</dc:creator>
    </item>
    <item>
      <title>TRGCN: A Hybrid Framework for Social Network Rumor Detection</title>
      <link>https://arxiv.org/abs/2601.13573</link>
      <description>arXiv:2601.13573v1 Announce Type: new 
Abstract: Accurate and efficient rumor detection is critical for information governance, particularly in the context of the rapid spread of misinformation on social networks. Traditional rumor detection relied primarily on manual analysis. With the continuous advancement of technology, machine learning and deep learning approaches for rumor identification have gradually emerged and gained prominence. However, previous approaches often struggle to simultaneously capture both the sequential and the global structural relationships among topological nodes within a social network. To tackle this issue, we introduce a hybrid model for detecting rumors that integrates a Graph Convolutional Network (GCN) with a Transformer architecture, aiming to leverage the complementary strengths of structural and semantic feature extraction. Positional encoding helps preserve the sequential order of these nodes within the propagation structure. The use of Multi-head attention mechanisms enables the model to capture features across diverse representational subspaces, thereby enhancing both the richness and depth of text comprehension. This integration allows the framework to concurrently identify the key propagation network of rumors, the textual content, the long-range dependencies, and the sequence among propagation nodes. Experimental evaluations on publicly available datasets, including Twitter 15 and Twitter 16, demonstrate that our proposed fusion model significantly outperforms both standalone models and existing mainstream methods in terms of accuracy. These results validate the effectiveness and superiority of our approach for the rumor detection task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13573v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanqin Yan, Suiyu Zhang, Dingguo Yu, Yijie Zhou, Cheng-Jun Wang, Ke-ke Shang</dc:creator>
    </item>
    <item>
      <title>Consensus Stability of Community Notes on X</title>
      <link>https://arxiv.org/abs/2601.14002</link>
      <description>arXiv:2601.14002v1 Announce Type: new 
Abstract: Community-based fact-checking systems, such as Community Notes on X (formerly Twitter), aim to mitigate online misinformation by surfacing annotations judged helpful by contributors with diverse viewpoints. While prior work has shown that the platform's bridging-based algorithm effectively selects helpful notes at the time of display, little is known about how evaluations change after notes become visible. Using a large-scale dataset of 437,396 community notes and 35 million ratings from over 580,000 contributors, we examine the stability of helpful notes and the rating dynamics that follow their initial display. We find that 30.2% of displayed notes later lose their helpful status and disappear. Using interrupted time series models, we further show that note display triggers a sharp increase in rating volume and a significant shift in rating leaning, but these effects differ across rater groups. Contributors with viewpoints similar to note authors tend to increase supportive ratings, while dissimilar contributors increase negative ratings, producing systematic post-display polarization. Counterfactual analyses suggest that this post-display polarization, particularly from dissimilar raters, plays a substantial role in note disappearance. These findings highlight the vulnerability of consensus-based fact-checking systems to polarized rating behavior and suggest pathways for improving their resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14002v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3774904.3792987</arxiv:DOI>
      <dc:creator>Yuwei Chuai, Gabriele Lenzini, Nicolas Pr\"ollochs</dc:creator>
    </item>
    <item>
      <title>Beyond Polarization: Opinion Mixing and Social Influence in Deliberation</title>
      <link>https://arxiv.org/abs/2601.14221</link>
      <description>arXiv:2601.14221v1 Announce Type: new 
Abstract: Deliberative processes are often discussed as increasing or decreasing polarization. This approach misses a different, and arguably more diagnostic, dimension of opinion change: whether deliberation reshuffles who agrees with whom, or simply moves everyone in parallel while preserving the pre-deliberation rank ordering. We introduce \opinion mixing, measured by Kendall's rank correlation (\tau) between pre- and post-deliberation responses, as a complement to variance-based polarization metrics. Across two large online deliberative polls spanning 32 countries (MCF-2022: n=6,342; MCF-2023: n=1,529), deliberation increases opinion mixing relative to survey-only controls: treatment groups exhibit lower rank correlation on (97%) and (93%) of opinion questions, respectively. Polarization measures based on variance tell a more heterogeneous story: controls consistently converge, while treated groups sometimes converge and sometimes diverge depending on the issue.
  To probe mechanisms, we link transcripts and surveys in a third event (SOF: (n=617), 116 groups) and use LLM-assisted coding of 6,232 discussion statements. Expressed support in discussion statements strongly predicts subsequent group-level opinion shifts; this correlation is amplified by justification quality in the statements but not by argument novelty. To our knowledge, we are the first to observe how different notions of argument quality have different associations with the outcome of deliberation. This suggests that opinion change after deliberation is related to selective uptake of well-reasoned arguments, producing complex patterns of opinion reorganization that standard polarization metrics may miss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14221v1</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohak Goyal, Lodewijk Gelauff, Naman Gupta, Ashish Goel, Kamesh Munagala</dc:creator>
    </item>
    <item>
      <title>Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets</title>
      <link>https://arxiv.org/abs/2509.02908</link>
      <description>arXiv:2509.02908v1 Announce Type: cross 
Abstract: Individuals from sexual and gender minority groups experience disproportionately high rates of poor health outcomes and mental disorders compared to their heterosexual and cisgender counterparts, largely as a consequence of minority stress as described by Meyer's (2003) model. This study presents the first comprehensive evaluation of transformer-based architectures for detecting minority stress in online discourse. We benchmark multiple transformer models including ELECTRA, BERT, RoBERTa, and BART against traditional machine learning baselines and graph-augmented variants. We further assess zero-shot and few-shot learning paradigms to assess their applicability on underrepresented datasets. Experiments are conducted on the two largest publicly available Reddit corpora for minority stress detection, comprising 12,645 and 5,789 posts, and are repeated over five random seeds to ensure robustness. Our results demonstrate that integrating graph structure consistently improves detection performance across transformer-only models and that supervised fine-tuning with relational context outperforms zero and few-shot approaches. Theoretical analysis reveals that modeling social connectivity and conversational context via graph augmentation sharpens the models' ability to identify key linguistic markers such as identity concealment, internalized stigma, and calls for support, suggesting that graph-enhanced transformers offer the most reliable foundation for digital health interventions and public health policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02908v1</guid>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13278-025-01521-z</arxiv:DOI>
      <dc:creator>Santosh Chapagain, Cory J Cascalheira, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi, Jillian R. Scheer</dc:creator>
    </item>
    <item>
      <title>Logarithmic scaling and stochastic criticality in collective attention</title>
      <link>https://arxiv.org/abs/2601.12306</link>
      <description>arXiv:2601.12306v1 Announce Type: cross 
Abstract: We uncover a universal scaling law governing the dispersion of collective attention and identify its underlying stochastic criticality. By analysing large-scale ensembles of Wikipedia page views, we find that the variance of logarithmic attention grows ultraslowly, $\operatorname{Var}[\ln{X(t)}]\propto\ln{t}$, in sharp contrast to the power-law scaling typically expected for diffusive processes. We show that this behaviour is captured by a minimal stochastic differential equation driven by fractional Brownian motion, in which long-range memory ($H$) and temporal decay of volatility ($\eta$) enter through the single exponent $\xi\equiv H-\eta$. At marginality, $\xi=0$, the variance grows logarithmically, marking the critical boundary between power-law growth ($\xi&gt;0$) and saturation ($\xi&lt;0$). By incorporating article-level heterogeneity through a Gaussian mixture model, we further reconstruct the empirical distribution of cumulative attention within the same framework. Our results place collective attention in a distinct class of non-Markovian stochastic processes, with close affinity to ageing-like and ultraslow dynamics in glassy systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12306v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keisuke Okamura</dc:creator>
    </item>
    <item>
      <title>Emergence of Structural Disparities in theWeb of Scientific Citations</title>
      <link>https://arxiv.org/abs/2601.12665</link>
      <description>arXiv:2601.12665v1 Announce Type: cross 
Abstract: Scientific attention is unevenly distributed, creating inequities in recognition and distorting access to opportunities. Using citations as a proxy, we quantify disparities in attention by gender and institutional prestige. We find that women receive systematically fewer citations than men, and that attention is increasingly concentrated among authors from elite institutions -- patterns not fully explained by underrepresentation alone. To explain these dynamics, we introduce a model of citation network growth that incorporates homophily (tendency to cite similar authors), preferential attachment (favoring highly cited authors) and group size (underrepresentation). The model shows that disparities arise not only from group size imbalances but also from cumulative advantage amplifying biased citation preferences. Importantly, increasing representation alone is often insufficient to reduce disparities. Effective strategies should also include reducing homophily, amplifying the visibility of underrepresented groups, and supporting equitable integration of newcomers. Our findings highlight the challenges of mitigating inequities in asymmetric networks like citations, where recognition flows in one direction. By making visible the mechanisms through which attention is distributed, we contribute to efforts toward a more responsible web of science that is fairer, more transparent, and more inclusive, and that better sustains innovation and knowledge production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12665v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buddhika Nettasinghe, Nazanin Alipourfard, Vikram Krishnamurthy, Kristina Lerman</dc:creator>
    </item>
    <item>
      <title>Persuasion in Online Conversations Is Associated with Alignment in Expressed Human Values</title>
      <link>https://arxiv.org/abs/2601.12685</link>
      <description>arXiv:2601.12685v1 Announce Type: cross 
Abstract: Online disagreements often fail to produce understanding, instead reinforcing existing positions or escalating conflict. Prior work on predictors of successful persuasion in online discourse has largely focused on surface features such as linguistic style or conversational structure, leaving open the role of underlying principles or concerns that participants bring to an interaction. In this paper, we investigate how the expression and alignment of human values in back-and-forth online discussions relate to persuasion. Using data from Reddit's ChangeMyView subreddit, where successful persuasion is explicitly signaled through the awarding of deltas, we analyze one-on-one exchanges and characterize participants' value expression by drawing from Schwartz's Refined Theory of Basic Human Values. We find that successful persuasion is associated with two complementary processes: pre-existing compatibility between participants' value priorities even before the exchange happens, and the emergence of value alignment over the course of a conversation. At the same time, successful persuasion does not depend on commenters making large departures from their typical value expression patterns. We discuss implications of our findings for the design of online social platforms that aim to support constructive engagement across disagreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12685v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bhavesh Vuyyuru, Farnaz Jahanbakhsh</dc:creator>
    </item>
    <item>
      <title>Modelling viable supply networks with cooperative adaptive financing</title>
      <link>https://arxiv.org/abs/2601.13210</link>
      <description>arXiv:2601.13210v1 Announce Type: cross 
Abstract: We propose a financial liquidity policy sharing method for firm-to-firm supply networks, introducing a scalable autonomous control function for viable complex adaptive supply networks. Cooperation and competition in supply chains is reconciled through overlapping collaborative sets, making firms interdependent and enabling distributed risk governance. How cooperative range - visibility - affects viability is studied using dynamic complex adaptive systems modelling. We find that viability needs cooperation; visibility and viability grow together in scale-free supply networks; and distributed control, where firms only have limited partner information, outperforms centralised control. This suggests that policy toward network viability should implement distributed supply chain financial governance, supporting interfirm collaboration, to enable autonomous control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13210v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>econ.TH</category>
      <category>eess.SY</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaniv Proselkov, Liming Xu, Alexandra Brintrup</dc:creator>
    </item>
    <item>
      <title>RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions</title>
      <link>https://arxiv.org/abs/2601.13235</link>
      <description>arXiv:2601.13235v1 Announce Type: cross 
Abstract: Caregivers seeking AI-mediated support express complex needs -- information-seeking, emotional validation, and distress cues -- that warrant careful evaluation of response safety and appropriateness. Existing AI evaluation frameworks, primarily focused on general risks (toxicity, hallucinations, policy violations, etc), may not adequately capture the nuanced risks of LLM-responses in caregiving-contexts. We introduce RubRIX (Rubric-based Risk Index), a theory-driven, clinician-validated framework for evaluating risks in LLM caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Inattention, Bias &amp; Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected. Rubric-guided refinement consistently reduced risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts. Our findings highlight the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts. We release benchmark datasets to enable future research on contextual risk evaluation in AI-mediated support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13235v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Drishti Goel, Jeongah Lee, Qiuyue Joy Zhong, Violeta J. Rodriguez, Daniel S. Brown, Ravi Karkar, Dong Whi Yoo, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>CooperBench: Why Coding Agents Cannot be Your Teammates Yet</title>
      <link>https://arxiv.org/abs/2601.13295</link>
      <description>arXiv:2601.13295v1 Announce Type: cross 
Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13295v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</dc:creator>
    </item>
    <item>
      <title>Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse</title>
      <link>https://arxiv.org/abs/2601.13317</link>
      <description>arXiv:2601.13317v1 Announce Type: cross 
Abstract: Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13317v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samantha Sudhoff, Pranav Perumal, Zhaoqing Wu, Tunazzina Islam</dc:creator>
    </item>
    <item>
      <title>Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games</title>
      <link>https://arxiv.org/abs/2601.13709</link>
      <description>arXiv:2601.13709v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13709v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Kao, Vanshika Vats, James Davis</dc:creator>
    </item>
    <item>
      <title>A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems</title>
      <link>https://arxiv.org/abs/2601.13772</link>
      <description>arXiv:2601.13772v1 Announce Type: cross 
Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13772v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Vaccargiu, Azmat Ullah, Pierluigi Gallo</dc:creator>
    </item>
    <item>
      <title>Block-Fitness Modeling of the Global Air Mobility Network</title>
      <link>https://arxiv.org/abs/2601.13867</link>
      <description>arXiv:2601.13867v1 Announce Type: cross 
Abstract: Accurate representations of the World Air Transportation Network (WAN) are fundamental inputs to models of global mobility, epidemic risk, and infrastructure planning. However, high-resolution, real-time data on the WAN are largely commercial and proprietary, therefore often inaccessible to the research community. Here we introduce a generative model of the WAN that treats air travel as a stochastic process within a maximum-entropy framework. The model uses airport-level passenger flows to probabilistically generate connections while preserving traffic volumes across geographic regions. The resulting reconstructed networks reproduce key structural properties of the WAN and enable simulations of dynamic spreading that closely match those obtained using the real network. Our approach provides a scalable, interpretable, and computationally efficient framework for forecasting and policy design in global mobility systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13867v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Fischetti, Anna Mancini, Giulio Cimini, Jessica T. Davis, Abby Leung, Alessandro Vespignani, Guido Caldarelli</dc:creator>
    </item>
    <item>
      <title>Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure</title>
      <link>https://arxiv.org/abs/2601.14047</link>
      <description>arXiv:2601.14047v1 Announce Type: cross 
Abstract: Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14047v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>econ.TH</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexey V. Osipov, Nikolay N. Osipov</dc:creator>
    </item>
    <item>
      <title>Predicting Movie Success with Multi-Task Learning: A Hybrid Framework Combining GPT-Based Sentiment Analysis and SIR Propagation</title>
      <link>https://arxiv.org/abs/2509.02809</link>
      <description>arXiv:2509.02809v3 Announce Type: replace 
Abstract: This study presents a hybrid framework for predicting movie success. The framework integrates multi-task learning (MTL), GPT-based sentiment analysis, and Susceptible-Infected-Recovered (SIR) propagation modeling. The study examines limitations in existing approaches. It models static production attributes, information dissemination, and audience sentiment at the same time. The framework uses 5,840 films from 2004 to 2024 and approximate 300,000 user reviews. It shows predictive performance with classification accuracy of 0.964 and regression metrics of MAE 0.388. Ablation analysis indicates component interactions. Selective feature combinations perform better than the comprehensive model. This result questions assumptions about feature integration. The model shows virality patterns between successful and unsuccessful films. Innovations include epidemiological modeling for information diffusion, multidimensional sentiment features from GPT-based analysis, and a shared representation architecture that optimizes multiple success metrics. The framework provides applications in the film production lifecycle. It also contributes to understanding how audience engagement leads to commercial outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02809v3</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlan Xie</dc:creator>
    </item>
    <item>
      <title>MASH: A Multiplatform and Multimodal Annotated Dataset for Societal Impact of Hurricane</title>
      <link>https://arxiv.org/abs/2509.23627</link>
      <description>arXiv:2509.23627v2 Announce Type: replace 
Abstract: Natural disasters cause multidimensional threats to human societies, with hurricanes exemplifying one of the most disruptive events that not only caused severe physical damage but also sparked widespread discussion on social media platforms. Existing datasets for studying societal impacts of hurricanes often focus on outdated hurricanes and are limited to a single social media platform, failing to capture the broader societal impact in today's diverse social media environment. Moreover, existing datasets annotate visual and textual content of the post separately, failing to account for the multimodal nature of social media posts. To address these gaps, we present a multiplatform and Multimodal Annotated Dataset for Societal Impact of Hurricane (MASH) that includes 59,607 relevant social media data posts from Reddit, TikTok, and YouTube. In addition, all relevant social media data posts are annotated in a multimodal approach that considers both textual and visual content on three dimensions: Humanitarian Classes, Bias Classes, and Information Integrity Classes. To our best knowledge, MASH is the first large-scale, multi-platform, multimodal, and multi-dimensionally annotated dataset centered on hurricane disasters. In addition, we introduce an online platform that supports interactive data exploration, provides preliminary analytical results, and allows users to share their insights regarding the societal impacts of hurricanes. We envision that MASH can contribute to the study of hurricanes' impact on society, such as disaster response, disaster severity classification, public sentiment analysis, disaster policy making, and bias identification. The dataset is publicly available at https://huggingface.co/datasets/YRC10/MASH under the Creative Commons Attribution 4.0 (CC BY 4.0) license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23627v2</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruichen Yao, Aslanbek Murzakhmetov, Raaghav Pillai, Aliya Maussymbayeva, Zelin Li, Yifan Liu, Yaokun Liu, Lanyu Shang, Yang Zhang, Na Wei, Ximing Cai, Dong Wang</dc:creator>
    </item>
    <item>
      <title>FTSCommDetector: Discovering Behavioral Communities through Temporal Synchronization</title>
      <link>https://arxiv.org/abs/2510.00014</link>
      <description>arXiv:2510.00014v3 Announce Type: replace 
Abstract: Why do trillion-dollar tech giants AAPL and MSFT diverge into different response patterns during market disruptions despite identical sector classifications? This paradox reveals a fundamental limitation: traditional community detection methods fail to capture synchronization-desynchronization patterns where entities move independently yet align during critical moments. To this end, we introduce FTSCommDetector, implementing our Temporal Coherence Architecture (TCA) to discover similar and dissimilar communities in continuous multivariate time series. Unlike existing methods that process each timestamp independently, causing unstable community assignments and missing evolving relationships, our approach maintains coherence through dual-scale encoding and static topology with dynamic attention. Furthermore, we establish information-theoretic foundations demonstrating how scale separation maximizes complementary information and introduce Normalized Temporal Profiles (NTP) for scale-invariant evaluation. As a result, FTSCommDetector achieves consistent improvements across four diverse financial markets (SP100, SP500, SP1000, Nikkei 225), with gains ranging from 3.5% to 11.1% over the strongest baselines. The method demonstrates remarkable robustness with only 2% performance variation across window sizes from 60 to 120 days, making dataset-specific tuning unnecessary, providing practical insights for portfolio construction and risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00014v3</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyang Luo, Xikun Zhang, Dongjin Song</dc:creator>
    </item>
    <item>
      <title>Unsupervised Multimodal Graph-based Model for Geo-social Analysis</title>
      <link>https://arxiv.org/abs/2512.03063</link>
      <description>arXiv:2512.03063v2 Announce Type: replace 
Abstract: The systematic analysis of user-generated social media content, especially when enriched with geospatial context, plays a vital role in domains such as disaster management and public opinion monitoring. Although multimodal approaches have made significant progress, most existing models remain fragmented, processing each modality separately rather than integrating them into a unified end-to-end model. To address this, we propose an unsupervised, multimodal graph-based methodology that jointly embeds semantic and geographic information into a shared representation space. The proposed methodology comprises two architectural paradigms: a mono graph (MonoGrah) model that jointly encodes both modalities, and a multi graph (MultiGraph) model that separately models semantic and geographic relationships and subsequently integrates them through multi-head attention mechanisms. A composite loss, combining contrastive, coherence, and alignment objectives, guides the learning process to produce semantically coherent and spatially compact clusters. Experiments on four real-world disaster datasets demonstrate that our models consistently outperform existing baselines in topic quality, spatial coherence, and interpretability. Inherently domain-independent, the framework can be readily extended to diverse forms of multimodal data and a wide range of downstream analysis tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03063v2</guid>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsaneddin Jalilian, Bernd Resch</dc:creator>
    </item>
    <item>
      <title>The direct democracy paradox: Microtargeting and issue ownership in Swiss online political ads</title>
      <link>https://arxiv.org/abs/2512.14564</link>
      <description>arXiv:2512.14564v2 Announce Type: replace 
Abstract: Political advertising on social media has fundamentally reshaped democratic deliberation, playing a central role in electoral campaigns and propaganda. However, its systemic impact remains largely theoretical or unexplored, raising critical concerns about institutional fairness and algorithmic transparency. This paper provides the first data-driven analysis of the relationship between direct democracy and political advertising on social media, leveraging a novel dataset of 40,000 political ads published on Meta in Switzerland between 2021 and 2025. Switzerland's system of direct democracy, characterized by frequent referenda, provides an ideal context for examining this relationship beyond standard electoral cycles. The results reveal the sheer scale of digital campaigning, with 560 million impressions targeting 5.6 million voters, and suggest that greater exposure to "pro-Yes" advertising significantly correlates with referendum approval outcomes. Demographic microtargeting analysis suggests partisan strategies: Centrist and right-wing parties predominantly target older men, whereas left-wing parties focus on young women. Regarding textual content, a clear pattern of "talking past each other" is identified; in line with the issue ownership theory, parties avoid debating shared issues, preferring to promote exclusively owned topics. Furthermore, the parties' strategies are so distinctive that a machine learning model trained only on audience and topic features can accurately predict the author of an advertisement. This article highlights how demographic microtargeting, issue divergence, and tailored messages could undermine democratic deliberation, exposing a paradox: Referenda are designed to be the ultimate expression of the popular will, yet they are highly susceptible to invisible algorithmic persuasion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14564v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Capozzi</dc:creator>
    </item>
    <item>
      <title>FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks</title>
      <link>https://arxiv.org/abs/2601.09394</link>
      <description>arXiv:2601.09394v2 Announce Type: replace 
Abstract: Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09394v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang Hou, Enyan Dai, Feng Xia</dc:creator>
    </item>
    <item>
      <title>MMT: A Multilingual and Multi-Topic Indian Social Media Dataset</title>
      <link>https://arxiv.org/abs/2304.00634</link>
      <description>arXiv:2304.00634v2 Announce Type: replace-cross 
Abstract: Social media plays a significant role in cross-cultural communication. A vast amount of this occurs in code-mixed and multilingual form, posing a significant challenge to Natural Language Processing (NLP) tools for processing such information, like language identification, topic modeling, and named-entity recognition. To address this, we introduce a large-scale multilingual, and multi-topic dataset (MMT) collected from Twitter (1.7 million Tweets), encompassing 13 coarse-grained and 63 fine-grained topics in the Indian context. We further annotate a subset of 5,346 tweets from the MMT dataset with various Indian languages and their code-mixed counterparts. Also, we demonstrate that the currently existing tools fail to capture the linguistic diversity in MMT on two downstream tasks, i.e., topic modeling and language identification. To facilitate future research, we have make the anonymized and annotated dataset available at https://huggingface.co/datasets/LingoIITGN/MMT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00634v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>EACL Workshop C3NLP 2023</arxiv:journal_reference>
      <dc:creator>Dwip Dalal, Vivek Srivastava, Mayank Singh</dc:creator>
    </item>
    <item>
      <title>Network-Level Measures of Mobility from Aggregated Origin-Destination Data</title>
      <link>https://arxiv.org/abs/2502.04162</link>
      <description>arXiv:2502.04162v2 Announce Type: replace-cross 
Abstract: We introduce a framework for defining and interpreting collective mobility measures from spatially and temporally aggregated origin--destination (OD) data. Rather than characterizing individual behavior, these measures describe properties of the mobility system itself: how network organization, spatial structure, and routing constraints shape and channel population movement. In this view, aggregate mobility flows reveal aspects of connectivity, functional organization, and large-scale daily activity patterns encoded in the underlying transport and spatial network.
  To support interpretation and provide a controlled reference for the proposed time-elapsed calculations, we first employ an independent, network-driven synthetic data generator in which trajectories arise from prescribed system structure rather than observed data. This controlled setting provides a concrete reference for understanding how the proposed measures reflect network organization and flow constraints.
  We then apply the measures to fully anonymized data from the NetMob 2024 Data Challenge, examining their behavior under realistic limitations of spatial and temporal aggregation. While such data constraints restrict dynamical resolution, the resulting metrics still exhibit interpretable large-scale structure and temporal variation at the city scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04162v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alisha Foster, David A. Meyer, Asif Shakeel</dc:creator>
    </item>
    <item>
      <title>Global evidence for a consistent spatial footprint of intra-urban centers</title>
      <link>https://arxiv.org/abs/2503.06445</link>
      <description>arXiv:2503.06445v2 Announce Type: replace-cross 
Abstract: Urban space is highly heterogeneous, with population and human activities concentrating in localized centers. However, the global organization of such intra-urban centers remains poorly understood due to the lack of consistent, comparable data. Here we develop a scalable geospatial framework to identify intra-urban activity centers worldwide using nighttime light observations. Applying this approach to more than 9,500 cities, we construct a high-resolution global dataset of over 15,000 centers. We uncover a striking regularity: despite vast differences in city size, regional development, and population density, the built-up area associated with individual centers remains remarkably consistent. Across cities, total urban area scales proportionally with the number of centers, yielding a stable mean spatial footprint. This regularity holds at the micro-scale, where Voronoi-based service areas exhibit a characteristic size that is persistent across countries and independent of local population concentration. As a geometric consequence, this polycentric multiplication maintains stable average distances to the nearest center as cities expand, preventing the accessibility decay inherent in monocentric growth. These findings reveal a universal organizing principle whereby urban expansion is accommodated through the replication of activity centers with a consistent spatial extent, providing a new empirical foundation for understanding the nature of urban growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06445v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Pang, Junlong Zhang, Yu Liu, Lei Dong</dc:creator>
    </item>
    <item>
      <title>Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions</title>
      <link>https://arxiv.org/abs/2505.08464</link>
      <description>arXiv:2505.08464v2 Announce Type: replace-cross 
Abstract: Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08464v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar, Nagendra Kumar</dc:creator>
    </item>
    <item>
      <title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
      <link>https://arxiv.org/abs/2509.05362</link>
      <description>arXiv:2509.05362v4 Announce Type: replace-cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05362v4</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ismail Hossain, Sai Puppala, Md Jahangir Alam, Sajedul Talukder</dc:creator>
    </item>
    <item>
      <title>Navigating the Ethics of Internet Measurement: Researchers' Perspectives from a Case Study in the EU</title>
      <link>https://arxiv.org/abs/2511.10408</link>
      <description>arXiv:2511.10408v2 Announce Type: replace-cross 
Abstract: Internet measurement research is essential for understanding, improving, and securing Internet infrastructure. However, its methods often involve large-scale data collection and user observation, raising complex ethical questions. While recent research has identified ethical challenges in Internet measurement research and laid out best practices, little is known about how researchers actually make ethical decisions in their research practice. To understand how these practices take shape day-to-day from the perspective of Internet measurement researchers, we interviewed 16 researchers from an Internet measurement research group in the EU. Through thematic analysis, we find that researchers deal with five main ethical challenges: privacy and consent issues, the possibility of unintended harm, balancing transparency with security and accountability, uncertain ethical boundaries, and hurdles in the ethics review process. Researchers address these by lab testing, rate limiting, setting up clear communication channels, and relying heavily on mentors and colleagues for guidance. Researchers express that ethical requirements vary across institutions, jurisdictions and conferences, and ethics review boards often lack the technical knowledge to evaluate Internet measurement research. We also highlight the invisible labor of Internet measurement researchers and describe their ethics practices as craft knowledge, both of which are crucial in upholding responsible research practices in the Internet measurement community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10408v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahibzada Farhan Amin, Sana Athar, Anja Feldmann, Ha Dao, Mannat Kaur</dc:creator>
    </item>
  </channel>
</rss>
