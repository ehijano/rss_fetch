<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Aug 2025 04:01:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Quantile estimation of CO2 marginal abatement cost across emission-generating technologies</title>
      <link>https://arxiv.org/abs/2508.11912</link>
      <description>arXiv:2508.11912v1 Announce Type: new 
Abstract: Marginal abatement cost (MAC) is a critical metric for designing efficient and cost-effective mitigation policies. However, existing MAC estimates are typically derived under different assumptions about emission-generating technologies, yet few studies have systematically compared these technologies. Moreover, conventional estimators often exhibit biases arising from limited abatement options, production inefficiencies, and data noise. To address these limitations, this paper analyzes the abatement behavior of three emission-generating technologies: by-production, joint disposability, and weak G-disposability, each consistent with the material balance principle. We employ both full and quantile frontier estimation methods to identify optimal abatement strategies. Using data from U.S. coal-fired power plants in 2022, the empirical results suggest that reducing electricity output, rather than cutting emission-generating inputs such as fossil fuels, provides a more cost-effective mitigation pathway. Furthermore, Monte Carlo simulations demonstrate that the quantile estimator consistently delivers more accurate results than the full frontier estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11912v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haleh Delnava, Sheng Dai</dc:creator>
    </item>
    <item>
      <title>A Wavelet-Based Framework for Mapping Long Memory in Resting-State fMRI: Age-Related Changes in the Hippocampus from the ADHD-200 Dataset</title>
      <link>https://arxiv.org/abs/2508.11920</link>
      <description>arXiv:2508.11920v1 Announce Type: new 
Abstract: Functional magnetic resonance imaging (fMRI) time series are known to exhibit long-range temporal dependencies that challenge traditional modeling approaches. In this study, we propose a novel computational pipeline to characterize and interpret these dependencies using a long-memory (LM) framework, which captures the slow, power-law decay of autocorrelation in resting-state fMRI (rs-fMRI) signals. The pipeline involves voxelwise estimation of LM parameters via a wavelet-based Bayesian method, yielding spatial maps that reflect temporal dependence across the brain. These maps are then projected onto a lower-dimensional space via a composite basis and are then related to individual-level covariates through group-level regression. We applied this approach to the ADHD-200 dataset and found significant positive associations between age in children and the LM parameter in the hippocampus, after adjusting for ADHD symptom severity and medication status. These findings complement prior neuroimaging work by linking long-range temporal dependence to developmental changes in memory-related brain regions. Overall, the proposed methodology enables detailed mapping of intrinsic temporal dynamics in rs-fMRI and offers new insights into the relationship between functional signal memory and brain development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11920v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasaman Shahhosseini, C\'edric Beaulac, Farouk S. Nathoo, Michelle F. Miranda</dc:creator>
    </item>
    <item>
      <title>Graph Neural Poisson Models for Supply Chain Relationship Forecasting</title>
      <link>https://arxiv.org/abs/2508.12044</link>
      <description>arXiv:2508.12044v1 Announce Type: new 
Abstract: In supply chain networks, firms dynamically form or dissolve partnerships to adapt to market fluctuations, posing a challenge for predicting future supply relationships. We model the occurrence of supply edges (firm i to firm j) as a non-homogeneous Poisson process (NHPP), using historical event counts to estimate the Poisson intensity function up to time t. However, forecasting future intensities is hindered by the limitations of historical data alone. To overcome this, we propose a novel Graph Double Exponential Smoothing (GDES) model, which integrates graph neural networks (GNNs) with a nonparametric double exponential smoothing approach to predict the probability of future supply edge formations.Recognizing the interdependent economic dynamics between upstream and downstream firms, we assume that the Poisson intensity functions of supply edges are correlated, aligning with the non-homogeneous nature of the process.Our model is interpretable, decomposing intensity increments into contributions from the current edge's historical data and influences from neighboring edges in the supply chain network. Evaluated on a large-scale supply chain dataset with 87,969 firms, our approach achieves an AUC of 93.84 % in dynamic link prediction, demonstrating its effectiveness in capturing complex supply chain interactions for accurate forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12044v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ling Xiang, Quan Hu, Xiang Zhang, Wei Lan, Bin Liu</dc:creator>
    </item>
    <item>
      <title>Latent Spatial Heterogeneity in U.S. Cancer Mortality: A Multi-Site Clustering and Spatial Autocorrelation Analysis</title>
      <link>https://arxiv.org/abs/2508.12188</link>
      <description>arXiv:2508.12188v1 Announce Type: new 
Abstract: This research set out to explore and delineate spatial patterns and mortality distributions for various cancer types across U.S. states between 1999 and 2021. The aim was to uncover region-specific cancer burdens and inform geographically targeted prevention efforts. We analyzed state-level cancer mortality records sourced from the CDC WONDER platform, concentrating on cancer sites consistently reported across the 48 contiguous states and Washington, D.C., excluding Hawaii, Alaska, and Puerto Rico. Multivariate clustering using Mahalanobis distance grouped states according to similarities in mortality profiles. Spatial autocorrelation was examined for each cancer type using both Global Moran's I and Local Indicators of Spatial Association (LISA). Additionally, the Getis-Ord statistic was applied to detect cancer-specific hotspots and cold spots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12188v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. Kubuafor, D. Baidoo, A. Duah, R. Amevor, O. J. Okeke, D. Quaye, P. O. Appiah</dc:creator>
    </item>
    <item>
      <title>Forecasting Extreme Day and Night Heat in Paris</title>
      <link>https://arxiv.org/abs/2508.12886</link>
      <description>arXiv:2508.12886v1 Announce Type: new 
Abstract: As a demonstration of concept, quantile gradient boosting is used to forecast diurnal and nocturnal Q(.90) air temperatures for Paris, France during late the spring and summer months of 2020. The data are provided by the Paris-Montsouris weather station. Q(.90) values are estimated because the 90th percentile requires that the temperatures be relatively rare and extreme. Predictors include seven routinely collected indicators of weather conditions, lagged by 14 days; the temperature forecasts are produced two weeks in advance. Conformal prediction regions capture forecasting uncertainty with provably valid properties. For both diurnal and nocturnal temperatures, forecasting accuracy is promising, and sound measures of uncertainty are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12886v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>Relationship Between Leisure Activities, Stress Management Methods, Study Methods, and Methods of Learning New Things Among First-Year Statistics Students</title>
      <link>https://arxiv.org/abs/2508.11726</link>
      <description>arXiv:2508.11726v1 Announce Type: cross 
Abstract: The interplay between leisure activities, stress management methods, studying methods, and methods of learning new things is crucial and affects performance in all aspects of life. On the other hand, data science and statistics are rapidly growing fields with high demands across universities. Thus, this study aimed to identify the similarities and dissimilarities between the four dimensions: leisure activities, stress management methods, studying methods and methods of learning new things. The participants of this study were first-year undergraduates studying statistics at one of the universities in Sri Lanka. There were 117 students in the sample (female-65, male-52). A self-reported questionnaire was used to collect data. First, individual responses for each question under each dimension were visualized using tile maps separately for males and females to identify similarities and dissimilarities in responses. Next, individuals were clustered based on the responses for each dimension separately. Finally, all resulting clusters were re-clustered to identify the relationships between the dimensions. In all cluster analyses, we used Jaccard distance with hierarchical clustering using the complete linkage method. The results were visualized using tile maps. Across all four dimensions we considered, the top activities were either listening to music or lectures and watching videos or TV shows, suggesting that individuals are introverts and passive learners. There was no strong relationship between these dimensions. By identifying these clusters and relationships, educators can tailor instructional approaches to enhance engagement and effectiveness in diverse learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11726v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thiyanga S. Talagala</dc:creator>
    </item>
    <item>
      <title>Binomial maps: stochastically evolving iterated integer maps for finite population</title>
      <link>https://arxiv.org/abs/2508.11974</link>
      <description>arXiv:2508.11974v1 Announce Type: cross 
Abstract: Deterministic nature of iterated map models of population dynamics is justifiable for infinite-sized populations, as the stochastic fluctuations are negligible in this limit. However, they are ill-suited for finite-population systems where finite-size effects like demographic noise and extinction cannot be ignored. Adding noise to the equations cannot model the demographic noise as it can only represent environmental stochasticity. An approach, sometimes used in ecological literature, but surprisingly uncommon in dynamical systems community, is \emph{Binomial maps}, which allow stochastic evolution of deterministic iterated map models of population. Here we present their formulation in a way so as to make their connection to the agent-based models explicit, and demonstrate it for the Logistic and Ricker maps. We also show that the Binomial maps are not completely equivalent to their deterministic counterparts, and derive sufficient conditions under which the equivalence holds. This approach enables rigorous finite-population analysis within familiar map-based models, opening the door to systematic study of extinction risk, quasi-stationarity, and noise-induced transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11974v1</guid>
      <category>q-bio.PE</category>
      <category>nlin.CD</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Snehal M. Shekatkar</dc:creator>
    </item>
    <item>
      <title>Citation accuracy, citation noise, and citation bias: A foundation of citation analysis</title>
      <link>https://arxiv.org/abs/2508.12735</link>
      <description>arXiv:2508.12735v1 Announce Type: cross 
Abstract: Citation analysis is widely used in research evaluation to assess the impact of scientific papers. These analyses rest on the assumption that citation decisions by authors are accurate, representing flow of knowledge from cited to citing papers. However, in practice, researchers often cite for reasons other than attributing intellectual credit to previous research. Citations made for rhetorical reasons or without reading the cited work compromise the value of citations as instrument for research evaluation. Past research on threats to the accuracy of citations has mainly focused on citation bias as the primary concern. In this paper, we argue that citation noise - the undesirable variance in citation decisions - represents an equally critical but underexplored challenge in citation analysis. We define and differentiate two types of citation noise: citation level noise and citation pattern noise. Each type of noise is described in terms of how it arises and the specific ways it can undermine the validity of citation-based research assessments. By conceptually differing citation noise from citation accuracy and citation bias, we propose a framework for the foundation of citation analysis. We discuss strategies and interventions to minimize citation noise, aiming to improve the reliability and validity of citation analysis in research evaluation. We recommend that the current professional reform movement in research evaluation such as the Coalition for Advancing Research Assessment (CoARA) pick up these strategies and interventions as an additional building block for careful, responsible use of bibliometric indicators in research evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12735v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lutz Bornmann, Christian Leibel</dc:creator>
    </item>
    <item>
      <title>Multilingual hierarchical classification of job advertisements for job vacancy statistics</title>
      <link>https://arxiv.org/abs/2411.03779</link>
      <description>arXiv:2411.03779v2 Announce Type: replace 
Abstract: The goal of this paper is to develop a multilingual classifier and conditional probability estimator of occupation codes for online job advertisements in accordance with the International Standard Classification of Occupations (ISCO) extended with the Polish Classification of Occupations and Specializations (KZiS), which is analogous to the European Classification of Occupations. In this paper, we utilise a range of data sources, including a novel one, namely the Central Job Offers Database, which is a register of all vacancies submitted to Public Employment Offices. Their staff members code the vacancies according to the ISCO and KZiS. A hierarchical multi-class classifier has been developed based on the transformer architecture. The classifier begins by encoding the jobs found in advertisements to the widest 1-digit occupational group, and then narrows the assignment to a 6-digit occupation code. We show that incorporation of the hierarchical structure of occupations improves prediction accuracy by 1-2 percentage points, particularly for the hand-coded online job advertisements. Finally, a bilingual (Polish and English) and multilingual (24 languages) model is developed based on data translated using closed and open-source software. The open-source software is provided for the benefit of the official statistics community, with a particular focus on international comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03779v2</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marek Wydmuch, Herman Cherniaiev, Robert Pater</dc:creator>
    </item>
    <item>
      <title>Improving Maximum Tolerated Dose Selection in Model-Assisted Designs for Phase I Trials through Bayesian Dose-Response Model</title>
      <link>https://arxiv.org/abs/2502.14278</link>
      <description>arXiv:2502.14278v2 Announce Type: replace 
Abstract: Model-assisted designs have garnered significant attention in recent years due to their high accuracy in identifying the maximum tolerated dose (MTD) and their operational simplicity. To identify the MTD, they employ estimated dose limiting toxicity (DLT) probabilities via isotonic regression with pool-adjacent violators algorithm (PAVA) after trials have been completed. PAVA adjusts independently estimated DLT probabilities with the Bayesian binomial model at each dose level using posterior variances ensure the monotonicity that toxicity increases with dose. However, in small sample settings such as Phase I oncology trials, this approach can lead to unstable DLT probability estimates and reduce MTD selection accuracy. To address this problem, we propose a novel MTD identification strategy in model-assisted designs that leverages a Bayesian dose-response model. Employing the dose-response model allows for stable estimation of the DLT probabilities under the monotonicity by borrowing information across dose levels, leading to an improvement in MTD identification accuracy. We discuss the specification of prior distributions that can incorporate information from similar trials or the absence of such information. We examine dose-response models employing logit, log-log, and complementary log-log link functions to assess the impact of link function differences on the accuracy of MTD selection. Through extensive simulations, we demonstrate that the proposed approach improves MTD selection accuracy by more than 10\% in some scenarios and by approximately 6\% on average compared to conventional approach. These findings indicate that the proposed approach can contribute to further enhancing the efficiency of Phase I oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14278v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rentaro Wakayama, Tomotaka Momozaki, Shuji Ando</dc:creator>
    </item>
    <item>
      <title>Rashomon perspective for measuring uncertainty in the survival predictive maintenance models</title>
      <link>https://arxiv.org/abs/2502.15772</link>
      <description>arXiv:2502.15772v2 Announce Type: replace 
Abstract: The prediction of the Remaining Useful Life of aircraft engines is a critical area in high-reliability sectors such as aerospace and defense. Early failure predictions help ensure operational continuity, reduce maintenance costs, and prevent unexpected failures. Traditional regression models struggle with censored data, which can lead to biased predictions. Survival models, on the other hand, effectively handle censored data, improving predictive accuracy in maintenance processes. This paper introduces a novel approach based on the Rashomon perspective, which considers multiple models that achieve similar performance rather than relying on a single best model. This enables uncertainty quantification in survival probability predictions and enhances decision-making in predictive maintenance. The Rashomon survival curve was introduced to represent the range of survival probability estimates, providing insights into model agreement and uncertainty over time. The results on the CMAPSS dataset demonstrate that relying solely on a single model for RUL estimation may increase risk in some scenarios. The censoring levels significantly impact prediction uncertainty, with longer censoring times leading to greater variability in survival probabilities. These findings underscore the importance of incorporating model multiplicity in predictive maintenance frameworks to achieve more reliable and robust failure predictions. This paper contributes to uncertainty quantification in RUL prediction and highlights the Rashomon perspective as a powerful tool for predictive modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15772v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SIU66497.2025.11112410</arxiv:DOI>
      <arxiv:journal_reference>2025 33rd Signal Processing and Communications Applications Conference (SIU), Sile, Istanbul, Turkiye, 2025, pp. 1-4</arxiv:journal_reference>
      <dc:creator>Yigitcan Yardimci, Mustafa Cavus</dc:creator>
    </item>
    <item>
      <title>Partially stochastic deep learning with uncertainty quantification for model predictive heating control</title>
      <link>https://arxiv.org/abs/2504.03350</link>
      <description>arXiv:2504.03350v2 Announce Type: replace 
Abstract: Improving the energy efficiency of building heating systems is crucial for reducing global energy consumption and greenhouse gas emissions. Traditional control methods rely on static heating curves that are based solely on outdoor temperature, neglecting system state measurements, such as indoor temperature, and free heat sources, such as solar gain. A more effective strategy is model predictive control (MPC), which optimizes heating control by incorporating system state predictions based on weather forecasts, among other factors. However, current industrial MPC solutions often employ simplified physics-inspired indoor temperature models, sacrificing accuracy for robustness and interpretability. To bridge this gap, we propose a partially stochastic deep learning (DL) architecture for building-specific indoor temperature modeling. Unlike most studies that evaluate model performance through simulations or limited test buildings, our experiments across a large dataset of 100 real-world buildings, covering various heating season conditions, demonstrate that the proposed model outperforms a widely used industrial physics-based model in predictive accuracy. The proposed DL architecture shows significant potential to improve thermal comfort and energy efficiency in heating MPC solutions. Although its computational cost is higher than that of the reference model, we discuss why this trade-off is manageable, even in large-scale applications. Unlike deterministic black-box approaches, the partially stochastic DL model offers a critical advantage by enabling pre-assessment of model feasibility through predictive uncertainty quantification. This work advances heating MPC, particularly for buildings with comprehensive datasets on their thermal behavior under various weather conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03350v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Hannula, Arttu H\"akkinen, Antti Solonen, Felipe Uribe, Jana de Wiljes, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Multiply robust estimation for causal survival analysis with treatment noncompliance</title>
      <link>https://arxiv.org/abs/2305.13443</link>
      <description>arXiv:2305.13443v3 Announce Type: replace-cross 
Abstract: Comparative effectiveness research frequently addresses a time-to-event outcome and can require unique considerations in the presence of treatment noncompliance. Motivated by the challenges in addressing noncompliance in the ADAPTABLE pragmatic clinical trial, we develop a multiply robust estimator to estimate the principal survival causal effects under the principal ignorability and monotonicity. The multiply robust estimator is consistent even if one, and sometimes two, of the required models are misspecified. We apply the multiply robust method in the ADAPTABLE trial to evaluate the effect of low- versus high-dose aspirin assignment on patients' death and hospitalization from cardiovascular diseases. We find that, comparing to low-dose assignment, assignment to the high-dose leads to differential effects among always high-dose takers, compliers, and always low-dose takers. Such treatment effect heterogeneity contributes to the null intention-to-treatment effect. We further perform a formal sensitivity analysis for investigating the robustness of our causal conclusions under violation of two identification assumptions specific to noncompliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13443v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Bo Liu, Lisa Wruck, Fan Li, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Mapping of Mortality Clusters</title>
      <link>https://arxiv.org/abs/2407.19135</link>
      <description>arXiv:2407.19135v3 Announce Type: replace-cross 
Abstract: Disease mapping analyses the distribution of several disease outcomes within a territory. Primary goals include identifying areas with unexpected changes in mortality rates, studying the relation among multiple diseases, and dividing the analysed territory into clusters based on the observed levels of disease incidence or mortality. In this work, we focus on detecting spatial mortality clusters, that occur when neighbouring areas within a territory exhibit similar mortality levels due to one or more diseases. When multiple causes of death are examined together, it is relevant to identify not only the spatial boundaries of the clusters but also the diseases that lead to their formation. However, existing methods in literature struggle to address this dual problem effectively and simultaneously. To overcome these limitations, we introduce Perla, a multivariate Bayesian model that clusters areas in a territory according to the observed mortality rates of multiple causes of death, also exploiting the information of external covariates. Our model incorporates the spatial structure of data directly into the clustering probabilities by leveraging the stick-breaking formulation of the multinomial distribution. Additionally, it exploits suitable global-local shrinkage priors to ensure that the detection of clusters depends on diseases showing concrete increases or decreases in mortality levels, while excluding uninformative diseases. We propose an MCMC algorithm for posterior inference that consists of closed-form Gibbs sampling moves for nearly every model parameter. To demonstrate the flexibility and effectiveness of our methodology, we validate Perla with a series of simulation experiments and two extensive case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19135v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Enrico Bovo, Pietro Belloni, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>PanelMatch: Matching Methods for Causal Inference with Time-Series Cross-Section Data</title>
      <link>https://arxiv.org/abs/2503.02073</link>
      <description>arXiv:2503.02073v2 Announce Type: replace-cross 
Abstract: Analyzing time-series cross-sectional (also known as longitudinal or panel) data is an important process across a number of fields, including the social sciences, economics, finance, and medicine. PanelMatch is an R package that implements a set of tools enabling researchers to apply matching methods for causal inference with time-series cross-sectional data. Relative to other commonly used methods for longitudinal analyses, like regression with fixed effects, the matching-based approach implemented in PanelMatch makes fewer parametric assumptions and offers more diagnostics. In this paper, we discuss the PanelMatch package, showing users a recommended pipeline for doing causal inference analysis with it and highlighting useful diagnostic and visualization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02073v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Rauh, In Song Kim, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model</title>
      <link>https://arxiv.org/abs/2504.21795</link>
      <description>arXiv:2504.21795v3 Announce Type: replace-cross 
Abstract: The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on Duke-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21795v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuankang Zhao, Matthew Engelhard</dc:creator>
    </item>
  </channel>
</rss>
