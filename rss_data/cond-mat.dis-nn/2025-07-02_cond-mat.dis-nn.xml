<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jul 2025 01:32:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Temperature chaos as a logical consequence of reentrant transition in spin glasses</title>
      <link>https://arxiv.org/abs/2507.00276</link>
      <description>arXiv:2507.00276v1 Announce Type: new 
Abstract: Temperature chaos is a striking phenomenon in spin glasses, where even slight changes in temperature lead to a complete reconfiguration of the spin state. Another intriguing effect is the reentrant transition, in which lowering the temperature drives the system from a ferromagnetic phase into a less ordered spin-glass or paramagnetic phase. In the present paper, we reveal an unexpected connection between these seemingly unrelated phenomena in the finite-dimensional Edwards-Anderson model of spin glasses by introducing a generalized formulation that incorporates correlations among disorder variables. Assuming the existence of a spin glass phase at finite temperature, we establish that temperature chaos arises as a logical consequence of reentrance in the Edwards-Anderson model. Our findings uncover a previously hidden mathematical structure relating reentrance and temperature chaos, offering a new perspective on the physics of spin glasses beyond the mean-field theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00276v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hidetoshi Nishimori, Masayuki Ohzeki, Manaka Okuyama</dc:creator>
    </item>
    <item>
      <title>Efficient GPU-Accelerated Training of a Neuroevolution Potential with Analytical Gradients</title>
      <link>https://arxiv.org/abs/2507.00528</link>
      <description>arXiv:2507.00528v1 Announce Type: new 
Abstract: Machine-learning interatomic potentials (MLIPs) such as neuroevolution potentials (NEP) combine quantum-mechanical accuracy with computational efficiency significantly accelerate atomistic dynamic simulations. Trained by derivative-free optimization, the normal NEP achieves good accuracy, but suffers from inefficiency due to the high-dimensional parameter search. To overcome this problem, we present a gradient-optimized NEP (GNEP) training framework employing explicit analytical gradients and the Adam optimizer. This approach greatly improves training efficiency and convergence speedily while maintaining accuracy and physical interpretability. By applying GNEP to the training of Sb-Te material systems(datasets include crystalline, liquid, and disordered phases), the fitting time has been substantially reduced-often by orders of magnitude-compared to the NEP training framework. The fitted potentials are validated by DFT reference calculations, demonstrating satisfactory agreement in equation of state and radial distribution functions. These results confirm that GNEP retains high predictive accuracy and transferability while considerably improved computational efficiency, making it well-suited for large-scale molecular dynamics simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00528v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongfu Huang, Junhao Peng, Kaiqi Li, Jian Zhou, Zhimei Sun</dc:creator>
    </item>
    <item>
      <title>Generalization performance of narrow one-hidden layer networks in the teacher-student setting</title>
      <link>https://arxiv.org/abs/2507.00629</link>
      <description>arXiv:2507.00629v2 Announce Type: new 
Abstract: Understanding the generalization abilities of neural networks for simple input-output distributions is crucial to account for their learning performance on real datasets. The classical teacher-student setting, where a network is trained from data obtained thanks to a label-generating teacher model, serves as a perfect theoretical test bed. In this context, a complete theoretical account of the performance of fully connected one-hidden layer networks in the presence of generic activation functions is lacking. In this work, we develop such a general theory for narrow networks, i.e. networks with a large number of hidden units, yet much smaller than the input dimension. Using methods from statistical physics, we provide closed-form expressions for the typical performance of both finite temperature (Bayesian) and empirical risk minimization estimators, in terms of a small number of weight statistics. In doing so, we highlight the presence of a transition where hidden neurons specialize when the number of samples is sufficiently large and proportional to the number of parameters of the network. Our theory accurately predicts the generalization error of neural networks trained on regression or classification tasks with either noisy full-batch gradient descent (Langevin dynamics) or full-batch gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00629v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean Barbier, Federica Gerace, Alessandro Ingrosso, Clarissa Lauditi, Enrico M. Malatesta, Gibbs Nwemadji, Rodrigo P\'erez Ortiz</dc:creator>
    </item>
    <item>
      <title>How do Probabilistic Graphical Models and Graph Neural Networks Look at Network Data?</title>
      <link>https://arxiv.org/abs/2506.11869</link>
      <description>arXiv:2506.11869v2 Announce Type: cross 
Abstract: Graphs are a powerful data structure for representing relational data and are widely used to describe complex real-world systems. Probabilistic Graphical Models (PGMs) and Graph Neural Networks (GNNs) can both leverage graph-structured data, but their inherent functioning is different. The question is how do they compare in capturing the information contained in networked datasets? We address this objective by solving a link prediction task and we conduct three main experiments, on both synthetic and real networks: one focuses on how PGMs and GNNs handle input features, while the other two investigate their robustness to noisy features and increasing heterophily of the graph. PGMs do not necessarily require features on nodes, while GNNs cannot exploit the network edges alone, and the choice of input features matters. We find that GNNs are outperformed by PGMs when input features are low-dimensional or noisy, mimicking many real scenarios where node attributes might be scalar or noisy. Then, we find that PGMs are more robust than GNNs when the heterophily of the graph is increased. Finally, to assess performance beyond prediction tasks, we also compare the two frameworks in terms of their computational complexity and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11869v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michela Lapenna, Caterina De Bacco</dc:creator>
    </item>
    <item>
      <title>Structural Order Drives Diffusion in a Granular Packing</title>
      <link>https://arxiv.org/abs/2507.00684</link>
      <description>arXiv:2507.00684v1 Announce Type: cross 
Abstract: We investigate how structural ordering, i.e. crystallization, affects the flow of bidisperse granular materials in a quasi-two-dimensional silo. By systematically varying the mass fraction of two particle sizes, we finely tune the degree of local order. Using high-speed imaging and kinematic modeling, we show that crystallization significantly enhances the diffusion length $b$, a key parameter controlling the velocity profiles within the flowing medium. We reveal a strong correlation between $b$ and the hexatic order parameter $\left&lt;|\psi_6|\right&gt;_t$, highlighting the role of local structural organization in governing macroscopic flow behavior. Furthermore, we demonstrate that pressure gradients within the silo promote the stabilization of orientational order even in the absence of crystallization, thus intrinsically increasing $b$ with height. These findings establish a direct link between microstructural order, pressure, and transport properties in granular silo flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00684v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Luce, Adrien Gans, S\'ebastien Kiesgen de Richter, Nicolas Vandewalle</dc:creator>
    </item>
    <item>
      <title>Suppression of shot noise at a Kondo destruction quantum critical point</title>
      <link>https://arxiv.org/abs/2507.00960</link>
      <description>arXiv:2507.00960v1 Announce Type: cross 
Abstract: Strange metal behavior has been observed in an expanding list of quantum materials, with heavy fermion metals serving as a prototype setting. Among the intriguing questions is the nature of charge carriers; there is an increasing recognition that the quasiparticles are lost, as captured by Kondo destruction quantum criticality. Among the recent experimental advances is the measurement of shot noise in a heavy-fermion strange metal. We are thus motivated to study current fluctuations by advancing a minimal Bose-Fermi Kondo lattice model, which admits a well-defined large-$N$ limit. Showing that the model in equilibrium captures the essential physics of Kondo destruction, we proceed to derive quantum kinetic equations and compute shot noise to the leading nontrivial order in $1/N$. Our results reveal a strong suppression of the shot noise at the Kondo destruction quantum critical point, thereby providing the understanding of the striking experiment. Broader implications of our results are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00960v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mes-hall</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiming Wang, Shouvik Sur, Fang Xie, Haoyu Hu, Silke Paschen, Douglas Natelson, Qimiao Si</dc:creator>
    </item>
    <item>
      <title>How to realize compact and non-compact localized states in disorder-free hypercube networks</title>
      <link>https://arxiv.org/abs/2410.10763</link>
      <description>arXiv:2410.10763v2 Announce Type: replace 
Abstract: We present a method for realizing various zero-energy localized states on disorder-free hypercube graphs. Previous works have already indicated that disorder is not essential for observing localization phenomena in noninteracting systems, with some prominent examples including the 1D Aubry-Andr\'e model, characterized solely by incommensurate potentials, or 2D incommensurate Moir\'e lattices, which exhibit localization due to the flat band spectrum. Moreover, flat band systems with translational invariance can also possess so-called compact localized states, characterized by exactly zero amplitude outside a finite region of the lattice. Here, we demonstrate that both compact and non-compact (i.e., Anderson-like) localized states naturally emerge in disorder-free hypercubes, which can be systematically constructed using Cartan products. This construction ensures the robustness of these localized states against perturbations. Furthermore, we show that the hypercubes can be associated with the Fock space of interacting spin systems exhibiting localization. Viewing localization from the hypercube perspective, with its inherently simple eigenspace structure, offers a clearer and more intuitive understanding of the underlying Fock-space many-body localization phenomena. Our findings can be readily tested on existing experimental platforms, where hypercube graphs can be emulated, e.g., by photonic networks of coupled optical cavities or waveguides. The results can pave the way for the development of novel quantum information protocols and enable effective simulation of quantum many-body localization phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10763v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.optics</category>
      <category>quant-ph</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/4s2w-y1xx</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Research 7, 033002 (2025)</arxiv:journal_reference>
      <dc:creator>Ievgen I. Arkhipov, Fabrizio Minganti, Franco Nori</dc:creator>
    </item>
    <item>
      <title>Uncertainty in AI-driven Monte Carlo simulations</title>
      <link>https://arxiv.org/abs/2506.14594</link>
      <description>arXiv:2506.14594v2 Announce Type: replace 
Abstract: In the study of complex systems, evaluating physical observables often requires sampling representative configurations via Monte Carlo techniques. These methods rely on repeated evaluations of the system's energy and force fields, which can become computationally expensive. To accelerate these simulations, deep learning models are increasingly employed as surrogate functions to approximate the energy landscape or force fields. However, such models introduce epistemic uncertainty in their predictions, which may propagate through the sampling process and affect the system's macroscopic behavior. In our work, we present the Penalty Ensemble Method (PEM) to quantify epistemic uncertainty and mitigate its impact on Monte Carlo sampling. Our approach introduces an uncertainty-aware modification of the Metropolis acceptance rule, which increases the rejection probability in regions of high uncertainty, thereby enhancing the reliability of the simulation outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14594v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Tzivrailis, Alberto Rosso, Eiji Kawasaki</dc:creator>
    </item>
  </channel>
</rss>
