<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 02:51:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimation of Industrial Heterogeneity from Maximum Entropy and Zonotopes Using the Enterprise Surveys</title>
      <link>https://arxiv.org/abs/2512.00002</link>
      <description>arXiv:2512.00002v1 Announce Type: new 
Abstract: This study introduces a novel framework for estimating industrial heterogeneity by integrating maximum entropy (ME) estimation of production functions with Zonotope-based measures. Traditional production function estimations often rely on restrictive parametric models, failing to capture firm behavior under uncertainty. This research addresses these limitations by applying Hang K. Ryu's ME method to estimate production functions using World Bank Enterprise Survey (WBES) data from Bangladesh, Colombia, Egypt, and India. The study normalizes entropy values to quantify heterogeneity and compares these measures with a Zonotope-based Gini index. Results demonstrate the ME method's superiority in capturing nuanced, functional heterogeneity often missed by traditional techniques. Furthermore, the study incorporates a "Tangent Against Input Axes" method to dynamically assess technical change within industries. By integrating information theory with production economics, this unified framework quantifies structural and functional differences across industries using firm-level data, advancing both methodological and empirical understanding of heterogeneity. A numerical simulation confirms the ME regression functions can approximate actual industrial heterogeneity. The research also highlights the superior ability of the ME method to provide a precise and economically meaningful measure of industry heterogeneity, particularly for longitudinal analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00002v1</guid>
      <category>econ.EM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ting-Yen Wang</dc:creator>
    </item>
    <item>
      <title>High-dimensional Penalized Linear IV Estimation &amp; Inference using BRIDGE and Adaptive LASSO</title>
      <link>https://arxiv.org/abs/2512.00265</link>
      <description>arXiv:2512.00265v1 Announce Type: new 
Abstract: This paper is an exposition of how BRIDGE and adaptive LASSO can be used in a two-stage least squares problem, to estimate the second-stage coefficients when the number of parameters p in both stages is growing with the sample size n. Facing a larger class of problems compared to the usual analysis in the literature, i.e., replacing the assumption of normal with sub-Gaussian errors, I prove that both methods ensure model selection consistency and oracle efficiency even when the number of instruments and covariates exceeds the sample size. For BRIDGE, I also prove that if the former is growing but slower than the latter, the same properties hold even without sub-Gaussian errors. When p is greater than n, BRIDGE requires a slightly weaker set of assumptions to have the desirable properties, as adaptive LASSO requires a good initial estimator of the relevant weights. However, adaptive LASSO is expected to be much faster computationally, so the methods are competitive on different fronts and the one that is recommended depends on the researcher's resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00265v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleftheria Kelekidou</dc:creator>
    </item>
    <item>
      <title>Explainable Machine Learning for Macroeconomic and Financial Nowcasting: A Decision-Grade Framework for Business and Policy</title>
      <link>https://arxiv.org/abs/2512.00399</link>
      <description>arXiv:2512.00399v1 Announce Type: new 
Abstract: Macroeconomic nowcasting sits at the intersection of traditional econometrics, data-rich information systems, and AI applications in business, economics, and policy. Machine learning (ML) methods are increasingly used to nowcast quarterly GDP growth, but adoption in high-stakes settings requires that predictive accuracy be matched by interpretability and robust uncertainty quantification. This article reviews recent developments in macroeconomic nowcasting and compares econometric benchmarks with ML approaches in data-rich and shock-prone environments, emphasizing the use of nowcasts as decision inputs rather than as mere error-minimization exercises. The discussion is organized along three axes. First, we contrast penalized regressions, dimension-reduction techniques, tree ensembles, and neural networks with autoregressive models, Dynamic Factor Models, and Random Walks, emphasizing how each family handles small samples, collinearity, mixed frequencies, and regime shifts. Second, we examine explainability tools (intrinsic measures and model-agnostic XAI methods), focusing on temporal stability, sign coherence, and their ability to sustain credible economic narratives and nowcast revisions. Third, we analyze non-parametric uncertainty quantification via block bootstrapping for predictive intervals and confidence bands on feature importance under serial dependence and ragged edge. We translate these elements into a reference workflow for "decision-grade" nowcasting systems, including vintage management, time-aware validation, and automated reliability audits, and we outline a research agenda on regime-dependent model comparison, bootstrap design for latent components, and temporal stability of explanations. Explainable ML and uncertainty quantification emerge as structural components of a responsible forecasting pipeline, not optional refinements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00399v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Attolico</dc:creator>
    </item>
    <item>
      <title>Improved Inference for Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2512.00566</link>
      <description>arXiv:2512.00566v1 Announce Type: new 
Abstract: Nonparametric regression estimators, including those employed in regression-discontinuity designs (RDD), are central to the economist's toolbox. Their application, however, is complicated by the presence of asymptotic bias, which undermines coverage accuracy of conventional confidence intervals. Extant solutions to the problem include debiasing methods, such as the widely applied robust bias-corrected (RBC) confidence interval of Calonico et al. (2014, 2018). We show that this interval is equivalent to a prepivoted interval based on an invalid residual-based bootstrap method. Specifically, prepivoting performs an implicit bias correction while adjusting the nonparametric regression estimator's standard error to account for the additional uncertainty introduced by debiasing. This idea can also be applied to other bootstrap schemes, leading to new implicit bias corrections and corresponding standard error adjustments. We propose a prepivoted interval based on a bootstrap that generates observations from nonparametric regression estimates at each regressor value and show how it can be implemented as an RBC-type interval without the need for resampling. Importantly, we show that the new interval is shorter than the existing RBC interval. For example, with the Epanechnikov kernel, the length is reduced by 17%, while maintaining accurate coverage probability. This result holds irrespectively of: (a) the evaluation point being in the interior or on the boundary; (b) the use of a 'small' or 'large' bandwidths; (c) the distribution of the regressor and the error term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00566v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, S\'ilvia Gon\c{c}alves, Morten {\O}rregaard Nielsen, Edoardo Zanelli</dc:creator>
    </item>
    <item>
      <title>Market Sensitivities and Growth Differentials Across Australian Housing Markets</title>
      <link>https://arxiv.org/abs/2512.01139</link>
      <description>arXiv:2512.01139v1 Announce Type: new 
Abstract: Australian house prices have risen strongly since the mid-1990s, but growth has been highly uneven across regions. Raw growth figures obscure whether these differences reflect persistent structural trends or cyclical fluctuations. We address this by estimating a three-factor model in levels for regional repeat-sales log price indexes over 1995-2024. The model decomposes each regional index into a national Market factor, two stationary spreads (Mining and Lifestyle) that capture mean-reverting geographic cycles, and a city-specific residual. The Mining spread, proxied by a Perth-Sydney index differential, reflects resource-driven oscillations in relative performance; the Lifestyle spread captures amenity-driven coastal and regional cycles. The Market loading isolates each region's fundamental sensitivity, beta, to national growth, so that a city's growth under an assumed national change is calculated from its beta once mean-reverting spreads are netted out. Comparing realised paths to these factor-implied trajectories indicates when a city is historically elevated or depressed, and attributes the gap to Mining or Lifestyle spreads.
  Expanding-window ARIMAX estimation reveals that Market betas are stable across major shocks (the mining boom, the Global Financial Crisis, and COVID-19), while Mining and Lifestyle behave as stationary spreads that widen forecast funnels without overturning the cross-sectional ranking implied by beta. Melbourne amplifies national growth, Sydney tracks the national trend closely, and regional areas dampen it. The framework thus provides a simple, factor-based tool for interpreting regional growth differentials and their persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01139v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Willem P Sijp</dc:creator>
    </item>
    <item>
      <title>Prejudiced Futures? Algorithmic Bias in Time Series Forecasting and Its Ethical Implications</title>
      <link>https://arxiv.org/abs/2512.01877</link>
      <description>arXiv:2512.01877v1 Announce Type: new 
Abstract: Time series prediction algorithms are increasingly central to decision-making in high-stakes domains such as healthcare, energy management, and economic planning. Yet, these systems often inherit and amplify biases embedded in historical data, flawed problem specifications, and socio-technical design decisions. This paper critically examines the ethical foundations and mitigation strategies for algorithmic bias in time series prediction. We outline how predictive models, particularly in temporally dynamic domains, can reproduce structural inequalities and emergent discrimination through proxy variables and feedback loops. The paper advances a threefold contribution: First, it reframes algorithmic bias as a socio- technical phenomenon rooted in normative choices and institutional constraints. Second, it offers a structured diagnosis of bias sources across the pipeline, emphasizing the need for causal modeling, interpretable systems, and inclusive design practices. Third, it advocates for structural reforms that embed fairness through participatory governance, stakeholder engagement, and legally enforceable safeguards. Special attention is given to fairness validation in dynamic environments, proposing multi-metric, temporally-aware, and context- sensitive evaluation methods. Ultimately, we call for an integrated ethics-by-design approach that positions fairness not as a trade-off against performance, but as a co-requisite of responsible innovation. This framework is essential to developing predictive systems that are not only effective and adaptive but also aligned with democratic values and social equity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01877v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bagattini Alexander, Chen Shao</dc:creator>
    </item>
    <item>
      <title>Foundation Priors</title>
      <link>https://arxiv.org/abs/2512.01107</link>
      <description>arXiv:2512.01107v1 Announce Type: cross 
Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01107v1</guid>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjog Misra</dc:creator>
    </item>
    <item>
      <title>Data-Driven Policy Learning for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2402.02535</link>
      <description>arXiv:2402.02535v3 Announce Type: replace 
Abstract: This paper studies policy learning for continuous treatments from observational data. Continuous treatments present more significant challenges than discrete ones because population welfare may need nonparametric estimation, and policy space may be infinite-dimensional and may satisfy shape restrictions. We propose to approximate the policy space with a sequence of finite-dimensional spaces and, for any given policy, obtain the empirical welfare by applying the kernel method. We consider two cases: known and unknown propensity scores. In the latter case, we allow for machine learning of the propensity score and modify the empirical welfare to account for the effect of machine learning. The learned policy maximizes the empirical welfare or the modified empirical welfare over the approximating space. In both cases, we modify the penalty algorithm proposed in Mbakop and Tabord-Meehan (2021) to data-automate the tuning parameters (i.e., bandwidth and dimension of the approximating space) and establish an oracle inequality for the welfare regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02535v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunrong Ai, Yue Fang, Haitian Xie</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Detecting Collusion and Capacity Withholding in Wholesale Electricity Markets</title>
      <link>https://arxiv.org/abs/2508.09885</link>
      <description>arXiv:2508.09885v2 Announce Type: replace 
Abstract: Collusion and capacity withholding in electricity wholesale markets are important mechanisms of market manipulation. This study applies a refined machine learning-based cartel detection algorithm to two cartel cases in the Italian electricity market and evaluates its out-of-sample performance. Specifically, we consider an ensemble machine learning method that uses statistical screens constructed from the offer price distribution as predictors for the incidence of collusion among electricity providers in specific regions. We propose novel screens related to the capacity-withholding behavior of electricity providers and find that including such screens derived from the day-ahead spot market as predictors can improve cartel detection. We find that, under complete cartels - where collusion in a tender presumably involves all suppliers - the method correctly classifies up to roughly 95% of tenders in our data as collusive or competitive, improving classification accuracy compared to using only previously available screens. However, when trained on larger datasets including non-cartel members and applying algorithms tailored to detect incomplete cartels, the previously existing screens are sufficient to achieve 98% accuracy, and the addition of our newly proposed capacity-withholding screens does not further improve performance. Overall, this study highlights the promising potential of supervised machine learning techniques for detecting and dismantling cartels in electricity markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09885v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Proz, Martin Huber</dc:creator>
    </item>
    <item>
      <title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
      <link>https://arxiv.org/abs/2112.13495</link>
      <description>arXiv:2112.13495v4 Announce Type: replace-cross 
Abstract: Completely randomized experiments, originally developed by Fisher and Neyman in the 1930s, are still widely used in practice, even in online experimentation. However, such designs are of limited value for answering standard questions in marketplaces, where multiple populations of agents interact strategically, leading to complex patterns of spillover effects. In this paper, we derive the finite-sample properties of tractable estimators for "Simple Multiple Randomization Designs" (SMRDs), a new class of experimental designs which account for complex spillover effects in randomized experiments. Our derivations are obtained under a natural and general form of cross-unit interference, which we call "local interference". We discuss the estimation of main effects, direct effects, and spillovers, and present associated central limit theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13495v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf073</arxiv:DOI>
      <dc:creator>Lorenzo Masoero, Suhas Vijaykumar, Thomas Richardson, James McQueen, Ido Rosen, Brian Burdick, Pat Bajari, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Bayesian Synthetic Control with a Soft Simplex Constraint</title>
      <link>https://arxiv.org/abs/2503.06454</link>
      <description>arXiv:2503.06454v2 Announce Type: replace-cross 
Abstract: The challenges posed by high-dimensional data and use of the simplex constraint are two major concerns in the empirical application of the synthetic control method (SCM) in econometric studies. To address both issues simultaneously, we propose a Bayesian SCM that integrates a soft simplex constraint within spike-and-slab variable selection. The hierarchical prior structure captures the extent to which the data supports the simplex constraint, allowing for more efficient and data-adaptive counterfactual estimation. The intractable marginal likelihood induced by the soft simplex constraint presents a major computational challenge, which we resolve by developing a novel Metropolis-within-Gibbs algorithm that updates the regression coefficients of two predictors simultaneously. Our main theoretical contribution is a high-dimensional selection consistency result for the spike-and-slab variable selection under the simplex constraint, which significantly extends the current theory for high-dimensional Bayesian variable selection. Simulation studies demonstrate that our method performs well across diverse settings. To illustrate its practical values, we apply it to two empirical examples for estimating the effect of economic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06454v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Quan Zhou</dc:creator>
    </item>
  </channel>
</rss>
