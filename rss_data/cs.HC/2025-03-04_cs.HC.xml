<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Mar 2025 05:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learner and Instructor Needs in AI-Supported Programming Learning Tools: Design Implications for Features and Adaptive Control</title>
      <link>https://arxiv.org/abs/2503.00144</link>
      <description>arXiv:2503.00144v1 Announce Type: new 
Abstract: AI-supported tools can help learners overcome challenges in programming education by providing adaptive assistance. However, existing research often focuses on individual tools rather than deriving broader design recommendations. A key challenge in designing these systems is balancing learner control with system-driven guidance. To explore user preferences for AI-supported programming learning tools, we conducted a participatory design study with 15 undergraduate novice programmers and 10 instructors to gather insights on their desired help features and control preferences, as well as a follow-up survey with 172 introductory programming students.
  Our qualitative findings show that learners prefer help that is encouraging, incorporates visual aids, and includes peer-related insights, whereas instructors prioritize scaffolding that reflects learners' progress and reinforces best practices. Both groups favor shared control, though learners generally prefer more autonomy, while instructors lean toward greater system guidance to prevent cognitive overload. Additionally, our interviews revealed individual differences in control preferences.
  Based on our findings, we propose design guidelines for AI-supported programming tools, particularly regarding user-centered help features and adaptive control mechanisms. Our work contributes to the human-centered design of AI-supported learning environments by informing the development of systems that effectively balance autonomy and guidance, enhancing AI-supported educational tools for programming and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00144v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zihan Wu, Yicheng Tang, Barbara Ericson</dc:creator>
    </item>
    <item>
      <title>Tactile Vega-Lite: Rapidly Prototyping Tactile Charts with Smart Defaults</title>
      <link>https://arxiv.org/abs/2503.00149</link>
      <description>arXiv:2503.00149v1 Announce Type: new 
Abstract: Tactile charts are essential for conveying data to blind and low vision (BLV) readers but are difficult for designers to construct. Non-expert designers face barriers to entry due to complex guidelines, while experts struggle with fragmented and time-consuming workflows that involve extensive customization. Inspired by formative interviews with expert tactile graphics designers, we created Tactile Vega-Lite (TVL): an extension of Vega-Lite that offers tactile-specific abstractions and synthesizes existing guidelines into a series of smart defaults. Predefined stylistic choices enable non-experts to produce guideline-compliant tactile charts quickly. Expert users can override defaults to tailor customizations for their intended audience. In a user study with 12 tactile graphics creators, we show that Tactile Vega-Lite enhances flexibility and consistency by automating tasks like adjusting spacing and translating braille while accelerating iterations through pre-defined textures and line styles. Through expert critique, we also learn more about tactile chart design best practices and design decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00149v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714132</arxiv:DOI>
      <dc:creator> Mengzhu (Katie),  Chen, Isabella Pedraza Pineros, Arvind Satyanarayan, Jonathan Zong</dc:creator>
    </item>
    <item>
      <title>Predicting Human-Chatbot Relationships: A Mixed-Method Study on the Key Psychological Factors</title>
      <link>https://arxiv.org/abs/2503.00195</link>
      <description>arXiv:2503.00195v1 Announce Type: new 
Abstract: Romantic relationships with social chatbots are becoming increasingly prevalent, raising important questions about their societal and psychological implications. Despite this growing trend, little is known about the individuals entering these synthetic relationships. This three-part study seeks to enhance understanding of the factors encompassing human-chatbot relationships by quantitatively examining the commonly discussed characteristics romantic and sexual fantasy, loneliness, attachment style, anthropomorphism, and sexual sensation seeking (Study 1A), comparing the impact of romantic and sexual fantasizing for human-chatbot versus human-human relationships (Study 1B), and providing qualitative insights into how individuals conceptualize romantic and sexual fantasies in their interactions with chatbots (Study 2). Individuals with romantic chatbot connections were interviewed (N=15) or surveyed (N=92), while participants in the comparison groups, long-distance (N=90) and cohabiting relationships (N=82), completed a questionnaire. Romantic fantasizing emerged as the strongest predictor of human-chatbot relationships, alongside anthropomorphism and anxious-avoidant attachment. Notably, romantic fantasy also predicted partner closeness across all relationship types, revealing shared psychological dynamics between human-chatbot and human-human bonds. Interviews further reinforced this, with all participants engaging in fantasy exploration while desiring their chatbot to feel as human as possible. This paper provides a novel and multifaceted examination of the psychological dynamics within human-chatbot relationships, highlighting the central yet understudied role of fantasy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00195v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paula Ebner, Jessica Szczuka</dc:creator>
    </item>
    <item>
      <title>Seeing Eye to AI? Applying Deep-Feature-Based Similarity Metrics to Information Visualization</title>
      <link>https://arxiv.org/abs/2503.00228</link>
      <description>arXiv:2503.00228v1 Announce Type: new 
Abstract: Judging the similarity of visualizations is crucial to various applications, such as visualization-based search and visualization recommendation systems. Recent studies show deep-feature-based similarity metrics correlate well with perceptual judgments of image similarity and serve as effective loss functions for tasks like image super-resolution and style transfer. We explore the application of such metrics to judgments of visualization similarity. We extend a similarity metric using five ML architectures and three pre-trained weight sets. We replicate results from previous crowd-sourced studies on scatterplot and visual channel similarity perception. Notably, our metric using pre-trained ImageNet weights outperformed gradient-descent tuned MS-SSIM, a multi-scale similarity metric based on luminance, contrast, and structure. Our work contributes to understanding how deep-feature-based metrics can enhance similarity assessments in visualization, potentially improving visual analysis tools and techniques. Supplementary materials are available at https://osf.io/dj2ms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00228v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713955</arxiv:DOI>
      <dc:creator>Sheng Long, Angelos Chatzimparmpas, Emma Alexander, Matthew Kay, Jessica Hullman</dc:creator>
    </item>
    <item>
      <title>Doraemon's Gadget Lab: Unpacking Human Needs and Interaction Design in Speculative Technology</title>
      <link>https://arxiv.org/abs/2503.00257</link>
      <description>arXiv:2503.00257v1 Announce Type: new 
Abstract: Speculative technologies in science fiction have long inspired advancements in Human-Computer Interaction (HCI). Doraemon, a Japanese manga featuring a robotic cat from the 22nd century, presents an extensive collection of futuristic gadgets-an underexplored source of speculative technologies. This study systematically analyses 379 of these gadgets, categorising them into 33 subcategories within 10 high-level groupings, to examine the fundamental human needs they address, their parallels to contemporary technologies, and their potential insights for HCI design. The findings reveal that while human needs remain constant, the ways in which technology fulfils them differ. Doraemon's gadgets emphasise tangible, single-purpose interactions with built-in reversibility, contrasting with the increasing complexity and software-driven nature of modern systems. By examining these speculative technologies, this study highlights alternative interaction paradigms that challenge current HCI trends and offer inspiration for future user-centred innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00257v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719977</arxiv:DOI>
      <dc:creator>Tram Thi Minh Tran</dc:creator>
    </item>
    <item>
      <title>Leveraging Complementary AI Explanations to Mitigate Misunderstanding in XAI</title>
      <link>https://arxiv.org/abs/2503.00303</link>
      <description>arXiv:2503.00303v1 Announce Type: new 
Abstract: Artificial intelligence explanations make complex predictive models more comprehensible. Effective explanations, however, should also anticipate and mitigate possible misinterpretations, e.g., arising when users infer incorrect information that is not explicitly conveyed. To this end, we propose complementary explanations -- a novel method that pairs explanations to compensate for their respective limitations. A complementary explanation adds insights that clarify potential misconceptions stemming from the primary explanation while ensuring their coherence and avoiding redundancy. We also introduce a framework for designing and evaluating complementary explanation pairs based on pertinent qualitative properties and quantitative metrics. Applying our approach allows to construct complementary explanations that minimise the chance of their misinterpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00303v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqing Xuan, Kacper Sokol, Mark Sanderson, Jeffrey Chan</dc:creator>
    </item>
    <item>
      <title>Characterizing LLM-Empowered Personalized Story-Reading and Interaction for Children: Insights from Multi-Stakeholder Perspectives</title>
      <link>https://arxiv.org/abs/2503.00590</link>
      <description>arXiv:2503.00590v1 Announce Type: new 
Abstract: Personalized interaction is highly valued by parents in their story-reading activities with children. While AI-empowered story-reading tools have been increasingly used, their abilities to support personalized interaction with children are still limited. Recent advances in large language models (LLMs) show promise in facilitating personalized interactions, but little is known about how to effectively and appropriately use LLMs to enhance children's personalized story-reading experiences. This work explores this question through a design-based study. Drawing on a formative study, we designed and developed StoryMate, an LLM-empowered personalized interactive story-reading tool for children, following an empirical study with children, parents, and education experts. Our participants valued the personalized features in StoryMate, and also highlighted the need to support personalized content, guiding mechanisms, reading context variations, and interactive interfaces. Based on these findings, we propose a series of design recommendations for better using LLMs to empower children's personalized story reading and interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00590v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713275</arxiv:DOI>
      <dc:creator>Jiaju Chen, Minglong Tang, Yuxuan Lu, Bingsheng Yao, Elissa Fan, Xiaojuan Ma, Ying Xu, Dakuo Wang, Yuling Sun, Liang He</dc:creator>
    </item>
    <item>
      <title>From Prompting to Partnering: Personalization Features for Human-LLM Interactions</title>
      <link>https://arxiv.org/abs/2503.00681</link>
      <description>arXiv:2503.00681v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, exhibit advanced capabilities in generating text, images, and videos. However, their effective use remains constrained by challenges in prompt formulation, personalization, and opaque decision-making processes. To investigate these challenges and identify design opportunities, we conducted a two-phase qualitative study. In Phase 1, we performed in-depth interviews with eight everyday LLM users after they engaged in structured tasks using ChatGPT across both familiar and unfamiliar domains. Our findings revealed key user difficulties in constructing effective prompts, iteratively refining AI-generated responses, and assessing response reliability especially in domains beyond users' expertise. Informed by these insights, we designed a high-fidelity prototype incorporating Reflective Prompting, Section Regeneration, Input-Output Mapping, Confidence Indicators, and a Customization Panel. In Phase 2, user testing of the prototype indicated that these interface-level improvements may prove useful for reducing cognitive load, increasing transparency, and fostering more intuitive and collaborative human-AI interactions. Our study contributes to the growing discourse on human-centred AI, advocating for human-LLM interactions that enhance user agency, transparency, and co-creative interaction, ultimately supporting more intuitive, accessible, and trustworthy generative AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00681v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Si Thu, A. Baki Kocaballi</dc:creator>
    </item>
    <item>
      <title>Exploring the Design Space of Real-time LLM Knowledge Support Systems: A Case Study of Jargon Explanations</title>
      <link>https://arxiv.org/abs/2503.00715</link>
      <description>arXiv:2503.00715v1 Announce Type: new 
Abstract: Knowledge gaps often arise during communication due to diverse backgrounds, knowledge bases, and vocabularies. With recent LLM developments, providing real-time knowledge support is increasingly viable, but is challenging due to shared and individual cognitive limitations (e.g., attention, memory, and comprehension) and the difficulty in understanding the user's context and internal knowledge. To address these challenges, we explore the key question of understanding how people want to receive real-time knowledge support. We built StopGap -- a prototype that provides real-time knowledge support for explaining jargon words in videos -- to conduct a design probe study (N=24) that explored multiple visual knowledge representation formats. Our study revealed individual differences in preferred representations and highlighted the importance of user agency, personalization, and mixed-initiative assistance. Based on our findings, we map out six key design dimensions for real-time LLM knowledge support systems and offer insights for future research in this space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00715v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhan Liu, Aadit Shah, Jordan Ackerman, Manaswi Saha</dc:creator>
    </item>
    <item>
      <title>Wikipedia Contributions in the Wake of ChatGPT</title>
      <link>https://arxiv.org/abs/2503.00757</link>
      <description>arXiv:2503.00757v1 Announce Type: new 
Abstract: How has Wikipedia activity changed for articles with content similar to ChatGPT following its introduction? We estimate the impact using differences-in-differences models, with dissimilar Wikipedia articles as a baseline for comparison, to examine how changes in voluntary knowledge contributions and information-seeking behavior differ by article content. Our analysis reveals that newly created, popular articles whose content overlaps with ChatGPT 3.5 saw a greater decline in editing and viewership after the November 2022 launch of ChatGPT than dissimilar articles did. These findings indicate heterogeneous substitution effects, where users selectively engage less with existing platforms when AI provides comparable content. This points to potential uneven impacts on the future of human-driven online knowledge contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00757v1</guid>
      <category>cs.HC</category>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Lyu, James Siderius, Hannah Li, Daron Acemoglu, Daniel Huttenlocher, Asuman Ozdaglar</dc:creator>
    </item>
    <item>
      <title>Expandora: Broadening Design Exploration with Text-to-Image Model</title>
      <link>https://arxiv.org/abs/2503.00791</link>
      <description>arXiv:2503.00791v1 Announce Type: new 
Abstract: Broad exploration of references is critical in the visual design process. While text-to-image (T2I) models offer efficiency and customization of exploration, they often limit support for divergence in exploration. We conducted a formative study (N=6) to investigate the limitations of current interaction with the T2I model for broad exploration and found that designers struggle to articulate exploratory intentions and manage iterative, non-linear workflows. To address these challenges, we developed Expandora. Users can specify their exploratory intentions and desired diversity levels through structured input, and using an LLM-based pipeline, Expandora generates tailored prompt variations. The results are displayed in a mindmap-like interface that encourages non-linear workflows. A user study (N=8) demonstrated that Expandora significantly increases prompt diversity, the number of prompts users tried within a given time, and user satisfaction compared to the baseline. Nonetheless, its limitations in supporting convergent thinking suggest opportunities for holistically improving creative processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00791v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3720189</arxiv:DOI>
      <dc:creator>DaEun Choi, Kihoon Son, Hyunjoon Jung, Juho Kim</dc:creator>
    </item>
    <item>
      <title>Who Reaps All the Superchats? A Large-Scale Analysis of Income Inequality in Virtual YouTuber Livestreaming</title>
      <link>https://arxiv.org/abs/2503.00825</link>
      <description>arXiv:2503.00825v1 Announce Type: new 
Abstract: The explosive growth of Virtual YouTubers (VTubers)-streamers who perform behind virtual anime avatars-has created a unique digital economy with profound implications for content creators, platforms, and viewers. Understanding the economic landscape of VTubers is crucial for designing equitable platforms, supporting content creator livelihoods, and fostering sustainable digital communities. To this end, we conducted a large-scale study of over 1 million hours of publicly available streaming records from 1,923 VTubers on YouTube, covering tens of millions of dollars in actual profits. Our analysis reveals stark inequality within the VTuber community and characterizes the sources of income for VTubers from multiple perspectives. Furthermore, we also found that the VTuber community is increasingly monopolized by two agencies, driving the financial disparity. This research illuminates the financial dynamics of VTuber communities, informing the design of equitable platforms and sustainable support systems for digital content creators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00825v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijing Zhao, Brian Diep, Jiaxin Pei, Dongwook Yoon, David Jurgens, Jian Zhu</dc:creator>
    </item>
    <item>
      <title>Intuit: Explain Quantum Computing Concepts via AR-based Analogy</title>
      <link>https://arxiv.org/abs/2503.00835</link>
      <description>arXiv:2503.00835v1 Announce Type: new 
Abstract: Quantum computing has shown great potential to revolutionize traditional computing and can provide an exponential speedup for a wide range of possible applications, attracting various stakeholders. However, understanding fundamental quantum computing concepts remains a significant challenge for novices because of their abstract and counterintuitive nature. Thus, we propose an analogy-based characterization framework to construct the mental mapping between quantum computing concepts and daily objects, informed by in-depth expert interviews and a literature review, covering key quantum concepts and characteristics like number of qubits, output state duality, quantum concept type, and probability quantification. Then, we developed an AR-based prototype system, Intuit, using situated analytics to explain quantum concepts through daily objects and phenomena (e.g., rotating coins, paper cutters). We thoroughly evaluated our approach through in-depth user and expert interviews. The Results demonstrate the effectiveness and usability of Intuit in helping learners understand abstract concepts in an intuitive and engaging manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00835v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3720085</arxiv:DOI>
      <dc:creator>Manusha Karunathilaka, Shaolun Ruan, Lin-Ping Yuan, Jiannan Li, Zhiding Liang, Kavinda Athapaththu, Qiang Guan, Yong Wang</dc:creator>
    </item>
    <item>
      <title>VTuber's Atelier: The Design Space, Challenges, and Opportunities for VTubing</title>
      <link>https://arxiv.org/abs/2503.00842</link>
      <description>arXiv:2503.00842v1 Announce Type: new 
Abstract: VTubing, the practice of live streaming using virtual avatars, has gained worldwide popularity among streamers seeking to maintain anonymity. While previous research has primarily focused on the social and cultural aspects of VTubing, there is a noticeable lack of studies examining the practical challenges VTubers face in creating and operating their avatars. To address this gap, we surveyed VTubers' equipment and expanded the live-streaming design space by introducing six new dimensions related to avatar creation and control. Additionally, we conducted interviews with 16 professional VTubers to comprehensively explore their practices, strategies, and challenges throughout the VTubing process. Our findings reveal that VTubers face significant burdens compared to real-person streamers due to fragmented tools and the multi-tasking nature of VTubing, leading to unique workarounds. Finally, we summarize these challenges and propose design opportunities to improve the effectiveness and efficiency of VTubing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00842v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714107</arxiv:DOI>
      <dc:creator>Daye Kim, Sebin Lee, Yoonseo Jun, Yujin Shin, Jungjin Lee</dc:creator>
    </item>
    <item>
      <title>Applying the Gricean Maxims to a Human-LLM Interaction Cycle: Design Insights from a Participatory Approach</title>
      <link>https://arxiv.org/abs/2503.00858</link>
      <description>arXiv:2503.00858v1 Announce Type: new 
Abstract: While large language models (LLMs) are increasingly used to assist users in various tasks through natural language interactions, these interactions often fall short due to LLMs' limited ability to infer contextual nuances and user intentions, unlike humans. To address this challenge, we draw inspiration from the Gricean Maxims--human communication theory that suggests principles of effective communication--and aim to derive design insights for enhancing human-AI interactions (HAI). Through participatory design workshops with communication experts, designers, and end-users, we identified ways to apply these maxims across the stages of the HAI cycle. Our findings include reinterpreted maxims tailored to human-LLM contexts and nine actionable design considerations categorized by interaction stage. These insights provide a concrete framework for designing more cooperative and user-centered LLM-based systems, bridging theoretical foundations in communication with practical applications in HAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00858v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, Juho Kim</dc:creator>
    </item>
    <item>
      <title>Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions</title>
      <link>https://arxiv.org/abs/2503.00940</link>
      <description>arXiv:2503.00940v1 Announce Type: new 
Abstract: A growing body of work in Ethical AI attempts to capture human moral judgments through simple computational models. The key question we address in this work is whether such simple AI models capture {the critical} nuances of moral decision-making by focusing on the use case of kidney allocation. We conducted twenty interviews where participants explained their rationale for their judgments about who should receive a kidney. We observe participants: (a) value patients' morally-relevant attributes to different degrees; (b) use diverse decision-making processes, citing heuristics to reduce decision complexity; (c) can change their opinions; (d) sometimes lack confidence in their decisions (e.g., due to incomplete information); and (e) express enthusiasm and concern regarding AI assisting humans in kidney allocation decisions. Based on these findings, we discuss challenges of computationally modeling moral judgments {as a stand-in for human input}, highlight drawbacks of current approaches, and suggest future directions to address these issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00940v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714167</arxiv:DOI>
      <dc:creator>Vijay Keswani, Vincent Conitzer, Walter Sinnott-Armstrong, Breanna K. Nguyen, Hoda Heidari, Jana Schaich Borg</dc:creator>
    </item>
    <item>
      <title>A Review of LLM-Assisted Ideation</title>
      <link>https://arxiv.org/abs/2503.00946</link>
      <description>arXiv:2503.00946v1 Announce Type: new 
Abstract: We present a comprehensive, in-depth review of ideation assisted by large language models (LLMs), highlighting emerging trends and identifying unaddressed research gaps. In total, we examined 61 studies investigating the application of LLMs in both group and individual ideation processes. From these studies, we derived the Hourglass Ideation Framework for LLM-assisted ideation, comprising three phases and seven key ideation stages, which served as the basis for our systematic survey. Our analysis reveals that LLMs are most frequently used for idea generation and refinement, but their use in scope specification, foundational material structuring and multi-idea evaluation and selection remains limited. We provide our findings in extensive tabular and online formats. These catalogues detail research on LLM-assisted, purely LLM-based, and human-only activities across the seven ideation stages for each of the 61 studies. These also detail creative domains, publication outlets, interaction designs, user study designs, and assessment methods. Our analysis of system interaction design reveals a predominant focus on supporting individual ideation activities and text-based interaction, with a growing trend of incorporating multimedia elements. However, in group ideation, tools and interaction modalities targeting both synchronous and asynchronous collaboration are much scarcer. We synthesize the primary findings of our review and outline promising directions for future research in LLM-assisted ideation. We hope this review will help researchers quickly gain an overview of this rapidly expanding area, efficiently locate relevant work, and identify underexplored areas for further investigation. In addition, we believe the framework we present here will form the basis for the development of future problem and solution space taxonomies, and methodologies for LLM-assisted ideation development and use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00946v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitong Li, Stefano Padilla, Pierre Le Bras, Junyu Dong, Mike Chantler</dc:creator>
    </item>
    <item>
      <title>How Do Teachers Create Pedagogical Chatbots?: Current Practices and Challenges</title>
      <link>https://arxiv.org/abs/2503.00967</link>
      <description>arXiv:2503.00967v1 Announce Type: new 
Abstract: AI chatbots have emerged as promising educational tools for personalized learning experiences, with advances in large language models (LLMs) enabling teachers to create and customize these chatbots for their specific classroom needs. However, there is a limited understanding of how teachers create pedagogical chatbots and integrate them into their lessons. Through semi-structured interviews with seven K-12 teachers, we examined their practices and challenges when designing, implementing, and deploying chatbots. Our findings revealed that teachers prioritize developing task-specific chatbots aligned with their lessons. Teachers engaged in various creation practices and had different challenges; novices struggled mainly with initial design and technical implementation, while experienced teachers faced challenges with technical aspects and analyzing conversational data. Based on these insights, we explore approaches to supporting teachers' chatbot creation process and opportunities for designing future chatbot creation systems. This work provides practical insights into deepening the understanding of teacher-driven AI chatbots and AI-augmented learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00967v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Minju Yoo, Hyoungwook Jin, Juho Kim</dc:creator>
    </item>
    <item>
      <title>AlphaPIG: The Nicest Way to Prolong Interactive Gestures in Extended Reality</title>
      <link>https://arxiv.org/abs/2503.01011</link>
      <description>arXiv:2503.01011v1 Announce Type: new 
Abstract: Mid-air gestures serve as a common interaction modality across Extended Reality (XR) applications, enhancing engagement and ownership through intuitive body movements. However, prolonged arm movements induce shoulder fatigue, known as "Gorilla Arm Syndrome", degrading user experience and reducing interaction duration. Although existing ergonomic techniques derived from Fitts' law (such as reducing target distance, increasing target width, and modifying control-display gain) provide some fatigue mitigation, their implementation in XR applications remains challenging due to the complex balance between user engagement and physical exertion. We present AlphaPIG, a meta-technique designed to Prolong Interactive Gestures by leveraging real-time fatigue predictions. AlphaPIG assists designers in extending and improving XR interactions by enabling automated fatigue-based interventions. Through adjustment of intervention timing and intensity decay rate, designers can explore and control the trade-off between fatigue reduction and potential effects such as decreased body ownership. We validated AlphaPIG's effectiveness through a study (N=22) implementing the widely-used Go-Go technique. Results demonstrated that AlphaPIG significantly reduces shoulder fatigue compared to non-adaptive Go-Go, while maintaining comparable perceived body ownership and agency. Based on these findings, we discuss positive and negative perceptions of the intervention. By integrating real-time fatigue prediction with adaptive intervention mechanisms, AlphaPIG constitutes a critical first step towards creating fatigue-aware applications in XR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01011v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714249</arxiv:DOI>
      <dc:creator>Yi Li, Florian Fischer, Tim Dwyer, Barrett Ens, Robert Crowther, Per Ola Kristensson, Benjamin Tag</dc:creator>
    </item>
    <item>
      <title>Optimizing Curve-Based Selection with On-Body Surfaces in Virtual Environments</title>
      <link>https://arxiv.org/abs/2503.01015</link>
      <description>arXiv:2503.01015v1 Announce Type: new 
Abstract: Virtual Reality (VR) interfaces often rely on linear ray-casting for object selection but struggle with precision in dense or occluded environments. This late-breaking work introduces an optimized dual-layered selection mechanism combining dynamic Bezier Curves, controlled via finger gestures, with on-body interaction surfaces to enhance precision and immersion. Bezier Curves offer fine-grained control and flexibility in complex scenarios, while on-body surfaces project nearby virtual objects onto the user's forearm, leveraging proprioception and tactile feedback. A preliminary qualitative study ($N$ = 24) compared two interaction paradigms (Bezier Curve vs. Linear Ray) and two interaction media (On-body vs. Mid-air). Participants praised the Bezier Curve's ability to target occluded objects but noted the physical demand. On-body interactions were favored for their immersive qualities, while mid-air interactions were appreciated for maintaining focus on the virtual scene. These findings highlight the importance of balancing ease of learning and precise control when designing VR selection techniques, opening avenues for further exploration of curve-based and on-body interactions in dense virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01015v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719908</arxiv:DOI>
      <dc:creator>Xiang Li, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>Bridging Generations using AI-Supported Co-Creative Activities</title>
      <link>https://arxiv.org/abs/2503.01154</link>
      <description>arXiv:2503.01154v1 Announce Type: new 
Abstract: Intergenerational co-creation using technology between grandparents and grandchildren can be challenging due to differences in technological familiarity. AI has emerged as a promising tool to support co-creative activities, offering flexibility and creative assistance, but its role in facilitating intergenerational connection remains underexplored. In this study, we conducted a user study with 29 grandparent-grandchild groups engaged in AI-supported story creation to examine how AI-assisted co-creation can foster meaningful intergenerational bonds. Our findings show that grandchildren managed the technical aspects, while grandparents contributed creative ideas and guided the storytelling. AI played a key role in structuring the activity, facilitating brainstorming, enhancing storytelling, and balancing the contributions of both generations. The process fostered mutual appreciation, with each generation recognizing the strengths of the other, leading to an engaging and cohesive co-creation process. We offer design implications for integrating AI into intergenerational co-creative activities, emphasizing how AI can enhance connection across skill levels and technological familiarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01154v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713718</arxiv:DOI>
      <dc:creator>Callie Y. Kim, Arissa J. Sato, Nathan Thomas White, Hui-Ru Ho, Christine P. Lee, Yuna Hwang, Bilge Mutlu</dc:creator>
    </item>
    <item>
      <title>HaloTouch: Using IR Multi-path Interference to Support Touch Interactions With General Surfaces</title>
      <link>https://arxiv.org/abs/2503.01197</link>
      <description>arXiv:2503.01197v1 Announce Type: new 
Abstract: Sensing touch on arbitrary surfaces has long been a goal of ubiquitous computing, but often requires instrumenting the surface. Depth camera-based systems have emerged as a promising solution for minimizing instrumentation, but at the cost of high touch-down detection error rates, high touch latency, and high minimum hover distance, limiting them to basic tasks. We developed HaloTouch, a vision-based system which exploits a multipath interference effect from an off-the-shelf time-of-flight depth camera to enable fast, accurate touch interactions on general surfaces. HaloTouch achieves a 99.2% touch-down detection accuracy across various materials, with a motion-to-photon latency of 150 ms. With a brief (20s) user-specific calibration, HaloTouch supports millimeter-accurate hover sensing as well as continuous pressure sensing. We conducted a user study with 12 participants, including a typing task demonstrating text input at 26.3 AWPM. HaloTouch shows promise for more robust, dynamic touch interactions without instrumenting surfaces or adding hardware to users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01197v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714179</arxiv:DOI>
      <dc:creator>Ziyi Xia, Xincheng Huang, Sidney S Fels, Robert Xiao</dc:creator>
    </item>
    <item>
      <title>Augmenting Online Meetings with Context-Aware Real-time Music Generation</title>
      <link>https://arxiv.org/abs/2503.01354</link>
      <description>arXiv:2503.01354v1 Announce Type: new 
Abstract: As online communication continues to expand, participants often face cognitive fatigue and reduced engagement. Cognitive augmentation, which leverages technology to enhance human abilities, offers promising solutions to these challenges. In this study, we investigate the potential of generative artificial intelligence (GenAI) for real-time music generation to enrich online meetings. We introduce Discussion Jockey 2, a system that dynamically produces background music in response to live conversation transcripts. Through a user study involving 14 participants in an online interview setting, we examine the system's impact on relaxation, concentration, and overall user experience. The findings reveal that AI-generated background music significantly enhances user relaxation (average score: 5.75/9) and concentration (average score: 5.86/9). This research underscores the promise of context-aware music generation in improving the quality of online communication and points to future directions for optimizing its implementation across various virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01354v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haruki Suzawa, Ko Watanabe, Andreas Dengel, Shoya Ishimaru</dc:creator>
    </item>
    <item>
      <title>RemiHaven: Integrating "In-Town" and "Out-of-Town" Peers to Provide Personalized Reminiscence Support for Older Drifters</title>
      <link>https://arxiv.org/abs/2503.01358</link>
      <description>arXiv:2503.01358v1 Announce Type: new 
Abstract: With increasing social mobility and an aging society, more older adults in China are migrating to new cities, known as "older drifters." Due to fewer social connections and cultural adaptation challenges, they face negative emotions such as loneliness and depression. While reminiscence-based interventions have been used to improve older adults' psychological well-being, challenges such as the lack of tangible materials and limited social resources constrain the feasibility of traditional reminiscence approaches for older drifters. To address this challenge, we designed RemiHaven, a personalized reminiscence support tool based on a two-phase formative study. It integrates "In-Town" and "Out-of-Town" peer agents to enhance personalization, engagement, and emotional resonance in the reminiscence process, powered by Multimodal Large Language Models (MLLMs). Our evaluations show RemiHaven's strengths in supporting reminiscence while identifying potential challenges. We conclude by offering insights for the future design of reminiscence support tools for older migrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01358v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuechen Zhang, Changyang He, Peng Zhang, Hansu Gu, Ning Gu, Qi Shen, Zhan Hu, Tun Lu</dc:creator>
    </item>
    <item>
      <title>RevTogether: Supporting Science Story Revision with Multiple AI Agents</title>
      <link>https://arxiv.org/abs/2503.01608</link>
      <description>arXiv:2503.01608v1 Announce Type: new 
Abstract: As a popular form of science communication, science stories attract readers because they combine engaging narratives with comprehensible scientific knowledge. However, crafting such stories requires substantial skill and effort, as writers must navigate complex scientific concepts and transform them into coherent and accessible narratives tailored to audiences with varying levels of scientific literacy. To address the challenge, we propose RevTogether, a multi-agent system (MAS) designed to support revision of science stories with human-like AI agents (using GPT-4o). RevTogether allows AI agents to simulate affects in addition to providing comments and writing suggestions, while offering varying degrees of user agency. Our preliminary user study with non-expert writers (N=3) highlighted the need for transparency in AI agents' decision-making processes to support learning and suggested that emotional interactions could enhance human-AI collaboration in science storytelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01608v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719888</arxiv:DOI>
      <dc:creator>Yu Zhang, Kexue Fu, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>Lost in Moderation: How Commercial Content Moderation APIs Over- and Under-Moderate Group-Targeted Hate Speech and Linguistic Variations</title>
      <link>https://arxiv.org/abs/2503.01623</link>
      <description>arXiv:2503.01623v1 Announce Type: new 
Abstract: Commercial content moderation APIs are marketed as scalable solutions to combat online hate speech. However, the reliance on these APIs risks both silencing legitimate speech, called over-moderation, and failing to protect online platforms from harmful speech, known as under-moderation. To assess such risks, this paper introduces a framework for auditing black-box NLP systems. Using the framework, we systematically evaluate five widely used commercial content moderation APIs. Analyzing five million queries based on four datasets, we find that APIs frequently rely on group identity terms, such as ``black'', to predict hate speech. While OpenAI's and Amazon's services perform slightly better, all providers under-moderate implicit hate speech, which uses codified messages, especially against LGBTQIA+ individuals. Simultaneously, they over-moderate counter-speech, reclaimed slurs and content related to Black, LGBTQIA+, Jewish, and Muslim people. We recommend that API providers offer better guidance on API implementation and threshold setting and more transparency on their APIs' limitations.
  Warning: This paper contains offensive and hateful terms and concepts. We have chosen to reproduce these terms for reasons of transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01623v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713998</arxiv:DOI>
      <dc:creator>David Hartmann, Amin Oueslati, Dimitri Staufer, Lena Pohlmann, Simon Munzert, Hendrik Heuer</dc:creator>
    </item>
    <item>
      <title>No Evidence for LLMs Being Useful in Problem Reframing</title>
      <link>https://arxiv.org/abs/2503.01631</link>
      <description>arXiv:2503.01631v1 Announce Type: new 
Abstract: Problem reframing is a designerly activity wherein alternative perspectives are created to recast what a stated design problem is about. Generating alternative problem frames is challenging because it requires devising novel and useful perspectives that fit the given problem context. Large language models (LLMs) could assist this activity via their generative capability. However, it is not clear whether they can help designers produce high-quality frames. Therefore, we asked if there are benefits to working with LLMs. To this end, we compared three ways of using LLMs (N=280): 1) free-form, 2) direct generation, and 3) a structured approach informed by a theory of reframing. We found that using LLMs does not help improve the quality of problem frames. In fact, it increases the competence gap between experienced and inexperienced designers. Also, inexperienced ones perceived lower agency when working with LLMs. We conclude that there is no benefit to using LLMs in problem reframing and discuss possible factors for this lack of effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01631v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713273</arxiv:DOI>
      <dc:creator>Joongi Shin, Anna Polyanskaya, Andr\'es Lucero, Antti Oulasvirta</dc:creator>
    </item>
    <item>
      <title>DISCOVER: Data-driven Identification of Sub-activities via Clustering and Visualization for Enhanced Activity Recognition in Smart Homes</title>
      <link>https://arxiv.org/abs/2503.01733</link>
      <description>arXiv:2503.01733v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) using ambient sensors has great potential for practical applications, particularly in elder care and independent living. However, deploying HAR systems in real-world settings remains challenging due to the high cost of labeled data, the need for pre-segmented sensor streams, and the lack of flexibility in activity granularity. To address these limitations, we introduce DISCOVER, a method designed to discover fine-grained human sub-activities from unlabeled sensor data without relying on pre-segmentation. DISCOVER combines unsupervised feature extraction and clustering with a user-friendly visualization tool to streamline the labeling process. DISCOVER enables domain experts to efficiently annotate only a minimal set of representative cluster centroids, reducing the annotation workload to a small number of samples (0.05% of our dataset). We demonstrate DISCOVER's effectiveness through a re-annotation exercise on widely used HAR datasets, showing that it uncovers finer-grained activities and produces more nuanced annotations than traditional coarse labels. DISCOVER represents a step toward practical, deployable HAR systems that adapt to diverse real environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01733v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Karpekov, Sonia Chernova, Thomas Pl\"otz</dc:creator>
    </item>
    <item>
      <title>Designing VR Simulation System for Clinical Communication Training with LLMs-Based Embodied Conversational Agents</title>
      <link>https://arxiv.org/abs/2503.01767</link>
      <description>arXiv:2503.01767v1 Announce Type: new 
Abstract: VR simulation in Health Professions (HP) education demonstrates huge potential, but fixed learning content with little customization limits its application beyond lab environments. To address these limitations in the context of VR for patient communication training, we conducted a user-centered study involving semi-structured interviews with advanced HP students to understand their challenges in clinical communication training and perceptions of VR-based solutions. From this, we derived design insights emphasizing the importance of realistic scenarios, simple interactions, and unpredictable dialogues. Building on these insights, we developed the Virtual AI Patient Simulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and Embodied Conversational Agents (ECAs), supporting dynamic and customizable patient interactions for immersive learning. We also provided an example of how clinical professors could use user-friendly design forms to create personalized scenarios that align with course objectives in VAPS and discuss future implications of integrating AI-driven technologies into VR education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01767v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiuqi Tommy Zhu, Heidi Cheerman, Minxin Cheng, Sheri Kiami, Leanne Chukoskie, Eileen McGivney</dc:creator>
    </item>
    <item>
      <title>Using Collective Dialogues and AI to Find Common Ground Between Israeli and Palestinian Peacebuilders</title>
      <link>https://arxiv.org/abs/2503.01769</link>
      <description>arXiv:2503.01769v1 Announce Type: new 
Abstract: A growing body of work has shown that AI-assisted methods -- leveraging large language models (LLMs), social choice methods, and collective dialogues -- can help reduce polarization and foster common ground in controlled lab settings. But what can these approaches contribute in real-world contexts? We present a case study applying these techniques to find common ground between Israeli and Palestinian peacebuilders in the period following October 7th, 2023. From April to July 2024 an iterative deliberative process combining LLMs, bridging-based ranking, and collective dialogues was conducted in partnership with the Alliance for Middle East Peace. More than 100 civil society peacebuilders participated including Israeli Jews, Palestinian citizens of Israel, and Palestinians from the West Bank and Gaza. The process culminated in a set of collective statements, including joint demands to world leaders, with at least 84% agreement from participants on each side. In this paper we review the mechanics and implementation of the process, discuss results and learnings, and highlight open problems that warrant future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01769v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Konya, Luke Thorburn, Wasim Almasri, Oded Adomi Leshem, Ariel D. Procaccia, Lisa Schirch, Michiel A. Bakker</dc:creator>
    </item>
    <item>
      <title>Generalization of CNNs on Relational Reasoning with Bar Charts</title>
      <link>https://arxiv.org/abs/2503.00086</link>
      <description>arXiv:2503.00086v1 Announce Type: cross 
Abstract: This paper presents a systematic study of the generalization of convolutional neural networks (CNNs) and humans on relational reasoning tasks with bar charts. We first revisit previous experiments on graphical perception and update the benchmark performance of CNNs. We then test the generalization performance of CNNs on a classic relational reasoning task: estimating bar length ratios in a bar chart, by progressively perturbing the standard visualizations. We further conduct a user study to compare the performance of CNNs and humans. Our results show that CNNs outperform humans only when the training and test data have the same visual encodings. Otherwise, they may perform worse. We also find that CNNs are sensitive to perturbations in various visual encodings, regardless of their relevance to the target bars. Yet, humans are mainly influenced by bar lengths. Our study suggests that robust relational reasoning with visualizations is challenging for CNNs. Improving CNNs' generalization performance may require training them to better recognize task-related visual properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00086v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenxing Cui, Lu Chen, Yunhai Wang, Daniel Haehn, Yong Wang, Hanspeter Pfister</dc:creator>
    </item>
    <item>
      <title>Evaluation of LLMs-based Hidden States as Author Representations for Psychological Human-Centered NLP Tasks</title>
      <link>https://arxiv.org/abs/2503.00124</link>
      <description>arXiv:2503.00124v1 Announce Type: cross 
Abstract: Like most of NLP, models for human-centered NLP tasks -- tasks attempting to assess author-level information -- predominantly use representations derived from hidden states of Transformer-based LLMs. However, what component of the LM is used for the representation varies widely. Moreover, there is a need for Human Language Models (HuLMs) that implicitly model the author and provide a user-level hidden state. Here, we systematically evaluate different ways of representing documents and users using different LM and HuLM architectures to predict task outcomes as both dynamically changing states and averaged trait-like user-level attributes of valence, arousal, empathy, and distress. We find that representing documents as an average of the token hidden states performs the best generally. Further, while a user-level hidden state itself is rarely the best representation, we find its inclusion in the model strengthens token or document embeddings used to derive document- and user-level representations resulting in best performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00124v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Soni, Pranav Chitale, Khushboo Singh, Niranjan Balasubramanian, H. Andrew Schwartz</dc:creator>
    </item>
    <item>
      <title>Human-AI Collaboration: Trade-offs Between Performance and Preferences</title>
      <link>https://arxiv.org/abs/2503.00248</link>
      <description>arXiv:2503.00248v1 Announce Type: cross 
Abstract: Despite the growing interest in collaborative AI, designing systems that seamlessly integrate human input remains a major challenge. In this study, we developed a task to systematically examine human preferences for collaborative agents. We created and evaluated five collaborative AI agents with strategies that differ in the manner and degree they adapt to human actions. Participants interacted with a subset of these agents, evaluated their perceived traits, and selected their preferred agent. We used a Bayesian model to understand how agents' strategies influence the Human-AI team performance, AI's perceived traits, and the factors shaping human-preferences in pairwise agent comparisons. Our results show that agents who are more considerate of human actions are preferred over purely performance-maximizing agents. Moreover, we show that such human-centric design can improve the likability of AI collaborators without reducing performance. We find evidence for inequality-aversion effects being a driver of human choices, suggesting that people prefer collaborative agents which allow them to meaningfully contribute to the team. Taken together, these findings demonstrate how collaboration with AI can benefit from development efforts which include both subjective and objective metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00248v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas William Mayer, Sheer Karny, Jackie Ayoub, Miao Song, Danyang Tian, Ehsan Moradi-Pari, Mark Steyvers</dc:creator>
    </item>
    <item>
      <title>Xpress: A System For Dynamic, Context-Aware Robot Facial Expressions using Language Models</title>
      <link>https://arxiv.org/abs/2503.00283</link>
      <description>arXiv:2503.00283v1 Announce Type: cross 
Abstract: Facial expressions are vital in human communication and significantly influence outcomes in human-robot interaction (HRI), such as likeability, trust, and companionship. However, current methods for generating robotic facial expressions are often labor-intensive, lack adaptability across contexts and platforms, and have limited expressive ranges--leading to repetitive behaviors that reduce interaction quality, particularly in long-term scenarios. We introduce Xpress, a system that leverages language models (LMs) to dynamically generate context-aware facial expressions for robots through a three-phase process: encoding temporal flow, conditioning expressions on context, and generating facial expression code. We demonstrated Xpress as a proof-of-concept through two user studies (n=15x2) and a case study with children and parents (n=13), in storytelling and conversational scenarios to assess the system's context-awareness, expressiveness, and dynamism. Results demonstrate Xpress's ability to dynamically produce expressive and contextually appropriate facial expressions, highlighting its versatility and potential in HRI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00283v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Nikhil Antony, Maia Stiber, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Peek into the `White-Box': A Field Study on Bystander Engagement with Urban Robot Uncertainty</title>
      <link>https://arxiv.org/abs/2503.00337</link>
      <description>arXiv:2503.00337v1 Announce Type: cross 
Abstract: Uncertainty inherently exists in the autonomous decision-making process of robots. Involving humans in resolving this uncertainty not only helps robots mitigate it but is also crucial for improving human-robot interactions. However, in public urban spaces filled with unpredictability, robots often face heightened uncertainty without direct human collaborators. This study investigates how robots can engage bystanders for assistance in public spaces when encountering uncertainty and examines how these interactions impact bystanders' perceptions and attitudes towards robots. We designed and tested a speculative `peephole' concept that engages bystanders in resolving urban robot uncertainty. Our design is guided by considerations of non-intrusiveness and eliciting initiative in an implicit manner, considering bystanders' unique role as non-obligated participants in relation to urban robots. Drawing from field study findings, we highlight the potential of involving bystanders to mitigate urban robots' technological imperfections to both address operational challenges and foster public acceptance of urban robots. Furthermore, we offer design implications to encourage bystanders' involvement in mitigating the imperfections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00337v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713790</arxiv:DOI>
      <dc:creator>Xinyan Yu, Marius Hoggenmueller, Tram Thi Minh Tran, Yiyuan Wang, Qiuming Zhang, Martin Tomitsch</dc:creator>
    </item>
    <item>
      <title>Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriented Pivot Tasks</title>
      <link>https://arxiv.org/abs/2503.00401</link>
      <description>arXiv:2503.00401v1 Announce Type: cross 
Abstract: Perception-enhanced pre-training, particularly through grounding techniques, is widely adopted to enhance the performance of graphical user interface (GUI) agents. However, in resource-constrained scenarios, the format discrepancy between coordinate-oriented grounding and action-oriented reasoning limits the effectiveness of grounding for reasoning tasks. To address this challenge, we propose a query-oriented pivot approach called query inference, which serves as a bridge between GUI grounding and reasoning. By inferring potential user queries from a screenshot and its associated element coordinates, query inference improves the understanding of coordinates while aligning more closely with reasoning tasks. Experimental results show that query inference outperforms previous grounding techniques under the same training data scale. Notably, query inference achieves comparable or even better performance to large-scale grounding-enhanced OS-Atlas with less than 0.1% of training data. Furthermore, we explore the impact of reasoning formats and demonstrate that integrating additional semantic information into the input further boosts reasoning performance. The code is publicly available athttps://github.com/ZrW00/GUIPivot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00401v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongru Wu, Pengzhou Cheng, Zheng Wu, Tianjie Ju, Zhuosheng Zhang, Gongshen Liu</dc:creator>
    </item>
    <item>
      <title>Interacting with AI Reasoning Models: Harnessing "Thoughts" for AI-Driven Software Engineering</title>
      <link>https://arxiv.org/abs/2503.00483</link>
      <description>arXiv:2503.00483v1 Announce Type: cross 
Abstract: Recent advances in AI reasoning models provide unprecedented transparency into their decision-making processes, transforming them from traditional black-box systems into models that articulate step-by-step chains of thought rather than producing opaque outputs. This shift has the potential to improve software quality, explainability, and trust in AI-augmented development. However, software engineers rarely have the time or cognitive bandwidth to analyze, verify, and interpret every AI-generated thought in detail. Without an effective interface, this transparency could become a burden rather than a benefit.
  In this paper, we propose a vision for structuring the interaction between AI reasoning models and software engineers to maximize trust, efficiency, and decision-making power. We argue that simply exposing AI's reasoning is not enough -- software engineers need tools and frameworks that selectively highlight critical insights, filter out noise, and facilitate rapid validation of key assumptions. To illustrate this challenge, we present motivating examples in which AI reasoning models state their assumptions when deciding which external library to use and produce divergent reasoning paths and recommendations about security vulnerabilities, highlighting the need for an interface that prioritizes actionable insights while managing uncertainty and resolving conflicts. We then outline a research roadmap for integrating automated summarization, assumption validation, and multi-model conflict resolution into software engineering workflows. Achieving this vision will unlock the full potential of AI reasoning models to enable software engineers to make faster, more informed decisions without being overwhelmed by unnecessary detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00483v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Treude, Raula Gaikovina Kula</dc:creator>
    </item>
    <item>
      <title>Embracing Diversity: A Multi-Perspective Approach with Soft Labels</title>
      <link>https://arxiv.org/abs/2503.00489</link>
      <description>arXiv:2503.00489v1 Announce Type: cross 
Abstract: Prior studies show that adopting the annotation diversity shaped by different backgrounds and life experiences and incorporating them into the model learning, i.e. multi-perspective approach, contribute to the development of more responsible models. Thus, in this paper we propose a new framework for designing and further evaluating perspective-aware models on stance detection task,in which multiple annotators assign stances based on a controversial topic. We also share a new dataset established through obtaining both human and LLM annotations. Results show that the multi-perspective approach yields better classification performance (higher F1-scores), outperforming the traditional approaches that use a single ground-truth, while displaying lower model confidence scores, probably due to the high level of subjectivity of the stance detection task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00489v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, Fosca Giannotti, Tommaso Cucinotta</dc:creator>
    </item>
    <item>
      <title>Show Me Why It's Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison</title>
      <link>https://arxiv.org/abs/2503.00618</link>
      <description>arXiv:2503.00618v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) holds the promise of alleviating the burden of debugging and fixing software bugs. Despite this, developers still need to manually inspect each patch to confirm its correctness, which is tedious and time-consuming. This challenge is exacerbated in the presence of plausible patches, which accidentally pass test cases but may not correctly fix the bug. To address this challenge, we propose an interactive approach called iFix to facilitate patch understanding and comparison based on their runtime difference. iFix performs static analysis to identify runtime variables related to the buggy statement and captures their runtime values during execution for each patch. These values are then aligned across different patch candidates, allowing users to compare and contrast their runtime behavior. To evaluate iFix, we conducted a within-subjects user study with 28 participants. Compared with manual inspection and a state-of-the-art interactive patch filtering technique, iFix reduced participants' task completion time by 36% and 33% while also improving their confidence by 50% and 20%, respectively. Besides, quantitative experiments demonstrate that iFix improves the ranking of correct patches by at least 39% compared with other patch ranking methods and is generalizable to different APR tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00618v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3720510</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang. 9, OOPSLA1, Article 145 (April 2025)</arxiv:journal_reference>
      <dc:creator>Ruixin Wang, Zhongkai Zhao, Le Fang, Nan Jiang, Yiling Lou, Lin Tan, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>Speculative Ad-hoc Querying</title>
      <link>https://arxiv.org/abs/2503.00714</link>
      <description>arXiv:2503.00714v1 Announce Type: cross 
Abstract: Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that leverages Large Language Models (LLMs) to predict likely queries based on the database schema, the user's past queries, and their incomplete query. Since exact query prediction is infeasible, SpeQL speculates on partial queries in two ways: 1) it predicts the query structure to compile and plan queries in advance, and 2) it precomputes smaller temporary tables that are much smaller than the original database, but are still predicted to contain all information necessary to answer the user's final query. Additionally, SpeQL continuously displays results for speculated queries and subqueries in real time, aiding exploratory analysis. A utility/user study showed that SpeQL improved task completion time, and participants reported that its speculative display of results helped them discover patterns in the data more quickly. In the study, SpeQL improves user's query latency by up to $289\times$ and kept the overhead reasonable, at $\$4$ per hour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00714v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Li, Srikanth Kandula, Maria Angels de Luis Balaguer, Aditya Akella, Venkat Arun</dc:creator>
    </item>
    <item>
      <title>Evidence of conceptual mastery in the application of rules by Large Language Models</title>
      <link>https://arxiv.org/abs/2503.00992</link>
      <description>arXiv:2503.00992v1 Announce Type: cross 
Abstract: In this paper we leverage psychological methods to investigate LLMs' conceptual mastery in applying rules. We introduce a novel procedure to match the diversity of thought generated by LLMs to that observed in a human sample. We then conducted two experiments comparing rule-based decision-making in humans and LLMs. Study 1 found that all investigated LLMs replicated human patterns regardless of whether they are prompted with scenarios created before or after their training cut-off. Moreover, we found unanticipated differences between the two sets of scenarios among humans. Surprisingly, even these differences were replicated in LLM responses. Study 2 turned to a contextual feature of human rule application: under forced time delay, human samples rely more heavily on a rule's text than on other considerations such as a rule's purpose.. Our results revealed that some models (Gemini Pro and Claude 3) responded in a human-like manner to a prompt describing either forced delay or time pressure, while others (GPT-4o and Llama 3.2 90b) did not. We argue that the evidence gathered suggests that LLMs have mastery over the concept of rule, with implications for both legal decision making and philosophical inquiry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00992v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e Luiz Nunes, Guilherme FCF Almeida, Brian Flanagan</dc:creator>
    </item>
    <item>
      <title>Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers</title>
      <link>https://arxiv.org/abs/2503.01163</link>
      <description>arXiv:2503.01163v1 Announce Type: cross 
Abstract: Prompt optimization aims to search for effective prompts that enhance the performance of large language models (LLMs). Although existing prompt optimization methods have discovered effective prompts, they often differ from sophisticated prompts carefully designed by human experts. Prompt design strategies, representing best practices for improving prompt performance, can be key to improving prompt optimization. Recently, a method termed the Autonomous Prompt Engineering Toolbox (APET) has incorporated various prompt design strategies into the prompt optimization process. In APET, the LLM is needed to implicitly select and apply the appropriate strategies because prompt design strategies can have negative effects. This implicit selection may be suboptimal due to the limited optimization capabilities of LLMs. This paper introduces Optimizing Prompts with sTrategy Selection (OPTS), which implements explicit selection mechanisms for prompt design. We propose three mechanisms, including a Thompson sampling-based approach, and integrate them into EvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for two LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench Hard. Our results show that the selection of prompt design strategies improves the performance of EvoPrompt, and the Thompson sampling-based mechanism achieves the best overall results. Our experimental code is provided at https://github.com/shiralab/OPTS .</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01163v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rin Ashizawa, Yoichi Hirose, Nozomu Yoshinari, Kento Uchida, Shinichi Shirakawa</dc:creator>
    </item>
    <item>
      <title>Victim-Centred Abuse Investigations and Defenses for Social Media Platforms</title>
      <link>https://arxiv.org/abs/2503.01327</link>
      <description>arXiv:2503.01327v1 Announce Type: cross 
Abstract: Online abuse, a persistent aspect of social platform interactions, impacts user well-being and exposes flaws in platform designs that include insufficient detection efforts and inadequate victim protection measures. Ensuring safety in platform interactions requires the integration of victim perspectives in the design of abuse detection and response systems. In this paper, we conduct surveys (n = 230) and semi-structured interviews (n = 15) with students at a minority-serving institution in the US, to explore their experiences with abuse on a variety of social platforms, their defense strategies, and their recommendations for social platforms to improve abuse responses. We build on study findings to propose design requirements for abuse defense systems and discuss the role of privacy, anonymity, and abuse attribution requirements in their implementation. We introduce ARI, a blueprint for a unified, transparent, and personalized abuse response system for social platforms that sustainably detects abuse by leveraging the expertise of platform users, incentivized with proceeds obtained from abusers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01327v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaid Hakami, Ashfaq Ali Shafin, Peter J. Clarke, Niki Pissinou, Bogdan Carbunar</dc:creator>
    </item>
    <item>
      <title>Blind Augmentation: Calibration-free Camera Distortion Model Estimation for Real-time Mixed-reality Consistency</title>
      <link>https://arxiv.org/abs/2503.01387</link>
      <description>arXiv:2503.01387v1 Announce Type: cross 
Abstract: Real camera footage is subject to noise, motion blur (MB) and depth of field (DoF). In some applications these might be considered distortions to be removed, but in others it is important to model them because it would be ineffective, or interfere with an aesthetic choice, to simply remove them. In augmented reality applications where virtual content is composed into a live video feed, we can model noise, MB and DoF to make the virtual content visually consistent with the video. Existing methods for this typically suffer two main limitations. First, they require a camera calibration step to relate a known calibration target to the specific cameras response. Second, existing work require methods that can be (differentiably) tuned to the calibration, such as slow and specialized neural networks. We propose a method which estimates parameters for noise, MB and DoF instantly, which allows using off-the-shelf real-time simulation methods from e.g., a game engine in compositing augmented content. Our main idea is to unlock both features by showing how to use modern computer vision methods that can remove noise, MB and DoF from the video stream, essentially providing self-calibration. This allows to auto-tune any black-box real-time noise+MB+DoF method to deliver fast and high-fidelity augmentation consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01387v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddhant Prakash, David R. Walton, Rafael K. dos Anjos, Anthony Steed, Tobias Ritschel</dc:creator>
    </item>
    <item>
      <title>Student engagement in collaborative learning with AI agents in an LLM-empowered learning environment: A cluster analysis</title>
      <link>https://arxiv.org/abs/2503.01694</link>
      <description>arXiv:2503.01694v1 Announce Type: cross 
Abstract: Integrating LLM models into educational practice fosters personalized learning by accommodating the diverse behavioral patterns of different learner types. This study aims to explore these learner types within a novel interactive setting, providing a detailed analysis of their distinctive characteristics and interaction dynamics. The research involved 110 students from a university in China, who engaged with multiple LLM agents in an LLM-empowered learning environment, completing coursework across six modules. Data on the students' non-cognitive traits, course engagement, and AI interaction patterns were collected and analyzed. Using hierarchical cluster analysis, the students were classified into three distinct groups: active questioners, responsive navigators, and silent listeners. Epistemic network analysis was then applied to further delineate the interaction profiles and cognitive engagement of different types of learners. The findings underscore how different learner types engage with human-AI interactive learning and offer practical implications for the design of adaptive educational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01694v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanxin Hao, Jianxiao Jiang, Jifan Yu, Zhiyuan Liu, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm &amp; Accelerator Co-Design</title>
      <link>https://arxiv.org/abs/2206.00877</link>
      <description>arXiv:2206.00877v3 Announce Type: replace 
Abstract: Eye tracking has become an essential human-machine interaction modality for providing immersive experience in numerous virtual and augmented reality (VR/AR) applications desiring high throughput (e.g., 240 FPS), small-form, and enhanced visual privacy. However, existing eye tracking systems are still limited by their: (1) large form-factor largely due to the adopted bulky lens-based cameras; and (2) high communication cost required between the camera and backend processor, thus prohibiting their more extensive applications. To this end, we propose a lensless FlatCam-based eye tracking algorithm and accelerator co-design framework dubbed EyeCoD to enable eye tracking systems with a much reduced form-factor and boosted system efficiency without sacrificing the tracking accuracy, paving the way for next-generation eye tracking solutions. On the system level, we advocate the use of lensless FlatCams to facilitate the small form-factor need in mobile eye tracking systems. On the algorithm level, EyeCoD integrates a predict-then-focus pipeline that first predicts the region-of-interest (ROI) via segmentation and then only focuses on the ROI parts to estimate gaze directions, greatly reducing redundant computations and data movements. On the hardware level, we further develop a dedicated accelerator that (1) integrates a novel workload orchestration between the aforementioned segmentation and gaze estimation models, (2) leverages intra-channel reuse opportunities for depth-wise layers, and (3) utilizes input feature-wise partition to save activation memory size. On-silicon measurement validates that our EyeCoD consistently reduces both the communication and computation costs, leading to an overall system speedup of 10.95x, 3.21x, and 12.85x over CPUs, GPUs, and a prior-art eye tracking processor called CIS-GEP, respectively, while maintaining the tracking accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.00877v3</guid>
      <category>cs.HC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3470496.3527443</arxiv:DOI>
      <dc:creator>Haoran You, Cheng Wan, Yang Zhao, Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Shang Wu, Shunyao Zhang, Yongan Zhang, Chaojian Li, Vivek Boominathan, Ashok Veeraraghavan, Ziyun Li, Yingyan Celine Lin</dc:creator>
    </item>
    <item>
      <title>What-if Analysis for Business Professionals: Current Practices and Future Opportunities</title>
      <link>https://arxiv.org/abs/2212.13643</link>
      <description>arXiv:2212.13643v4 Announce Type: replace 
Abstract: What-if analysis (WIA) is essential for data-driven decision-making, allowing users to assess how changes in variables impact outcomes and explore alternative scenarios. Existing WIA research primarily supports the workflows of data scientists and analysts, and largely overlooks business professionals who engage in WIA through non-technical means. To bridge this gap, we conduct a two-part user study with 22 business professionals across marketing, sales, product, and operations roles. The first study examines their existing WIA practices, tools, and challenges. Findings reveal that business professionals perform many WIA techniques independently using rudimentary tools due to various constraints. We then implement representative WIA techniques in a visual analytics prototype and use it as a probe to conduct a follow-up study evaluating business professionals' practical use of the techniques. Results show that these techniques improve decision-making efficiency and confidence while underscoring the need for better data preparation, risk assessment, and domain knowledge integration support. Finally, we offer design recommendations to enhance future business analytics systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13643v4</guid>
      <category>cs.HC</category>
      <category>cs.DB</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sneha Gathani, Zhicheng Liu, Peter J. Haas, \c{C}a\u{g}atay Demiralp</dc:creator>
    </item>
    <item>
      <title>Visualizing Causality in Mixed Reality for Manual Task Learning: An Exploratory Study</title>
      <link>https://arxiv.org/abs/2310.13167</link>
      <description>arXiv:2310.13167v4 Announce Type: replace 
Abstract: Mixed Reality (MR) is gaining prominence in manual task skill learning due to its in-situ, embodied, and immersive experience. To teach manual tasks, current methodologies break the task into hierarchies (tasks into subtasks) and visualize the current subtask and future in terms of causality. Existing psychology literature also shows that humans learn tasks by breaking them into hierarchies. In order to understand the design space of information visualized to the learner for better task understanding, we conducted a user study with 48 users. The study was conducted using a complex assembly task, which involves learning of both actions and tool usage. We aim to explore the effect of visualization of causality in the hierarchy for manual task learning in MR by four options: no causality, event level causality, interaction level causality, and gesture level causality. The results show that the user understands and performs best when all the level of causality is shown to the user. Based on the results, we further provide design recommendations and in-depth discussions for future manual task learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13167v4</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2025.3542949</arxiv:DOI>
      <dc:creator>Rahul Jain, Jingyu Shi, Andrew Benton, Moiz Rasheed, Hyungjun Doh, Subramanian Chidambaram, Karthik Ramani</dc:creator>
    </item>
    <item>
      <title>Canvil: Designerly Adaptation for LLM-Powered User Experiences</title>
      <link>https://arxiv.org/abs/2401.09051</link>
      <description>arXiv:2401.09051v3 Announce Type: replace 
Abstract: Advancements in large language models (LLMs) are sparking a proliferation of LLM-powered user experiences (UX). In product teams, designers often craft UX to meet user needs, but it is unclear how they engage with LLMs as a novel design material. Through a formative study with 12 designers, we find that designers seek a translational process that enables design requirements to shape and be shaped by LLM behavior, motivating a need for designerly adaptation to facilitate this translation. We then built Canvil, a Figma widget that operationalizes designerly adaptation. We used Canvil as a probe to study designerly adaptation in a group-based design study (6 groups, N=17), finding that designers constructively iterated on both adaptation approaches and interface designs to enhance end-user interaction with LLMs. Furthermore, designers identified promising collaborative workflows for designerly adaptation. Our work opens new avenues for processes and tools that foreground designers' human-centered expertise when developing LLM-powered applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09051v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. J. Kevin Feng, Q. Vera Liao, Ziang Xiao, Jennifer Wortman Vaughan, Amy X. Zhang, David W. McDonald</dc:creator>
    </item>
    <item>
      <title>End User Authoring of Personalized Content Classifiers: Comparing Example Labeling, Rule Writing, and LLM Prompting</title>
      <link>https://arxiv.org/abs/2409.03247</link>
      <description>arXiv:2409.03247v2 Announce Type: replace 
Abstract: Existing tools for laypeople to create personal classifiers often assume a motivated user working uninterrupted in a single, lengthy session. However, users tend to engage with social media casually, with many short sessions on an ongoing, daily basis. To make creating personal classifiers for content curation easier for such users, tools should support rapid initialization and iterative refinement. In this work, we compare three strategies -- (1) example labeling, (2) rule writing, and (3) large language model (LLM) prompting -- for end users to build personal content classifiers. From an experiment with 37 non-programmers tasked with creating personalized moderation filters, we found that participants preferred different initializing strategies in different contexts, despite LLM prompting's better performance. However, all strategies faced challenges with iterative refinement. To overcome challenges in iterating on their prompts, participants even adopted hybrid approaches such as providing examples as in-context examples or writing rule-like prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03247v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leijie Wang, Kathryn Yurechko, Pranati Dani, Quan Ze Chen, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and Risk Detection of Cancer Treatment-Induced Cardiotoxicity</title>
      <link>https://arxiv.org/abs/2410.04592</link>
      <description>arXiv:2410.04592v3 Announce Type: replace 
Abstract: Despite recent advances in cancer treatments that prolong patients' lives, treatment-induced cardiotoxicity remains one severe side effect. The clinical decision-making of cardiotoxicity is challenging, as non-clinical symptoms can be missed until life-threatening events occur at a later stage, and clinicians already have a high workload centered on the treatment, not the side effects. Our project starts with a participatory design study with 11 clinicians to understand their practices and needs; then we build a multimodal AI system, CardioAI, that integrates wearables and LLM-powered voice assistants to monitor multimodal non-clinical symptoms. Also, the system includes an explainable risk prediction module that can generate cardiotoxicity risk scores and summaries as explanations to support clinicians' decision-making. We conducted a heuristic evaluation with four clinical experts and found that they all believe CardioAI integrates well into their workflow, reduces their information overload, and enables them to make more informed decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04592v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siyi Wu, Weidan Cao, Shihan Fu, Bingsheng Yao, Ziqi Yang, Changchang Yin, Varun Mishra, Daniel Addison, Ping Zhang, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching Assistant's Perspective</title>
      <link>https://arxiv.org/abs/2410.08899</link>
      <description>arXiv:2410.08899v2 Announce Type: replace 
Abstract: Integrating large language models (LLMs) like ChatGPT into computer science education offers transformative potential for complex courses such as data structures and algorithms (DSA). This study examines ChatGPT as a supplementary tool for teaching assistants (TAs), guided by structured prompts and human oversight, to enhance instruction and student outcomes. A controlled experiment compared traditional TA-led instruction with a hybrid approach where TAs used ChatGPT-4o and ChatGPT o1 to generate exercises, clarify concepts, and provide feedback. Structured prompts emphasized problem decomposition, real-world context, and code examples, enabling tailored support while mitigating over-reliance on AI. Results demonstrated the hybrid approach's efficacy, with students in the ChatGPT-assisted group scoring 16.50 points higher on average and excelling in advanced topics. However, ChatGPT's limitations necessitated TA verification. This framework highlights the dual role of LLMs: augmenting TA efficiency while ensuring accuracy through human oversight, offering a scalable solution for human-AI collaboration in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08899v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3720291</arxiv:DOI>
      <dc:creator>Pooriya Jamie, Reyhaneh Hajihashemi, Sharareh Alipour</dc:creator>
    </item>
    <item>
      <title>CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR</title>
      <link>https://arxiv.org/abs/2411.04671</link>
      <description>arXiv:2411.04671v3 Announce Type: replace 
Abstract: Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering affordable consumer-grade head-mounted displays (HMDs), XR will likely become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)--powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. This paper provides the community with an open-source, customizable, extendable, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with widely used LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: https://gitlab.lrz.de/hctl/cuify</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04671v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/AIxVR63409.2025.00037</arxiv:DOI>
      <dc:creator>Kadir Burak Buldu, S\"uleyman \"Ozdel, Ka Hei Carrie Lau, Mengdi Wang, Daniel Saad, Sofie Sch\"onborn, Auxane Boch, Enkelejda Kasneci, Efe Bozkir</dc:creator>
    </item>
    <item>
      <title>The Illusion of Empathy: How AI Chatbots Shape Conversation Perception</title>
      <link>https://arxiv.org/abs/2411.12877</link>
      <description>arXiv:2411.12877v3 Announce Type: replace 
Abstract: As AI chatbots increasingly incorporate empathy, understanding user-centered perceptions of chatbot empathy and its impact on conversation quality remains essential yet under-explored. This study examines how chatbot identity and perceived empathy influence users' overall conversation experience. Analyzing 155 conversations from two datasets, we found that while GPT-based chatbots were rated significantly higher in conversational quality, they were consistently perceived as less empathetic than human conversational partners. Empathy ratings from GPT-4o annotations aligned with user ratings, reinforcing the perception of lower empathy in chatbots compared to humans. Our findings underscore the critical role of perceived empathy in shaping conversation quality, revealing that achieving high-quality human-AI interactions requires more than simply embedding empathetic language; it necessitates addressing the nuanced ways users interpret and experience empathy in conversations with chatbots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12877v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tingting Liu, Salvatore Giorgi, Ankit Aich, Allison Lahnala, Brenda Curtis, Lyle Ungar, Jo\~ao Sedoc</dc:creator>
    </item>
    <item>
      <title>Unveiling High-dimensional Backstage: A Survey for Reliable Visual Analytics with Dimensionality Reduction</title>
      <link>https://arxiv.org/abs/2501.10168</link>
      <description>arXiv:2501.10168v2 Announce Type: replace 
Abstract: Dimensionality reduction (DR) techniques are essential for visually analyzing high-dimensional data. However, visual analytics using DR often face unreliability, stemming from factors such as inherent distortions in DR projections. This unreliability can lead to analytic insights that misrepresent the underlying data, potentially resulting in misguided decisions. To tackle these reliability challenges, we review 133 papers that address the unreliability of visual analytics using DR. Through this review, we contribute (1) a workflow model that describes the interaction between analysts and machines in visual analytics using DR, and (2) a taxonomy that identifies where and why reliability issues arise within the workflow, along with existing solutions for addressing them. Our review reveals ongoing challenges in the field, whose significance and urgency are validated by five expert researchers. This review also finds that the current research landscape is skewed toward developing new DR techniques rather than their interpretation or evaluation, where we discuss how the HCI community can contribute to broadening this focus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10168v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713551</arxiv:DOI>
      <dc:creator>Hyeon Jeon, Hyunwook Lee, Yun-Hsin Kuo, Taehyun Yang, Daniel Archambault, Sungahn Ko, Takanori Fujiwara, Kwan-Liu Ma, Jinwook Seo</dc:creator>
    </item>
    <item>
      <title>Unsettling the Hegemony of Intention: Agonistic Image Generation</title>
      <link>https://arxiv.org/abs/2502.15242</link>
      <description>arXiv:2502.15242v2 Announce Type: replace 
Abstract: Current image generation paradigms prioritize actualizing user intention - "see what you intend" - but often neglect the sociopolitical dimensions of this process. However, it is increasingly evident that image generation is political, contributing to broader social struggles over visual meaning. This sociopolitical aspect was highlighted by the March 2024 Gemini controversy, where Gemini faced criticism for inappropriately injecting demographic diversity into user prompts. Although the developers sought to redress image generation's sociopolitical dimension by introducing diversity "corrections," their opaque imposition of a standard for "diversity" ultimately proved counterproductive. In this paper, we present an alternative approach: an image generation interface designed to embrace open negotiation along the sociopolitical dimensions of image creation. Grounded in the principles of agonistic pluralism (from the Greek agon, meaning struggle), our interface actively engages users with competing visual interpretations of their prompts. Through a lab study with 29 participants, we evaluate our agonistic interface on its ability to facilitate reflection - engagement with other perspectives and challenging dominant assumptions - a core principle that underpins agonistic contestation. We compare it to three existing paradigms: a standard interface, a Gemini-style interface that produces "diverse" images, and an intention-centric interface suggesting prompt refinements. Our findings demonstrate that the agonistic interface enhances reflection across multiple measures, but also that reflection depends on users perceiving the interface as both appropriate and empowering; introducing diversity without grounding it in relevant political contexts was perceived as inauthentic. Our results suggest that diversity and user intention should not be treated as opposing values to be balanced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15242v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Shaw, Andre Ye, Ranjay Krishna, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation</title>
      <link>https://arxiv.org/abs/2502.19082</link>
      <description>arXiv:2502.19082v2 Announce Type: replace 
Abstract: Through a three-part co-design study involving 19 teens aged 13-18, we identify key barriers to effective boundary regulation on social media, including ambiguous audience expectations, social risks associated with oversharing, and the lack of design affordances that facilitate trust-building. Our findings reveal that while adolescents seek casual, frequent sharing to strengthen relationships, existing platform norms and designs often discourage such interactions, leading to withdrawal. To address these challenges, we introduce trust-enabled privacy as a design framework that recognizes trust, whether building or eroding, as central to boundary regulation. When trust is supported, boundary regulation becomes more adaptive and empowering; when it erodes, users default to self-censorship or withdrawal. We propose concrete design affordances, including guided disclosure, contextual audience segmentation, intentional engagement signaling, and trust-centered norms, to help platforms foster a more dynamic and nuanced privacy experience for teen social media users. By reframing privacy as a trust-driven process rather than a rigid control-based trade-off, this work provides empirical insights and actionable guidelines for designing social media environments that empower teens to manage their online presence while fostering meaningful social connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19082v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JaeWon Kim, Robert Wolfe, Ramya Bhagirathi Subramanian, Mei-Hsuan Lee, Jessica Colnago, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention</title>
      <link>https://arxiv.org/abs/2502.19877</link>
      <description>arXiv:2502.19877v2 Announce Type: replace 
Abstract: Joint attention is a critical component of early speech-language development and a key indicator of effective parent-child interaction. However, research on detecting and analysing joint attention remains limited, particularly for Multimodal Large Language Models (MLLMs). This study evaluates MLLMs' ability to comprehend joint attention by analysing 26 parent-child interaction videos annotated by two speech-language pathologists. These annotations identify strong and poor joint attention segments, serving as benchmarks for evaluating the models' interpretive capabilities. Our findings reveal that current MLLMs struggle to accurately interpret joint attention due to a lack of nuanced understanding of child-initiated eye contact, a crucial component of joint attention dynamics. This study highlights the importance of incorporating detailed eye contact to enhance MLLMs' multimodal reasoning. Addressing these gaps is essential for future research to advance the use of MLLMs in analysing and supporting parent-child interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19877v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Weiyan Shi, Viet Hai Le, Kenny Tsu Wei Choo</dc:creator>
    </item>
    <item>
      <title>AR You on Track? Investigating Effects of Augmented Reality Anchoring on Dual-Task Performance While Walking</title>
      <link>https://arxiv.org/abs/2502.20944</link>
      <description>arXiv:2502.20944v2 Announce Type: replace 
Abstract: With the increasing spread of AR head-mounted displays suitable for everyday use, interaction with information becomes ubiquitous, even while walking. However, this requires constant shifts of our attention between walking and interacting with virtual information to fulfill both tasks adequately. Accordingly, we as a community need a thorough understanding of the mutual influences of walking and interacting with digital information to design safe yet effective interactions. Thus, we systematically investigate the effects of different AR anchors (hand, head, torso) and task difficulties on user experience and performance. We engage participants (n=26) in a dual-task paradigm involving a visual working memory task while walking. We assess the impact of dual-tasking on both virtual and walking performance, and subjective evaluations of mental and physical load. Our results show that head-anchored AR content least affected walking while allowing for fast and accurate virtual task interaction, while hand-anchored content increased reaction times and workload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20944v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714258</arxiv:DOI>
      <dc:creator>Julian Rasch, Matthias Wilhalm, Florian M\"uller, Francesco Chiossi</dc:creator>
    </item>
    <item>
      <title>Through the Looking-Glass: Transparency Implications and Challenges in Enterprise AI Knowledge Systems</title>
      <link>https://arxiv.org/abs/2401.09410</link>
      <description>arXiv:2401.09410v4 Announce Type: replace-cross 
Abstract: Knowledge can't be disentangled from people. As AI knowledge systems mine vast volumes of work-related data, the knowledge that's being extracted and surfaced is intrinsically linked to the people who create and use it. When predictive algorithms that learn from data are used to link knowledge and people, inaccuracies in knowledge extraction and surfacing can lead to disproportionate harms, influencing how individuals see each other and how they see themselves at work. In this paper, we present a reflective analysis of transparency requirements and impacts in this type of systems. We conduct a multidisciplinary literature review to understand the impacts of transparency in workplace settings, introducing the looking-glass metaphor to conceptualize AI knowledge systems as systems that reflect and distort, expanding our view on transparency requirements, implications and challenges. We formulate transparency as a key mediator in shaping different ways of seeing, including seeing into the system, which unveils its capabilities, limitations and behavior, and seeing through the system, which shapes workers' perceptions of their own contributions and others within the organization. Recognizing the sociotechnical nature of these systems, we identify three transparency dimensions necessary to realize the value of AI knowledge systems, namely system transparency, procedural transparency and transparency of outcomes. We discuss key challenges hindering the implementation of these forms of transparency, bringing to light the wider sociotechnical gap and highlighting directions for future Computer-supported Cooperative Work (CSCW) research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09410v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karina Corti\~nas-Lorenzo, Si\^an Lindley, Ida Larsen-Ledet, Bhaskar Mitra</dc:creator>
    </item>
    <item>
      <title>AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems</title>
      <link>https://arxiv.org/abs/2402.06287</link>
      <description>arXiv:2402.06287v2 Announce Type: replace-cross 
Abstract: Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06287v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Punzi, Roberto Pellungrini, Mattia Setzu, Fosca Giannotti, Dino Pedreschi</dc:creator>
    </item>
    <item>
      <title>Designing Robots to Help Women</title>
      <link>https://arxiv.org/abs/2404.04123</link>
      <description>arXiv:2404.04123v2 Announce Type: replace-cross 
Abstract: Robots are being designed to help people in an increasing variety of settings--but seemingly little attention has been given so far to the specific needs of women, who represent roughly half of the world's population but are highly underrepresented in robotics. Here we used a speculative prototyping approach to explore this expansive design space: First, we identified some potential challenges of interest, including crimes and illnesses that disproportionately affect women, as well as potential opportunities for designers, which were visualized in five sketches. Then, one of the sketched scenarios was further explored by developing a prototype, of a robotic helper drone equipped with computer vision to detect hidden cameras that could be used to spy on women. While object detection introduced some errors, hidden cameras were identified with a reasonable accuracy of 80% (Intersection over Union (IoU) score: 0.40). Our aim is that the identified challenges and opportunities could help spark discussion and inspire designers, toward realizing a safer, more inclusive future through responsible use of technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04123v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3384/ecp208019</arxiv:DOI>
      <arxiv:journal_reference>14th Scandinavian Conference on Artificial Intelligence (SCAI 2024): AI for a better society</arxiv:journal_reference>
      <dc:creator>Martin Cooney, Lena Klas\'en, Fernando Alonso-Fernandez</dc:creator>
    </item>
    <item>
      <title>An Open-Source Reproducible Chess Robot for Human-Robot Interaction Research</title>
      <link>https://arxiv.org/abs/2405.18170</link>
      <description>arXiv:2405.18170v5 Announce Type: replace-cross 
Abstract: Recent advancements in AI have accelerated the evolution of versatile robot designs. Chess provides a standardized environment for evaluating the impact of robot behavior on human behavior. This article presents an open-source chess robot for human-robot interaction (HRI) research, specifically focusing on verbal and non-verbal interactions. The OpenChessRobot recognizes chess pieces using computer vision, executes moves, and interacts with the human player through voice and robotic gestures. We detail the software design, provide quantitative evaluations of the efficacy of the robot, and offer a guide for its reproducibility. An online survey examining people's views of the robot in three possible scenarios was conducted with 597 participants. The robot received the highest ratings in the robotics education and the chess coach scenarios, while the home entertainment scenario received the lowest scores. The code is accessible on GitHub: https://github.com/renchizhhhh/OpenChessRobot</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18170v5</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Renchi Zhang, Joost de Winter, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma</dc:creator>
    </item>
    <item>
      <title>Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools</title>
      <link>https://arxiv.org/abs/2408.04650</link>
      <description>arXiv:2408.04650v2 Announce Type: replace-cross 
Abstract: Objective: This study aims to develop and validate an evaluation framework to ensure the safety and reliability of mental health chatbots, which are increasingly popular due to their accessibility, human-like interactions, and context-aware support. Materials and Methods: We created an evaluation framework with 100 benchmark questions and ideal responses, and five guideline questions for chatbot responses. This framework, validated by mental health experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation methods explored included large language model (LLM)-based scoring, an agentic approach using real-time data, and embedding models to compare chatbot responses against ground truth standards. Results: The results highlight the importance of guidelines and ground truth for improving LLM evaluation accuracy. The agentic method, dynamically accessing reliable information, demonstrated the best alignment with human assessments. Adherence to a standardized, expert-validated framework significantly enhanced chatbot response safety and reliability. Discussion: Our findings emphasize the need for comprehensive, expert-tailored safety evaluation metrics for mental health chatbots. While LLMs have significant potential, careful implementation is necessary to mitigate risks. The superior performance of the agentic approach underscores the importance of real-time data access in enhancing chatbot reliability. Conclusion: The study validated an evaluation framework for mental health chatbots, proving its effectiveness in improving safety and reliability. Future work should extend evaluations to accuracy, bias, empathy, and privacy to ensure holistic assessment and responsible integration into healthcare. Standardized evaluations will build trust among users and professionals, facilitating broader adoption and improved mental health support through technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04650v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jung In Park, Mahyar Abbasian, Iman Azimi, Dawn T. Bounds, Angela Jun, Jaesu Han, Robert M. McCarron, Jessica Borelli, Parmida Safavi, Sanaz Mirbaha, Jia Li, Mona Mahmoudi, Carmen Wiedenhoeft, Amir M. Rahmani</dc:creator>
    </item>
    <item>
      <title>Enhanced Optimization Strategies to Design an Underactuated Hand Exoskeleton</title>
      <link>https://arxiv.org/abs/2408.07384</link>
      <description>arXiv:2408.07384v2 Announce Type: replace-cross 
Abstract: Exoskeletons can boost human strength and provide assistance to individuals with physical disabilities. However, ensuring safety and optimal performance in their design poses substantial challenges. This study presents the design process for an underactuated hand exoskeleton (U-HEx), first including a single objective (maximizing force transmission), then expanding into multi objective (also minimizing torque variance and actuator displacement). The optimization relies on a Genetic Algorithm, the Big Bang-Big Crunch Algorithm, and their versions for multi-objective optimization. Analyses revealed that using Big Bang-Big Crunch provides high and more consistent results in terms of optimality with lower convergence time. In addition, adding more objectives offers a variety of trade-off solutions to the designers, who might later set priorities for the objectives without repeating the process - at the cost of complicating the optimization algorithm and computational burden. These findings underline the importance of performing proper optimization while designing exoskeletons, as well as providing a significant improvement to this specific robotic design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07384v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.NE</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baris Akbas, Huseyin Taner Yuksel, Aleyna Soylemez, Mine Sarac, Fabio Stroppa</dc:creator>
    </item>
    <item>
      <title>MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation</title>
      <link>https://arxiv.org/abs/2410.13757</link>
      <description>arXiv:2410.13757v2 Announce Type: replace-cross 
Abstract: Existing Multimodal Large Language Model (MLLM)-based agents face significant challenges in handling complex GUI (Graphical User Interface) interactions on devices. These challenges arise from the dynamic and structured nature of GUI environments, which integrate text, images, and spatial relationships, as well as the variability in action spaces across different pages and tasks. To address these limitations, we propose MobA, a novel MLLM-based mobile assistant system. MobA introduces an adaptive planning module that incorporates a reflection mechanism for error recovery and dynamically adjusts plans to align with the real environment contexts and action module's execution capacity. Additionally, a multifaceted memory module provides comprehensive memory support to enhance adaptability and efficiency. We also present MobBench, a dataset designed for complex mobile interactions. Experimental results on MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI environments and perform complex mobile task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13757v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zichen Zhu, Hao Tang, Yansi Li, Dingye Liu, Hongshen Xu, Kunyao Lan, Danyang Zhang, Yixuan Jiang, Hao Zhou, Chenrun Wang, Situo Zhang, Liangtai Sun, Yixiao Wang, Yuheng Sun, Lu Chen, Kai Yu</dc:creator>
    </item>
    <item>
      <title>L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection And Enhancement</title>
      <link>https://arxiv.org/abs/2412.09765</link>
      <description>arXiv:2412.09765v2 Announce Type: replace-cross 
Abstract: The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09765v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morgan B. Talbot, Gabriel Kreiman, James J. DiCarlo, Guy Gaziv</dc:creator>
    </item>
    <item>
      <title>The Good, the Bad, and the (Un)Usable: A Rapid Literature Review on Privacy as Code</title>
      <link>https://arxiv.org/abs/2412.16667</link>
      <description>arXiv:2412.16667v2 Announce Type: replace-cross 
Abstract: Privacy and security are central to the design of information systems endowed with sound data protection and cyber resilience capabilities. Still, developers often struggle to incorporate these properties into software projects as they either lack proper cybersecurity training or do not consider them a priority. Prior work has tried to support privacy and security engineering activities through threat modeling methods for scrutinizing flaws in system architectures. Moreover, several techniques for the automatic identification of vulnerabilities and the generation of secure code implementations have also been proposed in the current literature. Conversely, such as-code approaches seem under-investigated in the privacy domain, with little work elaborating on (i) the automatic detection of privacy properties in source code or (ii) the generation of privacy-friendly code. In this work, we seek to characterize the current research landscape of Privacy as Code (PaC) methods and tools by conducting a rapid literature review. Our results suggest that PaC research is in its infancy, especially regarding the performance evaluation and usability assessment of the existing approaches. Based on these findings, we outline and discuss prospective research directions concerning empirical studies with software practitioners, the curation of benchmark datasets, and the role of generative AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16667v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicol\'as E. D\'iaz Ferreyra, Sirine Khelifi, Nalin Arachchilage, Riccardo Scandariato</dc:creator>
    </item>
    <item>
      <title>Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2502.11882</link>
      <description>arXiv:2502.11882v3 Announce Type: replace-cross 
Abstract: Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11882v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen</dc:creator>
    </item>
    <item>
      <title>A-MEM: Agentic Memory for LLM Agents</title>
      <link>https://arxiv.org/abs/2502.12110</link>
      <description>arXiv:2502.12110v2 Announce Type: replace-cross 
Abstract: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at https://github.com/WujiangXu/AgenticMemory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12110v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang</dc:creator>
    </item>
    <item>
      <title>User Intent to Use DeepSeek for Healthcare Purposes and their Trust in the Large Language Model: Multinational Survey Study</title>
      <link>https://arxiv.org/abs/2502.17487</link>
      <description>arXiv:2502.17487v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) increasingly serve as interactive healthcare resources, yet user acceptance remains underexplored. This study examines how ease of use, perceived usefulness, trust, and risk perception interact to shape intentions to adopt DeepSeek, an emerging LLM-based platform, for healthcare purposes. A cross-sectional survey of 556 participants from India, the United Kingdom, and the United States was conducted to measure perceptions and usage patterns. Structural equation modeling assessed both direct and indirect effects, including potential quadratic relationships. Results revealed that trust plays a pivotal mediating role: ease of use exerts a significant indirect effect on usage intentions through trust, while perceived usefulness contributes to both trust development and direct adoption. By contrast, risk perception negatively affects usage intent, emphasizing the importance of robust data governance and transparency. Notably, significant non-linear paths were observed for ease of use and risk, indicating threshold or plateau effects. The measurement model demonstrated strong reliability and validity, supported by high composite reliabilities, average variance extracted, and discriminant validity measures. These findings extend technology acceptance and health informatics research by illuminating the multifaceted nature of user adoption in sensitive domains. Stakeholders should invest in trust-building strategies, user-centric design, and risk mitigation measures to encourage sustained and safe uptake of LLMs in healthcare. Future work can employ longitudinal designs or examine culture-specific variables to further clarify how user perceptions evolve over time and across different regulatory environments. Such insights are critical for harnessing AI to enhance outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17487v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avishek Choudhury, Yeganeh Shahsavar, Hamid Shamszare</dc:creator>
    </item>
  </channel>
</rss>
