<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Dec 2025 02:31:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>State and Parameter Estimation for a Neural Model of Local Field Potentials</title>
      <link>https://arxiv.org/abs/2512.07842</link>
      <description>arXiv:2512.07842v1 Announce Type: new 
Abstract: The study of cortical dynamics during different states such as decision making, sleep and movement, is an important topic in Neuroscience. Modelling efforts aim to relate the neural rhythms present in cortical recordings to the underlying dynamics responsible for their emergence. We present an effort to characterize the neural activity from the cortex of a mouse during natural sleep, captured through local field potential measurements. Our approach relies on using a discretized Wilson--Cowan Amari neural field model for neural activity, along with a data assimilation method that allows the Bayesian joint estimation of the state and parameters. We demonstrate the feasibility of our approach on synthetic measurements before applying it to a dataset available in literature. Our findings suggest the potential of our approach to characterize the stimulus received by the cortex from other brain regions, while simultaneously inferring a state that aligns with the observed signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07842v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Avitabile, Gabriel J. Lord, Khadija Meddouni</dc:creator>
    </item>
    <item>
      <title>Manifolds and Modules: How Function Develops in a Neural Foundation Model</title>
      <link>https://arxiv.org/abs/2512.07869</link>
      <description>arXiv:2512.07869v1 Announce Type: new 
Abstract: Foundation models have shown remarkable success in fitting biological visual systems; however, their black-box nature inherently limits their utility for understanding brain function. Here, we peek inside a SOTA foundation model of neural activity (Wang et al., 2025) as a physiologist might, characterizing each 'neuron' based on its temporal response properties to parametric stimuli. We analyze how different stimuli are represented in neural activity space by building decoding manifolds, and we analyze how different neurons are represented in stimulus-response space by building neural encoding manifolds. We find that the different processing stages of the model (i.e., the feedforward encoder, recurrent, and readout modules) each exhibit qualitatively different representational structures in these manifolds. The recurrent module shows a jump in capabilities over the encoder module by 'pushing apart' the representations of different temporal stimulus patterns; while the readout module achieves biological fidelity by using numerous specialized feature maps rather than biologically plausible mechanisms. Overall, we present this work as a study of the inner workings of a prominent neural foundation model, gaining insights into the biological relevance of its internals through the novel analysis of its neurons' joint temporal response patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07869v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Bertram, Luciano Dyballa, T. Anderson Keller, Savik Kinger, Steven W. Zucker</dc:creator>
    </item>
    <item>
      <title>Revised comment on the paper titled "The Origin of Quantum Mechanical Statistics: Insights from Research on Human Language</title>
      <link>https://arxiv.org/abs/2512.07881</link>
      <description>arXiv:2512.07881v1 Announce Type: new 
Abstract: This short note comments on \citet{Aerts2024Origin}, which proposes that ranked word frequencies in texts should be read through the lens of Bose--Einstein (BE) statistics and even used to illuminate the origin of quantum statistics in physics. The core message here is modest: the paper offers an interesting analogy and an eye-catching fit, but several key steps mix physical claims with definitions and curve-fitting choices. We highlight three such points: (i) a normalization issue that is presented as "bosonic enhancement", (ii) an identification of rank with energy that makes the BE fit only weakly diagnostic of an underlying mechanism, and (iii) a baseline comparison that is too weak to support an ontological conclusion. We also briefly flag a few additional concerns (interpretation drift, parameter semantics, and reproducibility).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07881v1</guid>
      <category>q-bio.NC</category>
      <category>physics.hist-ph</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miko{\l}aj Sienicki, Krzysztof Sienicki</dc:creator>
    </item>
    <item>
      <title>Multi state neurons</title>
      <link>https://arxiv.org/abs/2512.08815</link>
      <description>arXiv:2512.08815v1 Announce Type: new 
Abstract: Neurons, as eukaryotic cells, have powerful internal computation capabilities. One neuron can have many distinct states, and brains can use this capability. Processes of neuron growth and maintenance use chemical signalling between cell bodies and synapses, ferrying chemical messengers over microtubules and actin fibres within cells. These processes are computations which, while slower than neural electrical signalling, could allow any neuron to change its state over intervals of seconds or minutes. Based on its state, a single neuron can selectively de-activate some of its synapses, sculpting a dynamic neural net from the static neural connections of the brain. Without this dynamic selection, the static neural networks in brains are too amorphous and dilute to do the computations of neural cognitive models. The use of multi-state neurons in animal brains is illustrated in hierarchical Bayesian object recognition. Multi-state neurons may support a design which is more efficient than two-state neurons, and scales better as object complexity increases. Brains could have evolved to use multi-state neurons. Multi-state neurons could be used in artificial neural networks, to use a kind of non-Hebbian learning which is faster and more focused and controllable than traditional neural net learning. This possibility has not yet been explored in computational models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08815v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Worden</dc:creator>
    </item>
    <item>
      <title>Sleep effects on brain, cognition, and mental health during adolescence are mediated by the glymphatic system</title>
      <link>https://arxiv.org/abs/2512.08704</link>
      <description>arXiv:2512.08704v1 Announce Type: cross 
Abstract: Background: Adolescence is a critical period of brain maturation and heightened vulnerability to cognitive and mental health disorders. Sleep plays a vital role in neurodevelopment, yet the mechanisms linking insufficient sleep to adverse brain and behavioral outcomes remain unclear. The glymphatic system (GS), a brain-wide clearance pathway, may provide a key mechanistic link. Methods: Participants from the Adolescent Brain Cognitive Development (ABCD) Study (n =6,800; age ~ 11 years) were categorized into sleep-sufficient (&gt;=9 h/night) and sleep-insufficient (&lt;9 h/night) groups. Linear models tested associations among sleep, PVS burden, brain volumes, and behavioral outcomes. Mediation analyses evaluated whether PVS burden explained sleep-related effects. Results: Adolescents with insufficient sleep exhibited significantly greater PVS burden, reduced cortical, subcortical, and white matter volumes, poorer cognitive performance across multiple domains (largest effect in crystallized intelligence), and elevated psychopathology (largest effect in general problems). Sleep duration and quality were strongly associated with PVS burden. Mediation analyses revealed that PVS burden partially mediated sleep effects on cognition and mental health, with indirect proportions up to 10.9%. Sequential models suggested a pathway from sleep -&gt; PVS -&gt; brain volume -&gt; behavior as the most plausible route. Conclusions: Insufficient sleep during adolescence is linked to glymphatic dysfunction, reflected by increased PVS burden, which partially accounts for adverse effects on brain structure, cognition, and mental health. These findings highlight the GS as a potential mechanistic pathway and imaging biomarker, underscoring the importance of promoting adequate sleep to support neurodevelopment and mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08704v1</guid>
      <category>q-bio.PE</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinglin Zeng, Yiran Li, Fan Nils Yang, Gianpaolo Del Mauro, Jiaao Yu, Ruoxi Lu, Jiachen Zhuo, Laura Rowland, Wickwire Emerson, Ze Wang</dc:creator>
    </item>
    <item>
      <title>How random connectivity shapes the fluctuating dynamics of finite-size neural populations</title>
      <link>https://arxiv.org/abs/2412.16111</link>
      <description>arXiv:2412.16111v2 Announce Type: replace 
Abstract: Mesoscopic models of finite-size neuronal populations are crucial to understand the dynamics of neural networks in the brain, especially their fluctuations and response to stimuli. However, current theories to derive such models are based on homogeneous all-to-all (full) connectivity. This assumption neglects the variance in the connectivity of biologically realistic networks with connection probabilities $p&lt;1$ (non-full connectivity). To gain insight into the different fluctuation mechanisms underlying neural variability at the population level, we derive and analyze a stochastic mean-field model for finite-size networks of Poisson neurons with random connectivity (including non-full connectivity), external noise and disordered mean inputs. We treat the quenched disorder of the connectivity by an annealed approximation enabling a doubly stochastic description of synaptic inputs for finite network size. A further reduction leads to a low-dimensional closed system of coupled Langevin equations for the mean and variance of the membrane potentials as well as a variable capturing finite-size fluctuations. Compared to microscopic simulations, the mesoscopic model describes the fluctuations and nonlinearities well and outperforms previous theories that neglected the variance in the connectivity. The joint effect of connectivity disorder and finite network size can be analytically understood by a softening of the effective nonlinearity and the multiplicative character of spiking noise. The mesoscopic theory shows that quenched disorder can stabilize the asynchronous state, and it correctly predicts large quantitative and non-trivial qualitative effects of connection probability on the variance of the population firing rate and its dependence on stimulus strength. In conclusion, our theory elucidates how disordered connectivity shapes nonlinear dynamics and fluctuations of neural populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16111v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils E. Greven, Jonas Ranft, Tilo Schwalger</dc:creator>
    </item>
    <item>
      <title>Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations</title>
      <link>https://arxiv.org/abs/2507.00269</link>
      <description>arXiv:2507.00269v2 Announce Type: replace 
Abstract: Current sparse autoencoder (SAE) approaches to neural network interpretability assume that activations can be decomposed through linear superposition into sparse, interpretable features. Despite high reconstruction fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit pathological behavioral errors. We propose that neural networks encode information in two complementary spaces compressed into the same substrate: feature identity and feature integration. To test this dual encoding hypothesis, we develop sequential and joint-training architectures to capture identity and integration patterns simultaneously. Joint training achieves 41.3% reconstruction improvement and 51.6% reduction in KL divergence errors. This architecture spontaneously develops bimodal feature organization: low squared norm features contributing to integration pathways and the rest contributing directly to the residual. Small nonlinear components (3% of parameters) achieve 16.5% standalone improvements, demonstrating parameter-efficient capture of computational relationships crucial for behavior. Additionally, intervention experiments using 2x2 factorial stimulus designs demonstrated that integration features exhibit selective sensitivity to experimental manipulations and produce systematic behavioral effects on model outputs, including significant statistical interaction effects across semantic dimensions. This work provides systematic evidence for (1) dual encoding in neural representations, (2) meaningful nonlinearly encoded feature interactions, and (3) introduces an architectural paradigm shift from post-hoc feature analysis to integrated computational design, establishing foundations for next-generation SAEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00269v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Claflin</dc:creator>
    </item>
    <item>
      <title>Surface Waves and Axoplasmic Pressure Waves in Action Potential Propagation: Fundamentally Different Physics or Two Sides of the Same Coin?</title>
      <link>https://arxiv.org/abs/2505.24580</link>
      <description>arXiv:2505.24580v2 Announce Type: replace-cross 
Abstract: In this commentary, we argue that El Hady and Machta's "surface wave" model for mechanical waves accompanying action potential (AP) propagation describes the same underlying process as the "axoplasmic pressure wave" model introduced earlier by Rvachev. Both models describe mechanical modes that store potential energy in the elastic components of the axon (axonal membrane, cytoskeleton, bulk axoplasmic deformation), with kinetic energy carried by the axoplasmic fluid and axoplasmic viscosity playing a significant role. The "surface wave" model quantitatively considers driving by the traveling electrical depolarization wave of the AP, whereas the "axoplasmic pressure wave" model qualitatively considers driving not only by the AP's electrical depolarization but also by other mechanisms, such as cytoskeletal actomyosin contractility. In addition, the "axoplasmic pressure wave" model considers mechanisms for synchronizing the depolarization wave and the pressure wave. Although derived using different approaches, the two models yield identical dependencies for the mechanical modes in key limits. The confusion in the literature, which treats these models as describing distinct processes, needs to be resolved to improve comprehensive understanding of the AP phenomenon and to guide future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24580v2</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1142/S1793048025500055</arxiv:DOI>
      <dc:creator>Marat M. Rvachev, Benjamin Drukarch</dc:creator>
    </item>
  </channel>
</rss>
