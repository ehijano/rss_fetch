<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 02:24:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion</title>
      <link>https://arxiv.org/abs/2506.21933</link>
      <description>arXiv:2506.21933v1 Announce Type: new 
Abstract: With the rapid development of the low-altitude economy, air-ground integrated multi-access edge computing (MEC) systems are facing increasing demands for real-time and intelligent task scheduling. In such systems, task offloading and resource allocation encounter multiple challenges, including node heterogeneity, unstable communication links, and dynamic task variations. To address these issues, this paper constructs a three-layer heterogeneous MEC system architecture for low-altitude economic networks, encompassing aerial and ground users as well as edge servers. The system is systematically modeled from the perspectives of communication channels, computational costs, and constraint conditions, and the joint optimization problem of offloading decisions and resource allocation is uniformly abstracted into a graph-structured modeling task. On this basis, we propose a graph attention diffusion-based solution generator (GADSG). This method integrates the contextual awareness of graph attention networks with the solution distribution learning capability of diffusion models, enabling joint modeling and optimization of discrete offloading variables and continuous resource allocation variables within a high-dimensional latent space. We construct multiple simulation datasets with varying scales and topologies. Extensive experiments demonstrate that the proposed GADSG model significantly outperforms existing baseline methods in terms of optimization performance, robustness, and generalization across task structures, showing strong potential for efficient task scheduling in dynamic and complex low-altitude economic network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21933v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Xue, Ruihuai Liang, Bo Yang, Xuelin Cao, Zhiwen Yu, M\'erouane Debbah, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Resilient Communication For Avalanche Response in Infrastructure-Limited Environments</title>
      <link>https://arxiv.org/abs/2506.22148</link>
      <description>arXiv:2506.22148v1 Announce Type: new 
Abstract: Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining communication in infrastructure limited environments, such as those encountered during natural disasters. This paper investigates the viability of leveraging an existing national transport system - the Swiss rail network - as a data mule backbone for disseminating critical avalanche alerts. Using The Opportunistic Network Environment (ONE) simulator, we model the entire Swiss rail network and conduct a rigorous comparative analysis of two seminal DTN routing protocols: Epidemic and PROPHET. Experiments are performed in two distinct scenarios: alerts originating from dense urban centres and from sparse, remote mountainous regions. Our results demonstrate that the rail network provides robust connectivity for opportunistic communication in both environments thus validating the integration of DTN principles in remote scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22148v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Goulton, Milena Radenkovic</dc:creator>
    </item>
    <item>
      <title>V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles</title>
      <link>https://arxiv.org/abs/2506.22223</link>
      <description>arXiv:2506.22223v1 Announce Type: new 
Abstract: This paper introduces a novel intention-sharing mechanism for Electrically Power-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing the ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete predicted trajectory points with a compact elliptical geographical area representation derived via quadratic polynomial fitting and Least Squares Method (LSM). This approach encodes trajectory predictions with fixed-size data payloads, independent of the number of forecasted points, enabling higher-frequency transmissions and improved network reliability. Simulation results demonstrate superior inter-packet gap (IPG) performance compared to standard ETSI VAMs, particularly under constrained communication conditions. A physical experiment validates the feasibility of real-time deployment on embedded systems. The method supports scalable, low-latency intention sharing, contributing to cooperative perception and enhanced safety for vulnerable road users in connected and automated mobility ecosystems. Finally, we discuss the viability of LSM and open the door to other methods for prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22223v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Felipe Valle Quiroz, Johan Elfing, Joel P{\aa}lsson, Elena Haller, Oscar Amador Molina</dc:creator>
    </item>
    <item>
      <title>Design and Evaluation of IEEE 802.11ax Uplink Orthogonal Frequency Division Multiple Random Access in ns-3</title>
      <link>https://arxiv.org/abs/2506.22260</link>
      <description>arXiv:2506.22260v1 Announce Type: new 
Abstract: Wi-Fi networks have long relied on the Enhanced Distributed Channel Access (EDCA) mechanism, allowing stations to compete for transmission opportunities. However, as networks become denser and emerging applications demand lower latency and higher reliability, the limitations of EDCA such as overhead due to contention and collisions have become more pronounced. To address these challenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been introduced in Wi-Fi, enabling more efficient channel utilization through scheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal Frequency Division Multiple Random Access (UORA), a hybrid mechanism that combines both scheduled and random access, balancing efficiency and responsiveness in resource allocation. Despite significant research on UORA, most studies rely on custom simulators that are not publicly available, limiting reproducibility and preventing validation of the presented results. The only known open-source UORA implementation in the ns-3 simulator exhibits key limitations, such as usage of the same trigger frame (TF) to schedule resources for buffer status reports and data transmissions, and lack of signaling for UORA configuration. In this paper, we present a fully standard-compliant and open source UORA implementation that is compatible with ns-3 version 3.38, addressing these limitations to improve resource allocation efficiency and adaptability. This implementation enables more accurate and flexible evaluation of UORA, fostering future research on Wi-Fi resource allocation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22260v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Douglas Dziedzorm Agbeve, Andrey Belogaev, Jeroen Famaey</dc:creator>
    </item>
    <item>
      <title>Concept-Level AI for Telecom: Moving Beyond Large Language Models</title>
      <link>https://arxiv.org/abs/2506.22359</link>
      <description>arXiv:2506.22359v1 Announce Type: new 
Abstract: The telecommunications and networking domain stands at the precipice of a transformative era, driven by the necessity to manage increasingly complex, hierarchical, multi administrative domains (i.e., several operators on the same path) and multilingual systems. Recent research has demonstrated that Large Language Models (LLMs), with their exceptional general-purpose text analysis and code generation capabilities, can be effectively applied to certain telecom problems (e.g., auto-configuration of data plan to meet certain application requirements). However, due to their inherent token-by-token processing and limited capacity for maintaining extended context, LLMs struggle to fulfill telecom-specific requirements such as cross-layer dependency cascades (i.e., over OSI), temporal-spatial fault correlation, and real-time distributed coordination. In contrast, Large Concept Models (LCMs), which reason at the abstraction level of semantic concepts rather than individual lexical tokens, offer a fundamentally superior approach for addressing these telecom challenges. By employing hyperbolic latent spaces for hierarchical representation and encapsulating complex multi-layered network interactions within concise concept embeddings, LCMs overcome critical shortcomings of LLMs in terms of memory efficiency, cross-layer correlation, and native multimodal integration. This paper argues that adopting LCMs is not simply an incremental step, but a necessary evolutionary leap toward achieving robust and effective AI-driven telecom management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22359v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Viswanath Kumarskandpriya, Abdulhalim Dandoush, Abbas Bradai, Ali Belgacem</dc:creator>
    </item>
    <item>
      <title>Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles</title>
      <link>https://arxiv.org/abs/2506.22052</link>
      <description>arXiv:2506.22052v1 Announce Type: cross 
Abstract: V2X communication has become crucial for enhancing road safety, especially for Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the increasing number of devices communicating on the same channels will lead to significant channel load. To address this issue this study evaluates the effectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM), focusing specifically on cyclists. The objective of RM is to minimize the transmission of redundant information. We conducted a simulation study using a urban scenario with a high bicycle density based on traffic data from Hannover, Germany. This study assessed the impact of RM on channel load, measured by Channel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in simulation. To evaluate the accuracy and reliability of the RM mechanisms, we analyzed the actual differences in position, speed, and heading between the ego VRU and the VRU, which was assumed to be redundant. Our findings indicate that while RM can reduce channel congestion, it also leads to a decrease in VPR. The analysis of actual differences revealed that the RM mechanism standardized by ETSI often uses outdated information, leading to significant discrepancies in position, speed, and heading, which could result in dangerous situations. To address these limitations, we propose an adapted RM mechanism that improves the balance between reducing channel load and maintaining VRU awareness. The adapted approach shows a significant reduction in maximum CBR and a less significant decrease in VPR compared to the standardized RM. Moreover, it demonstrates better performance in the actual differences in position, speed, and heading, thereby enhancing overall safety. Our results highlight the need for further research to optimize RM techniques and ensure they effectively enhance V2X communication without compromising the safety of VRUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22052v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nico Ostendorf, Keno Garlichs, Lars Wolf</dc:creator>
    </item>
    <item>
      <title>Autonomic Microservice Management via Agentic AI and MAPE-K Integration</title>
      <link>https://arxiv.org/abs/2506.22185</link>
      <description>arXiv:2506.22185v1 Announce Type: cross 
Abstract: While microservices are revolutionizing cloud computing by offering unparalleled scalability and independent deployment, their decentralized nature poses significant security and management challenges that can threaten system stability. We propose a framework based on MAPE-K, which leverages agentic AI, for autonomous anomaly detection and remediation to address the daunting task of highly distributed system management. Our framework offers practical, industry-ready solutions for maintaining robust and secure microservices. Practitioners and researchers can customize the framework to enhance system stability, reduce downtime, and monitor broader system quality attributes such as system performance level, resilience, security, and anomaly management, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22185v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Alexander Bakhtin, Noman Ahmad, Mikel Robredo, Ruoyu Su, Valentina Lenarduzzi, Davide Taibi</dc:creator>
    </item>
    <item>
      <title>Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America</title>
      <link>https://arxiv.org/abs/2506.22323</link>
      <description>arXiv:2506.22323v1 Announce Type: cross 
Abstract: A sophisticated malspam campaign was recently uncovered targeting Latin American countries, with a particular focus on Brazil. This operation utilizes a highly deceptive phishing email to trick users into executing a malicious MSI file, initiating a multi-stage infection. The core of the attack leverages DLL side-loading, where a legitimate executable from Valve Corporation is used to load a trojanized DLL, thereby bypassing standard security defenses.
  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is capable of a wide range of malicious activities. It is designed to steal sensitive browser-stored credentials and banking information, the latter through fake login windows mimicking well-known Brazilian banks. The threat establishes persistence by modifying the Windows registry , captures user keystrokes through keylogging , and exfiltrates stolen data to a Command-and-Control (C2) server using encrypted payloads. Despite its advanced capabilities, the malware code exhibits signs of rushed development, with inefficiencies and poor error handling that suggest the threat actors prioritized rapid deployment over meticulous design. Nonetheless, the campaign extensive reach and sophisticated mechanisms pose a serious and immediate threat to the targeted regions, underscoring the need for robust cybersecurity defenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22323v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <category>cs.PL</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessio Di Santo</dc:creator>
    </item>
    <item>
      <title>Generative AI for O-RAN Slicing: A Semi-Supervised Approach with VAE and Contrastive Learning</title>
      <link>https://arxiv.org/abs/2401.08861</link>
      <description>arXiv:2401.08861v3 Announce Type: replace 
Abstract: This paper introduces a novel generative AI (GAI)-driven, unified semi-supervised learning architecture for optimizing resource allocation and network slicing in O-RAN. Termed Generative Semi-Supervised VAE-Contrastive Learning, our approach maximizes the weighted user equipment (UE) throughput and allocates physical resource blocks (PRBs) to enhance the quality of service for eMBB and URLLC services. The GAI framework utilizes a dedicated xApp for intelligent power control and PRB allocation. This integrated GAI model synergistically combines the generative power of a VAE with contrastive learning to achieve robustness in an end-to-end trainable system. It is a semi-supervised training approach that concurrently optimizes supervised regression of resource allocation decisions (i.e., power, UE association, PRB) and unsupervised contrastive objectives. This intrinsic fusion improves the precision of resource management and model generalization in dynamic mobile networks. We evaluated our GAI methodology against exhaustive search and deep Q-Network algorithms using key performance metrics. Results show our integrated GAI approach offers superior efficiency and effectiveness in various scenarios, presenting a compelling GAI-based solution for critical network slicing and resource management challenges in next-generation O-RAN systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08861v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salar Nouri, Mojdeh Karbalaee Motalleb, Vahid Shah-Mansouri, Seyed Pooya Shariatpanahi</dc:creator>
    </item>
    <item>
      <title>Empirically Measuring Data Localization in the EU</title>
      <link>https://arxiv.org/abs/2504.09019</link>
      <description>arXiv:2504.09019v2 Announce Type: replace 
Abstract: EU data localization regulations limit data transfers to non-EU countries with the GDPR. However, BGP, DNS and other Internet protocols were not designed to enforce jurisdictional constraints, so implementing data localization is challenging. Despite initial research on the topic, little is known about if or how companies currently operate their server infrastructure to comply with the regulations. We close this knowledge gap by empirically measuring the extent to which servers and routers that process EU requests are located outside of the EU (and a handful of ``adequate'' non-EU countries). The key challenge is that both browser measurements (to infer relevant endpoints) and data-plane measurements (to infer relevant IP addresses) are needed, but no large-scale public infrastructure allows both. We build a novel methodology that combines BrightData (browser) and RIPE Atlas (data-plane) probes, with joint measurements from over 1,000 networks in 19 EU countries. We find that, on average, 2.3% of servers serving users in each EU country are located in non-adequate destination countries (1.4% of known trackers). Our findings suggest that data localization policies are largely being followed by content providers, though there are exceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09019v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Gamero-Garrido, Kicho Yu, Sumukh Vasisht Shankar, Sachin Kumar Singh, Sindhya Balasubramanian, Alexander Wilcox, David Choffnes</dc:creator>
    </item>
    <item>
      <title>A Novel Open Set Energy-based Flow Classifier for Network Intrusion Detection</title>
      <link>https://arxiv.org/abs/2109.11224</link>
      <description>arXiv:2109.11224v3 Announce Type: replace-cross 
Abstract: Several machine learning-based Network Intrusion Detection Systems (NIDS) have been proposed in recent years. Still, most of them were developed and evaluated under the assumption that the training context is similar to the test context. This assumption is false in real networks, given the emergence of new attacks and variants of known attacks. To deal with this reality, the open set recognition field, which is the most general task of recognizing classes not seen during training in any domain, began to gain importance in machine learning based NIDS research. Yet, existing solutions are often bound to high temporal complexities and performance bottlenecks. In this work, we propose an algorithm to be used in NIDS that performs open set recognition. Our proposal is an adaptation of the single-class Energy-based Flow Classifier (EFC), which proved to be an algorithm with strong generalization capability and low computational cost. The new version of EFC correctly classifies not only known attacks, but also unknown ones, and differs from other proposals from the literature by presenting a single layer with low temporal complexity. Our proposal was evaluated against well-established multi-class algorithms and as an open set classifier. It proved to be an accurate classifier in both evaluations, similar to the state of the art. As a conclusion of our work, we consider EFC a promising algorithm to be used in NIDS for its high performance and applicability in real networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.11224v3</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cose.2025.104569</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Security, 104569 (2025)</arxiv:journal_reference>
      <dc:creator>Manuela M. C. Souza, Camila Pontes, Joao Gondim, Luis P. F. Garcia, Luiz DaSilva, Eduardo F. M. Cavalcante, Marcelo A. Marotta</dc:creator>
    </item>
    <item>
      <title>eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2506.16535</link>
      <description>arXiv:2506.16535v2 Announce Type: replace-cross 
Abstract: As autonomous vehicles edge closer to widespread adoption, enhancing road safety through collision avoidance and minimization of collateral damage becomes imperative. Vehicle-to-everything (V2X) technologies, which include vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud (V2C), are being proposed as mechanisms to achieve this safety improvement.
  Simulation-based testing is crucial for early-stage evaluation of Connected Autonomous Vehicle (CAV) control systems, offering a safer and more cost-effective alternative to real-world tests. However, simulating large 3D environments with many complex single- and multi-vehicle sensors and controllers is computationally intensive. There is currently no evaluation framework that can effectively evaluate realistic scenarios involving large numbers of autonomous vehicles.
  We propose eCAV -- an efficient, modular, and scalable evaluation platform to facilitate both functional validation of algorithmic approaches to increasing road safety, as well as performance prediction of algorithms of various V2X technologies, including a futuristic Vehicle-to-Edge control plane and correspondingly designed control algorithms. eCAV can model up to 256 vehicles running individual control algorithms without perception enabled, which is $8\times$ more vehicles than what is possible with state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16535v2</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler Landle, Jordan Rapp, Dean Blank, Chandramouli Amarnath, Abhijit Chatterjee, Alexandros Daglis, Umakishore Ramachandran</dc:creator>
    </item>
  </channel>
</rss>
