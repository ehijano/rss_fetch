<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 01:20:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Bayesian Semiparametric Mixture Model for Clustering Zero-Inflated Microbiome Data</title>
      <link>https://arxiv.org/abs/2508.14184</link>
      <description>arXiv:2508.14184v1 Announce Type: new 
Abstract: Microbiome research has immense potential for unlocking insights into human health and disease. A common goal in human microbiome research is identifying subgroups of individuals with similar microbial composition that may be linked to specific health states or environmental exposures. However, existing clustering methods are often not equipped to accommodate the complex structure of microbiome data and typically make limiting assumptions regarding the number of clusters in the data which can bias inference. Designed for zero-inflated multivariate compositional count data collected in microbiome research, we propose a novel Bayesian semiparametric mixture modeling framework that simultaneously learns the number of clusters in the data while performing cluster allocation. In simulation, we demonstrate the clustering performance of our method compared to distance- and model-based alternatives and the importance of accommodating zero-inflation when present in the data. We then apply the model to identify clusters in microbiome data collected in a study designed to investigate the relation between gut microbial composition and enteric diarrheal disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14184v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suppapat Korsurat, Matthew D. Koslovsky</dc:creator>
    </item>
    <item>
      <title>Bridge Sampling Diagnostics</title>
      <link>https://arxiv.org/abs/2508.14487</link>
      <description>arXiv:2508.14487v1 Announce Type: new 
Abstract: In Bayesian statistics, the marginal likelihood is used for model selection and averaging, yet it is often challenging to compute accurately for complex models. Approaches such as bridge sampling, while effective, may suffer from issues of high variability of the estimates. We present how to estimate Monte Carlo standard error (MCSE) for bridge sampling, and how to diagnose the reliability of MCSE estimates using Pareto-$\hat{k}$ and block reshuffling diagnostics without the need to repeatedly re-run full posterior inference. We demonstrate the behavior with increasingly more difficult simulated posteriors and many real posteriors from the posteriordb database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14487v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgio Micaletto, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Sequential Correct Screening and Post-Screening Inference</title>
      <link>https://arxiv.org/abs/2508.14596</link>
      <description>arXiv:2508.14596v1 Announce Type: new 
Abstract: Selecting the top-$m$ variables with the $m$ largest population parameters from a larger set of candidates is a fundamental problem in statistics. In this paper, we propose a novel methodology called Sequential Correct Screening (SCS), which sequentially screens out variables that are not among the top-$m$. A key feature of our method is its anytime validity; it provides a sequence of variable subsets that, with high probability, always contain the true top-$m$ variables. Furthermore, we develop a post-screening inference (PSI) procedure to construct confidence intervals for the selected parameters. Importantly, this procedure is designed to control the false coverage rate (FCR) whenever it is conducted -- an aspect that has been largely overlooked in the existing literature. We establish theoretical guarantees for both SCS and PSI, and demonstrate their performance through simulation studies and an application to a real-world dataset on suicide rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14596v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masaki Toyoda, Yoshimasa Uematsu</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bounds for Evaluating the Clinical Utility of Treatment Rules</title>
      <link>https://arxiv.org/abs/2508.14653</link>
      <description>arXiv:2508.14653v1 Announce Type: new 
Abstract: Evaluating the value of new clinical treatment rules based on patient characteristics is important but often complicated by hidden confounding factors in observational studies. Standard methods for estimating the average patient outcome if a new rule were universally adopted typically rely on strong, untestable assumptions about these hidden factors. This paper tackles this challenge by developing nonparametric bounds - a range of plausible values - for the expected outcome under a new rule, even with unobserved confounders present. We propose and investigate two main strategies for derivation of these bounds. We extend these techniques to incorporate Instrumental Variables (IVs), which can help narrow the bounds, and to directly estimate bounds on the difference in expected outcomes between the new rule and an existing clinical guideline. In simulation studies we compare the performance and width of bounds generated by the reduction and conditioning strategies in different scenarios. The methods are illustrated with a real-data example about prevention of peanut allergy in children. Our bounding frameworks provide robust tools for assessing the potential impact of new clinical treatment rules when unmeasured confounding is a concern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14653v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Hruza, Erin Gabriel, Arvid Sj\"olander, Samir Bhatt, Michael Sachs</dc:creator>
    </item>
    <item>
      <title>An Integrated Target Study and Target Trial Framework to Evaluate Intervention Effects on Disparities</title>
      <link>https://arxiv.org/abs/2508.14690</link>
      <description>arXiv:2508.14690v1 Announce Type: new 
Abstract: We present a novel framework -- the integrated Target Study + Target Trial (TS+TT) -- to evaluate the effects of interventions on disparities. This framework combines the ethical clarity of the Target Study, which balances allowable covariates across social groups to define meaningful disparity measures, with the causal rigor of the Target Trial, which emulates randomized trials to estimate intervention effects. TS+TT achieves two forms of balance: (1) stratified sampling ensures that allowable covariates are balanced across social groups to enable an ethically interpretable disparity contrast; (2) intervention-randomization within social groups balances both allowable and non-allowable covariates across intervention arms within each group to support unconfounded estimation of intervention effects on disparity. We describe the key components of protocol specification and its emulation and demonstrate the approach using electronic medical record data to evaluate how hypothetical interventions on pulse oximeter racial bias affect disparities in treatment receipt in clinical care. We also extend semiparametric G-computation for time-to-event outcomes in continuous time to accommodate continuous, stochastic interventions, allowing counterfactual estimation of disparities in time-to-treatment. More broadly, the framework accommodates a wide range of intervention and outcome types. The TS+TT framework offers a versatile and policy-relevant tool for generating ethically aligned causal evidence to help eliminate disparities and avoid unintentionally exacerbating disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14690v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinyi Sun, Theodore J. Iwashyna, Emmanuel F. Drabo, Deidra C. Crews, Kadija Ferryman, John W. Jackson</dc:creator>
    </item>
    <item>
      <title>Non-Existent Outcomes in Research on Inequality: A Causal Approach</title>
      <link>https://arxiv.org/abs/2508.14770</link>
      <description>arXiv:2508.14770v1 Announce Type: new 
Abstract: Scholars of social stratification often study exposures that shape life outcomes. But some outcomes (such as wage) only exist for some people (such as those who are employed). We show how a common practice -- dropping cases with non-existent outcomes -- can obscure causal effects when a treatment affects both outcome existence and outcome values. The effects of both beneficial and harmful treatments can be underestimated. Drawing on existing approaches for principal stratification, we show how to study (1) the average effect on whether an outcome exists and (2) the average effect on the outcome among the latent subgroup whose outcome would exist in either treatment condition. To extend our approach to the selection-on-observables settings common in applied research, we develop a framework involving regression and simulation to enable principal stratification estimates that adjust for measured confounders. We illustrate through an empirical example about the effects of parenthood on labor market outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14770v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ian Lundberg, Soonhong Cho</dc:creator>
    </item>
    <item>
      <title>Quantifying How Much Has Been Learned from a Research Study</title>
      <link>https://arxiv.org/abs/2508.14789</link>
      <description>arXiv:2508.14789v1 Announce Type: new 
Abstract: How much does a research study contribute to a scientific literature? We propose a learning metric to quantify how much a research community learns from a given study. To do so, we adopt a Bayesian perspective and assess changes in the community's beliefs once updated with a new study's evidence. We recommend the Wasserstein-2 distance as a way to describe how the research community's prior beliefs change to incorporate a study's findings. We illustrate this approach through stylized examples and empirical applications, showing how it differs from more traditional evaluative standards, such as statistical significance. We then extend the framework to the prospective setting, offering a way for decision-makers to evaluate the expected amount of learning from a proposed study. While assessments about what has or could be learned from a research program are often expressed informally, our learning metric provides a principled tool for judging scientific contributions. By formalizing these judgments, our measure has the potential to allow for more transparent assessments of past and prospective research contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14789v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Joint estimation of asymmetric community numbers in directed networks</title>
      <link>https://arxiv.org/abs/2508.14816</link>
      <description>arXiv:2508.14816v1 Announce Type: new 
Abstract: Community detection in directed networks is a central task in network analysis. Unlike undirected networks, directed networks encode inherently asymmetric relationships, giving rise to sender and receiver roles that may each follow distinct community organizations with possibly different numbers of communities. Estimating these two community counts simultaneously is therefore considerably more challenging than in the undirected setting, yet it is essential for faithful model specification and reliable downstream inference. This work addresses this challenge within the stochastic co-block model (ScBM), a powerful statistical framework for capturing asymmetric relational structures inherent in directed networks. We propose a novel goodness-of-fit test based on the deviation of the largest singular value of a normalized residual matrix from the constant value 2. We show that the upper bound of this test statistic converges to zero under the null hypothesis, while this statistic goes to infinity if the true model has finer communities than hypothesized. Leveraging this tail bounds behavior, we develop an efficient sequential testing algorithm that lexicographically explores candidate community number pairs. To enhance robustness in practical settings, we further introduce a ratio-based variant that detects the transition point in the test statistic sequence. We rigorously show both algorithms' consistency in recovering the true sender and receiver community counts under ScBM. Numerical experiments demonstrate the accuracy and robustness of our methods in estimating community numbers across diverse ScBM settings. %To our knowledge, this work presents the first theoretically guaranteed approach for jointly estimating the numbers of sender and receiver communities within the ScBM framework, providing a critical tool for reliable directed network analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14816v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>Data Fusion for High-Resolution Estimation</title>
      <link>https://arxiv.org/abs/2508.14858</link>
      <description>arXiv:2508.14858v1 Announce Type: new 
Abstract: High-resolution estimates of population health indicators are critical for precision public health. We propose a method for high-resolution estimation that fuses distinct data sources: an unbiased, low-resolution data source (e.g. aggregated administrative data) and a potentially biased, high-resolution data source (e.g. individual-level online survey responses). We assume that the potentially biased, high-resolution data source is generated from the population under a model of sampling bias where observables can have arbitrary impact on the probability of response but the difference in the log probabilities of response between units with the same observables is linear in the difference between sufficient statistics of their observables and outcomes. Our data fusion method learns a distribution that is closest (in the sense of KL divergence) to the online survey distribution and consistent with the aggregated administrative data and our model of sampling bias. This method outperforms baselines that rely on either data source alone on a testbed that includes repeated measurements of three indicators measured by both the (online) Household Pulse Survey and ground-truth data sources at two geographic resolutions over the same time period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14858v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amy Guan, Marissa Reitsma, Roshni Sahoo, Joshua Salomon, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Novel Knockoff Generation and Importance Measures with Heterogeneous Data via Conditional Residuals and Local Gradients</title>
      <link>https://arxiv.org/abs/2508.14882</link>
      <description>arXiv:2508.14882v1 Announce Type: new 
Abstract: Knockoff variable selection is a powerful framework that creates synthetic knockoff variables to mirror the correlation structure of the observed features, enabling principled control of the false discovery rate in variable selection. However, existing methods often assume homogeneous data types or known distributions, limiting their applicability in real-world settings with heterogeneous, distribution-free data. Moreover, common variable importance measures rely on linear outcome models, hindering their effectiveness for complex relationships. We propose a flexible knockoff generation framework based on conditional residuals that accommodates mixed data types without assuming known distributions. To assess variable importance, we introduce the Mean Absolute Local Derivative (MALD), an interpretable metric compatible with nonlinear outcome functions, including random forests and neural networks. Simulations show that our approach achieves better false discovery rate control and higher power than existing methods. We demonstrate its practical utility on a DNA methylation dataset from mouse tissues, identifying CpG sites linked to aging. Software is available in R (rangerKnockoff) and Python (MALDimportance).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14882v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Mason, Zhe Fei</dc:creator>
    </item>
    <item>
      <title>Comparing Model-agnostic Feature Selection Methods through Relative Efficiency</title>
      <link>https://arxiv.org/abs/2508.14268</link>
      <description>arXiv:2508.14268v1 Announce Type: cross 
Abstract: Feature selection and importance estimation in a model-agnostic setting is an ongoing challenge of significant interest. Wrapper methods are commonly used because they are typically model-agnostic, even though they are computationally intensive. In this paper, we focus on feature selection methods related to the Generalized Covariance Measure (GCM) and Leave-One-Covariate-Out (LOCO) estimation, and provide a comparison based on relative efficiency. In particular, we present a theoretical comparison under three model settings: linear models, non-linear additive models, and single index models that mimic a single-layer neural network. We complement this with extensive simulations and real data examples. Our theoretical results, along with empirical findings, demonstrate that GCM-related methods generally outperform LOCO under suitable regularity conditions. Furthermore, we quantify the asymptotic relative efficiency of these approaches. Our simulations and real data analysis include widely used machine learning methods such as neural networks and gradient boosting trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14268v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenghui Zheng, Garvesh Raskutti</dc:creator>
    </item>
    <item>
      <title>Flexible yet Sparse Bayesian Survival Models with Time-Varying Coefficients and Unobserved Heterogeneity</title>
      <link>https://arxiv.org/abs/2206.11320</link>
      <description>arXiv:2206.11320v3 Announce Type: replace 
Abstract: Survival analysis is an important area of medical research, yet existing models often struggle to balance simplicity with flexibility. Simple models require minimal adjustments but come with strong assumptions, while more flexible models require significant input and tuning from researchers. We present a survival model using a Bayesian hierarchical shrinkage method that automatically determines whether each covariate should be treated as static, time-varying, or excluded altogether. This approach strikes a balance between simplicity and flexibility, minimizes the need for tuning, and naturally quantifies uncertainty. The method is supported by an efficient Markov chain Monte Carlo sampler, implemented in the R package shrinkDSM. Comprehensive simulation studies and an application to a clinical dataset involving patients with adenocarcinoma of the gastroesophageal junction showcase the advantages of our approach compared to existing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11320v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus, Daniel Winkler, Sebastian F. Schoppmann, Gerd Jomrich</dc:creator>
    </item>
    <item>
      <title>Simultaneous Modeling of Disease Screening and Severity Prediction: A Multi-task and Sparse Regularization Approach</title>
      <link>https://arxiv.org/abs/2309.04685</link>
      <description>arXiv:2309.04685v3 Announce Type: replace 
Abstract: Identifying clinically relevant biomarkers and developing predictive models are central challenges in biomedical research. Biomarkers are commonly used for disease screening, and some provide information not only on the presence or absence of a disease but also on its severity. Such biomarkers can contribute to treatment prioritization and support clinical decision-making. To address both disease screening and severity prediction, this paper focuses on regression modeling for ordinal outcomes with a hierarchical structure. When the response variable is a combination of the presence of disease and severity, such as {healthy, mild, intermediate, severe}, a straightforward approach is to apply the conventional ordinal regression model. However, such models may lack the flexibility needed to capture heterogeneity in how predictors relate to response levels, particularly when the response levels have a heterogeneous association structure with predictors. Therefore, this paper proposes a model that treats screening and severity prediction as separate tasks, along with an estimation method based on structural sparse regularization. This method is designed to leverage a shared structure between the tasks. In numerical experiments, the proposed method demonstrated stable performance across many scenarios compared to existing ordinal regression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04685v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.eswa.2025.129408</arxiv:DOI>
      <dc:creator>Kazuharu Harada, Shuichi Kawano, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>Spectral analysis for the inference of noisy Hawkes processes</title>
      <link>https://arxiv.org/abs/2405.12581</link>
      <description>arXiv:2405.12581v2 Announce Type: replace 
Abstract: Classic estimation methods for Hawkes processes rely on the assumption that observed event times are indeed a realisation of a Hawkes process, without considering any potential perturbation of the model. However, in practice, observations are often altered by some noise, the form of which depends on the context. It is then required to model the alteration mechanism in order to infer accurately such a noisy Hawkes process. While several models exist, we consider, in this work, the observations to be the indistinguishable union of event times coming from a Hawkes process and from an independent Poisson process. Since standard inference methods (such as maximum likelihood or Expectation-Maximisation) are either unworkable or numerically prohibitive in this context, we propose an estimation procedure based on the spectral analysis of second order properties of the noisy Hawkes process. Novel results include sufficient conditions for identifiability of the ensuing statistical model with exponential interaction functions for both univariate and bivariate processes, along with consistency and asymptotic normality guarantees of our estimator in the univariate case. Although we mainly focus on the exponential scenario, other types of kernels are investigated and discussed. A new estimator based on maximising the spectral log-likelihood is then described, and its behaviour is numerically illustrated on both synthetic data and neuronal data. Besides being free from knowing the source of each observed time (Hawkes or Poisson process), the proposed estimator is shown to perform accurately in estimating both processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12581v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bonnet (LPSM), Felix Cheysson (LAMA), Miguel Martinez Herrera (LPSM), Maxime Sangnier (LPSM)</dc:creator>
    </item>
    <item>
      <title>Discrete Autoregressive Switching Processes with Cumulative Shrinkage Priors for Graphical Modeling of Time Series Data</title>
      <link>https://arxiv.org/abs/2406.03385</link>
      <description>arXiv:2406.03385v2 Announce Type: replace 
Abstract: We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03385v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Beniamino Hadj-Amar, Aaron M. Bornstein, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Exploring the Difficulty of Estimating Win Probability: A Simulation Study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v5 Announce Type: replace 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators use machine learning to fit the relationship between a binary win/loss outcome variable and certain game-state variables. To illustrate just how difficult it is to accurately fit such a model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. Further, to achieve approximately valid marginal coverage, win probability confidence intervals need to be substantially wide. Concisely, these are high variance estimators subject to substantial uncertainty. Our findings are not unique to the particular application of estimating win probability; they are broadly applicable across sports analytics, as myriad other sports datasets are clustered into groups of observations that share the same outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests</title>
      <link>https://arxiv.org/abs/2407.01036</link>
      <description>arXiv:2407.01036v3 Announce Type: replace 
Abstract: A/B testing is a core tool for decision-making in business experimentation, particularly in digital platforms and marketplaces. Practitioners often prioritize lift in performance metrics while seeking to control the costs of false discoveries. This paper develops a decision-theoretic framework for maximizing expected profit subject to a constraint on the cost-weighted false discovery rate (FDR). We propose an empirical Bayes approach that uses a greedy knapsack algorithm to rank experiments based on the ratio of expected lift to cost, incorporating the local false discovery rate (lfdr) as a key statistic. The resulting oracle rule is valid and rank-optimal. In large-scale settings, we establish the asymptotic validity of a data-driven implementation and demonstrate superior finite-sample performance over existing FDR-controlling methods. An application to A/B tests run on the Optimizely platform highlights the business value of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01036v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pallavi Basu, Ron Berman</dc:creator>
    </item>
    <item>
      <title>Repeated sampling of different individuals but the same clusters to improve precision of difference-in-differences estimators: the DISC design</title>
      <link>https://arxiv.org/abs/2411.17905</link>
      <description>arXiv:2411.17905v2 Announce Type: replace 
Abstract: We describe the DISC (Different Individuals, Same Clusters) design, a sampling scheme that can improve the precision of difference-in-differences (DID) estimators in settings involving repeated sampling of a population at multiple time points. Although cohort designs typically lead to more efficient DID estimators relative to repeated cross-sectional (RCS) designs, they are often impractical in practice due to high rates of loss-to-follow-up, individuals leaving the risk set, or other reasons. The DISC design represents a hybrid between a cohort sampling design and a RCS sampling design, an alternative strategy in which the researcher takes a single sample of clusters, but then takes different cross-sectional samples of individuals within each cluster at two or more time points. We show that the DISC design can yield DID estimators with much higher precision relative to a RCS design, particularly if random cluster effects are present in the data-generating mechanism. For example, for a design in which 40 clusters and 25 individuals per cluster are sampled (for a total sample size of n=1,000), the variance of a commonly-used DID treatment effect estimator is 2.3 times higher in the RCS design for an intraclass correlation coefficient (ICC) of 0.05, 3.8 times higher for an ICC of 0.1, and 7.3 times higher for an ICC of 0.2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17905v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Downey, Avi Kenny</dc:creator>
    </item>
    <item>
      <title>Cost-aware Portfolios in a Large Universe of Assets</title>
      <link>https://arxiv.org/abs/2412.11575</link>
      <description>arXiv:2412.11575v2 Announce Type: replace 
Abstract: This paper considers the finite horizon portfolio rebalancing problem in terms of mean-variance optimization, where decisions are made based on current information on asset returns and transaction costs. The study's novelty is that the transaction costs are integrated within the optimization problem in a high-dimensional portfolio setting where the number of assets is larger than the sample size. We propose portfolio construction and rebalancing models with nonconvex penalty considering two types of transaction cost, the proportional transaction cost and the quadratic transaction cost. We establish the desired theoretical properties under mild regularity conditions. Monte Carlo simulations and empirical studies using S&amp;P 500 and Russell 2000 stocks show the satisfactory performance of the proposed portfolio and highlight the importance of involving the transaction costs when rebalancing a portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11575v2</guid>
      <category>stat.ME</category>
      <category>q-fin.PM</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingliang Fan, Marcelo C. Medeiros, Hanming Yang, Songshan Yang</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Estimation in Causal Survival Analysis: Practical Recommendations</title>
      <link>https://arxiv.org/abs/2501.05836</link>
      <description>arXiv:2501.05836v2 Announce Type: replace 
Abstract: Causal survival analysis combines survival analysis and causal inference to evaluate the effect of a treatment or intervention on a time-to-event outcome, such as survival time. It offers an alternative to relying solely on Cox models for assessing these effects. In this paper, we present a comprehensive review of estimators for the average treatment effect measured with the restricted mean survival time, including regression-based methods, weighting approaches, and hybrid techniques. We investigate their theoretical properties and compare their performance through extensive numerical experiments. Our analysis focuses on the finite-sample behavior of these estimators, the influence of nuisance parameter selection, and their robustness and stability under model misspecification. By bridging theoretical insights with practical evaluation, we aim to equip practitioners with both state-of-the-art implementations of these methods and practical guidelines for selecting appropriate estimators for treatment effect estimation. Among the approaches considered, G-formula two-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival forests emerge as particularly promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05836v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Voinot (PREMEDICAL, Sanofi Gentilly), Cl\'ement Berenfeld (Sanofi Gentilly), Imke Mayer (Sanofi Gentilly), Bernard Sebastien (Sanofi Gentilly), Julie Josse (PREMEDICAL)</dc:creator>
    </item>
    <item>
      <title>Inequality Restricted Minimum Density Power Divergence Estimation in Panel Count Data</title>
      <link>https://arxiv.org/abs/2503.21534</link>
      <description>arXiv:2503.21534v3 Announce Type: replace 
Abstract: Analysis of panel count data has garnered a considerable amount of attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most of the works focus on leveraging estimating equations-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present the robust density power divergence estimation for panel count data arising from nonhomogeneous Poisson processes, correlated through a latent frailty variable. In order to cope with real-world incidents, it is often desired to impose certain inequality constraints on the parameter space, giving rise to the restricted minimum density power divergence estimator. The significant contribution of this study lies in deriving its asymptotic properties. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21534v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Udita Goswami, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>nonprobsvy -- An R package for modern methods for non-probability surveys</title>
      <link>https://arxiv.org/abs/2504.04255</link>
      <description>arXiv:2504.04255v2 Announce Type: replace 
Abstract: The following paper presents nonprobsvy -- an R package for inference based on non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. In the package, we assume the existence of either population-level data or probability-based population information and leverage the survey package for inference. The package implements both analytical and bootstrap variance estimation for the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at scientists and researchers who would like to use non-probability samples (e.g.big data, opt-in web panels, social media) to accurately estimate population characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04255v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Chrostowski, Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>A likelihood-based Bayesian inference framework for the calibration of and selection between stochastic velocity-jump models</title>
      <link>https://arxiv.org/abs/2505.19292</link>
      <description>arXiv:2505.19292v2 Announce Type: replace 
Abstract: Advances in experimental techniques allow the collection of high-resolution spatio-temporal data that track individual motile entities. These tracking data can be used to calibrate mathematical models describing the motility of individual entities. The challenges in calibrating models for single-agent motion derive from the intrinsic characteristics of experimental data, collected at discrete time steps and with measurement noise. We consider motion of individual agents that can be described by velocity-jump models in one spatial dimension. These agents transition between a network of \textit{n} states, in which each state is associated with a fixed velocity and fixed rates of switching to every other state. Exploiting approximate solutions to the resultant stochastic process, we develop a Bayesian inference framework to calibrate these models to discrete-time noisy data. We first demonstrate that the framework can be used to effectively recover the model parameters of data simulated from two-state and three-state models. Finally, we explore the question of model selection first using simulated data and then using experimental data tracking mRNA transport inside \textit{Drosophila} neurons. Overall, our results demonstrate that the framework is effective and efficient in calibrating and selecting between velocity-jump models and it can be applied to a range of motion processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19292v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arianna Ceccarelli, Alexander P. Browning, Tai Chaiamarit, Ilan Davis, Ruth E. Baker</dc:creator>
    </item>
    <item>
      <title>Joint Quantile Shrinkage: A State-Space Approach toward Non-Crossing Bayesian Quantile Models</title>
      <link>https://arxiv.org/abs/2506.13257</link>
      <description>arXiv:2506.13257v2 Announce Type: replace 
Abstract: Crossing of fitted conditional quantiles is a prevalent problem for quantile regression models. We propose a new Bayesian modelling framework that penalises multiple quantile regression functions toward the desired non-crossing space. We achieve this by estimating multiple quantiles jointly with a prior on variation across quantiles, a fused shrinkage prior with quantile adaptivity. The posterior is derived from a decision-theoretic general Bayes perspective, whose form yields a natural state-space interpretation aligned with Time-Varying Parameter (TVP) models. Taken together our approach leads to a Quantile-Varying Parameter (QVP) model, for which we develop efficient sampling algorithms. We demonstrate that our proposed modelling framework provides superior parameter recovery and predictive performance compared to competing Bayesian and frequentist quantile regression estimators in simulated experiments and a real-data application to multivariate quantile estimation in macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13257v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kohns, Tibor Szendrei</dc:creator>
    </item>
    <item>
      <title>Kernel Two-Sample Testing via Directional Components Analysis</title>
      <link>https://arxiv.org/abs/2508.08564</link>
      <description>arXiv:2508.08564v2 Announce Type: replace 
Abstract: We propose a novel kernel-based two-sample test that leverages the spectral decomposition of the maximum mean discrepancy (MMD) statistic to identify and utilize well-estimated directional components in reproducing kernel Hilbert space (RKHS). Our approach is motivated by the observation that the estimation quality of these components varies significantly, with leading eigen-directions being more reliably estimated in finite samples. By focusing on these directions and aggregating information across multiple kernels, the proposed test achieves higher power and improved robustness, especially in high-dimensional and unbalanced sample settings. We further develop a computationally efficient multiplier bootstrap procedure for approximating critical values, which is theoretically justified and significantly faster than permutation-based alternatives. Extensive simulations and empirical studies on microarray datasets demonstrate that our method maintains the nominal Type I error rate and delivers superior power compared to other existing MMD-based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08564v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Cui, Yuhao Li, Xiaojun Song</dc:creator>
    </item>
    <item>
      <title>Regularized e-processes: anytime valid inference with knowledge-based efficiency gains</title>
      <link>https://arxiv.org/abs/2410.01427</link>
      <description>arXiv:2410.01427v4 Announce Type: replace-cross 
Abstract: Classical statistical methods have theoretical justification when the sample size is predetermined. In applications, however, it's often the case that sample sizes are data-dependent rather than predetermined. The aforementioned methods aren't reliable in this latter case, hence the recent interest in e-processes and methods that are anytime valid, i.e., reliable for any dynamic data-collection plan. But if the investigator has relevant-yet-incomplete prior information about the quantity of interest, then there's an opportunity for efficiency gain. This paper proposes a regularized e-process framework featuring a knowledge-based, imprecise-probabilistic regularization with improved efficiency. A generalized version of Ville's inequality is established, ensuring that inference based on the regularized e-process are anytime valid in a novel, knowledge-dependent sense. Regularized e-processes also facilitate possibility-theoretic uncertainty quantification with strong frequentist-like calibration properties and other Bayesian-like properties: satisfies the likelihood principle, avoids sure-loss, and offers formal decision-making with reliability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01427v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Gender disparities in rehospitalisations after coronary artery bypass grafting: evidence from a sparse functional causal mediation analysis of the MIMIC-IV data</title>
      <link>https://arxiv.org/abs/2410.22502</link>
      <description>arXiv:2410.22502v3 Announce Type: replace-cross 
Abstract: Hospital readmissions following coronary artery bypass grafting (CABG) not only impose a substantial cost burden on healthcare systems but also serve as a potential indicator of the quality of medical care. Previous studies of gender effects on complications after CABG surgery have consistently revealed that women tend to suffer worse outcomes. To better understand the causal pathway from gender to the number of rehospitalisations, we study the postoperative central venous pressure (CVP), recorded over the first 24 hours of patients' intensive care unit (ICU) stay after the CABG surgery, as sparse observations of a functional mediator. Confronted with time-varying CVP measurements and zero-inflated rehospitalisation counts within 60 days following discharge, we propose a parameter-simulating quasi-Bayesian Monte Carlo approximation method that accommodates a sparse functional mediator and a zero-inflated count outcome for causal mediation analysis. We find a causal relationship between the female gender and increased rehospitalisation counts after CABG, and that time-varying central venous pressure mediates this causal effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22502v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henan Xu, Yeying Zhu, Donna L. Coffman</dc:creator>
    </item>
  </channel>
</rss>
