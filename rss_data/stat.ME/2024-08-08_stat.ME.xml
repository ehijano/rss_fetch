<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2024 01:37:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Novel Approximate Bayesian Inference Method for Compartmental Models in Epidemiology using Stan</title>
      <link>https://arxiv.org/abs/2408.03415</link>
      <description>arXiv:2408.03415v1 Announce Type: new 
Abstract: Mechanistic compartmental models are widely used in epidemiology to study the dynamics of infectious disease transmission. These models have significantly contributed to designing and evaluating effective control strategies during pandemics. However, the increasing complexity and the number of parameters needed to describe rapidly evolving transmission scenarios present significant challenges for parameter estimation due to intractable likelihoods. To overcome this issue, likelihood-free methods have proven effective for accurately and efficiently fitting these models to data. In this study, we focus on approximate Bayesian computation (ABC) and synthetic likelihood methods for parameter inference. We develop a method that employs ABC to select the most informative subset of summary statistics, which are then used to construct a synthetic likelihood for posterior sampling. Posterior sampling is performed using Hamiltonian Monte Carlo as implemented in the Stan software. The proposed algorithm is demonstrated through simulation studies, showing promising results for inference in a simulated epidemic scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03415v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiahui Li, Ben Swallow, Fergus J. Chadwick</dc:creator>
    </item>
    <item>
      <title>Identifying treatment response subgroups in observational time-to-event data</title>
      <link>https://arxiv.org/abs/2408.03463</link>
      <description>arXiv:2408.03463v1 Announce Type: new 
Abstract: Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for subgroup analysis primarily focus on Randomised Controlled Trials (RCTs), in which treatment assignment is randomised. Furthermore, the patient cohort of an RCT is often constrained by cost, and is not representative of the heterogeneity of patients likely to receive treatment in real-world clinical practice. Therefore, when applied to observational studies, such approaches suffer from significant statistical biases because of the non-randomisation of treatment. Our work introduces a novel, outcome-guided method for identifying treatment response subgroups in observational studies. Our approach assigns each patient to a subgroup associated with two time-to-event distributions: one under treatment and one under control regime. It hence positions itself in between individualised and average treatment effect estimation. The assumptions of our model result in a simple correction of the statistical bias from treatment non-randomisation through inverse propensity weighting. In experiments, our approach significantly outperforms the current state-of-the-art method for outcome-guided subgroup analysis in both randomised and observational treatment regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03463v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Jeanselme, Chang Ho Yoon, Fabian Falck, Brian Tom, Jessica Barrett</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis using the Metamodel of Optimal Prognosis</title>
      <link>https://arxiv.org/abs/2408.03590</link>
      <description>arXiv:2408.03590v1 Announce Type: new 
Abstract: In real case applications within the virtual prototyping process, it is not always possible to reduce the complexity of the physical models and to obtain numerical models which can be solved quickly. Usually, every single numerical simulation takes hours or even days. Although the progresses in numerical methods and high performance computing, in such cases, it is not possible to explore various model configurations, hence efficient surrogate models are required. Generally the available meta-model techniques show several advantages and disadvantages depending on the investigated problem. In this paper we present an automatic approach for the selection of the optimal suitable meta-model for the actual problem. Together with an automatic reduction of the variable space using advanced filter techniques an efficient approximation is enabled also for high dimensional problems. This filter techniques enable a reduction of the high dimensional variable space to a much smaller subspace where meta-model-based sensitivity analyses are carried out to assess the influence of important variables and to identify the optimal subspace with corresponding surrogate model which enables the most accurate probabilistic analysis. For this purpose we investigate variance-based and moment-free sensitivity measures in combination with advanced meta-models as moving least squares and kriging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03590v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Most, Johannes Will</dc:creator>
    </item>
    <item>
      <title>Piecewise Constant Hazard Estimation with the Fused Lasso</title>
      <link>https://arxiv.org/abs/2408.03602</link>
      <description>arXiv:2408.03602v1 Announce Type: new 
Abstract: In applied time-to-event analysis, a flexible parametric approach is to model the hazard rate as a piecewise constant function of time. However, the change points and values of the piecewise constant hazard are usually unknown and need to be estimated. In this paper, we develop a fully data-driven procedure for piecewise constant hazard estimation. We work in a general counting process framework which nests a wide range of popular models in time-to-event analysis including Cox's proportional hazards model with potentially high-dimensional covariates, competing risks models as well as more general multi-state models. To construct our estimator, we set up a regression model for the increments of the Breslow estimator and then use fused lasso techniques to approximate the piecewise constant signal in this regression model. In the theoretical part of the paper, we derive the convergence rate of our estimator as well as some results on how well the change points of the piecewise constant hazard are approximated by our method. We complement the theory by both simulations and a real data example, illustrating that our results apply in rather general event histories such as multi-state models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03602v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Rosenbaum, Jan Beyersmann, Michael Vogt</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for the generalized extreme value distribution: a method that combines bootstrapping and r largest order statistics</title>
      <link>https://arxiv.org/abs/2408.03738</link>
      <description>arXiv:2408.03738v1 Announce Type: new 
Abstract: A critical problem in extreme value theory (EVT) is the estimation of parameters for the limit probability distributions. Block maxima (BM), an approach in EVT that seeks estimates of parameters of the generalized extreme value distribution (GEV), can be generalized to take into account not just the maximum realization from a given dataset, but the r largest order statistics for a given r. In this work we propose a parameter estimation method that combines the r largest order statistic (r-LOS) extension of BM with permutation bootstrapping: surrogate realizations are obtained by randomly reordering the original data set, and then r-LOS is applied to these shuffled measurements - the mean estimate computed from these surrogate realizations is the desired estimate. We used synthetic observations and real meteorological time series to verify the performance of our method; we found that the combination of r-LOS and bootstrapping resulted in estimates more accurate than when either approach was implemented separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03738v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Juan L. P. Soto</dc:creator>
    </item>
    <item>
      <title>Combining BART and Principal Stratification to estimate the effect of intermediate on primary outcomes with application to estimating the effect of family planning on employment in sub-Saharan Africa</title>
      <link>https://arxiv.org/abs/2408.03777</link>
      <description>arXiv:2408.03777v1 Announce Type: new 
Abstract: There is interest in learning about the causal effect of family planning (FP) on empowerment related outcomes. Experimental data related to this question are available from trials in which FP programs increase access to FP. While program assignment is unconfounded, FP uptake and subsequent empowerment may share common causes. We use principal stratification to estimate the causal effect of an intermediate FP outcome on a primary outcome of interest, among women affected by a FP program. Within strata defined by the potential reaction to the program, FP uptake is unconfounded. To minimize the need for parametric assumptions, we propose to use Bayesian Additive Regression Trees (BART) for modeling stratum membership and outcomes of interest. We refer to the combined approach as Prince BART. We evaluate Prince BART through a simulation study and use it to assess the causal effect of modern contraceptive use on employment in six cities in Nigeria, based on quasi-experimental data from a FP program trial during the first half of the 2010s. We show that findings differ between Prince BART and alternative modeling approaches based on parametric assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03777v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Sequential Conditional Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness</title>
      <link>https://arxiv.org/abs/2408.03425</link>
      <description>arXiv:2408.03425v1 Announce Type: cross 
Abstract: In this paper, we link two existing approaches to derive counterfactuals: adaptations based on a causal graph, as suggested in Ple\v{c}ko and Meinshausen (2020) and optimal transport, as in De Lara et al. (2024). We extend "Knothe's rearrangement" Bonnotte (2013) and "triangular transport" Zech and Marzouk (2022a) to probabilistic graphical models, and use this counterfactual approach, referred to as sequential transport, to discuss individual fairness. After establishing the theoretical foundations of the proposed method, we demonstrate its application through numerical experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03425v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic</dc:creator>
    </item>
    <item>
      <title>InPer: Whole-Process Domain Generalization via Causal Intervention and Perturbation</title>
      <link>https://arxiv.org/abs/2408.03608</link>
      <description>arXiv:2408.03608v1 Announce Type: cross 
Abstract: Despite the considerable advancements achieved by deep neural networks, their performance tends to degenerate when the test environment diverges from the training ones. Domain generalization (DG) solves this issue by learning representations independent of domain-related information, thus facilitating extrapolation to unseen environments. Existing approaches typically focus on formulating tailored training objectives to extract shared features from the source data. However, the disjointed training and testing procedures may compromise robustness, particularly in the face of unforeseen variations during deployment. In this paper, we propose a novel and holistic framework based on causality, named InPer, designed to enhance model generalization by incorporating causal intervention during training and causal perturbation during testing. Specifically, during the training phase, we employ entropy-based causal intervention (EnIn) to refine the selection of causal variables. To identify samples with anti-interference causal variables from the target domain, we propose a novel metric, homeostatic score, through causal perturbation (HoPer) to construct a prototype classifier in test time. Experimental results across multiple cross-domain tasks confirm the efficacy of InPer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03608v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Xinghao Ding, Yue Huang</dc:creator>
    </item>
    <item>
      <title>On the choice of the non-trainable internal weights in random feature maps</title>
      <link>https://arxiv.org/abs/2408.03626</link>
      <description>arXiv:2408.03626v1 Announce Type: cross 
Abstract: The computationally cheap machine learning architecture of random feature maps can be viewed as a single-layer feedforward network in which the weights of the hidden layer are random but fixed and only the outer weights are learned via linear regression. The internal weights are typically chosen from a prescribed distribution. The choice of the internal weights significantly impacts the accuracy of random feature maps. We address here the task of how to best select the internal weights. In particular, we consider the forecasting problem whereby random feature maps are used to learn a one-step propagator map for a dynamical system. We provide a computationally cheap hit-and-run algorithm to select good internal weights which lead to good forecasting skill. We show that the number of good features is the main factor controlling the forecasting skill of random feature maps and acts as an effective feature dimension. Lastly, we compare random feature maps with single-layer feedforward neural networks in which the internal weights are now learned using gradient descent. We find that random feature maps have superior forecasting capabilities whilst having several orders of magnitude lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03626v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinak Mandal, Georg A. Gottwald</dc:creator>
    </item>
    <item>
      <title>Robust changepoint detection in the variability of multivariate functional data</title>
      <link>https://arxiv.org/abs/2112.01611</link>
      <description>arXiv:2112.01611v3 Announce Type: replace 
Abstract: We consider the problem of robustly detecting changepoints in the variability of a sequence of independent multivariate functions. We develop a novel changepoint procedure, called the functional Kruskal--Wallis for covariance (FKWC) changepoint procedure, based on rank statistics and multivariate functional data depth. The FKWC changepoint procedure allows the user to test for at most one changepoint (AMOC) or an epidemic period, or to estimate the number and locations of an unknown amount of changepoints in the data. We show that when the ``signal-to-noise'' ratio is bounded below, the changepoint estimates produced by the FKWC procedure attain the minimax localization rate for detecting general changes in distribution in the univariate setting (Theorem 1). We also provide the behavior of the proposed test statistics for the AMOC and epidemic setting under the null hypothesis (Theorem 2) and, as a simple consequence of our main result, these tests are consistent (Corollary 1). In simulation, we show that our method is particularly robust when compared to similar changepoint methods. We present an application of the FKWC procedure to intraday asset returns and f-MRI scans. As a by-product of Theorem 1, we provide a concentration result for integrated functional depth functions (Lemma 2), which may be of general interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.01611v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Shoja'eddin Chenouri</dc:creator>
    </item>
    <item>
      <title>Predicting milk traits from spectral data using Bayesian probabilistic partial least squares regression</title>
      <link>https://arxiv.org/abs/2307.04457</link>
      <description>arXiv:2307.04457v4 Announce Type: replace 
Abstract: High-dimensional spectral data -- routinely generated in dairy production -- are used to predict a range of traits in milk products. Partial least squares (PLS) regression is ubiquitously used for these prediction tasks. However, PLS regression is not typically viewed as arising from a probabilistic model, and parameter uncertainty is rarely quantified. Additionally, PLS regression does not easily lend itself to model-based modifications, coherent prediction intervals are not readily available, and the process of choosing the latent-space dimension, $\mathtt{Q}$, can be subjective and sensitive to data size. We introduce a Bayesian latent-variable model, emulating the desirable properties of PLS regression while accounting for parameter uncertainty in prediction. The need to choose $\mathtt{Q}$ is eschewed through a nonparametric shrinkage prior. The flexibility of the proposed Bayesian partial least squares (BPLS) regression framework is exemplified by considering sparsity modifications and allowing for multivariate response prediction. The BPLS regression framework is used in two motivating settings: 1) multivariate trait prediction from mid-infrared spectral analyses of milk samples, and 2) milk pH prediction from surface-enhanced Raman spectral data. The prediction performance of BPLS regression at least matches that of PLS regression. Additionally, the provision of correctly calibrated prediction intervals objectively provides richer, more informative inference for stakeholders in dairy production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04457v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Szymon Urbas, Pierre Lovera, Robert Daly, Alan O'Riordan, Donagh Berry, Isobel Claire Gormley</dc:creator>
    </item>
    <item>
      <title>Nonparametric Linear Feature Learning in Regression Through Regularisation</title>
      <link>https://arxiv.org/abs/2307.12754</link>
      <description>arXiv:2307.12754v4 Announce Type: replace 
Abstract: Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for joint linear feature learning and non-parametric function estimation, aimed at more effectively leveraging hidden features for learning. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By using alternative minimisation, we iteratively rotate the data to improve alignment with leading directions. We establish that the expected risk of our method converges in high-probability to the minimal risk under minimal assumptions and with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12754v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bertille Follain, Francis Bach</dc:creator>
    </item>
    <item>
      <title>The curse of isotropy: from principal components to principal subspaces</title>
      <link>https://arxiv.org/abs/2307.15348</link>
      <description>arXiv:2307.15348v3 Announce Type: replace 
Abstract: This paper raises an important issue about the interpretation of principal component analysis. The curse of isotropy states that a covariance matrix with repeated eigenvalues yields rotation-invariant eigenvectors. In other words, principal components associated with equal eigenvalues show large intersample variability and are arbitrary combinations of potentially more interpretable components. However, empirical eigenvalues are never exactly equal in practice due to sampling errors. Therefore, most users overlook the problem. In this paper, we propose to identify datasets that are likely to suffer from the curse of isotropy by introducing a generative Gaussian model with repeated eigenvalues and comparing it to traditional models via the principle of parsimony. This yields an explicit criterion to detect the curse of isotropy in practice. We notably argue that in a dataset with 1000 samples, all the eigenvalue pairs with a relative eigengap lower than 21% should be assumed equal. This demonstrates that the curse of isotropy cannot be overlooked. In this context, we propose to transition from fuzzy principal components to much-more-interpretable principal subspaces. The final methodology (principal subspace analysis) is extremely simple and shows promising results on a variety of datasets from different fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15348v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Xavier Pennec</dc:creator>
    </item>
    <item>
      <title>Efficient estimation and correction of selection-induced bias with order statistics</title>
      <link>https://arxiv.org/abs/2309.03742</link>
      <description>arXiv:2309.03742v3 Announce Type: replace 
Abstract: Model selection aims to identify a sufficiently well performing model that is possibly simpler than the most complex model among a pool of candidates. However, the decision-making process itself can inadvertently introduce non-negligible bias when the cross-validation estimates of predictive performance are marred by excessive noise. In finite data regimes, cross-validated estimates can encourage the statistician to select one model over another when it is not actually better for future data. While this bias remains negligible in the case of few models, when the pool of candidates grows, and model selection decisions are compounded (as in step-wise selection), the expected magnitude of selection-induced bias is likely to grow too. This paper introduces an efficient approach to estimate and correct selection-induced bias based on order statistics. Numerical experiments demonstrate the reliability of our approach in estimating both selection-induced bias and over-fitting along compounded model selection decisions, with specific application to forward search. This work represents a light-weight alternative to more computationally expensive approaches to correcting selection-induced bias, such as nested cross-validation and the bootstrap. Our approach rests on several theoretic assumptions, and we provide a diagnostic to help understand when these may not be valid and when to fall back on safer, albeit more computationally expensive approaches. The accompanying code facilitates its practical implementation and fosters further exploration in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03742v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-024-10442-4</arxiv:DOI>
      <arxiv:journal_reference>Stat Comput 34, 132 (2024)</arxiv:journal_reference>
      <dc:creator>Yann McLatchie, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>A novel CFA+EFA model to detect aberrant respondents</title>
      <link>https://arxiv.org/abs/2311.15988</link>
      <description>arXiv:2311.15988v2 Announce Type: replace 
Abstract: Aberrant respondents are common but yet extremely detrimental to the quality of social surveys or questionnaires. Recently, factor mixture models have been employed to identify individuals providing deceptive or careless responses. We propose a comprehensive factor mixture model for continuous outcomes that combines confirmatory and exploratory factor models to classify both the non-aberrant and aberrant respondents. The flexibility of the proposed {classification model} allows for the identification of two of the most common aberrant response styles, namely faking and careless responding. We validated our approach by means of two simulations and two case studies. The results indicate the effectiveness of the proposed model in dealing with aberrant responses in social and behavioural surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15988v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssc/qlae036</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society Series C, Oxford University Press, 2024</arxiv:journal_reference>
      <dc:creator>Niccol\`o Cao, Livio Finos, Luigi Lombardi, Antonio Calcagn\`i</dc:creator>
    </item>
    <item>
      <title>Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information</title>
      <link>https://arxiv.org/abs/2406.08738</link>
      <description>arXiv:2406.08738v3 Announce Type: replace 
Abstract: We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08738v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David P. Lundquist, Daniel J. Eck</dc:creator>
    </item>
    <item>
      <title>Computationally efficient multi-level Gaussian process regression for functional data observed under completely or partially regular sampling designs</title>
      <link>https://arxiv.org/abs/2406.13691</link>
      <description>arXiv:2406.13691v2 Announce Type: replace 
Abstract: Gaussian process regression is a frequently used statistical method for flexible yet fully probabilistic non-linear regression modeling. A common obstacle is its computational complexity which scales poorly with the number of observations. This is especially an issue when applying Gaussian process models to multiple functions simultaneously in various applications of functional data analysis.
  We consider a multi-level Gaussian process regression model where a common mean function and individual subject-specific deviations are modeled simultaneously as latent Gaussian processes. We derive exact analytic and computationally efficient expressions for the log-likelihood function and the posterior distributions in the case where the observations are sampled on either a completely or partially regular grid. This enables us to fit the model to large data sets that are currently computationally inaccessible using a standard implementation. We show through a simulation study that our analytic expressions are several orders of magnitude faster compared to a standard implementation, and we provide an implementation in the probabilistic programming language Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13691v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Gorm Hoffmann, Claus Thorn Ekstr{\o}m, Andreas Kryger Jensen</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariate Quantile Regression with alternative Time-varying Volatility Specifications</title>
      <link>https://arxiv.org/abs/2211.16121</link>
      <description>arXiv:2211.16121v2 Announce Type: replace-cross 
Abstract: This article proposes a novel Bayesian multivariate quantile regression to forecast the tail behavior of energy commodities, where the homoskedasticity assumption is relaxed to allow for time-varying volatility. In particular, we exploit the mixture representation of the multivariate asymmetric Laplace likelihood and the Cholesky-type decomposition of the scale matrix to introduce stochastic volatility and GARCH processes and then provide an efficient MCMC to estimate them. The proposed models outperform the homoskedastic benchmark mainly when predicting the distribution's tails. We provide a model combination using a quantile score-based weighting scheme, which leads to improved performances, notably when no single model uniformly outperforms the other across quantiles, time, or variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16121v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Francesco Ravazzolo, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>Probabilistically Plausible Counterfactual Explanations with Normalizing Flows</title>
      <link>https://arxiv.org/abs/2405.17640</link>
      <description>arXiv:2405.17640v2 Announce Type: replace-cross 
Abstract: We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs). PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework. Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions. This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data's probability density. For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution. Furthermore, we introduce a novel loss that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term. PPCEF's unconstrained formulation allows for efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods. Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties. Finally, extensive evaluations demonstrate PPCEF's superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings. This makes PPCEF a powerful tool for not only interpreting complex machine learning models but also for improving fairness, accountability, and trust in AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17640v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patryk Wielopolski, Oleksii Furman, Jerzy Stefanowski, Maciej Zi\k{e}ba</dc:creator>
    </item>
    <item>
      <title>Practical Guide for Causal Pathways and Sub-group Disparity Analysis</title>
      <link>https://arxiv.org/abs/2407.02702</link>
      <description>arXiv:2407.02702v3 Announce Type: replace-cross 
Abstract: In this study, we introduce the application of causal disparity analysis to unveil intricate relationships and causal pathways between sensitive attributes and the targeted outcomes within real-world observational data. Our methodology involves employing causal decomposition analysis to quantify and examine the causal interplay between sensitive attributes and outcomes. We also emphasize the significance of integrating heterogeneity assessment in causal disparity analysis to gain deeper insights into the impact of sensitive attributes within specific sub-groups on outcomes. Our two-step investigation focuses on datasets where race serves as the sensitive attribute. The results on two datasets indicate the benefit of leveraging causal analysis and heterogeneity assessment not only for quantifying biases in the data but also for disentangling their influences on outcomes. We demonstrate that the sub-groups identified by our approach to be affected the most by disparities are the ones with the largest ML classification errors. We also show that grouping the data only based on a sensitive attribute is not enough, and through these analyses, we can find sub-groups that are directly affected by disparities. We hope that our findings will encourage the adoption of such methodologies in future ethical AI practices and bias audits, fostering a more equitable and fair technological landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02702v3</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farnaz Kohankhaki, Shaina Raza, Oluwanifemi Bamgbose, Deval Pandya, Elham Dolatabadi</dc:creator>
    </item>
  </channel>
</rss>
