<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Jun 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Experimental Characterization of Combined RowHammer and RowPress Read Disturbance in Modern DRAM Chips</title>
      <link>https://arxiv.org/abs/2406.13080</link>
      <description>arXiv:2406.13080v1 Announce Type: new 
Abstract: DRAM read disturbance can break memory isolation, a fundamental property to ensure system robustness (i.e., reliability, security, safety). RowHammer and RowPress are two different DRAM read disturbance phenomena. RowHammer induces bitflips in physically adjacent victim DRAM rows by repeatedly opening and closing an aggressor DRAM row, while RowPress induces bitflips by keeping an aggressor DRAM row open for a long period of time. In this study, we characterize a DRAM access pattern that combines RowHammer and RowPress in 84 real DDR4 DRAM chips from all three major DRAM manufacturers. Our key results show that 1) this combined RowHammer and RowPress pattern takes significantly smaller amount of time (up to 46.1% faster) to induce the first bitflip compared to the state-of-the-art RowPress pattern, and 2) at the minimum aggressor row activation count to induce at least one bitflip, the bits that flip are different across RowHammer, RowPress, and the combined patterns. Based on our results, we provide a key hypothesis that the read disturbance effect caused by RowPress from one of the two aggressor rows in a double-sided pattern is much more significant than the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13080v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haocong Luo, Ismail Emir Y\"uksel, Ataberk Olgun, A. Giray Ya\u{g}l{\i}k\c{c}{\i}, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Demonstration of low power and highly uniform 6-bit operation in SiO2-based memristors embedded with Pt nanoparticles</title>
      <link>https://arxiv.org/abs/2406.13505</link>
      <description>arXiv:2406.13505v1 Announce Type: new 
Abstract: In this work, an optimized method was implemented for attaining stable multibit operation with low energy consumption in a two-terminal memory element made from the following layers: Ag/Pt nanoparticles (NPs)/SiO2/TiN in a 1-Transistor-1-Memristor configuration. Compared to the reference sample where no NPs were embedded, an enlarged memory window was recorded in conjunction with reduced variability for both switching states. A comprehensive numerical model was also applied to shed light on this enhanced performance, which was attributed to the spatial confinement effect induced by the presence of the Pt NPs and its impact on the properties of the percolating conducting filaments (CFs). Although 5-bit precision was demonstrated with the application of the incremental-step-pulse-programming (ISPP) algorithm, the reset process was unreliable and the output current increased abnormally when exceeded the value of 150 uA. As a result, the multibit operation was limited. To address this issue, a modified scheme was developed to accurately control the distance between the various resistance levels and achieve highly reliable 6-bit precision. Our work provides valuable insights for the development of energy-efficient memories for applications where a high density of conductance levels is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13505v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>physics.app-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Kleitsiotis, P. Bousoulas, S. D. Mantas, C. Tsioustas, I. A. Fyrigos, G. Sirakoulis, D. Tsoukalas</dc:creator>
    </item>
    <item>
      <title>AMC: Access to Miss Correlation Prefetcher for Evolving Graph Analytics</title>
      <link>https://arxiv.org/abs/2406.14008</link>
      <description>arXiv:2406.14008v1 Announce Type: new 
Abstract: Modern memory hierarchies work well with applications that have good spatial locality. Evolving (dynamic) graphs are important applications widely used to model graphs and networks with edge and vertex changes. They exhibit irregular memory access patterns and suffer from a high miss ratio and long miss penalty. Prefetching can be employed to predict and fetch future demand misses. However, current hardware prefetchers can not efficiently predict for applications with irregular memory accesses. In evolving graph applications, vertices that do not change during graph changes exhibit the same access correlation patterns. Current temporal prefetchers use one-to-one or one-to-many correlation to exploit these patterns. Similar patterns are recorded in the same entry, which causes aliasing and can lead to poor prefetch accuracy and coverage. This work proposes a software-assisted hardware prefetcher for evolving graphs. The key idea is to record the correlations between a sequence of vertex accesses and the following misses and then prefetch when the same vertex access sequence occurs in the future. The proposed Access-to-Miss Correlation (AMC) prefetcher provides a lightweight programming interface to identify the data structures of interest and sets the iteration boundary to update the correlation table. For the evaluated applications, AMC achieves a geomean speedup of 1.5x as compared to the best-performing prefetcher in prior work (VLDP). AMC can achieve an average of 62% accuracy and coverage, whereas VLDP has an accuracy of 31% and coverage of 23%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14008v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Singh, Christian Schulte, Xiaochen Guo</dc:creator>
    </item>
    <item>
      <title>COOK Access Control on an embedded Volta GPU</title>
      <link>https://arxiv.org/abs/2406.14081</link>
      <description>arXiv:2406.14081v1 Announce Type: new 
Abstract: The last decade has seen the emergence of a new generation of multi-core in response to advances in machine learning, and in particular Deep Neural Network (DNN) training and inference tasks. These platforms, like the JETSON AGX XAVIER, embed several cores and accelerators in a SWaP- efficient (Size Weight and Power) package with a limited set of resources. However, concurrent applications tend to interfere on shared resources, resulting in high execution time variability for applications compared to their behaviour in isolation.Access control techniques aim to selectively restrict the flow of operations executed by a resource. To reduce the impact of interference on the JETSON Volta GPU, we specify and implement an access control technique to ensure each GPU operation executes in isolation to reduce its timing variability. We implement the controller using three different strategies and assess their complexity and impact on the application performance. Our evaluation shows the benefits of adding the access control: its transparency to applications, reduced timing variability, isolation between GPU operations, and small code complexity. However, the strategies may cause some potential slowdowns for applications even in isolation but which are reasonable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14081v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Lesage, Fr\'ed\'eric Boniol, Claire Pagetti</dc:creator>
    </item>
    <item>
      <title>Scalable and RISC-V Programmable Near-Memory Computing Architectures for Edge Nodes</title>
      <link>https://arxiv.org/abs/2406.14263</link>
      <description>arXiv:2406.14263v1 Announce Type: new 
Abstract: The widespread adoption of data-centric algorithms, particularly Artificial Intelligence (AI) and Machine Learning (ML), has exposed the limitations of centralized processing infrastructures, driving a shift towards edge computing. This necessitates stringent constraints on energy efficiency, which traditional von Neumann architectures struggle to meet. The Compute-In-Memory (CIM) paradigm has emerged as a superior candidate due to its efficient exploitation of available memory bandwidth. However, existing CIM solutions require high implementation effort and lack flexibility from a software integration standpoint. This work proposes a novel, software-friendly, general-purpose, and low-integration-effort Near-Memory Computing (NMC) approach, paving the way for the adoption of CIM-based systems in the next generation of edge computing nodes. Two architectural variants, NM-Caesar and NM-Carus, are proposed and characterized to target different trade-offs in area efficiency, performance, and flexibility, covering a wide range of embedded microcontrollers. Post-layout simulations show up to $25.8\times$ and $50.0\times$ lower execution time and $23.2\times$ and $33.1\times$ higher energy efficiency at the system level, respectively, compared to executing the same tasks on a state-of-the-art RISC-V CPU (RV32IMC). NM-Carus achieves a peak energy efficiency of $306.7$ GOPS/W in 8-bit matrix multiplications, surpassing recent state-of-the-art in- and near-memory circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14263v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Michele Caon (Politecnico di Torino), Cl\'ement Chon\'e (\'Ecole Polytechnique F\'ed\'erale de Lausanne), Pasquale Davide Schiavone (\'Ecole Polytechnique F\'ed\'erale de Lausanne), Alexandre Levisse (\'Ecole Polytechnique F\'ed\'erale de Lausanne), Guido Masera (Politecnico di Torino), Maurizio Martina (Politecnico di Torino), David Atienza (\'Ecole Polytechnique F\'ed\'erale de Lausanne)</dc:creator>
    </item>
    <item>
      <title>Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization</title>
      <link>https://arxiv.org/abs/2406.12930</link>
      <description>arXiv:2406.12930v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate outstanding performance in various tasks in machine learning and have thus become one of the most important workloads in today's computing landscape. However, deploying LLM inference poses challenges due to the high compute and memory requirements stemming from the enormous model size and the difficulty of running it in the integer pipelines. In this paper, we present Tender, an algorithm-hardware co-design solution that enables efficient deployment of LLM inference at low precision. Based on our analysis of outlier values in LLMs, we propose a decomposed quantization technique in which the scale factors of decomposed matrices are powers of two apart. The proposed scheme allows us to avoid explicit requantization (i.e., dequantization/quantization) when accumulating the partial sums from the decomposed matrices, with a minimal extension to the commodity tensor compute hardware. Our evaluation shows that Tender achieves higher accuracy and inference performance compared to the state-of-the-art methods while also being significantly less intrusive to the existing accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12930v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jungi Lee, Wonbeom Lee, Jaewoong Sim</dc:creator>
    </item>
    <item>
      <title>WWW: What, When, Where to Compute-in-Memory</title>
      <link>https://arxiv.org/abs/2312.15896</link>
      <description>arXiv:2312.15896v2 Announce Type: replace 
Abstract: Compute-in-memory (CiM) has emerged as a highly energy efficient solution for performing matrix multiplication during Machine Learning (ML) inference. However, integrating compute in memory poses key questions, such as 1) What type of CiM to use: Given a multitude of CiM design characteristics, determining their suitability from architecture perspective is needed. 2) When to use CiM: ML inference includes workloads with a variety of memory and compute requirements, making it difficult to identify when CiM is more beneficial. 3) Where to integrate CiM: Each memory level has different bandwidth and capacity, creating different data reuse opportunities for CiM integration.
  To answer such questions regarding on-chip CiM integration for accelerating ML workloads, we use an analytical architecture evaluation methodology where we tailor the dataflow mapping. The mapping algorithm aims to achieve highest weight reuse and reduced data movements for a given CiM prototype and workload. Our experiments show that CiM integrated memory improves energy efficiency by up to 3.4x and throughput by up to 15.6x compared to tensor-core-like baseline architecture, with INT-8 precision under iso-area constraints. We believe the proposed work provides insights into what type of CiM to use, and when and where to optimally integrate it in the cache hierarchy for efficient matrix multiplication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15896v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanvi Sharma, Mustafa Ali, Indranil Chakraborty, Kaushik Roy</dc:creator>
    </item>
    <item>
      <title>DG-RePlAce: A Dataflow-Driven GPU-Accelerated Analytical Global Placement Framework for Machine Learning Accelerators</title>
      <link>https://arxiv.org/abs/2404.13049</link>
      <description>arXiv:2404.13049v2 Announce Type: replace 
Abstract: Global placement is a fundamental step in VLSI physical design. The wide use of 2D processing element (PE) arrays in machine learning accelerators poses new challenges of scalability and Quality of Results (QoR) for state-of-the-art academic global placers. In this work, we develop DG-RePlAce, a new and fast GPU-accelerated global placement framework built on top of the OpenROAD infrastructure, which exploits the inherent dataflow and datapath structures of machine learning accelerators. Experimental results with a variety of machine learning accelerators using a commercial 12nm enablement show that, compared with RePlAce (DREAMPlace), our approach achieves an average reduction in routed wirelength by 10% (7%) and total negative slack (TNS) by 31% (34%), with faster global placement and on-par total runtimes relative to DREAMPlace. Empirical studies on the TILOS MacroPlacement Benchmarks further demonstrate that post-route improvements over RePlAce and DREAMPlace may reach beyond the motivating application to machine learning accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13049v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew B. Kahng, Zhiang Wang</dc:creator>
    </item>
    <item>
      <title>Low latency optical-based mode tracking with machine learning deployed on FPGAs on a tokamak</title>
      <link>https://arxiv.org/abs/2312.00128</link>
      <description>arXiv:2312.00128v2 Announce Type: replace-cross 
Abstract: Active feedback control in magnetic confinement fusion devices is desirable to mitigate plasma instabilities and enable robust operation. Optical high-speed cameras provide a powerful, non-invasive diagnostic and can be suitable for these applications. In this study, we process fast camera data, at rates exceeding 100kfps, on $\textit{in situ}$ Field Programmable Gate Array (FPGA) hardware to track magnetohydrodynamic (MHD) mode evolution and generate control signals in real-time. Our system utilizes a convolutional neural network (CNN) model which predicts the $n$=1 MHD mode amplitude and phase using camera images with better accuracy than other tested non-deep-learning-based methods. By implementing this model directly within the standard FPGA readout hardware of the high-speed camera diagnostic, our mode tracking system achieves a total trigger-to-output latency of 17.6$\mu$s and a throughput of up to 120kfps. This study at the High Beta Tokamak-Extended Pulse (HBT-EP) experiment demonstrates an FPGA-based high-speed camera data acquisition and processing system, enabling application in real-time machine-learning-based tokamak diagnostic and control as well as potential applications in other scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00128v2</guid>
      <category>physics.plasm-ph</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>physics.ins-det</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumou Wei, Ryan F. Forelli, Chris Hansen, Jeffrey P. Levesque, Nhan Tran, Joshua C. Agar, Giuseppe Di Guglielmo, Michael E. Mauel, Gerald A. Navratil</dc:creator>
    </item>
    <item>
      <title>LLM-Enhanced Bayesian Optimization for Efficient Analog Layout Constraint Generation</title>
      <link>https://arxiv.org/abs/2406.05250</link>
      <description>arXiv:2406.05250v2 Announce Type: replace-cross 
Abstract: Analog layout synthesis faces significant challenges due to its dependence on manual processes, considerable time requirements, and performance instability. Current Bayesian Optimization (BO)-based techniques for analog layout synthesis, despite their potential for automation, suffer from slow convergence and extensive data needs, limiting their practical application. This paper presents the \texttt{LLANA} framework, a novel approach that leverages Large Language Models (LLMs) to enhance BO by exploiting the few-shot learning abilities of LLMs for more efficient generation of analog design-dependent parameter constraints. Experimental results demonstrate that \texttt{LLANA} not only achieves performance comparable to state-of-the-art (SOTA) BO methods but also enables a more effective exploration of the analog circuit design space, thanks to LLM's superior contextual understanding and learning efficiency. The code is available at https://github.com/dekura/LLANA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05250v2</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guojin Chen, Keren Zhu, Seunggeun Kim, Hanqing Zhu, Yao Lai, Bei Yu, David Z. Pan</dc:creator>
    </item>
  </channel>
</rss>
