<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 04:04:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Identification and estimation of dynamic random coefficient models</title>
      <link>https://arxiv.org/abs/2505.01600</link>
      <description>arXiv:2505.01600v1 Announce Type: new 
Abstract: I study panel data linear models with predetermined regressors (such as lagged dependent variables) where coefficients are individual-specific, allowing for heterogeneity in the effects of the regressors on the dependent variable. I show that the model is not point-identified in a short panel context but rather partially identified, and I characterize the identified sets for the mean, variance, and CDF of the coefficient distribution. This characterization is general, accommodating discrete, continuous, and unbounded data, and it leads to computationally tractable estimation and inference procedures. I apply the method to study lifecycle earnings dynamics among U.S. households using the Panel Study of Income Dynamics (PSID) dataset. The results suggest substantial unobserved heterogeneity in earnings persistence, implying that households face varying levels of earnings risk which, in turn, contribute to heterogeneity in their consumption and savings behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01600v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wooyong Lee</dc:creator>
    </item>
    <item>
      <title>Slope Consistency of Quasi-Maximum Likelihood Estimator for Binary Choice Models</title>
      <link>https://arxiv.org/abs/2505.02327</link>
      <description>arXiv:2505.02327v1 Announce Type: new 
Abstract: This paper revisits the slope consistency of QMLE for binary choice models. Ruud (1983, \emph{Econometrica}) introduced a set of conditions under which QMLE may yield a constant multiple of the slope coefficient of binary choice models asymptotically. However, he did not fully establish slope consistency of QMLE, which requires the existence of a positive multiple of slope coefficient identified as an interior maximizer of the population QMLE likelihood function over an appropriately restricted parameter space. We fill this gap by providing a formal proof for slope consistency under the same set of conditions for any binary choice model identified as in Horowitz (1992, \emph{Econometrica}). Our result implies that the logistic regression, which is used extensively in machine learning to analyze binary outcomes associated with a large number of covariates, yields a consistent estimate for the slope coefficient of binary choice models under suitable conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02327v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoosoon Chang, Joon Y. Park, Guo Yan</dc:creator>
    </item>
    <item>
      <title>Multiscale Causal Analysis of Market Efficiency via News Uncertainty Networks and the Financial Chaos Index</title>
      <link>https://arxiv.org/abs/2505.01543</link>
      <description>arXiv:2505.01543v1 Announce Type: cross 
Abstract: This study evaluates the scale-dependent informational efficiency of stock markets using the Financial Chaos Index, a tensor-eigenvalue-based measure of realized volatility. Incorporating Granger causality and network-theoretic analysis across a range of economic, policy, and news-based uncertainty indices, we assess whether public information is efficiently incorporated into asset price fluctuations. Based on a 34-year time period from 1990 to 2023, at the daily frequency, the semi-strong form of the Efficient Market Hypothesis is rejected at the 1\% level of significance, indicating that asset price changes respond predictably to lagged news-based uncertainty. In contrast, at the monthly frequency, such predictive structure largely vanishes, supporting informational efficiency at coarser temporal resolutions. A structural analysis of the Granger causality network reveals that fiscal and monetary policy uncertainties act as core initiators of systemic volatility, while peripheral indices, such as those related to healthcare and consumer prices, serve as latent bridges that become activated under crisis conditions. These findings underscore the role of time-scale decomposition and structural asymmetries in diagnosing market inefficiencies and mapping the propagation of macro-financial uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01543v1</guid>
      <category>q-fin.ST</category>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Ataei</dc:creator>
    </item>
    <item>
      <title>Asset Pricing in Transformer</title>
      <link>https://arxiv.org/abs/2505.01575</link>
      <description>arXiv:2505.01575v1 Announce Type: cross 
Abstract: This paper proposes an innovative Transformer model, Single-directional representative from Transformer (SERT), for US large capital stock pricing. It also innovatively applies the pre-trained Transformer models under the stock pricing and factor investment context. They are compared with standard Transformer models and encoder-only Transformer models in three periods covering the entire COVID-19 pandemic to examine the model adaptivity and suitability during the extreme market fluctuations. Namely, pre-COVID-19 period (mild up-trend), COVID-19 period (sharp up-trend with deep down shock) and 1-year post-COVID-19 (high fluctuation sideways movement). The best proposed SERT model achieves the highest out-of-sample R2, 11.2% and 10.91% respectively, when extreme market fluctuation takes place followed by pre-trained Transformer models (10.38% and 9.15%). Their Trend-following-based strategy wise performance also proves their excellent capability for hedging downside risks during market shocks. The proposed SERT model achieves a Sortino ratio 47% higher than the buy-and-hold benchmark in the equal-weighted portfolio and 28% higher in the value-weighted portfolio when the pandemic period is attended. It proves that Transformer models have a great capability to capture patterns of temporal sparsity data in the asset pricing factor model, especially with considerable volatilities. We also find the softmax signal filter as the common configuration of Transformer models in alternative contexts, which only eliminates differences between models, but does not improve strategy-wise performance, while increasing attention heads improve the model performance insignificantly and applying the 'layer norm first' method do not boost the model performance in our case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01575v1</guid>
      <category>q-fin.CP</category>
      <category>econ.EM</category>
      <category>q-fin.PR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.15327831</arxiv:DOI>
      <dc:creator>Shanyan Lai</dc:creator>
    </item>
    <item>
      <title>Unemployment Dynamics Forecasting with Machine Learning Regression Models</title>
      <link>https://arxiv.org/abs/2505.01933</link>
      <description>arXiv:2505.01933v1 Announce Type: cross 
Abstract: In this paper, I explored how a range of regression and machine learning techniques can be applied to monthly U.S. unemployment data to produce timely forecasts. I compared seven models: Linear Regression, SGDRegressor, Random Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network, training each on a historical span of data and then evaluating on a later hold-out period. Input features include macro indicators (GDP growth, CPI), labor market measures (job openings, initial claims), financial variables (interest rates, equity indices), and consumer sentiment.
  I tuned model hyperparameters via cross-validation and assessed performance with standard error metrics and the ability to predict the correct unemployment direction. Across the board, tree-based ensembles (and CatBoost in particular) deliver noticeably better forecasts than simple linear approaches, while the LSTM captures underlying temporal patterns more effectively than other nonlinear methods. SVR and SGDRegressor yield modest gains over standard regression but don't match the consistency of the ensemble and deep-learning models.
  Interpretability tools ,feature importance rankings and SHAP values, point to job openings and consumer sentiment as the most influential predictors across all methods. By directly comparing linear, ensemble, and deep-learning approaches on the same dataset, our study shows how modern machine-learning techniques can enhance real-time unemployment forecasting, offering economists and policymakers richer insights into labor market trends.
  In the comparative evaluation of the models, I employed a dataset comprising thirty distinct features over the period from January 2020 through December 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01933v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyungsu Kim</dc:creator>
    </item>
    <item>
      <title>Latent Variable Estimation in Bayesian Black-Litterman Models</title>
      <link>https://arxiv.org/abs/2505.02185</link>
      <description>arXiv:2505.02185v1 Announce Type: cross 
Abstract: We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its reliance on subjective investor views. Classical BL requires an investor "view": a forecast vector $q$ and its uncertainty matrix $\Omega$ that describe how much a chosen portfolio should outperform the market. Our key idea is to treat $(q,\Omega)$ as latent variables and learn them from market data within a single Bayesian network. Consequently, the resulting posterior estimation admits closed-form expression, enabling fast inference and stable portfolio weights. Building on these, we propose two mechanisms to capture how features interact with returns: shared-latent parametrization and feature-influenced views; both recover classical BL and Markowitz portfolios as special cases. Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the index baselines. This work turns BL into a fully data-driven, view-free, and coherent Bayesian framework for portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02185v1</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Y. L. Lin, Jerry Yao-Chieh Hu, Paul W. Chiou, Peter Lin</dc:creator>
    </item>
    <item>
      <title>Bounding Treatment Effects by Pooling Limited Information across Observations</title>
      <link>https://arxiv.org/abs/2111.05243</link>
      <description>arXiv:2111.05243v5 Announce Type: replace 
Abstract: We provide novel bounds on average treatment effects (on the treated) that are valid under an unconfoundedness assumption. Our bounds are designed to be robust in challenging situations, for example, when the conditioning variables take on a large number of different values in the observed sample, or when the overlap condition is violated. This robustness is achieved by only using limited "pooling" of information across observations. Namely, the bounds are constructed as sample averages over functions of the observed outcomes such that the contribution of each outcome only depends on the treatment status of a limited number of observations. No information pooling across observations leads to so-called "Manski bounds", while unlimited information pooling leads to standard inverse propensity score weighting. We explore the intermediate range between these two extremes and provide corresponding inference methods. We show in Monte Carlo experiments and through two empirical application that our bounds are indeed robust and informative in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.05243v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sokbae Lee, Martin Weidner</dc:creator>
    </item>
    <item>
      <title>Social Interactions in Endogenous Groups</title>
      <link>https://arxiv.org/abs/2306.01544</link>
      <description>arXiv:2306.01544v2 Announce Type: replace 
Abstract: This paper investigates social interactions in endogenous groups. We specify a two-sided many-to-one matching model, where individuals select groups based on preferences, while groups admit individuals based on qualifications until reaching capacities. Endogenous formation of groups leads to selection bias in peer effect estimation, which is complicated by equilibrium effects and alternative groups. We propose novel methods to simplify selection bias and develop a sieve OLS estimator for peer effects that is \sqrt{n}-consistent and asymptotically normal. Using Chilean data, we find that ignoring selection into high schools leads to overestimated peer influence and distorts the estimation of school effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01544v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyang Sheng, Xiaoting Sun</dc:creator>
    </item>
    <item>
      <title>Dynamic Local Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.01463</link>
      <description>arXiv:2405.01463v3 Announce Type: replace 
Abstract: We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment effect contrasts for the respective complier subpopulations. Under One Sided Noncompliance and sequential extensions of the assumptions in Imbens and Angrist (1994), we show that one can identify Dynamic LATEs that correspond to treating at single time steps. In Staggered Adoption settings, we show that the assumptions are sufficient to identify Dynamic LATEs for treating in multiple time periods. Moreover, this result extends to any setting where the effect of a treatment in one period is uncorrelated with the compliance event in a subsequent period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01463v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ravi B. Sojitra, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>The Dynamic, the Static, and the Weak: Factor models and the analysis of high-dimensional time series</title>
      <link>https://arxiv.org/abs/2407.10653</link>
      <description>arXiv:2407.10653v3 Announce Type: replace 
Abstract: Several fundamental and closely interconnected issues related to factor models are reviewed and discussed: dynamic versus static loadings, rate-strong versus rate-weak factors, the concept of weakly common component recently introduced by Gersing et al. (2023), the irrelevance of cross-sectional ordering and the assumption of cross-sectional exchangeability, the impact of undetected strong factors, and the problem of combining common and idiosyncratic forecasts. Conclusions all point to the advantages of the General Dynamic Factor Model approach of Forni et al. (2000) over the widely used Static Approximate Factor Model introduced by Chamberlain and Rothschild (1983).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10653v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Marc Hallin</dc:creator>
    </item>
    <item>
      <title>Endogeneity Corrections in Binary Outcome Models with Nonlinear Transformations: Identification and Inference</title>
      <link>https://arxiv.org/abs/2408.06977</link>
      <description>arXiv:2408.06977v5 Announce Type: replace 
Abstract: For binary outcome models, an endogeneity correction based on nonlinear rank-based transformations is proposed. Identification without external instruments is achieved under one of two assumptions: either the endogenous regressor is a nonlinear function of one component of the error term, conditional on the exogenous regressors, or the dependence between the endogenous and exogenous regressors is nonlinear. Under these conditions, we prove consistency and asymptotic normality. Monte Carlo simulations and an application on German insolvency data illustrate the usefulness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06977v5</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Mayer, Dominik Wied</dc:creator>
    </item>
    <item>
      <title>On the (Mis)Use of Machine Learning with Panel Data</title>
      <link>https://arxiv.org/abs/2411.09218</link>
      <description>arXiv:2411.09218v2 Announce Type: replace 
Abstract: We provide the first systematic assessment of data leakage issues in the use of machine learning on panel data. Our organizing framework clarifies why neglecting the cross-sectional and longitudinal structure of these data leads to hard-to-detect data leakage, inflated out-of-sample performance, and an inadvertent overestimation of the real-world usefulness and applicability of machine learning models. We then offer empirical guidelines for practitioners to ensure the correct implementation of supervised machine learning in panel data environments. An empirical application, using data from over 3,000 U.S. counties spanning 2000-2019 and focused on income prediction, illustrates the practical relevance of these points across nearly 500 models for both classification and regression tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09218v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Augusto Cerqua, Marco Letta, Gabriele Pinto</dc:creator>
    </item>
    <item>
      <title>Weak instrumental variables due to nonlinearities in panel data: A Super Learner Control Function estimator</title>
      <link>https://arxiv.org/abs/2504.03228</link>
      <description>arXiv:2504.03228v3 Announce Type: replace 
Abstract: A triangular structural panel data model with additive separable individual-specific effects is used to model the causal effect of a covariate on an outcome variable when there are unobservable confounders with some of them time-invariant. In this setup, a linear reduced-form equation might be problematic when the conditional mean of the endogenous covariate and the instrumental variables is nonlinear. The reason is that ignoring the nonlinearity could lead to weak instruments As a solution, we propose a triangular simultaneous equation model for panel data with additive separable individual-specific fixed effects composed of a linear structural equation with a nonlinear reduced form equation. The parameter of interest is the structural parameter of the endogenous variable. The identification of this parameter is obtained under the assumption of available exclusion restrictions and using a control function approach. Estimating the parameter of interest is done using an estimator that we call Super Learner Control Function estimator (SLCFE). The estimation procedure is composed of two main steps and sample splitting. We estimate the control function using a super learner using sample splitting. In the following step, we use the estimated control function to control for endogeneity in the structural equation. Sample splitting is done across the individual dimension. We perform a Monte Carlo simulation to test the performance of the estimators proposed. We conclude that the Super Learner Control Function Estimators significantly outperform Within 2SLS estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03228v3</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monika Avila Marquez</dc:creator>
    </item>
    <item>
      <title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2402.14264</link>
      <description>arXiv:2402.14264v3 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14264v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v2 Announce Type: replace-cross 
Abstract: We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
  </channel>
</rss>
