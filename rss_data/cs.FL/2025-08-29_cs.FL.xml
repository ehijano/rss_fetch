<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:02:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Formal equivalence between global optimization consistency and random search</title>
      <link>https://arxiv.org/abs/2508.20671</link>
      <description>arXiv:2508.20671v1 Announce Type: new 
Abstract: We formalize a proof that any stochastic and iterative global optimization algorithm is consistent over Lipschitz continuous functions if and only if it samples the whole search space. To achieve this, we use the L$\exists$$\forall$N theorem prover and the Mathlib library. The major challenge of this formalization, apart from the technical aspects of the proof itself, is to converge to a definition of a stochastic and iterative global optimization algorithm that is both general enough to encompass all algorithms of this type and specific enough to be used in a formal proof. We define such an algorithm as a pair of an initial probability measure and a sequence of Markov kernels that describe the distribution of the next point sampled by the algorithm given the previous points and their evaluations. We then construct a probability measure on finite and infinite sequences of iterations of the algorithm using the Ionescu-Tulcea theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20671v1</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ga\"etan Serr\'e (ENS Paris Saclay, CB)</dc:creator>
    </item>
    <item>
      <title>Evaluating Massively Parallel Algorithms for DFA Minimisation, Equivalence Checking and Inclusion Checking</title>
      <link>https://arxiv.org/abs/2508.20735</link>
      <description>arXiv:2508.20735v1 Announce Type: new 
Abstract: We study parallel algorithms for the minimisation and equivalence checking of Deterministic Finite Automata (DFAs). Regarding DFA minimisation, we implement four different massively parallel algorithms on Graphics Processing Units~(GPUs). Our results confirm the expectations that the algorithm with the theoretically best time complexity is not practically suitable to run on GPUs due to the large amount of resources needed. We empirically verify that parallel partition refinement algorithms from the literature perform better in practice, even though their time complexity is worse. Furthermore, we introduce a novel algorithm based on partition refinement with an extra parallel partial transitive closure step and show that on specific benchmarks it has better run-time complexity and performs better in practice.
  In addition, we address checking the language equivalence and inclusion of two DFAs. We consider the Hopcroft-Karp algorithm, and explain how a variant of it can be parallelised for GPUs. We note that these problems can be encoded for the GPU-accelerated model checker \GPUexplore, allowing the use its lockless hash table and fine-grained parallel work distribution mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20735v1</guid>
      <category>cs.FL</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Heemstra, Jan Martens, Anton Wijs</dc:creator>
    </item>
    <item>
      <title>Unclustered BWTs of any Length over Non-Binary Alphabets</title>
      <link>https://arxiv.org/abs/2508.20879</link>
      <description>arXiv:2508.20879v1 Announce Type: cross 
Abstract: We prove that for every integer $n &gt; 0$ and for every alphabet $\Sigma_k$ of size $k \geq 3$, there exists a necklace of length $n$ whose Burrows-Wheeler Transform (BWT) is completely unclustered, i.e., it consists of exactly $n$ runs with no two consecutive equal symbols. These words represent the worst-case behavior of the BWT for clustering, since the number of BWT runs is maximized. We also establish a lower bound on their number. This contrasts with the binary case, where the existence of infinitely many completely unclustered BWTs is still an open problem, related to Artin's conjecture on primitive roots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20879v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <category>math.CO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Fici, Est\'eban Gabory, Giuseppe Romana, Marinella Sciortino</dc:creator>
    </item>
    <item>
      <title>QIP $ \subseteq $ AM(2QCFA)</title>
      <link>https://arxiv.org/abs/2508.21020</link>
      <description>arXiv:2508.21020v1 Announce Type: cross 
Abstract: The class of languages having polynomial-time classical or quantum interactive proof systems ($\mathsf{IP}$ or $\mathsf{QIP}$, respectively) is identical to $\mathsf{PSPACE}$. We show that $\mathsf{PSPACE}$ (and so $\mathsf{QIP}$) is subset of $\mathsf{AM(2QCFA)}$, the class of languages having Arthur-Merlin proof systems where the verifiers are two-way finite automata with quantum and classical states (2QCFAs) communicating with the provers classically. Our protocols use only rational-valued quantum transitions and run in double-exponential expected time. Moreover, the member strings are accepted with probability 1 (i.e., perfect-completeness).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21020v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abuzer Yakary{\i}lmaz</dc:creator>
    </item>
    <item>
      <title>Rethinking meaning and ontologies from the perspective of ontological units</title>
      <link>https://arxiv.org/abs/2503.21661</link>
      <description>arXiv:2503.21661v2 Announce Type: replace 
Abstract: Ontologies enable knowledge sharing and interdisciplinary collaboration by providing standardized, structured vocabularies for diverse communities. While logical axioms are a cornerstone of ontology design, natural language elements such as annotations are equally critical for conveying intended meaning and ensuring consistent term usage. This paper explores how meaning is represented in ontologies and how it can be effectively represented and communicated, addressing challenges such as indeterminacy of reference and meaning holism. To this end, instead of following the conventional approach of beginning with existing ontologies and working toward alignment or modularization, this article proposes a reversal of perspective: taking the ontological term as the starting point and introducing a new structure, named 'ontological unit', characterized by: a term-centered design; enhanced characterization of both formal and natural language statements; and an operationalizable definition of communicated meaning based on general assertions. By formalizing the meaning of ontological units, this work seeks to enhance the semantic robustness of terms, improving their clarity and accessibility across domains. Furthermore, it may offer a more effective foundation for ontology generation and significantly improves support for key maintenance tasks such as reuse and versioning. This article aims to establish the theoretical groundwork for the proposed approach and to lay the foundations for future applications in applied ontologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21661v2</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Fabry, Adrien Barton, Jean-Fran\c{c}ois \'Ethier</dc:creator>
    </item>
    <item>
      <title>Power of Counting by Nonuniform Families of Polynomial-Size Finite Automata</title>
      <link>https://arxiv.org/abs/2310.18965</link>
      <description>arXiv:2310.18965v3 Announce Type: replace-cross 
Abstract: Lately, there have been intensive studies on strengths and limitations of nonuniform families of promise decision problems solvable by various types of polynomial-size finite automata families, where "polynomial-size" refers to the polynomially-bounded state complexity of a finite automata family. In this line of study, we further expand the scope of these studies to families of partial counting and gap functions, defined in terms of nonuniform families of polynomial-size nondeterministic finite automata, and their relevant families of promise decision problems. Counting functions have an ability of counting the number of accepting computation paths produced by nondeterministic finite automata. With no unproven hardness assumption, we show numerous separations and collapses of complexity classes of those partial counting and gap function families and their induced promise decision problem families. We also investigate their relationships to pushdown automata families of polynomial stack-state complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18965v3</guid>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoyuki Yamakami</dc:creator>
    </item>
    <item>
      <title>Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks (extended version)</title>
      <link>https://arxiv.org/abs/2508.19430</link>
      <description>arXiv:2508.19430v2 Announce Type: replace-cross 
Abstract: Formal verification is crucial for ensuring the robustness of security protocols against adversarial attacks. The Needham-Schroeder protocol, a foundational authentication mechanism, has been extensively studied, including its integration with Physical Layer Security (PLS) techniques such as watermarking and jamming. Recent research has used ProVerif to verify these mechanisms in terms of secrecy. However, the ProVerif-based approach limits the ability to improve understanding of security beyond verification results. To overcome these limitations, we re-model the same protocol using an Isabelle formalism that generates sound animation, enabling interactive and automated formal verification of security protocols. Our modelling and verification framework is generic and highly configurable, supporting both cryptography and PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy and authenticity in four different eavesdropper locations under both passive and active attacks) using our new web interface. Our findings not only successfully reproduce and reinforce previous results on secrecy but also reveal an uncommon but expected outcome: authenticity is preserved across all examined scenarios, even in cases where secrecy is compromised. We have proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and jamming, and our analysis shows that it is secure for deriving a session key with required authentication. These highlight the advantages of our novel approach, demonstrating its robustness in formally verifying security properties beyond conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19430v2</guid>
      <category>cs.CR</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangfeng Ye, Roberto Metere, Jim Woodcock, Poonam Yadav</dc:creator>
    </item>
  </channel>
</rss>
