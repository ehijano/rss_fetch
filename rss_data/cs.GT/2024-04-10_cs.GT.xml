<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Apr 2024 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Convergence to Nash Equilibrium and No-regret Guarantee in (Markov) Potential Games</title>
      <link>https://arxiv.org/abs/2404.06516</link>
      <description>arXiv:2404.06516v1 Announce Type: new 
Abstract: In this work, we study potential games and Markov potential games under stochastic cost and bandit feedback. We propose a variant of the Frank-Wolfe algorithm with sufficient exploration and recursive gradient estimation, which provably converges to the Nash equilibrium while attaining sublinear regret for each individual player. Our algorithm simultaneously achieves a Nash regret and a regret bound of $O(T^{4/5})$ for potential games, which matches the best available result, without using additional projection steps. Through carefully balancing the reuse of past samples and exploration of new samples, we then extend the results to Markov potential games and improve the best available Nash regret from $O(T^{5/6})$ to $O(T^{4/5})$. Moreover, our algorithm requires no knowledge of the game, such as the distribution mismatch coefficient, which provides more flexibility in its practical implementation. Experimental results corroborate our theoretical findings and underscore the practical effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06516v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Dong, Baoxiang Wang, Yaoliang Yu</dc:creator>
    </item>
    <item>
      <title>Best Response Shaping</title>
      <link>https://arxiv.org/abs/2404.06519</link>
      <description>arXiv:2404.06519v1 Announce Type: new 
Abstract: We investigate the challenge of multi-agent deep reinforcement learning in partially competitive environments, where traditional methods struggle to foster reciprocity-based cooperation. LOLA and POLA agents learn reciprocity-based cooperative policies by differentiation through a few look-ahead optimization steps of their opponent. However, there is a key limitation in these techniques. Because they consider a few optimization steps, a learning opponent that takes many steps to optimize its return may exploit them. In response, we introduce a novel approach, Best Response Shaping (BRS), which differentiates through an opponent approximating the best response, termed the "detective." To condition the detective on the agent's policy for complex games we propose a state-aware differentiable conditioning mechanism, facilitated by a question answering (QA) method that extracts a representation of the agent based on its behaviour on specific environment states. To empirically validate our method, we showcase its enhanced performance against a Monte Carlo Tree Search (MCTS) opponent, which serves as an approximation to the best response in the Coin Game. This work expands the applicability of multi-agent RL in partially competitive environments and provides a new pathway towards achieving improved social welfare in general sum games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06519v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, Shunichi Akatsuka, Aaron Courville</dc:creator>
    </item>
    <item>
      <title>Algorithms and Analysis for Optimizing Robust Objectives in Fair Machine Learning</title>
      <link>https://arxiv.org/abs/2404.06703</link>
      <description>arXiv:2404.06703v1 Announce Type: new 
Abstract: The original position or veil of ignorance argument of John Rawls, perhaps the most famous argument for egalitarianism, states that our concept of fairness, justice, or welfare should be decided from behind a veil of ignorance, and thus must consider everyone impartially (invariant to our identity). This can be posed as a zero-sum game, where a Daemon constructs a world, and an adversarial Angel then places the Daemon into the world. This game incentivizes the Daemon to maximize the minimum utility over all people (i.e., to maximize egalitarian welfare). In some sense, this is the most extreme form of risk aversion or robustness, and we show that by weakening the Angel, milder robust objectives arise, which we argue are effective robust proxies for fair learning or allocation tasks. In particular, the utilitarian, Gini, and power-mean welfare concepts arise from special cases of the adversarial game, which has philosophical implications for the understanding of each of these concepts. We also motivate a new fairness concept that essentially fuses the nonlinearity of the power-mean with the piecewise nature of the Gini class. Then, exploiting the relationship between fairness and robustness, we show that these robust fairness concepts can all be efficiently optimized under mild conditions via standard maximin optimization techniques. Finally, we show that such methods apply in machine learning contexts, and moreover we show generalization bounds for robust fair machine learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06703v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrus Cousins</dc:creator>
    </item>
    <item>
      <title>Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks</title>
      <link>https://arxiv.org/abs/2404.07139</link>
      <description>arXiv:2404.07139v1 Announce Type: cross 
Abstract: Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA). Existing works have only analyzed MIA in a single "what if" interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings. Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members. In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method. We model such interactions by employing a continuous-time stochastic signaling game framework. In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately. First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA. Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system. By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07139v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kavita Kumari, Murtuza Jadliwala, Sumit Kumar Jha, Anindya Maiti</dc:creator>
    </item>
    <item>
      <title>On Incentivizing Social Information Sharing in Routing Games</title>
      <link>https://arxiv.org/abs/2308.13301</link>
      <description>arXiv:2308.13301v4 Announce Type: replace 
Abstract: Crowdsourcing services, such as Waze, leverage a mass of mobile users to learn massive point-of-interest (PoI) information while traveling and share it as a public good. Given that crowdsourced users mind their travel costs and possess various preferences over the PoI information along different paths, we formulate the problem as a novel non-atomic multi-path routing game with positive network externalities among users in social information sharing. In the absence of any incentive design, our price of anarchy (PoA) analysis shows that users' selfish routing on the path with the lowest cost will limit information diversity and lead to $PoA = 0$ with an arbitrarily large efficiency loss from the social optimum. This motivates us to design effective incentive mechanisms to remedy while upholding desirable properties such as individual rationality, incentive compatibility, and budget balance for practical users. Without requiring a specific user's path preference, we present a non-monetary mechanism called Adaptive Information Restriction (AIR) that reduces non-cooperative users' access to the public good as an indirect penalty, which meets all the desirable properties. By meticulously adapting penalty fractions to the actual user flows along different paths, our AIR achieves non-trivial $PoA = \frac{1}{4}$ with low complexity $O(k\log k+\log m)$, where $k$ and $m$ denote the numbers of involved paths and user types, respectively. If the system can further enable pricing for users, we then propose a new monetary mechanism called Adaptive Side-Payment (ASP), which adaptively charges and rewards users according to their chosen paths, respectively. Our ASP mechanism successively achieves a $PoA = \frac{1}{2}$ with even reduced complexity $O(k\log k)$. Finally, our theoretical findings are well corroborated by our experimental results using a real-world public dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13301v4</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songhua Li, Lingjie Duan</dc:creator>
    </item>
    <item>
      <title>Design and Characterization of Strategy-Proof Mechanisms for Two-Facility Game on a Line</title>
      <link>https://arxiv.org/abs/2404.06252</link>
      <description>arXiv:2404.06252v2 Announce Type: replace 
Abstract: We focus on the problem of placing two facilities along a linear space to serve a group of agents. Each agent is committed to minimizing the distance between her location and the closest facility. A mechanism is an algorithm that maps the reported agent locations to the facility locations. We are interested in mechanisms without money that are deterministic, strategy-proof, and provide a bounded approximation ratio for social cost.
  It is a fundamental problem to characterize the family of strategy-proof mechanisms with a bounded approximation ratio. Fotakis and Tzamos already demonstrated that the deterministic strategy-proof mechanisms for the 2-facility game problem are mechanisms with a unique dictator and the leftmost-rightmost mechanism. In this paper, we first present a more refined characterization of the first family.
  We then reveal three new classes of strategy-proof mechanisms that show the intricacy of structure within this family. This helps us get a more complete picture of the characterization of the 2-facility game problem, and may also have value in understanding and solving more general facility allocation game problems.
  Besides, based on our refined characterization, we surprisingly find that prediction cannot effectively improve the performance of the mechanism in the two-facility game problem, while this methodology to overcome bad approximation ratio works in many other mechanism design problems. We show that if we require that the mechanism admits a bounded approximation ratio when the prediction is arbitrarily bad, then at the same time, the mechanism can never achieve sublinear approximation ratios even with perfect prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06252v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinyan Lu, Zihan Luo, Jialin Zhang</dc:creator>
    </item>
    <item>
      <title>Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications</title>
      <link>https://arxiv.org/abs/2307.11732</link>
      <description>arXiv:2307.11732v2 Announce Type: replace-cross 
Abstract: Contemporary real-world online ad auctions differ from canonical models [Edelman et al., 2007; Varian, 2009] in at least four ways: (1) values and click-through rates can depend upon users' search queries, but advertisers can only partially "tune" their bids to specific queries; (2) advertisers do not know the number, identity, and precise value distribution of competing bidders; (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially known to bidders. These features make it virtually impossible to fully characterize equilibrium bidding behavior. This paper shows that, nevertheless, one can still gain useful insight into modern ad auctions by modeling advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. To demonstrate our approach, we first simulate "soft-floor" auctions [Zeithammer, 2019], a complex, real-world pricing rule for which no complete equilibrium characterization is known. We find that (i) when values and click-through rates are query-dependent, soft floors can improve revenues relative to standard auction formats even if bidder types are drawn from the same distribution; and (ii) with distributional asymmetries that reflect relevant real-world scenario, we find that soft floors yield lower revenues than suitably chosen reserve prices, even restricting attention to a single query. We then demonstrate how to infer advertiser value distributions from observed bids for a variety of pricing rules, and illustrate our approach with aggregate data from an e-commerce website.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11732v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Chen, Sareh Nabi, Marciano Siniscalchi</dc:creator>
    </item>
  </channel>
</rss>
