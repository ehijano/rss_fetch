<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Dec 2024 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FAIR Digital Objects for the Realization of Globally Aligned Data Spaces</title>
      <link>https://arxiv.org/abs/2411.18663</link>
      <description>arXiv:2411.18663v1 Announce Type: new 
Abstract: The FAIR principles are globally accepted guidelines for improved data management practices with the potential to align data spaces on a global scale. In practice, this is only marginally achieved through the different ways in which organizations interpret and implement these principles. The concept of FAIR Digital Objects provides a way to realize a domain-independent abstraction layer that could solve this problem, but its specifications are currently diverse, contradictory, and restricted to semantic models. In this work, we introduce a rigorously formalized data model with a set of assertions using formal expressions to provide a common baseline for the implementation of FAIR Digital Objects. The model defines how these objects enable machine-actionable decisions based on the principles of abstraction, encapsulation, and entity relationship to fulfill FAIR criteria for the digital resources they represent. We provide implementation examples in the context of two use cases and explain how our model can facilitate the (re)use of data across domains. We also compare how our model assertions are met by FAIR Digital Objects as they have been described in other projects. Finally, we discuss our results' adoption criteria, limitations, and perspectives in the big data context. Overall, our work represents an important milestone for various communities working towards globally aligned data spaces through FAIRification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18663v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Blumenroehr, Philipp-Joachim Ost, Felix Kraus, Achim Streit</dc:creator>
    </item>
    <item>
      <title>MV4PG: Materialized Views for Property Graphs</title>
      <link>https://arxiv.org/abs/2411.18847</link>
      <description>arXiv:2411.18847v1 Announce Type: new 
Abstract: Graph databases are getting more and more attention in the highly interconnected data domain, and the demand for efficient querying of big data is increasing. We noticed that there are duplicate patterns in graph database queries, and the results of these patterns can be stored as materialized views first, which can speed up the query rate. So we propose materialized views on property graphs, including three parts: view creation, view maintenance, and query optimization using views, and we propose for the first time an efficient templated view maintenance method for containing variable-length edges, which can be applied to multiple graph databases. In order to verify the effect of materialized views, we prototype on TuGraph and experiment on both TuGraph and Neo4j. The experiment results show that our query optimization on read statements is much higher than the additional view maintenance cost brought by write statements. The speedup ratio of the whole workload reaches up to 28.71x, and the speedup ratio of a single query reaches up to nearly 100x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18847v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaijun Xu, Xingdi Wei, Yu Zhang, Kaiwei Li, Xiaowei Zhu, Ke Huang, Tao Wang, Shipeng Qi</dc:creator>
    </item>
    <item>
      <title>A Data Source Discovery Method using Several Domain Ontologies in P2P Environments</title>
      <link>https://arxiv.org/abs/2411.19016</link>
      <description>arXiv:2411.19016v1 Announce Type: new 
Abstract: Several data source discovery methods take into account the semantic heterogeneity problems by using several Domain Ontologies (DOs). However, most of them impose a topology of mapping links between DOs. DOs and mapping links are available on Internet but with an arbitrary topology. In this paper, we propose a data source Discovery method Adapted to any Mapping links Topology (DAMT) and taking into account semantic problems. Peers using the same DO are grouped in a Virtual Organization (VO) and connected in a Distributed Hash Table (DHT). Lookups within a same VO consists in a classical search in a DHT. Regarding the inter-VO discovery process, we propose an addressing system, based on the existing mapping links between DOs, to interconnect VOs. Furthermore, we adopt a lazy maintenance in order to reduce the number of messages required to update the system due to the dynamicity of peers. The performance analysis of the proposed method shows good results for inter-VO lookup queries. Also, it confirms a significant maintenance cost reduction when peers join and leave the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19016v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riad Mokadem (IRIT-PYRAMIDE, IRIT)</dc:creator>
    </item>
    <item>
      <title>Federated Continual Graph Learning</title>
      <link>https://arxiv.org/abs/2411.18919</link>
      <description>arXiv:2411.18919v1 Announce Type: cross 
Abstract: In the era of big data, managing evolving graph data poses substantial challenges due to storage costs and privacy issues. Training graph neural networks (GNNs) on such evolving data usually causes catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they predominantly operate in centralized architectures and overlook the potential of distributed graph databases to harness collective intelligence for enhanced performance optimization. To address these challenges, we present a pioneering study on Federated Continual Graph Learning (FCGL), which adapts GNNs to multiple evolving graphs within decentralized settings while adhering to storage and privacy constraints. Our work begins with a comprehensive empirical analysis of FCGL, assessing its data characteristics, feasibility, and effectiveness, and reveals two principal challenges: local graph forgetting (LGF), where local GNNs forget prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To tackle these, we propose the POWER framework, which mitigates LGF by preserving and replaying experience nodes with maximum local-global coverage at each client and addresses GEC by using a pseudo prototype reconstruction strategy and trajectory-aware knowledge transfer at the central server. Extensive evaluations across multiple graph datasets demonstrate POWER's superior performance over straightforward federated extensions of the centralized CGL algorithms and vision-focused federated continual learning algorithms. Our code is available at https://github.com/zyl24/FCGL_POWER.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18919v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.SI</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinlin Zhu, Xunkai Li, Miao Hu, Di Wu</dc:creator>
    </item>
    <item>
      <title>Scalable Order-Preserving Pattern Mining</title>
      <link>https://arxiv.org/abs/2411.19511</link>
      <description>arXiv:2411.19511v1 Announce Type: cross 
Abstract: Time series are ubiquitous in domains ranging from medicine to marketing and finance. Frequent Pattern Mining (FPM) from a time series has thus received much attention. Recently, it has been studied under the order-preserving (OP) matching relation stating that a match occurs when two time series have the same relative order on their elements. Here, we propose exact, highly scalable algorithms for FPM in the OP setting. Our algorithms employ an OP suffix tree (OPST) as an index to store and query time series efficiently. Unfortunately, there are no practical algorithms for OPST construction. Thus, we first propose a novel and practical $\mathcal{O}(n\sigma\log \sigma)$-time and $\mathcal{O}(n)$-space algorithm for constructing the OPST of a length-$n$ time series over an alphabet of size $\sigma$. We also propose an alternative faster OPST construction algorithm running in $\mathcal{O}(n\log \sigma)$ time using $\mathcal{O}(n)$ space; this algorithm is mainly of theoretical interest. Then, we propose an exact $\mathcal{O}(n)$-time and $\mathcal{O}(n)$-space algorithm for mining all maximal frequent OP patterns, given an OPST. This significantly improves on the state of the art, which takes $\Omega(n^3)$ time in the worst case. We also formalize the notion of closed frequent OP patterns and propose an exact $\mathcal{O}(n)$-time and $\mathcal{O}(n)$-space algorithm for mining all closed frequent OP patterns, given an OPST. We conducted experiments using real-world, multi-million letter time series showing that our $\mathcal{O}(n\sigma \log \sigma)$-time OPST construction algorithm runs in $\mathcal{O}(n)$ time on these datasets despite the $\mathcal{O}(n\sigma \log \sigma)$ bound; that our frequent pattern mining algorithms are up to orders of magnitude faster than the state of the art and natural Apriori-like baselines; and that OP pattern-based clustering is effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19511v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ling Li, Wiktor Zuba, Grigorios Loukides, Solon P. Pissis, Maria Matsangidou</dc:creator>
    </item>
    <item>
      <title>Work-Efficient Query Evaluation with PRAMs</title>
      <link>https://arxiv.org/abs/2301.08178</link>
      <description>arXiv:2301.08178v2 Announce Type: replace 
Abstract: The article studies query evaluation in parallel constant time in the CRCW PRAM model. While it is well-known that all relational algebra queries can be evaluated in constant time on an appropriate CRCW PRAM model, this article is interested in the efficiency of evaluation algorithms, that is, in the number of processors or, asymptotically equivalent, in the work. Naive evaluation in the parallel setting results in huge (polynomial) bounds on the work of such algorithms and in presentations of the result sets that can be extremely scattered in memory. The article discusses some obstacles for constant-time PRAM query evaluation. It presents algorithms for relational operators and explores three settings, in which efficient sequential query evaluation algorithms exist: acyclic queries, semijoin algebra queries, and join queries -- the latter in the worst-case optimal framework. Under mild assumptions -- that data values are numbers of polynomial size in the size of the database or that the relations of the database are suitably sorted -- constant-time algorithms are presented that are weakly work-efficient in the sense that work $\mathcal{O}(T^{1+\varepsilon})$ can be achieved, for every $\varepsilon&gt;0$, compared to the time $T$ of an optimal sequential algorithm. Important tools are the algorithms for approximate prefix sums and compaction from Goldberg and Zwick (1995).</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08178v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jens Keppeler, Thomas Schwentick, Christopher Spinrath</dc:creator>
    </item>
    <item>
      <title>Blend: A Unified Data Discovery System</title>
      <link>https://arxiv.org/abs/2310.02656</link>
      <description>arXiv:2310.02656v2 Announce Type: replace 
Abstract: Most research on data discovery has so far focused on improving individual discovery operators such as join, correlation, or union discovery. However, in practice, a combination of these techniques and their corresponding indexes may be necessary to support arbitrary discovery tasks. We propose BLEND, a comprehensive data discovery system that supports existing operators and enables their flexible pipelining. BLEND is based on a set of lower-level operators that serve as fundamental building blocks for more complex and sophisticated user tasks. To reduce the execution runtime of discovery pipelines, we propose a unified index structure and a rule-based optimizer that rewrites SQL statements into low-level operators when possible. We show the superior flexibility and efficiency of our system compared to ad-hoc discovery pipelines and stand-alone solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02656v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Esmailoghli, Christoph Schnell, Ren\'ee J. Miller, Ziawasch Abedjan</dc:creator>
    </item>
    <item>
      <title>GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2311.03157</link>
      <description>arXiv:2311.03157v2 Announce Type: replace 
Abstract: Modern database management systems (DBMS) expose hundreds of configurable knobs to control system behaviours. Determining the appropriate values for these knobs to improve DBMS performance is a long-standing problem in the database community. As there is an increasing number of knobs to tune and each knob could be in continuous or categorical values, manual tuning becomes impractical. Recently, automatic tuning systems using machine learning methods have shown great potentials. However, existing approaches still incur significant tuning costs or only yield sub-optimal performance. This is because they either ignore the extensive domain knowledge available (e.g., DBMS manuals and forum discussions) and only rely on the runtime feedback of benchmark evaluations to guide the optimization, or they utilize the domain knowledge in a limited way. Hence, we propose GPTuner, a manual-reading database tuning system. Firstly, we develop a Large Language Model (LLM)-based pipeline to collect and refine heterogeneous knowledge, and propose a prompt ensemble algorithm to unify a structured view of the refined knowledge. Secondly, using the structured knowledge, we (1) design a workload-aware and training-free knob selection strategy, (2) develop a search space optimization technique considering the value range of each knob, and (3) propose a Coarse-to-Fine Bayesian Optimization Framework to explore the optimized space. Finally, we evaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics (throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to the state-of-the-art approaches, GPTuner identifies better configurations in 16x less time on average. Moreover, GPTuner achieves up to 30% performance improvement (higher throughput or lower latency) over the best-performing alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03157v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.14778/3659437.3659449</arxiv:DOI>
      <dc:creator>Jiale Lao, Yibo Wang, Yufei Li, Jianping Wang, Yunjia Zhang, Zhiyuan Cheng, Wanghu Chen, Mingjie Tang, Jianguo Wang</dc:creator>
    </item>
    <item>
      <title>Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries</title>
      <link>https://arxiv.org/abs/2402.08349</link>
      <description>arXiv:2402.08349v3 Announce Type: replace 
Abstract: Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA World Cup 2022, during which about 6K natural language questions were asked and executed. All of our data is based on real user questions that were asked live to the system. We manually labeled and translated a subset of these questions for three different data models. For each data model, we explore the performance of representative Text-to-SQL systems and language models. We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time. Our comprehensive evaluation sheds light on the design choices of real-world Text-to-SQL systems and their impact on moving from research prototypes to real deployments. Last, we provide a new benchmark dataset to the community, which is the first to enable the evaluation of different data models for the same dataset and is substantially more challenging than most previous datasets in terms of query complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08349v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan F\"urst, Catherine Kosten, Farhad Nooralahzadeh, Yi Zhang, Kurt Stockinger</dc:creator>
    </item>
    <item>
      <title>Online Marketplace: A Benchmark for Data Management in Microservices</title>
      <link>https://arxiv.org/abs/2403.12605</link>
      <description>arXiv:2403.12605v3 Announce Type: replace 
Abstract: Microservice architectures have become a popular approach for designing scalable distributed applications. Despite their extensive use in industrial settings for over a decade, there is limited understanding of the data management challenges that arise in these applications. Consequently, it has been difficult to advance data system technologies that effectively support microservice applications. To fill this gap, we present Online Marketplace, a microservice benchmark that highlights core data management challenges that existing benchmarks fail to address. These challenges include transaction processing, query processing, event processing, constraint enforcement, and data replication. We have defined criteria for various data management issues to enable proper comparison across data systems and platforms.
  Through case studies with state-of-the-art data platforms, we discuss the issues encountered while implementing and meeting Online Marketplace's criteria. By capturing the overhead of meeting the key data management requirements that are overlooked by existing benchmarks, we gain actionable insights into the experimental platforms. This highlights the significance of Online Marketplace in advancing future data systems to meet the needs of microservice practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12605v3</guid>
      <category>cs.DB</category>
      <category>cs.SE</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rodrigo Laigner, Zhexiang Zhang, Yijian Liu, Leonardo Freitas Gomes, Yongluan Zhou</dc:creator>
    </item>
    <item>
      <title>Controlling Dataflows with a Bolt-on Data Escrow</title>
      <link>https://arxiv.org/abs/2408.01580</link>
      <description>arXiv:2408.01580v2 Announce Type: replace 
Abstract: In today's data-driven economy, individuals share their data with platforms in exchange for services such as search, social networks, and health recommendations, platforms use the data to provide those services and create other revenue-generating opportunities, e.g., selling the data to data brokers, all of which generate tremendous value. With the ever-expanding data economy comes the growing concern about potential data misuse. While most platforms give individuals specific control over their data (i.e., what data is being shared), individuals cannot limit the purposes of sharing their data since they cannot control how their data is used once it is shared.
  In this paper, we introduce a data management solution to this socio-technical problem. We present a data escrow design that permits individuals to observe all dataflows -- not just what data is shared but also for what purpose it will be used. Rather than having individuals' data flowing to the platform, the platform delegates their computation to the escrow, where individuals can observe and manage their data. We propose a minimally invasive programming interface to enable the escrow's delegated computation model; developers specify dataflows via the interface and the escrow runs the computation based on developers' specifications. In addition to proposing the escrow design, which is general and applies to different ecosystems such as web browsers, wearables, and mobile platforms, we also contribute a concrete escrow implementation in the Apple ecosystem. In our evaluation, we analyze the dataflows in real-world applications and show that the escrow's programming interface supports implementing a wide range of dataflows, and thus applications. We show that our escrow-based solution is a feasible and practical alternative to today's data governance and has minimum overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01580v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiru Zhu, Raul Castro Fernandez</dc:creator>
    </item>
    <item>
      <title>On Enforcing Satisfiable, Coherent, and Minimal Sets of Dyadic Relation Constraints in MatBase</title>
      <link>https://arxiv.org/abs/2410.23485</link>
      <description>arXiv:2410.23485v2 Announce Type: replace 
Abstract: This paper rigorously and concisely defines, in the context of our (Elementary) Mathematical Data Model ((E)MDM), the mathematical concepts of dyadic relation, reflexivity, irreflexivity, symmetry, asymmetry, transitivity, intransitivity, Euclideanity, inEuclideanity, equivalence, acyclicity, connectivity, the properties that relate them, and the corresponding corollaries on the coherence and minimality of sets made of such dyadic relation properties viewed as database constraints. Its main contribution is the pseudocode algorithm used by MatBase, our intelligent database management system prototype based on both (E)MDM, the relational, and the entity-relationship data models, for enforcing dyadic relation constraint sets. We proved that this algorithm guarantees the satisfiability, coherence, and minimality of such sets, while being very fast, solid, complete, and minimal. In the sequel, we also presented the relevant MatBase user interface as well as the tables of its metacatalog used by this algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23485v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.56831/PSEN-05-166</arxiv:DOI>
      <arxiv:journal_reference>Primera Scientific Engineering Journal 5(6):02-14, 2024</arxiv:journal_reference>
      <dc:creator>Christian Mancas</dc:creator>
    </item>
    <item>
      <title>Range (R\'enyi) Entropy Queries and Partitioning</title>
      <link>https://arxiv.org/abs/2312.15959</link>
      <description>arXiv:2312.15959v2 Announce Type: replace-cross 
Abstract: Data partitioning that maximizes/minimizes the Shannon entropy, or more generally the R\'enyi entropy is a crucial subroutine in data compression, columnar storage, and cardinality estimation algorithms. These partition algorithms can be accelerated if we have a data structure to compute the entropy in different subsets of data when the algorithm needs to decide what block to construct. Such a data structure will also be useful for data analysts exploring different subsets of data to identify areas of interest. While it is generally known how to compute the Shannon or the R\'enyi entropy of a discrete distribution in the offline or streaming setting efficiently, we focus on the query setting where we aim to efficiently derive the entropy among a subset of data that satisfy some linear predicates. We solve this problem in a typical setting when we deal with real data, where data items are geometric points and each requested area is a query (hyper)rectangle. More specifically, we consider a set $P$ of $n$ weighted and colored points in $\mathbb{R}^d$. For the range S-entropy (resp. R-entropy) query problem, the goal is to construct a low space data structure, such that given a query (hyper)rectangle $R$, it computes the Shannon (resp. R\'enyi) entropy based on the colors and the weights of the points in $P\cap R$, in sublinear time. We show conditional lower bounds proving that we cannot hope for data structures with near-linear space and near-constant query time for both the range S-entropy and R-entropy query problems. Then, we propose exact data structures for $d=1$ and $d&gt;1$ with $o(n^{2d})$ space and $o(n)$ query time for both problems. Finally, we propose near linear space data structures for returning either an additive or a multiplicative approximation of the Shannon (resp. R\'enyi) entropy in $P\cap R$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15959v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryan Esmailpour, Sanjay Krishnan, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>Towards Robust Federated Analytics via Differentially Private Measurements of Statistical Heterogeneity</title>
      <link>https://arxiv.org/abs/2411.04579</link>
      <description>arXiv:2411.04579v2 Announce Type: replace-cross 
Abstract: Statistical heterogeneity is a measure of how skewed the samples of a dataset are. It is a common problem in the study of differential privacy that the usage of a statistically heterogeneous dataset results in a significant loss of accuracy. In federated scenarios, statistical heterogeneity is more likely to happen, and so the above problem is even more pressing. We explore the three most promising ways to measure statistical heterogeneity and give formulae for their accuracy, while simultaneously incorporating differential privacy. We find the optimum privacy parameters via an analytic mechanism, which incorporates root finding methods. We validate the main theorems and related hypotheses experimentally, and test the robustness of the analytic mechanism to different heterogeneity levels. The analytic mechanism in a distributed setting delivers superior accuracy to all combinations involving the classic mechanism and/or the centralized setting. All measures of statistical heterogeneity do not lose significant accuracy when a heterogeneous sample is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04579v2</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mary Scott, Graham Cormode, Carsten Maple</dc:creator>
    </item>
    <item>
      <title>Distributed, communication-efficient, and differentially private estimation of KL divergence</title>
      <link>https://arxiv.org/abs/2411.16478</link>
      <description>arXiv:2411.16478v2 Announce Type: replace-cross 
Abstract: A key task in managing distributed, sensitive data is to measure the extent to which a distribution changes. Understanding this drift can effectively support a variety of federated learning and analytics tasks. However, in many practical settings sharing such information can be undesirable (e.g., for privacy concerns) or infeasible (e.g., for high communication costs). In this work, we describe novel algorithmic approaches for estimating the KL divergence of data across federated models of computation, under differential privacy. We analyze their theoretical properties and present an empirical study of their performance. We explore parameter settings that optimize the accuracy of the algorithm catering to each of the settings; these provide sub-variations that are applicable to real-world tasks, addressing different context- and application-specific trust level requirements. Our experimental results confirm that our private estimators achieve accuracy comparable to a baseline algorithm without differential privacy guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16478v2</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mary Scott, Sayan Biswas, Graham Cormode, Carsten Maple</dc:creator>
    </item>
  </channel>
</rss>
