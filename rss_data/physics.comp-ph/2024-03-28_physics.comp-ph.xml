<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.comp-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.comp-ph</link>
    <description>physics.comp-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.comp-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Mar 2024 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive Loss Weighting for Machine Learning Interatomic Potentials</title>
      <link>https://arxiv.org/abs/2403.18122</link>
      <description>arXiv:2403.18122v1 Announce Type: new 
Abstract: Training machine learning interatomic potentials often requires optimizing a loss function composed of three variables: potential energies, forces, and stress. The contribution of each variable to the total loss is typically weighted using fixed coefficients. Identifying these coefficients usually relies on iterative or heuristic methods, which may yield sub-optimal
  results. To address this issue, we propose an adaptive loss weighting algorithm that automatically adjusts the loss weights of these variables during the training of potentials, dynamically adapting to the characteristics of the training dataset. The comparative analysis of models trained with fixed and adaptive loss weights demonstrates that the adaptive method not only achieves a more balanced predictions across the three variables but also improves overall prediction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18122v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Ocampoa, Daniela Possob, Reza Namakiana, Wei Gao</dc:creator>
    </item>
    <item>
      <title>Bayesian electron density determination from sparse and noisy single-molecule X-ray scattering images</title>
      <link>https://arxiv.org/abs/2403.18391</link>
      <description>arXiv:2403.18391v1 Announce Type: new 
Abstract: Single molecule X-ray scattering experiments using free electron lasers hold the potential to resolve both single structures and structural ensembles of biomolecules. However, molecular electron density determination has so far not been achieved due to low photon counts, high noise levels and low hit rates. Most analysis approaches therefore focus on large specimen like entire viruses, which scatter substantially more photons per image, such that it becomes possible to determine the molecular orientation for each image. In contrast, for small specimen like proteins, the molecular orientation cannot be determined for each image, and must be considered random and unknown.
  Here we developed and tested a rigorous Bayesian approach to overcome these limitations, and also taking into account intensity fluctuations, beam polarization, irregular detector shapes, incoherent scattering and background scattering. We demonstrate using synthetic scattering images that it is possible to determine electron densities of small proteins in this extreme high noise Poisson regime. Tests on published experimental data from the coliphage PR772 achieved the detector-limited resolution of $9\,\mathrm{nm}$, using only $0.01\,\%$ of the available photons per image.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18391v1</guid>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steffen Schultze, Helmut Grubm\"uller</dc:creator>
    </item>
    <item>
      <title>Modifying Gibbs sampling to avoid self transitions</title>
      <link>https://arxiv.org/abs/2403.18054</link>
      <description>arXiv:2403.18054v1 Announce Type: cross 
Abstract: Gibbs sampling repeatedly samples from the conditional distribution of one variable, x_i, given other variables, either choosing i randomly, or updating sequentially using some systematic or random order. When x_i is discrete, a Gibbs sampling update may choose a new value that is the same as the old value. A theorem of Peskun indicates that, when i is chosen randomly, a reversible method that reduces the probability of such self transitions, while increasing the probabilities of transitioning to each of the other values, will decrease the asymptotic variance of estimates. This has inspired two modified Gibbs sampling methods, originally due to Frigessi, et al and to Liu, though these do not always reduce self transitions to the minimum possible. Methods that do reduce the probability of self transitions to the minimum, but do not satisfy the conditions of Peskun's theorem, have also been devised, by Suwa and Todo. I review past methods, and introduce a broader class of reversible methods, based on what I call "antithetic modification", which also reduce asymptotic variance compared to Gibbs sampling, even when not satisfying the conditions of Peskun's theorem. A modification of one method in this class reduces self transitions to the minimum possible, while still always reducing asymptotic variance compared to Gibbs sampling. I introduce another new class of non-reversible methods based on slice sampling that can also minimize self transition probabilities. I provide explicit, efficient implementations of all these methods, and compare their performance in simulations of a 2D Potts model, a Bayesian mixture model, and a belief network with unobserved variables. The non-reversibility produced by sequential updating can be beneficial, but no consistent benefit is seen from the individual updates being done by a non-reversible method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18054v1</guid>
      <category>stat.CO</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Radford M. Neal</dc:creator>
    </item>
    <item>
      <title>Kinetic data-driven approach to turbulence subgrid modeling</title>
      <link>https://arxiv.org/abs/2403.18466</link>
      <description>arXiv:2403.18466v1 Announce Type: cross 
Abstract: Numerical simulations of turbulent flows are well known to pose extreme computational challenges due to the huge number of dynamical degrees of freedom required to correctly describe the complex multi-scale statistical correlations of the velocity. On the other hand, kinetic mesoscale approaches based on the Boltzmann equation, have the potential to describe a broad range of flows, stretching well beyond the special case of gases close to equilibrium, which results in the ordinary Navier-Stokes dynamics. Here we demonstrate that, by properly tuning, a kinetic approach can statistically reproduce the quantitative dynamics of the larger scales in turbulence, thereby providing an alternative, computationally efficient and physically rooted approach towards subgrid scale (SGS) modeling in turbulence. More specifically we show that by leveraging on data from fully resolved Direct Numerical Simulation (DNS) data we can learn a collision operator for the discretized Boltzmann equation solver (the lattice Boltzmann method), which effectively implies a turbulence subgrid closure model. The mesoscopic nature of our formulation makes the learning problem fully local in both space and time, leading to reduced computational costs and enhanced generalization capabilities. We show that the model offers superior performance compared to traditional methods, such as the Smagorinsky model, being less dissipative and, therefore, being able to more closely capture the intermittency of higher-order velocity correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18466v1</guid>
      <category>physics.flu-dyn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giulio Ortali, Alessandro Gabbana, Nicola Demo, Gianluigi Rozza, Federico Toschi</dc:creator>
    </item>
    <item>
      <title>Shotgun crystal structure prediction using machine-learned formation energies</title>
      <link>https://arxiv.org/abs/2305.02158</link>
      <description>arXiv:2305.02158v4 Announce Type: replace 
Abstract: Stable or metastable crystal structures of assembled atoms can be predicted by finding the global or local minima of the energy surface defined on the space of the atomic configurations. Generally, this requires repeated first-principles energy calculations that are impractical for large systems, such as those containing more than 30 atoms in the unit cell. Here, we have made significant progress in solving the crystal structure prediction problem with a simple but powerful machine-learning workflow; using a machine-learning surrogate for first-principles energy calculations, we performed non-iterative, single-shot screening using a large library of virtually created crystal structures. The present method relies on two key technical components: transfer learning, which enables a highly accurate energy prediction of pre-relaxed crystalline states given only a small set of training samples from first-principles calculations, and generative models to create promising and diverse crystal structures for screening. Here, first-principles calculations were performed only to generate the training samples, and for the optimization of a dozen or fewer finally narrowed-down crystal structures. Our shotgun method proved to be computationally less demanding compared to conventional methods, which heavily rely on iterations of first-principles calculations, and achieved an exceptional prediction accuracy, reaching 92.2% in a benchmark task involving the prediction of 90 different crystal structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02158v4</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Liu (The Institute of Statistical Mathematics), Hiromasa Tamaki (Panasonic Holdings Corporation), Tomoyasu Yokoyama (Panasonic Holdings Corporation), Kensuke Wakasugi (Panasonic Holdings Corporation), Satoshi Yotsuhashi (Panasonic Holdings Corporation), Minoru Kusaba (The Institute of Statistical Mathematics), Ryo Yoshida (The Institute of Statistical Mathematics, The Graduate University for Advanced Studies)</dc:creator>
    </item>
    <item>
      <title>Automated atomistic simulations of dissociated dislocations with ab initio accuracy</title>
      <link>https://arxiv.org/abs/2311.01830</link>
      <description>arXiv:2311.01830v2 Announce Type: replace 
Abstract: In (M Hodapp and A Shapeev 2020 Mach. Learn.: Sci. Technol. 1 045005), we have proposed an algorithm that fully automatically trains machine-learning interatomic potentials (MLIPs) during large-scale simulations, and successfully applied it to simulate screw dislocation motion in body-centered cubic tungsten. The algorithm identifies local subregions of the large-scale simulation region where the potential extrapolates, and then constructs periodic configurations of 100--200 atoms out of these non-periodic subregions that can be efficiently computed with plane-wave Density Functional Theory (DFT) codes.
  In this work, we extend this algorithm to dissociated dislocations with arbitrary character angles and apply it to partial dislocations in face-centered cubic aluminum. Given the excellent agreement with available DFT reference results, we argue that our algorithm has the potential to become a universal way of simulating dissociated dislocations in face-centered cubic and possibly also other materials, such as hexagonal closed-packed magnesium, and their alloys. Moreover, it can be used to construct reliable training sets for MLIPs to be used in large-scale simulations of curved dislocations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01830v2</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.109.094120</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 109, 094120 (2024)</arxiv:journal_reference>
      <dc:creator>Laura Mismetti, Max Hodapp</dc:creator>
    </item>
    <item>
      <title>Neural Downscaling for Complex Systems: from Large-scale to Small-scale by Neural Operator</title>
      <link>https://arxiv.org/abs/2403.13016</link>
      <description>arXiv:2403.13016v2 Announce Type: replace 
Abstract: Predicting and understanding the chaotic dynamics in complex systems is essential in various applications. However, conventional approaches, whether full-scale simulations or small-scale omissions, fail to offer a comprehensive solution. This instigates exploration into whether modeling or omitting small-scale dynamics could benefit from the well-captured large-scale dynamics. In this paper, we introduce a novel methodology called Neural Downscaling (ND), which integrates neural operator techniques with the principles of inertial manifold and nonlinear Galerkin theory. ND effectively infers small-scale dynamics within a complementary subspace from corresponding large-scale dynamics well-represented in a low-dimensional space. The effectiveness and generalization of the method are demonstrated on the complex systems governed by the Kuramoto-Sivashinsky and Navier-Stokes equations. As the first comprehensive deterministic model targeting small-scale dynamics, ND sheds light on the intricate spatiotemporal nonlinear dynamics of complex systems, revealing how small-scale dynamics are intricately linked with and influenced by large-scale dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13016v2</guid>
      <category>physics.comp-ph</category>
      <category>nlin.AO</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengyu Lai, Jing Wang, Rui Wang, Dewu Yang, Haoqi Fei, Hui Xu</dc:creator>
    </item>
    <item>
      <title>Universality conditions of unified classical and quantum reservoir computing</title>
      <link>https://arxiv.org/abs/2401.15067</link>
      <description>arXiv:2401.15067v2 Announce Type: replace-cross 
Abstract: Reservoir computing is a versatile paradigm in computational neuroscience and machine learning, that exploits the non-linear dynamics of a dynamical system - the reservoir - to efficiently process time-dependent information. Since its introduction, it has exhibited remarkable capabilities in various applications. As widely known, classes of reservoir computers serve as universal approximators of functionals with fading memory. The construction of such universal classes often appears context-specific, but in fact, they follow the same principles. Here we present a unified theoretical framework and we propose a ready-made setting to secure universality. We test the result in the arising context of quantum reservoir computing. Guided by such a unified theorem we suggest why spatial multiplexing may serve as a computational resource when dealing with quantum registers, as empirically observed in specific implementations on quantum hardware. The analysis sheds light on a unified view of classical and quantum reservoir computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15067v2</guid>
      <category>quant-ph</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Monzani, Enrico Prati</dc:creator>
    </item>
  </channel>
</rss>
