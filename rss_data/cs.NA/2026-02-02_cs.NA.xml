<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Feb 2026 04:30:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Large Language Models: A Mathematical Formulation</title>
      <link>https://arxiv.org/abs/2601.22170</link>
      <description>arXiv:2601.22170v1 Announce Type: new 
Abstract: Large language models (LLMs) process and predict sequences containing text to answer questions, and address tasks including document summarization, providing recommendations, writing software and solving quantitative problems. We provide a mathematical framework for LLMs by describing the encoding of text sequences into sequences of tokens, defining the architecture for next-token prediction models, explaining how these models are learned from data, and demonstrating how they are deployed to address a variety of tasks. The mathematical sophistication required to understand this material is not high, and relies on straightforward ideas from information theory, probability and optimization. Nonetheless, the combination of ideas resting on these different components from the mathematical sciences yields a complex algorithmic structure; and this algorithmic structure has demonstrated remarkable empirical successes. The mathematical framework established here provides a platform from which it is possible to formulate and address questions concerning the accuracy, efficiency and robustness of the algorithms that constitute LLMs. The framework also suggests directions for development of modified and new methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22170v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Andrew Stuart, Son Tran</dc:creator>
    </item>
    <item>
      <title>On the $L^p$-Convergence and Denoising Performance of Durrmeyer-Type Max-Min Neural Network Operators</title>
      <link>https://arxiv.org/abs/2601.22174</link>
      <description>arXiv:2601.22174v1 Announce Type: new 
Abstract: In this paper, we investigate Durrmeyer-type generalizations of maximum-minimum neural network operators. The primary objective of this study is to establish the convergence of these operators in the $L^{p}$ norm for functions $f\in L^{p}([a,b],[0,1])$ with $1\leq p&lt;\infty$. To this end, we analyze the properties of sigmoidal functions and maximum-minimum operations, subsequently establishing the convergence of the proposed operator in pointwise, supremum, and $L^{p}$ norms. Furthermore, we derive quantitative estimates for the rates of convergence. In the applications section, numerical and graphical examples demonstrate that the proposed Durrmeyer-type operators provide smoother approximations compared to Kantorovich-type and standard max-min operators. Finally, we highlight the superior filtering performance of these operators in signal analysis, validating their effectiveness in both approximation and data processing tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22174v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Berke \c{S}ahin, \.Ismail Aslan</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the Discrete Constrained Saddle Dynamics and Their Momentum Variants</title>
      <link>https://arxiv.org/abs/2601.22341</link>
      <description>arXiv:2601.22341v1 Announce Type: new 
Abstract: We study the discrete constrained saddle dynamics and their momentum variants for locating saddle points on manifolds. Under the assumption of exact unstable eigenvectors, we establish a local linear convergence of the discrete constrained saddle dynamics and show that the convergence rate depends on the condition number of the Riemannian Hessian. To mitigate this dependence, we introduce a momentum-based constrained saddle dynamics and prove local convergence of the continuous-time dynamics as well as the corresponding discrete scheme, which further demonstrates that momentum accelerates convergence, particularly in ill-conditioned settings. In addition, we show that a single-step eigenvector update is sufficient to guarantee local convergence; thus, the assumption of exact unstable eigenvectors is not necessary, which substantially reduces the computational cost. Finally, numerical experiments, including applications to the Thomson problem, the Rayleigh quotient on the Stiefel manifold, and the energy functional of Bose-Einstein condensates, are presented to complement the theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22341v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Du, Baoming Shi</dc:creator>
    </item>
    <item>
      <title>Low-Rank Approximation by Randomly Pivoted LU</title>
      <link>https://arxiv.org/abs/2601.22344</link>
      <description>arXiv:2601.22344v1 Announce Type: new 
Abstract: The low-rank approximation properties of Randomly Pivoted LU (RPLU), a variant of Gaussian elimination where pivots are sampled proportional to the squared entries of the Schur complement, are analyzed. It is shown that the RPLU iterates converge geometrically in expectation for matrices with rapidly decaying singular values. RPLU outperforms existing low-rank approximation algorithms in two settings: first, when memory is limited, RPLU can be implemented with $\mathcal{O}(k^2 + m + n)$ storage and $\mathcal{O}( k(m + n)+ k\mathcal{M}(\mat{A}) + k^3)$ operations, where $\mathcal{M}(\mat{A})$ is the cost of a matvec with $\mat{A}\in\mathbb{C}^{n\times m}$ or its adjoint, for a rank-$k$ approximation. Second, when the matrix and its Schur complements share exploitable structure, such as for Cauchy-like matrices. The efficacy of RPLU is illustrated with several examples, including applications in rational approximation and solving large linear systems on GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22344v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Aur\`ele Gilles, Heather Wilber</dc:creator>
    </item>
    <item>
      <title>Forward-KL Convergence of Time-Inhomogeneous Langevin Diffusions</title>
      <link>https://arxiv.org/abs/2601.22349</link>
      <description>arXiv:2601.22349v1 Announce Type: new 
Abstract: Many practical samplers rely on time-dependent drifts -- often induced by annealing or tempering schedules -- to improve exploration and stability. This motivates a unified non-asymptotic analysis of the corresponding Langevin diffusions and their discretizations. We provide a convergence analysis that includes non-asymptotic bounds for the continuous-time diffusion and its Euler--Maruyama discretization in the forward-Kullback--Leibler divergence under a single set of abstract conditions on the time-dependent drift. The results apply to many practically-relevant annealing schemes, including geometric tempering and annealed Langevin sampling. In addition, we provide numerical experiments comparing the annealing schemes covered by our theory in low- and high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22349v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Habring, Martin Zach</dc:creator>
    </item>
    <item>
      <title>Inverse acoustic scattering for random obstacles with multi-frequency data</title>
      <link>https://arxiv.org/abs/2601.22560</link>
      <description>arXiv:2601.22560v1 Announce Type: new 
Abstract: We study an inverse random obstacle scattering problems in $\mathbb{R}^2$ where the scatterer is formulated by a Gaussian process defined on the angular parameter domain. Equipped with a modified covariance function which is mathematically well-defined and physically consistent, the Gaussian process admits a parameterization via Karhunen--Lo\`eve (KL) expansion. Based on observed multi-frequency data, we develop a two-stage inversion method: the first stage reconstructs the baseline shape of the random scatterer and the second stage estimates the statistical characteristics of the boundary fluctuations, including KL eigenvalues and covariance hyperparameters. We further provide theoretical justifications for the modeling and inversion pipeline, covering well-definedness of the Gaussian-process model, convergence for the two-stage procedure and a brief discussion on uniqueness. Numerical experiments demonstrate stable recovery of both geometric and statistical information for obstacles with simple and more complex shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22560v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqi Sun, Xiang Xu, Yiwen Lin</dc:creator>
    </item>
    <item>
      <title>An ultra-weak three-field finite element formulation for the biharmonic and extended Fisher--Kolmogorov equations</title>
      <link>https://arxiv.org/abs/2601.22587</link>
      <description>arXiv:2601.22587v1 Announce Type: new 
Abstract: This paper discusses a so-called ultra-weak three-field formulation of the biharmonic problem where the solution, its gradient, and an additional Lagrange multiplier are the three unknowns. We establish the well-posedness of the problem using the abstract theory for saddle-point problems, and develop a conforming finite element scheme based on Raviart--Thomas discretisations of the two auxiliary variables. The well-posedness of the discrete formulation and the corresponding a priori error estimate are proved using a discrete inf-sup condition. We further extend the analysis to the time-dependent semilinear equation, namely extended Fisher--Kolmogorov equation. We present a few numerical examples to demonstrate the performance of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22587v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rekha Khot, Bishnu P. Lamichhane, Ricardo Ruiz-Baier</dc:creator>
    </item>
    <item>
      <title>An inertial minimal-deformation-rate framework for shape optimization</title>
      <link>https://arxiv.org/abs/2601.22605</link>
      <description>arXiv:2601.22605v1 Announce Type: new 
Abstract: We propose a robust numerical framework for PDE-constrained shape optimization and Willmore-driven surface hole filling. To address two central challenges -- slow progress in flat energy landscapes, which can trigger premature stagnation at suboptimal configurations, and mesh deterioration during geometric evolution -- we couple a second-order inertial flow with a minimal-deformation-rate (MDR) mesh motion strategy. This coupling accelerates convergence while preserving mesh quality and thus avoids remeshing. To further enhance robustness for non-smooth or non-convex initial geometries, we incorporate surface-diffusion regularization within the Barrett-Garcke-N"urnberg (BGN) framework. Moreover, we extend the inertial MDR methodology to Willmore-type surface hole filling, enabling high-order smooth reconstructions even from incompatible initial data. Numerical experiments demonstrate markedly faster convergence to lower original objective values, together with consistently superior mesh preservation throughout the evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22605v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Falai Chen, Buyang Li, Jiajie Li, Rong Tang</dc:creator>
    </item>
    <item>
      <title>A Mathematical Analysis of a Smooth-Convex-Concave Splitting Scheme for the Swift--Hohenberg Equation</title>
      <link>https://arxiv.org/abs/2601.22687</link>
      <description>arXiv:2601.22687v1 Announce Type: new 
Abstract: The Swift--Hohenberg equation is a widely studied fourth-order model, originally proposed to describe hydrodynamic fluctuations. It admits an energy-dissipation law and, under suitable assumptions, bounded solutions. Many structure-preserving numerical schemes have been proposed to retain such properties; however, existing approaches are often fully implicit and therefore computationally expensive. We introduce a simple design principle for constructing dissipation-preserving finite difference schemes and apply it to the Swift--Hohenberg equation in three spatial dimensions. Our analysis relies on discrete inequalities for the underlying energy, assuming a Lipschitz continuous gradient and either convexity or $\mu$-strong convexity of the relevant terms. The resulting method is linearly implicit, yet it preserves the original energy-dissipation law, guarantees unique solvability, ensures boundedness of numerical solutions, and admits an a priori error estimate, provided that the time step is sufficiently small. To the best of our knowledge, this is the first linearly implicit finite difference scheme for the Swift--Hohenberg equation for which all of these properties are established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22687v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Yonekura, Daiki Iwade, Shun Sato, Takayasu Matsuo</dc:creator>
    </item>
    <item>
      <title>Numerical Differentiation of Functions of Two Variables Using Chebyshev Polynomials</title>
      <link>https://arxiv.org/abs/2601.22762</link>
      <description>arXiv:2601.22762v1 Announce Type: new 
Abstract: We investigate the problem of numerical differentiation of bivariate functions from weighted Wiener classes using Chebyshev polynomial expansions. We develop and analyze a new version of the truncation method based on Chebyshev polynomials and the idea of hyperbolic cross to reconstruct partial derivatives of arbitrary order. The method exploits the approximation properties of Chebyshev polynomials and their natural connection to weighted spaces through the Chebyshev weight function. We derive a choice rule for the truncation parameter as a function of the noise level, smoothness parameters of the function class, and the order of differentiation. This approach allows us to establish explicit error estimates in both weighted integral norms and uniform metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22762v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Kyselov, Sergiy G. Solodky</dc:creator>
    </item>
    <item>
      <title>Approximation of PDE solution manifolds: Sparse-grid interpolation and quadrature</title>
      <link>https://arxiv.org/abs/2601.22825</link>
      <description>arXiv:2601.22825v1 Announce Type: new 
Abstract: We study fully-discrete approximations and quadratures of infinite-variate functions in abstract Bochner spaces associated with a Hilbert space $X$ and an infinite-tensor-product Jacobi measure. For target infinite-variate functions taking values in $X$ which admit absolutely convergent Jacobi generalized polynomial chaos expansions, with suitable weighted summability conditions for the coefficient sequences, we generalize and improve prior results on construction of sequences of finite sparse-grid tensor-product polynomial interpolation approximations and quadratures, based on the univariate Chebyshev points. For a generic stable discretization of $X$ in terms of a dense sequence $(V_m)_{m \in \mathbb{N}_0}$ of finite-dimensional subspaces, we obtain fully-discrete, linear approximations in terms of so-called sparse-grid tensor-product projectors, with convergence rates of approximations as well as of sparse-grid tensor-product quadratures of the target functions.
  We verify the abstract assumptions in two fundamental application settings: first, a linear elliptic diffusion equation with affine-parametric coefficients and second, abstract holomorphic maps between separable Hilbert spaces with affine-parametric input data encoding. For these settings, as in [37,20], cancellation of anti-symmetric terms in ultra-spherical Jacobi generalized polynomial chaos expansion coefficients implies crucially improved convergence rates of sparse-grid tensor-product quadrature with respect to the infinite-tensor-product Jacobi weight, free from the ``curse-of-dimension".
  Largely self-contained proofs of all results are developed. Approximation convergence rate results in the present setting which are based on construction of neural network surrogates, for unbounded parameter ranges with Gaussian measures, will be developed in extensions of the present work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22825v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dinh D\~ung, Van Kien Nguyen, Duong Thanh Pham, Christoph Schwab</dc:creator>
    </item>
    <item>
      <title>On the convergence and efficiency of splitting schemes for the Cahn-Hilliard-Biot model</title>
      <link>https://arxiv.org/abs/2601.22854</link>
      <description>arXiv:2601.22854v1 Announce Type: new 
Abstract: In this paper, we present a novel solution strategy for the Cahn-Hilliard-Biot model, a three-way coupled system that features the interplay of solid phase separation, fluid dynamics, and elastic deformations in porous media. It is a phase-field model that combines the Cahn-Hilliard regularized interface equation and Biot's equations of poroelasticity. Solving the system poses significant challenges due to its coupled, nonlinear, and non-convex nature. The main goal of this work is to provide a consistent and efficient solution strategy. With this in mind, we introduce a semi-implicit time discretization such that the resulting discrete system is equivalent to a convex minimization problem. Then, using abstract theory for convex problems, we prove the convergence of an alternating minimization method to the time-discrete system. The solution strategy is relatively flexible in terms of spatial discretization, although we require standard inverse inequalities for the guaranteed convergence of the alternating minimization method. Finally, we perform some numerical experiments that show the promise of the proposed solution strategy, both in terms of efficiency and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22854v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cedric Riethm\"uller, Erlend Storvik</dc:creator>
    </item>
    <item>
      <title>Bayesian Interpolating Neural Network (B-INN): a scalable and reliable Bayesian model for large-scale physical systems</title>
      <link>https://arxiv.org/abs/2601.22860</link>
      <description>arXiv:2601.22860v1 Announce Type: new 
Abstract: Neural networks and machine learning models for uncertainty quantification suffer from limited scalability and poor reliability compared to their deterministic counterparts. In industry-scale active learning settings, where generating a single high-fidelity simulation may require days or weeks of computation and produce data volumes on the order of gigabytes, they quickly become impractical. This paper proposes a scalable and reliable Bayesian surrogate model, termed the Bayesian Interpolating Neural Network (B-INN). The B-INN combines high-order interpolation theory with tensor decomposition and alternating direction algorithm to enable effective dimensionality reduction without compromising predictive accuracy. We theoretically show that the function space of a B-INN is a subset of that of Gaussian processes, while its Bayesian inference exhibits linear complexity, $\mathcal{O}(N)$, with respect to the number of training samples. Numerical experiments demonstrate that B-INNs can be from 20 times to 10,000 times faster with a robust uncertainty estimation compared to Bayesian neural networks and Gaussian processes. These capabilities make B-INN a practical foundation for uncertainty-driven active learning in large-scale industrial simulations, where computational efficiency and robust uncertainty calibration are paramount.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22860v1</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chanwook Park, Brian Kim, Jiachen Guo, Wing Kam Liu</dc:creator>
    </item>
    <item>
      <title>Randomized Methods for Kernelized DMD</title>
      <link>https://arxiv.org/abs/2601.22867</link>
      <description>arXiv:2601.22867v1 Announce Type: new 
Abstract: Dynamic Mode Decomposition (DMD) is a data-driven method related to Koopman operator theory that extracts information about dominant dynamics from data snapshots. In this paper we examine techniques to accelerate the application of DMD to large-scale data sets with an eye on randomized techniques. Randomized techniques exploit low-rank matrix approximations at a much smaller computational cost, therefore permitting the use of increased data set sizes. In particular, we propose the application of the RPCholesky algorithm in the setting of kernelized DMD (KDMD). This algorithm relies on adaptive randomized sampling to approximate positive semidefinite kernel matrices and provides better stability guarantees than previously implemented randomized methods for KDMD. Differences between existing competitive randomized techniques and our proposed implementation are discussed with a focus on numerical stability and tradeoff between exploration and exploitation of information obtained from data. The efficacy of this new combination of algorithms is demonstrated on well-established benchmark problems from DMD literature increasing in problem dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22867v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Oehme</dc:creator>
    </item>
    <item>
      <title>FNWoS: Fractional Neural Walk-on-Spheres Methods for High-Dimensional PDEs Driven by $\alpha$-stable L\'{e}vy Process on Irregular Domains</title>
      <link>https://arxiv.org/abs/2601.22942</link>
      <description>arXiv:2601.22942v1 Announce Type: new 
Abstract: In this paper, we develop a highly parallel and derivative-free fractional neural walk-on-spheres method (FNWoS) for solving high-dimensional fractional Poisson equations on irregular domains. We first propose a simplified fractional walk-on-spheres (FWoS) scheme that replaces the high-dimensional normalized weight integral with a constant weight and adopts a correspondingly simpler sampling density, substantially reducing per-trajectory cost. To mitigate the slow convergence of standard Monte Carlo sampling, FNWoS is then proposed via integrating this simplified FWoS estimator, derived from the Feynman-Kac representation, with a neural network surrogate. By amortizing sampling effort over the entire domain during training, FNWoS achieves more accurate evaluation at arbitrary query points with dramatically fewer trajectories than classical FWoS. To further enhance efficiency in regimes where the fractional order $\alpha$ is close to 2 and trajectories become excessively long, we introduce a truncated path strategy with a prescribed maximum step count. Building on this, we propose a buffered supervision mechanism that caches training pairs and progressively refines their Monte Carlo targets during training, removing the need to precompute a highly accurate training set and yielding the buffered fractional neural walk-on-spheres method (BFNWoS). Extensive numerical experiments, including tests on irregular domains and problems with dimensions up to $1000$, demonstrate the accuracy, scalability, and computational efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22942v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Guo, Mingxin Qin, Changtao Sheng, Hao Wu, Fanhai Zeng</dc:creator>
    </item>
    <item>
      <title>Preconditioning and Numerical Stability in Neural Network Training for Parametric PDEs</title>
      <link>https://arxiv.org/abs/2601.23185</link>
      <description>arXiv:2601.23185v1 Announce Type: new 
Abstract: In the context of training neural network-based approximations of solutions of parameter-dependent PDEs, we investigate the effect of preconditioning via well-conditioned frame representations of operators and demonstrate a significant improvement on the performance of standard training methods. We also observe that standard representations of preconditioned matrices are insufficient for obtaining numerical stability and propose a generally applicable form of stable representations that enables computations with single- and half-precision floating point numbers without loss of precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23185v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Bachmayr, Wolfgang Dahmen, Chenguang Duan, Mathias Oster</dc:creator>
    </item>
    <item>
      <title>Applications of QR-based Vector-Valued Rational Approximation</title>
      <link>https://arxiv.org/abs/2601.23237</link>
      <description>arXiv:2601.23237v1 Announce Type: new 
Abstract: Several applications of the QR-AAA algorithm, a greedy scheme for vector-valued rational approximation, are presented. The focus is on demonstrating the flexibility and practical effectiveness of QR-AAA in a variety of computational settings, including Stokes flow computation, multivariate rational approximation, function extension, the development of novel quadrature methods and near-field approximation in the boundary element method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23237v1</guid>
      <category>math.NA</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Dirckx</dc:creator>
    </item>
    <item>
      <title>A Primal-Dual Level Set Method for Computing Geodesic Distances</title>
      <link>https://arxiv.org/abs/2601.23244</link>
      <description>arXiv:2601.23244v1 Announce Type: new 
Abstract: The numerical computation of shortest paths or geodesics on surfaces, along with the associated geodesic distance, has a wide range of applications. Compared to Euclidean distance computation, these tasks are more complex due to the influence of surface geometry on the behavior of shortest paths. This paper introduces a primal-dual level set method for computing geodesic distances. A key insight is that the underlying surface can be implicitly represented as a zero level set, allowing us to formulate a constraint minimization problem. We employ the primal-dual methodology, along with regularization and acceleration techniques, to develop our algorithm. This approach is robust, efficient, and easy to implement. We establish a convergence result for the high-resolution PDE system, and numerical evidence suggests that the method converges to a geodesic in the limit of refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23244v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hailiang Liu, Laura Zinnel</dc:creator>
    </item>
    <item>
      <title>Rank Reduction AutoEncoders for Mechanical Design: Advancing Novel and Efficient Data-Driven Topology Optimization</title>
      <link>https://arxiv.org/abs/2601.23269</link>
      <description>arXiv:2601.23269v1 Announce Type: new 
Abstract: This work presents a data-driven framework for fast forward and inverse analysis in topology optimization (TO) by combining Rank Reduction Autoencoders (RRAEs) with neural latent-space mappings. The methodology targets the efficient approximation of the relationship between optimized geometries and their corresponding mechanical responses or Quantity of Interest (QoI), with a particular focus on compliance-minimized linear elastic structures. High-dimensional TO results are first compressed using RRAEs, which encode the data into a low-rank approximation via Singular Value Decomposition (SVD), obtained in this sense the most important features that approximate the data. Separate RRAE models are trained for geometry and for different types of QoIs, including scalar metrics, one-dimensional stress fields, and full two-dimensional von Mises stress distributions. The resulting low-dimensional latent coefficients of the latent space are then related through multilayer perceptrons to address both direct problems -- predicting structural responses from geometry -- and inverse problems -- recovering geometries from prescribed performance targets. The proposed approach is demonstrated on a benchmark TO problem based on a half MBB beam, using datasets generated via density-based Solid Isotropic Material with Penalization (SIMP) optimization. Numerical results show that the framework enables accurate and computationally efficient surrogate models, with increasing robustness and fidelity as richer QoIs are considered. The methodology also provides a foundation for generative mechanical design by enabling the synthesis of new geometries and responses through latent-space exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23269v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismael Ben-Yelun, Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Francisco Chinesta</dc:creator>
    </item>
    <item>
      <title>Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series</title>
      <link>https://arxiv.org/abs/2601.22200</link>
      <description>arXiv:2601.22200v1 Announce Type: cross 
Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22200v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Ontaneda Mijares, Nick Firoozye</dc:creator>
    </item>
    <item>
      <title>Exact closed-form Gaussian moments of residual layers</title>
      <link>https://arxiv.org/abs/2601.22307</link>
      <description>arXiv:2601.22307v1 Announce Type: cross 
Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22307v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Kuang, Xinfan Lin</dc:creator>
    </item>
    <item>
      <title>Parametric vector flows for registration fields in bounded domains with applications to nonlinear interpolation of shock-dominated flows</title>
      <link>https://arxiv.org/abs/2601.22712</link>
      <description>arXiv:2601.22712v1 Announce Type: cross 
Abstract: We present a registration procedure for parametric model order reduction (MOR) in two- and three-dimensional bounded domains. In the MOR framework, registration methods exploit solution snapshots to identify a parametric coordinate transformation that improves the approximation of the solution set through linear subspaces. For each training parameter, optimization-based (or variational) registration methods minimize a target function that measures the alignment of the coherent structures of interest (e.g., shocks, shear layers, cracks) for different parameter values, over a family of bijections of the computational domain $\Omega$. We consider diffeomorphisms $\Phi$ that are vector flows of given velocity fields $v$ with vanishing normal component on $\partial \Omega$; we rely on a sensor to extract appropriate point clouds from the solution snapshots and we develop an expectation-maximization procedure to simultaneously solve the point cloud matching problem and to determine the velocity $v$ (and thus the bijection $\Phi$); finally, we combine our registration method with the nonlinear interpolation technique of [Iollo, Taddei, J. Comput. Phys., 2022] to perform accurate interpolations of fluid dynamic fields in the presence of shocks. Numerical results for a two-dimensional inviscid transonic flow past a NACA airfoil and a three-dimensional viscous transonic flow past an ONERA M6 wing illustrate the many elements of the methodology and demonstrate the effectiveness of nonlinear interpolation for shock-dominated fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22712v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jon Labatut, Jean-Baptiste Chapelier, Angelo Iollo, Tommaso Taddei</dc:creator>
    </item>
    <item>
      <title>Discovering Scaling Exponents with Physics-Informed M\"untz-Sz\'asz Networks</title>
      <link>https://arxiv.org/abs/2601.22751</link>
      <description>arXiv:2601.22751v1 Announce Type: cross 
Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|\mu - \alpha|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22751v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gnankan Landry Regis N'guessan, Bum Jun Kim</dc:creator>
    </item>
    <item>
      <title>Decoupled Diffusion Sampling for Inverse Problems on Function Spaces</title>
      <link>https://arxiv.org/abs/2601.23280</link>
      <description>arXiv:2601.23280v1 Announce Type: cross 
Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23280v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Y. L. Lin, Jiachen Yao, Lufang Chiang, Julius Berner, Anima Anandkumar</dc:creator>
    </item>
    <item>
      <title>Numerical analysis of a constrained strain energy minimization problem</title>
      <link>https://arxiv.org/abs/2411.19089</link>
      <description>arXiv:2411.19089v2 Announce Type: replace 
Abstract: We consider a setting in which an evolving surface is implicitly characterized as the zero level of a level set function. Such an implicit surface does not encode any information about the path of a single point on the evolving surface. In the literature different approaches for determining a velocity that induces corresponding paths of points on the surface have been proposed. One of these is based on minimization of the strain energy functional. This then leads to a constrained minimization problem, which has a corresponding equivalent formulation as a saddle point problem. The main topic of this paper is a detailed analysis of this saddle point problem and of a finite element discretization of this problem. We derive well-posedness results for the continuous and discrete problems and optimal error estimates for a finite element discretization that uses standard $H^1$-conforming finite element spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19089v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tilman Aleman, Arnold Reusken</dc:creator>
    </item>
    <item>
      <title>Sparsity-Guided Multi-Parameter Selection in $\ell_1$-Regularized Models via a Fixed-Point Proximity Approach</title>
      <link>https://arxiv.org/abs/2502.00655</link>
      <description>arXiv:2502.00655v2 Announce Type: replace 
Abstract: We study a regularization framework that combines a convex fidelity term with multiple $\ell_1$-based regularizers, each linked to a distinct linear transform. This multi-penalty model enhances flexibility in promoting structured sparsity. We analyze how the choice of regularization parameters governs the sparsity of solutions under the given transforms and derive a precise relationship between the parameters and resulting sparsity patterns. This insight enables the development of an iterative strategy for selecting parameters to achieve prescribed sparsity levels. A key computational challenge arises in practice: effective parameter tuning requires simultaneous access to the regularized solution and two auxiliary vectors derived from the sparsity analysis. To address this, we propose a fixed-point proximity algorithm that jointly computes all three vectors. Together with our theoretical characterization, this algorithm forms the basis of a practical multi-parameter selection scheme. Numerical experiments demonstrate that the proposed method reliably produces solutions with desired sparsity patterns and strong approximation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00655v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianru Liu, Rui Wang, Yuesheng Xu</dc:creator>
    </item>
    <item>
      <title>Analysis and Elimination of Numerical Pressure Dependency in Coupled Stokes-Darcy Problem</title>
      <link>https://arxiv.org/abs/2504.19116</link>
      <description>arXiv:2504.19116v2 Announce Type: replace 
Abstract: This paper analyses the classical mixed finite element method (FEM) and a pressure-robust variant with divergence-free reconstruction operators for the coupled Stokes-Darcy problem. Its main contribution is to provide viscosity-explicit a priori error estimates that clearly distinguish the pressure dependence of the two discretizations: the velocity error of the classical scheme depends on both the exact pressure and the viscosity, whereas the pressure-robust method eliminates both entirely. Moreover, we derive pressure error estimates and quantify their dependence on the exact solution and model parameters. Two-dimensional numerical experiments validate the theoretical findings, including higher-order tests up to polynomial degree three and a lid-driven cavity benchmark with a piecewise linear interface. The implementation code is made publicly available to facilitate reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19116v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachuan Zhang</dc:creator>
    </item>
    <item>
      <title>A hybrid isogeometric and finite element method: NURBS-enhanced finite element method for hexahedral meshes (NEFEM-HEX)</title>
      <link>https://arxiv.org/abs/2506.13694</link>
      <description>arXiv:2506.13694v3 Announce Type: replace 
Abstract: In this paper, we present a NURBS-enhanced finite element method that integrates the NURBS-based boundary representation of a geometric domain into a standard finite element framework for hexahedral meshes. We decompose an open, bounded, convex three-dimensional domain with a NURBS boundary into two parts, define NURBS-enhanced finite elements over the boundary layer, and use piecewise-linear Lagrange finite elements in the interior region. We introduce a special quadrature rule and a stable interpolation operator for the NURBS-enhanced elements. We discuss how the h-refinement in finite element analysis and the knot insertion in isogeometric analysis can be utilized in the refinement of the NURBS-enhanced elements. To illustrate an application of our methodology, we utilize a generic weak formulation of a second-order linear elliptic boundary value problem and derive a priori error estimates in the $H^{1}$ norm. In addition, we use the Poisson problem as a model problem and provide numerical results that support the theoretical results. The proposed methodology combines the efficiency of finite element analysis with the geometric precision of NURBS, and may enable more accurate and efficient simulations over complex geometries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13694v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duygu Sap</dc:creator>
    </item>
    <item>
      <title>The lightning method for the heat equation</title>
      <link>https://arxiv.org/abs/2506.22576</link>
      <description>arXiv:2506.22576v3 Announce Type: replace 
Abstract: This paper introduces a new method for solving the planar heat equation based on the Lightning Method. The lightning method is a recent development in the numerical solution of linear PDEs which expresses solutions using sums of polynomials and rational functions, or more generally as sums of fundamental solutions. The method is particularly well suited to handle domains with sharp corners where solution singularities are present. Boundary conditions are formed on a set of collocation points which is then solved as an overdetermined linear system. The approach of the present work is to utilize the Laplace transform to obtain a modified Helmholtz equation which is solved by an application of the lightning method. The numerical inversion of the Laplace transform is then performed by means of Talbot integration. Our validation of the method against existing results and multiple challenging test problems shows the method attains spectral accuracy with root-exponential convergence while being robust across a wide range of time intervals and adaptable to a variety of geometric scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22576v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.CV</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hunter La Croix, Alan E. Lindsay</dc:creator>
    </item>
    <item>
      <title>A Generalized Alternating Anderson Acceleration Method</title>
      <link>https://arxiv.org/abs/2508.10158</link>
      <description>arXiv:2508.10158v2 Announce Type: replace 
Abstract: In this work, we propose a generalized alternating Anderson acceleration method, a periodic scheme composed of $t$ fixed-point iteration steps, interleaved with $s$ steps of Anderson acceleration with window size $m$, to solve linear and nonlinear problems. This allows flexibility to use different combinations of fixed-point iteration and Anderson iteration. We present a convergence analysis of the proposed scheme for accelerating the Richardson iteration in the linear case, with a focus on specific parameter choices of interest. Specifically, we prove convergence of the proposed method under contractive fixed-point iteration and provide a sufficient condition for convergence when the Richardson iteration matrix is diagonalizable and noncontractive. To demonstrate the broader applicability of our proposed method, we use it to accelerate Jacobi iteration, Picard iteration, gradient descent, and the alternating direction method of multipliers in solving partial differential equations and nonlinear, nonsmooth optimization problems. The numerical results illustrate that the proposed scheme is more efficient than the existing windowed Anderson acceleration and alternating Anderson ($s=1$) in terms of iteration number and CPU time for careful choice of parameters $m, s, t$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10158v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhui He, Santolo Leveque</dc:creator>
    </item>
    <item>
      <title>Data-integrated neural networks for solving partial differential equations</title>
      <link>https://arxiv.org/abs/2511.12055</link>
      <description>arXiv:2511.12055v4 Announce Type: replace 
Abstract: In this work, we propose data-integrated neural networks (DataInNet) for solving partial differential equations (PDEs), offering a novel approach to leveraging data (e.g., source terms, initial conditions, and boundary conditions). The core of this work lies in the integration of data into a unified network framework. DataInNet comprises two subnetworks: a data integration neural network responsible for accommodating and fusing various types of data, and a fully connected neural network dedicated to learning the residual physical information not captured by the data integration neural network. This network architecture inherently excludes function classes that violate known physical constraints, thereby substantially narrowing the solution search space. Numerical experiments demonstrate that the proposed DataInNet delivers superior performance on challenging problems, such as the Helmholtz equation (relative \(L^2\) error: O(\(10^{-6}\))) and PDEs with high frequency solutions (relative \(L^2\) error: O(\(10^{-5}\))).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12055v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiachun Zheng, Yunqing Huang, Nianyu Yi, Yunlei Yang</dc:creator>
    </item>
    <item>
      <title>Generalizations of the Normalized Radon Cumulative Distribution Transform for Limited Data Recognition</title>
      <link>https://arxiv.org/abs/2512.08099</link>
      <description>arXiv:2512.08099v2 Announce Type: replace 
Abstract: The Radon cumulative distribution transform (R-CDT) exploits one-dimensional Wasserstein transport and the Radon transform to represent prominent features in images. It is closely related to the sliced Wasserstein distance and facilitates classification tasks, especially in the small data regime, like the recognition of watermarks in filigranology. Here, a typical issue is that the given data may be subject to affine transformations caused by the measuring process. To make the R-CDT invariant under arbitrary affine transformations, a two-step normalization of the R-CDT has been proposed in our earlier works. The aim of this paper is twofold. First, we propose a family of generalized normalizations to enhance flexibility for applications. Second, we study multi-dimensional and non-Euclidean settings by making use of generalized Radon transforms. We prove that our novel feature representations are invariant under certain transformations and allow for linear separation in feature space. Our theoretical results are supported by numerical experiments based on 2d images, 3d shapes and 3d rotation matrices, showing near perfect classification accuracies and clustering results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08099v2</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Beckmann, Robert Beinert, Jonas Bresch</dc:creator>
    </item>
    <item>
      <title>Study of a TPFA scheme for the stochastic Allen-Cahn problem with constraint through numerical experiments</title>
      <link>https://arxiv.org/abs/2512.17712</link>
      <description>arXiv:2512.17712v3 Announce Type: replace 
Abstract: This contribution provides numerical experiments for a finite volume scheme for an approximation of the stochastic Allen-Cahn equation with homogeneous Neumann boundary conditions. The approximation is done by a Yosida approximation of the subdifferential operator. The problem is set on a polygonal bounded domain in two or three dimensions. The non-linear character of the projection term induces challenges to implement the scheme. To this end, we provide a splitting method for the finite volume scheme. We show that the splitting method is accurate. The computational error estimates induce that the squared $L^2$-error w.r.t. time is of order $1$ as long as the noise term is small enough. For larger noise terms the order of convergence w.r.t. time might become worse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17712v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Sapountzoglou, Aleksandra Zimmermann</dc:creator>
    </item>
    <item>
      <title>A locking-free mixed virtual element discretization for the elasticity eigenvalue problem</title>
      <link>https://arxiv.org/abs/2601.20807</link>
      <description>arXiv:2601.20807v2 Announce Type: replace 
Abstract: In this paper, we introduce a mixed virtual element method to approximate the eigenvalues and eigenfunctions of the two-dimensional elasticity eigenvalue problem. Under standard assumptions on the meshes, we prove the convergence of the discrete solution operator to the continuous one as the mesh size tends to zero. Using the theory of compact operators, we analyze the convergence of the method and derive error estimates for both the eigenvalues and eigenfunctions. We validate our theoretical results with a series of numerical tests, in which we compute convergence orders and show that the method is locking-free and capable of accurately approximating the spectrum independently of the shape of the polygons on the meshes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20807v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Lepe, Gonzalo Rivera</dc:creator>
    </item>
    <item>
      <title>Identification of space-dependent coefficients in two competing terms of a nonlinear subdiffusion equation</title>
      <link>https://arxiv.org/abs/2601.21018</link>
      <description>arXiv:2601.21018v2 Announce Type: replace 
Abstract: We consider a (sub)diffusion equation with a nonlinearity of the form $pf(u)-qu$, where $p$ and $q$ are space dependent functions. Prominent examples are the Fisher-KPP, the Frank-Kamenetskii-Zeldovich and the Allen-Cahn equations. We devise a fixed point scheme for reconstructing the spatially varying coefficients from interior observations a) at final time under two different excitations b) at two different time instances under a single excitation. Convergence of the scheme as well as local uniqueness of these coefficients is proven. Numerical experiments illustrate the performance of the reconstruction scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21018v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barbara Kaltenbacher, William Rundell</dc:creator>
    </item>
    <item>
      <title>Iterative execution of discrete and inverse discrete Fourier transforms with applications for signal denoising via sparsification</title>
      <link>https://arxiv.org/abs/2211.09284</link>
      <description>arXiv:2211.09284v4 Announce Type: replace-cross 
Abstract: We describe a family of iterative algorithms that involve the repeated execution of discrete and inverse discrete Fourier transforms. One interesting member of this family is motivated by the discrete Fourier transform uncertainty principle and involves the application of a sparsification operation to both the real domain and frequency domain data with convergence obtained when real domain sparsity hits a stable pattern. This sparsification variant has practical utility for signal denoising, in particular the recovery of a periodic spike signal in the presence of Gaussian noise. General convergence properties and denoising performance relative to existing methods are demonstrated using simulation studies. An R package implementing this technique and related resources can be found at https://hrfrost.host.dartmouth.edu/IterativeFT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09284v4</guid>
      <category>eess.SP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. Robert Frost</dc:creator>
    </item>
    <item>
      <title>Frank--Wolfe algorithms for piecewise star-convex functions with a nonsmooth difference-of-convex structure</title>
      <link>https://arxiv.org/abs/2308.16444</link>
      <description>arXiv:2308.16444v4 Announce Type: replace-cross 
Abstract: In the present paper, we formulate two versions of Frank--Wolfe algorithm or conditional gradient method to solve the DC optimization problem with an adaptive step size. The DC objective function consists of two components; the first is thought to be differentiable with a continuous Lipschitz gradient, while the second is only thought to be convex. The second version is based on the first and employs finite differences to approximate the gradient of the first component of the objective function. In contrast to past formulations that used the curvature/Lipschitz-type constant of the objective function, the step size computed does not require any constant associated with the components. For the first version, we established that the algorithm is well-defined of the algorithm and that every limit point of the generated sequence is a stationary point of the problem. We also introduce the class of weak-star-convex functions and show that, despite the fact that these functions are non-convex in general, the rate of convergence of the first version of the algorithm to minimize these functions is ${\cal O}(1/k)$. The finite difference used to approximate the gradient in the second version of the Frank-Wolfe algorithm is computed with the step-size adaptively updated using two previous iterations. Unlike previous applications of finite difference in the Frank-Wolfe algorithm, which provided approximate gradients with absolute error, the one used here provides us with a relative error, simplifying the algorithm analysis. In this case, we show that all limit points of the generated sequence for the second version of the Frank-Wolfe algorithm are stationary points for the problem under consideration, and we establish that the rate of convergence for the duality gap is ${\cal O}(1/\sqrt{k})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.16444v4</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. D\'iaz Mill\'an, O. P. Ferreira, J. Ugon</dc:creator>
    </item>
    <item>
      <title>Stein's method for marginals on large graphical models</title>
      <link>https://arxiv.org/abs/2410.11771</link>
      <description>arXiv:2410.11771v3 Announce Type: replace-cross 
Abstract: Many spatial models exhibit locality structures that effectively reduce their intrinsic dimensionality, enabling efficient approximation and sampling of high-dimensional distributions. However, existing approximation techniques primarily focus on joint distributions and do not provide precise accuracy control for low-dimensional marginals, which are of primary interest in many practical scenarios. By leveraging the locality structures, we establish a dimension independent uniform error bound for the marginals of approximate distributions. Inspired by the Stein's method, we introduce a novel $\delta$-locality condition that quantifies the locality in distributions, and link it to the structural assumptions such as the sparse graphical models. The theoretical guarantee motivates the localization of existing sampling methods, as we illustrate through the localized likelihood-informed subspace method and localized score matching. We show that by leveraging the locality structure, these methods greatly reduce the sample complexity and computational cost via localized and parallel implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11771v3</guid>
      <category>stat.ML</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiangang Cui, Shuigen Liu, Xin T. Tong</dc:creator>
    </item>
    <item>
      <title>State Estimation Using Sparse DEIM and Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2410.15982</link>
      <description>arXiv:2410.15982v3 Announce Type: replace-cross 
Abstract: Sparse Discrete Empirical Interpolation Method (S-DEIM) was recently proposed for state estimation in dynamical systems when only a sparse subset of the state variables can be observed. The S-DEIM estimate involves a kernel vector whose optimal value is inferred through a data assimilation algorithm. This data assimilation step suffers from two drawbacks: (i) It requires the knowledge of the governing equations of the dynamical system, and (ii) It is not generally guaranteed to converge to the optimal kernel vector. To address these issues, here we introduce an equation-free S-DEIM framework that estimates the optimal kernel vector from sparse observational time series using recurrent neural networks (RNNs). We show that the recurrent architecture is necessary since the kernel vector cannot be estimated from instantaneous observations. But RNNs, which incorporate the past history of the observations in the learning process, lead to nearly optimal estimations. We demonstrate the efficacy of our method on three numerical examples with increasing degree of spatiotemporal complexity: a conceptual model of atmospheric flow known as the Lorenz-96 system, the Kuramoto-Sivashinsky equation, and the Rayleigh-Benard convection. In each case, the resulting S-DEIM estimates are satisfactory even when a relatively simple RNN architecture, namely the reservoir computing network, is used. More specifically, our RNN-based S-DEIM state estimations reduce the relative error between 42% and 58% when compared to Q-DEIM which ignores the kernel vector by setting it equal to zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15982v3</guid>
      <category>math.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>nlin.CD</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Farazmand</dc:creator>
    </item>
    <item>
      <title>Well-Posedness of the Linear Regularized 13-Moment Equations Using Tensor-Valued Korn Inequalities</title>
      <link>https://arxiv.org/abs/2501.14108</link>
      <description>arXiv:2501.14108v2 Announce Type: replace-cross 
Abstract: In this paper, we finally prove the well-posedness of the linearized R13 moment model, which describes, e.g., rarefied gas flows. As an extension of the classical fluid equations, moment models are robust and have been frequently used, yet they are challenging to analyze due to their additional equations. By effectively grouping variables, we identify a 2-by-2 block structure, allowing us to analyze well-posedness within the abstract LBB framework for saddle point problems. Due to the unique tensorial structure of the equations, in addition to an interesting combination of tools from Stokes' and linear elasticity theory, we also need new coercivity estimates for tensor fields. These Korn-type inequalities are established by analyzing the symbol map of the symmetric and trace-free part of tensor derivative fields. Together with the corresponding right inverse of the tensorial divergence, we obtain the existence and uniqueness of weak solutions. This result also serves as the basis for future numerical analysis of corresponding discretization schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14108v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Lewintan, Lambert Theisen, Manuel Torrilhon</dc:creator>
    </item>
    <item>
      <title>Antithetic Noise in Diffusion Models</title>
      <link>https://arxiv.org/abs/2506.06185</link>
      <description>arXiv:2506.06185v2 Announce Type: replace-cross 
Abstract: We systematically study antithetic initial noise in diffusion models, discovering that pairing each noise sample with its negation consistently produces strong negative correlation. This universal phenomenon holds across datasets, model architectures, conditional and unconditional sampling, and even other generative models such as VAEs and Normalizing Flows. To explain it, we combine experiments and theory and propose a \textit{symmetry conjecture} that the learned score function is approximately affine antisymmetric (odd symmetry up to a constant shift), supported by empirical evidence. This negative correlation leads to substantially more reliable uncertainty quantification with up to $90\%$ narrower confidence intervals. We demonstrate these gains on tasks including estimating pixel-wise statistics and evaluating diffusion inverse solvers. We also provide extensions with randomized quasi-Monte Carlo noise designs for uncertainty quantification, and explore additional applications of the antithetic noise design to improve image editing and generation diversity. Our framework is training-free, model-agnostic, and adds no runtime overhead. Code is available at https://github.com/jjia131/Antithetic-Noise-in-Diffusion-Models-page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06185v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Jia, Sifan Liu, Bowen Song, Wei Yuan, Liyue Shen, Guanyang Wang</dc:creator>
    </item>
    <item>
      <title>The Compound BSDE Method: A Fully Forward Method for Option Pricing and Optimal Stopping Problems in Finance</title>
      <link>https://arxiv.org/abs/2601.18634</link>
      <description>arXiv:2601.18634v2 Announce Type: replace-cross 
Abstract: We propose the Compound BSDE method, a fully forward, deep-learning-based approach for solving a broad class of problems in financial mathematics, including optimal stopping. The method is based on a reformulation of option pricing problems in terms of a system of backward stochastic differential equations (BSDEs), which offers a new perspective on the numerical treatment of compound options and optimal stopping problems such as Bermudan option pricing. Building on the classical deep BSDE method for a single BSDE, we develop an algorithm for compound BSDEs and establish its convergence properties. In particular, we derive an a posteriori error estimate for the proposed method. Numerical experiments demonstrate the accuracy and computational efficiency of the approach, and illustrate its effectiveness for high-dimensional option pricing and optimal stopping problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18634v2</guid>
      <category>q-fin.CP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-fin.PR</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhipeng Huang, Cornelis W. Oosterlee</dc:creator>
    </item>
  </channel>
</rss>
