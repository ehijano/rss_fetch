<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 03:01:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluation of adaptive sampling methods in scenario generation for virtual safety impact assessment of pre-crash safety systems</title>
      <link>https://arxiv.org/abs/2503.00815</link>
      <description>arXiv:2503.00815v1 Announce Type: new 
Abstract: Virtual safety assessment plays a vital role in evaluating the safety impact of pre-crash safety systems such as advanced driver assistance systems (ADAS) and automated driving systems (ADS). However, as the number of parameters in simulation-based scenario generation increases, the number of crash scenarios to simulate grows exponentially, making complete enumeration computationally infeasible. Efficient sampling methods, such as importance sampling and active sampling, have been proposed to address this challenge. However, a comprehensive evaluation of how domain knowledge, stratification, and batch sampling affect their efficiency remains limited.
  This study evaluates the performance of importance sampling and active sampling in scenario generation, incorporating two domain-knowledge-driven features: adaptive sample space reduction (ASSR) and stratification. Additionally, we assess the effects of a third feature, batch sampling, on computational efficiency in terms of both CPU and wall-clock time. Based on our findings, we provide practical recommendations for applying ASSR, stratification, and batch sampling to optimize sampling performance.
  Our results demonstrate that ASSR substantially improves sampling efficiency for both importance sampling and active sampling. When integrated into active sampling, ASSR reduces the root mean squared estimation error (RMSE) of the estimates by up to 90\%. Stratification further improves sampling performance for both methods, regardless of ASSR implementation. When ASSR and/or stratification are applied, importance sampling performs on par with active sampling, whereas when neither feature is used, active sampling is more efficient. Larger batch sizes reduce wall-clock time but increase the number of simulations required to achieve the same estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00815v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaomi Yang, Henrik Imberg, Carol Flannagan, Jonas B\"argman</dc:creator>
    </item>
    <item>
      <title>Probabilistic Record Linkage of Two Gun Violence Data Sets</title>
      <link>https://arxiv.org/abs/2503.01054</link>
      <description>arXiv:2503.01054v1 Announce Type: new 
Abstract: Objective: Gun violence is a serious public health issue in the United States. The Gun Violence Archive (GVA) provides detailed geographic information, while The National Violent Death Reporting System (NVDRS) offers demographic, socioeconomic, and narrative data about gun homicides. We develop and test a method for merging data sets, each with its own strengths, to overcome their individual limitations. This merged data set can inform analysis and strategies to reduce high gun violence rates in the US.
  Methods: After preprocessing the data, we used a probabilistic record linkage program to link records from the Gun Violence Archive (GVA) (n=36,245) with records from The National Violent Death Reporting System (NVDRS) (n=30,592). Sensitivity (the false match rate) was evaluated using a manual approach.
  Results: The linkage returned $27,420$ matches of gun violence incidents from the GVA and NVDRS data sets. Of these cases, 942 records were able to be manually evaluated due to the restricted details accessible from GVA records. Our framework achieves a 90.12% accuracy rate in linking GVA incidents with corresponding NVDRS records.
  Conclusion: Electronic linkage of gun violence data from two different sources is feasible, and can be used to increase the utility of the data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01054v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iris Horng, Qishuo Yin, William Chan, Jared Murray, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Academic Literature Recommendation in Large-scale Citation Networks Enhanced by Large Language Models</title>
      <link>https://arxiv.org/abs/2503.01189</link>
      <description>arXiv:2503.01189v1 Announce Type: new 
Abstract: Literature recommendation is essential for researchers to find relevant articles in an ever-growing academic field. However, traditional methods often struggle due to data limitations and methodological challenges. In this work, we construct a large citation network and propose a hybrid recommendation framework for scientific article recommendation. Specifically, the citation network contains 190,381 articles from 70 journals, covering statistics, econometrics, and computer science, spanning from 1981 to 2022. The recommendation mechanism integrates network-based citation patterns with content-based semantic similarities. To enhance content-based recommendations, we employ text-embedding-3-small model of OpenAI to generate an embedding vector for the abstract of each article. The model has two key advantages: computational efficiency and embedding stability during incremental updates, which is crucial for handling dynamic academic databases. Additionally, the recommendation mechanism is designed to allow users to adjust weights according to their preferences, providing flexibility and personalization. Extensive experiments have been conducted to verify the effectiveness of our approach. In summary, our work not only provides a complete data system for building and analyzing citation networks, but also introduces a practical recommendation method that helps researchers navigate the growing volume of academic literature, making it easier to find the most relevant and influential articles in the era of information overload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01189v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Liu, Yan Zhang, Rui Pan, Tianchen Gao, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>A network psychometric analysis of maths anxiety factors in Italian psychology students</title>
      <link>https://arxiv.org/abs/2503.01568</link>
      <description>arXiv:2503.01568v1 Announce Type: new 
Abstract: Dealing with mathematics can induce significant anxiety, strongly affecting psychology students' academic performance and career prospects. This phenomenon is known as maths anxiety and several scales can measure it. Most scales were created in English and abbreviated versions were translated and validated among Italian populations (e.g. Abbreviated Maths Anxiety Scale). This study translated the 3-factor MAS-UK scale in Italian to produce a new tool, MAS-IT, validated specifically in a sample of Italian undergraduates enrolled in psychology or related BSc programmes. A sample of 324 Italian undergraduates completed the MAS-IT. The data were analysed using confirmatory Factor Analysis (CFA), testing the original MAS-UK 3-factor model. CFA results revealed that the original MAS-UK 3-factor model did not fit the Italian data. A subsequent Exploratory Graph Analysis (EGA) identified 4 distinct components/factors of maths anxiety detected by MAS-IT. The items relative to "Passive Observation maths anxiety" factor remained stable across the analyses, whereas "Evaluation maths anxiety" and "Everyday/Social maths anxiety" items showed a reduced or poor item stability. Quantitative findings indicated potential cultural or contextual differences in the expression of maths anxiety in today's psychology undergraduates, underlining the need for more appropriate tools to be used among psychology students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01568v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Franchino, Luciana Ciringione, Luisa Canal, Ottavia Marina Epifania, Luigi Lombardi, Gianluca Lattanzi, Massimo Stella</dc:creator>
    </item>
    <item>
      <title>Failure of Optimal Design Theory? A Case Study in Toxicology Using Sequential Robust Optimal Design Framework</title>
      <link>https://arxiv.org/abs/2503.00002</link>
      <description>arXiv:2503.00002v1 Announce Type: cross 
Abstract: This paper presents a quasi-sequential optimal design framework for toxicology experiments, specifically applied to sea urchin embryos. The authors propose a novel approach combining robust optimal design with adaptive, stage-based testing to improve efficiency in toxicological studies, particularly where traditional uniform designs fall short. The methodology uses statistical models to refine dose levels across experimental phases, aiming for increased precision while reducing costs and complexity. Key components include selecting an initial design, iterative dose optimization based on preliminary results, and assessing various model fits to ensure robust, data-driven adjustments. Through case studies, we demonstrate improved statistical efficiency and adaptability in toxicology, with potential applications in other experimental domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00002v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Michael Collins, Jessica Munson, Weng Kee Wong</dc:creator>
    </item>
    <item>
      <title>Neural Posterior Estimation for Cataloging Astronomical Images with Spatially Varying Backgrounds and Point Spread Functions</title>
      <link>https://arxiv.org/abs/2503.00156</link>
      <description>arXiv:2503.00156v1 Announce Type: cross 
Abstract: Neural posterior estimation (NPE), a type of amortized variational inference, is a computationally efficient means of constructing probabilistic catalogs of light sources from astronomical images. To date, NPE has not been used to perform inference in models with spatially varying covariates. However, ground-based astronomical images have spatially varying sky backgrounds and point spread functions (PSFs), and accounting for this variation is essential for constructing accurate catalogs of imaged light sources. In this work, we introduce a method of performing NPE with spatially varying backgrounds and PSFs. In this method, we generate synthetic catalogs and semi-synthetic images for these catalogs using randomly sampled PSF and background estimates from existing surveys. Using this data, we train a neural network, which takes an astronomical image and representations of its background and PSF as input, to output a probabilistic catalog. Our experiments with Sloan Digital Sky Survey data demonstrate the effectiveness of NPE in the presence of spatially varying backgrounds and PSFs for light source detection, star/galaxy separation, and flux measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00156v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aakash Patel, Tianqing Zhang, Camille Avestruz, Jeffrey Regier, the LSST Dark Energy Science Collaboration</dc:creator>
    </item>
    <item>
      <title>Heteroscedastic Growth Curve Modeling with Shape-Restricted Splines</title>
      <link>https://arxiv.org/abs/2503.00254</link>
      <description>arXiv:2503.00254v1 Announce Type: cross 
Abstract: Growth curve analysis (GCA) has a wide range of applications in various fields where growth trajectories need to be modeled. Heteroscedasticity is often present in the error term, which can not be handled with sufficient flexibility by standard linear fixed or mixed-effects models. One situation that has been addressed is where the error variance is characterized by a linear predictor with certain covariates. A frequently encountered scenario in GCA, however, is one in which the variance is a smooth function of the mean with known shape restrictions. A naive application of standard linear mixed-effects models would underestimate the variance of the fixed effects estimators and, consequently, the uncertainty of the estimated growth curve. We propose to model the variance of the response variable as a shape-restricted (increasing/decreasing; convex/concave) function of the marginal or conditional mean using shape-restricted splines. A simple iteratively reweighted fitting algorithm that takes advantage of existing software for linear mixed-effects models is developed. For inference, a parametric bootstrap procedure is recommended. Our simulation study shows that the proposed method gives satisfactory inference with moderate sample sizes. The utility of the method is demonstrated using two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00254v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieying Jiao, Wenling Song, Yishu Xue, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Common indicators hurt armed conflict prediction</title>
      <link>https://arxiv.org/abs/2503.00265</link>
      <description>arXiv:2503.00265v1 Announce Type: cross 
Abstract: Are big conflicts different from small or medium size conflicts? To answer this question, we leverage fine-grained conflict data, which we map to climate, geography, infrastructure, economics, raw demographics, and demographic composition in Africa. With an unsupervised learning model, we find three overarching conflict types representing ``major unrest,'' ``local conflict,'' and ``sporadic and spillover events.'' Major unrest predominantly propagates around densely populated areas with well-developed infrastructure and flat, riparian geography. Local conflicts are in regions of median population density, are diverse socio-economically and geographically, and are often confined within country borders. Finally, sporadic and spillover conflicts remain small, often in low population density areas, with little infrastructure and poor economic conditions. The three types stratify into a hierarchy of factors that highlights population, infrastructure, economics, and geography, respectively, as the most discriminative indicators. Specifying conflict type negatively impacts the predictability of conflict intensity such as fatalities, conflict duration, and other measures of conflict size. The competitive effect is a general consequence of weak statistical dependence. Hence, we develop an empirical and bottom-up methodology to identify conflict types, knowledge of which can hurt predictability and cautions us about the limited utility of commonly available indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00265v1</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niraj Kushwaha, Woi Sok Oh, Shlok Shah, Edward D. Lee</dc:creator>
    </item>
    <item>
      <title>Strategic decision points in experiments: A predictive Bayesian optional stopping method</title>
      <link>https://arxiv.org/abs/2503.00818</link>
      <description>arXiv:2503.00818v1 Announce Type: cross 
Abstract: Sample size determination is crucial in experimental design, especially in traffic and transport research. Frequentist statistics require a fixed sample size determined by power analysis, which cannot be adjusted once the experiment starts. Bayesian sample size determination, with proper priors, offers an alternative. Bayesian optional stopping (BOS) allows experiments to stop when statistical targets are met. We introduce predictive Bayesian optional stopping (pBOS), combining BOS with Bayesian rehearsal simulations to predict future data and stop experiments if targets are unlikely to be met within resource constraints. We identified and corrected a bias in predictions using multiple linear regression. pBOS shows up to 118% better cost benefit than traditional BOS and is more efficient than frequentist methods. pBOS allows researchers to, under certain conditions, stop experiments when resources are insufficient or when enough data is collected, optimizing resource use and cost savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00818v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaomi Yang, Carol Flannagan, Jonas B\"argman</dc:creator>
    </item>
    <item>
      <title>Improving the statistical efficiency of cross-conformal prediction</title>
      <link>https://arxiv.org/abs/2503.01495</link>
      <description>arXiv:2503.01495v1 Announce Type: cross 
Abstract: Vovk (2015) introduced cross-conformal prediction, a modification of split conformal designed to improve the width of prediction sets. The method, when trained with a miscoverage rate equal to $\alpha$ and $n \gg K$, ensures a marginal coverage of at least $1 - 2\alpha - 2(1-\alpha)(K-1)/(n+K)$, where $n$ is the number of observations and $K$ denotes the number of folds. A simple modification of the method achieves coverage of at least $1-2\alpha$. In this work, we propose new variants of both methods that yield smaller prediction sets without compromising the latter theoretical guarantee. The proposed methods are based on recent results deriving more statistically efficient combination of p-values that leverage exchangeability and randomization. Simulations confirm the theoretical findings and bring out some important tradeoffs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01495v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Gasparin, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Efficient Long-Term Structural Reliability Estimation with Non-Gaussian Stochastic Models: A Design of Experiments Approach</title>
      <link>https://arxiv.org/abs/2503.01566</link>
      <description>arXiv:2503.01566v1 Announce Type: cross 
Abstract: Extreme response assessment is important in the design and operation of engineering structures, and is a crucial part of structural risk and reliability analyses. Structures should be designed in a way that enables them to withstand the environmental loads they are expected to experience over their lifetime, without designs being unnecessarily conservative and costly. An accurate risk estimate is essential but difficult to obtain because the long-term behaviour of a structure is typically too complex to calculate analytically or with brute force Monte Carlo simulation. Therefore, approximation methods are required to estimate the extreme response using only a limited number of short-term conditional response calculations. Combining surrogate models with Design of Experiments is an approximation approach that has gained popularity due to its ability to account for both long-term environment variability and short-term response variability. In this paper, we propose a method for estimating the extreme response of black-box, stochastic models with heteroscedastic non-Gaussian noise. We present a mathematically founded extreme response estimation process that enables Design of Experiment approaches that are prohibitively expensive with surrogate Monte Carlo. The theory leads us to speculate this method can robustly produce more confident extreme response estimates, and is suitable for a variety of domains. While this needs to be further validated empirically, the method offers a promising tool for reducing the uncertainty decision-makers face, allowing them to make better informed choices and create more optimal structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01566v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Winter, Christian Agrell, Juan Camilo Guevara G\'omez, Erik Vanem</dc:creator>
    </item>
    <item>
      <title>Augmented quantization: a general approach to mixture models</title>
      <link>https://arxiv.org/abs/2309.08389</link>
      <description>arXiv:2309.08389v5 Announce Type: replace 
Abstract: The investigation of mixture models is a key to understand and visualize the distribution of multivariate data. Most mixture models approaches are based on likelihoods, and are not adapted to distribution with finite support or without a well-defined density function. This study proposes the Augmented Quantization method, which is a reformulation of the classical quantization problem but which uses the p-Wasserstein distance. This metric can be computed in very general distribution spaces, in particular with varying supports. The clustering interpretation of quantization is revisited in a more general framework. The performance of Augmented Quantization is first demonstrated through analytical toy problems. Subsequently, it is applied to a practical case study involving river flooding, wherein mixtures of Dirac and Uniform distributions are built in the input space, enabling the identification of the most influential variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08389v5</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlie Sire, Didier Rulli\`ere, Rodolphe Le Riche, J\'er\'emy Rohmer, Yann Richet, Lucie Pheulpin</dc:creator>
    </item>
    <item>
      <title>Penalized Principal Component Analysis Using Smoothing</title>
      <link>https://arxiv.org/abs/2309.13838</link>
      <description>arXiv:2309.13838v2 Announce Type: replace 
Abstract: Principal components computed via PCA (principal component analysis) are traditionally used to reduce dimensionality in genomic data or to correct for population stratification. In this paper, we explore the penalized eigenvalue problem (PEP) which reformulates the computation of the first eigenvector as an optimization problem and adds an $L_1$ penalty constraint to enforce sparseness of the solution. The contribution of our article is threefold. First, we extend PEP by applying smoothing to the original LASSO-type $L_1$ penalty. This allows one to compute analytical gradients which enable faster and more efficient minimization of the objective function associated with the optimization problem. Second, we demonstrate how higher order eigenvectors can be calculated with PEP using established results from singular value decomposition (SVD). Third, we present four experimental studies to demonstrate the usefulness of the smoothed penalized eigenvectors. Using data from the 1000 Genomes Project dataset, we empirically demonstrate that our proposed smoothed PEP allows one to increase numerical stability and obtain meaningful eigenvectors. We also employ the penalized eigenvector approach in two additional real data applications (computation of a polygenic risk score and clustering), demonstrating that exchanging the penalized eigenvectors for their smoothed counterparts can increase prediction accuracy in polygenic risk scores and enhance discernibility of clusterings. Moreover, we compare our proposed smoothed PEP to seven state-of-the-art algorithms for sparse PCA and evaluate the accuracy of the obtained eigenvectors, their support recovery, and their runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13838v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca M. Hurwitz, Georg Hahn</dc:creator>
    </item>
    <item>
      <title>Reliability modeling and statistical analysis of accelerated degradation process with memory effects and unit-to-unit variability</title>
      <link>https://arxiv.org/abs/2310.18567</link>
      <description>arXiv:2310.18567v4 Announce Type: replace 
Abstract: A reasonable description of the degradation process is essential for credible reliability assessment in accelerated degradation testing. Existing methods usually use Markovian stochastic processes to describe the degradation process. However, degradation processes of some products are non-Markovian due to the interaction with environments. Misinterpretation of the degradation pattern may lead to biased reliability evaluations. Besides, owing to the differences in materials and manufacturing processes, products from the same population exhibit diverse degradation paths, further increasing the difficulty of accurate reliability estimation. To address the above issues, this paper proposes an accelerated degradation model incorporating memory effects and unit-to-unit variability. The memory effect in the degradation process is captured by the fractional Brownian motion, which reflects the non-Markovian characteristic of degradation. The unit-to-unit variability is considered in the acceleration model to describe diverse degradation paths. Then, lifetime and reliability under normal operating conditions are presented. Furthermore, to give an accurate estimation of the memory effect, a new statistical analysis method based on the expectation maximization algorithm is devised. The effectiveness of the proposed method is verified by a simulation case and a real-world tuner reliability analysis case. The code for the simulation case is publicly available at https://github.com/dirge1/FBM_ADT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18567v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.apm.2024.115788</arxiv:DOI>
      <arxiv:journal_reference>Applied Mathematical Modelling, 2024: 115788</arxiv:journal_reference>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li, Wenrui Xie</dc:creator>
    </item>
    <item>
      <title>Comparison of global sensitivity analysis methods for a fire spread model with a segmented characteristic</title>
      <link>https://arxiv.org/abs/2407.17718</link>
      <description>arXiv:2407.17718v2 Announce Type: replace 
Abstract: Global sensitivity analysis (GSA) can provide rich information for controlling output uncertainty. In practical applications, segmented models are commonly used to describe an abrupt model change. For segmented models, the complicated uncertainty propagation during the transition region may lead to different importance rankings of different GSA methods. If an unsuitable GSA method is applied, misleading results will be obtained, resulting in suboptimal or even wrong decisions. In this paper, four GSA indices, i.e., Sobol index, mutual information, delta index and PAWN index, are applied for a segmented fire spread model (Dry Eucalypt). The results show that four GSA indices give different importance rankings during the transition region since segmented characteristics affect different GSA indices in different ways. We suggest that analysts should rely on the results of different GSA indices according to their practical purpose, especially when making decisions for segmented models during the transition region. All of our source codes are publicly available at https://github.com/dirge1/GSA_segmented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17718v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2024.10.012</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, 2024: (229)304-318</arxiv:journal_reference>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li</dc:creator>
    </item>
    <item>
      <title>Examining Differential Item Functioning (DIF) in Self-Reported Health Survey Data: Via Multilevel Modeling</title>
      <link>https://arxiv.org/abs/2408.13702</link>
      <description>arXiv:2408.13702v3 Announce Type: replace 
Abstract: Few health-related constructs or measures have received a critical evaluation in terms of measurement equivalence, such as self-reported health survey data. Differential item functioning (DIF) analysis is crucial for evaluating measurement equivalence in self-reported health surveys, which are often hierarchical in structure. Traditional single-level DIF methods in this case fall short, making multilevel models a better alternative. We highlight the benefits of multilevel modeling for DIF analysis, when applying a health survey data set to multilevel binary logistic regression (for analyzing binary response data) and multilevel multinominal logistic regression (for analyzing polytomous response data), and comparing them with their single-level counterparts. Our findings show that multilevel models fit better and explain more variance than single-level models. This article is expected to raise awareness of multilevel modeling and help healthcare researchers and practitioners understand the use of multilevel modeling for DIF analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13702v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11136-025-03936-9</arxiv:DOI>
      <dc:creator>Dandan Chen Kaptur, Yiqing Liu, Bradley Kaptur, Nicholas Peterman, Jinming Zhang, Justin Kern, Carolyn Anderson</dc:creator>
    </item>
    <item>
      <title>A Bayesian framework for analyzing alleged cheating in sports through hidden codes, with applications to bridge and baseball</title>
      <link>https://arxiv.org/abs/2409.08172</link>
      <description>arXiv:2409.08172v4 Announce Type: replace 
Abstract: We develop a statistical framework to evaluate evidence of alleged cheating involving illegal signaling in sports from a forensic perspective. We explain why, instead of a frequentist procedure, a Bayesian approach is called for. We apply this framework to cases of alleged cheating in professional bridge and professional baseball. The diversity of these applications illustrates the generality of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08172v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.serev.2025.100050</arxiv:DOI>
      <dc:creator>Aafko Boonstra, Ronald Meester</dc:creator>
    </item>
    <item>
      <title>Can AI Detect Wash Trading? Evidence from NFTs</title>
      <link>https://arxiv.org/abs/2311.18717</link>
      <description>arXiv:2311.18717v3 Announce Type: replace-cross 
Abstract: Existing studies on crypto wash trading often use indirect statistical methods or leaked private data, both with inherent limitations. This paper leverages public on-chain NFT data for a more direct and granular estimation. Analyzing three major exchanges, we find that ~38% (30-40%) of trades and ~60% (25-95%) of traded value likely involve manipulation, with significant variation across exchanges. This direct evidence enables a critical reassessment of existing indirect methods, identifying roundedness-based regressions \`a la Cong et al. (2023) as most promising, though still error-prone in the NFT setting. To address this, we develop an AI-based estimator that integrates these regressions in a machine learning framework, significantly reducing both exchange- and trade-level estimation errors in NFT markets (and beyond).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18717v3</guid>
      <category>econ.GN</category>
      <category>cs.CR</category>
      <category>cs.MA</category>
      <category>q-fin.EC</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brett Hemenway Falk, Gerry Tsoukalas, Niuniu Zhang</dc:creator>
    </item>
    <item>
      <title>Decomposing Global Bank Network Connectedness: What is Common, Idiosyncratic and When?</title>
      <link>https://arxiv.org/abs/2402.02482</link>
      <description>arXiv:2402.02482v2 Announce Type: replace-cross 
Abstract: We propose a novel approach to estimate high-dimensional global bank network connectedness in both the time and frequency domains. By employing a factor model with sparse VAR idiosyncratic components, we decompose system-wide connectedness (SWC) into two key drivers: (i) common component shocks and (ii) idiosyncratic shocks. We also provide bootstrap confidence bands for all SWC measures. Furthermore, spectral density estimation allows us to disentangle SWC into short-, medium-, and long-term frequency responses to these shocks. We apply our methodology to two datasets of daily stock price volatilities for over 90 global banks, spanning the periods 2003-2013 and 2014-2023. Our empirical analysis reveals that SWC spikes during global crises, primarily driven by common component shocks and their short term effects. Conversely, in normal times, SWC is largely influenced by idiosyncratic shocks and medium-term dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02482v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Krampe, Luca Margaritella</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v4 Announce Type: replace-cross 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators use machine learning to fit the relationship between a binary win/loss outcome variable and certain game-state variables. To illustrate just how difficult it is to accurately fit such a model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. Further, to achieve approximately valid marginal coverage, win probability confidence intervals need to be substantially wide. Concisely, these are high variance estimators subject to substantial uncertainty. Our findings are not unique to the particular application of estimating win probability; they are broadly applicable across sports analytics, as myriad other sports datasets are clustered into groups of observations that share the same outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in SAR Imaging Based on MSTAR Data</title>
      <link>https://arxiv.org/abs/2410.03816</link>
      <description>arXiv:2410.03816v2 Announce Type: replace-cross 
Abstract: The statistical analysis of land clutter for Synthetic Aperture Radar (SAR) imaging has become an increasingly important subject for research and investigation. It is also absolutely necessary for designing robust algorithms capable of performing the task of target detection in the background clutter. Any attempt to extract the energy of the desired targets from the land clutter requires complete knowledge of the statistical properties of the background clutter. In this paper, the spatial as well as the temporal characteristics of the land clutter are studied. Since the data for each image has been collected based on a different aspect angle; therefore, the temporal analysis contains variation in the aspect angle. Consequently, the temporal analysis includes the characteristics of the radar cross section with respect to the aspect angle based on which the data has been collected. In order to perform the statistical analysis, several well-known and relevant distributions, namely, Weibull, Log-normal, Gamma, and Rayleigh are considered as prime candidates to model the land clutter. The goodness-of-fit test is based on the Kullback-Leibler (KL) Divergence metric. The detailed analysis presented in this paper demonstrates that the Weibull distribution is a more accurate fit for the temporal-aspect-angle statistical analysis while the Rayleigh distribution models the spatial characteristics of the background clutter with higher accuracy. Finally, based on the aforementioned statistical analyses and by utilizing the Constant False Alarm Rate (CFAR) algorithm, we perform target detection in land clutter. The overall verification of the analysis is performed by exploiting the Moving and Stationary Target Acquisition and Recognition (MSTAR) data-set, which has been collected in spotlight mode at X-band, and the results are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03816v2</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shahrokh Hamidi</dc:creator>
    </item>
    <item>
      <title>Deep Optimal Sensor Placement for Black Box Stochastic Simulations</title>
      <link>https://arxiv.org/abs/2410.12036</link>
      <description>arXiv:2410.12036v2 Announce Type: replace-cross 
Abstract: Selecting cost-effective optimal sensor configurations for subsequent inference of parameters in black-box stochastic systems faces significant computational barriers. We propose a novel and robust approach, modelling the joint distribution over input parameters and solution with a joint energy-based model, trained on simulation data. Unlike existing simulation-based inference approaches, which must be tied to a specific set of point evaluations, we learn a functional representation of parameters and solution. This is used as a resolution-independent plug-and-play surrogate for the joint distribution, which can be conditioned over any set of points, permitting an efficient approach to sensor placement. We demonstrate the validity of our framework on a variety of stochastic problems, showing that our method provides highly informative sensor locations at a lower computational cost compared to conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12036v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Camera ready version for the 28th International Conference on Artificial Intelligence and Statistics (AISTATS 2025)</arxiv:journal_reference>
      <dc:creator>Paula Cordero-Encinar, Tobias Schr\"oder, Peter Yatsyshin, Andrew Duncan</dc:creator>
    </item>
    <item>
      <title>Marginally interpretable spatial logistic regression with bridge processes</title>
      <link>https://arxiv.org/abs/2412.04744</link>
      <description>arXiv:2412.04744v2 Announce Type: replace-cross 
Abstract: In including random effects to account for dependent observations, the odds ratio interpretation of logistic regression coefficients is changed from population-averaged to subject-specific. This is unappealing in many applications, motivating a rich literature on methods that maintain the marginal logistic regression structure without random effects, such as generalized estimating equations. However, for spatial data, random effect approaches are appealing in providing a full probabilistic characterization of the data that can be used for prediction. We propose a new class of spatial logistic regression models that maintain both population-averaged and subject-specific interpretations through a novel class of bridge processes for spatial random effects. These processes are shown to have appealing computational and theoretical properties, including a scale mixture of normal representation. The new methodology is illustrated with simulations and an analysis of childhood malaria prevalence data in the Gambia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04744v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A review of Bayesian sensor-based estimation and uncertainty quantification of aerodynamic flows</title>
      <link>https://arxiv.org/abs/2502.20280</link>
      <description>arXiv:2502.20280v2 Announce Type: replace-cross 
Abstract: Many applications in aerodynamics depend on the use of sensors to estimate the evolving state of the flow. In particular, a wide variety of traditional and learning-based strategies for closed-loop control rely on some knowledge of the aerodynamic state in order to decide on actions. This estimation task is inherently accompanied by uncertainty due to the noisy measurements of sensors or the non-uniqueness of the underlying mapping, and knowledge of this uncertainty can be as important for decision-making as that of the state itself. The tracking of uncertainty is challenged by the often-nonlinear relationship between the sensor measurements and the flow state. For example, a collection of passing vortices leaves a footprint in wall pressure sensors that depends nonlinearly on the strengths and positions of the vortices. In this paper, we will review the recent body of work on flow estimation. We will discuss the basic tools of probability, including sampling and estimation, in the powerful setting of Bayesian inference and demonstrate these tools in static flow estimation examples. We will then proceed to unsteady examples and illustrate the application of sequential estimation, and particularly, the ensemble Kalman filter. Finally, we will discuss uncertainty quantification in neural network approximations of the mappings between sensor measurements and flow states. Recent aerodynamic applications of neural networks have shown that the flow state can be encoded into a very low-dimensional latent space, and we will discuss the implications of this encoding on uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20280v2</guid>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jeff D. Eldredge, Hanieh Mousavi</dc:creator>
    </item>
  </channel>
</rss>
