<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jul 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The role of gain neuromodulation in layer-5 pyramidal neurons</title>
      <link>https://arxiv.org/abs/2507.03222</link>
      <description>arXiv:2507.03222v1 Announce Type: new 
Abstract: Biological and artificial learning systems alike confront the plasticity-stability dilemma. In the brain, neuromodulators such as acetylcholine and noradrenaline relieve this tension by tuning neuronal gain and inhibitory gating, balancing segregation and integration of circuits. Fed by dense cholinergic and noradrenergic projections from the ascending arousal system, layer-5 pyramidal neurons in the cerebral cortex offer a relevant substrate for understanding these dynamics. When distal dendritic signals coincide with back-propagating action potentials, calcium plateaus turn a single somatic spike into a high-gain burst, and interneuron inhibition sculpts the output. These properties make layer-5 cells gain-tunable amplifiers that translate neuromodulatory cues into flexible cortical activity. To capture this mechanism we developed a two-compartment Izhikevich model for pyramidal neurons and single-compartment somatostatin (SOM) and parvalbumin (PV) interneurons, linked by Gaussian connectivity and spike-timing-dependent plasticity (STDP). The soma and apical dendrite are so coupled that somatic spikes back-propagate, while dendritic plateaus can switch the soma from regular firing to bursting by shifting reset and adaptation variables. We show that stronger dendritic drive or tighter coupling raise gain by increasing the likelihood of calcium-triggered somatic bursts. In contrast, dendritic-targeted inhibition suppresses gain, while somatic-targeted inhibition raises the firing threshold of neighboring neurons, thus gating neurons output. Notably, bursting accelerates STDP, supporting rapid synaptic reconfiguration and flexibility.This suggests that brief gain pulses driven by neuromodulators could serve as an adaptive two-timescale optimization mechanism, effectively modulating the synaptic weight updates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03222v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Rodriguez-Garcia, Christopher J. Whyte, Brandon R. Munn, Jie Mei, James M. Shine, Srikanth Ramaswamy</dc:creator>
    </item>
    <item>
      <title>Statistical-Spatial Model for Motor Potentials Evoked Through Transcranial Magnetic Stimulation for the Development of Closed-Loop Procedures</title>
      <link>https://arxiv.org/abs/2507.03416</link>
      <description>arXiv:2507.03416v1 Announce Type: new 
Abstract: The primary motor cortex appears to be in the center of transcranial magnetic stimulation (TMS). It is one of few locations that provide directly observable responses, and its physiology serves as model or reference for almost all other TMS targets, e.g., through the motor threshold and spatial targeting relative to its position. It furthermore sets the safety limits for the entire brain. Its easily detectable responses have led to closed-loop methods for a range of aspects, e.g., for automated thresholding, amplitude tracking, and targeting. The high variability of brain stimulation methods would substantially benefit from fast unbiased closed-loop methods. However, the development of more potent methods would early on in the design phase require proper models that allowed tuning and testing with sufficient without a high number of experiments, which are time-consuming and expensive or even impossible at the needed scale. On the one hand, theoretical researchers without access to experiments miss realistic spatial response models of brain stimulation to develop better methods. On the other hand, subjects should potentially not be exposed to early closed-loop-methods without sufficient prior testing as not yet well tuned feed-back as needed for closed-loop operation is known to erratic behavior.
  To bridge this gap, we developed a digital-twin-style population model that generates motor evoked potentials in response to virtual stimuli and includes statistical information on spatial (coil position and orientation) as well as recruitment in the population to represent inter- and intra-individual variability. The model allows users to simulate different subjects and millions of runs for software-in-the loop testing. The model includes all code to stimulate further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03416v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Farahmandrad, Stefan Goetz</dc:creator>
    </item>
    <item>
      <title>Entropy measures as indicators of connectivity paths in the human brain</title>
      <link>https://arxiv.org/abs/2507.04442</link>
      <description>arXiv:2507.04442v1 Announce Type: new 
Abstract: How does the information flow between different brain regions during various stimuli? This is the question we aim to address by studying complex cognitive paradigms in terms of Information Theory. To assess creativity and the emergence of patterns from a Shannon perspective, we applied a range of tools, including Entropy Density, Effective Measure Complexity, and the Lempel-Ziv distance. These entropic tools enable the detection of both linear and non-linear dynamics without relying on pre-established parameters, models, or prior assumptions about the data. To identify connections between different brain regions, we analyse task-based fMRI data from subjects during motor, working memory, emotion recognition, and language stimuli to gain insight into these complex cognitive processes. Since this method does not rely on prior knowledge, it is particularly well-suited for exploratory research, facilitating the discovery of previously unidentified connections or patterns in the brain. The capacity to identify non-linear dynamics is especially important for studying brain connectivity, as the brain exhibits significant non-linear interactions across multiple functional levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04442v1</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ania Mesa-Rodr\'iguez, Ernesto Estevez-Rams, Holger Kantz</dc:creator>
    </item>
    <item>
      <title>Lilith: Developmental Modular LLMs with Chemical Signaling</title>
      <link>https://arxiv.org/abs/2507.04575</link>
      <description>arXiv:2507.04575v1 Announce Type: new 
Abstract: Current paradigms in Artificial Intelligence rely on layers of feedforward networks which model brain activity at the neuronal level. We conjecture that expanding to the level of multiple brain regions with chemical signaling may be a productive step toward understanding the emergence of consciousness. We propose LILITH, a novel architecture that combines developmental training of modular language models with brain-inspired token-based communication protocols, mirroring chemical signaling in the brain. Our approach models distinct brain regions as specialized LLM modules including thinking, memory, sensory, and regulatory components that communicate through emergent token-based signaling protocols analogous to neurotransmitter networks. Unlike traditional pre-trained systems, LILITH would employ developmental training where untrained LLM architectures learn through simulated life experiences, developing communication pathways and cognitive abilities through environmental interaction and evolutionary optimization. This framework would enable direct empirical investigation of consciousness emergence using Integrated Information Theory metrics while providing unprecedented insight into inter-module signaling patterns during development. By optimizing for consciousness emergence rather than task performance, LILITH could provide insight into different emergent phenomena at multiple levels of neural correlates, contrasting neuronal-level processing with multi-region coordination dynamics. The goal of this paper is to put the idea forward while recognizing the substantial challenges in implementing such a system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04575v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohid Farooqi, Alejandro Comas-Leon</dc:creator>
    </item>
    <item>
      <title>Linking Homeostasis to Reinforcement Learning: Internal State Control of Motivated Behavior</title>
      <link>https://arxiv.org/abs/2507.04998</link>
      <description>arXiv:2507.04998v1 Announce Type: new 
Abstract: For living beings, survival depends on effective regulation of internal physiological states through motivated behaviors. In this perspective we propose that Homeostatically Regulated Reinforcement Learning (HRRL) as a framework to describe biological agents that optimize internal states via learned predictive control strategies, integrating biological principles with computational learning. We show that HRRL inherently produces multiple behaviors such as risk aversion, anticipatory regulation, and adaptive movement, aligning with observed biological phenomena. Its extension to deep reinforcement learning enables autonomous exploration, hierarchical behavior, and potential real-world robotic applications. We argue further that HRRL offers a biologically plausible foundation for understanding motivation, learning, and decision-making, with broad implications for artificial intelligence, neuroscience, and understanding the causes of psychiatric disorders, ultimately advancing our understanding of adaptive behavior in complex environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04998v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naoto Yoshida, Henning Sprekeler, Boris Gutkin</dc:creator>
    </item>
    <item>
      <title>Exploring a Specialized Eccentricity-Based Deep Neural Network Model to Simulate Visual Attention</title>
      <link>https://arxiv.org/abs/2507.05031</link>
      <description>arXiv:2507.05031v1 Announce Type: new 
Abstract: While visual attention theories abound, neurodevelopmental research remains constrained by infants' unreliable responses and limited attention spans. Through collaboration with Project Prakash, we accessed a unique population: patients gaining vision later in life. This cohort enables investigation of visual process development in cognitively mature, cooperative participants rather than infants. We collected data from pre-operation patients, post-operation patients tracked longitudinally (1, 3, 6, and 12 months), and neurotypical controls wearing blurred goggles matched to patients' post-surgical acuity. All participants performed a modified pre-attentive pop-out visual search task. We implemented the eccNET CNN model (Gupta et al., 2021) to simulate visual search asymmetry, subjecting it to identical tasks as human participants. Reaction time comparisons revealed both convergent and divergent patterns between human and model performance. These findings enabled systematic model ablations informed by human physiological constraints and developmental trajectories observed across patient groups. Critically, human-model divergences proved most informative, directing our focus toward specific architectural modifications needed to better approximate human visual development patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05031v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Manvi Jain</dc:creator>
    </item>
    <item>
      <title>The Application of Large Language Models on Major Depressive Disorder Support Based on African Natural Products</title>
      <link>https://arxiv.org/abs/2507.02947</link>
      <description>arXiv:2507.02947v1 Announce Type: cross 
Abstract: Major depressive disorder represents one of the most significant global health challenges of the 21st century, affecting millions of people worldwide and creating substantial economic and social burdens. While conventional antidepressant therapies have provided relief for many individuals, their limitations including delayed onset of action, significant side effects, and treatment resistance in a substantial portion of patients have prompted researchers and healthcare providers to explore alternative therapeutic approaches (Kasneci et al.). African traditional medicine, with its rich heritage of plant-based remedies developed over millennia, offers a valuable resource for developing novel antidepressant treatments that may address some of these limitations. This paper examines the integration of large language models with African natural products for depression support, combining traditional knowledge with modern artificial intelligence technology to create accessible, evidence-based mental health support systems.
  The research presented here encompasses a comprehensive analysis of African medicinal plants with documented antidepressant properties, their pharmacological mechanisms, and the development of an AI-powered support system that leverages DeepSeek's advanced language model capabilities. The system provides evidence-based information about African herbal medicines, their clinical applications, safety considerations, and therapeutic protocols while maintaining scientific rigor and appropriate safety standards. Our findings demonstrate the potential for large language models to serve as bridges between traditional knowledge and modern healthcare, offering personalized, culturally appropriate depression support that honors both traditional wisdom and contemporary medical understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02947v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linyan Zou</dc:creator>
    </item>
    <item>
      <title>Scientific Machine Learning of Chaotic Systems Discovers Governing Equations for Neural Populations</title>
      <link>https://arxiv.org/abs/2507.03631</link>
      <description>arXiv:2507.03631v1 Announce Type: cross 
Abstract: Discovering governing equations that describe complex chaotic systems remains a fundamental challenge in physics and neuroscience. Here, we introduce the PEM-UDE method, which combines the prediction-error method with universal differential equations to extract interpretable mathematical expressions from chaotic dynamical systems, even with limited or noisy observations. This approach succeeds where traditional techniques fail by smoothing optimization landscapes and removing the chaotic properties during the fitting process without distorting optimal parameters. We demonstrate its efficacy by recovering hidden states in the Rossler system and reconstructing dynamics from noise-corrupted electrical circuit data, where the correct functional form of the dynamics is recovered even when one of the observed time series is corrupted by noise 5x the magnitude of the true signal. We demonstrate that this method is capable of recovering the correct dynamics, whereas direct symbolic regression methods, such as SINDy, fail to do so with the given amount of data and noise. Importantly, when applied to neural populations, our method derives novel governing equations that respect biological constraints such as network sparsity - a constraint necessary for cortical information processing yet not captured in next-generation neural mass models - while preserving microscale neuronal parameters. These equations predict an emergent relationship between connection density and both oscillation frequency and synchrony in neural circuits. We validate these predictions using three intracranial electrode recording datasets from the medial entorhinal cortex, prefrontal cortex, and orbitofrontal cortex. Our work provides a pathway to develop mechanistic, multi-scale brain models that generalize across diverse neural architectures, bridging the gap between single-neuron dynamics and macroscale brain activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03631v1</guid>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony G. Chesebro, David Hofmann, Vaibhav Dixit, Earl K. Miller, Richard H. Granger, Alan Edelman, Christopher V. Rackauckas, Lilianne R. Mujica-Parodi, Helmut H. Strey</dc:creator>
    </item>
    <item>
      <title>QF: Quick Feedforward AI Model Training without Gradient Back Propagation</title>
      <link>https://arxiv.org/abs/2507.04300</link>
      <description>arXiv:2507.04300v1 Announce Type: cross 
Abstract: We propose Quick Feedforward (QF) Learning, a novel knowledge consolidation framework for transformer-based models that enables efficient transfer of instruction derived knowledge into model weights through feedforward activations without any gradient back propagation. Unlike traditional finetuning, QF updates are computed in closed form, require minimal parameter modification, and preserve prior knowledge. Importantly, QF allows models to train and infer within the same runtime environment, making the process more resource efficient and closely aligned with how the human brain operates. Code and models are open sourced on GitHub. I hope QF Learning inspires a more efficient and brain-like paradigm for AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04300v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng Qi</dc:creator>
    </item>
    <item>
      <title>Self-consistent moment dynamics for networks of spiking neurons</title>
      <link>https://arxiv.org/abs/2507.05117</link>
      <description>arXiv:2507.05117v1 Announce Type: cross 
Abstract: A novel approach to moment closure problem is used to derive low dimensional laws for the dynamics of the moments of the membrane potential distribution in a population of spiking neurons. Using spectral expansion of the density equation we derive the recursive and nonlinear relation between the moments, such as the mean potential, and the population firing rates. The self-consistent dynamics found relies on the dominant eigenvalues of the evolution operator, tightly related to the moments of the single-neuron inter-spike interval distribution. Contrary to previous attempts our system can be applied both in noise- and drift-dominated regime, and both for weakly and strongly coupled population. We demonstrate the applicability of the theory for the case of a network of leaky integrate-and-fire neurons deriving closed analytical expressions. Truncating the mode decomposition to the first few more relevant moments, results to effectively describe the population dynamics both out-of-equilibrium and in response to strongly-varying inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05117v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianni Valerio Vinci, Roberto Benzi, Maurizio Mattia</dc:creator>
    </item>
    <item>
      <title>Phase codes emerge in recurrent neural networks optimized for modular arithmetic</title>
      <link>https://arxiv.org/abs/2310.07908</link>
      <description>arXiv:2310.07908v2 Announce Type: replace 
Abstract: Recurrent neural networks (RNNs) can implement complex computations by leveraging a range of dynamics, such as oscillations, attractors, and transient trajectories. A growing body of work has highlighted the emergence of phase codes, a type of oscillatory activity where information is encoded in the relative phase of network activity, in RNNs trained for working memory tasks. However, these studies rely on architectural constraints or regularization schemes that explicitly promote oscillatory solutions. Here, we investigate whether phase coding can emerge purely from task optimization by training continuous-time RNNs to perform a simple modular arithmetic task without oscillatory-promoting biases. We find that in the absence of such biases, RNNs can learn phase code solutions. Surprisingly, we also uncover a rich diversity of alternative solutions that solve our modular arithmetic task via qualitatively distinct dynamics and dynamical mechanisms. We map the solution space for our task and show that the phase code solution occupies a distinct region. These results suggest that phase coding can be a natural but not inevitable outcome of training RNNs on modular arithmetic, and highlight the diversity of solutions RNNs can learn to solve simple tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07908v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keith T. Murray</dc:creator>
    </item>
    <item>
      <title>A comprehensive framework for statistical testing of brain dynamics</title>
      <link>https://arxiv.org/abs/2505.02541</link>
      <description>arXiv:2505.02541v2 Announce Type: replace 
Abstract: We introduce a comprehensive statistical framework for analysing brain dynamics and testing their associations with behavioural, physiological and other non-imaging variables. Based on a generalisation of the Hidden Markov Model (HMM) - the Gaussian-Linear HMM - our open-source Python package supports multiple experimental paradigms, including task-based and resting-state studies, and addresses a wide range of questions in neuroscience and related scientific fields. The toolbox is available as both a Python library and a graphical interface so it can be used by researchers with or without programming experience. Statistical inference is performed using permutation-based methods and structured Monte Carlo resampling, and the framework can easily handle confounding variables, multiple testing corrections, and hierarchical relationships within the data, among other features. The package includes tools for intuitive visualisation of statistical results, along with comprehensive documentation and step-by-step tutorials. Altogether, it provides a broadly applicable, end-to-end pipeline for analysis of functional neural data and its dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02541v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nick Yao Larsen, Laura Paulsen, Anderson M. Winkler, Diego Vidaurre</dc:creator>
    </item>
  </channel>
</rss>
