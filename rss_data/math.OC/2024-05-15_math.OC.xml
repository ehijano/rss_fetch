<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 May 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Subspace Minimization Barzilai-Borwein Method for Multiobjective Optimization Problems</title>
      <link>https://arxiv.org/abs/2405.07996</link>
      <description>arXiv:2405.07996v1 Announce Type: new 
Abstract: Nonlinear conjugate gradient methods have recently garnered significant attention within the multiobjective optimization community. These methods aim to maintain consistency in conjugate parameters with their single-objective optimization counterparts. However, the preservation of the attractive conjugate property of search directions remains uncertain, even for quadratic cases, in multiobjective conjugate gradient methods. This loss of interpretability of the last search direction significantly limits the applicability of these methods. To shed light on the role of the last search direction, we introduce a novel approach called the subspace minimization Barzilai-Borwein method for multiobjective optimization problems (SMBBMO). In SMBBMO, each search direction is derived by optimizing a preconditioned Barzilai-Borwein subproblem within a two-dimensional subspace generated by the last search direction and the current Barzilai-Borwein descent direction. Furthermore, to ensure the global convergence of SMBBMO, we employ a modified Cholesky factorization on a transformed scale matrix, capturing the local curvature information of the problem within the two-dimensional subspace. Under mild assumptions, we establish both global and $Q$-linear convergence of the proposed method. Finally, comparative numerical experiments confirm the efficacy of SMBBMO, even when tackling large-scale and ill-conditioned problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07996v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Chen, Liping Tang. Xinmin Yang</dc:creator>
    </item>
    <item>
      <title>Preconditioned Nonlinear Conjugate Gradient Method for Real-time Interior-point Hyperelasticity</title>
      <link>https://arxiv.org/abs/2405.08001</link>
      <description>arXiv:2405.08001v1 Announce Type: new 
Abstract: The linear conjugate gradient method is widely used in physical simulation, particularly for solving large-scale linear systems derived from Newton's method. The nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization, which is extensively utilized in solving practical large-scale unconstrained optimization problems. However, it is rarely discussed in physical simulation due to the requirement of multiple vector-vector dot products. Fortunately, with the advancement of GPU-parallel acceleration techniques, it is no longer a bottleneck. In this paper, we propose a Jacobi preconditioned nonlinear conjugate gradient method for elastic deformation using interior-point methods. Our method is straightforward, GPU-parallelizable, and exhibits fast convergence and robustness against large time steps. The employment of the barrier function in interior-point methods necessitates continuous collision detection per iteration to obtain a penetration-free step size, which is computationally expensive and challenging to parallelize on GPUs. To address this issue, we introduce a line search strategy that deduces an appropriate step size in a single pass, eliminating the need for additional collision detection. Furthermore, we simplify and accelerate the computations of Jacobi preconditioning and Hessian-vector product for hyperelasticity and barrier function. Our method can accurately simulate objects comprising over 100,000 tetrahedra in complex self-collision scenarios at real-time speeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08001v1</guid>
      <category>math.OC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xing Shen, Runyuan Cai, Mengxiao Bi, Tangjie Lv</dc:creator>
    </item>
    <item>
      <title>Graphon Mean Field Games with A Representative Player: Analysis and Learning Algorithm</title>
      <link>https://arxiv.org/abs/2405.08005</link>
      <description>arXiv:2405.08005v1 Announce Type: new 
Abstract: We propose a discrete-time graphon game formulation on continuous state and action spaces using a representative player to study stochastic games with heterogeneous interaction among agents. This formulation admits both philosophical and mathematical advantages, compared to a widely adopted formulation using a continuum of players. We prove the existence and uniqueness of the graphon equilibrium with mild assumptions, and show that this equilibrium can be used to construct an approximate solution for finite player game on networks, which is challenging to analyze and solve due to curse of dimensionality. An online oracle-free learning algorithm is developed to solve the equilibrium numerically, and sample complexity analysis is provided for its convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08005v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fuzhong Zhou, Chenyu Zhang, Xu Chen, Xuan Di</dc:creator>
    </item>
    <item>
      <title>Approximating the common fixed point of enriched interpolative matkowski type mapping in Banach space</title>
      <link>https://arxiv.org/abs/2405.08009</link>
      <description>arXiv:2405.08009v1 Announce Type: new 
Abstract: In the Normed space theory, the existence of fixed points is one of the main tools in improving efficiency of iterative algorithms in optimization, numerical analysis and various mathematical applications. This study introduces and investigates a recent concept termed "enriched interpolative Matkowski-type mapping". Building upon the well-established foundation of Matkowski-type contractions. This extension incorporates an interpolative enrichment mechanism, yielding a refined framework for analyzing contraction mappings. The proposed concept is motivated by the desire to enhance the convergence behavior and applicability of contraction mapping principles in various mathematical and scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08009v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akansha Tyagi, Sachin Vashistha</dc:creator>
    </item>
    <item>
      <title>Zero-Sum Games for piecewise deterministic Markov decision processes with risk-sensitive finite-horizon cost criterion</title>
      <link>https://arxiv.org/abs/2405.08012</link>
      <description>arXiv:2405.08012v1 Announce Type: new 
Abstract: This paper investigates the two-person zero-sum stochastic games for piece-wise deterministic Markov decision processes with risk-sensitive finite-horizon cost criterion on a general state space. Here, the transition and cost/reward rates are allowed to be un-unbounded from below and above. Under some mild conditions, we show the existence of the value of the game and an optimal randomized Markov saddle-point equilibrium in the class of all admissible feedback strategies. By studying the corresponding risk-sensitive finite-horizon optimal differential equations out of a class of possibly unbounded functions, to which the extended Feynman-Kac formula is also justified to hold, we obtain our required results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08012v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subrata Golui</dc:creator>
    </item>
    <item>
      <title>Autonomous Sparse Mean-CVaR Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2405.08047</link>
      <description>arXiv:2405.08047v1 Announce Type: new 
Abstract: The $\ell_0$-constrained mean-CVaR model poses a significant challenge due to its NP-hard nature, typically tackled through combinatorial methods characterized by high computational demands. From a markedly different perspective, we propose an innovative autonomous sparse mean-CVaR portfolio model, capable of approximating the original $\ell_0$-constrained mean-CVaR model with arbitrary accuracy. The core idea is to convert the $\ell_0$ constraint into an indicator function and subsequently handle it through a tailed approximation. We then propose a proximal alternating linearized minimization algorithm, coupled with a nested fixed-point proximity algorithm (both convergent), to iteratively solve the model. Autonomy in sparsity refers to retaining a significant portion of assets within the selected asset pool during adjustments in pool size. Consequently, our framework offers a theoretically guaranteed approximation of the $\ell_0$-constrained mean-CVaR model, improving computational efficiency while providing a robust asset selection scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08047v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>q-fin.PM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizun Lin, Yangyu Zhang, Zhao-Rong Lai, Cheng Li</dc:creator>
    </item>
    <item>
      <title>Power of $\ell_1$-Norm Regularized Kaczmarz Algorithms for High-Order Tensor Recovery</title>
      <link>https://arxiv.org/abs/2405.08275</link>
      <description>arXiv:2405.08275v1 Announce Type: new 
Abstract: Tensors serve as a crucial tool in the representation and analysis of complex, multi-dimensional data. As data volumes continue to expand, there is an increasing demand for developing optimization algorithms that can directly operate on tensors to deliver fast and effective computations. Many problems in real-world applications can be formulated as the task of recovering high-order tensors characterized by sparse and/or low-rank structures. In this work, we propose novel Kaczmarz algorithms with a power of the $\ell_1$-norm regularization for reconstructing high-order tensors by exploiting sparsity and/or low-rankness of tensor data. In addition, we develop both a block and an accelerated variant, along with a thorough convergence analysis of these algorithms. A variety of numerical experiments on both synthetic and real-world datasets demonstrate the effectiveness and significant potential of the proposed methods in image and video processing tasks, such as image sequence destriping and video deconvolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08275v1</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katherine Henneberger, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Flight Path Optimization with Optimal Control Method</title>
      <link>https://arxiv.org/abs/2405.08306</link>
      <description>arXiv:2405.08306v1 Announce Type: new 
Abstract: This paper is based on a crucial issue in the aviation world: how to optimize the trajectory and controls given to the aircraft in order to optimize flight time and fuel consumption. This study aims to provide elements of a response to this problem and to define, under certain simplifying assumptions, an optimal response, using Constrained Finite Time Optimal Control(CFTOC). The first step is to define the dynamic model of the aircraft in accordance with the controllable inputs and wind disturbances. Then we will identify a precise objective in terms of optimization and implement an optimization program to solve it under the circumstances of simulated real flight situation. Finally, the optimization result is validated and discussed by different scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08306v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaofeng Su, Xi Cheng, Siyuan Feng, Ke Liu, Jilin Song, Jianan Chen, Chen Zhu, Hui Lin</dc:creator>
    </item>
    <item>
      <title>A Riemannian Proximal Newton-CG Method</title>
      <link>https://arxiv.org/abs/2405.08365</link>
      <description>arXiv:2405.08365v1 Announce Type: new 
Abstract: Recently, a Riemannian proximal Newton method has been developed for optimizing problems in the form of $\min_{x\in\mathcal{M}} f(x) + \mu \|x\|_1$, where $\mathcal{M}$ is a compact embedded submanifold and $f(x)$ is smooth. Although this method converges superlinearly locally, global convergence is not guaranteed. The existing remedy relies on a hybrid approach: running a Riemannian proximal gradient method until the iterate is sufficiently accurate and switching to the Riemannian proximal Newton method. This existing approach is sensitive to the switching parameter. This paper proposes a Riemannian proximal Newton-CG method that merges the truncated conjugate gradient method with the Riemannian proximal Newton method. The global convergence and local superlinear convergence are proven. Numerical experiments show that the proposed method outperforms other state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08365v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wen Huang, Wutao Si</dc:creator>
    </item>
    <item>
      <title>Detecting and Handling Reflection Symmetries in Mixed-Integer (Nonlinear) Programming</title>
      <link>https://arxiv.org/abs/2405.08379</link>
      <description>arXiv:2405.08379v1 Announce Type: new 
Abstract: Symmetries in mixed-integer (nonlinear) programs (MINLP), if not handled appropriately, are known to negatively impact the performance of (spatial) branch-and-bound algorithms. Usually one thus tries to remove symmetries from the problem formulation or is relying on a solver that automatically detects and handles symmetries. While modelers of a problem can handle various kinds of symmetries, automatic symmetry detection and handling is mostly restricted to permutation symmetries. This article therefore develops techniques such that also black-box solvers can automatically detect and handle a broader class of symmetries.
  Inspired from geometric packing problems such as the kissing number problem, we focus on reflection symmetries of MINLPs. We develop a generic and easily applicable framework that allows to automatically detect reflection symmetries for MINLPs. To handle this broader class of symmetries, we discuss generalizations of state-of-the-art methods for permutation symmetries, and develop dedicated symmetry handling methods for special reflection symmetry groups. Our symmetry detection framework has been implemented in the open-source solver SCIP and we provide a comprehensive discussion of the implementation. The article concludes with a detailed numerical evaluation of our symmetry handling methods when solving MINLPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08379v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Hojny</dc:creator>
    </item>
    <item>
      <title>A constraint-based approach to function interpolation, with application to performance estimation for weakly convex optimisation</title>
      <link>https://arxiv.org/abs/2405.08405</link>
      <description>arXiv:2405.08405v1 Announce Type: new 
Abstract: We propose a novel approach to obtain interpolation constraints for a wide range of function classes, i.e. necessary and sufficient constraints that a set of points, functions values and (sub)gradients must satisfy to ensure the existence of a global function of the class considered, consistent with this set. The derivation of such constraints is crucial for instance in the performance analysis of optimization methods, since obtaining a priori tight performance guarantees requires using a tight description of function classes of interest. Our method allows setting aside all analytic properties of the function class to work only at an algebraic level, and to easily obtain counterexamples when a condition characterizing a function class cannot serve as an interpolation constraint. As an illustration, we provide interpolation constraints for a class of non convex non smooth functions: weakly convex functions with bounded subgradients, and rely on these new interpolation constraints to outperform state of the art bounds on the performance of the subgradient method on this class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08405v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Rubbens, Julien M. Hendrickx</dc:creator>
    </item>
    <item>
      <title>Effective Front-Descent Algorithms with Convergence Guarantees</title>
      <link>https://arxiv.org/abs/2405.08450</link>
      <description>arXiv:2405.08450v1 Announce Type: new 
Abstract: In this manuscript, we address continuous unconstrained optimization problems and we discuss descent type methods for the reconstruction of the Pareto set. Specifically, we analyze the class of Front Descent methods, which generalizes the Front Steepest Descent algorithm allowing the employment of suitable, effective search directions (e.g., Newton, Quasi-Newton, Barzilai-Borwein). We provide a deep characterization of the behavior and the mechanisms of the algorithmic framework, and we prove that, under reasonable assumptions, standard convergence results and some complexity bounds hold for the generalized approach. Moreover, we prove that popular search directions can indeed be soundly used within the framework. Then, we provide a completely novel type of convergence results, concerning the sequence of sets produced by the procedure. In particular, iterate sets are shown to asymptotically approach stationarity for all of their points; additionally, in finite precision settings, the sets are shown to only be enriched through exploration steps in later iterations, and suitable stopping conditions can be devised. Finally, the results from a large experimental benchmark show that the proposed class of approaches far outperforms state-of-the-art methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08450v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Lapucci, Pierluigi Mansueto, Davide Pucci</dc:creator>
    </item>
    <item>
      <title>Doubly relaxed forward-Douglas--Rachford splitting for the sum of two nonconvex and a DC function</title>
      <link>https://arxiv.org/abs/2405.08485</link>
      <description>arXiv:2405.08485v1 Announce Type: new 
Abstract: In this paper, we consider a class of structured nonconvex nonsmooth optimization problems whose objective function is the sum of three nonconvex functions, one of which is expressed in a difference-of-convex (DC) form. This problem class covers several important structures in the literature including the sum of three functions and the general DC program. We propose a splitting algorithm and prove the subsequential convergence to a stationary point of the problem. The full sequential convergence, along with convergence rates for both the iterates and objective function values, is then established without requiring differentiability of the concave part. Our analysis not only extends but also unifies and improves recent convergence analyses in nonconvex settings. We benchmark our proposed algorithm with notable algorithms in the literature to show its competitiveness on both synthetic data and real power system load data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08485v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minh N. Dao, Tan Nhat Pham, Phan Thanh Tung</dc:creator>
    </item>
    <item>
      <title>Cryptography-Based Privacy-Preserving Method for Distributed Optimization over Time-Varying Directed Graphs with Enhanced Efficiency</title>
      <link>https://arxiv.org/abs/2405.08518</link>
      <description>arXiv:2405.08518v1 Announce Type: new 
Abstract: In this paper, we study the privacy-preserving distributed optimization problem, aiming to prevent attackers from stealing the private information of agents. For this purpose, we propose a novel privacy-preserving algorithm based on the Advanced Encryption Standard (AES), which is both secure and computationally efficient. By appropriately constructing the underlying weight matrices, our algorithm can be applied to time-varying directed networks. We show that the proposed algorithm can protect an agent's privacy if the agent has at least one legitimate neighbor at the initial iteration. Under the assumption that the objective function is strongly convex and Lipschitz smooth, we rigorously prove that the proposed algorithm has a linear convergence rate. Finally, the effectiveness of the proposed algorithm is demonstrated by numerical simulations of the canonical sensor fusion problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08518v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Liu, Furan Xie, Li Chai</dc:creator>
    </item>
    <item>
      <title>Accelerated Alternating Direction Method of Multipliers Gradient Tracking for Distributed Optimization</title>
      <link>https://arxiv.org/abs/2405.08590</link>
      <description>arXiv:2405.08590v1 Announce Type: new 
Abstract: This paper presents a novel accelerated distributed algorithm for unconstrained consensus optimization over static undirected networks. The proposed algorithm combines the benefits of acceleration from momentum, the robustness of the alternating direction method of multipliers, and the computational efficiency of gradient tracking to surpass existing state-of-the-art methods in convergence speed, while preserving their computational and communication cost. First, we prove that, by applying momentum on the average dynamic consensus protocol over the estimates and gradient, we can study the algorithm as an interconnection of two singularly perturbed systems: the outer system connects the consensus variables and the optimization variables, and the inner system connects the estimates of the optimum and the auxiliary optimization variables. Next, we prove that, by adding momentum to the auxiliary dynamics, our algorithm always achieves faster convergence than the achievable linear convergence rate for the non-accelerated alternating direction method of multipliers gradient tracking algorithm case. Through simulations, we numerically show that our accelerated algorithm surpasses the existing accelerated and non-accelerated distributed consensus first-order optimization protocols in convergence speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08590v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo Sebasti\'an, Mauro Franceschelli, Andrea Gasparri, Eduardo Montijano, Carlos Sag\"u\'es</dc:creator>
    </item>
    <item>
      <title>Stabilization and Optimal Control of Interconnected SDE - Scalar PDE System</title>
      <link>https://arxiv.org/abs/2405.08600</link>
      <description>arXiv:2405.08600v1 Announce Type: new 
Abstract: In this paper, we design a controller for an interconnected system consisting of a linear Stochastic Differential Equation (SDE) actuated through a linear hyperbolic Partial Differential Equation (PDE). Our approach aims to minimize the variance of the state of the SDE component. We leverage a backstepping technique to transform the original PDE into an uncoupled stochastic PDE. As such, we reformulate our initial problem as the control of a delayed SDE with a non-deterministic drift. Under standard controllability assumptions, we design a controller steering the mean of the states to zero while keeping its covariance bounded. As final step, we address the optimal control of the delayed SDE employing Artstein's transformation and Linear Quadratic stochastic control techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08600v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Velho, Jean Auriol, Riccardo Bonalli, Islam Boussaada</dc:creator>
    </item>
    <item>
      <title>Approaches to iterative algorithms for solving nonlinear equations with an application in tomographic absorption spectroscopy</title>
      <link>https://arxiv.org/abs/2405.08635</link>
      <description>arXiv:2405.08635v1 Announce Type: new 
Abstract: In this paper we propose an approach for solving systems of nonlinear equations without computing function derivatives. Motivated by the application area of tomographic absorption spectroscopy, which is a highly-nonlinear problem with variables coupling, we consider a situation where straightforward translation to a fixed point problem is not possible because the operators that represent the relevant systems of nonlinear equations are not self-mappings, i.e., they operate between spaces of different dimensions. To overcome this difficulty we suggest an "alternating common fixed points algorithm" that acts alternatingly on the different vector variables. This approach translates the original problem to a common fixed point problem for which iterative algorithms are abound and exhibits a viable alternative to translation to an optimization problem, which usually requires derivatives information. However, to apply any of these iterative algorithms requires to ascertain the conditions that appear in their convergence theorems. To circumvent the need to verify conditions for convergence, we propose and motivate a derivative-free algorithm that better suits the tomographic absorption spectroscopy problem at hand and is even further improved by applying to it the superiorization approach. This is presented along with experimental results that demonstrate our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08635v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. J. Arag\'on-Artacho, W. Cai, Y. Censor, A. Gibali, C. Shui, D. Torregrosa-Bel\'en</dc:creator>
    </item>
    <item>
      <title>Metric lines in Engel-type groups and the nilpotent group $N_{6,3,1}$</title>
      <link>https://arxiv.org/abs/2405.08186</link>
      <description>arXiv:2405.08186v1 Announce Type: cross 
Abstract: Given a sub-Riemannian manifold, which geodesics are "metric lines" (i.e. globally minimizing geodesics)? This article takes the first steps in answering this question for "arbitrary rank" and "non-integrable" Carnot groups. We classify the metric lines of the Engel-type groups $Eng(n)$ (Theorem B) and give a partial classification for the group of four-by-four nilpotent triangular matrices $N_{6,3,1}$ (Theorem C). The sub-Riamannian structure of the former group is defined on a non-integrable distribution of rank $n+1$ and the geodesic flow of the latter group is not algebraically integrable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08186v1</guid>
      <category>math.DG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Bravo-Doddoli</dc:creator>
    </item>
    <item>
      <title>Thompson Sampling for Infinite-Horizon Discounted Decision Processes</title>
      <link>https://arxiv.org/abs/2405.08253</link>
      <description>arXiv:2405.08253v1 Announce Type: cross 
Abstract: We model a Markov decision process, parametrized by an unknown parameter, and study the asymptotic behavior of a sampling-based algorithm, called Thompson sampling. The standard definition of regret is not always suitable to evaluate a policy, especially when the underlying chain structure is general. We show that the standard (expected) regret can grow (super-)linearly and fails to capture the notion of learning in realistic settings with non-trivial state evolution. By decomposing the standard (expected) regret, we develop a new metric, called the expected residual regret, which forgets the immutable consequences of past actions. Instead, it measures regret against the optimal reward moving forward from the current period. We show that the expected residual regret of the Thompson sampling algorithm is upper bounded by a term which converges exponentially fast to 0. We present conditions under which the posterior sampling error of Thompson sampling converges to 0 almost surely. We then introduce the probabilistic version of the expected residual regret and present conditions under which it converges to 0 almost surely. Thus, we provide a viable concept of learning for sampling algorithms which will serve useful in broader settings than had been considered previously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08253v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Adelman, Cagla Keceli, Alba V. Olivares Nadal</dc:creator>
    </item>
    <item>
      <title>On saturation of the discrepancy principle for nonlinear Tikhonov regularization in Hilbert spaces</title>
      <link>https://arxiv.org/abs/2405.08269</link>
      <description>arXiv:2405.08269v1 Announce Type: cross 
Abstract: In this paper we revisit the discrepancy principle for Tikhonov regularization of nonlinear ill-posed problems in Hilbert spaces and provide some new and improved saturation results under less restrictive conditions, comparing with the existing results in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08269v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qinian Jin</dc:creator>
    </item>
    <item>
      <title>Future Trends in the Design of Memetic Algorithms: the Case of the Linear Ordering Problem</title>
      <link>https://arxiv.org/abs/2405.08285</link>
      <description>arXiv:2405.08285v1 Announce Type: cross 
Abstract: The way heuristic optimizers are designed has evolved over the decades, as computing power has increased. Initially, trajectory metaheuristics used to shape the state of the art in many problems, whereas today, population-based mechanisms tend to be more effective.Such has been the case for the Linear Ordering Problem (LOP), a field in which strategies such as Iterated Local Search and Variable Neighborhood Search led the way during the 1990s, but which have now been surpassed by evolutionary and memetic schemes. This paper focuses on understanding how the design of LOP optimizers will change in the future, as computing power continues to increase, yielding two main contributions. On the one hand, a metaheuristic was designed that is capable of effectively exploiting a large amount of computational resources, specifically, computing power equivalent to what a recent core can output during runs lasting over four months. Our analysis of this aspect relied on parallelization, and allowed us to conclude that as the power of the computational resources increases, it will be necessary to boost the capacities of the intensification methods applied in the memetic algorithms to keep the population from stagnating. And on the other, the best-known results for today's most challenging set of instances (xLOLIB2) were significantly outperformed. Instances with sizes ranging from 300 to 1000 were analyzed, and new bounds were established that provide a frame of reference for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08285v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L\'azaro Lugo, Carlos Segura, Gara Miranda</dc:creator>
    </item>
    <item>
      <title>Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More</title>
      <link>https://arxiv.org/abs/2405.08424</link>
      <description>arXiv:2405.08424v1 Announce Type: cross 
Abstract: Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable. Karalias &amp; Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO. First, we concretize the targets for objective construction and derandomization with theoretical justification. Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08424v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanchen Bu, Hyeonsoo Jo, Soo Yong Lee, Sungsoo Ahn, Kijung Shin</dc:creator>
    </item>
    <item>
      <title>The distributed biased min-consensus protocol revisited: pre-specified finite time control strategies and small-gain based analysis</title>
      <link>https://arxiv.org/abs/2405.08599</link>
      <description>arXiv:2405.08599v1 Announce Type: cross 
Abstract: Unlike the classical distributed consensus protocols enabling the group of agents as a whole to reach an agreement regarding a certain quantity of interest in a distributed fashion, the distributed biased min-consensus protocol (DBMC) has been proven to generate advanced complexity pertaining to solving the shortest path problem. As such a protocol is commonly incorporated as the first step of a hierarchical architecture in real applications, e.g., robots path planning, management of dispersed computing services, an impedance limiting the application potential of DBMC lies in, the lack of results regarding to its convergence within a user-assigned time. In this paper, we first propose two control strategies ensuring the state error of DBMC decrease exactly to zero or a desired level manipulated by the user, respectively. To compensate the high feedback gains incurred by these two control strategies, this paper further investigates the nominal DBMC itself. By leveraging small gain based stability tools, this paper also proves the global exponential input-to-state stability of DBMC, outperforming its current stability results. Simulations have been provided to validate the efficacy of our theoretical result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08599v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanqiu Mo, He Wang</dc:creator>
    </item>
    <item>
      <title>Gradient Estimation and Variance Reduction in Stochastic and Deterministic Models</title>
      <link>https://arxiv.org/abs/2405.08661</link>
      <description>arXiv:2405.08661v1 Announce Type: cross 
Abstract: It seems that in the current age, computers, computation, and data have an increasingly important role to play in scientific research and discovery. This is reflected in part by the rise of machine learning and artificial intelligence, which have become great areas of interest not just for computer science but also for many other fields of study. More generally, there have been trends moving towards the use of bigger, more complex and higher capacity models. It also seems that stochastic models, and stochastic variants of existing deterministic models, have become important research directions in various fields. For all of these types of models, gradient-based optimization remains as the dominant paradigm for model fitting, control, and more. This dissertation considers unconstrained, nonlinear optimization problems, with a focus on the gradient itself, that key quantity which enables the solution of such problems.
  In chapter 1, we introduce the notion of reverse differentiation, a term which describes the body of techniques which enables the efficient computation of gradients. We cover relevant techniques both in the deterministic and stochastic cases. We present a new framework for calculating the gradient of problems which involve both deterministic and stochastic elements. In chapter 2, we analyze the properties of the gradient estimator, with a focus on those properties which are typically assumed in convergence proofs of optimization algorithms. Chapter 3 gives various examples of applying our new gradient estimator. We further explore the idea of working with piecewise continuous models, that is, models with distinct branches and if statements which define what specific branch to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08661v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.7298/pjz7-ef27</arxiv:DOI>
      <dc:creator>Ronan Keane</dc:creator>
    </item>
    <item>
      <title>An optimization-based construction procedure for function space based summation-by-parts operators on arbitrary grids</title>
      <link>https://arxiv.org/abs/2405.08770</link>
      <description>arXiv:2405.08770v1 Announce Type: cross 
Abstract: We introduce a novel construction procedure for one-dimensional summation-by-parts (SBP) operators. Existing construction procedures for FSBP operators of the form $D = P^{-1} Q$ proceed as follows: Given a boundary operator $B$, the norm matrix $P$ is first determined and then in a second step the complementary matrix $Q$ is calculated to finally get the FSBP operator $D$. In contrast, the approach proposed here determines the norm and complementary matrices, $P$ and $Q$, simultaneously by solving an optimization problem. The proposed construction procedure applies to classical SBP operators based on polynomial approximation and the broader class of function space SBP (FSBP) operators. According to our experiments, the presented approach yields a numerically stable construction procedure and FSBP operators with higher accuracy for diagonal norm difference operators at the boundaries than the traditional approach. Through numerical simulations, we highlight the advantages of our proposed technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08770v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Glaubitz, Jan Nordstr\"om, Philipp \"Offner</dc:creator>
    </item>
    <item>
      <title>Internal and String Stability of an Observer-based Controller for Vehicle Platooning under the MPF Topology</title>
      <link>https://arxiv.org/abs/2108.09497</link>
      <description>arXiv:2108.09497v3 Announce Type: replace 
Abstract: In this paper, we study the internal stability and string stability of a vehicle platoon under the constant time headway spacing (CTHS) policy and the multiple-predecessor-following (MPF) vehicle-to-vehicle information flow topology. More specifically, we depart from the conventional Proportional-Integral-Derivative (PID) controller design for such systems and we propose the design of an observer-based controller. For designing our observer-based controller, we first design a distributed observer, with which each follower estimates their position, speed and acceleration error with respect to the leader. The observer is designed by means of constructing an observer matrix whose parameters should be determined. Next, we simplify the design of the matrix of the observer in such a way that the design boils down to choosing a single scalar value; this design further simplifies the structure of the controller, whose simplicity enables the derivation of string stability conditions by means of a frequency response method. Subsequently, the string stability conditions for a given time headway, are transformed to conditions for the controller parameters. We obtain controller parameters that satisfy the stability conditions by designing a novel heuristic search algorithm. Furthermore, we extend the search algorithm by incorporating a bisection-like algorithm, which allows to obtain (within some deviation tolerance) the minimum available value of the time headway. Finally, we provide insights about how to finalize the observer-based controller parameters from above algorithms to avoid the peaking phenomenon. The performance of the proposed observer-based controller is demonstrated via illustrative examples. Additionally, a comparison with a widely-used PID controller for MPF topology shows that our proposed observer-based controller has better convergence performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.09497v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Jiang, Elham Abolfazli, Themistoklis Charalambous</dc:creator>
    </item>
    <item>
      <title>Combining Learning and Control for Data-driven Approaches of Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2109.09055</link>
      <description>arXiv:2109.09055v3 Announce Type: replace 
Abstract: Cyber-physical systems (CPS), in most instances, represent systems of systems with an informationally decentralized structure such as emerging mobility systems, networked control systems, sustainable manufacturing, smart power grids, power systems, mobility markets, social media platforms, cooperation of robots, and internet of things. To optimize the operation of such systems, we typically assume an ideal model. Such model-based control approaches cannot effectively facilitate optimal solutions with performance guarantees due to the discrepancy between the model and the actual CPS. On the other hand, in most CPS there is a large volume of data with a dynamic nature which is added to the system gradually in real time and not altogether in advance. Thus, traditional supervised learning approaches cannot always facilitate robust solutions using data derived offline. By contrast, applying reinforcement learning approaches directly to the actual CPS might impose significant implications on the safety and robust operation of the system. The overarching goal of the Information and Decision Science (IDS) Lab is to investigate how to circumvent these challenges by developing data-driven system approaches at the intersection of learning and control. The emphasis is on how to improve energy efficiency and reduce greenhouse gas emissions in applications related to emerging mobility systems, e.g., connected and automated vehicles (CAVs), shared mobility, sociotechnical systems, and smart cities, and thus contribute to the health of the planet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.09055v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Malikopoulos</dc:creator>
    </item>
    <item>
      <title>On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness Results and Improved Analysis</title>
      <link>https://arxiv.org/abs/2301.00712</link>
      <description>arXiv:2301.00712v5 Announce Type: replace 
Abstract: Bilevel optimization reveals the inner structure of otherwise oblique optimization problems, such as hyperparameter tuning, neural architecture search, and meta-learning. A common goal in bilevel optimization is to minimize a hyper-objective that implicitly depends on the solution set of the lower-level function. Although this hyper-objective approach is widely used, its theoretical properties have not been thoroughly investigated in cases where the lower-level functions lack strong convexity. In this work, we first provide hardness results to show that the goal of finding stationary points of the hyper-objective for nonconvex-convex bilevel optimization can be intractable for zero-respecting algorithms. Then we study a class of tractable nonconvex-nonconvex bilevel problems when the lower-level function satisfies the Polyak-{\L}ojasiewicz (PL) condition. We show a simple first-order algorithm can achieve better complexity bounds of $\tilde{\mathcal{O}}(\epsilon^{-2})$, $\tilde{\mathcal{O}}(\epsilon^{-4})$ and $\tilde{\mathcal{O}}(\epsilon^{-6})$ in the deterministic, partially stochastic, and fully stochastic setting respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00712v5</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesi Chen, Jing Xu, Jingzhao Zhang</dc:creator>
    </item>
    <item>
      <title>Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2301.06428</link>
      <description>arXiv:2301.06428v3 Announce Type: replace 
Abstract: We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06428v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesi Chen, Jing Xu, Luo Luo</dc:creator>
    </item>
    <item>
      <title>Globally optimal solutions to a class of fractional optimization problems based on proximity gradient algorithm</title>
      <link>https://arxiv.org/abs/2306.11286</link>
      <description>arXiv:2306.11286v2 Announce Type: replace 
Abstract: In this paper, we investigate a category of constrained fractional optimization problems that emerge in various practical applications. The objective function for this category is characterized by the ratio of a numerator and denominator, both being convex, semi-algebraic, Lipschitz continuous, and differentiable with Lipschitz continuous gradients over the constraint sets. The constrained sets associated with these problems are closed, convex, and semi-algebraic. We propose an efficient algorithm that is inspired by the proximal gradient method, and we provide a thorough convergence analysis. Our algorithm offers several benefits compared to existing methods. It requires only a single proximal gradient operation per iteration, thus avoiding the complicated inner-loop concave maximization usually required. Additionally, our method converges to a critical point without the typical need for a nonnegative numerator, and this critical point becomes a globally optimal solution with an appropriate condition. Our approach is adaptable to unbounded constraint sets as well. Therefore, our approach is viable for many more practical models. Numerical experiments show that our method not only reliably reaches ground-truth solutions in some model problems but also outperforms several existing methods in maximizing the Sharpe ratio with real-world financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11286v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizun Lin, Jian-Feng Cai, Zhao-Rong Lai, Cheng Li</dc:creator>
    </item>
    <item>
      <title>Unbalanced Optimal Transport and Maximum Mean Discrepancies: Interconnections and Rapid Evaluation</title>
      <link>https://arxiv.org/abs/2306.13618</link>
      <description>arXiv:2306.13618v3 Announce Type: replace 
Abstract: This contribution presents substantial computational advancements to compare measures even with varying masses. Specifically, we utilize the nonequispaced fast Fourier transform to accelerate the radial kernel convolution in unbalanced optimal transport approximation, built upon the Sinkhorn algorithm. We also present accelerated schemes for maximum mean discrepancies involving kernels. Our approaches reduce the arithmetic operations needed to compute distances from $\mathcal O(n^2)$ to $\mathcal O(n\log n)$, opening the door to handle large and high-dimensional datasets efficiently. Furthermore, we establish robust connections between transportation problems, encompassing Wasserstein distance and unbalanced optimal transport, and maximum mean discrepancies. This empowers practitioners with compelling rationale to opt for adaptable distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13618v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajmadan Lakshmanan, Alois Pichler</dc:creator>
    </item>
    <item>
      <title>SGD-type Methods with Guaranteed Global Stability in Nonsmooth Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2307.10053</link>
      <description>arXiv:2307.10053v3 Announce Type: replace 
Abstract: In this paper, we focus on providing convergence guarantees for variants of the stochastic subgradient descent (SGD) method in minimizing nonsmooth nonconvex functions. We first develop a general framework to establish global stability for general stochastic subgradient methods, where the corresponding differential inclusion admits a coercive Lyapunov function. We prove that, with sufficiently small stepsizes and controlled noises, the iterates asymptotically stabilize around the stable set of its corresponding differential inclusion. Then we introduce a scheme for developing SGD-type methods with regularized update directions for the primal variables. Based on our developed framework, we prove the global stability of our proposed scheme under mild conditions. We further illustrate that our scheme yields variants of SGD-type methods, which enjoy guaranteed convergence in training nonsmooth neural networks. In particular, by employing the sign map to regularize the update directions, we propose a novel subgradient method named the Sign-map Regularized SGD method (SRSGD). Preliminary numerical experiments exhibit the high efficiency of SRSGD in training deep neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10053v3</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nachuan Xiao, Xiaoyin Hu, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>T-semidefinite programming relaxation with third-order tensors for constrained polynomial optimization</title>
      <link>https://arxiv.org/abs/2402.08438</link>
      <description>arXiv:2402.08438v2 Announce Type: replace 
Abstract: We study T-semidefinite programming (SDP) relaxation for constrained polynomial optimization problems (POPs). T-SDP relaxation for unconstrained POPs was introduced by Zheng, Huang and Hu in 2022. In this work, we propose a T-SDP relaxation for POPs with polynomial inequality constraints and show that the resulting T-SDP relaxation formulated with third-order tensors can be transformed into the standard SDP relaxation with block-diagonal structures. The convergence of the T-SDP relaxation to the optimal value of a given constrained POP is established under moderate assumptions as the relaxation level increases. Additionally, the feasibility and optimality of the T-SDP relaxation are discussed. Numerical results illustrate that the proposed T-SDP relaxation enhances numerical efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08438v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Marumo, Sunyoung Kim, Makoto Yamashita</dc:creator>
    </item>
    <item>
      <title>Low-rank approximated Kalman-Bucy filters using Oja's principal component flow for linear time-invariant systems</title>
      <link>https://arxiv.org/abs/2403.03104</link>
      <description>arXiv:2403.03104v2 Announce Type: replace 
Abstract: The Kalman-Bucy filter is extensively utilized across various applications. However, its computational complexity increases significantly in large-scale systems. To mitigate this challenge, a low-rank approximated Kalman--Bucy filter was proposed, comprising Oja's principal component flow and a low-dimensional Riccati differential equation. Previously, the estimation error was confirmed solely for linear time-invariant systems with a symmetric system matrix. This study extends the application by eliminating the constraint on the symmetricity of the system matrix and describes the equilibrium points of the Oja flow along with their stability for general matrices. In addition, the domain of attraction for a set of stable equilibrium points is estimated. Based on these findings, we demonstrate that the low-rank approximated Kalman--Bucy filter with a suitable rank maintains a bounded estimation error covariance matrix if the system is controllable and observable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03104v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daiki Tsuzuki, Kentaro Ohki</dc:creator>
    </item>
    <item>
      <title>From Linear to Linearizable Optimization: A Novel Framework with Applications to Stationary and Non-stationary DR-submodular Optimization</title>
      <link>https://arxiv.org/abs/2405.00065</link>
      <description>arXiv:2405.00065v2 Announce Type: replace 
Abstract: This paper introduces the notion of upper linearizable/quadratizable functions, a class that extends concavity and DR-submodularity in various settings, including monotone and non-monotone cases over different convex sets. A general meta-algorithm is devised to convert algorithms for linear/quadratic maximization into ones that optimize upper quadratizable functions, offering a unified approach to tackling concave and DR-submodular optimization problems. The paper extends these results to multiple feedback settings, facilitating conversions between semi-bandit/first-order feedback and bandit/zeroth-order feedback, as well as between first/zeroth-order feedback and semi-bandit/bandit feedback. Leveraging this framework, new algorithms are derived using existing results as base algorithms for convex optimization, improving upon state-of-the-art results in various cases. Dynamic and adaptive regret guarantees are obtained for DR-submodular maximization, marking the first algorithms to achieve such guarantees in these settings. Notably, the paper achieves these advancements with fewer assumptions compared to existing state-of-the-art results, underscoring its broad applicability and theoretical contributions to non-convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00065v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Pedramfar, Vaneet Aggarwal</dc:creator>
    </item>
    <item>
      <title>Convergence Rates of Online Critic Value Function Approximation in Native Spaces</title>
      <link>https://arxiv.org/abs/2405.05887</link>
      <description>arXiv:2405.05887v2 Announce Type: replace 
Abstract: In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs). Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases. The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning. The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05887v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengyuan Niu, Ali Bouland, Haoran Wang, Filippos Fotiadis, Andrew Kurdila, Andrea L'Afflitto, Sai Tej Paruchuri, Kyriakos G. Vamvoudakis</dc:creator>
    </item>
    <item>
      <title>Open Set Recognition For Music Genre Classification</title>
      <link>https://arxiv.org/abs/2209.07548</link>
      <description>arXiv:2209.07548v2 Announce Type: replace-cross 
Abstract: We explore segmentation of known and unknown genre classes using the open source GTZAN and FMA datasets. For each, we begin with best-case closed set genre classification, then we apply open set recognition methods. We offer an algorithm for the music genre classification task using OSR. We demonstrate the ability to retrieve known genres and as well identification of aural patterns for novel genres (not appearing in a training set). We conduct four experiments, each containing a different set of known and unknown classes, using the GTZAN and the FMA datasets to establish a baseline capacity for novel genre detection. We employ grid search on both OpenMax and softmax to determine the optimal total classification accuracy for each experimental setup, and illustrate interaction between genre labelling and open set recognition accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.07548v2</guid>
      <category>eess.AS</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin Liu, Julien DeMori, Kobi Abayomi</dc:creator>
    </item>
    <item>
      <title>An Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization</title>
      <link>https://arxiv.org/abs/2212.02387</link>
      <description>arXiv:2212.02387v4 Announce Type: replace-cross 
Abstract: This paper studies the stochastic nonconvex-strongly-concave minimax optimization over a multi-agent network. We propose an efficient algorithm, called Decentralized Recursive gradient descEnt Ascent Method (DREAM), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary points. Concretely, it requires $\mathcal{O}(\min (\kappa^3\epsilon^{-3},\kappa^2 \sqrt{N} \epsilon^{-2} ))$ stochastic first-order oracle (SFO) calls and $\tilde{\mathcal{O}}(\kappa^2 \epsilon^{-2})$ communication rounds, where $\kappa$ is the condition number and $N$ is the total number of individual functions. Our numerical experiments also validate the superiority of DREAM over previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02387v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesi Chen, Haishan Ye, Luo Luo</dc:creator>
    </item>
    <item>
      <title>Locally Adaptive Federated Learning</title>
      <link>https://arxiv.org/abs/2307.06306</link>
      <description>arXiv:2307.06306v2 Announce Type: replace-cross 
Abstract: Federated learning is a paradigm of distributed machine learning in which multiple clients coordinate with a central server to learn a model, without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) ensure balance among the clients by using the same stepsize for local updates on all clients. However, this means that all clients need to respect the global geometry of the function which could yield slow convergence. In this work, we propose locally adaptive federated learning algorithms, that leverage the local geometric information for each client function. We show that such locally adaptive methods with uncoordinated stepsizes across all clients can be particularly efficient in interpolated (overparameterized) settings, and analyze their convergence in the presence of heterogeneous data for convex and strongly convex settings. We validate our theoretical claims by performing illustrative experiments for both i.i.d. non-i.i.d. cases. Our proposed algorithms match the optimization performance of tuned FedAvg in the convex setting, outperform FedAvg as well as state-of-the-art adaptive federated algorithms like FedAMS for non-convex experiments, and come with superior generalization performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06306v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sohom Mukherjee, Nicolas Loizou, Sebastian U. Stich</dc:creator>
    </item>
    <item>
      <title>Insights of using Control Theory for minimizing Induced Seismicity in Underground Reservoirs</title>
      <link>https://arxiv.org/abs/2310.02700</link>
      <description>arXiv:2310.02700v3 Announce Type: replace-cross 
Abstract: Deep Geothermal Energy, Carbon Capture, and Storage and Hydrogen Storage have significant potential to meet the large-scale needs of the energy sector and reduce the CO$_2$ emissions. However, the injection of fluids into the earth's crust, upon which these activities rely, can lead to the formation of new seismogenic faults or the reactivation of existing ones, thereby causing earthquakes. In this study, we propose a novel approach based on control theory to address this issue. First, we obtain a simplified model of induced seismicity due to fluid injections in an underground reservoir using a diffusion equation in three dimensions. Then, we design a robust tracking control approach to force the seismicity rate to follow desired references. In this way, the induced seismicity is minimized while ensuring fluid circulation for the needs of renewable energy production and storage. The designed control guarantees the achievement of the control objectives even in the presence of system uncertainties and unknown dynamics. Finally, we present simulations of a simplified geothermal reservoir under different scenarios of energy demand to show the reliability and performance of the control approach, opening new perspectives for field experiments based on real-time regulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02700v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Diego Gutierrez-Oribio, Ioannis Stefanou</dc:creator>
    </item>
    <item>
      <title>The allocation of FIFA World Cup slots based on the ranking of confederations</title>
      <link>https://arxiv.org/abs/2310.19100</link>
      <description>arXiv:2310.19100v3 Announce Type: replace-cross 
Abstract: Qualifications for several world championships in sports are organised such that distinct sets of teams play in their own tournament for a predetermined number of slots. Inspired by a recent work studying the problem with the tools from the literature on fair allocation, this paper provides an alternative approach based on historical matches between these sets of teams. We focus on the FIFA World Cup due to the existence of an official rating system and its recent expansion to 48 teams, as well as to allow for a comparison with the already suggested allocations. Our proposal extends the methodology of the FIFA World Ranking to compare the strengths of five confederations. Various allocations are presented depending on the length of the sample, the set of teams considered, as well as the frequency of rating updates. The results show that more European and South American teams should play in the FIFA World Cup. The ranking of continents by the number of deserved slots is different from the ranking implied by FIFA policy. We recommend allocating at least some slots transparently, based on historical performances, similar to the access list of the UEFA Champions League.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19100v3</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, L\'aszl\'o Marcell Kiss, Zsombor Sz\'adoczki</dc:creator>
    </item>
    <item>
      <title>A Generalized Approach to Online Convex Optimization</title>
      <link>https://arxiv.org/abs/2402.08621</link>
      <description>arXiv:2402.08621v2 Announce Type: replace-cross 
Abstract: In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08621v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Pedramfar, Vaneet Aggarwal</dc:creator>
    </item>
    <item>
      <title>Stability and Performance Analysis of Discrete-Time ReLU Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2405.05236</link>
      <description>arXiv:2405.05236v3 Announce Type: replace-cross 
Abstract: This paper presents sufficient conditions for the stability and $\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions. These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU. Our stability and performance condition uses these QCs along with a "lifted" representation for the ReLU RNN. We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU. We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05236v3</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahel Vahedi Noori, Bin Hu, Geir Dullerud, Peter Seiler</dc:creator>
    </item>
    <item>
      <title>The Riemann hypothesis and dynamics of Backtracking New Q-Newton's method</title>
      <link>https://arxiv.org/abs/2405.05834</link>
      <description>arXiv:2405.05834v2 Announce Type: replace-cross 
Abstract: A new variant of Newton's method -- named Backtracking New Q-Newton's method (BNQN) -- was recently introduced by the second author. This method has good convergence guarantees, specially concerning finding roots of meromorphic functions. This paper explores using BNQN for the Riemann xi function. We show in particular that the Riemann hypothesis is equivalent to that all attractors of BNQN lie on the critical line. We also explain how an apparent relation between the basins of attraction of BNQN and Voronoi's diagram can be helpful for verifying the Riemann hypothesis or finding a counterexample to it. Some illustrating experimental results are included, which convey some interesting phenomena. The experiments show that BNQN works very stably with highly transcendental functions like the Riemann xi function and its derivatives. Based on insights from the experiments, we discuss some concrete steps on using BNQN towards the Riemann hypothesis. Ideas and results from this paper can be extended to other zeta functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05834v2</guid>
      <category>math.DS</category>
      <category>math.CV</category>
      <category>math.NT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thuan Quang Tran, Tuyen Trung Truong</dc:creator>
    </item>
  </channel>
</rss>
