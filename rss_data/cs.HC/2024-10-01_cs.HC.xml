<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 02:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Outlining the Borders for LLM Applications in Patient Education: Developing an Expert-in-the-Loop LLM-Powered Chatbot for Prostate Cancer Patient Education</title>
      <link>https://arxiv.org/abs/2409.19100</link>
      <description>arXiv:2409.19100v1 Announce Type: new 
Abstract: Cancer patients often struggle to transition swiftly to treatment due to limited institutional resources, lack of sophisticated professional guidance, and low health literacy. The emergence of Large Language Models (LLMs) offers new opportunities for such patients to access the wealth of existing patient education materials. The current paper presents the development process for an LLM-based chatbot focused on prostate cancer education, including needs assessment, co-design, and usability studies. The resulting application, MedEduChat, integrates with patients' electronic health record data and features a closed-domain, semi-structured, patient-centered approach to address real-world needs. This paper contributes to the growing field of patient-LLM interaction by demonstrating the potential of LLM-based chatbots to enhance prostate cancer patient education and by offering co-design guidelines for future LLM-based healthcare downstream applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19100v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuexing Hao, Jason Holmes, Mark Waddle, Nathan Yu, Kirstin Vickers, Heather Preston, Drew Margolin, Corinna E. L\"ockenhoff, Aditya Vashistha, Marzyeh Ghassemi, Saleh Kalantari, Wei Liu</dc:creator>
    </item>
    <item>
      <title>Responsible AI in Open Ecosystems: Reconciling Innovation with Risk Assessment and Disclosure</title>
      <link>https://arxiv.org/abs/2409.19104</link>
      <description>arXiv:2409.19104v1 Announce Type: new 
Abstract: The rapid scaling of AI has spurred a growing emphasis on ethical considerations in both development and practice. This has led to the formulation of increasingly sophisticated model auditing and reporting requirements, as well as governance frameworks to mitigate potential risks to individuals and society. At this critical juncture, we review the practical challenges of promoting responsible AI and transparency in informal sectors like OSS that support vital infrastructure and see widespread use. We focus on how model performance evaluation may inform or inhibit probing of model limitations, biases, and other risks. Our controlled analysis of 7903 Hugging Face projects found that risk documentation is strongly associated with evaluation practices. Yet, submissions (N=789) from the platform's most popular competitive leaderboard showed less accountability among high performers. Our findings can inform AI providers and legal scholars in designing interventions and policies that preserve open-source innovation while incentivizing ethical uptake.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19104v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.SE</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahasweta Chakraborti, Bert Joseph Prestoza, Nicholas Vincent, Seth Frey</dc:creator>
    </item>
    <item>
      <title>Gaze-informed Signatures of Trust and Collaboration in Human-Autonomy Teams</title>
      <link>https://arxiv.org/abs/2409.19139</link>
      <description>arXiv:2409.19139v1 Announce Type: new 
Abstract: In the evolving landscape of human-autonomy teaming (HAT), fostering effective collaboration and trust between human and autonomous agents is increasingly important. To explore this, we used the game Overcooked AI to create dynamic teaming scenarios featuring varying agent behaviors (clumsy, rigid, adaptive) and environmental complexities (low, medium, high). Our objectives were to assess the performance of adaptive AI agents designed with hierarchical reinforcement learning for better teamwork and measure eye tracking signals related to changes in trust and collaboration. The results indicate that the adaptive agent was more effective in managing teaming and creating an equitable task distribution across environments compared to the other agents. Working with the adaptive agent resulted in better coordination, reduced collisions, more balanced task contributions, and higher trust ratings. Reduced gaze allocation, across all agents, was associated with higher trust levels, while blink count, scan path length, agent revisits and trust were predictive of the humans contribution to the team. Notably, fixation revisits on the agent increased with environmental complexity and decreased with agent versatility, offering a unique metric for measuring teammate performance monitoring. These findings underscore the importance of designing autonomous teammates that not only excel in task performance but also enhance teamwork by being more predictable and reducing the cognitive load on human team members. Additionally, this study highlights the potential of eye-tracking as an unobtrusive measure for evaluating and improving human-autonomy teams, suggesting eye gaze could be used by agents to dynamically adapt their behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19139v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony J. Ries, St\'ephane Aroca-Ouellette, Alessandro Roncone, Ewart J. de Visser</dc:creator>
    </item>
    <item>
      <title>SensoPatch: A Reconfigurable Haptic Feedback with High-Density Tactile Sensing Glove</title>
      <link>https://arxiv.org/abs/2409.19155</link>
      <description>arXiv:2409.19155v1 Announce Type: new 
Abstract: Haptic feedback is integral to the improved experience of prosthetic users and the reduction in prosthesis rejection. Prior studies have explored various methods to encode tactile information and deliver vibration feedback. However, a comprehensive study comparing performance across different stimulation locations and feedback modalities for wearable devices is absent and there is no test platform. This paper proposes an open-source reconfigurable haptic feedback system which incorporates 25 sensors and wireless communication to allow customized number of vibration motors, adjustable motor placement, and programmable encoding of tactile data to change feedback modalities. To demonstrate potential studies that can be investigated using SensoPatch, we conducted two experiments: 1) to assess the vibration discrimination accuracy on 3 body parts 2) to assess the effect of 6 methods of mapping tactile data to varying number of motors on object manipulation. SensoPatch utilizes low-cost off-the-shelf components, enabling large-scale comparative studies of feedback modalities and stimulation sites to optimize vibrotactile feedback and facilitate its deployment in upper limb prostheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19155v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanisa Angkanapiwat, Ariel Slepyan, Rebecca J. Greene, Nitish Thakor</dc:creator>
    </item>
    <item>
      <title>Esports Training, Periodization, and Tools -- a Scoping Review</title>
      <link>https://arxiv.org/abs/2409.19180</link>
      <description>arXiv:2409.19180v2 Announce Type: new 
Abstract: Electronic sports (esports) and research on this emerging field are interdisciplinary in nature. By extension, it is essential to understand how to standardize and structure training with the help of existing tools developed by years of research in sports sciences and informatics. Our goal in this article was to verify if the current body of research contains substantial evidence of the training systems applied to training esports players. To verify the existing sources, we have applied a framework of scoping review to address the search from multiple scientific databases with further local processing. We conclude that the current research on esports dealt mainly with describing and modeling performance metrics spanned over multiple fragmented research areas (psychology, nutrition, informatics), and yet these building blocks were not assembled into an existing well-functioning theory of performance in esports by providing exercise regimes, and ways of periodization for esports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19180v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrzej Bia{\l}ecki, Bart{\l}omiej Michalak, Jan Gajewski</dc:creator>
    </item>
    <item>
      <title>Feature-Prescribed Iterative Learning Control of Waggle Dance Movement for Social Motor Coordination in Joint Actions</title>
      <link>https://arxiv.org/abs/2409.19213</link>
      <description>arXiv:2409.19213v1 Announce Type: new 
Abstract: Extensive experiments suggest that motor coordination among human participants may contribute to social affinity and emotional attachment, which has great potential in the clinical treatment of social disorders or schizophrenia. Mirror game provides an effective experimental paradigm for studying social motor coordination. Nevertheless, the lack of movement richness prevents the emergence of high-level coordination in the existing one-dimensional experiments. To tackle this problem, this work develops a two-dimensional experimental paradigm of mirror game by playing waggle dance between two participants. In particular, an online control architecture of customized virtual player is created to coordinate with human player. Therein, an iterative learning control algorithm is proposed by integrating position tracking and behavior imitation with prescribed kinematic feature. Moreover, convergence analysis of control algorithm is conducted to guarantee the online performance of virtual player. Finally, the proposed control strategy is validated by matching experimental data and compared with other control methods using a set of performance indexes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19213v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowen Guo, Chao Zhai</dc:creator>
    </item>
    <item>
      <title>Gesture Recognition for Feedback Based Mixed Reality and Robotic Fabrication: A Case Study of the UnLog Tower</title>
      <link>https://arxiv.org/abs/2409.19281</link>
      <description>arXiv:2409.19281v1 Announce Type: new 
Abstract: Mixed Reality (MR) platforms enable users to interact with three-dimensional holographic instructions during the assembly and fabrication of highly custom and parametric architectural constructions without the necessity of two-dimensional drawings. Previous MR fabrication projects have primarily relied on digital menus and custom buttons as the interface for user interaction with the MR environment. Despite this approach being widely adopted, it is limited in its ability to allow for direct human interaction with physical objects to modify fabrication instructions within the MR environment. This research integrates user interactions with physical objects through real-time gesture recognition as input to modify, update or generate new digital information enabling reciprocal stimuli between the physical and the virtual environment. Consequently, the digital environment is generative of the user's provided interaction with physical objects to allow seamless feedback in the fabrication process. This research investigates gesture recognition for feedback-based MR workflows for robotic fabrication, human assembly, and quality control in the construction of the UnLog Tower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19281v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-99-8405-3_28</arxiv:DOI>
      <dc:creator>Alexander Htet Kyaw, Lawson Spencer, Sasa Zivkovic, Leslie Lok</dc:creator>
    </item>
    <item>
      <title>'Simulacrum of Stories': Examining Large Language Models as Qualitative Research Participants</title>
      <link>https://arxiv.org/abs/2409.19430</link>
      <description>arXiv:2409.19430v1 Announce Type: new 
Abstract: The recent excitement around generative models has sparked a wave of proposals suggesting the replacement of human participation and labor in research and development--e.g., through surveys, experiments, and interviews--with synthetic research data generated by large language models (LLMs). We conducted interviews with 19 qualitative researchers to understand their perspectives on this paradigm shift. Initially skeptical, researchers were surprised to see similar narratives emerge in the LLM-generated data when using the interview probe. However, over several conversational turns, they went on to identify fundamental limitations, such as how LLMs foreclose participants' consent and agency, produce responses lacking in palpability and contextual depth, and risk delegitimizing qualitative research methods. We argue that the use of LLMs as proxies for participants enacts the surrogate effect, raising ethical and epistemological concerns that extend beyond the technical limitations of current models to the core of whether LLMs fit within qualitative ways of knowing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19430v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivani Kapania, William Agnew, Motahhare Eslami, Hoda Heidari, Sarah Fox</dc:creator>
    </item>
    <item>
      <title>Secret Use of Large Language Models</title>
      <link>https://arxiv.org/abs/2409.19450</link>
      <description>arXiv:2409.19450v1 Announce Type: new 
Abstract: The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, users' secret use of LLM, raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19450v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiping Zhang, Chenxinran Shen, Bingsheng Yao, Dakuo Wang, Tianshi Li</dc:creator>
    </item>
    <item>
      <title>See Where You Read with Eye Gaze Tracking and Large Language Model</title>
      <link>https://arxiv.org/abs/2409.19454</link>
      <description>arXiv:2409.19454v1 Announce Type: new 
Abstract: Losing track of reading progress during line switching can be frustrating. Eye gaze tracking technology offers a potential solution by highlighting read paragraphs, aiding users in avoiding wrong line switches. However, the gap between gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes direct application impractical. Existing methods leverage the linear reading pattern but fail during jump reading. This paper presents a reading tracking and highlighting system that supports both linear and jump reading. Based on experimental insights from the gaze nature study of 16 users, two gaze error models are designed to enable both jump reading detection and relocation. The system further leverages the large language model's contextual perception capability in aiding reading tracking. A reading tracking domain-specific line-gaze alignment opportunity is also exploited to enable dynamic and frequent calibration of the gaze results. Controlled experiments demonstrate reliable linear reading tracking, as well as 84% accuracy in tracking jump reading. Furthermore, real field tests with 18 volunteers demonstrated the system's effectiveness in tracking and highlighting read paragraphs, improving reading efficiency, and enhancing user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19454v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sikai Yang, Gang Yan</dc:creator>
    </item>
    <item>
      <title>Upper-body musculoskeletal pain and eye strain among language professionals: a descriptive, cross-sectional study</title>
      <link>https://arxiv.org/abs/2409.19598</link>
      <description>arXiv:2409.19598v1 Announce Type: new 
Abstract: Language professionals spend long hours at the computer, which may have an impact on their short- and long-term physical health. In 2023, I ran a survey to investigate workstation ergonomics, eye and upper-body problems, and self-reported strategies that alleviate those problems among language professionals who work sitting or standing at a desk. Of the 791 respondents, about one third reported eye problems and over two-thirds reported upper-body aches or pains in the past 12 months, with significantly higher upper-body pain prevalence among females than males, and also among younger respondents than older ones. While the pain prevalence rate in the survey was similar to figures published in the literature, as was the sex risk factor, the association of higher pain prevalence among younger people contrasted with other studies that have found increasing age to be a risk factor for pain. In this article I share the survey results in detail and discuss possible explanations for the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19598v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Goldsmith</dc:creator>
    </item>
    <item>
      <title>The Future of HCI-Policy Collaboration</title>
      <link>https://arxiv.org/abs/2409.19738</link>
      <description>arXiv:2409.19738v1 Announce Type: new 
Abstract: Policies significantly shape computation's societal impact, a crucial HCI concern. However, challenges persist when HCI professionals attempt to integrate policy into their work or affect policy outcomes. Prior research considered these challenges at the ``border'' of HCI and policy. This paper asks: What if HCI considers policy integral to its intellectual concerns, placing system-people-policy interaction not at the border but nearer the center of HCI research, practice, and education? What if HCI fosters a mosaic of methods and knowledge contributions that blend system, human, and policy expertise in various ways, just like HCI has done with blending system and human expertise? We present this re-imagined HCI-policy relationship as a provocation and highlight its usefulness: It spotlights previously overlooked system-people-policy interaction work in HCI. It unveils new opportunities for HCI's futuring, empirical, and design projects. It allows HCI to coordinate its diverse policy engagements, enhancing its collective impact on policy outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19738v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642771</arxiv:DOI>
      <dc:creator>Qian Yang, Richmond Y Wong, Steven J Jackson, Sabine Junginger, Margaret D Hagan, Thomas Gilbert, John Zimmerman</dc:creator>
    </item>
    <item>
      <title>Understanding Challenges and Opportunities in Body Movement Education of People who are Blind or have Low Vision</title>
      <link>https://arxiv.org/abs/2409.19935</link>
      <description>arXiv:2409.19935v1 Announce Type: new 
Abstract: Actively participating in body movement such as dance, sports, and fitness activities is challenging for people who are blind or have low vision (BLV). Teachers primarily rely on verbal instructions and physical demonstrations with limited accessibility. Recent work shows that technology can support body movement education for BLV people. However, there is limited involvement with the BLV community and their teachers to understand their needs. By conducting a series of two surveys, 23 interviews and four focus groups, we gather the voices and perspectives of BLV people and their teachers. This provides a rich understanding of the challenges of body movement education. We identify ten major themes, four key design challenges, and propose potential solutions. We encourage the assistive technologies community to co-design potential solutions to these identified design challenges promoting the quality of life of BLV people and supporting the teachers in the provision of inclusive education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19935v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3597638.3608409</arxiv:DOI>
      <dc:creator>Madhuka Thisuri De Silva, Sarah Goodwin, Leona M Holloway, Matthew Butler</dc:creator>
    </item>
    <item>
      <title>Understanding How Psychological Distance Influences User Preferences in Conversational Versus Web Search</title>
      <link>https://arxiv.org/abs/2409.19982</link>
      <description>arXiv:2409.19982v1 Announce Type: new 
Abstract: Conversational search offers an easier and faster alternative to conventional web search, while having downsides like lack of source verification. Research has examined performance disparities between these two systems in different settings. However, little work has considered the effects of variations within a given search task. We hypothesize that psychological distance - one''s perceived closeness to a target event - affects information needs in search tasks, and investigate the corresponding effects on user preferences between web and conversational search systems. We find that with greater psychological distances, users perceive conversational search as more credible, useful, enjoyable, and easy to use, and demonstrate increased preference for this system. We reveal qualitative reasons for these differences and provide design implications for search system designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19982v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yitian Yang, Yugin Tan, Yang Chen Lin, Jung-Tai King, Zihan Liu, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>Optimising EEG decoding with refined sampling and multimodal feature integration</title>
      <link>https://arxiv.org/abs/2409.20086</link>
      <description>arXiv:2409.20086v1 Announce Type: new 
Abstract: Electroencephalography (EEG) is a neuroimaging technique that records brain neural activity with high temporal resolution. Unlike other methods, EEG does not require prohibitively expensive equipment and can be easily set up using commercially available portable EEG caps, making it an ideal candidate for brain-computer interfaces. However, EEG signals are characterised by poor spatial resolution and high noise levels, complicating their decoding. In this study, we employ a contrastive learning framework to align encoded EEG features with pretrained CLIP features, achieving a 7% improvement over the state-of-the-art in EEG decoding of object categories. This enhancement is equally attributed to (1) a novel online sampling method that boosts the signal-to-noise ratio and (2) multimodal representations leveraging visual and language features to enhance the alignment space. Our analysis reveals a systematic interaction between the architecture and dataset of pretrained features and their alignment efficacy for EEG signal decoding. This interaction correlates with the generalisation power of the pretrained features on ImageNet-O/A datasets ($r=.5$). These findings extend beyond EEG signal alignment, offering potential for broader applications in neuroimaging decoding and generic feature alignments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20086v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arash Akbarinia</dc:creator>
    </item>
    <item>
      <title>Factory Operators' Perspectives on Cognitive Assistants for Knowledge Sharing: Challenges, Risks, and Impact on Work</title>
      <link>https://arxiv.org/abs/2409.20192</link>
      <description>arXiv:2409.20192v1 Announce Type: new 
Abstract: In the shift towards human-centered manufacturing, our two-year longitudinal study investigates the real-world impact of deploying Cognitive Assistants (CAs) in factories. The CAs were designed to facilitate knowledge sharing among factory operators. Our investigation focused on smartphone-based voice assistants and LLM-powered chatbots, examining their usability and utility in a real-world factory setting. Based on the qualitative feedback we collected during the deployments of CAs at the factories, we conducted a thematic analysis to investigate the perceptions, challenges, and overall impact on workflow and knowledge sharing.
  Our results indicate that while CAs have the potential to significantly improve efficiency through knowledge sharing and quicker resolution of production issues, they also introduce concerns around workplace surveillance, the types of knowledge that can be shared, and shortcomings compared to human-to-human knowledge sharing. Additionally, our findings stress the importance of addressing privacy, knowledge contribution burdens, and tensions between factory operators and their managers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20192v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Kernan Freire, Tianhao He, Chaofan Wang, Evangelos Niforatos, Alessandro Bozzon</dc:creator>
    </item>
    <item>
      <title>Investigating Creation Perspectives and Icon Placement Preferences for On-Body Menus in Virtual Reality</title>
      <link>https://arxiv.org/abs/2409.20238</link>
      <description>arXiv:2409.20238v1 Announce Type: new 
Abstract: On-body menus present a novel interaction paradigm within Virtual Reality (VR) environments by embedding virtual interfaces directly onto the user's body. Unlike traditional screen-based interfaces, on-body menus enable users to interact with virtual options or icons visually attached to their physical form. In this paper, We investigated the impact of the creation process on the effectiveness of on-body menus, comparing first-person, third-person, and mirror perspectives. Our first study ($N$ = 12) revealed that the mirror perspective led to faster creation times and more accurate recall compared to the other two perspectives. To further explore user preferences, we conducted a second study ($N$ = 18) utilizing a VR system with integrated body tracking. By combining distributions of icons from both studies ($N$ = 30), we confirmed significant preferences in on-body menu placement based on icon category (e.g., Social Media icons were consistently placed on forearms). We also discovered associations between categories, such as Leisure and Social Media icons frequently co-occurring. Our findings highlight the importance of the creation process, uncover user preferences for on-body menu organization, and provide insights to guide the development of intuitive and effective on-body interactions within virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20238v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698136</arxiv:DOI>
      <dc:creator>Xiang Li, Wei He, Shan Jin, Jan Gugenheimer, Pan Hui, Hai-Ning Liang, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>Automation from the Worker's Perspective</title>
      <link>https://arxiv.org/abs/2409.20387</link>
      <description>arXiv:2409.20387v1 Announce Type: new 
Abstract: Common narratives about automation often pit new technologies against workers. The introduction of advanced machine tools, industrial robots, and AI have all been met with concern that technological progress will mean fewer jobs. However, workers themselves offer a more optimistic, nuanced perspective. Drawing on a far-reaching 2024 survey of more than 9,000 workers across nine countries, this paper finds that more workers report potential benefits from new technologies like robots and AI for their safety and comfort at work, their pay, and their autonomy on the job than report potential costs. Workers with jobs that ask them to solve complex problems, workers who feel valued by their employers, and workers who are motivated to move up in their careers are all more likely to see new technologies as beneficial. In contrast to assumptions in previous research, more formal education is in some cases associated with more negative attitudes toward automation and its impact on work. In an experimental setting, the prospect of financial incentives for workers improve their perceptions of automation technologies, whereas the prospect of increased input about how new technologies are used does not have a significant effect on workers' attitudes toward automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20387v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Armstrong, Valerie K. Chen, Alex Cuellar, Alexandra Forsey-Smerek, Julie A. Shah</dc:creator>
    </item>
    <item>
      <title>Does Positive Reinforcement Work?: A Quasi-Experimental Study of the Effects of Positive Feedback on Reddit</title>
      <link>https://arxiv.org/abs/2409.20410</link>
      <description>arXiv:2409.20410v1 Announce Type: new 
Abstract: Social media platform design often incorporates explicit signals of positive feedback. Some moderators provide positive feedback with the goal of positive reinforcement, but are often unsure of their ability to actually influence user behavior. Despite its widespread use and theory touting positive feedback as crucial for user motivation, its effect on recipients is relatively unknown. This paper examines how positive feedback impacts Reddit users and evaluates its differential effects to understand who benefits most from receiving positive feedback. Through a causal inference study of 11M posts across 4 months, we find that users who received positive feedback made more frequent (2% per day) and higher quality (57% higher score; 2% fewer removals per day) posts compared to a set of matched control users. Our findings highlight the need for platforms and communities to expand their perspective on moderation and complement punitive approaches with positive reinforcement strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20410v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Lambert, Koustuv Saha, Eshwar Chandrasekharan</dc:creator>
    </item>
    <item>
      <title>Understanding #vent Channels on Discord</title>
      <link>https://arxiv.org/abs/2409.19166</link>
      <description>arXiv:2409.19166v1 Announce Type: cross 
Abstract: Vent channels on Discord, which are chat channels developed for people to express frustrations, can become an informal type of peer support system. This paper is a qualitative study of experiences with vent channels on Discord, examining the experiences of 13 participants through semi-structured interviews. We find that participants are able to meet their needs for social support via vent channels by receiving commiseration, advice, and validation from the responses of others. At the same time, vent channels can lead to frustration when participants have conflicting expectations for their interactions. We suggest ways that Discord or Discord server moderators can provide enhanced structure, clarity, and transparency in order to enable participants to have better experiences in vent channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19166v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kayode Oladeji, Tony Wang, Diyi Yang, Amy Bruckman</dc:creator>
    </item>
    <item>
      <title>DOTA: Distributional Test-Time Adaptation of Vision-Language Models</title>
      <link>https://arxiv.org/abs/2409.19375</link>
      <description>arXiv:2409.19375v1 Announce Type: cross 
Abstract: Vision-language foundation models (e.g., CLIP) have shown remarkable performance across a wide range of tasks. However, deploying these models may be unreliable when significant distribution gaps exist between the training and test data. The training-free test-time dynamic adapter (TDA) is a promising approach to address this issue by storing representative test samples to guide the classification of subsequent ones. However, TDA only naively maintains a limited number of reference samples in the cache, leading to severe test-time catastrophic forgetting when the cache is updated by dropping samples. In this paper, we propose a simple yet effective method for DistributiOnal Test-time Adaptation (Dota). Instead of naively memorizing representative test samples, Dota continually estimates the distributions of test samples, allowing the model to continually adapt to the deployment environment. The test-time posterior probabilities are then computed using the estimated distributions based on Bayes' theorem for adaptation purposes. To further enhance the adaptability on the uncertain samples, we introduce a new human-in-the-loop paradigm which identifies uncertain samples, collects human-feedback, and incorporates it into the Dota framework. Extensive experiments validate that Dota enables CLIP to continually learn, resulting in a significant improvement compared to current state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19375v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zongbo Han, Jialong Yang, Junfan Li, Qinghua Hu, Qianli Xu, Mike Zheng Shou, Changqing Zhang</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap in Hybrid Decision-Making Systems</title>
      <link>https://arxiv.org/abs/2409.19415</link>
      <description>arXiv:2409.19415v1 Announce Type: cross 
Abstract: We introduce BRIDGET, a novel human-in-the-loop system for hybrid decision-making, aiding the user to label records from an un-labeled dataset, attempting to ``bridge the gap'' between the two most popular Hybrid Decision-Making paradigms: those featuring the human in a leading position, and the other with a machine making most of the decisions. BRIDGET understands when either a machine or a human user should be in charge, dynamically switching between two statuses. In the different statuses, BRIDGET still fosters the human-AI interaction, either having a machine learning model assuming skeptical stances towards the user and offering them suggestions, or towards itself and calling the user back. We believe our proposal lays the groundwork for future synergistic systems involving a human and a machine decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19415v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Mazzoni, Roberto Pellungrini, Riccardo Guidotti</dc:creator>
    </item>
    <item>
      <title>Lessons Learned from Developing a Human-Centered Guide Dog Robot for Mobility Assistance</title>
      <link>https://arxiv.org/abs/2409.19778</link>
      <description>arXiv:2409.19778v1 Announce Type: cross 
Abstract: While guide dogs offer essential mobility assistance, their high cost, limited availability, and care requirements make them inaccessible to most blind or low vision (BLV) individuals. Recent advances in quadruped robots provide a scalable solution for mobility assistance, but many current designs fail to meet real-world needs due to a lack of understanding of handler and guide dog interactions. In this paper, we share lessons learned from developing a human-centered guide dog robot, addressing challenges such as optimal hardware design, robust navigation, and informative scene description for user adoption. By conducting semi-structured interviews and human experiments with BLV individuals, guide-dog handlers, and trainers, we identified key design principles to improve safety, trust, and usability in robotic mobility aids. Our findings lay the building blocks for future development of guide dog robots, ultimately enhancing independence and quality of life for BLV individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19778v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hochul Hwang, Ken Suzuki, Nicholas A Giudice, Joydeep Biswas, Sunghoon Ivan Lee, Donghyun Kim</dc:creator>
    </item>
    <item>
      <title>Enabling Multi-Robot Collaboration from Single-Human Guidance</title>
      <link>https://arxiv.org/abs/2409.19831</link>
      <description>arXiv:2409.19831v1 Announce Type: cross 
Abstract: Learning collaborative behaviors is essential for multi-agent systems. Traditionally, multi-agent reinforcement learning solves this implicitly through a joint reward and centralized observations, assuming collaborative behavior will emerge. Other studies propose to learn from demonstrations of a group of collaborative experts. Instead, we propose an efficient and explicit way of learning collaborative behaviors in multi-agent systems by leveraging expertise from only a single human. Our insight is that humans can naturally take on various roles in a team. We show that agents can effectively learn to collaborate by allowing a human operator to dynamically switch between controlling agents for a short period and incorporating a human-like theory-of-mind model of teammates. Our experiments showed that our method improves the success rate of a challenging collaborative hide-and-seek task by up to 58$% with only 40 minutes of human guidance. We further demonstrate our findings transfer to the real world by conducting multi-robot experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19831v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengran Ji, Lingyu Zhang, Paul Sajda, Boyuan Chen</dc:creator>
    </item>
    <item>
      <title>Benchmarking Adaptive Intelligence and Computer Vision on Human-Robot Collaboration</title>
      <link>https://arxiv.org/abs/2409.19856</link>
      <description>arXiv:2409.19856v1 Announce Type: cross 
Abstract: Human-Robot Collaboration (HRC) is vital in Industry 4.0, using sensors, digital twins, collaborative robots (cobots), and intention-recognition models to have efficient manufacturing processes. However, Concept Drift is a significant challenge, where robots struggle to adapt to new environments. We address concept drift by integrating Adaptive Intelligence and self-labeling (SLB) to improve the resilience of intention-recognition in an HRC system. Our methodology begins with data collection using cameras and weight sensors, which is followed by annotation of intentions and state changes. Then we train various deep learning models with different preprocessing techniques for recognizing and predicting the intentions. Additionally, we developed a custom state detection algorithm for enhancing the accuracy of SLB, offering precise state-change definitions and timestamps to label intentions. Our results show that the MViT2 model with skeletal posture preprocessing achieves an accuracy of 83% on our data environment, compared to the 79% accuracy of MViT2 without skeleton posture extraction. Additionally, our SLB mechanism achieves a labeling accuracy of 91%, reducing a significant amount of time that would've been spent on manual annotation. Lastly, we observe swift scaling of model performance that combats concept drift by fine tuning on different increments of self-labeled data in a shifted domain that has key differences from the original training environment.. This study demonstrates the potential for rapid deployment of intelligent cobots in manufacturing through the steps shown in our methodology, paving a way for more adaptive and efficient HRC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19856v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salaar Saraj (California Institute for Telecommunications,Information Technology), Gregory Shklovski (California Institute for Telecommunications,Information Technology), Kristopher Irizarry (California Institute for Telecommunications,Information Technology), Jonathan Vet (California Institute for Telecommunications,Information Technology), Yutian Ren (California Institute for Telecommunications,Information Technology)</dc:creator>
    </item>
    <item>
      <title>Professor X: Manipulating EEG BCI with Invisible and Robust Backdoor Attack</title>
      <link>https://arxiv.org/abs/2409.20158</link>
      <description>arXiv:2409.20158v1 Announce Type: cross 
Abstract: While electroencephalogram (EEG) based brain-computer interface (BCI) has been widely used for medical diagnosis, health care, and device control, the safety of EEG BCI has long been neglected. In this paper, we propose Professor X, an invisible and robust "mind-controller" that can arbitrarily manipulate the outputs of EEG BCI through backdoor attack, to alert the EEG community of the potential hazard. However, existing EEG attacks mainly focus on single-target class attacks, and they either require engaging the training stage of the target BCI, or fail to maintain high stealthiness. Addressing these limitations, Professor X exploits a three-stage clean label poisoning attack: 1) selecting one trigger for each class; 2) learning optimal injecting EEG electrodes and frequencies strategy with reinforcement learning for each trigger; 3) generating poisoned samples by injecting the corresponding trigger's frequencies into poisoned data for each class by linearly interpolating the spectral amplitude of both data according to previously learned strategies. Experiments on datasets of three common EEG tasks demonstrate the effectiveness and robustness of Professor X, which also easily bypasses existing backdoor defenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20158v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuan-Hao Liu, Xinhao Song, Dexuan He, Bao-Liang Lu, Wei-Long Zheng</dc:creator>
    </item>
    <item>
      <title>Co-Movement and Trust Development in Human-Robot Teams</title>
      <link>https://arxiv.org/abs/2409.20218</link>
      <description>arXiv:2409.20218v1 Announce Type: cross 
Abstract: For humans and robots to form an effective human-robot team (HRT) there must be sufficient trust between team members throughout a mission. We analyze data from an HRT experiment focused on trust dynamics in teams of one human and two robots, where trust was manipulated by robots becoming temporarily unresponsive. Whole-body movement tracking was achieved using ultrasound beacons, alongside communications and performance logs from a human-robot interface. We find evidence that synchronization between time series of human-robot movement, within a certain spatial proximity, is correlated with changes in self-reported trust. This suggests that the interplay of proxemics and kinesics, i.e. moving together through space, where implicit communication via coordination can occur, could play a role in building and maintaining trust in human-robot teams. Thus, quantitative indicators of coordination dynamics between team members could be used to predict trust over time and also provide early warning signals of the need for timely trust repair if trust is damaged. Hence, we aim to develop the metrology of trust in mobile human-robot teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20218v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Webb, Sanja Milivojevic, Mehdi Sobhani, Zachary R. Madin, James C. Ward, Sagir Yusuf, Chris Baber, Edmund R. Hunt</dc:creator>
    </item>
    <item>
      <title>Computer-mediated therapies for stroke rehabilitation: a systematic review and meta-Analysis</title>
      <link>https://arxiv.org/abs/2409.20260</link>
      <description>arXiv:2409.20260v1 Announce Type: cross 
Abstract: OBJECTIVE: To evaluate the efficacy of different forms of virtual reality (VR) treatments as either immersive virtual reality (IVR) or non-immersive virtual reality (NIVR) in comparison to conventional therapy (CT) in improving physical and psychological status among stroke patients. METHODS: The literature search was conducted on seven databases. ACM Digital Library, Medline (via PubMed), Cochrane, IEEE Xplore, Web of Science, and Scopus. The effect sizes of the main outcomes were calculated using Cohen's d. Pooled results were used to present an overall estimate of the treatment effect using a random-effects model. RESULTS: A total of 22 randomized controlled trials were evaluated. 3 trials demonstrated that immersive virtual reality improved upper limb activity, function and activity of daily life in a way comparable to CT. 18 trials showed that NIVR had similar benefits to CT for upper limb activity and function, balance and mobility, activities of daily living and participation. A comparison between the different forms of VR showed that IVR may be more beneficial than NIVR for upper limb training and activities of daily life. CONCLUSIONS: This study found out that IVR therapies may be more effective than NIVR but not CT to improve upper limb activity, function, and daily life activities. However, there is no evidence of the durability of IVR treatment. More research involving studies with larger samples is needed to assess the long-term effects and promising benefits of immersive virtual reality technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20260v1</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jstrokecerebrovasdis.2022.106454</arxiv:DOI>
      <dc:creator>Stanley Mugisha. Mirko Job. Matteo Zoppi, Marco Testa, Rezia Molfino</dc:creator>
    </item>
    <item>
      <title>Robi Butler: Remote Multimodal Interactions with Household Robot Assistant</title>
      <link>https://arxiv.org/abs/2409.20548</link>
      <description>arXiv:2409.20548v1 Announce Type: cross 
Abstract: In this paper, we introduce Robi Butler, a novel household robotic system that enables multimodal interactions with remote users. Building on the advanced communication interfaces, Robi Butler allows users to monitor the robot's status, send text or voice instructions, and select target objects by hand pointing. At the core of our system is a high-level behavior module, powered by Large Language Models (LLMs), that interprets multimodal instructions to generate action plans. These plans are composed of a set of open vocabulary primitives supported by Vision Language Models (VLMs) that handle both text and pointing queries. The integration of the above components allows Robi Butler to ground remote multimodal instructions in the real-world home environment in a zero-shot manner. We demonstrate the effectiveness and efficiency of this system using a variety of daily household tasks that involve remote users giving multimodal instructions. Additionally, we conducted a user study to analyze how multimodal interactions affect efficiency and user experience during remote human-robot interaction and discuss the potential improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20548v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu</dc:creator>
    </item>
    <item>
      <title>Resistance Against Manipulative AI: key factors and possible actions</title>
      <link>https://arxiv.org/abs/2404.14230</link>
      <description>arXiv:2404.14230v2 Announce Type: replace 
Abstract: If AI is the new electricity, what should we do to keep ourselves from getting electrocuted? In this work, we explore factors related to the potential of large language models (LLMs) to manipulate human decisions. We describe the results of two experiments designed to determine what characteristics of humans are associated with their susceptibility to LLM manipulation, and what characteristics of LLMs are associated with their manipulativeness potential. We explore human factors by conducting user studies in which participants answer general knowledge questions using LLM-generated hints, whereas LLM factors by provoking language models to create manipulative statements. Then, we analyze their obedience, the persuasion strategies used, and the choice of vocabulary. Based on these experiments, we discuss two actions that can protect us from LLM manipulation. In the long term, we put AI literacy at the forefront, arguing that educating society would minimize the risk of manipulation and its consequences. We also propose an ad hoc solution, a classifier that detects manipulation of LLMs - a Manipulation Fuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14230v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Wilczy\'nski, Wiktoria Mieleszczenko-Kowszewicz, Przemys{\l}aw Biecek</dc:creator>
    </item>
    <item>
      <title>Automatic Classification of Subjective Time Perception Using Multi-modal Physiological Data of Air Traffic Controllers</title>
      <link>https://arxiv.org/abs/2404.15213</link>
      <description>arXiv:2404.15213v3 Announce Type: replace 
Abstract: In high-pressure environments where human individuals must simultaneously monitor multiple entities, communicate effectively, and maintain intense focus, the perception of time becomes a critical factor influencing performance and well-being. One indicator of well-being can be the person's subjective time perception. In our project $ChronoPilot$, we aim to develop a device that modulates human subjective time perception. In this study, we present a method to automatically assess the subjective time perception of air traffic controllers, a group often faced with demanding conditions, using their physiological data and eleven state-of-the-art machine learning classifiers. The physiological data consist of photoplethysmogram, electrodermal activity, and temperature data. We find that the support vector classifier works best with an accuracy of 79 % and electrodermal activity provides the most descriptive biomarker. These findings are an important step towards closing the feedback loop of our $ChronoPilot$-device to automatically modulate the user's subjective time perception. This technological advancement may promise improvements in task management, stress reduction, and overall productivity in high-stakes professions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15213v3</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Till Aust, Eirini Balta, Argiro Vatakis, Heiko Hamann</dc:creator>
    </item>
    <item>
      <title>POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</title>
      <link>https://arxiv.org/abs/2406.03843</link>
      <description>arXiv:2406.03843v3 Announce Type: replace 
Abstract: Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings. Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities within multimodal inputs. This oversight hinders the development of effective prompts that guide model multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities. In this paper, we present POEM, a visual analytics system to facilitate efficient prompt engineering for enhancing the multimodal reasoning performance of LLMs. The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts. Through diverse recommendations of demonstration examples and instructional principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights. The effectiveness and efficiency of our system are validated through two case studies and interviews with experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03843v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics</title>
      <link>https://arxiv.org/abs/2407.02885</link>
      <description>arXiv:2407.02885v4 Announce Type: replace 
Abstract: Integrating cognitive ergonomics with LLMs is crucial for improving safety, reliability, and user satisfaction in human-AI interactions. Current LLM designs often lack this integration, resulting in systems that may not fully align with human cognitive capabilities and limitations. This oversight exacerbates biases in LLM outputs and leads to suboptimal user experiences due to inconsistent application of user-centered design principles. Researchers are increasingly leveraging NLP, particularly LLMs, to model and understand human behavior across social sciences, psychology, psychiatry, health, and neuroscience. Our position paper explores the need to integrate cognitive ergonomics into LLM design, providing a comprehensive framework and practical guidelines for ethical development. By addressing these challenges, we aim to advance safer, more reliable, and ethically sound human-AI interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02885v4</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, Mst Rafia Islam</dc:creator>
    </item>
    <item>
      <title>HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and Style Generation in Fashion Design</title>
      <link>https://arxiv.org/abs/2408.00855</link>
      <description>arXiv:2408.00855v3 Announce Type: replace 
Abstract: The process of fashion design usually involves sketching, refining, and coloring, with designers drawing inspiration from various images to fuel their creative endeavors. However, conventional image search methods often yield irrelevant results, impeding the design process. Moreover, creating and coloring sketches can be time-consuming and demanding, acting as a bottleneck in the design workflow. In this work, we introduce HAIGEN (Human-AI Collaboration for GENeration), an efficient fashion design system for Human-AI collaboration developed to aid designers. Specifically, HAIGEN consists of four modules. T2IM, located in the cloud, generates reference inspiration images directly from text prompts. With three other modules situated locally, the I2SM batch generates the image material library into a certain designer-style sketch material library. The SRM recommends similar sketches in the generated library to designers for further refinement, and the STM colors the refined sketch according to the styles of inspiration images. Through our system, any designer can perform local personalized fine-tuning and leverage the powerful generation capabilities of large models in the cloud, streamlining the entire design development process. Given that our approach integrates both cloud and local model deployment schemes, it effectively safeguards design privacy by avoiding the need to upload personalized data from local designers. We validated the effectiveness of each module through extensive qualitative and quantitative experiments. User surveys also confirmed that HAIGEN offers significant advantages in design efficiency, positioning it as a new generation of aid-tool for designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00855v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3678518</arxiv:DOI>
      <dc:creator>Jianan Jiang, Di Wu, Hanhui Deng, Yidan Long, Wenyi Tang, Xiang Li, Can Liu, Zhanpeng Jin, Wenlei Zhang, Tangquan Qi</dc:creator>
    </item>
    <item>
      <title>Evaluation of Task Specific Productivity Improvements Using a Generative Artificial Intelligence Personal Assistant Tool</title>
      <link>https://arxiv.org/abs/2409.14511</link>
      <description>arXiv:2409.14511v2 Announce Type: replace 
Abstract: This study evaluates the productivity improvements achieved using a generative artificial intelligence personal assistant tool (PAT) developed by Trane Technologies. The PAT, based on OpenAI's GPT 3.5 model, was deployed on Microsoft Azure to ensure secure access and protection of intellectual property. To assess the tool's productivity effectiveness, an experiment was conducted comparing the completion times and content quality of four common office tasks: writing an email, summarizing an article, creating instructions for a simple task, and preparing a presentation outline. Sixty-three (63) participants were randomly divided into a test group using the PAT and a control group performing the tasks manually. Results indicated significant productivity enhancements, particularly for tasks involving summarization and instruction creation, with improvements ranging from 3.3% to 69%. The study further analyzed factors such as the age of users, response word counts, and quality of responses, revealing that the PAT users generated more verbose and higher-quality content. An 'LLM-as-a-judge' method employing GPT-4 was used to grade the quality of responses, which effectively distinguished between high and low-quality outputs. The findings underscore the potential of PATs in enhancing workplace productivity and highlight areas for further research and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14511v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian S. Freeman, Kendall Arriola, Dan Cottell, Emmett Lawlor, Matt Erdman, Trevor Sutherland, Brian Wells</dc:creator>
    </item>
    <item>
      <title>Socially-Minded Intelligence: How Individuals, Groups, and AI Systems Can Make Each-Other Smarter (or Not)</title>
      <link>https://arxiv.org/abs/2409.15336</link>
      <description>arXiv:2409.15336v2 Announce Type: replace 
Abstract: A core part of human intelligence is the ability to work flexibly with others to achieve both individual and collective goals. The incorporation of artificial agents into human spaces is making increasing demands on artificial intelligence (AI) to demonstrate and facilitate this ability. However, this kind of flexibility is not well understood because existing approaches to intelligence typically focus either on the individual or the collective level of analysis. At the individual level, intelligence is seen as an individual-difference trait that exists independently of the social environment. At the collective level intelligence is conceptualized as a property of groups, but not in a way that can be used to understand how groups can make group members smarter or how group members acting as individuals might make the group itself more intelligent. In the present paper we argue that by focusing either on individual or collective intelligence without considering their interaction, existing conceptualizations of intelligence limit the potential of people and machines. To address this impasse, we identify and explore a new kind of intelligence - socially-minded intelligence - that can be applied to both individuals (in a social context) and collectives (of individual minds). From a socially-minded intelligence perspective, the potential intelligence of individuals is unlocked in groups, while the potential intelligence of groups is maximized by the flexible, context-sensitive commitment of individual group members. We propose ways in which socially-minded intelligence might be measured and cultivated within people, as well as how it might be modelled in AI systems. Finally, we discuss ways in which socially-minded intelligence might be used to improve human-AI teaming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15336v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William J. Bingley, S. Alexander Haslam, Janet Wiles</dc:creator>
    </item>
    <item>
      <title>Wrapped in Anansi's Web: Unweaving the Impacts of Generative-AI Personalization and VR Immersion in Oral Storytelling</title>
      <link>https://arxiv.org/abs/2409.16894</link>
      <description>arXiv:2409.16894v2 Announce Type: replace 
Abstract: Oral traditions, vital to cultural identity, are losing relevance among youth due to the dominance of modern media. This study addresses the revitalization of these traditions by reconnecting young people with folklore. We introduce Anansi the Spider VR, a novel virtual space that combines first-person virtual reality (VR) with generative artificial intelligence (Gen-AI)-driven narrative personalization. This space immerses users in the Anansi Spider story, empowering them to influence the narrative as they envision themselves as the `protagonists,' thereby enhancing personal reflection. In a 2 by 2 between-subjects study with 48 participants, we employed a mixed-method approach to measure user engagement and changes in interest, complemented by semi-structured interviews providing qualitative insights into personalization and immersion. Our results indicate that personalization in VR significantly boosts engagement and cultural learning interest. We recommend that future studies using VR and Gen-AI to revitalize oral storytelling prioritize respecting cultural integrity and honoring original storytellers and communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16894v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ka Hei Carrie Lau, Bhada Yun, Samuel Saruba, Efe Bozkir, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Understanding Currencies in Video Games: A Review</title>
      <link>https://arxiv.org/abs/2203.14253</link>
      <description>arXiv:2203.14253v2 Announce Type: replace-cross 
Abstract: This paper presents a review of the status of currencies in video games. The business of video games is a multibillion-dollar industry, and its internal economy design is an important field to investigate. In this study, we have distinguished virtual currencies in terms of game mechanics and virtual currency schema, and we have examined 11 games that have used virtual currencies in a significant way and have provided insight for game designers on the internal game economy by showing tangible examples of game mechanics presented in our model</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14253v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/DGRC.2018.8712047</arxiv:DOI>
      <dc:creator>Amir Reza Asadi, Reza Hemadi</dc:creator>
    </item>
    <item>
      <title>GCCRR: A Short Sequence Gait Cycle Segmentation Method Based on Ear-Worn IMU</title>
      <link>https://arxiv.org/abs/2409.00983</link>
      <description>arXiv:2409.00983v2 Announce Type: replace-cross 
Abstract: This paper addresses the critical task of gait cycle segmentation using short sequences from ear-worn IMUs, a practical and non-invasive approach for home-based monitoring and rehabilitation of patients with impaired motor function. While previous studies have focused on IMUs positioned on the lower limbs, ear-worn IMUs offer a unique advantage in capturing gait dynamics with minimal intrusion. To address the challenges of gait cycle segmentation using short sequences, we introduce the Gait Characteristic Curve Regression and Restoration (GCCRR) method, a novel two-stage approach designed for fine-grained gait phase segmentation. The first stage transforms the segmentation task into a regression task on the Gait Characteristic Curve (GCC), which is a one-dimensional feature sequence incorporating periodic information. The second stage restores the gait cycle using peak detection techniques. Our method employs Bi-LSTM-based deep learning algorithms for regression to ensure reliable segmentation for short gait sequences. Evaluation on the HamlynGait dataset demonstrates that GCCRR achieves over 80\% Accuracy, with a Timestamp Error below one sampling interval. Despite its promising results, the performance lags behind methods using more extensive sensor systems, highlighting the need for larger, more diverse datasets. Future work will focus on data augmentation using motion capture systems and improving algorithmic generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00983v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3675094.3680520</arxiv:DOI>
      <dc:creator>Zhenye Xu, Yao Guo</dc:creator>
    </item>
  </channel>
</rss>
