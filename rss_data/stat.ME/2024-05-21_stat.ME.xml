<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 May 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spectral analysis for noisy Hawkes processes inference</title>
      <link>https://arxiv.org/abs/2405.12581</link>
      <description>arXiv:2405.12581v1 Announce Type: new 
Abstract: Classic estimation methods for Hawkes processes rely on the assumption that observed event times are indeed a realisation of a Hawkes process, without considering any potential perturbation of the model. However, in practice, observations are often altered by some noise, the form of which depends on the context.It is then required to model the alteration mechanism in order to infer accurately such a noisy Hawkes process. While several models exist, we consider, in this work, the observations to be the indistinguishable union of event times coming from a Hawkes process and from an independent Poisson process. Since standard inference methods (such as maximum likelihood or Expectation-Maximisation) are either unworkable or numerically prohibitive in this context, we propose an estimation procedure based on the spectral analysis of second order properties of the noisy Hawkes process. Novel results include sufficient conditions for identifiability of the ensuing statistical model with exponential interaction functions for both univariate and bivariate processes. Although we mainly focus on the exponential scenario, other types of kernels are investigated and discussed. A new estimator based on maximising the spectral log-likelihood is then described, and its behaviour is numerically illustrated on synthetic data. Besides being free from knowing the source of each observed time (Hawkes or Poisson process), the proposed estimator is shown to perform accurately in estimating both processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12581v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bonnet (LPSM), Felix Cheysson (LAMA), Miguel Martinez Herrera (LPSM), Maxime Sangnier (LPSM)</dc:creator>
    </item>
    <item>
      <title>Asymptotic Properties of Matthews Correlation Coefficient</title>
      <link>https://arxiv.org/abs/2405.12622</link>
      <description>arXiv:2405.12622v1 Announce Type: new 
Abstract: Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12622v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Itaya, Jun Tamura, Kenichi Hayashi, Kouji Yamamoto</dc:creator>
    </item>
    <item>
      <title>Short and simple introduction to Bellman filtering and smoothing</title>
      <link>https://arxiv.org/abs/2405.12668</link>
      <description>arXiv:2405.12668v1 Announce Type: new 
Abstract: Based on Bellman's dynamic-programming principle, Lange (2024) presents an approximate method for filtering, smoothing and parameter estimation for possibly non-linear and/or non-Gaussian state-space models. While the approach applies more generally, this pedagogical note highlights the main results in the case where (i) the state transition remains linear and Gaussian while (ii) the observation density is log-concave and sufficiently smooth in the state variable. I demonstrate how Kalman's (1960) filter and Rauch et al.'s (1965) smoother can be obtained as special cases within the proposed framework. The main aim is to present non-experts (and my own students) with an accessible introduction, enabling them to implement the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12668v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rutger-Jan Lange</dc:creator>
    </item>
    <item>
      <title>Parameter estimation in Comparative Judgement</title>
      <link>https://arxiv.org/abs/2405.12694</link>
      <description>arXiv:2405.12694v1 Announce Type: new 
Abstract: Comparative Judgement is an assessment method where item ratings are estimated based on rankings of subsets of the items. These rankings are typically pairwise, with ratings taken to be the estimated parameters from fitting a Bradley-Terry model. Likelihood penalization is often employed. Adaptive scheduling of the comparisons can increase the efficiency of the assessment. We show that the most commonly used penalty is not the best-performing penalty under adaptive scheduling and can lead to substantial bias in parameter estimates. We demonstrate this using simulated and real data and provide a theoretical explanation for the relative performance of the penalties considered. Further, we propose a superior approach based on bootstrapping. It is shown to produce better parameter estimates for adaptive schedules and to be robust to variations in underlying strength distributions and initial penalization method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12694v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Hamilton, Nick Tawn</dc:creator>
    </item>
    <item>
      <title>A Non-Parametric Box-Cox Approach to Robustifying High-Dimensional Linear Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2405.12816</link>
      <description>arXiv:2405.12816v1 Announce Type: new 
Abstract: The mainstream theory of hypothesis testing in high-dimensional regression typically assumes the underlying true model is a low-dimensional linear regression model, yet the Box-Cox transformation is a regression technique commonly used to mitigate anomalies like non-additivity and heteroscedasticity. This paper introduces a more flexible framework, the non-parametric Box-Cox model with unspecified transformation, to address model mis-specification in high-dimensional linear hypothesis testing while preserving the interpretation of regression coefficients. Model estimation and computation in high dimensions poses challenges beyond traditional sparse penalization methods. We propose the constrained partial penalized composite probit regression method for sparse estimation and investigate its statistical properties. Additionally, we present a computationally efficient algorithm using augmented Lagrangian and coordinate majorization descent for solving regularization problems with folded concave penalization and linear constraints. For testing linear hypotheses, we propose the partial penalized composite likelihood ratio test, score test and Wald test, and show that their limiting distributions under null and local alternatives follow generalized chi-squared distributions with the same degrees of freedom and noncentral parameter. Extensive simulation studies are conducted to examine the finite sample performance of the proposed tests. Our analysis of supermarket data illustrates potential discrepancies between our testing procedures and standard high-dimensional methods, highlighting the importance of our robustified approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12816v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>He Zhou, Hui Zou</dc:creator>
    </item>
    <item>
      <title>Robust Nonparametric Regression for Compositional Data: the Simplicial--Real case</title>
      <link>https://arxiv.org/abs/2405.12924</link>
      <description>arXiv:2405.12924v1 Announce Type: new 
Abstract: Statistical analysis on compositional data has gained a lot of attention due to their great potential of applications. A feature of these data is that they are multivariate vectors that lie in the simplex, that is, the components of each vector are positive and sum up a constant value. This fact poses a challenge to the analyst due to the internal dependency of the components which exhibit a spurious negative correlation. Since classical multivariate techniques are not appropriate in this scenario, it is necessary to endow the simplex of a suitable algebraic-geometrical structure, which is a starting point to develop adequate methodology and strategies to handle compositions. We centered our attention on regression problems with real responses and compositional covariates and we adopt a nonparametric approach due to the flexibility it provides. Aware of the potential damage that outliers may produce, we introduce a robust estimator in the framework of nonparametric regression for compositional data. The performance of the estimators is investigated by means of a numerical study where different contamination schemes are simulated. Through a real data analysis the advantages of using a robust procedure is illustrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12924v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana M. Bianco, Graciela Boente, Wenceslao Gonz\'alez--Manteiga, Francisco Gude Sampedro, Ana P\'erez--Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction</title>
      <link>https://arxiv.org/abs/2405.12953</link>
      <description>arXiv:2405.12953v1 Announce Type: new 
Abstract: To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12953v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheshi Zheng, Bo Yang, Peter Song</dc:creator>
    </item>
    <item>
      <title>Determine the Number of States in Hidden Markov Models via Marginal Likelihood</title>
      <link>https://arxiv.org/abs/2405.12343</link>
      <description>arXiv:2405.12343v1 Announce Type: cross 
Abstract: Hidden Markov models (HMM) have been widely used by scientists to model stochastic systems: the underlying process is a discrete Markov chain and the observations are noisy realizations of the underlying process. Determining the number of hidden states for an HMM is a model selection problem, which is yet to be satisfactorily solved, especially for the popular Gaussian HMM with heterogeneous covariance. In this paper, we propose a consistent method for determining the number of hidden states of HMM based on the marginal likelihood, which is obtained by integrating out both the parameters and hidden states. Moreover, we show that the model selection problem of HMM includes the order selection problem of finite mixture models as a special case. We give rigorous proof of the consistency of the proposed marginal likelihood method and provide an efficient computation method for practical implementation. We numerically compare the proposed method with the Bayesian information criterion (BIC), demonstrating the effectiveness of the proposed marginal likelihood method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12343v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yang Chen, Cheng-Der Fuh, Chu-Lan Michael Kao</dc:creator>
    </item>
    <item>
      <title>Change Point Detection for High-dimensional Linear Models: A General Tail-adaptive Approach</title>
      <link>https://arxiv.org/abs/2207.11532</link>
      <description>arXiv:2207.11532v3 Announce Type: replace 
Abstract: We propose a novel approach for detecting change points in high-dimensional linear regression models. Unlike previous research that relied on strict Gaussian/sub-Gaussian error assumptions and had prior knowledge of change points, we propose a tail-adaptive method for change point detection and estimation. We use a weighted combination of composite quantile and least squared losses to build a new loss function, allowing us to leverage information from both conditional means and quantiles. For change point testing, we develop a family of individual testing statistics with different weights to account for unknown tail structures. These individual tests are further aggregated to construct a powerful tail-adaptive test for sparse regression coefficient changes. For change point estimation, we propose a family of argmax-based individual estimators. We provide theoretical justifications for the validity of these tests and change point estimators. Additionally, we introduce a new algorithm for detecting multiple change points in a tail-adaptive manner using the wild binary segmentation. Extensive numerical results show the effectiveness of our method. Lastly, an R package called ``TailAdaptiveCpt" is developed to implement our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.11532v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Liu, Zhengling Qi, Xinsheng Zhang, Yufeng Liu</dc:creator>
    </item>
    <item>
      <title>A spectral based goodness-of-fit test for stochastic block models</title>
      <link>https://arxiv.org/abs/2303.14508</link>
      <description>arXiv:2303.14508v2 Announce Type: replace 
Abstract: Community detection is a fundamental problem in complex network data analysis. Though many methods have been proposed, most existing methods require the number of communities to be the known parameter, which is not in practice. In this paper, we propose a novel goodness-of-fit test for the stochastic block model. The test statistic is based on the linear spectral of the adjacency matrix. Under the null hypothesis, we prove that the linear spectral statistic converges in distribution to $N(0,1)$. Some recent results in generalized Wigner matrices are used to prove the main theorems. Numerical experiments and real world data examples illustrate that our proposed linear spectral statistic has good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.14508v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyong Wu, Jiang Hu</dc:creator>
    </item>
    <item>
      <title>Bayesian Geostatistics Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2304.12414</link>
      <description>arXiv:2304.12414v2 Announce Type: replace 
Abstract: We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12414v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang, Wenpin Tang, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Evaluating Binary Outcome Classifiers Estimated from Survey Data</title>
      <link>https://arxiv.org/abs/2311.00596</link>
      <description>arXiv:2311.00596v3 Announce Type: replace 
Abstract: Surveys are commonly used to facilitate research in epidemiology, health, and the social and behavioral sciences. Often, these surveys are not simple random samples, and respondents are given weights reflecting their probability of selection into the survey. It is well known that analysts can use these survey weights to produce unbiased estimates of population quantities like totals. In this article, we show that survey weights also can be beneficial for evaluating the quality of predictive models when splitting data into training and test sets. In particular, we characterize model assessment statistics, such as sensitivity and specificity, as finite population quantities, and compute survey-weighted estimates of these quantities with sample test data comprising a random subset of the original data.Using simulations with data from the National Survey on Drug Use and Health and the National Comorbidity Survey, we show that unweighted metrics estimated with sample test data can misrepresent population performance, but weighted metrics appropriately adjust for the complex sampling design. We also show that this conclusion holds for models trained using upsampling for mitigating class imbalance. The results suggest that weighted metrics should be used when evaluating performance on sample test data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00596v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adway S. Wadekar, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>The case for specifying the "ideal" target trial</title>
      <link>https://arxiv.org/abs/2405.10026</link>
      <description>arXiv:2405.10026v2 Announce Type: replace 
Abstract: The target trial is an increasingly popular conceptual device for guiding the design and analysis of observational studies that seek to perform causal inference. As tends to occur with concepts like this, there is variability in how certain aspects of the approach are understood, which may lead to potentially consequential differences in how the approach is taught, implemented, and interpreted in practice. In this paper, we consider two of these aspects: how the target trial should be specified, and relatedly, how the target trial fits within a formal causal inference framework. We first describe two challenges with what we call the "aligned" approach to target trial specification, which is common in evaluations of medical interventions using healthcare databases and seeks to specify a target trial that is closely aligned to the observational data so that all protocol components apart from randomization can be closely emulated. We then argue how these challenges can be circumvented by an approach that focusses on specifying the "ideal" target trial: a trial with certain idealized aspects that ensure it is relevant to answer the research question. In essence, this approach is applicable in a broader range of settings and enables systematic assessment of all potential sources of causal bias. Importantly, this view provides clarification for how the target trial approach fits within a formal causal inference framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10026v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margarita Moreno-Betancur, Rushani Wijesuriya, John B. Carlin</dc:creator>
    </item>
    <item>
      <title>A Global Wavelet Based Bootstrapped Test of Covariance Stationarity</title>
      <link>https://arxiv.org/abs/2210.14086</link>
      <description>arXiv:2210.14086v3 Announce Type: replace-cross 
Abstract: We propose a covariance stationarity test for an otherwise dependent and possibly globally non-stationary time series. We work in a generalized version of the new setting in Jin, Wang and Wang (2015), who exploit Walsh (1923) functions in order to compare sub-sample covariances with the full sample counterpart. They impose strict stationarity under the null, only consider linear processes under either hypothesis in order to achieve a parametric estimator for an inverted high dimensional asymptotic covariance matrix, and do not consider any other orthonormal basis. Conversely, we work with a general orthonormal basis under mild conditions that include Haar wavelet and Walsh functions; and we allow for linear or nonlinear processes with possibly non-iid innovations. This is important in macroeconomics and finance where nonlinear feedback and random volatility occur in many settings. We completely sidestep asymptotic covariance matrix estimation and inversion by bootstrapping a max-correlation difference statistic, where the maximum is taken over the correlation lag $h$ and basis generated sub-sample counter $k$ (the number of systematic samples). We achieve a higher feasible rate of increase for the maximum lag and counter $\mathcal{H}_{T}$ and $\mathcal{K}_{T}$. Of particular note, our test is capable of detecting breaks in variance, and distant, or very mild, deviations from stationarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.14086v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan B. Hill, Tianqi Li</dc:creator>
    </item>
    <item>
      <title>Leveraging text data for causal inference using electronic health records</title>
      <link>https://arxiv.org/abs/2307.03687</link>
      <description>arXiv:2307.03687v2 Announce Type: replace-cross 
Abstract: In studies that rely on data from electronic health records (EHRs), unstructured text data such as clinical progress notes offer a rich source of information about patient characteristics and care that may be missing from structured data. Despite the prevalence of text in clinical research, these data are often ignored for the purposes of quantitative analysis due their complexity. This paper presents a unified framework for leveraging text data to support causal inference with electronic health data at multiple stages of analysis. In particular, we consider how natural language processing and statistical text analysis can be combined with standard inferential techniques to address common challenges due to missing data, confounding bias, and treatment effect heterogeneity. Through an application to a recent EHR study investigating the effects of a non-randomized medical intervention on patient outcomes, we show how incorporating text data in a traditional matching analysis can help strengthen the validity of an estimated treatment effect and identify patient subgroups that may benefit most from treatment. We believe these methods have the potential to expand the scope of secondary analysis of clinical data to domains where structured EHR data is limited, such as in developing countries. To this end, we provide code and open-source replication materials to encourage adoption and broader exploration of these techniques in clinical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03687v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reagan Mozer, Aaron R. Kaufman, Leo A. Celi, Luke Miratrix</dc:creator>
    </item>
    <item>
      <title>Improving Ego-Cluster for Network Effect Measurement</title>
      <link>https://arxiv.org/abs/2308.05945</link>
      <description>arXiv:2308.05945v3 Announce Type: replace-cross 
Abstract: The network effect, wherein one user's activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn's overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05945v3</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wentao Su, Weitao Duan</dc:creator>
    </item>
    <item>
      <title>On the Injectivity of Euler Integral Transforms with Hyperplanes and Quadric Hypersurfaces</title>
      <link>https://arxiv.org/abs/2312.10002</link>
      <description>arXiv:2312.10002v2 Announce Type: replace-cross 
Abstract: The Euler characteristic transform (ECT) is an integral transform used widely in topological data analysis. Previous efforts by Curry et al. and Ghrist et al. have independently shown that the ECT is injective on all compact definable sets. In this work, we first study the injectivity of the ECT on definable sets that are not necessarily compact and prove a complete classification of constructible functions that the Euler characteristic transform is not injective on. We then introduce the quadric Euler characteristic transform (QECT) as a natural generalization of the ECT by detecting definable shapes with quadric hypersurfaces rather than hyperplanes. We also discuss some criteria for the injectivity of QECT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10002v2</guid>
      <category>cs.CG</category>
      <category>math.AT</category>
      <category>math.GN</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mattie Ji</dc:creator>
    </item>
    <item>
      <title>Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</title>
      <link>https://arxiv.org/abs/2402.08845</link>
      <description>arXiv:2402.08845v2 Announce Type: replace-cross 
Abstract: We investigate the problem of explainability in machine learning. To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations. However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation. In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the Probability of Necessity and Sufficiency (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance. Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional). In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Our source code is available at \url{https://github.com/DMIRLAB-Group/FANS}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08845v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title>
      <link>https://arxiv.org/abs/2405.00161</link>
      <description>arXiv:2405.00161v2 Announce Type: replace-cross 
Abstract: Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for "item-level" HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials containing 2.3 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00161v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</dc:creator>
    </item>
  </channel>
</rss>
