<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Asymptotic Optimality of Least Squares Model Averaging When True Model Is Included</title>
      <link>https://arxiv.org/abs/2411.09258</link>
      <description>arXiv:2411.09258v1 Announce Type: new 
Abstract: Asymptotic optimality is a key theoretical property in model averaging. Due to technical difficulties, existing studies rely on restricted weight sets or the assumption that there is no true model with fixed dimensions in the candidate set. The focus of this paper is to overcome these difficulties. Surprisingly, we discover that when the penalty factor in the weight selection criterion diverges with a certain order and the true model dimension is fixed, asymptotic loss optimality does not hold, but asymptotic risk optimality does. This result differs from the corresponding result of Fang et al. (2023, Econometric Theory 39, 412-441) and reveals that using the discrete weight set of Hansen (2007, Econometrica 75, 1175-1189) can yield opposite asymptotic properties compared to using the usual weight set. Simulation studies illustrate the theoretical findings in a variety of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09258v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenchao Xu, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>On importance sampling and independent Metropolis-Hastings with an unbounded weight function</title>
      <link>https://arxiv.org/abs/2411.09514</link>
      <description>arXiv:2411.09514v1 Announce Type: new 
Abstract: Importance sampling and independent Metropolis-Hastings (IMH) are among the fundamental building blocks of Monte Carlo methods. Both require a proposal distribution that globally approximates the target distribution. The Radon-Nikodym derivative of the target distribution relative to the proposal is called the weight function. Under the weak assumption that the weight is unbounded but has a number of finite moments under the proposal distribution, we obtain new results on the approximation error of importance sampling and of the particle independent Metropolis-Hastings algorithm (PIMH), which includes IMH as a special case. For IMH and PIMH, we show that the common random numbers coupling is maximal. Using that coupling we derive bounds on the total variation distance of a PIMH chain to the target distribution. The bounds are sharp with respect to the number of particles and the number of iterations. Our results allow a formal comparison of the finite-time biases of importance sampling and IMH. We further consider bias removal techniques using couplings of PIMH, and provide conditions under which the resulting unbiased estimators have finite moments. We compare the asymptotic efficiency of regular and unbiased importance sampling estimators as the number of particles goes to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09514v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Deligiannidis (University of Oxford), Pierre E. Jacob (ESSEC Business School), El Mahdi Khribch (ESSEC Business School), Guanyang Wang (Rutgers University)</dc:creator>
    </item>
    <item>
      <title>Debiased machine learning for counterfactual survival functionals based on left-truncated right-censored data</title>
      <link>https://arxiv.org/abs/2411.09017</link>
      <description>arXiv:2411.09017v1 Announce Type: cross 
Abstract: Learning causal effects of a binary exposure on time-to-event endpoints can be challenging because survival times may be partially observed due to censoring and systematically biased due to truncation. In this work, we present debiased machine learning-based nonparametric estimators of the joint distribution of a counterfactual survival time and baseline covariates for use when the observed data are subject to covariate-dependent left truncation and right censoring and when baseline covariates suffice to deconfound the relationship between exposure and survival time. Our inferential procedures explicitly allow the integration of flexible machine learning tools for nuisance estimation, and enjoy certain robustness properties. The approach we propose can be directly used to make pointwise or uniform inference on smooth summaries of the joint counterfactual survival time and covariate distribution, and can be valuable even in the absence of interventions, when summaries of a marginal survival distribution are of interest. We showcase how our procedures can be used to learn a variety of inferential targets and illustrate their performance in simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09017v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric R. Morenz, Charles J. Wolock, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Arrangements and Likelihood</title>
      <link>https://arxiv.org/abs/2411.09508</link>
      <description>arXiv:2411.09508v1 Announce Type: cross 
Abstract: We develop novel tools for computing the likelihood correspondence of an arrangement of hypersurfaces in a projective space. This uses the module of logarithmic derivations. This object is well-studied in the linear case, when the hypersurfaces are hyperplanes. We here focus on nonlinear scenarios and their applications in statistics and physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09508v1</guid>
      <category>math.AC</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Kahle, Lukas K\"uhne, Leonie M\"uhlherr, Bernd Sturmfels, Maximilian Wiesmann</dc:creator>
    </item>
    <item>
      <title>Sharp Matrix Empirical Bernstein Inequalities</title>
      <link>https://arxiv.org/abs/2411.09516</link>
      <description>arXiv:2411.09516v1 Announce Type: cross 
Abstract: We present two sharp empirical Bernstein inequalities for symmetric random matrices with bounded eigenvalues. By sharp, we mean that both inequalities adapt to the unknown variance in a tight manner: the deviation captured by the first-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernstein inequality exactly, including constants, the latter requiring knowledge of the variance. Our first inequality holds for the sample mean of independent matrices, and our second inequality holds for a mean estimator under martingale dependence at stopping times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09516v1</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>M-Variance Asymptotics and Uniqueness of Descriptors</title>
      <link>https://arxiv.org/abs/2011.14762</link>
      <description>arXiv:2011.14762v2 Announce Type: replace 
Abstract: Asymptotic theory for M-estimation problems usually focuses on the asymptotic convergence of the sample descriptor, defined as the minimizer of the sample loss function. Here, we explore a related question and formulate asymptotic theory for the minimum value of sample loss, the M-variance. Since the loss function value is always a real number, the asymptotic theory for the M-variance is comparatively simple. M-variance often satisfies a standard central limit theorem, even in situations where the asymptotics of the descriptor is more complicated as for example in case of smeariness, or if no asymptotic distribution can be given as can be the case if the descriptor space is a general metric space. We use the asymptotic results for the M-variance to formulate a hypothesis test to systematically determine for a given sample whether the underlying population loss function may have multiple global minima. We discuss three applications of our test to data, each of which presents a typical scenario in which non-uniqueness of descriptors may occur. These model scenarios are the mean on a non-euclidean space, non-linear regression and Gaussian mixture clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.14762v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Eltzner</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v2 Announce Type: replace 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. The book is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes three chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains five chapters of advanced topics. We hope that, by putting the materials together in this book, the concept of e-values becomes more accessible for educational, research, and practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Fourier analysis of spatial point processes</title>
      <link>https://arxiv.org/abs/2401.06403</link>
      <description>arXiv:2401.06403v2 Announce Type: replace-cross 
Abstract: In this article, we develop comprehensive frequency domain methods for estimating and inferring the second-order structure of spatial point processes. The main element here is on utilizing the discrete Fourier transform (DFT) of the point pattern and its tapered counterpart. Under second-order stationarity, we show that both the DFTs and the tapered DFTs are asymptotically jointly independent Gaussian even when the DFTs share the same limiting frequencies. Based on these results, we establish an $\alpha$-mixing central limit theorem for a statistic formulated as a quadratic form of the tapered DFT. As applications, we derive the asymptotic distribution of the kernel spectral density estimator and establish a frequency domain inferential method for parametric stationary point processes. For the latter, the resulting model parameter estimator is computationally tractable and yields meaningful interpretations even in the case of model misspecification. We investigate the finite sample performance of our estimator through simulations, considering scenarios of both correctly specified and misspecified models. Furthermore, we extend our proposed DFT-based frequency domain methods to a class of non-stationary spatial point processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06403v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang, Yongtao Guan</dc:creator>
    </item>
    <item>
      <title>Learning Properties of Quantum States Without the I.I.D. Assumption</title>
      <link>https://arxiv.org/abs/2401.16922</link>
      <description>arXiv:2401.16922v2 Announce Type: replace-cross 
Abstract: We develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. We prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in training data size (aka sample complexity). Importantly, this polynomial increase in sample complexity can be substantially improved to polylogarithmic if the learning algorithm in question only requires non-adaptive, single-copy measurements. Among other applications, this allows us to generalize the classical shadow framework to the non-i.i.d. setting while only incurring a comparatively small loss in sample efficiency. We use rigorous quantum information theory to prove our main results. In particular, we leverage permutation invariance and randomized single-copy measurements to derive a new quantum de Finetti theorem that mainly addresses measurement outcome statistics and, in turn, scales much more favorably in Hilbert space dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16922v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41467-024-53765-6</arxiv:DOI>
      <arxiv:journal_reference>Nature Communications, 15, Article number: 9677 (2024)</arxiv:journal_reference>
      <dc:creator>Omar Fawzi, Richard Kueng, Damian Markham, Aadil Oufkir</dc:creator>
    </item>
    <item>
      <title>The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms</title>
      <link>https://arxiv.org/abs/2405.19585</link>
      <description>arXiv:2405.19585v2 Announce Type: replace-cross 
Abstract: We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provide our code for evaluation at https://github.com/amackenzie1/highline2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19585v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabeth Collins-Woodfin, Inbar Seroussi, Bego\~na Garc\'ia Malaxechebarr\'ia, Andrew W. Mackenzie, Elliot Paquette, Courtney Paquette</dc:creator>
    </item>
    <item>
      <title>Joint identifiability of ancestral sequence, phylogeny and mutation rates under the TKF91 model</title>
      <link>https://arxiv.org/abs/2410.09620</link>
      <description>arXiv:2410.09620v2 Announce Type: replace-cross 
Abstract: We consider the problem of identifying jointly the ancestral sequence, the phylogeny and the parameters in models of DNA sequence evolution with insertion and deletion (indel). Under the classical TKF91 model of sequence evolution, we obtained explicit formulas for the root sequence, the pairwise distances of leaf sequences, as well as the scaled rates of indel and substitution in terms of the distribution of the leaf sequences of an arbitrary phylogeny. These explicit formulas not only strengthen existing invertibility results and work for phylogeny that are not necessarily ultrametric, but also lead to new estimators with less assumption compared with the existing literature. Our simulation study demonstrates that these estimators are statistically consistent as the number of independent samples tends to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09620v2</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Xue, Brandon Legried, Wai-Tong Louis Fan</dc:creator>
    </item>
  </channel>
</rss>
