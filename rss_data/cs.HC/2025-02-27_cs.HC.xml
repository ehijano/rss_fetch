<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 02:52:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Investigating Youth AI Auditing</title>
      <link>https://arxiv.org/abs/2502.18576</link>
      <description>arXiv:2502.18576v1 Announce Type: new 
Abstract: Youth are active users and stakeholders of artificial intelligence (AI), yet they are often not included in responsible AI (RAI) practices. Emerging efforts in RAI largely focus on adult populations, missing an opportunity to get unique perspectives of youth. This study explores the potential of youth (teens under the age of 18) to engage meaningfully in RAI, specifically through AI auditing. In a workshop study with 17 teens, we investigated how youth can actively identify problematic behaviors in youth-relevant ubiquitous AI (text-to-image generative AI, autocompletion in search bar, image search) and the impacts of supporting AI auditing with critical AI literacy scaffolding with guided discussion about AI ethics and an auditing tool. We found that youth can contribute quality insights, shaped by their expertise (e.g., hobbies and passions), lived experiences (e.g., social identities), and age-related knowledge (e.g., understanding of fast-moving trends). We discuss how empowering youth in AI auditing can result in more responsible AI, support their learning through doing, and lead to implications for including youth in various participatory RAI processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18576v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaemarie Solyst, Cindy Peng, Wesley Hanwen Deng, Praneetha Pratapa, Jessica Hammer, Amy Ogan, Jason Hong, Motahhare Eslami</dc:creator>
    </item>
    <item>
      <title>Towards Using Voice for Hedonic Shopping Motivations</title>
      <link>https://arxiv.org/abs/2502.18614</link>
      <description>arXiv:2502.18614v1 Announce Type: new 
Abstract: Besides the utilitarian aspects of online shopping, hedonic motivations play a significant role in shaping the shopping behavior of online users. With the increased popularity of voice-enabled devices, online shopping platforms have attempted to drive online shopping on voice. However, we explain why voice might be more suitable for the hedonic aspects of shopping. We introduce a prototype that enables such focus in a voice experience and share our findings from a qualitative study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18614v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morteza Behrooz, Preetham Kolari, Fred Zaw, Lindsay Kenzig, Arnav Jhala</dc:creator>
    </item>
    <item>
      <title>eXplainMR: Generating Real-time Textual and Visual eXplanations to Facilitate UltraSonography Learning in MR</title>
      <link>https://arxiv.org/abs/2502.18640</link>
      <description>arXiv:2502.18640v1 Announce Type: new 
Abstract: eXplainMR is a Mixed Reality tutoring system designed for basic cardiac surface ultrasound training. Trainees wear a head-mounted display (HMD) and hold a controller, mimicking a real ultrasound probe, while treating a desk surface as the patient's body for low-cost and anywhere training. eXplainMR engages trainees with troubleshooting questions and provides automated feedback through four key mechanisms: 1) subgoals that break down tasks into single-movement steps, 2) textual explanations comparing the current incorrect view with the target view, 3) real-time segmentation and annotation of ultrasound images for direct visualization, and 4) the 3D visual cues provide further explanations on the intersection between the slicing plane and anatomies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18640v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714015</arxiv:DOI>
      <arxiv:journal_reference>CHI'2025</arxiv:journal_reference>
      <dc:creator>Jingying Wang, Jingjing Zhang, Juana Nicoll Capizzano, Matthew Sigakis, Xu Wang, Vitaliy Popov</dc:creator>
    </item>
    <item>
      <title>WhatELSE: Shaping Narrative Spaces at Configurable Level of Abstraction for AI-bridged Interactive Storytelling</title>
      <link>https://arxiv.org/abs/2502.18641</link>
      <description>arXiv:2502.18641v1 Announce Type: new 
Abstract: Generative AI significantly enhances player agency in interactive narratives (IN) by enabling just-in-time content generation that adapts to player actions. While delegating generation to AI makes IN more interactive, it becomes challenging for authors to control the space of possible narratives - within which the final story experienced by the player emerges from their interaction with AI. In this paper, we present WhatELSE, an AI-bridged IN authoring system that creates narrative possibility spaces from example stories. WhatELSE provides three views (narrative pivot, outline, and variants) to help authors understand the narrative space and corresponding tools leveraging linguistic abstraction to control the boundaries of the narrative space. Taking innovative LLM-based narrative planning approaches, WhatELSE further unfolds the narrative space into executable game events. Through a user study (N=12) and technical evaluations, we found that WhatELSE enables authors to perceive and edit the narrative space and generates engaging interactive narratives at play-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18641v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhuoran Lu, Qian Zhou, Yi Wang</dc:creator>
    </item>
    <item>
      <title>Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support</title>
      <link>https://arxiv.org/abs/2502.18658</link>
      <description>arXiv:2502.18658v1 Announce Type: new 
Abstract: AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and \revise{interaction context support} alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18658v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Pu, Daniel Lazaro, Ian Arawjo, Haijun Xia, Ziang Xiao, Tovi Grossman, Yan Chen</dc:creator>
    </item>
    <item>
      <title>Scaffolding Empathy: Training Counselors with Simulated Patients and Utterance-level Performance Visualizations</title>
      <link>https://arxiv.org/abs/2502.18673</link>
      <description>arXiv:2502.18673v1 Announce Type: new 
Abstract: Learning therapeutic counseling involves significant role-play experience with mock patients, with current manual training methods providing only intermittent granular feedback. We seek to accelerate and optimize counselor training by providing frequent, detailed feedback to trainees as they interact with a simulated patient. Our first application domain involves training motivational interviewing skills for counselors. Motivational interviewing is a collaborative counseling style in which patients are guided to talk about changing their behavior, with empathetic counseling an essential ingredient. We developed and evaluated an LLM-powered training system that features a simulated patient and visualizations of turn-by-turn performance feedback tailored to the needs of counselors learning motivational interviewing. We conducted an evaluation study with professional and student counselors, demonstrating high usability and satisfaction with the system. We present design implications for the development of automated systems that train users in counseling skills and their generalizability to other types of social skills training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18673v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714014</arxiv:DOI>
      <dc:creator>Ian Steenstra, Farnaz Nouraei, Timothy W. Bickmore</dc:creator>
    </item>
    <item>
      <title>Interacting with Thoughtful AI</title>
      <link>https://arxiv.org/abs/2502.18676</link>
      <description>arXiv:2502.18676v1 Announce Type: new 
Abstract: We envision the concept of Thoughtful AI, a new human-AI interaction paradigm in which the AI behaves as a continuously thinking entity. Unlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful AI autonomously generates, develops, and communicates its evolving thought process throughout an interaction. In this position paper, we argue that this thoughtfulness unlocks new possibilities for human-AI interaction by enabling proactive AI behavior, facilitating continuous cognitive alignment with users, and fostering more dynamic interaction experiences. We outline the conceptual foundations of Thoughtful AI, illustrate its potential through example projects, and envision how this paradigm can transform human-AI interaction in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18676v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Bruce Liu, Haijun Xia, Xiang Anthony Chen</dc:creator>
    </item>
    <item>
      <title>Comparing Native and Non-native English Speakers' Behaviors in Collaborative Writing through Visual Analytics</title>
      <link>https://arxiv.org/abs/2502.18681</link>
      <description>arXiv:2502.18681v1 Announce Type: new 
Abstract: Understanding collaborative writing dynamics between native speakers (NS) and non-native speakers (NNS) is critical for enhancing collaboration quality and team inclusivity. In this paper, we partnered with communication researchers to develop visual analytics solutions for comparing NS and NNS behaviors in 162 writing sessions across 27 teams. The primary challenges in analyzing writing behaviors are data complexity and the uncertainties introduced by automated methods. In response, we present \textsc{COALA}, a novel visual analytics tool that improves model interpretability by displaying uncertainties in author clusters, generating behavior summaries using large language models, and visualizing writing-related actions at multiple granularities. We validated the effectiveness of \textsc{COALA} through user studies with domain experts (N=2+2) and researchers with relevant experience (N=8). We present the insights discovered by participants using \textsc{COALA}, suggest features for future AI-assisted collaborative writing tools, and discuss the broader implications for analyzing collaborative processes beyond writing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18681v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713693</arxiv:DOI>
      <dc:creator>Yuexi Chen, Yimin Xiao, Kazi Tasnim Zinat, Naomi Yamashita, Ge Gao, Zhicheng Liu</dc:creator>
    </item>
    <item>
      <title>AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development</title>
      <link>https://arxiv.org/abs/2502.18682</link>
      <description>arXiv:2502.18682v1 Announce Type: new 
Abstract: AI systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant "AI Mismatches", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18682v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devansh Saxena, Ji-Youn Jung, Jodi Forlizzi, Kenneth Holstein, John Zimmerman</dc:creator>
    </item>
    <item>
      <title>Robots, Chatbots, Self-Driving Cars: Perceptions of Mind and Morality Across Artificial Intelligences</title>
      <link>https://arxiv.org/abs/2502.18683</link>
      <description>arXiv:2502.18683v1 Announce Type: new 
Abstract: AI systems have rapidly advanced, diversified, and proliferated, but our knowledge of people's perceptions of mind and morality in them is limited, despite its importance for outcomes such as whether people trust AIs and how they assign responsibility for AI-caused harms. In a preregistered online study, 975 participants rated 26 AI and non-AI entities. Overall, AIs were perceived to have low-to-moderate agency (e.g., planning, acting), between inanimate objects and ants, and low experience (e.g., sensing, feeling). For example, ChatGPT was rated only as capable of feeling pleasure and pain as a rock. The analogous moral faculties, moral agency (doing right or wrong) and moral patiency (being treated rightly or wrongly) were higher and more varied, particularly moral agency: The highest-rated AI, a Tesla Full Self-Driving car, was rated as morally responsible for harm as a chimpanzee. We discuss how design choices can help manage perceptions, particularly in high-stakes moral contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18683v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713130</arxiv:DOI>
      <dc:creator>Ali Ladak, Matti Wilks, Steve Loughnan, Jacy Reese Anthis</dc:creator>
    </item>
    <item>
      <title>Emerging Practices in Participatory AI Design in Public Sector Innovation</title>
      <link>https://arxiv.org/abs/2502.18689</link>
      <description>arXiv:2502.18689v1 Announce Type: new 
Abstract: Local and federal agencies are rapidly adopting AI systems to augment or automate critical decisions, efficiently use resources, and improve public service delivery. AI systems are being used to support tasks associated with urban planning, security, surveillance, energy and critical infrastructure, and support decisions that directly affect citizens and their ability to access essential services. Local governments act as the governance tier closest to citizens and must play a critical role in upholding democratic values and building community trust especially as it relates to smart city initiatives that seek to transform public services through the adoption of AI. Community-centered and participatory approaches have been central for ensuring the appropriate adoption of technology; however, AI innovation introduces new challenges in this context because participatory AI design methods require more robust formulation and face higher standards for implementation in the public sector compared to the private sector. This requires us to reassess traditional methods used in this space as well as develop new resources and methods. This workshop will explore emerging practices in participatory algorithm design - or the use of public participation and community engagement - in the scoping, design, adoption, and implementation of public sector algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18689v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devansh Saxena, Zoe Kahn, Erina Seh-Young Moon, Lauren M. Chambers, Corey Jackson, Min Kyung Lee, Motahhare Eslami, Shion Guha, Sheena Erete, Lilly Irani, Deirdre Mulligan, John Zimmerman</dc:creator>
    </item>
    <item>
      <title>From Cluttered to Clear: Improving the Web Accessibility Design for Screen Reader Users in E-commerce With Generative AI</title>
      <link>https://arxiv.org/abs/2502.18701</link>
      <description>arXiv:2502.18701v1 Announce Type: new 
Abstract: Online interactions and e-commerce are commonplace among BLV users. Despite the implementation of web accessibility standards, many e-commerce platforms continue to present challenges to screen reader users, particularly in areas like webpage navigation and information retrieval. We investigate the difficulties encountered by screen reader users during online shopping experiences. We conducted a formative study with BLV users and designed a web browser plugin that uses GenAI to restructure webpage content in real time. Our approach improved the header hierarchy and provided correct labeling for essential information. We evaluated the effectiveness of this solution using an automated accessibility tool and through user interviews. Our results show that the revised webpages generated by our system offer significant improvements over the original webpages regarding screen reader navigation experience. Based on our findings, we discuss its potential usage as both a user and developer tool that can significantly enhance screen reader accessibility of webpages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18701v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaman Yu, Bektur Ryskeldiev, Ayaka Tsutsui, Matthew Gillingham, Yang Wang</dc:creator>
    </item>
    <item>
      <title>Understanding Children's Avatar Making in Social Online Games</title>
      <link>https://arxiv.org/abs/2502.18705</link>
      <description>arXiv:2502.18705v1 Announce Type: new 
Abstract: Social online games like Minecraft and Roblox have become increasingly integral to children's daily lives. Our study explores how children aged 8 to 13 create and customize avatars in these virtual environments. Through semi-structured interviews and gameplay observations with 48 participants, we investigate the motivations behind children's avatar-making. Our findings show that children's avatar creation is motivated by self-representation, experimenting with alter ego identities, fulfilling social needs, and improving in-game performance. In addition, designed monetization strategies play a role in shaping children's avatars. We identify the ''wardrobe effect,'' where children create multiple avatars but typically use only one favorite consistently. We discuss the impact of cultural consumerism and how social games can support children's identity exploration while balancing self-expression and social conformity. This work contributes to understanding how avatar shapes children's identity growth in social online games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18705v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yue Fu, Samuel Schwamm, Amanda Baughan, Nicole M Powell, Zoe Kronberg, Alicia Owens, Emily Renee Izenman, Dania Alsabeh, Elizabeth Hunt, Michael Rich, David Bickham, Jenny Radesky, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer Interfaces with Optimized Channel Selection</title>
      <link>https://arxiv.org/abs/2502.18719</link>
      <description>arXiv:2502.18719v1 Announce Type: new 
Abstract: Achieving high subject-independent accuracy in functional near-infrared spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a challenge, particularly when minimizing the number of channels. This study proposes a novel feature extraction scheme and a Pearson correlation-based channel selection algorithm to enhance classification accuracy while reducing hardware complexity. Using an open-access fNIRS dataset, our method improved average accuracy by 28.09% compared to existing approaches, achieving a peak subject-independent accuracy of 95.98% with only two channels. These results demonstrate the potential of our optimized feature extraction and channel selection methods for developing efficient, subject-independent fNIRS-based BCI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18719v1</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxin Li, Hao Fang, Wen Liu, Chuantong Cheng, Hongda Chen</dc:creator>
    </item>
    <item>
      <title>AI-Instruments: Embodying Prompts as Instruments to Abstract &amp; Reflect Graphical Interface Commands as General-Purpose Tools</title>
      <link>https://arxiv.org/abs/2502.18736</link>
      <description>arXiv:2502.18736v1 Announce Type: new 
Abstract: Chat-based prompts respond with verbose linear-sequential texts, making it difficult to explore and refine ambiguous intents, back up and reinterpret, or shift directions in creative AI-assisted design work. AI-Instruments instead embody "prompts" as interface objects via three key principles: (1) Reification of user-intent as reusable direct-manipulation instruments; (2) Reflection of multiple interpretations of ambiguous user-intents (Reflection-in-intent) as well as the range of AI-model responses (Reflection-in-response) to inform design "moves" towards a desired result; and (3) Grounding to instantiate an instrument from an example, result, or extrapolation directly from another instrument. Further, AI-Instruments leverage LLM's to suggest, vary, and refine new instruments, enabling a system that goes beyond hard-coded functionality by generating its own instrumental controls from content. We demonstrate four technology probes, applied to image generation, and qualitative insights from twelve participants, showing how AI-Instruments address challenges of intent formulation, steering via direct manipulation, and non-linear iterative workflows to reflect and resolve ambiguous intents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18736v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714259</arxiv:DOI>
      <dc:creator>Nathalie Riche, Anna Offenwanger, Frederic Gmeiner, David Brown, Hugo Romat, Michel Pahud, Nicolai Marquardt, Kori Inkpen, Ken Hinckley</dc:creator>
    </item>
    <item>
      <title>Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows</title>
      <link>https://arxiv.org/abs/2502.18737</link>
      <description>arXiv:2502.18737v1 Announce Type: new 
Abstract: Despite Generative AI (GenAI) systems' potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the AI system (prompt formulation), and insufficient flexibility of AI systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags - small, atomic conceptual units that encapsulate user intent - for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18737v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713861</arxiv:DOI>
      <dc:creator>Frederic Gmeiner, Nicolai Marquardt, Michael Bentley, Hugo Romat, Michel Pahud, David Brown, Asta Roseway, Nikolas Martelaro, Kenneth Holstein, Ken Hinckley, Nathalie Riche</dc:creator>
    </item>
    <item>
      <title>Reimagining Personal Data: Unlocking the Potential of AI-Generated Images in Personal Data Meaning-Making</title>
      <link>https://arxiv.org/abs/2502.18853</link>
      <description>arXiv:2502.18853v1 Announce Type: new 
Abstract: Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-based application as a probe that represents personal data through generative images utilizing Open AI's GPT-4 model and DALL-E 3. We then conducted a 21-day diary study and interviews using the probe with 16 participants to investigate users' in-depth experiences with images generated by AI in everyday lives. Our findings reveal new qualities of experiences in users' engagement with data, highlighting how participants constructed personal meaning from their data through imagination and speculation on AI-generated images. We conclude by discussing the potential and concerns of leveraging image-generative AI for personal data meaning-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18853v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soobin Park, Hankyung Kim, Youn-kyung Lim</dc:creator>
    </item>
    <item>
      <title>The Design Space for Online Restorative Justice Tools: A Case Study with ApoloBot</title>
      <link>https://arxiv.org/abs/2502.18861</link>
      <description>arXiv:2502.18861v1 Announce Type: new 
Abstract: Volunteer moderators use various strategies to address online harms within their communities. Although punitive measures like content removal or account bans are common, recent research has explored the potential for restorative justice as an alternative framework to address the distinct needs of victims, offenders, and community members. In this study, we take steps toward identifying a more concrete design space for restorative justice-oriented tools by developing ApoloBot, a Discord bot designed to facilitate apologies when harm occurs in online communities. We present results from two rounds of interviews: first, with moderators giving feedback about the design of ApoloBot, and second, after a subset of these moderators have deployed ApoloBot in their communities. This study builds on prior work to yield more detailed insights regarding the potential of adopting online restorative justice tools, including opportunities, challenges, and implications for future designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18861v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713598</arxiv:DOI>
      <dc:creator>Bich Ngoc (Rubi),  Doan, Joseph Seering</dc:creator>
    </item>
    <item>
      <title>Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration</title>
      <link>https://arxiv.org/abs/2502.18881</link>
      <description>arXiv:2502.18881v1 Announce Type: new 
Abstract: Young adults often encounter challenges in career exploration. Self-guided interventions, such as the letter-exchange exercise, where participants envision and adopt the perspective of their future selves by exchanging letters with their envisioned future selves, can support career development. However, the broader adoption of such interventions may be limited without structured guidance. To address this, we integrated Large Language Model (LLM)-based agents that simulate participants' future selves into the letter-exchange exercise and evaluated their effectiveness. A one-week experiment (N=36) compared three conditions: (1) participants manually writing replies to themselves from the perspective of their future selves (baseline), (2) future-self agents generating letters to participants, and (3) future-self agents engaging in chat conversations with participants. Results indicated that exchanging letters with future-self agents enhanced participants' engagement during the exercise, while overall benefits of the intervention on future orientation, career self-concept, and psychological support remained comparable across conditions. We discuss design implications for AI-augmented interventions for supporting young adults' career exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18881v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714206</arxiv:DOI>
      <dc:creator>Hayeon Jeon, Suhwoo Yoon, Keyeun Lee, Seo Hyeong Kim, Esther Hehsun Kim, Seonghye Cho, Yena Ko, Soeun Yang, Laura Dabbish, John Zimmerman, Eun-mee Kim, Hajin Lim</dc:creator>
    </item>
    <item>
      <title>Effects of Linear Modulation of Electrotactile Signals Using a Novel Device on Sensation Naturalness and Perceptual Intensity</title>
      <link>https://arxiv.org/abs/2502.19066</link>
      <description>arXiv:2502.19066v1 Announce Type: new 
Abstract: Electrotactile feedback is a promising method for delivering haptic sensations, but challenges such as the naturalness of sensations hinder its adoption in commercial devices. In this study, we introduce a novel device that enables the exploration of complex stimulation signals to enhance sensation naturalness. We designed six stimulation signals with linearly modulated frequency, amplitude, or both, across two frequency levels based on a ramp-and-hold shape, aiming to replicate sensation of pressing a button. Our results showed that these modulated signals achieve higher naturalness scores than tonic stimulations, with a 6.8% improvement. Moreover, we examined the relationship between perceived intensity and signal energy for these stimulation patterns. Our findings indicate that, under conditions of constant perceived intensity, signal energy is not uniform across different stimulation patterns. Instead, there is a distinct relationship between the energy levels of different patterns, which is consistently reflected in the energy of the stimulations selected by the participants. Based on our findings, we propose a predictive model that estimates the desired intensity for any stimulation pattern using this relationship between signal energies and the user's preferred intensity for a single reference pattern. This model demonstrated high reliability, with a mean R2 score of 83.33%. Using this approach, intensity calibration for different stimulation patterns can be streamlined, reducing calibration time by 87.5%, as only one out of eight reference pattern must be calibrated. These findings highlight the potential of stimulation signal modulation to improve sensation quality and validate the viability of our predictive model for automating intensity calibration. This approach is an essential step toward delivering complex and naturalistic sensations in advanced haptic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19066v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amirhossein Bayat, Melika Emami, Rahim Tafazolli, Atta Quddus</dc:creator>
    </item>
    <item>
      <title>Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation</title>
      <link>https://arxiv.org/abs/2502.19082</link>
      <description>arXiv:2502.19082v1 Announce Type: new 
Abstract: Through a three-part co-design study involving 19 teens aged 13-18, we identify key barriers to effective boundary regulation on social media, including ambiguous audience expectations, social risks associated with oversharing, and the lack of design affordances that facilitate trust-building. Our findings reveal that while adolescents seek casual, frequent sharing to strengthen relationships, existing platform norms and designs often discourage such interactions, leading to withdrawal. To address these challenges, we introduce trust-enabled privacy as a design framework that recognizes trust, whether building or eroding, as central to boundary regulation. When trust is supported, boundary regulation becomes more adaptive and empowering; when it erodes, users default to self-censorship or withdrawal. We propose concrete design affordances, including guided disclosure, contextual audience segmentation, intentional engagement signaling, and trust-centered norms, to help platforms foster a more dynamic and nuanced privacy experience for teen social media users. By reframing privacy as a trust-driven process rather than a rigid control-based trade-off, this work provides empirical insights and actionable guidelines for designing social media environments that empower teens to manage their online presence while fostering meaningful social connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19082v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JaeWon Kim, Robert Wolfe, Ramya Bhagirathi Subramanian, Mei-Hsuan Lee, Jessica Colnago, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>ZuantuSet: A Collection of Historical Chinese Visualizations and Illustrations</title>
      <link>https://arxiv.org/abs/2502.19093</link>
      <description>arXiv:2502.19093v1 Announce Type: new 
Abstract: Historical visualizations are a valuable resource for studying the history of visualization and inspecting the cultural context where they were created. When investigating historical visualizations, it is essential to consider contributions from different cultural frameworks to gain a comprehensive understanding. While there is extensive research on historical visualizations within the European cultural framework, this work shifts the focus to ancient China, a cultural context that remains underexplored by visualization researchers. To this aim, we propose a semi-automatic pipeline to collect, extract, and label historical Chinese visualizations. Through the pipeline, we curate ZuantuSet, a dataset with over 71K visualizations and 108K illustrations. We analyze distinctive design patterns of historical Chinese visualizations and their potential causes within the context of Chinese history and culture. We illustrate potential usage scenarios for this dataset, summarize the unique challenges and solutions associated with collecting historical Chinese visualizations, and outline future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19093v1</guid>
      <category>cs.HC</category>
      <category>cs.DL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713276</arxiv:DOI>
      <dc:creator>Xiyao Mei, Yu Zhang, Chaofan Yang, Rui Shi, Xiaoru Yuan</dc:creator>
    </item>
    <item>
      <title>DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM Co-Decomposition</title>
      <link>https://arxiv.org/abs/2502.19133</link>
      <description>arXiv:2502.19133v1 Announce Type: new 
Abstract: Decomposition is a fundamental skill in algorithmic programming, requiring learners to break down complex problems into smaller, manageable parts. However, current self-study methods, such as browsing reference solutions or using LLM assistants, often provide excessive or generic assistance that misaligns with learners' decomposition strategies, hindering independent problem-solving and critical thinking. To address this, we introduce Decomposition Box (DBox), an interactive LLM-based system that scaffolds and adapts to learners' personalized construction of a step tree through a "learner-LLM co-decomposition" approach, providing tailored support at an appropriate level. A within-subjects study (N=24) found that compared to the baseline, DBox significantly improved learning gains, cognitive engagement, and critical thinking. Learners also reported a stronger sense of achievement and found the assistance appropriate and helpful for learning. Additionally, we examined DBox's impact on cognitive load, identified usage patterns, and analyzed learners' strategies for managing system errors. We conclude with design implications for future AI-powered tools to better support algorithmic programming education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19133v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuai Ma, Junling Wang, Yuanhao Zhang, Xiaojuan Ma, April Yi Wang</dc:creator>
    </item>
    <item>
      <title>PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote Engagement in Urban Gardening</title>
      <link>https://arxiv.org/abs/2502.19171</link>
      <description>arXiv:2502.19171v1 Announce Type: new 
Abstract: Urban gardening is widely recognized for its numerous health and environmental benefits. However, the lack of suitable garden spaces, demanding daily schedules and limited gardening expertise present major roadblocks for citizens looking to engage in urban gardening. While prior research has explored smart home solutions to support urban gardeners, these approaches currently do not fully address these practical barriers. In this paper, we present PlantPal, a system that enables the cultivation of garden spaces irrespective of one's location, expertise level, or time constraints. PlantPal enables the shared operation of a precision agriculture robot (PAR) that is equipped with garden tools and a multi-camera system. Insights from a 3-week deployment (N=18) indicate that PlantPal facilitated the integration of gardening tasks into daily routines, fostered a sense of connection with one's field, and provided an engaging experience despite the remote setting. We contribute design considerations for future robot-assisted urban gardening concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19171v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albin Zeqiri, Julian Britten, Clara Schramm, Pascal Jansen, Michael Rietzler, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed Visual-Ability Families</title>
      <link>https://arxiv.org/abs/2502.19263</link>
      <description>arXiv:2502.19263v1 Announce Type: new 
Abstract: We introduce ArtInsight, a novel AI-powered system to facilitate deeper engagement with child-created artwork in mixed visual-ability families. ArtInsight leverages large language models (LLMs) to craft a respectful and thorough initial description of a child's artwork, and provides: creative AI-generated descriptions for a vivid overview, audio recording to capture the child's own description of their artwork, and a set of AI-generated questions to facilitate discussion between blind or low-vision (BLV) family members and their children. Alongside ArtInsight, we also contribute a new rubric to score AI-generated descriptions of child-created artwork and an assessment of state-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family members and their children, and as a case study with one BLV child therapist. Our findings highlight a preference for ArtInsight's longer, artistically-tailored descriptions over those generated by existing BLV AI tools. Participants highlighted the creative description and audio recording components as most beneficial, with the former helping ``bring a picture to life'' and the latter centering the child's narrative to generate context-aware AI responses. Our findings reveal different ways that AI can be used to support art engagement, including before, during, and after interaction with the child artist, as well as expectations that BLV adults and their sighted children have about AI-powered tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19263v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712082</arxiv:DOI>
      <arxiv:journal_reference>30th International Conference on Intelligent User Interfaces 2025</arxiv:journal_reference>
      <dc:creator>Arnavi Chheda-Kothary, Ritesh Kanchi, Chris Sanders, Kevin Xiao, Aditya Sengupta, Melanie Kneitmix, Jacob O. Wobbrock, Jon E. Froehlich</dc:creator>
    </item>
    <item>
      <title>Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices</title>
      <link>https://arxiv.org/abs/2502.19410</link>
      <description>arXiv:2502.19410v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable potential in recommending everyday actions as personal AI assistants, while Explainable AI (XAI) techniques are being increasingly utilized to help users understand why a recommendation is given. Personal AI assistants today are often located on ultra-small devices such as smartwatches, which have limited screen space. The verbosity of LLM-generated explanations, however, makes it challenging to deliver glanceable LLM explanations on such ultra-small devices. To address this, we explored 1) spatially structuring an LLM's explanation text using defined contextual components during prompting and 2) presenting temporally adaptive explanations to users based on confidence levels. We conducted a user study to understand how these approaches impacted user experiences when interacting with LLM recommendations and explanations on ultra-small devices. The results showed that structured explanations reduced users' time to action and cognitive load when reading an explanation. Always-on structured explanations increased users' acceptance of AI recommendations. However, users were less satisfied with structured explanations compared to unstructured ones due to their lack of sufficient, readable details. Additionally, adaptively presenting structured explanations was less effective at improving user perceptions of the AI compared to the always-on structured explanations. Together with users' interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19410v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinru Wang, Mengjie Yu, Hannah Nguyen, Michael Iuzzolino, Tianyi Wang, Peiqi Tang, Natasha Lynova, Co Tran, Ting Zhang, Naveen Sendhilnathan, Hrvoje Benko, Haijun Xia, Tanya Jonker</dc:creator>
    </item>
    <item>
      <title>Too Little, Too Late: Moderation of Misinformation around the Russo-Ukrainian Conflict</title>
      <link>https://arxiv.org/abs/2502.18500</link>
      <description>arXiv:2502.18500v1 Announce Type: cross 
Abstract: In this study, we examine the role of Twitter as a first line of defense against misinformation by tracking the public engagement with, and the platforms response to, 500 tweets concerning the RussoUkrainian conflict which were identified as misinformation. Using a realtime sample of 543 475 of their retweets, we find that users who geolocate themselves in the U.S. both produce and consume the largest portion of misinformation, however accounts claiming to be in Ukraine are the second largest source. At the time of writing, 84% of these tweets were still available on the platform, especially those having an anti-Russia narrative. For those that did receive some sanctions, the retweeting rate has already stabilized, pointing to ineffectiveness of the measures to stem their spread. These findings point to the need for a change in the existing anti-misinformation system ecosystem. We propose several design and research guidelines for its possible improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18500v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Kishore Shahi, Yelena Mejova</dc:creator>
    </item>
    <item>
      <title>Analyzing User Perceptions of Large Language Models (LLMs) on Reddit: Sentiment and Topic Modeling of ChatGPT and DeepSeek Discussions</title>
      <link>https://arxiv.org/abs/2502.18513</link>
      <description>arXiv:2502.18513v1 Announce Type: cross 
Abstract: While there is an increased discourse on large language models (LLMs) like ChatGPT and DeepSeek, there is no comprehensive understanding of how users of online platforms, like Reddit, perceive these models. This is an important omission because public opinion can influence AI development, trust, and future policy. This study aims at analyzing Reddit discussions about ChatGPT and DeepSeek using sentiment and topic modeling to advance the understanding of user attitudes. Some of the significant topics such as trust in AI, user expectations, potential uses of the tools, reservations about AI biases, and ethical implications of their use are explored in this study. By examining these concerns, the study provides a sense of how public sentiment might shape the direction of AI development going forward. The report also mentions whether users have faith in the technology and what they see as its future. A word frequency approach is used to identify broad topics and sentiment trends. Also, topic modeling through the Latent Dirichlet Allocation (LDA) method identifies top topics in users' language, for example, potential benefits of LLMs, their technological applications, and their overall social ramifications. The study aims to inform developers and policymakers by making it easier to see how users comprehend and experience these game-changing technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18513v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krishnaveni Katta</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality</title>
      <link>https://arxiv.org/abs/2502.18529</link>
      <description>arXiv:2502.18529v1 Announce Type: cross 
Abstract: The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18529v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Wang, Qiaoyi Fang, Junshan Zhang</dc:creator>
    </item>
    <item>
      <title>Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces</title>
      <link>https://arxiv.org/abs/2502.18594</link>
      <description>arXiv:2502.18594v1 Announce Type: cross 
Abstract: Brain-computer interfaces (BCIs) provide alternative communication methods for individuals with motor disabilities by allowing control and interaction with external devices. Non-invasive BCIs, especially those using electroencephalography (EEG), are practical and safe for various applications. However, their performance is often hindered by EEG non-stationarities, caused by changing mental states or device characteristics like electrode impedance. This challenge has spurred research into adaptive BCIs that can handle such variations. In recent years, interest has grown in using error-related potentials (ErrPs) to enhance BCI performance. ErrPs, neural responses to errors, can be detected non-invasively and have been integrated into different BCI paradigms to improve performance through error correction or adaptation.
  This research introduces a novel adaptive ErrP-based BCI approach using reinforcement learning (RL). We demonstrate the feasibility of an RL-driven adaptive framework incorporating ErrPs and motor imagery. Utilizing two RL agents, the framework adapts dynamically to EEG non-stationarities. Validation was conducted using a publicly available motor imagery dataset and a fast-paced game designed to boost user engagement. Results show the framework's promise, with RL agents learning control policies from user interactions and achieving robust performance across datasets. However, a critical insight from the game-based protocol revealed that motor imagery in a high-speed interaction paradigm was largely ineffective for participants, highlighting task design limitations in real-time BCI applications. These findings underscore the potential of RL for adaptive BCIs while pointing out practical constraints related to task complexity and user responsiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18594v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aline Xavier Fid\^encio, Felix Gr\"un, Christian Klaes, Ioannis Iossifidis</dc:creator>
    </item>
    <item>
      <title>Speaking the Right Language: The Impact of Expertise Alignment in User-AI Interactions</title>
      <link>https://arxiv.org/abs/2502.18685</link>
      <description>arXiv:2502.18685v1 Announce Type: cross 
Abstract: Using a sample of 25,000 Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that across a variety of topical domains, the agent largely responds at proficient or expert levels of expertise (77% of conversations) which correlates with positive user experience regardless of the user's level of expertise. Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience, with the impact more profound for more complex tasks. We also show that users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user. Our findings underscore the importance of alignment between user and AI when designing human-centered AI systems, to ensure satisfactory and productive interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18685v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shramay Palta, Nirupama Chandrasekaran, Rachel Rudinger, Scott Counts</dc:creator>
    </item>
    <item>
      <title>Rapidly Built Medical Crash Cart! Lessons Learned and Impacts on High-Stakes Team Collaboration in the Emergency Room</title>
      <link>https://arxiv.org/abs/2502.18688</link>
      <description>arXiv:2502.18688v1 Announce Type: cross 
Abstract: Designing robots to support high-stakes teamwork in emergency settings presents unique challenges, including seamless integration into fast-paced environments, facilitating effective communication among team members, and adapting to rapidly changing situations. While teleoperated robots have been successfully used in high-stakes domains such as firefighting and space exploration, autonomous robots that aid highs-takes teamwork remain underexplored. To address this gap, we conducted a rapid prototyping process to develop a series of seemingly autonomous robot designed to assist clinical teams in the Emergency Room. We transformed a standard crash cart--which stores medical equipment and emergency supplies into a medical robotic crash cart (MCCR). The MCCR was evaluated through field deployments to assess its impact on team workload and usability, identified taxonomies of failure, and refined the MCCR in collaboration with healthcare professionals. Our work advances the understanding of robot design for high-stakes, time-sensitive settings, providing insights into useful MCCR capabilities and considerations for effective human-robot collaboration. By publicly disseminating our MCCR tutorial, we hope to encourage HRI researchers to explore the design of robots for high-stakes teamwork.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18688v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Angelique Taylor, Tauhid Tanjim, Michael Joseph Sack, Maia Hirsch, Kexin Cheng, Kevin Ching, Jonathan St. George, Thijs Roumen, Malte F. Jung, Hee Rin Lee</dc:creator>
    </item>
    <item>
      <title>Adaptive and Accessible User Interfaces for Seniors Through Model-Driven Engineering</title>
      <link>https://arxiv.org/abs/2502.18828</link>
      <description>arXiv:2502.18828v1 Announce Type: cross 
Abstract: The use of diverse apps among senior users is increasing. However, despite their diverse age-related accessibility needs and preferences, these users often encounter apps with significant accessibility barriers. Even in the best-case scenarios, they are provided with one-size-fits-all user interfaces that offer very limited personalisation support. To address this issue, we describe AdaptForge, a novel model-driven engineering (MDE)-based approach to support sophisticated adaptations of Flutter app user interfaces and behaviour based on the age-related accessibility needs of senior users. We explain how AdaptForge employs Domain-Specific Languages to capture seniors' context-of-use scenarios and how this information is used via adaptation rules to perform design-time modifications to a Flutter app's source code. Additionally, we report on evaluations conducted with real-world Flutter developers to demonstrate the promise and practical applicability of AdaptForge, as well as with senior end-users using our adapted Flutter app prototypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18828v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shavindra Wickramathilaka, John Grundy, Kashumi Madampe, Omar Haggag</dc:creator>
    </item>
    <item>
      <title>Towards an AI co-scientist</title>
      <link>https://arxiv.org/abs/2502.18864</link>
      <description>arXiv:2502.18864v1 Announce Type: cross 
Abstract: Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18864v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>physics.soc-ph</category>
      <category>q-bio.OT</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, Jos\'e R Penad\'es, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, Vivek Natarajan</dc:creator>
    </item>
    <item>
      <title>Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Huality Text-to-Speech Method based on Contextual Semantic Understanding</title>
      <link>https://arxiv.org/abs/2502.18889</link>
      <description>arXiv:2502.18889v1 Announce Type: cross 
Abstract: Traditional text-to-speech (TTS) methods primarily focus on establishing a mapping between phonemes and mel-spectrograms. However, during the phoneme encoding stage, there is often a lack of real mel-spectrogram auxiliary information, which results in the encoding process lacking true semantic understanding. At the same time, traditional TTS systems often struggle to balance the inference speed of the model with the quality of the synthesized speech. Methods that generate high-quality synthesized speech tend to have slower inference speeds, while faster inference methods often sacrifice speech quality. In this paper, I propose Clip-TTS, a TTS method based on the Clip architecture. This method uses the Clip framework to establish a connection between text content and real mel-spectrograms during the text encoding stage, enabling the text encoder to directly learn the true semantics of the global context, thereby ensuring the quality of the synthesized speech. In terms of model architecture, I adopt the basic structure of Transformer, which allows Clip-TTS to achieve fast inference speeds. Experimental results show that on the LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves state-of-the-art MOS scores, and it also performs excellently on multi-emotion datasets.Audio samples are available at: https://ltydd1314.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18889v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyun Liu</dc:creator>
    </item>
    <item>
      <title>A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management</title>
      <link>https://arxiv.org/abs/2502.19135</link>
      <description>arXiv:2502.19135v1 Announce Type: cross 
Abstract: This paper presents a novel framework, called PLANTOR (PLanning with Natural language for Task-Oriented Robots), that integrates Large Language Models (LLMs) with Prolog-based knowledge management and planning for multi-robot tasks. The system employs a two-phase generation of a robot-oriented knowledge base, ensuring reusability and compositional reasoning, as well as a three-step planning procedure that handles temporal dependencies, resource constraints, and parallel task execution via mixed-integer linear programming. The final plan is converted into a Behaviour Tree for direct use in ROS2. We tested the framework in multi-robot assembly tasks within a block world and an arch-building scenario. Results demonstrate that LLMs can produce accurate knowledge bases with modest human feedback, while Prolog guarantees formal correctness and explainability. This approach underscores the potential of LLM integration for advanced robotics tasks requiring flexible, scalable, and human-understandable planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19135v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri</dc:creator>
    </item>
    <item>
      <title>FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users</title>
      <link>https://arxiv.org/abs/2502.19312</link>
      <description>arXiv:2502.19312v1 Announce Type: cross 
Abstract: Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19312v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn</dc:creator>
    </item>
    <item>
      <title>Confounding-Robust Policy Improvement with Human-AI Teams</title>
      <link>https://arxiv.org/abs/2310.08824</link>
      <description>arXiv:2310.08824v2 Announce Type: replace 
Abstract: Human-AI collaboration has the potential to transform various domains by leveraging the complementary strengths of human experts and Artificial Intelligence (AI) systems. However, unobserved confounding can undermine the effectiveness of this collaboration, leading to biased and unreliable outcomes. In this paper, we propose a novel solution to address unobserved confounding in human-AI collaboration by employing sensitivity analysis from causal inference. Our approach combines domain expertise with AI-driven statistical modeling to account for potentially hidden confounders. We present a deferral collaboration framework for incorporating the sensitivity model into offline policy learning, enabling the system to control for the influence of unobserved confounding factors. In addition, we propose a personalized deferral collaboration system to leverage the diverse expertise of different human decision-makers. By adjusting for potential biases, our proposed solution enhances the robustness and reliability of collaborative outcomes. The empirical and theoretical analyses demonstrate the efficacy of our approach in mitigating unobserved confounding and improving the overall performance of human-AI collaborations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08824v2</guid>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruijiang Gao, Mingzhang Yin</dc:creator>
    </item>
    <item>
      <title>The Last JITAI? Exploring Large Language Models for Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Conceptual Cardiac Rehabilitation Setting</title>
      <link>https://arxiv.org/abs/2402.08658</link>
      <description>arXiv:2402.08658v3 Announce Type: replace 
Abstract: We evaluated the viability of using Large Language Models (LLMs) to trigger and personalize content in Just-in-Time Adaptive Interventions (JITAIs) in digital health. As an interaction pattern representative of context-aware computing, JITAIs are being explored for their potential to support sustainable behavior change, adapting interventions to an individual's current context and needs. Challenging traditional JITAI implementation models, which face severe scalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs in the use case of heart-healthy activity in cardiac rehabilitation. Using three personas representing patients affected by CVD with varying severeness and five context sets per persona, we generated 450 JITAI decisions and messages. These were systematically evaluated against those created by 10 laypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated JITAIs surpassed human-generated intervention suggestions, outperforming both LayPs and HCPs across all metrics (i.e., appropriateness, engagement, effectiveness, and professionalism). These results highlight the potential of LLMs to enhance JITAI implementations in personalized health interventions, demonstrating how generative AI could revolutionize context-aware computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08658v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713307</arxiv:DOI>
      <dc:creator>David Haag, Devender Kumar, Sebastian Gruber, Dominik Hofer, Mahdi Sareban, Gunnar Treff, Josef Niebauer, Christopher Bull, Albrecht Schmidt, Jan David Smeddinck</dc:creator>
    </item>
    <item>
      <title>Predicting Quality of Video Gaming Experience Using Global-Scale Telemetry Data and Federated Learning</title>
      <link>https://arxiv.org/abs/2412.08950</link>
      <description>arXiv:2412.08950v3 Announce Type: replace 
Abstract: Frames Per Second (FPS) significantly affects the gaming experience. Providing players with accurate FPS estimates prior to purchase benefits both players and game developers. However, we have a limited understanding of how to predict a game's FPS performance on a specific device. In this paper, we first conduct a comprehensive analysis of a wide range of factors that may affect game FPS on a global-scale dataset to identify the determinants of FPS. This includes player-side and game-side characteristics, as well as country-level socio-economic statistics. Furthermore, recognizing that accurate FPS predictions require extensive user data, which raises privacy concerns, we propose a federated learning-based model to ensure user privacy. Each player and game is assigned a unique learnable knowledge kernel that gradually extracts latent features for improved accuracy. We also introduce a novel training and prediction scheme that allows these kernels to be dynamically plug-and-play, effectively addressing cold start issues. To train this model with minimal bias, we collected a large telemetry dataset from 224 countries and regions, 100,000 users, and 835 games. Our model achieved a mean Wasserstein distance of 0.469 between predicted and ground truth FPS distributions, outperforming all baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08950v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyang Zhang, Jinhe Wen, Zixi Chen, Dara Arbab, Sruti Sahani, Kent Giard, Bijan Arbab, Haojian Jin, Tauhidur Rahman</dc:creator>
    </item>
    <item>
      <title>Analyzing Privacy Dynamics within Groups using Gamified Auctions</title>
      <link>https://arxiv.org/abs/2502.10788</link>
      <description>arXiv:2502.10788v2 Announce Type: replace 
Abstract: Online shared content, such as group pictures, often contains information about multiple users. Developing technical solutions to manage the privacy of such "co-owned" content is challenging because each co-owner may have different preferences. Recent technical approaches advocate group-decision mechanisms, including auctions, to decide as how best to resolve these differences. However, it is not clear if users would participate in such mechanisms and if they do, whether they would act altruistically. Understanding the privacy dynamics is crucial to develop effective mechanisms for privacy-respecting collaborative systems. Accordingly, this work develops RESOLVE, a privacy auction game to understand the sharing behavior of users in groups. Our results of users' playing the game show that i) the users' understanding of individual vs. group privacy differs significantly; ii) often users fight for their preferences even at the cost of others' privacy; and iii) at times users collaborate to fight for the privacy of others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10788v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H\"useyin Ayd{\i}n, Onuralp Ulusoy, Ilaria Liccardi, P{\i}nar Yolum</dc:creator>
    </item>
    <item>
      <title>Understanding Generative AI Risks for Youth: A Taxonomy Based on Empirical Data</title>
      <link>https://arxiv.org/abs/2502.16383</link>
      <description>arXiv:2502.16383v2 Announce Type: replace 
Abstract: Generative AI (GAI) is reshaping the way young users engage with technology. This study introduces a taxonomy of risks associated with youth-GAI interactions, derived from an analysis of 344 chat transcripts between youth and GAI chatbots, 30,305 Reddit discussions concerning youth engagement with these systems, and 153 documented AI-related incidents. We categorize risks into six overarching themes, identifying 84 specific risks, which we further align with four distinct interaction pathways. Our findings highlight emerging concerns, such as risks to mental wellbeing, behavioral and social development, and novel forms of toxicity, privacy breaches, and misuse/exploitation that are not fully addressed in existing frameworks on child online safety or AI risks. By systematically grounding our taxonomy in empirical data, this work offers a structured approach to aiding AI developers, educators, caregivers, and policymakers in comprehending and mitigating risks associated with youth-GAI interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16383v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaman Yu, Yiren Liu, Jacky Zhang, Yun Huang, Yang Wang</dc:creator>
    </item>
    <item>
      <title>Diagrammatization and Abduction to Improve AI Interpretability With Domain-Aligned Explanations for Medical Diagnosis</title>
      <link>https://arxiv.org/abs/2302.01241</link>
      <description>arXiv:2302.01241v3 Announce Type: replace-cross 
Abstract: Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. Investigating XAI for high-stakes medical diagnosis, we propose improving domain alignment with diagrammatic and abductive reasoning to reduce the interpretability gap. We developed DiagramNet to predict cardiac diagnoses from heart auscultation, select the best-fitting hypothesis based on criteria evaluation, and explain with clinically-relevant murmur diagrams. The ante-hoc interpretable model leverages domain-relevant ontology, representation, and reasoning process to increase trust in expert users. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better performance than baseline models. We demonstrate the interpretability and trustworthiness of diagrammatic, abductive explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred over technical saliency map explanations. This work contributes insights into providing domain-aligned explanations for user-centric XAI in complex domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01241v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew</dc:creator>
    </item>
    <item>
      <title>Trustworthy and Practical AI for Healthcare: A Guided Deferral System with Large Language Models</title>
      <link>https://arxiv.org/abs/2406.07212</link>
      <description>arXiv:2406.07212v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) offer a valuable technology for various applications in healthcare. However, their tendency to hallucinate and the existing reliance on proprietary systems pose challenges in environments concerning critical decision-making and strict data privacy regulations, such as healthcare, where the trust in such systems is paramount. Through combining the strengths and discounting the weaknesses of humans and AI, the field of Human-AI Collaboration (HAIC) presents one front for tackling these challenges and hence improving trust. This paper presents a novel HAIC guided deferral system that can simultaneously parse medical reports for disorder classification, and defer uncertain predictions with intelligent guidance to humans. We develop methodology which builds efficient, effective and open-source LLMs for this purpose, for the real-world deployment in healthcare. We conduct a pilot study which showcases the effectiveness of our proposed system in practice. Additionally, we highlight drawbacks of standard calibration metrics in imbalanced data scenarios commonly found in healthcare, and suggest a simple yet effective solution: the Imbalanced Expected Calibration Error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07212v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Strong, Qianhui Men, Alison Noble</dc:creator>
    </item>
    <item>
      <title>Enabling Multi-Robot Collaboration from Single-Human Guidance</title>
      <link>https://arxiv.org/abs/2409.19831</link>
      <description>arXiv:2409.19831v2 Announce Type: replace-cross 
Abstract: Learning collaborative behaviors is essential for multi-agent systems. Traditionally, multi-agent reinforcement learning solves this implicitly through a joint reward and centralized observations, assuming collaborative behavior will emerge. Other studies propose to learn from demonstrations of a group of collaborative experts. Instead, we propose an efficient and explicit way of learning collaborative behaviors in multi-agent systems by leveraging expertise from only a single human. Our insight is that humans can naturally take on various roles in a team. We show that agents can effectively learn to collaborate by allowing a human operator to dynamically switch between controlling agents for a short period and incorporating a human-like theory-of-mind model of teammates. Our experiments showed that our method improves the success rate of a challenging collaborative hide-and-seek task by up to 58% with only 40 minutes of human guidance. We further demonstrate our findings transfer to the real world by conducting multi-robot experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19831v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengran Ji, Lingyu Zhang, Paul Sajda, Boyuan Chen</dc:creator>
    </item>
  </channel>
</rss>
