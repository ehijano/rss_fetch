<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 01:43:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Repairing Leaks in Resource Wrappers</title>
      <link>https://arxiv.org/abs/2510.03461</link>
      <description>arXiv:2510.03461v1 Announce Type: new 
Abstract: A resource leak occurs when a program fails to release a finite resource like a socket, file descriptor or database connection. While sound static analysis tools can detect all leaks, automatically repairing them remains challenging. Prior work took the output of a detection tool and attempted to repair only leaks from a hard-coded list of library resource types. That approach limits the scope of repairable leaks: real-world code uses resource wrappers that store a resource in a field and must themselves be closed. This paper makes four key contributions to improve resource leak repair in the presence of wrappers. (1) It integrates inference of resource management specifications into the repair pipeline, enabling extant fixing approaches to reason about wrappers. (2) It transforms programs into variants that are easier to analyze, making inference, detection, and fixing tools more effective; for instance, it makes detection tools report problems closer to the root cause, often in a client of a resource wrapper rather than within the wrapper class itself. (3) A novel field containment analysis reasons about resource lifetimes, enabling repair of more leaks involving resources stored in fields. (4) It introduces a new repair pattern and more precise reasoning to better handle resources stored in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR benchmark suite; our implementation Arodnap fixes 68%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03461v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025</arxiv:journal_reference>
      <dc:creator>Sanjay Malakar, Michael D. Ernst, Martin Kellogg, Manu Sridharan</dc:creator>
    </item>
    <item>
      <title>ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework</title>
      <link>https://arxiv.org/abs/2510.03463</link>
      <description>arXiv:2510.03463v1 Announce Type: new 
Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in applied LLM research across a number of fields. One notable area is software development, where researchers have advanced the automation of code implementation, code testing, code maintenance, inter alia, using LLM agents. However, software development is a multifaceted environment that extends beyond just code. As such, a successful LLM system must factor in multiple stages of the software development life-cycle (SDLC). In this paper, we propose a vision for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework, which follows the above SDLC philosophy such that it may work within an agile software development team to perform several tasks end-to-end. ALMAS aligns its agents with agile roles, and can be used in a modular fashion to seamlessly integrate with human developers and their development environment. We showcase the progress towards ALMAS through our published works and a use case demonstrating the framework, where ALMAS is able to seamlessly generate an application and add a new feature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03463v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vali Tawosi, Keshav Ramani, Salwa Alamir, Xiaomo Liu</dc:creator>
    </item>
    <item>
      <title>Relative Code Comprehensibility Prediction</title>
      <link>https://arxiv.org/abs/2510.03474</link>
      <description>arXiv:2510.03474v1 Announce Type: new 
Abstract: Automatically predicting how difficult it is for humans to understand a code snippet can assist developers in tasks like deciding when and where to refactor. Despite many proposed code comprehensibility metrics, studies have shown they often correlate poorly with actual measurements of human comprehensibility. This has motivated the use of machine learning models to predict human comprehensibility directly from code, but these models have also shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human comprehensibility data, which confuses models trained to predict it directly. To address this, we propose training models to predict the relative comprehensibility of two code snippets - that is, predicting which snippet a human would find easier to understand without predicting each snippet's comprehensibility in isolation. This mitigates noise in predicting 'absolute' comprehensibility measurements, but is still useful for downstream software-engineering tasks like assessing whether refactoring improves or hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and relative code comprehensibility prediction via machine learning. We used a dataset of 150 Java code snippets and 12.5k human comprehensibility measurements from prior user studies, comparing the models' performance with naive baselines (eg 'always predict the majority class'). Our findings indicate that absolute comprehensibility models improve over the baselines by at most 33.4% and frequently underperform. In contrast, relative comprehensibility models are substantially better, with average improvements of 137.8% and 74.7% for snippet-wise and developer-wise prediction, respectively. These results suggest that relative comprehensibility models learn more effectively from the data, supporting their practical applicability for downstream SE tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03474v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadeeshan De Silva, Martin Kellogg, Oscar Chaparro</dc:creator>
    </item>
    <item>
      <title>LLM Agents for Automated Dependency Upgrades</title>
      <link>https://arxiv.org/abs/2510.03480</link>
      <description>arXiv:2510.03480v1 Announce Type: new 
Abstract: As a codebase expands over time, its library dependencies can become outdated and require updates to maintain innovation and security. However, updating a library can introduce breaking changes in the code, necessitating significant developer time for maintenance. To address this, we introduce a framework of LLM agents to be used in combination with migration documentation to automatically recommend and apply code updates and ensure compatibility with new versions. Our solution can automatically localize updated library usages in live Java codebases and implement recommended fixes in a user-friendly manner. The system architecture consists of multiple key components: a Summary Agent, Control Agent, and Code Agent. To validate our approach, we apply the framework on an industrial use case by which we create three synthetic code repositories with major Upgrade changes and benchmark our approach against state-of-the-art methods. Results show that our approach not only performs upgrades using fewer tokens across all cases but also achieves a precision of 71.4%, highlighting its efficiency and effectiveness compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03480v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vali Tawosi, Salwa Alamir, Xiaomo Liu, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>AgentHub: A Research Agenda for Agent Sharing Infrastructure</title>
      <link>https://arxiv.org/abs/2510.03495</link>
      <description>arXiv:2510.03495v1 Announce Type: new 
Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for discovering, evaluating, and governing them remains fragmented compared to mature ecosystems like software package registries (e.g., npm) and model hubs (e.g., Hugging Face). Recent research and engineering works have begun to consider the requisite infrastructure, but so far they focus narrowly -- on distribution, naming, or protocol negotiation. However, considering broader software engineering requirements would improve open-source distribution and ease reuse. We therefore propose AgentHub, a research agenda for agent sharing. By framing the key challenges of capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration, AgentHub charts a community-wide agenda for building reliable and scalable agent ecosystems. Our vision is a future where agents can be shared, trusted, and composed as seamlessly as today's software libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03495v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Erik Pautsch, Tanmay Singla, Wenxin Jiang, Huiyun Peng, Behnaz Hassanshahi, Konstantin L\"aufer, George K. Thiruvathukal, James C. Davis</dc:creator>
    </item>
    <item>
      <title>REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement</title>
      <link>https://arxiv.org/abs/2510.03588</link>
      <description>arXiv:2510.03588v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently shown strong potential in automatic program repair (APR), especially in repository-level settings where the goal is to generate patches based on natural language issue descriptions, large codebases, and regression tests. However, despite their promise, current LLM-based APR techniques often struggle to produce correct fixes due to limited understanding of code context and over-reliance on incomplete test suites. As a result, they frequently generate Draft Patches-partially correct patches that either incompletely address the bug or overfit to the test cases. In this work, we propose a novel patch refinement framework, Refine, that systematically transforms Draft Patches into correct ones. Refine addresses three key challenges: disambiguating vague issue and code context, diversifying patch candidates through test-time scaling, and aggregating partial fixes via an LLM-powered code review process. We implement Refine as a general refinement module that can be integrated into both open-agent-based and workflow-based APR systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine achieves state-of-the-art results among workflow-based approaches and approaches the best-known performance across all APR categories. Specifically, Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of 51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine improves the resolution rate by 12.2%, and when integrated across multiple APR systems, it yields an average improvement of 14%-demonstrating its broad effectiveness and generalizability. These results highlight the effectiveness of refinement as a missing component in current APR pipelines and the potential of agentic collaboration in closing the gap between near-correct and correct patches. We also open source our code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03588v1</guid>
      <category>cs.SE</category>
      <category>cs.MA</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anvith Pabba, Simin Chen, Alex Mathai, Anindya Chakraborty, Baishakhi Ray</dc:creator>
    </item>
    <item>
      <title>Generating High-Level Test Cases from Requirements using LLM: An Industry Study</title>
      <link>https://arxiv.org/abs/2510.03641</link>
      <description>arXiv:2510.03641v1 Announce Type: new 
Abstract: Currently, generating high-level test cases described in natural language from requirement documents is performed manually. In the industry, including companies specializing in software testing, there is a significant demand for the automatic generation of high-level test cases from requirement documents using Large Language Models (LLMs). Efforts to utilize LLMs for requirement analysis are underway. In some cases, retrieval-augmented generation (RAG) is employed for generating high-level test cases using LLMs. However, in practical applications, it is necessary to create a RAG tailored to the knowledge system of each specific application, which is labor-intensive. Moreover, when applying high-level test case generation as a prompt, there is no established method for instructing the generation of high-level test cases at a level applicable to other specifications without using RAG. It is required to establish a method for the automatic generation of high-level test cases that can be generalized across a wider range of requirement documents. In this paper, we propose a method for generating high-level (GHL) test cases from requirement documents using only prompts, without creating RAGs. In the proposed method, first, the requirement document is input into the LLM to generate test design techniques corresponding to the requirement document. Then, high-level test cases are generated for each of the generated test design techniques. Furthermore, we verify an evaluation method based on semantic similarity of the generated high-level test cases. In the experiments, we confirmed the method using datasets from Bluetooth and Mozilla, where requirement documents and high-level test cases are available, achieving macro-recall measurement of 0.81 and 0.37, respectively. We believe that the method is feasible for practical application in generating high-level test cases without using RAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03641v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Satoshi Masuda, Satoshi Kouzawa, Kyousuke Sezai, Hidetoshi Suhara, Yasuaki Hiruta, Kunihiro Kudou</dc:creator>
    </item>
    <item>
      <title>Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems</title>
      <link>https://arxiv.org/abs/2510.03712</link>
      <description>arXiv:2510.03712v1 Announce Type: new 
Abstract: Modern distributed systems employ aggressive optimization strategies that create latent risks - hidden vulnerabilities where exceptional performance masks catastrophic fragility when optimizations fail. Cache layers achieving 99% hit rates can obscure database bottlenecks until cache failures trigger 100x load amplification and cascading collapse. Current reliability engineering focuses on reactive incident response rather than proactive detection of optimization-induced vulnerabilities. This paper presents the first comprehensive framework for systematic latent risk detection, prevention, and optimization through integrated mathematical modeling, intelligent perturbation testing, and risk-aware performance optimization. We introduce the Latent Risk Index (LRI) that correlates strongly with incident severity (r=0.863, p&lt;0.001), enabling predictive risk assessment. Our framework integrates three systems: HYDRA employing six optimization-aware perturbation strategies achieving 89.7% risk discovery rates, RAVEN providing continuous production monitoring with 92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling risk-aware optimization maintaining 96.6% baseline performance while reducing latent risks by 59.2%. Evaluation across three testbed environments demonstrates strong statistical validation with large effect sizes (Cohen d&gt;2.0) and exceptional reproducibility (r&gt;0.92). Production deployment over 24 weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity reduction, and 81 prevented incidents generating 1.44M USD average annual benefits with 3.2-month ROI. Our approach transforms reliability engineering from reactive incident management to proactive risk-aware optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03712v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jahidul Arafat, Kh. M. Moniruzzaman, Shamim Hossain, Fariha Tasmin,  Kamrujjaman, Ahsan Habib Tareq</dc:creator>
    </item>
    <item>
      <title>APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents</title>
      <link>https://arxiv.org/abs/2510.03743</link>
      <description>arXiv:2510.03743v1 Announce Type: new 
Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet they falter on niche or proprietary libraries because the multi-turn dialogue data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source pipeline that converts symbolic dialogue-act "scripts" into realistic, domain-grounded API Search conversations using a lightweight model for inexpensive training data generation. Phase I pairs a legacy dialogue planner with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on this corpus. Phase II drops the teacher and reuses the same planner with the fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without exposing source code to external services. The fine-tuned student improves BLEU from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while running entirely on a single consumer GPU. All components are modular and publicly released to serve as a conservative baseline for future work. APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a video demo is available at https://youtu.be/YqmZBHyGbPs .</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03743v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Eberhart, Collin McMillan</dc:creator>
    </item>
    <item>
      <title>Code4MeV2: a Research-oriented Code-completion Platform</title>
      <link>https://arxiv.org/abs/2510.03755</link>
      <description>arXiv:2510.03755v1 Announce Type: new 
Abstract: The adoption of AI-powered code completion tools in software development has increased substantially, yet the user interaction data produced by these systems remain proprietary within large corporations. This creates a barrier for the academic community, as researchers must often develop dedicated platforms to conduct studies on human--AI interaction, making reproducible research and large-scale data analysis impractical. In this work, we introduce Code4MeV2, a research-oriented, open-source code completion plugin for JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a client--server architecture and features inline code completion and a context-aware chat assistant. Its core contribution is a modular and transparent data collection framework that gives researchers fine-grained control over telemetry and context gathering. Code4MeV2 achieves industry-comparable performance in terms of code completion, with an average latency of 200~ms. We assess our tool through a combination of an expert evaluation and a user study with eight participants. Feedback from both researchers and daily users highlights its informativeness and usefulness. We invite the community to adopt and contribute to this tool. More information about the tool can be found at https://app.code4me.me.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03755v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roham Koohestani, Parham Bateni, Aydin Ebrahimi, Behdad Etezadi, Kiarash Karimi, Maliheh Izadi</dc:creator>
    </item>
    <item>
      <title>A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt</title>
      <link>https://arxiv.org/abs/2510.03802</link>
      <description>arXiv:2510.03802v1 Announce Type: new 
Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized software development, driving innovation across various domains. However, these systems also introduce unique challenges, particularly in maintaining software quality and performance. Among these challenges, Self-Admitted Technical Debt (SATD) has emerged as a growing concern, significantly impacting the maintainability and overall quality of ML and DL-enabled systems. Despite its critical implications, the lifecycle of DL-specific SATD, how developers introduce, acknowledge, and address it over time-remains underexplored. This study presents a preliminary analysis of the persistence and lifecycle of DL-specific SATD in DL-enabled systems. The purpose of this project is to uncover the patterns of SATD introduction, recognition, and durability during the development life cycle, providing information on how to manage these issues. Using mining software repository techniques, we examined 40 ML projects, focusing on 185 DL-specific SATD instances. The analysis tracked the introduction and persistence of SATD instances through project commit histories to assess their lifecycle and developer actions. The findings indicate that DL-specific SATD is predominantly introduced during the early and middle stages of project development. Training and Hardware phases showed the longest SATD durations, highlighting critical areas where debt accumulates and persists. Additionally, developers introduce DL-specific SATD more frequently during feature implementation and bug fixes. This study emphasizes the need for targeted DL-specific SATD management strategies in DL-enabled systems to mitigate its impact. By understanding the temporal characteristics and evolution of DL-specific SATD, developers can prioritize interventions at critical stages to improve the maintainability and quality of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03802v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilberto Recupito, Vincenzo De Martino, Dario Di Nucci, Fabio Palomba</dc:creator>
    </item>
    <item>
      <title>Smart Paste: Automatically Fixing Copy/Paste for Google Developers</title>
      <link>https://arxiv.org/abs/2510.03843</link>
      <description>arXiv:2510.03843v1 Announce Type: new 
Abstract: Manually editing pasted code is a long-standing developer pain point. In internal software development at Google, we observe that code is pasted 4 times more often than it is manually typed. These paste actions frequently require follow-up edits, ranging from simple reformatting and renaming to more complex style adjustments and cross-language translations. Prior work has shown deep learning can be used to predict these edits. In this work, we show how to iteratively develop and scale Smart Paste, an IDE feature for post-paste edit suggestions, to Google's development environment. This experience can serve as a guide for AI practitioners on a holistic approach to feature development, covering user experience, system integration, and model capabilities. Since deployment, Smart Paste has had overwhelmingly positive feedback with a 45% acceptance rate. At Google's enterprise scale, these accepted suggestions account substantially for over 1% of all code written company-wide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03843v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Nguyen, Guilherme Herzog, Jos\'e Cambronero, Marcus Revaj, Aditya Kini, Alexander Fr\"ommgen, Maxim Tabachnyk</dc:creator>
    </item>
    <item>
      <title>Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework</title>
      <link>https://arxiv.org/abs/2510.03862</link>
      <description>arXiv:2510.03862v1 Announce Type: new 
Abstract: The rise of large language models (LLMs) has introduced transformative potential in automated code generation, addressing a wide range of software engineering challenges. However, empirical evaluation of LLM-based code generation lacks standardization, with studies varying widely in goals, tasks, and metrics, which limits comparability and reproducibility. In this paper, we propose a theoretical framework for designing and reporting empirical studies on LLM-based code generation. The framework is grounded in both our prior experience conducting such experiments and a comparative analysis of key similarities and differences among recent studies. It organizes evaluation around core components such as problem sources, quality attributes, and metrics, supporting structured and systematic experimentation. We demonstrate its applicability through representative case mappings and identify opportunities for refinement. Looking forward, we plan to evolve the framework into a more robust and mature tool for standardizing LLM evaluation across software engineering contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03862v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathalia Nascimento, Everton Guimaraes, Paulo Alencar</dc:creator>
    </item>
    <item>
      <title>Adversarial Agent Collaboration for C to Rust Translation</title>
      <link>https://arxiv.org/abs/2510.03879</link>
      <description>arXiv:2510.03879v1 Announce Type: new 
Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory safety vulnerabilities that are prevalent in legacy C software. Existing approaches for C to safe Rust translation, including LLM-assisted ones, do not generalize on larger (&gt; 500 LoC) C codebases because they depend on complex program analyses that frequently break. In this work, we present ACToR (Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired by GANs, ACToR pits a generator agent against a discriminator agent, which collaborate to iteratively generate a Rust translation. On each iteration, the translator agent synthesizes and refines a Rust translation to pass an existing suite of tests, and then the discriminator agent finds new failing tests. We demonstrate that ACToR translates all of the 63 real-world command line utilities considered in our benchmarks, which have an average size of 485 lines of code, and it achieves over 90% test pass rate with zero human intervention. To our knowledge, it is the first such system that reliably translates C programs of this scale. Furthermore, ACToR improves translation correctness by up to 18.9% compared to baseline, non-adversarial approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03879v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Li, Ruishi Li, Bo Wang, Brandon Paulsen, Umang Mathur, Prateek Saxena</dc:creator>
    </item>
    <item>
      <title>Rethinking Services in the Quantum Age: The SOQ Paradigm</title>
      <link>https://arxiv.org/abs/2510.03890</link>
      <description>arXiv:2510.03890v1 Announce Type: new 
Abstract: Quantum computing is rapidly progressing from theoretical promise to practical implementation, offering significant computational advantages for tasks in optimization, simulation, cryptography, and machine learning. However, its integration into real-world software systems remains constrained by hardware fragility, platform heterogeneity, and the absence of robust software engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a novel paradigm that reimagines quantum software systems through the lens of classical service-oriented computing. Unlike prior approaches such as Quantum Service-Oriented Computing (QSOC), which treat quantum capabilities as auxiliary components within classical systems, SOQ positions quantum services as autonomous, composable, and interoperable entities. We define the foundational principles of SOQ, propose a layered technology stack to support its realization, and identify the key research and engineering challenges that must be addressed, including interoperability, hybridity, pricing models, service abstractions, and workforce development. This approach is of vital importance for the advancement of quantum technology because it enables the scalable, modular, and interoperable integration of quantum computing into real-world software systems independently and without relying on a dedicated classical environment to manage quantum processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03890v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jose Garcia-Alonso, Enrique Moguel, Jaime Alvarado-Valiente, Javier Romero-Alvarez, \'Alvaro M. Aparicio-Morales, Juan M. Murillo, Francisco Javier Cavero, Adri\'an Romero-Flores, Alfonso E. Marquez-Chamorro, Jos\'e Antonio Parejo, Antonio Ruiz-Cort\'es, Giuseppe Bisicchia, Alessandro Bocci, Antonio Brogi</dc:creator>
    </item>
    <item>
      <title>A Brief History of the Waterfall Model: Past, Present, and Future</title>
      <link>https://arxiv.org/abs/2510.03894</link>
      <description>arXiv:2510.03894v1 Announce Type: new 
Abstract: The waterfall model, one of the earliest software development methodologies, has played a foundational role in shaping contemporary software engineering practices. This paper provides a historical and critical overview of the model, tracing its conceptual origins in software engineering, its formalization by Royce, and its evolution through decades of industry adoption and critique. Although often criticized for its rigidity, shortcomings, and high failure rates, the waterfall model persists in specific domains. Its principles continue to influence contemporary hybrid development frameworks that combine traditional and agile methods. Drawing on a range of scholarly sources, this study synthesizes key developments in the perception and application of the waterfall model. The analysis highlights how the model has shifted from a standalone framework to a component within modern hybrid methodologies. By revisiting its origins, assessing its present utility, and examining its role in contemporary development practices, this paper argues that the waterfall model remains relevant, not as a relic of the past but as part of context-aware development strategies. The paper contends that the model's enduring relevance lies in its adaptability. By recognizing both its limitations and its strengths, and by understanding its integration within hybrid approaches, practitioners can make more informed decisions about methodology selection and process design in diverse development environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03894v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Saravanos (New York University)</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code</title>
      <link>https://arxiv.org/abs/2510.03902</link>
      <description>arXiv:2510.03902v1 Announce Type: new 
Abstract: The increasing complexity of cloud-native infrastructure has made Infrastructure-as-Code (IaC) essential for reproducible and scalable deployments. While large language models (LLMs) have shown promise in generating IaC snippets from natural language prompts, their monolithic, single-pass generation approach often results in syntactic errors, policy violations, and unscalable designs. In this paper, we propose MACOG (Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based architecture for IaC generation that decomposes the task into modular subtasks handled by specialized agents: Architect, Provider Harmonizer, Engineer, Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory Curator. The agents interact via a shared-blackboard, finite-state orchestrator layer, and collectively produce Terraform configurations that are not only syntactically valid but also policy-compliant and semantically coherent. To ensure infrastructure correctness and governance, we incorporate Terraform Plan for execution validation and Open Policy Agent (OPA) for customizable policy enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02 and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU, CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03902v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rana Nameer Hussain Khan, Dawood Wasif, Jin-Hee Cho, Ali Butt</dc:creator>
    </item>
    <item>
      <title>Refactoring with LLMs: Bridging Human Expertise and Machine Understanding</title>
      <link>https://arxiv.org/abs/2510.03914</link>
      <description>arXiv:2510.03914v1 Announce Type: new 
Abstract: Code refactoring is a fundamental software engineering practice aimed at improving code quality and maintainability. Despite its importance, developers often neglect refactoring due to the significant time, effort, and resources it requires, as well as the lack of immediate functional rewards. Although several automated refactoring tools have been proposed, they remain limited in supporting a broad spectrum of refactoring types. In this study, we explore whether instruction strategies inspired by human best-practice guidelines can enhance the ability of Large Language Models (LLMs) to perform diverse refactoring tasks automatically. Leveraging the instruction-following and code comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design multiple instruction strategies that encode motivations, procedural steps, and transformation objectives for 61 well-known refactoring types. We evaluate these strategies on benchmark examples and real-world code snippets from GitHub projects. Our results show that instruction designs grounded in Fowler's guidelines enable LLMs to successfully perform all benchmark refactoring types and preserve program semantics in real-world settings, an essential criterion for effective refactoring. Moreover, while descriptive instructions are more interpretable to humans, our results show that rule-based instructions often lead to better performance in specific scenarios. Interestingly, allowing models to focus on the overall goal of refactoring, rather than prescribing a fixed transformation type, can yield even greater improvements in code quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03914v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonnel Chen Kuang Piao, Jean Carlors Paul, Leuson Da Silva, Arghavan Moradi Dakhel, Mohammad Hamdaqa, Foutse Khomh</dc:creator>
    </item>
    <item>
      <title>Why Does the Engineering Manager Still Exist in Agile Software Development?</title>
      <link>https://arxiv.org/abs/2510.03920</link>
      <description>arXiv:2510.03920v1 Announce Type: new 
Abstract: Although Agile methodologies emphasize decentralized decision-making and team autonomy, engineering managers continue to be employed in Agile software organizations. This apparent paradox suggests that traditional managerial functions persist despite the theoretical displacement of managerial hierarchy in Agile. This paper explores the persistence of engineering managers through a multidimensional framework encompassing historical context, theoretical tensions, organizational realities, empirical evidence, evolving managerial roles, and practical implications. A systematic literature review underpins our multifaceted analysis, supplemented by illustrative case studies. We conclude by proposing a conceptual model that reconciles Agile principles with managerial necessity, offering guidance for practitioners, researchers, and tool designers. Implications for leadership development, tool integration, and future research are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03920v1</guid>
      <category>cs.SE</category>
      <category>cs.SI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijsea.2025.16502</arxiv:DOI>
      <arxiv:journal_reference>IJSEA. 16 (2025) 17-28</arxiv:journal_reference>
      <dc:creator>Ravi Kalluri</dc:creator>
    </item>
    <item>
      <title>Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework</title>
      <link>https://arxiv.org/abs/2510.04078</link>
      <description>arXiv:2510.04078v1 Announce Type: new 
Abstract: The permission mechanism in the Android Framework is integral to safeguarding the privacy of users by managing users' and processes' access to sensitive resources and operations. As such, developers need to be equipped with an in-depth understanding of API permissions to build robust Android apps. Unfortunately, the official API documentation by Android chronically suffers from imprecision and incompleteness, causing developers to spend significant effort to accurately discern necessary permissions. This potentially leads to incorrect permission declarations in Android app development, potentially resulting in security violations and app failures. Recent efforts in improving permission specification primarily leverage static and dynamic code analyses to uncover API-permission mappings within the Android framework. Yet, these methodologies encounter substantial shortcomings, including poor adaptability to Android SDK and Framework updates, restricted code coverage, and a propensity to overlook essential API-permission mappings in intricate codebases. This paper introduces a pioneering approach utilizing large language models (LLMs) for a systematic examination of API-permission mappings. In addition to employing LLMs, we integrate a dual-role prompting strategy and an API-driven code generation approach into our mapping discovery pipeline, resulting in the development of the corresponding tool, \tool{}. We formulate three research questions to evaluate the efficacy of \tool{} against state-of-the-art baselines, assess the completeness of official SDK documentation, and analyze the evolution of permission-required APIs across different SDK releases. Our experimental results reveal that \tool{} identifies 2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and 10 respectively, substantially outprforming existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04078v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Hu, Wei Minn, Yonghui Liu, Jiakun Liu, Ferdian Thung, Terry Yue Zhuo, Lwin Khin Shar, Debin Gao, David Lo</dc:creator>
    </item>
    <item>
      <title>GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization</title>
      <link>https://arxiv.org/abs/2510.04135</link>
      <description>arXiv:2510.04135v1 Announce Type: new 
Abstract: Coding agents powered by LLMs face critical sustainability and scalability challenges in industrial deployment, with single runs consuming over 100k tokens and incurring environmental costs that may exceed optimization benefits. This paper introduces GA4GC, the first framework to systematically optimize coding agent runtime (greener agent) and code performance (greener code) trade-offs by discovering Pareto-optimal agent hyperparameters and prompt templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x hypervolume improvement, reducing agent runtime by 37.7% while improving correctness. Our findings establish temperature as the most critical hyperparameter, and provide actionable strategies to balance agent sustainability with code optimization effectiveness in industrial deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04135v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingzhi Gong, Yixin Bian, Luis de la Cal, Giovanni Pinna, Anisha Uteem, David Williams, Mar Zamorano, Karine Even-Mendoza, W. B. Langdon, Hector Menendez, Federica Sarro</dc:creator>
    </item>
    <item>
      <title>Detecting Semantic Clones of Unseen Functionality</title>
      <link>https://arxiv.org/abs/2510.04143</link>
      <description>arXiv:2510.04143v1 Announce Type: new 
Abstract: Semantic code clone detection is the task of detecting whether two snippets of code implement the same functionality (e.g., Sort Array). Recently, many neural models achieved near-perfect performance on this task. These models seek to make inferences based on their training data. Consequently, they better detect clones similar to those they have seen during training and may struggle to detect those they have not. Developers seeking clones are, of course, interested in both types of clones. We confirm this claim through a literature review, identifying three practical clone detection tasks in which the model's goal is to detect clones of a functionality even if it was trained on clones of different functionalities. In light of this finding, we re-evaluate six state-of-the-art models, including both task-specific models and generative LLMs, on the task of detecting clones of unseen functionality. Our experiments reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs perform on par with task-specific models without explicit training for clone detection, but generalize better to unseen functionalities, where F1 drops up to 5% (average 3%) instead. We propose and evaluate the use of contrastive learning to improve the performance of existing models on clones of unseen functionality. We draw inspiration from the computer vision and natural language processing fields where contrastive learning excels at measuring similarity between two objects, even if they come from classes unseen during training. We replace the final classifier of the task-specific models with a contrastive classifier, while for the generative LLMs we propose contrastive in-context learning, guiding the LLMs to focus on the differences between clones and non-clones. The F1 on clones of unseen functionality is improved by up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04143v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Kitsios, Francesco Sovrano, Earl T. Barr, Alberto Bacchelli</dc:creator>
    </item>
    <item>
      <title>Multi Language Models for On-the-Fly Syntax Highlighting</title>
      <link>https://arxiv.org/abs/2510.04166</link>
      <description>arXiv:2510.04166v1 Announce Type: new 
Abstract: Syntax highlighting is a critical feature in modern software development environments, enhancing code readability and developer productivity. However, delivering accurate highlighting in real time remains challenging for online and web-based development tools due to strict time and memory constraints on backend services. These systems must serve highlights rapidly and frequently, even when code is partially valid or invalid. This has led to on-the-fly syntax highlighting, where visual annotations are generated just before content is served, often at high request rates and under incomplete input conditions. To meet these demands efficiently, state-of-the-art models use deep learning to learn the behavior of brute-force syntax highlighting resolvers, tools that are easy to implement but too slow for production. Through the Deep Abstraction process, brute-force strategies are encoded into fast statistical models that achieve both high accuracy and low-latency inference. Despite their success, such models face key challenges: they support only one programming language per model, require large datasets from slow brute-force generators, and involve resource-intensive training. In multi-language environments, this means maintaining multiple independent models, increasing system complexity and operational cost. This work addresses these issues by introducing a unified model capable of highlighting up to six mainstream programming languages, reducing deployment complexity by a factor of six and improving performance on unseen languages. A novel normalization technique significantly enhances model generalization, while few-shot learning experiments show that a small number of oracle samples can replace large datasets, minimizing dependence on brute-force generators. Combined, these innovations enable efficient, scalable, and cost-effective syntax highlighting across diverse programming languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04166v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Edoardo Palma, Pooja Rani, Harald C. Gall</dc:creator>
    </item>
    <item>
      <title>Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience</title>
      <link>https://arxiv.org/abs/2510.04274</link>
      <description>arXiv:2510.04274v1 Announce Type: new 
Abstract: This study investigates how access to Large Language Models (LLMs) and varying levels of professional software development experience affect the prioritization of cybersecurity requirements for web applications. Twenty-three postgraduate students participated in a research study to prioritize security requirements (SRs) using the MoSCoW method and subsequently rated their proposed solutions against multiple evaluation criteria. We divided participants into two groups (one with and the other without access to LLM support during the task). Results showed no significant differences related to LLM use, suggesting that access to LLMs did not noticeably influence how participants evaluated cybersecurity solutions. However, statistically significant differences emerged between experience groups for certain criteria, such as estimated cost to develop a feature, perceived impact on user experience, and risk assessment related to non-implementation of the proposed feature. Participants with more professional experience tended to provide higher ratings for user experience impact and lower risk estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04274v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Tenth International Conference on Cyber-Technologies and Cyber-Systems (CYBER 2025), September 28, 2025 to October 02, 2025 - Lisbon, Portugal</arxiv:journal_reference>
      <dc:creator>Damjan Fujs, Damjan Vavpoti\v{c}, Toma\v{z} Hovelja, Marko Po\v{z}enel</dc:creator>
    </item>
    <item>
      <title>Challenge on Optimization of Context Collection for Code Completion</title>
      <link>https://arxiv.org/abs/2510.04349</link>
      <description>arXiv:2510.04349v1 Announce Type: new 
Abstract: The rapid advancement of workflows and methods for software engineering using AI emphasizes the need for a systematic evaluation and analysis of their ability to leverage information from entire projects, particularly in large code bases. In this challenge on optimization of context collection for code completion, organized by JetBrains in collaboration with Mistral AI as part of the ASE 2025 conference, participants developed efficient mechanisms for collecting context from source code repositories to improve fill-in-the-middle code completions for Python and Kotlin. We constructed a large dataset of real-world code in these two programming languages using permissively licensed open-source projects. The submissions were evaluated based on their ability to maximize completion quality for multiple state-of-the-art neural models using the chrF metric. During the public phase of the competition, nineteen teams submitted solutions to the Python track and eight teams submitted solutions to the Kotlin track. In the private phase, six teams competed, of which five submitted papers to the workshop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04349v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Ustalov, Egor Bogomolov, Alexander Bezzubov, Yaroslav Golubev, Evgeniy Glukhov, Georgii Levtsov, Vladimir Kovalenko</dc:creator>
    </item>
    <item>
      <title>MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models</title>
      <link>https://arxiv.org/abs/2510.04363</link>
      <description>arXiv:2510.04363v1 Announce Type: new 
Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser automation programs from natural language goals by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like, Facebook-like, Discord-like, and Threads-like, covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification including DOM assertions and database snapshots, and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8 percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent, and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at 91.7 percent but fail on complex workflows at 0.0 percent, and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results to enable reproducible assessment of macro synthesis for web automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04363v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunjun Kim, Sejong Kim</dc:creator>
    </item>
    <item>
      <title>Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development</title>
      <link>https://arxiv.org/abs/2510.04380</link>
      <description>arXiv:2510.04380v1 Announce Type: new 
Abstract: Requirement Engineering (RE) is the foundation of successful software development. In RE, the goal is to ensure that implemented systems satisfy stakeholder needs through rigorous requirements elicitation, validation, and evaluation processes. Despite its critical role, RE continues to face persistent challenges, such as ambiguity, conflicting stakeholder needs, and the complexity of managing evolving requirements. A common view is that Artificial Intelligence (AI) has the potential to streamline the RE process, resulting in improved efficiency, accuracy, and management actions. However, using AI also introduces new concerns, such as ethical issues, biases, and lack of transparency. This paper explores how AI can enhance traditional RE practices by automating labor-intensive tasks, supporting requirement prioritization, and facilitating collaboration between stakeholders and AI systems. The paper also describes the opportunities and challenges that AI brings to RE. In particular, the vision calls for ethical practices in AI, along with a much-enhanced collaboration between academia and industry professionals. The focus should be on creating not only powerful but also trustworthy and practical AI solutions ready to adapt to the fast-paced world of software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04380v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-04190-6_11</arxiv:DOI>
      <arxiv:journal_reference>In: SEAA 2025 proceedings, LNCS vol. 16081, Springer</arxiv:journal_reference>
      <dc:creator>Mateen Ahmed Abbasi, Petri Ihantola, Tommi Mikkonen, Niko M\"akitalo</dc:creator>
    </item>
    <item>
      <title>Smart Hiring Redefined: An Intelligent Recruitment Management Platform</title>
      <link>https://arxiv.org/abs/2510.04437</link>
      <description>arXiv:2510.04437v2 Announce Type: new 
Abstract: Against the backdrop of deepening digital and intelligent transformation in human resource management,traditional recruitment models struggle to fully meet enterprises growing demand for precise talent acquisition due to limited efficiency,high costs,and information asymmetry.As a vital tool for optimizing recruitment processes,reducing labor and time costs,and enhancing core competitiveness,intelligent recruitment management systems become an indispensable component of modern organizational talent strategies.Compared with the labor intensive tasks of resume screening,candidate position matching,and interview coordination in traditional manual recruitment,intelligent recruitment systems significantly enhance the efficiency and accuracy of the hiring process through automation and data driven approaches.These systems enable rapid parsing of massive resume volumes,intelligent matching of candidates to positions,and automated scheduling of interview processes.This substantially reduces the workload on human resources departments while improving recruitment quality and response speed.This research leverages the Java technology framework to design and implement an intelligent recruitment management system tailored for campus recruitment scenarios.The system establishes a collaborative platform connecting students,enterprises, and administrators through information technology and intelligent solutions,offering comprehensive functionalities including job posting distribution,resume submission,candidate position matching,and process management.Guided by the vision of "Smart Campus Recruitment",the project delivers a more convenient job seeking experience for students and provides enterprises with more efficient talent screening and recruitment management services,thereby driving high quality development in university enterprise collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04437v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangzhe Wu, Dongyang Lyu, Xiaoqi Li</dc:creator>
    </item>
    <item>
      <title>Improving IR-based Bug Localization with Semantics-Driven Query Reduction</title>
      <link>https://arxiv.org/abs/2510.04468</link>
      <description>arXiv:2510.04468v1 Announce Type: new 
Abstract: Despite decades of research, software bug localization remains challenging due to heterogeneous content and inherent ambiguities in bug reports. Existing methods such as Information Retrieval (IR)-based approaches often attempt to match source documents to bug reports, overlooking the context and semantics of the source code. On the other hand, Large Language Models (LLM) (e.g., Transformer models) show promising results in understanding both texts and code. However, they have not been yet adapted well to localize software bugs against bug reports. They could be also data or resource-intensive. To bridge this gap, we propose, IQLoc, a novel bug localization approach that capitalizes on the strengths of both IR and LLM-based approaches. In particular, we leverage the program semantics understanding of transformer-based models to reason about the suspiciousness of code and reformulate queries during bug localization using Information Retrieval. To evaluate IQLoc, we refine the Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated IQLoc using three performance metrics and compare it against four baseline techniques. Experimental results demonstrate its superiority, achieving up to 58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in HIT@K for the test bug reports with random and time-wise splits, respectively. Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces, 72.73% for those that include code elements, and 65.38% for those containing only descriptions in natural language. By integrating program semantic understanding into Information Retrieval, IQLoc mitigates several longstanding challenges of traditional IR-based approaches in bug localization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04468v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asif Mohammed Samir, Mohammad Masudur Rahman</dc:creator>
    </item>
    <item>
      <title>DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing</title>
      <link>https://arxiv.org/abs/2510.04469</link>
      <description>arXiv:2510.04469v2 Announce Type: new 
Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches that treat individual seeds as tasks, DynamiQ leverages structural information from the program's call graph to define tasks and continuously refines task allocation using runtime feedback. This design significantly reduces redundant exploration and enhances fuzzing efficiency at scale. Built on top of the state-of-the-art LibAFL framework, DynamiQ incorporates several practical optimizations in both task allocation and task-aware fuzzing. Evaluated on 12 real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ outperforms state-of-the-art parallel fuzzers in both code coverage and vulnerability discovery, uncovering 9 previously unknown bugs in widely used and extensively fuzzed open-source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04469v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqi Yan, Toby Murray, Benjamin I. P. Rubinstein, Van-Thuan Pham</dc:creator>
    </item>
    <item>
      <title>Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem</title>
      <link>https://arxiv.org/abs/2510.04495</link>
      <description>arXiv:2510.04495v1 Announce Type: new 
Abstract: Trivial packages, small modules with low functionality, are common in the npm ecosystem and can pose security risks despite their simplicity. This paper refines existing definitions and introduce data-only packages that contain no executable logic. A rule-based static analysis method is developed to detect trivial and data-only packages and evaluate their prevalence and associated risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are trivial, with vulnerability levels comparable to non-trivial ones, and data-only packages, though rare, also contain risks. The proposed detection tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale analysis to reduce security exposure. This findings suggest that trivial and data-only packages warrant greater attention in dependency management to reduce potential technical debt and security exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04495v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Napasorn Tevarut, Brittany Reid, Yutaro Kashiwa, Pattara Leelaprute, Arnon Rungsawang, Bundit Manaskasemsak, Hajimu Iida</dc:creator>
    </item>
    <item>
      <title>Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation</title>
      <link>https://arxiv.org/abs/2510.04519</link>
      <description>arXiv:2510.04519v1 Announce Type: new 
Abstract: Distributed control systems (DCS) manage the automation for many industrial production processes (e.g., power plants, chemical refineries, steel mills). Programming the software for such systems remains a largely manual and tedious process, incurring costs of millions of dollars for extensive facilities. Large language models (LLMs) have been found helpful in generating DCS control logic, resulting in commercial copilot tools. Today, these tools are focused on textual notations, they provide limited automation, and have not been tested on large datasets with realistic test cases. We introduce Spec2Control, a highly automated LLM workflow to generate graphical control logic directly from natural language user requirements. Experiments using an open dataset with 10 control narratives and 65 complex test cases demonstrate that Spec2Control can successfully identify control strategies, can generate 98.6% of correct control strategy connections autonomously, and can save between 94-96% of human labor. Spec2Control is being integrated into commercial ABB engineering tools, but is also available as an open-source variant for independent validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04519v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heiko Koziolek, Thilo Braun, Virendra Ashiwal, Sofia Linsbauer, Marthe Ahlgreen Hansen, Karoline Grotterud</dc:creator>
    </item>
    <item>
      <title>Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes</title>
      <link>https://arxiv.org/abs/2510.04603</link>
      <description>arXiv:2510.04603v1 Announce Type: new 
Abstract: Context: Open Source Software (OSS) is a vital public good, included across most of modern software stacks, significantly impacting GDP and national tech growth, while supporting interoperability, sovereignty, and transparency. However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes by analyzing policies and support actions leveraging OSS for software reuse and collaborative development across 16 digitally mature countries, and proposing potential indicators for said indexes. It examines OSS policy formation, stated goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy documents with semi-structured interviews of government representatives, producing detailed country reports. These are cross-analyzed, focusing on OSS policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both inbound acquisition and outbound sharing, and are predominantly governed by central public sector organizations. Policy goals include interoperability, digital sovereignty, transparency, and cost efficiency, with security framed both as a risk and strength. Implementation is supported by diverse Open Source Program Offices (OSPOs) at multiple government levels, which foster capacity building, resource pooling, and sustainable project governance. Indicators are synthesized and proposed across 14 areas covering policy incentives and design, and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital transformation. Clear policy frameworks, coupled with institutional support such as OSPOs, are essential. International digital maturity frameworks should expand OSS indicators to better guide and assess government adoption and impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04603v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan Lin{\aa}ker, Sachiko Muto</dc:creator>
    </item>
    <item>
      <title>Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation</title>
      <link>https://arxiv.org/abs/2510.04605</link>
      <description>arXiv:2510.04605v1 Announce Type: new 
Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software engineering (SE) but face limitations in processing code structure information and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a promising alternative with global bidirectional encoding and decoupled generation steps. This work presents the first comprehensive evaluation of DLLMs across the software development lifecycle, including code generation, defect detection, and program repair. On a large-scale benchmark of 52,937 tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy improvement achieving a 113% gain on cross-file repair, while maintaining superior efficiency and reduced latency. Our results establish DLLMs as a superior paradigm for SE tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04605v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Zhang, Tianlin Li, Xiaoyu Zhang, Qiang Hu, Bin Shi</dc:creator>
    </item>
    <item>
      <title>A survey on the impact of emotions on the productivity among software developers</title>
      <link>https://arxiv.org/abs/2510.04611</link>
      <description>arXiv:2510.04611v1 Announce Type: new 
Abstract: The time pressure associated with software development, among other factors, often leads to a diminished emotional state among developers. However, whether emotions affect perceived productivity remains an open question. This study aims to determine the strength and direction of the relationship between emotional state and perceived productivity among software developers. We employed a two-stage approach. First, a survey was conducted with a pool of nine experts to validate the measurement model. Second, a survey was administered to a pool of 88 software developers to empirically test the formulated hypothesis by using Partial Least Squares, as the data analysis method. The results of the path analysis clearly confirm the formulated hypothesis, showing that the emotional state of a software developer has a strong positive, and significant impact (beta = 0.893, p &lt; 0.001) on perceived productivity among software developers. The findings highlight the importance of managing and improving developers emotional well-being to enhance productivity in software development environments. Additionally, interventions aimed at reducing burnout, stress, and other negative factors could have a considerable impact on their performance outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04611v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pawel Weichbroth, Maciej Lotysz, Michal Wrobel</dc:creator>
    </item>
    <item>
      <title>Evolaris: A Roadmap to Self-Evolving Software Intelligence Management</title>
      <link>https://arxiv.org/abs/2510.04689</link>
      <description>arXiv:2510.04689v1 Announce Type: new 
Abstract: In recent years, the landscape of software threats has become significantly more dynamic and distributed. Security vulnerabilities are no longer discovered and shared only through formal channels such as public vulnerability databases or vendor advisories. Increasingly, criti- cal threat information emerges informally through blogs, social media, developer forums, open source repositories, and even underground com- munities. To this end, capturing such intelligence in a timely manner is essential for maintaining situational awareness and enabling prompt security responses. However, this remains a complex challenge due to the fragmented nature of data sources and the technical difficulty of collecting, parsing, mapping, and validating information at scale. To ad- dress this, we propose Evolaris, a self-evolving software intelligence sys- tem built on a multi-agent framework. Evolaris is designed to support a full-stack workflow, where agents operate independently but coordinate through shared context to perform tasks such as information discovery, reasoning, gap completion, validation, and risk detection. This archi- tecture enables the platform to learn from new inputs, refine its internal knowledge, and adapt to emerging threat patterns over time, which could continuously improve the precision, timeliness, and scalability of software threat analysis, and offers a sustainable foundation for proactive secu- rity decision-making and strengthens the broader ecosystem of security threat understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04689v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chengwei Liu, Wenbo Guo, Yuxin Zhang, Limin Wang, Sen Chen, Lei Bu, Yang Liu</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures</title>
      <link>https://arxiv.org/abs/2510.04711</link>
      <description>arXiv:2510.04711v1 Announce Type: new 
Abstract: While cloud-native microservice architectures have transformed software development, their complexity makes Root Cause Analysis (RCA) both crucial and challenging. Although many data-driven RCA models have been proposed, we find that existing benchmarks are often oversimplified and fail to capture real-world conditions. Our preliminary study shows that simple rule-based methods can match or even outperform state-of-the-art (SOTA) models on four widely used benchmarks, suggesting performance overestimation due to benchmark simplicity. To address this, we systematically analyze popular RCA benchmarks and identify key limitations in fault injection, call graph design, and telemetry patterns. Based on these insights, we develop an automated framework to generate more realistic benchmarks, yielding a dataset of 1,430 validated failure cases from 9,152 injections, covering 25 fault types under dynamic workloads with hierarchical ground-truth labels and verified SLI impact. Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy (average 0.21, best 0.37) and significantly longer execution times. Our analysis highlights three common failure patterns: scalability issues, observability blind spots, and modeling bottlenecks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04711v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aoyang Fang, Songhan Zhang, Yifan Yang, Haotong Wu, Junjielong Xu, Xuyang Wang, Rui Wang, Manyi Wang, Qisheng Lu, Pinjia He</dc:creator>
    </item>
    <item>
      <title>Agile Software Effort Estimation using Regression Techniques</title>
      <link>https://arxiv.org/abs/2510.04760</link>
      <description>arXiv:2510.04760v1 Announce Type: new 
Abstract: Software development effort estimation is one of the most critical aspect in software development process, as the success or failure of the entire project depends on the accuracy of estimations. Researchers are still conducting studies on agile effort estimation. The aim of this research is to develop a story point based agile effort estimation model using LASSO and Elastic Net regression techniques. The experimental work is applied to the agile story point approach using 21 software projects collected from six firms. The two algorithms are trained using their default parameters and tuned grid search with 5-fold cross-validation to get an enhanced model. The experiment result shows LASSO regression achieved better predictive performance PRED (8%) and PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593, MdMER of 0.063, and MSE of 0.0007. The results are also compared with other related literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04760v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sisay Deresa Sima, Ayalew Belay Habtie</dc:creator>
    </item>
    <item>
      <title>GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes</title>
      <link>https://arxiv.org/abs/2510.04791</link>
      <description>arXiv:2510.04791v1 Announce Type: new 
Abstract: GUIs are foundational to interactive systems and play a pivotal role in early requirements elicitation through prototyping. Ensuring that GUI implementations fulfill NL requirements is essential for robust software engineering, especially as LLM-driven programming agents become increasingly integrated into development workflows. Existing GUI testing approaches, whether traditional or LLM-driven, often fall short in handling the complexity of modern interfaces, and typically lack actionable feedback and effective integration with automated development agents. In this paper, we introduce GUISpector, a novel framework that leverages a multi-modal (M)LLM-based agent for the automated verification of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to interpret and operationalize NL requirements, enabling to autonomously plan and execute verification trajectories across GUI applications. Second, GUISpector systematically extracts detailed NL feedback from the agent's verification process, providing developers with actionable insights that can be used to iteratively refine the GUI artifact or directly inform LLM-based code generation in a closed feedback loop. Third, we present an integrated tool that unifies these capabilities, offering practitioners an accessible interface for supervising verification runs, inspecting agent rationales and managing the end-to-end requirements verification process. We evaluated GUISpector on a comprehensive set of 150 requirements based on 900 acceptance criteria annotations across diverse GUI applications, demonstrating effective detection of requirement satisfaction and violations and highlighting its potential for seamless integration of actionable feedback into automated LLM-driven development workflows. The video presentation of GUISpector is available at: https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04791v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kristian Kolthoff, Felix Kretzer, Simone Paolo Ponzetto, Alexander Maedche, Christian Bartelt</dc:creator>
    </item>
    <item>
      <title>RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms</title>
      <link>https://arxiv.org/abs/2510.04796</link>
      <description>arXiv:2510.04796v1 Announce Type: new 
Abstract: Empirical research on code review processes is increasingly central to understanding software quality and collaboration. However, collecting and analyzing review data remains a time-consuming and technically intensive task. Most researchers follow similar workflows - writing ad hoc scripts to extract, filter, and analyze review data from platforms like GitHub and GitLab. This paper introduces RevMine, a conceptual tool that streamlines the entire code review mining pipeline using large language models (LLMs). RevMine guides users through authentication, endpoint discovery, and natural language-driven data collection, significantly reducing the need for manual scripting. After retrieving review data, it supports both quantitative and qualitative analysis based on user-defined filters or LLM-inferred patterns. This poster outlines the tool's architecture, use cases, and research potential. By lowering the barrier to entry, RevMine aims to democratize code review mining and enable a broader range of empirical software engineering studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04796v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Samah Kansab, Francis Bordeleau, Ali Tizghadam</dc:creator>
    </item>
    <item>
      <title>InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface</title>
      <link>https://arxiv.org/abs/2510.04835</link>
      <description>arXiv:2510.04835v1 Announce Type: new 
Abstract: Fuzzing is a highly effective automated testing method for uncovering software vulnerabilities. Despite advances in fuzzing techniques, such as coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus caused by fuzz blockers, limiting their ability to find deeper vulnerabilities. Human expertise can address these challenges, but analyzing fuzzing results to guide this support remains labor-intensive. To tackle this, we introduce InsightQL, the first human-assisting framework for fuzz blocker analysis. Powered by a unified database and an intuitive parameterized query interface, InsightQL aids developers in systematically extracting insights and efficiently unblocking fuzz blockers. Our experiments on 14 popular real-world libraries from the FuzzBench benchmark demonstrate the effectiveness of InsightQL, leading to the unblocking of many fuzz blockers and considerable improvements in code coverage (up to 13.90%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04835v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wentao Gao, Renata Borovica-Gajic, Sang Kil Cha, Tian Qiu, Van-Thuan Pham</dc:creator>
    </item>
    <item>
      <title>FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration</title>
      <link>https://arxiv.org/abs/2510.04852</link>
      <description>arXiv:2510.04852v1 Announce Type: new 
Abstract: AI coding assistants are rapidly becoming integral to modern software development. A key challenge in this space is the continual need to migrate and modernize codebases in response to evolving software ecosystems. Traditionally, such migrations have relied on rule-based systems and human intervention. With the advent of powerful large language models (LLMs), AI-driven agentic frameworks offer a promising alternative-but their effectiveness has not been systematically evaluated. In this paper, we introduce FreshBrew, a novel benchmark for evaluating AI agents on project-level Java migrations, with a specific focus on measuring an agent's ability to preserve program semantics and avoid reward hacking, which we argue requires projects with high test coverage for a rigorous and reliable evaluation. We benchmark several state-of-the-art LLMs, and compare their performance against established rule-based tools. Our evaluation of AI agents on this benchmark of 228 repositories shows that the top-performing model, Gemini 2.5 Flash, can successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis reveals novel insights into the critical strengths and limitations of current agentic approaches, offering actionable insights into their real-world applicability. Our empirical study reveals failure modes of current AI agents in realistic Java modernization tasks, providing a foundation for evaluating trustworthy code-migration systems. By releasing FreshBrew, we aim to facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven codebase modernization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04852v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor May, Diganta Misra, Yanqi Luo, Anjali Sridhar, Justine Gehring, Silvio Soares Ribeiro Junior</dc:creator>
    </item>
    <item>
      <title>Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches</title>
      <link>https://arxiv.org/abs/2510.04905</link>
      <description>arXiv:2510.04905v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have substantially improved automated code generation. While function-level and file-level generation have achieved promising results, real-world software development typically requires reasoning across entire repositories. This gives rise to the challenging task of Repository-Level Code Generation (RLCG), where models must capture long-range dependencies, ensure global semantic consistency, and generate coherent code spanning multiple files or modules. To address these challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm that integrates external retrieval mechanisms with LLMs, enhancing context-awareness and scalability. In this survey, we provide a comprehensive review of research on Retrieval-Augmented Code Generation (RACG), with an emphasis on repository-level approaches. We categorize existing work along several dimensions, including generation strategies, retrieval modalities, model architectures, training paradigms, and evaluation protocols. Furthermore, we summarize widely used datasets and benchmarks, analyze current limitations, and outline key challenges and opportunities for future research. Our goal is to establish a unified analytical framework for understanding this rapidly evolving field and to inspire continued progress in AI-powered software engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04905v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yicheng Tao, Yao Qin, Yepang Liu</dc:creator>
    </item>
    <item>
      <title>Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain</title>
      <link>https://arxiv.org/abs/2510.04964</link>
      <description>arXiv:2510.04964v1 Announce Type: new 
Abstract: Software signing provides a formal mechanism for provenance by ensuring artifact integrity and verifying producer identity. It also imposes tooling and operational costs to implement in practice. In an era of centralized registries such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask whether hardening registry security controls obviates the need for end-to-end artifact signing. In this work, we posit that the core guarantees of signing, provenance, integrity, and accountability are not automatically carried across different software distribution boundaries. These boundaries include mirrors, corporate proxies, re-hosting, and air-gapped transfers, where registry security controls alone cannot provide sufficient assurance. We synthesize historical practice and present a trust model for modern distribution modes to identify when signing is necessary to extend trust beyond registry control. Treating signing as a baseline layer of defense strengthens software supply chain assurance even when registries are secure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04964v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelechi G. Kalu, James C. Davis</dc:creator>
    </item>
    <item>
      <title>Quantum Computing as a Service - a Software Engineering Perspective</title>
      <link>https://arxiv.org/abs/2510.04982</link>
      <description>arXiv:2510.04982v1 Announce Type: new 
Abstract: Quantum systems have started to emerge as a disruptive technology and enabling platforms - exploiting the principles of quantum mechanics via programmable quantum bits (QuBits) - to achieve quantum supremacy in computing. Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and consortiums like 'Quantum Flagship' are striving to develop practically capable and commercially viable quantum computing (QC) systems and technologies. Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the philosophy of service-orientation that can offer QC resources and platforms, as utility computing, to individuals and organisations who do not own quantum computers. This research investigates a process-centric and architecture-driven approach to offer a software engineering perspective on enabling QCaaS - a.k.a quantum service-orientation. We employed a two-phase research method comprising (a) a systematic mapping study and (b) an architecture-based development, first to identify the phases of the quantum service development life cycle and subsequently to integrate these phases into a reference architecture that supports QCaaS. The SMS process retrieved a collection of potentially relevant research literature and based on a multi-step selection and qualitative assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs investigate (i) demographic details in terms of frequency, types, and trends of research, (ii) phases of quantum service development lifecycle to derive a reference architecture for conception, modeling, assembly, and deployment of services, and (iii) The results identify a 4-phased development lifecycle along with quantum significant requirements (QSRs), various modeling notations, catalogue of patterns, programming languages, and deployment platforms that can be integrated in a layered reference architecture to engineer QCaaS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04982v1</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aakash Ahmad, Muhammad Waseem, Bakheet Aljedaani, Mahdi Fahmideh, Peng Liang, Feras Awaysheh</dc:creator>
    </item>
    <item>
      <title>AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis</title>
      <link>https://arxiv.org/abs/2510.04997</link>
      <description>arXiv:2510.04997v1 Announce Type: new 
Abstract: Understanding software faults is essential for empirical research in software development and maintenance. However, traditional fault analysis, while valuable, typically involves multiple expert-driven steps such as collecting potential faults, filtering, and manual investigation. These processes are both labor-intensive and time-consuming, creating bottlenecks that hinder large-scale fault studies in complex yet critical software systems and slow the pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study into three key phases: (1) research objective definition, (2) data preparation, and (3) fault analysis, and we conduct an initial exploration study of applying Large Language Models (LLMs) for fault analysis of open-source software. Specifically, we perform the evaluation on 3,829 software faults drawn from a high-quality empirical study. Our results show that LLMs can substantially improve efficiency in fault analysis, with an average processing time of about two hours, compared to the weeks of manual effort typically required. We conclude by outlining a detailed research plan that highlights both the potential of LLMs for advancing empirical fault studies and the open challenges that required be addressed to achieve fully automated, end-to-end software fault analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04997v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiongchi Yu, Weipeng Jiang, Xiaoyu Zhang, Qiang Hu, Xiaofei Xie, Chao Shen</dc:creator>
    </item>
    <item>
      <title>LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain</title>
      <link>https://arxiv.org/abs/2510.03288</link>
      <description>arXiv:2510.03288v1 Announce Type: cross 
Abstract: Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: https://logaction.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03288v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Chiming Duan, Minghua He, Pei Xiao, Tong Jia, Xin Zhang, Zhewei Zhong, Xiang Luo, Yan Niu, Lingzhe Zhang, Yifan Wu, Siyu Yu, Weijie Hong, Ying Li, Gang Huang</dc:creator>
    </item>
    <item>
      <title>Embedding Sustainability in Software Engineering Curriculum: A Case Study</title>
      <link>https://arxiv.org/abs/2510.03321</link>
      <description>arXiv:2510.03321v1 Announce Type: cross 
Abstract: Sustainability is increasingly recognized as a critical dimension of engineering education, yet its integration into Software Engineering curricula remains a challenge. This paper reports on a case study that examines how sustainability is being embedded across modules in the Software Engineering program at one university. The paper outlines the process through which academics and students co-identified opportunities for integration, guided by the five dimensions of the Sustainability Awareness Framework, targeted discussion questions, and good practice examples drawn from the Green Software Foundation patterns. The study highlights practical steps - including the use of frameworks, illustrative examples, student engagement, and iterative consultative processes - that can support other institutions seeking to embed sustainability into their programs. We also discuss strategies for integrating sustainability into the Software Engineering curriculum and argue that such integration is a necessary and urgent step to prepare Software Engineering graduates as sustainability-aware professionals in our changing society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03321v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruzanna Chitchyan, Niki Mahmoudi</dc:creator>
    </item>
    <item>
      <title>PLSemanticsBench: Large Language Models As Programming Language Interpreters</title>
      <link>https://arxiv.org/abs/2510.03415</link>
      <description>arXiv:2510.03415v2 Announce Type: cross 
Abstract: As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03415v2</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric</dc:creator>
    </item>
    <item>
      <title>Formal Analysis of Metastable Failures in Software Systems</title>
      <link>https://arxiv.org/abs/2510.03551</link>
      <description>arXiv:2510.03551v1 Announce Type: cross 
Abstract: Many large-scale software systems demonstrate metastable failures. In this class of failures, a stressor such as a temporary spike in workload causes the system performance to drop and, subsequently, the system performance continues to remain low even when the stressor is removed. These failures have been reported by many large corporations and considered to be a rare but catastrophic source of availability outages in cloud systems.
  In this paper, we provide the mathematical foundations of metastability in request-response server systems. We model such systems using a domain-specific language. We show how to construct continuous-time Markov chains (CTMCs) that approximate the semantics of the programs through modeling and data-driven calibration. We use the structure of the CTMC models to provide a visualization of the qualitative behavior of the model. The visualization is a surprisingly effective way to identify system parameterizations that cause a system to show metastable behaviors.
  We complement the qualitative analysis with quantitative predictions. We provide a formal notion of metastable behaviors based on escape probabilities, and show that metastable behaviors are related to the eigenvalue structure of the CTMC. Our characterization leads to algorithmic tools to predict recovery times in metastable models of server systems.
  We have implemented our technique in a tool for the modeling and analysis of server systems. Through models inspired by failures in real request-response systems, we show that our qualitative visual analysis captures and predicts many instances of metastability that were observed in the field in a matter of milliseconds. Our algorithms confirm that recovery times surge as the system parameters approach metastable modes in the dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03551v1</guid>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Isaacs, Peter Alvaro, Rupak Majumdar, Kiran-Kumar Muniswamy-Reddy, Mahmoud Salamati, Sadegh Soudjani</dc:creator>
    </item>
    <item>
      <title>MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection</title>
      <link>https://arxiv.org/abs/2510.04397</link>
      <description>arXiv:2510.04397v1 Announce Type: cross 
Abstract: Software vulnerabilities (SVs) pose a critical threat to safety-critical systems, driving the adoption of AI-based approaches such as machine learning and deep learning for software vulnerability detection. Despite promising results, most existing methods are limited to a single programming language. This is problematic given the multilingual nature of modern software, which is often complex and written in multiple languages. Current approaches often face challenges in capturing both shared and language-specific knowledge of source code, which can limit their performance on diverse programming languages and real-world codebases. To address this gap, we propose MULVULN, a novel multilingual vulnerability detection approach that learns from source code across multiple languages. MULVULN captures both the shared knowledge that generalizes across languages and the language-specific knowledge that reflects unique coding conventions. By integrating these aspects, it achieves more robust and effective detection of vulnerabilities in real-world multilingual software systems. The rigorous and extensive experiments on the real-world and diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven programming languages, demonstrate the superiority of MULVULN over thirteen effective and state-of-the-art baselines. Notably, MULVULN achieves substantially higher F1-score, with improvements ranging from 1.45% to 23.59% compared to the baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04397v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Nguyen, Surya Nepal, Xingliang Yuan, Tingmin Wu, Fengchao Chen, Carsten Rudolph</dc:creator>
    </item>
    <item>
      <title>NaturalEdit: Code Modification through Direct Interaction with Adaptive Natural Language Representation</title>
      <link>https://arxiv.org/abs/2510.04494</link>
      <description>arXiv:2510.04494v1 Announce Type: cross 
Abstract: Code modification requires developers to comprehend code, plan changes, articulate intentions, and validate outcomes, making it a cognitively demanding process. Generated natural language code summaries aid comprehension but remain static and limited in supporting the full workflow. We present NaturalEdit, a system that makes code summaries interactive and adaptive representations directly linked to source code. Grounded in the Cognitive Dimensions of Notations, NaturalEdit implements a paradigm of code modification through interaction with natural language representations through three key features: (1) adaptive multi-faceted representation of code summaries with flexible Abstraction Gradient; (2) interactive mapping mechanisms between summaries and codes, ensuring a tight Closeness of Mapping; and (3) intent-driven, bidirectional synchronization that reduces Viscosity in editing and validation. A technical evaluation confirms the performance of NaturalEdit, and a user study with 12 developers shows that it enhances comprehension, intent articulation, and validation, giving developers greater confidence and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04494v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ningzhi Tang, David Meininger, Gelei Xu, Yiyu Shi, Yu Huang, Collin McMillan, Toby Jia-Jun Li</dc:creator>
    </item>
    <item>
      <title>A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance</title>
      <link>https://arxiv.org/abs/2510.04750</link>
      <description>arXiv:2510.04750v1 Announce Type: cross 
Abstract: Dyslexia in adults remains an under-researched and under-served area, particularly in non-English-speaking contexts, despite its significant impact on personal and professional lives. This work addresses that gap by focusing on Sinhala, a low-resource language with limited tools for linguistic accessibility. We present an assistive system explicitly designed for Sinhala-speaking adults with dyslexia. The system integrates Whisper for speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model trained for Sinhala to identify common dyslexic errors, and a combined mT5 and Mistral-based model to generate corrected text. Finally, the output is converted back to speech using gTTS, creating a complete multimodal feedback loop. Despite the challenges posed by limited Sinhala-language datasets, the system achieves 0.66 transcription accuracy and 0.7 correction accuracy with 0.65 overall system accuracy. These results demonstrate both the feasibility and effectiveness of the approach. Ultimately, this work highlights the importance of inclusive Natural Language Processing (NLP) technologies in underrepresented languages and showcases a practical</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04750v1</guid>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peshala Perera, Deshan Sumanathilaka</dc:creator>
    </item>
    <item>
      <title>Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization</title>
      <link>https://arxiv.org/abs/2510.04890</link>
      <description>arXiv:2510.04890v1 Announce Type: cross 
Abstract: Modern processors increasingly rely on SIMD instruction sets, such as AVX and RVV, to significantly enhance parallelism and computational performance. However, production-ready compilers like LLVM and GCC often fail to fully exploit available vectorization opportunities due to disjoint vectorization passes and limited extensibility. Although recent attempts in heuristics and intermediate representation (IR) designs have attempted to address these problems, efficiently simplifying control flow analysis and accurately identifying vectorization opportunities remain challenging tasks.
  To address these issues, we introduce a novel vectorization pipeline featuring two specialized IR extensions: SIR, which encodes high-level structural information, and VIR, which explicitly represents instruction dependencies through data dependency analysis. Leveraging the detailed dependency information provided by VIR, we develop a flexible and extensible vectorization framework. This approach substantially improves interoperability across vectorization passes and expands the search space for identifying isomorphic instructions, ultimately enhancing both the scope and efficiency of automatic vectorization. Experimental evaluations demonstrate that our proposed vectorization pipeline achieves significant performance improvements, delivering speedups of up to 53% and 58% compared to LLVM and GCC, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04890v1</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shihan Fang, Wenxin Zheng</dc:creator>
    </item>
    <item>
      <title>Test Schedule Generation for Acceptance Testing of Mission-Critical Satellite Systems</title>
      <link>https://arxiv.org/abs/2501.01224</link>
      <description>arXiv:2501.01224v2 Announce Type: replace 
Abstract: Mission-critical system, such as satellite systems, healthcare systems, and nuclear power plant control systems, undergo rigorous testing to ensure they meet specific operational requirements throughout their operation. This includes Operational Acceptance Testing (OAT), which aims to ensure that the system functions correctly under real-world operational conditions. In satellite development, In-Orbit Testing (IOT) is a crucial OAT activity performed regularly and as needed after deployment in orbit to check the satellite's performance and ensure that operational requirements are met. The scheduling of an IOT campaign, which executes multiple IOT procedures, is an important yet challenging problem, as it accounts for various factors, including satellite visibility, antenna usage costs, testing time periods, and operational constraints. To address the IOT scheduling problem, we propose a multi-objective approach to generate near-optimal IOT schedules, accounting for operational costs, fragmentation (i.e., the splitting of tests), and resource efficiency, which align with practitioners' objectives for IOT scheduling. Our industrial case study with SES Techcom shows significant improvements, as follows: an average improvement of 49.4% in the cost objective, 60.4% in the fragmentation objective, and 30% in the resource usage objective, compared to our baselines. Additionally, our approach improves cost efficiency by 538% and resource usage efficiency by 39.42% compared to manually constructed schedules provided by practitioners, while requiring only 12.5% of the time needed for manual IOT scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01224v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10664-025-10737-8</arxiv:DOI>
      <dc:creator>Rapha\"el Ollando, Seung Yeob Shin, Mario Minardi, Nikolas Sidiropoulos</dc:creator>
    </item>
    <item>
      <title>Towards Sustainable and Secure Reuse in Dependency Supply Chains: Initial Analysis of NPM packages at the End of the Chain</title>
      <link>https://arxiv.org/abs/2503.02804</link>
      <description>arXiv:2503.02804v3 Announce Type: replace 
Abstract: Much of the success of modern software development can be attributed to code reuse. The ability to reuse existing functionality via third-party dependencies has enabled massive gains in productivity, but for a long time the dominant philosophy has been to 'reuse as much as possible, without thought for what is being depended upon', creating fragile dependency chains. Heavy reliance has raised resiliency and maintenance concerns. In this vision paper, we investigate packages that challenge the typical concepts of reuse - that is, packages with no dependencies themselves that bear the responsibility of being at the end of the dependency supply chain. By avoiding dependencies, these packages at the end of the chain may also avoid the associated risks. Our initial analysis of the most depended upon NPM packages shows that such end-of-chain packages make up a significant portion of these critical dependency chain (over 50%). We find that these end-of-chain packages vary in characteristics and are not just packages that can be easily replaced, and present five cases. We then ask ourselves: Should maintainers minimize external dependencies? We argue that these packages reveal important lessons for strategic reuse-balancing the undeniable benefits of dependency ecosystems with sustainable, secure practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02804v3</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brittany Anne Reid, Raula Gaikovina Kula</dc:creator>
    </item>
    <item>
      <title>Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and Perspectives</title>
      <link>https://arxiv.org/abs/2503.13793</link>
      <description>arXiv:2503.13793v2 Announce Type: replace 
Abstract: Applications of Large Language Models (LLMs) are rapidly growing in industry and academia for various software engineering (SE) tasks. As these models become more integral to critical processes, ensuring their reliability and trustworthiness becomes essential. Consequently, the concept of trust in these systems is becoming increasingly critical. Well-calibrated trust is important, as excessive trust can lead to security vulnerabilities, and risks, while insufficient trust can hinder innovation. However, the landscape of trust-related concepts in LLMs in SE is relatively unclear, with concepts such as trust, distrust, and trustworthiness lacking clear conceptualizations in the SE community. To bring clarity to the current research status and identify opportunities for future work, we conducted a comprehensive review of $88$ papers: a systematic literature review of $18$ papers focused on LLMs in SE, complemented by an analysis of 70 papers from broader trust literature. Additionally, we conducted a survey study with 25 domain experts to gain insights into practitioners' understanding of trust and identify gaps between existing literature and developers' perceptions. The result of our analysis serves as a roadmap that covers trust-related concepts in LLMs in SE and highlights areas for future exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13793v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dipin Khati, Yijin Liu, David N. Palacio, Yixuan Zhang, Denys Poshyvanyk</dc:creator>
    </item>
    <item>
      <title>Speculative Automated Refactoring of Imperative Deep Learning Programs to Graph Execution</title>
      <link>https://arxiv.org/abs/2504.05424</link>
      <description>arXiv:2504.05424v4 Announce Type: replace 
Abstract: Efficiency is essential to support ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code -- supporting symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, imperative DL frameworks encouraging eager execution have emerged but at the expense of run-time performance. Though hybrid approaches aim for the "best of both worlds," using them effectively requires subtle considerations. Our key insight is that, while DL programs typically execute sequentially, hybridizing imperative DL code resembles parallelizing sequential code in traditional systems. Inspired by this, we present an automated refactoring approach that assists developers in determining which otherwise eagerly-executed imperative DL functions could be effectively and efficiently executed as graphs. The approach features novel static imperative tensor and side-effect analyses for Python. Due to its inherent dynamism, analyzing Python may be unsound; however, the conservative approach leverages a speculative (keyword-based) analysis for resolving difficult cases that informs developers of any assumptions made. The approach is: (i) implemented as a plug-in to the PyDev Eclipse IDE that integrates the WALA Ariadne analysis framework and (ii) evaluated on nineteen DL projects consisting of 132 KLOC. The results show that 326 of 766 candidate functions (42.56%) were refactorable, and an average relative speedup of 2.16x on performance tests was observed with negligible differences in model accuracy. The results indicate that the approach is useful in optimizing imperative DL code to its full potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05424v4</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE)</arxiv:journal_reference>
      <dc:creator>Raffi Khatchadourian, Tatiana Castro V\'elez, Mehdi Bagherzadeh, Nan Jia, Anita Raja</dc:creator>
    </item>
    <item>
      <title>The Ultimate Configuration Management Tool? Lessons from a Mixed Methods Study of Ansible's Challenges</title>
      <link>https://arxiv.org/abs/2504.08678</link>
      <description>arXiv:2504.08678v2 Announce Type: replace 
Abstract: Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed methods approach, we investigate challenges, obstacles, and issues faced by practitioners. We analyze 59,157 posts from Stack Overflow, Reddit, and the Ansible Forum to identify common pain points, complemented by 20 semi-structured interviews with practitioners of varying expertise levels. Based on our findings, we propose four main recommendations to improve Ansible: 1) refactoring to mitigate performance issues, 2) restructuring higher-level language concepts, 3) improved debugging and error reporting tools, and 4) better documentation and learning resources. By highlighting the real-world struggles of Ansible users, we provide actionable insights for tool designers, educators, and the broader IaC community, contributing to a deeper understanding of the trade-offs inherent in IaC tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08678v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Carreira, Nuno Saavedra, Alexandra Mendes, Jo\~ao F. Ferreira</dc:creator>
    </item>
    <item>
      <title>Identity resolution of software metadata using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.23500</link>
      <description>arXiv:2505.23500v2 Announce Type: replace 
Abstract: Software is an essential component of research. However, little attention has been paid to it compared with that paid to research data. Recently, there has been an increase in efforts to acknowledge and highlight the importance of software in research activities. Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy ToolShed offers valuable insights into research software in the Life Sciences. Although originally intended to support discovery and integration, this metadata can be repurposed for large-scale analysis of software practices. However, its quality and completeness vary across platforms, reflecting diverse documentation practices. To gain a comprehensive view of software development and sustainability, consolidating this metadata is necessary, but requires robust mechanisms to address its heterogeneity and scale.
  This article presents an evaluation of instruction-tuned large language models for the task of software metadata identity resolution, a critical step in assembling a cohesive collection of research software. Such a collection is the reference component for the Software Observatory at OpenEBench, a platform that aggregates metadata to monitor the FAIRness of research software in the Life Sciences. We benchmarked multiple models against a human-annotated gold standard, examined their behavior on ambiguous cases, and introduced an agreement-based proxy for high-confidence automated decisions. The proxy achieved high precision and statistical robustness, while also highlighting the limitations of current models and the broader challenges of automating semantic judgment in FAIR-aligned software metadata across registries and repositories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23500v2</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eva Mart\'in del Pico, Josep Llu\'is Gelp\'i, Salvador Capella-Guti\'errez</dc:creator>
    </item>
    <item>
      <title>RFCAudit: An LLM Agent for Functional Bug Detection in Network Protocols</title>
      <link>https://arxiv.org/abs/2506.00714</link>
      <description>arXiv:2506.00714v2 Announce Type: replace 
Abstract: Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCAudit, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCAudit comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCAudit across six real-world network protocol implementations. RFCAudit identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00714v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingwei Zheng, Chengpeng Wang, Xuwei Liu, Jinyao Guo, Shiwei Feng, Xiangyu Zhang</dc:creator>
    </item>
    <item>
      <title>LogSage: An LLM-Based Framework for CI/CD Failure Detection and Remediation with Industrial Validation</title>
      <link>https://arxiv.org/abs/2506.03691</link>
      <description>arXiv:2506.03691v2 Announce Type: replace 
Abstract: Continuous Integration and Deployment (CI/CD) pipelines are critical to modern software engineering, yet diagnosing and resolving their failures remains complex and labor-intensive. We present LogSage, the first end-to-end LLM-powered framework for root cause analysis (RCA) and automated remediation of CI/CD failures. LogSage employs a token-efficient log preprocessing pipeline to filter noise and extract critical errors, then performs structured diagnostic prompting for accurate RCA. For solution generation, it leverages retrieval-augmented generation (RAG) to reuse historical fixes and invokes automation fixes via LLM tool-calling. On a newly curated benchmark of 367 GitHub CI/CD failures, LogSage achieves over 98\% precision, near-perfect recall, and an F1 improvement of more than 38\% points in the RCA stage, compared with recent LLM-based baselines. In a year-long industrial deployment at ByteDance, it processed over 1.07M executions, with end-to-end precision exceeding 80\%. These results demonstrate that LogSage provides a scalable and practical solution for automating CI/CD failure management in real-world DevOps workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03691v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiyuan Xu, Juntao Luo, Tao Huang, Kaixin Sui, Jie Geng, Qijun Ma, Isami Akasaka, Xiaoxue Shi, Jing Tang, Peng Cai</dc:creator>
    </item>
    <item>
      <title>Refactoring Codebases through Library Design</title>
      <link>https://arxiv.org/abs/2506.11058</link>
      <description>arXiv:2506.11058v3 Announce Type: replace 
Abstract: Maintainable and general software allows developers to build robust applications efficiently, yet achieving these qualities often requires refactoring specialized solutions into reusable components. This challenge becomes particularly relevant as code agents become used to solve isolated one-off programming problems. We investigate code agents' capacity to refactor code in ways that support growth and reusability. We first investigate what makes a good refactoring, finding via simulation results and a human study that Minimum Description Length best correlates with preferable refactorings. We then present both a benchmark and a method for refactoring: MiniCode, a benchmark where multiple files must be refactored into a shared library, and Librarian, a sample-and-rerank method for generating reusable libraries. We compare Librarian to state-of-the-art library generation methods, and study it on real-world code bases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11058v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziga Kovacic, Justin T. Chiu, Celine Lee, Wenting Zhao, Kevin Ellis</dc:creator>
    </item>
    <item>
      <title>Wired for Reuse: Automating Context-Aware Code Adaptation in IDEs via LLM-Based Agent</title>
      <link>https://arxiv.org/abs/2507.01315</link>
      <description>arXiv:2507.01315v2 Announce Type: replace 
Abstract: Copy-paste-modify is a widespread and pragmatic practice in software development, where developers adapt reused code snippets, sourced from platforms such as Stack Overflow, GitHub, or LLM outputs, into their local codebase. A critical yet underexplored aspect of this adaptation is code wiring: the context-aware process of substituting unresolved variables in pasted code with suitable variables or expressions from the surrounding context. Existing solutions either rely on heuristic rules or historical templates, often failing to effectively utilize contextual information, despite studies showing that over half of adaptation cases are context-dependent. In this paper, we introduce WIRL, an LLM-based agent for code wiring framed as a Retrieval-Augmented Generation (RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an orchestration module to identify unresolved variables, retrieve context, and perform context-aware substitutions. To balance efficiency and autonomy, the agent adopts a mixed strategy: deterministic rule-based steps for common patterns, and a state-machine-guided decision process for intelligent exploration. We evaluate WIRL on a carefully curated, high-quality dataset consisting of real-world code adaptation scenarios. Our approach achieves an exact match precision of 91.7% and a recall of 90.0%, outperforming advanced LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively, and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results underscore its practical utility, particularly in contexts with complex variable dependencies or multiple unresolved variables. We believe WIRL paves the way for more intelligent and context-aware developer assistance in modern IDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01315v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taiming Wang, Yanjie Jiang, Chunhao Dong, Yuxia Zhang, Hui Liu</dc:creator>
    </item>
    <item>
      <title>Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval</title>
      <link>https://arxiv.org/abs/2507.09199</link>
      <description>arXiv:2507.09199v3 Announce Type: replace 
Abstract: Issue-commit linking, which connects issues with commits that fix them, is crucial for software maintenance. Existing approaches have shown promise in automatically recovering these links. Evaluations of these techniques assess their ability to identify genuine links from plausible but false links. However, these evaluations overlook the fact that, in reality, when a repository has more commits, the presence of more plausible yet unrelated commits may interfere with the tool in differentiating the correct fix commits. To address this, we propose the Realistic Distribution Setting (RDS) and use it to construct a more realistic evaluation dataset that includes 20 open-source projects. By evaluating tools on this dataset, we observe that the performance of the state-of-the-art deep learning-based approach drops by more than half, while the traditional Information Retrieval method, VSM, outperforms it. Inspired by these observations, we propose EasyLink, which utilizes a vector database as a modern Information Retrieval technique. To address the long-standing problem of the semantic gap between issues and commits, EasyLink leverages a large language model to rerank the commits retrieved from the database. Under our evaluation, EasyLink achieves an average Precision@1 of 75.03\%, improving over the state-of-the-art by over four times. Additionally, this paper provides practical guidelines for advancing research in issue-commit link recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09199v3</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huihui Huang, Ratnadira Widyasari, Ting Zhang, Ivana Clairine Irsan, Jieke Shi, Han Wei Ang, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, Hong Jin Kang, David Lo</dc:creator>
    </item>
    <item>
      <title>Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems</title>
      <link>https://arxiv.org/abs/2508.00244</link>
      <description>arXiv:2508.00244v3 Announce Type: replace 
Abstract: This study compares the impact of adopting object-oriented programming (OOP) or functional programming (FP) on the architectural characteristics of software systems. For that, it examines the design and implementation of a Digital Wallet system developed in Kotlin (for OOP) and Scala (for FP). The comparison is made through a mixed-method approach. The self-ethnographic qualitative analysis provides a side-by-side comparison of both implementations, revealing the perspective of those writing such code. The survey-based quantitative analysis gathers feedback from developers with diverse backgrounds, showing their impressions of those reading this code. Hopefully, these results may be useful for developers seeking to decide which paradigm is best suited for their next project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00244v3</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Briza Mel Dias de Sousa (University of S\~ao Paulo), Renato Cordeiro Ferreira (University of S\~ao Paulo, Jheronimus Academy of Data Science, Technical University of Eindhoven, Tilburg University), Alfredo Goldman (University of S\~ao Paulo)</dc:creator>
    </item>
    <item>
      <title>Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform</title>
      <link>https://arxiv.org/abs/2509.21292</link>
      <description>arXiv:2509.21292v2 Announce Type: replace 
Abstract: Promoting participation on digital platforms such as Brasil Participativo has emerged as a top priority for governments worldwide. However, due to the sheer volume of contributions, much of this engagement goes underutilized, as organizing it presents significant challenges: (1) manual classification is unfeasible at scale; (2) expert involvement is required; and (3) alignment with official taxonomies is necessary. In this paper, we introduce an approach that combines BERTopic with seed words and automatic validation by large language models. Initial results indicate that the generated topics are coherent and institutionally aligned, with minimal human effort. This methodology enables governments to transform large volumes of citizen input into actionable data for public policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21292v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronivaldo Ferreira, Guilherme da Silva, Carla Rocha, Gustavo Pinto</dc:creator>
    </item>
    <item>
      <title>Extracting Conceptual Knowledge to Locate Software Issues</title>
      <link>https://arxiv.org/abs/2509.21427</link>
      <description>arXiv:2509.21427v2 Announce Type: replace 
Abstract: Issue localization, which identifies faulty code elements such as files or functions, is critical for effective bug fixing. While recent LLM-based and LLM-agent-based approaches improve accuracy, they struggle in large-scale repositories due to concern tangling, where relevant logic is buried in large functions, and concern scattering, where related logic is dispersed across files.
  To address these challenges, we propose RepoLens, a novel approach that abstracts and leverages conceptual knowledge from code repositories. RepoLens decomposes fine-grained functionalities and recomposes them into high-level concerns, semantically coherent clusters of functionalities that guide LLMs. It operates in two stages: an offline stage that extracts and enriches conceptual knowledge into a repository-wide knowledge base, and an online stage that retrieves issue-specific terms, clusters and ranks concerns by relevance, and integrates them into localization workflows via minimally intrusive prompt enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies and manual evaluation confirm the effectiveness and reliability of the constructed concerns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21427v2</guid>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Wang, Wenjun Mao, Chong Wang, Zhenhao Zhou, Yicheng Zhou, Wenyun Zhao, Yiling Lou, Xin Peng</dc:creator>
    </item>
    <item>
      <title>HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing</title>
      <link>https://arxiv.org/abs/2509.23835</link>
      <description>arXiv:2509.23835v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used for code generation, but they face critical security risks when applied to practical production due to package hallucinations, in which LLMs recommend non-existent packages. These hallucinations can be exploited in software supply chain attacks, where malicious attackers exploit them to register harmful packages. It is critical to test LLMs for package hallucinations to mitigate package hallucinations and defend against potential attacks. Although researchers have proposed testing frameworks for fact-conflicting hallucinations in natural language generation, there is a lack of research on package hallucinations. To fill this gap, we propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for package hallucinations. HFUZZER adopts fuzzing technology and guides the model to infer a wider range of reasonable information based on phrases, thereby generating enough and diverse coding tasks. Furthermore, HFUZZER extracts phrases from package information or coding tasks to ensure the relevance of phrases and code, thereby improving the relevance of generated tasks and code. We evaluate HFUZZER on multiple LLMs and find that it triggers package hallucinations across all selected models. Compared to the mutational fuzzing framework, HFUZZER identifies 2.60x more unique hallucinated packages and generates more diverse tasks. Additionally, when testing the model GPT-4o, HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that for GPT-4o, LLMs exhibit package hallucinations not only during code generation but also when assisting with environment configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23835v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yukai Zhao, Menghan Wu, Xing Hu, Xin Xia</dc:creator>
    </item>
    <item>
      <title>Generating High-Quality Datasets for Code Editing via Open-Source Language Models</title>
      <link>https://arxiv.org/abs/2509.25203</link>
      <description>arXiv:2509.25203v3 Announce Type: replace 
Abstract: Code editing plays a vital role in software engineering, requiring developers to adjust existing code according to natural language instructions while keeping functionality intact and avoiding unnecessary modifications. However, commit-based datasets commonly used for this task are often noisy, lack diversity, and fail to reflect the style of real-world edit instructions. To address this, we introduce OpenCodeEdit, an open-source pipeline that leverages multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces both concise "lazy" instructions and more detailed "descriptive" ones, and applies filtering based on diffs and topics to guarantee data quality and variety. Using this process, we construct OCEDataFT, a curated dataset of 20K samples. Fine-tuning three advanced base models on OCEDataFT leads to significant performance boosts on the CanItEdit benchmark, with relative pass@1 improvements ranging from 4.50% to 20.79%. Notably, the resulting models achieve performance close to closed-source systems, narrowing the gap to GPT-4 to just 3.54%, without relying on proprietary resources or manual annotation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25203v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zekai Zhang, Mingwei Liu, Zhenxi Chen, Linxi Liang, Yuxuan Chen, Guangsheng Ou, Yanlin Wang, Dan Li, Xin Peng, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>Maven-Lockfile: High Integrity Rebuild of Past Java Releases</title>
      <link>https://arxiv.org/abs/2510.00730</link>
      <description>arXiv:2510.00730v2 Announce Type: replace 
Abstract: Modern software projects depend on many third-party libraries, complicating reproducible and secure builds. Several package managers address this with the generation of a lockfile that freezes dependency versions and can be used to verify the integrity of dependencies. Yet, Maven, one of the most important package managers in the Java ecosystem, lacks native support for a lockfile. We present Maven-Lockfile to generate and update lockfiles, with support for rebuilding projects from past versions. Our lockfiles capture all direct and transitive dependencies with their checksums, enabling high integrity builds. Our evaluation shows that Maven-Lockfile can reproduce builds from historical commits and is able to detect tampered artifacts. With minimal configuration, Maven-Lockfile equips Java projects with modern build integrity and build reproducibility, and fosters future research on software supply chain security in Java.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00730v2</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Larissa Schmid, Elias Lundell, Yogya Gamage, Benoit Baudry, Martin Monperrus</dc:creator>
    </item>
    <item>
      <title>Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study</title>
      <link>https://arxiv.org/abs/2502.02194</link>
      <description>arXiv:2502.02194v3 Announce Type: replace-cross 
Abstract: Integrated Development Environments increasingly implement AI-powered code completion tools (CCTs), which promise to enhance developer efficiency, accuracy, and productivity. However, interaction challenges with CCTs persist, mainly due to mismatches between developers' mental models and the unpredictable behavior of AI-generated suggestions, which is an aspect underexplored in the literature. We conducted an elicitation study with 56 developers using co-design workshops to elicit their mental models when interacting with CCTs. Different important findings that might drive the interaction design with CCTs emerged. For example, developers expressed diverse preferences on when and how code suggestions should be triggered (proactive, manual, hybrid), where and how they are displayed (inline, sidebar, popup, chatbot), as well as the level of detail. It also emerged that developers need to be supported by customization of activation timing, display modality, suggestion granularity, and explanation content, to better fit the CCT to their preferences. To demonstrate the feasibility of these and the other guidelines that emerged during the study, we developed ATHENA, a proof-of-concept CCT that dynamically adapts to developers' coding preferences and environments, ensuring seamless integration into diverse workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02194v3</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijhcs.2025.103648</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Human-Computer Studies (2025), Vol. 205, pag. 103648</arxiv:journal_reference>
      <dc:creator>Giuseppe Desolda, Andrea Esposito, Francesco Greco, Cesare Tucci, Paolo Buono, Antonio Piccinno</dc:creator>
    </item>
    <item>
      <title>DISC: Dynamic Decomposition Improves LLM Inference Scaling</title>
      <link>https://arxiv.org/abs/2502.16706</link>
      <description>arXiv:2502.16706v3 Announce Type: replace-cross 
Abstract: Inference scaling methods for LLMs often rely on decomposing problems into steps (or groups of tokens), followed by sampling and selecting the best next steps. However, these steps and their sizes are often predetermined or manually designed based on domain knowledge. We propose dynamic decomposition, a method that adaptively and automatically partitions solution and reasoning traces into manageable steps during inference. By more effectively allocating compute -- particularly through subdividing challenging steps and prioritizing their sampling -- dynamic decomposition significantly improves inference efficiency. Experiments on benchmarks such as APPS, MATH, and LiveCodeBench demonstrate that dynamic decomposition outperforms static approaches, including token-level, sentence-level, and single-step decompositions, reducing the pass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These findings highlight the potential of dynamic decomposition to improve a wide range of inference scaling techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16706v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Light, Wei Cheng, Benjamin Riviere, Wu Yue, Masafumi Oyamada, Mengdi Wang, Yisong Yue, Santiago Paternain, Haifeng Chen</dc:creator>
    </item>
    <item>
      <title>Optimization under uncertainty: understanding orders and testing programs with specifications</title>
      <link>https://arxiv.org/abs/2503.18561</link>
      <description>arXiv:2503.18561v2 Announce Type: replace-cross 
Abstract: One of the most ubiquitous problems in optimization is that of finding all the elements of a finite set at which a function $f$ attains its minimum (or maximum). When the codomain of $f$ is equipped with a total order, it is easy to specify, implement, and verify generic solutions to this problem. But what if $f$ is affected by uncertainties? What if one seeks values that minimize more than one objective, or if $f$ does not return a single result but a set of possible results, or even a probability distribution? Such situations are common in climate science, economics, and engineering. Developing trustworthy solution methods for optimization under uncertainty requires formulating and answering these questions rigorously, including deciding which order relations to apply in different cases. We show how functional programming can support this task, and apply it to specify and test solution methods for cases where optimization is affected by two conceptually different kinds of uncertainty: value and functorial uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18561v2</guid>
      <category>math.OC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Botta, Patrik Jansson, Tim Richter</dc:creator>
    </item>
    <item>
      <title>Backdoors in Code Summarizers: How Bad Is It?</title>
      <link>https://arxiv.org/abs/2506.01825</link>
      <description>arXiv:2506.01825v2 Announce Type: replace-cross 
Abstract: Code LLMs are increasingly employed in software development. However, studies have shown that they are vulnerable to backdoor attacks: when a trigger (a specific input pattern) appears in the input, the backdoor will be activated and cause the model to generate malicious outputs. Researchers have designed various triggers and demonstrated the feasibility of implanting backdoors by poisoning a fraction of the training data. Some basic conclusions have been made, such as backdoors becoming easier to implant when more training data is modified. However, existing research has not explored other factors influencing backdoor attacks on Code LLMs, such as training batch size, epoch number, and the broader design space for triggers, e.g., trigger length. To bridge this gap, we use code summarization as an example to perform an empirical study that systematically investigates the factors affecting backdoor effectiveness and understands the extent of the threat posed. Three categories of factors are considered: data, model, and inference, revealing previously overlooked findings. We find that the prevailing consensus -- that attacks are ineffective at extremely low poisoning rates -- is incorrect. The absolute number of poisoned samples matters as well. Specifically, poisoning just 20 out of 454K samples (0.004% poisoning rate -- far below the minimum setting of 0.1% in prior studies) successfully implants backdoors! Moreover, the common defense is incapable of removing even a single poisoned sample from it. Additionally, small batch sizes increase the risk of backdoor attacks. We also uncover other critical factors such as trigger types, trigger length, and the rarity of tokens in the triggers, leading to valuable insights for assessing Code LLMs' vulnerability to backdoor attacks. Our study highlights the urgent need for defense mechanisms against extremely low poisoning rate settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01825v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyu Wang, Zhou Yang, Yaniv Harel, David Lo</dc:creator>
    </item>
  </channel>
</rss>
