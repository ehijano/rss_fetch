<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:45:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Energy-convergence trade off for the training of neural networks on bio-inspired hardware</title>
      <link>https://arxiv.org/abs/2509.18121</link>
      <description>arXiv:2509.18121v1 Announce Type: new 
Abstract: The increasing deployment of wearable sensors and implantable devices is shifting AI processing demands to the extreme edge, necessitating ultra-low power for continuous operation. Inspired by the brain, emerging memristive devices promise to accelerate neural network training by eliminating costly data transfers between compute and memory. Though, balancing performance and energy efficiency remains a challenge. We investigate ferroelectric synaptic devices based on HfO2/ZrO2 superlattices and feed their experimentally measured weight updates into hardware-aware neural network simulations. Across pulse widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require more training epochs while still reducing total energy without sacrificing accuracy. Classification accuracy using plain stochastic gradient descent (SGD) is diminished compared to mixed-precision SGD. We analyze the causes and propose a ``symmetry point shifting'' technique, addressing asymmetric updates and restoring accuracy. These results highlight a trade-off among accuracy, convergence speed, and energy use, showing that short-pulse programming with tailored training significantly enhances on-chip learning efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18121v1</guid>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nikhil Garg, Paul Uriarte Vicandi, Yanming Zhang, Alexandre Baigol, Donato Francesco Falcone, Saketh Ram Mamidala, Bert Jan Offrein, Laura B\'egon-Lours</dc:creator>
    </item>
    <item>
      <title>Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron</title>
      <link>https://arxiv.org/abs/2509.18143</link>
      <description>arXiv:2509.18143v1 Announce Type: new 
Abstract: Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits offer the potential for highly energy-efficient Artificial Neural Network (ANN) computation in full custom analog IC designs. The efficient mapping of Artificial Neuron (AN) abstract weights, extracted from the software-trained ANNs, onto physical ACN capacitance values has, however, yet to be fully researched. In this paper, we explore the unexpected hidden complexities, challenges and properties of the mapping, as well as, the ramifications for IC designers in terms accuracy, design and implementation. We propose an optimal, AN to ACN methodology, that promotes smaller chip sizes and improved overall classification accuracy, necessary for successful practical deployment. Using TensorFlow and Larq software frameworks, we train three different ANN networks and map their weights into the energy-efficient DTSC ACN capacitance value domain to demonstrate 100% functional equivalency. Finally, we delve into the impact of weight quantization on ACN performance using novel metrics related to practical IC considerations, such as IC floor space and comparator decision-making efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18143v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Smart, Sachin Maheshwari, Himadri Singh Raghav, Alexander Serb</dc:creator>
    </item>
    <item>
      <title>Lightweight Targeted Estimation of Layout Noise in a Quantum Computer using Quality Indicator Circuits</title>
      <link>https://arxiv.org/abs/2509.18679</link>
      <description>arXiv:2509.18679v1 Announce Type: new 
Abstract: In the current era of quantum computing, minimizing noise is essential for reliably executing quantum circuits on hardware. A key factor affecting circuit performance is the mapping of the abstract quantum circuit to the physical layout of the quantum hardware. This mapping can significantly influence output quality, especially since hardware noise profiles are non-uniform and dynamic. Existing solutions such as Mapomatic and Just-In-Time (JIT) Transpilation attempt to address this issue but are limited either by relying on stale calibration data or high hardware usage, respectively. In this article, we propose Quality Indicator Circuits (QICs) as a lightweight, real-time method for assessing layout quality. A QIC is a small probe circuit that is designed to retain the basic structure of the user's circuit and whose ideal noiseless outcome is known. It is used to evaluate which region of the quantum hardware is best suited for executing the circuit of interest. We first propose a basic method where a QIC is executed for each isomorphic layout to detect the best among them. Although this requires several targeted circuit executions, we show that it still, in most cases, reduces the execution overheads as compared with JIT. To reduce the overheads further, we propose the union of multiple layouts with a Union QIC approach that has no overlaps, and a Distortion Threshold based approach allowing some overlap. Our results show that these outperform Mapomatic in the quality of layout selection while reducing the hardware overhead of JIT by 79 percent on average. This makes our proposed method lightweight and reliable, and a viable technique for layout selection in near-term quantum devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18679v1</guid>
      <category>cs.ET</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shikhar Srivastava (Indian Institute of Science), Ritajit Majumdar (IBM Quantum, IBM India Research Lab), Padmanabha Venkatagiri Seshadri (IBM Research, India), Anupama Ray (IBM Quantum, IBM India Research Lab), Yogesh Simmhan (Indian Institute of Science)</dc:creator>
    </item>
    <item>
      <title>Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks</title>
      <link>https://arxiv.org/abs/2509.18906</link>
      <description>arXiv:2509.18906v1 Announce Type: new 
Abstract: This paper introduces a novel framework for Edge Inference (EI) that bypasses the conventional practice of treating the wireless channel as noise. We utilize Stacked Intelligent Metasurfaces (SIMs) to control wireless propagation, enabling the channel itself to perform over-the-air computation. This eliminates the need for symbol estimation at the receiver, significantly reducing computational and communication overhead. Our approach models the transmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN) where the response of the SIM elements are trainable parameters. To address channel variability, we incorporate a dedicated DNN module responsible for dynamically adjusting transmission power leveraging user location information. Our performance evaluations showcase that the proposed metasurfaces-integrated DNN framework with deep SIM architectures are capable of balancing classification accuracy and power consumption under diverse scenarios, offering significant energy efficiency improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18906v1</guid>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyriakos Stylianopoulos, George C. Alexandropoulos</dc:creator>
    </item>
    <item>
      <title>A Stateless Transparent Voting Machine</title>
      <link>https://arxiv.org/abs/2509.19257</link>
      <description>arXiv:2509.19257v1 Announce Type: new 
Abstract: Transparency and security are essential in our voting system, and voting machines. This paper describes an implementation of a stateless, transparent voting machine (STVM). The STVM is a ballot marking device (BMD) that uses a transparent, interactive printing interface where voters can verify their paper ballots as they fill out the ballot. The transparent interface turns the paper ballot into an interactive interface. In this architecture, stateless describes the machine's boot sequence, where no information is stored or passed forward between reboots. The machine does not have a hard drive. Instead, it boots and runs from read-only media. This STVM design utilizes a Blu-ray Disc ROM (BD-R) to boot the voting software. This system's statelessness and the transparent interactive printing interface make this design the most secure BMD for voting. Unlike other voting methods, this system incorporates high usability, accessibility, and security for all voters. The STVM uses an open-source voting system that has a universally designed interface, making the system accessible for all voters independent of their ability or disability. This system can make voting safer by simultaneously addressing the issue of voters noticing a vote flip and making it difficult for a hack to persist or go unmitigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19257v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juan E. Gilbert, Jean D. Louis</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications</title>
      <link>https://arxiv.org/abs/2509.19185</link>
      <description>arXiv:2509.19185v2 Announce Type: cross 
Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.
  Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19185v2</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohammed Mehedi Hasan, Hao Li, Emad Fallahzadeh, Gopi Krishnan Rajbahadur, Bram Adams, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>Not All Qubits are Utilized Equally</title>
      <link>https://arxiv.org/abs/2509.19241</link>
      <description>arXiv:2509.19241v1 Announce Type: cross 
Abstract: Improvements to the functionality of modern Noisy Intermediate-Scale Quantum (NISQ) computers have coincided with an increase in the total number of physical qubits. Quantum programmers do not commonly design circuits that directly utilize these qubits; instead, they rely on various software suites to algorithmically transpile the circuit into one compatible with a target machine's architecture. For connectivity-constrained superconducting architectures in particular, the chosen syntheses, layout, and routing algorithms used to transpile a circuit drastically change the average utilization patterns of physical qubits. In this paper, we analyze average qubit utilization of a quantum hardware as a means to identify how various transpiler configurations change utilization patterns. We present the preliminary results of this analysis using IBM's 27-qubit Falcon R4 architecture on the Qiskit platform for a subset of qubits, gate distributions, and optimization configurations. We found a persistent bias towards trivial mapping, which can be addressed through increased optimization provided that the overall utilization of an architecture remains below a certain threshold. As a result, some qubits are overused whereas other remain underused. The implication of our study are many-fold namely, (a) potential reduction in calibration overhead by focusing on overused qubits, (b) refining optimization, mapping and routing algorithms to maximize the hardware utilization and (c) pricing underused qubits at low rate to motivate their usage and improve hardware throughput (applicable in multi-tenant environments).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19241v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremie Pope, Swaroop Ghosh</dc:creator>
    </item>
    <item>
      <title>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</title>
      <link>https://arxiv.org/abs/2503.05822</link>
      <description>arXiv:2503.05822v3 Announce Type: replace-cross 
Abstract: The potential of AI researchers in scientific discovery remains largely untapped. Over the past decade, AI for Science (AI4Science) publications in 145 Nature Index journals have increased fifteen-fold, yet they still account for less than 3% of the total publications. Drawing upon the Diffusion of Innovation theory, we project AI4Science's share of total publications to rise from 2.72% in 2024 to approximately 20% by 2050. Achieving this shift requires fully harnessing the potential of AI researchers, as nearly 95% of AI-driven research in these journals is led by experimental scientists. To facilitate this, we propose structured workflows and strategic interventions to position AI researchers at the forefront of scientific discovery. Specifically, we identify three critical pathways: equipping experimental scientists with accessible AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct involvement in scientific discovery, and proactively fostering a thriving AI-driven scientific ecosystem. By addressing these challenges, we aim to empower AI researchers as key drivers of future scientific breakthroughs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05822v3</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengjie Yu, Shuya Liu, Haiyun Yang, Yuping Yan, Maozhen Qu, Yaochu Jin</dc:creator>
    </item>
    <item>
      <title>MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</title>
      <link>https://arxiv.org/abs/2508.17341</link>
      <description>arXiv:2508.17341v2 Announce Type: replace-cross 
Abstract: The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17341v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas</dc:creator>
    </item>
  </channel>
</rss>
