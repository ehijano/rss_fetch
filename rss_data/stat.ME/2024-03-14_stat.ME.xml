<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Mar 2024 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Change Point Detection with Copula Entropy based Two-Sample Test</title>
      <link>https://arxiv.org/abs/2403.07892</link>
      <description>arXiv:2403.07892v1 Announce Type: new 
Abstract: Change point detection is a typical task that aim to find changes in time series and can be tackled with two-sample test. Copula Entropy is a mathematical concept for measuring statistical independence and a two-sample test based on it was introduced recently. In this paper we propose a nonparametric multivariate method for multiple change point detection with the copula entropy-based two-sample test. The single change point detection is first proposed as a group of two-sample tests on every points of time series data and the change point is considered as with the maximum of the test statistics. The multiple change point detection is then proposed by combining the single change point detection method with binary segmentation strategy. We verified the effectiveness of our method and compared it with the other similar methods on the simulated univariate and multivariate data and the Nile data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07892v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Ma</dc:creator>
    </item>
    <item>
      <title>Copula based dependent censoring in cure models</title>
      <link>https://arxiv.org/abs/2403.07963</link>
      <description>arXiv:2403.07963v1 Announce Type: new 
Abstract: In this paper we consider a time-to-event variable $T$ that is subject to random right censoring, and we assume that the censoring time $C$ is stochastically dependent on $T$ and that there is a positive probability of not observing the event. There are various situations in practice where this happens, and appropriate models and methods need to be considered to avoid biased estimators of the survival function or incorrect conclusions in clinical trials. We consider a fully parametric model for the bivariate distribution of $(T,C)$, that takes these features into account. The model depends on a parametric copula (with unknown association parameter) and on parametric marginal distributions for $T$ and $C$. Sufficient conditions are developed under which the model is identified, and an estimation procedure is proposed. In particular, our model allows to identify and estimate the association between $T$ and $C$, even though only the smallest of these variables is observable. The finite sample performance of the estimated parameters is illustrated by means of a thorough simulation study and the analysis of breast cancer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07963v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Morine Delhelle, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>Characterising harmful data sources when constructing multi-fidelity surrogate models</title>
      <link>https://arxiv.org/abs/2403.08118</link>
      <description>arXiv:2403.08118v1 Announce Type: new 
Abstract: Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown to potentially suffer from bias in the benchmarks used in the analysis. In this study, we present a characterisation of harmful low-fidelity sources using only the limited data available to train a surrogate model. We employ recently developed benchmark filtering techniques to conduct a bias-free assessment, providing objectively varied benchmark suites of different sizes for future research. Analysing one of these benchmark suites with the technique known as Instance Space Analysis, we provide an intuitive visualisation of when a low-fidelity source should be used and use this analysis to provide guidelines that can be used in an applied industrial setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08118v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicolau Andr\'es-Thi\'o, Mario Andr\'es Mu\~noz, Kate Smith-Miles</dc:creator>
    </item>
    <item>
      <title>Assessment of background noise properties in time and time-frequency domains in the context of vibration-based local damage detection in real environment</title>
      <link>https://arxiv.org/abs/2403.08359</link>
      <description>arXiv:2403.08359v1 Announce Type: new 
Abstract: Any measurement in condition monitoring applications is associated with disturbing noise. Till now, most of the diagnostic procedures have assumed the Gaussian distribution for the noise. This paper shares a novel perspective to the problem of local damage detection. The acquired vector of observations is considered as an additive mixture of signal of interest (SOI) and noise with strongly non-Gaussian, heavy-tailed properties, that masks the SOI. The distribution properties of the background noise influence the selection of tools used for the signal analysis, particularly for local damage detection. Thus, it is extremely important to recognize and identify possible non-Gaussian behavior of the noise. The problem considered here is more general than the classical goodness-of-fit testing. The paper highlights the important role of variance, as most of the methods for signal analysis are based on the assumption of the finite-variance distribution of the underlying signal. The finite variance assumption is crucial but implicit to most indicators used in condition monitoring, (such as the root-mean-square value, the power spectral density, the kurtosis, the spectral correlation, etc.), in view that infinite variance implies moments higher than 2 are also infinite. The problem is demonstrated based on three popular types of non-Gaussian distributions observed for real vibration signals. We demonstrate how the properties of noise distribution in the time domain may change by its transformations to the time-frequency domain (spectrogram). Additionally, we propose a procedure to check the presence of the infinite-variance of the background noise. Our investigations are illustrated using simulation studies and real vibration signals from various machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08359v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/J.YMSSP.2023.110465</arxiv:DOI>
      <arxiv:journal_reference>Mechanical Systems and Signal Processing, vol. 199, Sep. 2023, p. 110465</arxiv:journal_reference>
      <dc:creator>Katarzyna Skowronek, Tomasz Barszcz, Jerome Antoni, Rados{\l}aw Zimroz, Agnieszka Wy{\l}oma\'nska</dc:creator>
    </item>
    <item>
      <title>Spatial Latent Gaussian Modelling with Change of Support</title>
      <link>https://arxiv.org/abs/2403.08514</link>
      <description>arXiv:2403.08514v1 Announce Type: new 
Abstract: Spatial data are often derived from multiple sources (e.g. satellites, in-situ sensors, survey samples) with different supports, but associated with the same properties of a spatial phenomenon of interest. It is common for predictors to also be measured on different spatial supports than the response variables. Although there is no standard way to work with spatial data with different supports, a prevalent approach used by practitioners has been to use downscaling or interpolation to project all the variables of analysis towards a common support, and then using standard spatial models. The main disadvantage with this approach is that simple interpolation can introduce biases and, more importantly, the uncertainty associated with the change of support is not taken into account in parameter estimation. In this article, we propose a Bayesian spatial latent Gaussian model that can handle data with different rectilinear supports in both the response variable and predictors. Our approach allows to handle changes of support more naturally according to the properties of the spatial stochastic process being used, and to take into account the uncertainty from the change of support in parameter estimation and prediction. We use spatial stochastic processes as linear combinations of basis functions where Gaussian Markov random fields define the weights. Our hierarchical modelling approach can be described by the following steps: (i) define a latent model where response variables and predictors are considered as latent stochastic processes with continuous support, (ii) link the continuous-index set stochastic processes with its projection to the support of the observed data, (iii) link the projected process with the observed data. We show the applicability of our approach by simulation studies and modelling land suitability for improved grassland in Rhondda Cynon Taf, a county borough in Wales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08514v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erick A. Chac\'on-Montalv\'an, Peter M. Atkinson, Christopher Nemeth, Benjamin M. Taylor, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Evaluation and comparison of covariate balance metrics in studies with time-dependent confounding</title>
      <link>https://arxiv.org/abs/2403.08577</link>
      <description>arXiv:2403.08577v1 Announce Type: new 
Abstract: Marginal structural models have been increasingly used by analysts in recent years to account for confounding bias in studies with time-varying treatments. The parameters of these models are often estimated using inverse probability of treatment weighting. To ensure that the estimated weights adequately control confounding, it is possible to check for residual imbalance between treatment groups in the weighted data. Several balance metrics have been developed and compared in the cross-sectional case but have not yet been evaluated and compared in longitudinal studies with time-varying treatment. We have first extended the definition of several balance metrics to the case of a time-varying treatment, with or without censoring. We then compared the performance of these balance metrics in a simulation study by assessing the strength of the association between their estimated level of imbalance and bias. We found that the Mahalanobis balance performed best.Finally, the method was illustrated for estimating the cumulative effect of statins exposure over one year on the risk of cardiovascular disease or death in people aged 65 and over in population-wide administrative data. This illustration confirms the feasibility of employing our proposed metrics in large databases with multiple time-points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08577v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Adenyo, Jason R. Guertin, Bernard Candas, Caroline Sirois, Denis Talbot</dc:creator>
    </item>
    <item>
      <title>Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2403.08630</link>
      <description>arXiv:2403.08630v1 Announce Type: new 
Abstract: This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods. The latter include state-of-the-art transformer-based neural network architectures. Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08630v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy P Nason, James L. Wei</dc:creator>
    </item>
    <item>
      <title>Physics-Guided Inverse Regression for Crop Quality Assessment</title>
      <link>https://arxiv.org/abs/2403.08653</link>
      <description>arXiv:2403.08653v1 Announce Type: new 
Abstract: We present an innovative approach leveraging Physics-Guided Neural Networks (PGNNs) for enhancing agricultural quality assessments. Central to our methodology is the application of physics-guided inverse regression, a technique that significantly improves the model's ability to precisely predict quality metrics of crops. This approach directly addresses the challenges of scalability, speed, and practicality that traditional assessment methods face. By integrating physical principles, notably Fick`s second law of diffusion, into neural network architectures, our developed PGNN model achieves a notable advancement in enhancing both the interpretability and accuracy of assessments. Empirical validation conducted on cucumbers and mushrooms demonstrates the superior capability of our model in outperforming conventional computer vision techniques in postharvest quality evaluation. This underscores our contribution as a scalable and efficient solution to the pressing demands of global food supply challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08653v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Shulman, Assaf Israeli, Yael Botnaro, Ori Margalit, Oved Tamir, Shaul Naschitz, Dan Gamrasni, Ofer M. Shir, Itai Dattner</dc:creator>
    </item>
    <item>
      <title>BayesFLo: Bayesian fault localization of complex software systems</title>
      <link>https://arxiv.org/abs/2403.08079</link>
      <description>arXiv:2403.08079v1 Announce Type: cross 
Abstract: Software testing is essential for the reliable development of complex software systems. A key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. Existing fault localization methods, however, are largely deterministic, and thus do not provide a principled approach for assessing probabilistic risk of potential root causes, or for integrating domain and/or structural knowledge from test engineers. To address this, we propose a novel Bayesian fault localization framework called BayesFLo, which leverages a flexible Bayesian model on potential root cause combinations. A key feature of BayesFLo is its integration of the principles of combination hierarchy and heredity, which capture the structured nature of failure-inducing combinations. A critical challenge, however, is the sheer number of potential root cause scenarios to consider, which renders the computation of posterior root cause probabilities infeasible even for small software systems. We thus develop new algorithms for efficient computation of such probabilities, leveraging recent tools from integer programming and graph representations. We then demonstrate the effectiveness of BayesFLo over state-of-the-art fault localization methods, in a suite of numerical experiments and in two motivating case studies on the JMP XGBoost interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08079v1</guid>
      <category>cs.SE</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Ji, Simon Mak, Ryan Lekivetz, Joseph Morgan</dc:creator>
    </item>
    <item>
      <title>Causal Interpretation of Estimands Defined by Exposure Mappings</title>
      <link>https://arxiv.org/abs/2403.08183</link>
      <description>arXiv:2403.08183v1 Announce Type: cross 
Abstract: In settings with interference, it is common to utilize estimands defined by exposure mappings to summarize the impact of variation in treatment assignments local to the ego. This paper studies their causal interpretation under weak restrictions on interference. We demonstrate that the estimands can exhibit unpalatable sign reversals under conventional identification conditions. This motivates the formulation of sign preservation criteria for causal interpretability. To satisfy preferred criteria, it is necessary to impose restrictions on interference, either in potential outcomes or selection into treatment. We provide sufficient conditions and show that they are satisfied by a nonparametric model allowing for a complex form of interference in both the outcome and selection stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08183v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung</dc:creator>
    </item>
    <item>
      <title>Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables</title>
      <link>https://arxiv.org/abs/2403.08628</link>
      <description>arXiv:2403.08628v1 Announce Type: cross 
Abstract: This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08628v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Barreto, Olivier Marchal, Julyan Arbel</dc:creator>
    </item>
    <item>
      <title>Flexible control of the median of the false discovery proportion</title>
      <link>https://arxiv.org/abs/2208.11570</link>
      <description>arXiv:2208.11570v4 Announce Type: replace 
Abstract: We introduce a multiple testing procedure that controls the median of the proportion of false discoveries (FDP) in a flexible way. The procedure only requires a vector of p-values as input and is comparable to the Benjamini-Hochberg method, which controls the mean of the FDP. Our method allows freely choosing one or several values of alpha after seeing the data -- unlike Benjamini-Hochberg, which can be very liberal when alpha is chosen post hoc. We prove these claims and illustrate them with simulations. Our procedure is inspired by a popular estimator of the total number of true hypotheses. We adapt this estimator to provide simultaneously median unbiased estimators of the FDP, valid for finite samples. This simultaneity allows for the claimed flexibility. Our approach does not assume independence. The time complexity of our method is linear in the number of hypotheses, after sorting the p-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11570v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik, Aldo Solari, Jelle J Goeman</dc:creator>
    </item>
    <item>
      <title>Personalized Biopsy Schedules Using an Interval-censored Cause-specific Joint Model</title>
      <link>https://arxiv.org/abs/2209.00105</link>
      <description>arXiv:2209.00105v5 Announce Type: replace 
Abstract: Active surveillance (AS), where biopsies are conducted to detect cancer progression, has been acknowledged as an efficient way to reduce the overtreatment of prostate cancer. Most AS cohorts use fixed biopsy schedules for all patients. However, the ideal test frequency remains unknown, and the routine use of such invasive tests burdens the patients. An emerging idea is to generate personalized biopsy schedules based on each patient's progression-specific risk. To achieve that, we propose the interval-censored cause-specific joint model (ICJM), which models the impact of longitudinal biomarkers on cancer progression while considering the competing event of early treatment initiation. The underlying likelihood function incorporates the interval-censoring of cancer progression, the competing risk of treatment, and the uncertainty about whether cancer progression occurred since the last biopsy in patients that are right-censored or experience the competing event. The model can produce patient-specific risk profiles until a horizon time. If the risk exceeds a certain threshold, a biopsy is conducted. The optimal threshold can be chosen by balancing two indicators of the biopsy schedules: the expected number of biopsies and expected delay in detection of cancer progression. A simulation study showed that our personalized schedules could considerably reduce the number of biopsies per patient by 34%-54% compared to the fixed schedules, though at the cost of a slightly longer detection delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00105v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Eveline A. M. Heijnsdijk, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>On the minimum information checkerboard copulas under fixed Kendall's rank correlation</title>
      <link>https://arxiv.org/abs/2306.01604</link>
      <description>arXiv:2306.01604v2 Announce Type: replace 
Abstract: Copulas have gained widespread popularity as statistical models to represent dependence structures between multiple variables in various applications. The minimum information copula, given a finite number of constraints in advance, emerges as the copula closest to the uniform copula when measured in Kullback-Leibler divergence. In prior research, the focus has predominantly been on constraints related to expectations on moments, including Spearman's $\rho$. This approach allows for obtaining the copula through convex programming. However, the existing framework for minimum information copulas does not encompass non-linear constraints such as Kendall's $\tau$. To address this limitation, we introduce MICK, a novel minimum information copula under fixed Kendall's $\tau$. We first characterize MICK by its local dependence property. Despite being defined as the solution to a non-convex optimization problem, we demonstrate that the uniqueness of this copula is guaranteed when the correlation is sufficiently small. Additionally, we provide numerical insights into applying MICK to real financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01604v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Issey Sukeda, Tomonari Sei</dc:creator>
    </item>
    <item>
      <title>Sliced Wasserstein Regression</title>
      <link>https://arxiv.org/abs/2306.10601</link>
      <description>arXiv:2306.10601v2 Announce Type: replace 
Abstract: While statistical modeling of distributional data has gained increased attention, the case of multivariate distributions has been somewhat neglected despite its relevance in various applications. This is because the Wasserstein distance, commonly used in distributional data analysis, poses challenges for multivariate distributions. A promising alternative is the sliced Wasserstein distance, which offers a computationally simpler solution. We propose distributional regression models with multivariate distributions as responses paired with Euclidean vector predictors. The foundation of our methodology is a slicing transform from the multivariate distribution space to the sliced distribution space for which we establish a theoretical framework, with the Radon transform as a prominent example. We introduce and study the asymptotic properties of sample-based estimators for two regression approaches, one based on utilizing the sliced Wasserstein distance directly in the multivariate distribution space, and a second approach based on a new slice-wise distance, employing a univariate distribution regression for each slice. Both global and local Fr\'echet regression methods are deployed for these approaches and illustrated in simulations and through applications. These include joint distributions of excess winter death rates and winter temperature anomalies in European countries as a function of base winter temperature and also data from finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10601v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Chen, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Survey calibration for causal inference: a simple method to balance covariate distributions</title>
      <link>https://arxiv.org/abs/2310.11969</link>
      <description>arXiv:2310.11969v2 Announce Type: replace 
Abstract: This paper proposes a~simple, yet powerful, method for balancing distributions of covariates for causal inference based on observational studies. The method makes it possible to balance an arbitrary number of quantiles (e.g., medians, quartiles, or deciles) together with means if necessary. The proposed approach is based on the theory of calibration estimators (Deville and S\"arndal 1992), in particular, calibration estimators for quantiles, proposed by Harms and Duchesne (2006). The method does not require numerical integration, kernel density estimation or assumptions about the distributions. Valid estimates can be obtained by drawing on existing asymptotic theory. An~illustrative example of the proposed approach is presented for the entropy balancing method and the covariate balancing propensity score method. Results of a~simulation study indicate that the method efficiently estimates average treatment effects on the treated (ATT), the average treatment effect (ATE), the quantile treatment effect on the treated (QTT) and the quantile treatment effect (QTE), especially in the presence of non-linearity and mis-specification of the models. The proposed approach can be further generalized to other designs (e.g. multi-category, continuous) or methods (e.g. synthetic control method). An open source software implementing proposed methods is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11969v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Minimum Covariance Determinant: Spectral Embedding and Subset Size Determination</title>
      <link>https://arxiv.org/abs/2401.14359</link>
      <description>arXiv:2401.14359v2 Announce Type: replace 
Abstract: This paper introduces several enhancements to the minimum covariance determinant method of outlier detection and robust estimation of means and covariances. We leverage the principal component transform to achieve dimension reduction and ultimately better analyses. Our best subset selection algorithm strategically combines statistical depth and concentration steps. To ascertain the appropriate subset size and number of principal components, we introduce a bootstrap procedure that estimates the instability of the best subset algorithm. The parameter combination exhibiting minimal instability proves ideal for the purposes of outlier detection and robust estimation. Rigorous benchmarking against prominent MCD variants showcases our approach's superior statistical performance and computational speed in high dimensions. Application to a fruit spectra data set and a cancer genomics data set illustrates our claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14359v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Heng, Hui Shen, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis for Over-parameterized Linear Model without Sparsity</title>
      <link>https://arxiv.org/abs/2305.15754</link>
      <description>arXiv:2305.15754v2 Announce Type: replace-cross 
Abstract: In the field of high-dimensional Bayesian statistics, a plethora of methodologies have been developed, including various prior distributions that result in parameter sparsity. However, such priors exhibit limitations in handling the spectral eigenvector structure of data, rendering estimations less effective for analyzing the over-parameterized models (high-dimensional linear models that do not assume sparsity) developed in recent years. This study introduces a Bayesian approach that employs a prior distribution dependent on the eigenvectors of data covariance matrices without inducing parameter sparsity. We also provide contraction rates of the derived posterior estimation and develop a truncated Gaussian approximation of the posterior distribution. The former demonstrates the efficiency of posterior estimation, whereas the latter facilitates the uncertainty quantification of parameters via a Bernstein--von Mises-type approach. These findings suggest that Bayesian methods capable of handling data spectra and estimating non-sparse high-dimensional parameters are feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15754v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Increasing dimension asymptotics for two-way crossed mixed effect models</title>
      <link>https://arxiv.org/abs/2401.06446</link>
      <description>arXiv:2401.06446v2 Announce Type: replace-cross 
Abstract: This paper presents asymptotic results for the maximum likelihood and restricted maximum likelihood (REML) estimators within a two-way crossed mixed effect model as the sizes of the rows, columns, and cells tend to infinity. Under very mild conditions which do not require the assumption of normality, the estimators are proven to be asymptotically normal, possessing a structured covariance matrix. The growth rate for the number of rows, columns, and cells is unrestricted, whether considered pairwise or collectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06446v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Lyu, S. A. Sisson, A. H. Welsh</dc:creator>
    </item>
  </channel>
</rss>
