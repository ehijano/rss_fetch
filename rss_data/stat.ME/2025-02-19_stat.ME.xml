<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 02:38:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inference for Log-Gaussian Cox Point Processes using Bayesian Deep Learning: Application to Human Oral Microbiome Image Data</title>
      <link>https://arxiv.org/abs/2502.12334</link>
      <description>arXiv:2502.12334v1 Announce Type: new 
Abstract: It is common in nature to see aggregation of objects in space. Exploring the mechanism associated with the locations of such clustered observations can be essential to understanding the phenomenon, such as the source of spatial heterogeneity, or comparison to other event generating processes in the same domain. Log-Gaussian Cox processes (LGCPs) represent an important class of models for quantifying aggregation in a spatial point pattern. However, implementing likelihood-based Bayesian inference for such models presents many computational challenges, particularly in high dimensions. In this paper, we propose a novel likelihood-free inference approach for LGCPs using the recently developed BayesFlow approach, where invertible neural networks are employed to approximate the posterior distribution of the parameters of interest. BayesFlow is a neural simulation-based method based on "amortized" posterior estimation. That is, after an initial training procedure, fast feed-forward operations allow rapid posterior inference for any data within the same model family. Comprehensive numerical studies validate the reliability of the framework and show that BayesFlow achieves substantial computational gain in repeated application, especially for two-dimensional LGCPs. We demonstrate the utility and robustness of the method by applying it to two distinct oral microbial biofilm images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12334v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuwan Wang, Christopher K. Wikle, Athanasios C. Micheas, Jessica L. Mark Welch, Jacqueline R. Starr, Kyu Ha Lee</dc:creator>
    </item>
    <item>
      <title>Modelling the impact of Multi Cancer Early Detection tests: a review of natural history of disease models</title>
      <link>https://arxiv.org/abs/2502.12351</link>
      <description>arXiv:2502.12351v1 Announce Type: new 
Abstract: Introduction: The potential for multi-cancer early detection (MCED) tests to detect cancer at earlier stages is currently being evaluated in screening clinical trials. Once trial evidence becomes available, modelling will be necessary to predict impacts on final outcomes (benefits and harms), account for heterogeneity in determining clinical and cost-effectiveness, and explore alternative screening programme specifications. The natural history of disease (NHD) component of a MCED model will use statistical, mathematical or calibration methods. Methods: Modelling approaches for MCED screening that include an NHD component were identified from the literature, reviewed and critically appraised. Purposively selected (non-MCED) cancer screening models were also reviewed. The appraisal focussed on the scope, data sources, evaluation approaches and the structure and parameterisation of the models. Results: Five different MCED NHD models were identified and reviewed, alongside four additional (non-MCED) models. The critical appraisal highlighted several features of this literature. In the absence of trial evidence, MCED effects are based on predictions derived from test accuracy. These predictions rely on simplifying assumptions with unknown impacts, such as the stage-shift assumption used to estimate mortality impacts from predicted stage-shifts. None of the MCED models fully characterised uncertainty in the NHD or examined uncertainty in the stage-shift assumption. Conclusion: MCED technologies are developing rapidly, and large and costly clinical studies are being designed and implemented across the globe. Currently there is no modelling approach that can integrate clinical study evidence and therefore, in support of policy, it is important that similar efforts are made in the development of MCED models that make best use of the available data on benefits and harms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12351v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>O Mandrik, S Whyte, N Kunst, A Rayner, M Harden, S Dias, K Payne, S Palmer, MO Soares</dc:creator>
    </item>
    <item>
      <title>Time Series Treatment Effects Analysis with Always-Missing Controls</title>
      <link>https://arxiv.org/abs/2502.12393</link>
      <description>arXiv:2502.12393v1 Announce Type: new 
Abstract: Estimating treatment effects in time series data presents a significant challenge, especially when the control group is always unobservable. For example, in analyzing the effects of Christmas on retail sales, we lack direct observation of what would have occurred in late December without the Christmas impact. To address this, we try to recover the control group in the event period while accounting for confounders and temporal dependencies. Experimental results on the M5 Walmart retail sales data demonstrate robust estimation of the potential outcome of the control group as well as accurate predicted holiday effect. Furthermore, we provided theoretical guarantees for the estimated treatment effect, proving its consistency and asymptotic normality. The proposed methodology is applicable not only to this always-missing control scenario but also in other conventional time series causal inference settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12393v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Shu, Qiyu Han, George Chen, Xihao Cao, Kangming Luo, Dan Pallotta, Shivam Agrawal, Yuping Lu, Xiaoyu Zhang, Jawad Mansoor, Jyoti Anand</dc:creator>
    </item>
    <item>
      <title>On the peak height distribution of non-stationary Gaussian random fields: 1D general covariance and scale space</title>
      <link>https://arxiv.org/abs/2502.12452</link>
      <description>arXiv:2502.12452v1 Announce Type: new 
Abstract: We study the peak height distribution of certain non-stationary Gaussian random fields. The explicit peak height distribution of smooth, non-stationary Gaussian processes in 1D with general covariance is derived. The formula is determined by two parameters, each of which has a clear statistical meaning. For multidimensional non-stationary Gaussian random fields, we generalize these results to the setting of scale space fields, which play an important role in peak detection by helping to handle peaks of different spatial extents. We demonstrate that these properties not only offer a better interpretation of the scale space field but also simplify the computation of the peak height distribution. Finally, two efficient numerical algorithms are proposed as a general solution for computing the peak height distribution of smooth multidimensional Gaussian random fields in applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12452v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhao, Dan Cheng, Samuel Davenport, Armin Schwartzman</dc:creator>
    </item>
    <item>
      <title>A Primal Dual Active Set with Continuation Algorithm for $\ell_0$-Penalized High-dimensional Accelerated Failure Time Model</title>
      <link>https://arxiv.org/abs/2502.12621</link>
      <description>arXiv:2502.12621v1 Announce Type: new 
Abstract: The accelerated failure time model has garnered attention due to its intuitive linear regression interpretation and has been successfully applied in fields such as biostatistics, clinical medicine, economics, and social sciences. This paper considers a weighted least squares estimation method with an $\ell_0$-penalty based on right-censored data in a high-dimensional setting. For practical implementation, we adopt an efficient primal dual active set algorithm and utilize a continuous strategy to select the appropriate regularization parameter. By employing the mutual incoherence property and restricted isometry property of the covariate matrix, we perform an error analysis for the estimated variables in the active set during the iteration process. Furthermore, we identify a distinctive monotonicity in the active set and show that the algorithm terminates at the oracle solution in a finite number of steps. Finally, we perform extensive numerical experiments using both simulated data and real breast cancer datasets to assess the performance benefits of our method in comparison to other existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12621v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peili Li, Ruoying Hu, Yanyun Ding, Yunhai Xiao</dc:creator>
    </item>
    <item>
      <title>K-n\'ucleo: Una herramienta para detectar la estructura conceptual de los campos de investigaci\'on. El caso pr\'actico de la Altmetr\'ia</title>
      <link>https://arxiv.org/abs/2502.12682</link>
      <description>arXiv:2502.12682v1 Announce Type: new 
Abstract: In Social Network Analysis (SNA), k-core decomposition is used to detect hierarchical shells in networks. The application of the K-core decomposition to a network of keywords allows us to represent the conceptual structure of a research field. The objective of this work was to propose the application of k-core decomposition to show the evolution of the conceptual structure of the Altmetrics research field. The methodology was developed in several phases: data collection, keyword selection, elaboration of a keyword co-occurrence matrix, generation of a keyword network, k-core decomposition and visualization of the hierarchical structure. The result was the detection of five differentiated shells. A core shell with basic, densely interconnected concepts that formed the knowledge base of the field. An intermediate shell with mediating concepts that showed the evolution of knowledge in the field. A lateral shell with concepts that indicated the specialization of the research field. A border shell with peripheral and isolated concepts, which represented the conceptual fronts in development. In conclusion, the hierarchical decomposition of the keyword network achieved a deeper understanding of the conceptual structure of the research field</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12682v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5565/rev/redes.1079</arxiv:DOI>
      <arxiv:journal_reference>Revista Hispana para el An\'alisis de Redes Sociales, Vol. 36,#2,(2025), 148-162</arxiv:journal_reference>
      <dc:creator>Carmen G\'alvez</dc:creator>
    </item>
    <item>
      <title>Dependence and Uncertainty: Information Measures using Tsallis Entropy</title>
      <link>https://arxiv.org/abs/2502.12779</link>
      <description>arXiv:2502.12779v1 Announce Type: new 
Abstract: In multivariate analysis, uncertainty arises from two sources: the marginal distributions of the variables and their dependence structure. Quantifying the dependence structure is crucial, as it provides valuable insights into the relationships among components of a random vector. Copula functions effectively capture this dependence structure independent of marginals, making copula-based information measures highly significant. However, existing copula-based information measures, such as entropy, divergence, and mutual information, rely on copula densities, which may not exist in many scenarios, limiting their applicability. Recently, to address this issue, Arshad et al. (2024) introduced cumulative copula-based measures using Shannon entropy. In this paper, we extend this framework by using Tsallis entropy, a non-additive entropy that provides greater flexibility for quantifying uncertainties. We propose cumulative copula Tsallis entropy, derive its properties and bounds, and illustrate its utility through examples. We further develop a non-parametric version of the measure and validate it using coupled periodic and chaotic maps. Additionally, we extend Kerridge's inaccuracy measure and Kullback-Leibler (KL) divergence to the cumulative copula framework. Using the relationship between KL divergence and mutual information, we propose a new cumulative mutual information (CMI) measure, which outperform the limitations of density-based mutual information. Furthermore, we introduce a test procedure for testing the mutual independence among random variables using CMI measure. Finally, we illustrate the potential of the proposed CMI measure as an economic indicator through real bivariate financial time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12779v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Swaroop Georgy Zachariah, Mohd. Arshad, Ashok Kumar Pathak</dc:creator>
    </item>
    <item>
      <title>Identifying rapid changes in the hemodynamic response in event-related functional magnetic resonance imaging</title>
      <link>https://arxiv.org/abs/2502.12989</link>
      <description>arXiv:2502.12989v1 Announce Type: new 
Abstract: The hemodynamic response (HR) in event-related functional magnetic resonance imaging is typically assumed to be stationary. While there are some approaches in the literature to model nonstationary HRs, few focus on rapid changes. In this work, we propose two procedures to investigate rapid changes in the HR. Both procedures make inference on the existence of rapid changes for multi-subject data. We allow the change point locations to vary between subjects, conditions and brain regions. The first procedure utilizes available information about the change point locations to compare multiple shape parameters of the HR over time. In the second procedure, the change point locations are determined for each subject separately. To account for the estimation of the change point locations, we propose the notion of post selection variance. The power of the proposed procedures is assessed in simulation studies. We apply the procedure for pre-specified change point locations to data from a category learning experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12989v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Friederike Preusse, Thorsten Dickhaus, Andr\'e Brechmann</dc:creator>
    </item>
    <item>
      <title>A robust estimation and variable selection approach for sparse partially linear additive models</title>
      <link>https://arxiv.org/abs/2502.13126</link>
      <description>arXiv:2502.13126v1 Announce Type: new 
Abstract: In partially linear additive models the response variable is modelled with a linear component on a subset of covariates and an additive component in which the rest of the covariates enter to the model as a sum of univariate unknown functions. This structure is more flexible than the usual full linear or full nonparametric regression models, avoids the 'curse of dimensionality', is easily interpretable and allows the user to include discrete or categorical variables in the linear part. On the other hand, in practice, the user incorporates all the available variables in the model no matter how they would impact on the response variable. For this reason, variable selection plays an important role since including covariates that has a null impact on the responses will reduce the prediction capability of the model. As in other settings, outliers in the data may harm estimations based on strong assumptions, such as normality of the response variable, leading to conclusions that are not representative of the data set. In this work, we propose a family of robust estimators that estimate and select variables from both the linear and the additive part of the model simultaneously. This family considers an adaptive procedure on a general class of penalties in the regularization part of the objetive function that defines the estimators. We study the behaviour of the proposal againts its least-squares counterpart under simulations and show the advantages of its use on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13126v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandra Mercedes Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Stability Bounds for Smooth Optimal Transport Maps and their Statistical Implications</title>
      <link>https://arxiv.org/abs/2502.12326</link>
      <description>arXiv:2502.12326v1 Announce Type: cross 
Abstract: We study estimators of the optimal transport (OT) map between two probability distributions. We focus on plugin estimators derived from the OT map between estimates of the underlying distributions. We develop novel stability bounds for OT maps which generalize those in past work, and allow us to reduce the problem of optimally estimating the transport map to that of optimally estimating densities in the Wasserstein distance. In contrast, past work provided a partial connection between these problems and relied on regularity theory for the Monge-Ampere equation to bridge the gap, a step which required unnatural assumptions to obtain sharp guarantees. We also provide some new insights into the connections between stability bounds which arise in the analysis of plugin estimators and growth bounds for the semi-dual functional which arise in the analysis of Brenier potential-based estimators of the transport map. We illustrate the applicability of our new stability bounds by revisiting the smooth setting studied by Manole et al., analyzing two of their estimators under more general conditions. Critically, our bounds do not require smoothness or boundedness assumptions on the underlying measures. As an illustrative application, we develop and analyze a novel tuning parameter-free estimator for the OT map between two strongly log-concave distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12326v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivaraman Balakrishnan, Tudor Manole</dc:creator>
    </item>
    <item>
      <title>A Pathwise Coordinate Descent Algorithm for LASSO Penalized Quantile Regression</title>
      <link>https://arxiv.org/abs/2502.12363</link>
      <description>arXiv:2502.12363v1 Announce Type: cross 
Abstract: $\ell_1$ penalized quantile regression is used in many fields as an alternative to penalized least squares regressions for high-dimensional data analysis. Existing algorithms for penalized quantile regression either use linear programming, which does not scale well in high dimension, or an approximate coordinate descent (CD) which does not solve for exact coordinatewise minimum of the nonsmooth loss function. Further, neither approaches build fast, pathwise algorithms commonly used in high-dimensional statistics to leverage sparsity structure of the problem in large-scale data sets. To avoid the computational challenges associated with the nonsmooth quantile loss, some recent works have even advocated using smooth approximations to the exact problem. In this work, we develop a fast, pathwise coordinate descent algorithm to compute exact $\ell_1$ penalized quantile regression estimates for high-dimensional data. We derive an easy-to-compute exact solution for the coordinatewise nonsmooth loss minimization, which, to the best of our knowledge, has not been reported in the literature. We also employ a random perturbation strategy to help the algorithm avoid getting stuck along the regularization path. In simulated data sets, we show that our algorithm runs substantially faster than existing alternatives based on approximate CD and linear program, while retaining the same level of estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12363v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanghee Kim, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Federated Variational Inference for Bayesian Mixture Models</title>
      <link>https://arxiv.org/abs/2502.12684</link>
      <description>arXiv:2502.12684v1 Announce Type: cross 
Abstract: We present a federated learning approach for Bayesian model-based clustering of large-scale binary and categorical datasets. We introduce a principled 'divide and conquer' inference procedure using variational inference with local merge and delete moves within batches of the data in parallel, followed by 'global' merge moves across batches to find global clustering structures. We show that these merge moves require only summaries of the data in each batch, enabling federated learning across local nodes without requiring the full dataset to be shared. Empirical results on simulated and benchmark datasets demonstrate that our method performs well in comparison to existing clustering algorithms. We validate the practical utility of the method by applying it to large scale electronic health record (EHR) data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12684v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jackie Rao, Francesca L. Crowe, Tom Marshall, Sylvia Richardson, Paul D. W. Kirk</dc:creator>
    </item>
    <item>
      <title>Green LIME: Improving AI Explainability through Design of Experiments</title>
      <link>https://arxiv.org/abs/2502.12753</link>
      <description>arXiv:2502.12753v1 Announce Type: cross 
Abstract: In artificial intelligence (AI), the complexity of many models and processes often surpasses human interpretability, making it challenging to understand why a specific prediction is made. This lack of transparency is particularly problematic in critical fields like healthcare, where trust in a model's predictions is paramount. As a result, the explainability of machine learning (ML) and other complex models has become a key area of focus. Efforts to improve model interpretability often involve experimenting with AI systems and approximating their behavior through simpler mechanisms. However, these procedures can be resource-intensive. Optimal design of experiments, which seeks to maximize the information obtained from a limited number of observations, offers promising methods for improving the efficiency of these explainability techniques.
  To demonstrate this potential, we explore Local Interpretable Model-agnostic Explanations (LIME), a widely used method introduced by Ribeiro, Singh, and Guestrin, 2016. LIME provides explanations by generating new data points near the instance of interest and passing them through the model. While effective, this process can be computationally expensive, especially when predictions are costly or require many samples. LIME is highly versatile and can be applied to a wide range of models and datasets. In this work, we focus on models involving tabular data, regression tasks, and linear models as interpretable local approximations.
  By utilizing optimal design of experiments' techniques, we reduce the number of function evaluations of the complex model, thereby reducing the computational effort of LIME by a significant amount. We consider this modified version of LIME to be energy-efficient or "green".</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12753v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Stadler, Werner G. M\"uller, Radoslav Harman</dc:creator>
    </item>
    <item>
      <title>On Identification of Optimal Dynamic Treatment Regimes with Proxies of Hidden Confounders</title>
      <link>https://arxiv.org/abs/2402.14942</link>
      <description>arXiv:2402.14942v2 Announce Type: replace 
Abstract: We consider identification of optimal dynamic treatment regimes in a setting where time-varying treatments are confounded by hidden time-varying confounders, but proxy variables of the unmeasured confounders are available. We show that, with two independent proxy variables at each time point that are sufficiently relevant for the hidden confounders, identification of the joint distribution of counterfactuals is possible, thereby facilitating identification of an optimal treatment regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14942v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Zhang, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Analysis of cohort stepped wedge cluster-randomized trials with non-ignorable dropout via joint modeling</title>
      <link>https://arxiv.org/abs/2404.14840</link>
      <description>arXiv:2404.14840v3 Announce Type: replace 
Abstract: Stepped wedge cluster-randomized trial (CRTs) designs randomize clusters of individuals to intervention sequences, ensuring that every cluster eventually transitions from a control period to receive the intervention under study by the end of the study period. The analysis of stepped wedge CRTs is usually more complex than parallel-arm CRTs due to more complex intra-cluster correlation structures. A further challenge in the analysis of closed-cohort stepped wedge CRTs, which follow groups of individuals enrolled in each period longitudinally, is the occurrence of dropout. This is particularly problematic in studies of individuals at high risk for mortality, which causes non-ignorable missing outcomes. If not appropriately addressed, missing outcomes from death will erode statistical power, at best, and bias treatment effect estimates, at worst. Joint longitudinal-survival models can accommodate informative dropout and missingness patterns in longitudinal studies. Specifically, within the joint longitudinal-survival modeling framework, one directly models the dropout process via a time-to-event submodel together with the longitudinal outcome of interest. The two submodels are then linked using a variety of possible association structures. This work extends linear mixed-effects models by jointly modeling the dropout process to accommodate informative missing outcome data in closed-cohort stepped wedge CRTs. We focus on constant intervention and general time-on-treatment effect parametrizations for the longitudinal submodel and study the performance of the proposed methodology using Monte Carlo simulation under several data-generating scenarios. We illustrate the methodology in practice by reanalyzing data from the 'Frail Older Adults: Care in Transition' (ACT) trial, a stepped wedge CRT of a multifaceted geriatric care model versus usual care in 35 primary care practices in the Netherlands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14840v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.10347</arxiv:DOI>
      <dc:creator>Alessandro Gasparini, Michael J. Crowther, Emiel O. Hoogendijk, Fan Li, Michael O. Harhay</dc:creator>
    </item>
    <item>
      <title>Combining Priors with Experience: Confidence Calibration Based on Binomial Process Modeling</title>
      <link>https://arxiv.org/abs/2412.10658</link>
      <description>arXiv:2412.10658v3 Announce Type: replace 
Abstract: Confidence calibration of classification models is a technique to estimate the true posterior probability of the predicted class, which is critical for ensuring reliable decision-making in practical applications. Existing confidence calibration methods mostly use statistical techniques to estimate the calibration curve from data or fit a user-defined calibration function, but often overlook fully mining and utilizing the prior distribution behind the calibration curve. However, a well-informed prior distribution can provide valuable insights beyond the empirical data under the limited data or low-density regions of confidence scores. To fill this gap, this paper proposes a new method that integrates the prior distribution behind the calibration curve with empirical data to estimate a continuous calibration curve, which is realized by modeling the sampling process of calibration data as a binomial process and maximizing the likelihood function of the binomial process. We prove that the calibration curve estimating method is Lipschitz continuous with respect to data distribution and requires a sample size of $3/B$ of that required for histogram binning, where $B$ represents the number of bins. Also, a new calibration metric ($TCE_{bpm}$), which leverages the estimated calibration curve to estimate the true calibration error (TCE), is designed. $TCE_{bpm}$ is proven to be a consistent calibration measure. Furthermore, realistic calibration datasets can be generated by the binomial process modeling from a preset true calibration curve and confidence score distribution, which can serve as a benchmark to measure and compare the discrepancy between existing calibration metrics and the true calibration error. The effectiveness of our calibration method and metric are verified in real-world and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10658v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinzong Dong, Zhaohui Jiang, Dong Pan, Haoyang Yu</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control via Frequentist-assisted Horseshoe</title>
      <link>https://arxiv.org/abs/2502.05460</link>
      <description>arXiv:2502.05460v2 Announce Type: replace 
Abstract: The horseshoe prior, a widely used handy alternative to the spike-and-slab prior, has proven to be an exceptional default global-local shrinkage prior in Bayesian inference and machine learning. However, designing tests with frequentist false discovery rate (FDR) control using the horseshoe prior or the general class of global-local shrinkage priors remains an open problem. In this paper, we propose a frequentist-assisted horseshoe procedure that not only resolves this long-standing FDR control issue for the high dimensional normal means testing problem but also exhibits satisfactory finite-sample FDR control under any desired nominal level for both large-scale multiple independent and correlated tests. We carry out the frequentist-assisted horseshoe procedure in an easy and intuitive way by using the minimax estimator of the global parameter of the horseshoe prior while maintaining the remaining full Bayes vanilla horseshoe structure. The results of both intensive simulations under different sparsity levels, and real-world data demonstrate that the frequentist-assisted horseshoe procedure consistently achieves robust finite-sample FDR control. Existing frequentist or Bayesian FDR control procedures can lose finite-sample FDR control in a variety of common sparse cases. Based on the intimate relationship between the minimax estimation and the level of FDR control discovered in this work, we point out potential generalizations to achieve FDR control for both more complicated models and the general global-local shrinkage prior family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05460v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiaoyu Liang, Zihan Zhu, Ziang Fu, Michael Evans</dc:creator>
    </item>
    <item>
      <title>Distributional Instrumental Variable Method</title>
      <link>https://arxiv.org/abs/2502.07641</link>
      <description>arXiv:2502.07641v2 Announce Type: replace 
Abstract: The instrumental variable (IV) approach is commonly used to infer causal effects in the presence of unmeasured confounding. Existing methods typically aim to estimate the mean causal effects, whereas a few other methods focus on quantile treatment effects. The aim of this work is to estimate the entire interventional distribution. We propose a method called Distributional Instrumental Variable (DIV), which uses generative modelling in a nonlinear IV setting. We establish identifiability of the interventional distribution under general assumptions and demonstrate an 'under-identified' case, where DIV can identify the causal effects while two-step least squares fails to. Our empirical results show that the DIV method performs well for a broad range of simulated data, exhibiting advantages over existing IV approaches in terms of the identifiability and estimation error of the mean or quantile treatment effects. Furthermore, we apply DIV to an economic data set to examine the causal relation between institutional quality and economic development and our results align well with the original study. We also apply DIV to a single-cell data set, where we study the generalizability and stability in predicting gene expression under unseen interventions. The software implementations of DIV are available in R and Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07641v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasiia Holovchak, Sorawit Saengkyongam, Nicolai Meinshausen, Xinwei Shen</dc:creator>
    </item>
    <item>
      <title>Adjustment with Many Regressors Under Covariate-Adaptive Randomizations</title>
      <link>https://arxiv.org/abs/2304.08184</link>
      <description>arXiv:2304.08184v5 Announce Type: replace-cross 
Abstract: Our paper discovers a new trade-off of using regression adjustments (RAs) in causal inference under covariate-adaptive randomizations (CARs). On one hand, RAs can improve the efficiency of causal estimators by incorporating information from covariates that are not used in the randomization. On the other hand, RAs can degrade estimation efficiency due to their estimation errors, which are not asymptotically negligible when the number of regressors is of the same order as the sample size. Ignoring the estimation errors of RAs may result in serious over-rejection of causal inference under the null hypothesis. To address the issue, we construct a new ATE estimator by optimally linearly combining the estimators with and without RAs. We then develop a unified inference theory for this estimator under CARs. It has two features: (1) the Wald test based on it achieves the exact asymptotic size under the null hypothesis, regardless of whether the number of covariates is fixed or diverges no faster than the sample size; and (2) it guarantees weak efficiency improvement over estimators both with and without RAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08184v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Jiang, Liyao Li, Ke Miao, Yichong Zhang</dc:creator>
    </item>
    <item>
      <title>Asymptotically Unbiased Synthetic Control Methods by Density Matching</title>
      <link>https://arxiv.org/abs/2307.11127</link>
      <description>arXiv:2307.11127v4 Announce Type: replace-cross 
Abstract: Synthetic Control Methods (SCMs) have become a fundamental tool for comparative case studies. The core idea behind SCMs is to estimate treatment effects by predicting counterfactual outcomes for a treated unit using a weighted combination of observed outcomes from untreated units. The accuracy of these predictions is crucial for evaluating the treatment effect of a policy intervention. Subsequent research has therefore focused on estimating SC weights. In this study, we highlight a key endogeneity issue in existing SCMs-namely, the correlation between the outcomes of untreated units and the error term of the synthetic control, which leads to bias in both counterfactual outcome prediction and treatment effect estimation. To address this issue, we propose a novel SCM based on density matching, assuming that the outcome density of the treated unit can be approximated by a weighted mixture of the joint density of untreated units. Under this assumption, we estimate SC weights by matching the moments of the treated outcomes with the weighted sum of the moments of the untreated outcomes. Our method offers three advantages: first, under the mixture model assumption, our estimator is asymptotically unbiased; second, this asymptotic unbiasedness reduces the mean squared error in counterfactual predictions; and third, our method provides full densities of the treatment effect rather than just expected values, thereby broadening the applicability of SCMs. Finally, we present experimental results that demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11127v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Akari Ohda</dc:creator>
    </item>
    <item>
      <title>Invariant Subspace Decomposition</title>
      <link>https://arxiv.org/abs/2404.09962</link>
      <description>arXiv:2404.09962v2 Announce Type: replace-cross 
Abstract: We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. To additionally exploit observations further in the past, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09962v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Margherita Lazzaretto, Jonas Peters, Niklas Pfister</dc:creator>
    </item>
  </channel>
</rss>
