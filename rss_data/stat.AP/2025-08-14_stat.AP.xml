<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Aug 2025 04:01:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sex at birth could well be a biological coin toss.... Beware of conditioning on post-baseline information</title>
      <link>https://arxiv.org/abs/2508.10240</link>
      <description>arXiv:2508.10240v1 Announce Type: new 
Abstract: Wang et al. (2025) use statistics to argue that sex at birth is not a biological coin toss, by noticing that repeated patterns such as Male Male Male and Female Female Female occur in the Nurses Health Study more often than patterns like Male Female Male, Male Female Female, Female Male Female, or Female Male Male. This letter shows that this over-representation is likely due to a statistical artifact, arising from parent preferences for mixed-sex children. As noticed in Angrist and Evans (1998) and supported by the data in Wang et al. (2025), parents are more likely to have a third child if their first two children are of the same sex. We show mathematically and statistically that mixed-sex preferences lead to the over-representation of patterns like Male Male Male and Female Female Female. In fact, the patterns seen in the Nurses Health Study are perfectly consistent with sex at birth being a random coin toss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10240v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Judith J. Lok, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>A 4% withdrawal rate for retirement spending, derived from a discrete-time model of stochastic returns on assets</title>
      <link>https://arxiv.org/abs/2508.10273</link>
      <description>arXiv:2508.10273v1 Announce Type: new 
Abstract: What grounds the rule of thumb that a(n American) retiree can safely withdraw 4% of their initial retirement wealth in their first year of retirement, then increase that rate of consumption with inflation? I investigate that question with a discrete-time model of returns to a retirement portfolio consumed at a rate that grows by $s$ per period. The model hinges on the parameter $\gamma$, an $s$-adjusted rate of return to wealth, derived from the first 2-4 moments of the portfolio's probability distribution of returns; for a retirement lasting $t$ periods the model recommends a rate of consumption of $\gamma / (1 - (1 - \gamma)^t)$. Estimation of $\gamma$ (and hence of the implied rate of spending down in retirement) reveals that the 4% rule emerges from adjusting high expected rates of return down for: consumption growth, the variance in (and kurtosis of) returns to wealth, the longevity risk of a retiree potentially underestimating $t$, and the inclusion of bonds in retirement portfolios without leverage. The model supports leverage of retirement portfolios dominated by the S&amp;P 500, with leverage ratios $&gt; 1.6$ having been historically optimal under the model's approximations. Historical simulations of 30-year retirements suggest that the model proposes withdrawal rates having roughly even odds of success, that leverage greatly improves those odds for stocks-heavy portfolios, and that investing on margin could have allowed safe withdrawal rates $&gt; 6$% per year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10273v1</guid>
      <category>stat.AP</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Drew M. Thomas</dc:creator>
    </item>
    <item>
      <title>Maximum Entropy Models for Unimodal Time Series: Case Studies of Universe 25 and St. Matthew Island</title>
      <link>https://arxiv.org/abs/2508.10518</link>
      <description>arXiv:2508.10518v1 Announce Type: new 
Abstract: We present a maximum entropy modeling framework for unimodal time series: signals that begin at a reference level, rise to a single peak, and return. Such patterns are commonly observed in ecological collapse, population dynamics, and resource depletion. Traditional dynamical models are often inapplicable in these settings due to limited or sparse data, frequently consisting of only a single historical trajectory. In addition, standard fitting approaches can introduce structural bias, particularly near the mode, where most interpretive focus lies. Using the maximum entropy principle, we derive a least-biased functional form constrained only by minimal prior knowledge, such as the starting point and estimated end. This leads to analytically tractable and interpretable models.
  We apply this method to the collapse of the Universe 25 mouse population and the reindeer crash on St. Matthew Island. These case studies demonstrate the robustness and flexibility of the approach in fitting diverse unimodal time series with minimal assumptions. We also conduct a cross-comparison against established models, including the Richards, Skewnormal, and Generalized Gamma functions. While models typically fit their own generated data best, the maximum entropy models consistently achieve the lowest off-diagonal root-mean-square losses, indicating superior generalization. These results suggest that maximum entropy methods provide a unifying and efficient alternative to mechanistic models when data is limited and generalization is essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10518v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabin Roman</dc:creator>
    </item>
    <item>
      <title>Matters Arising: Spatial correlation in economic analysis of climate change</title>
      <link>https://arxiv.org/abs/2508.10575</link>
      <description>arXiv:2508.10575v1 Announce Type: new 
Abstract: Climate change poses substantial risks to the global economy. Kotz, Levermann and Wenz (Nature, 2024) statistically analyzed economic and climate data, finding significant projected damages until mid-century and a divergence in outcomes between high- and low-emission scenarios thereafter. We find that their analysis underestimates uncertainty owing to large, unaccounted-for spatial correlations on the subnational level, rendering their results statistically insignificant when properly corrected. Thus, their study does not provide the robust empirical evidence needed to inform climate policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10575v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41586-025-09206-5</arxiv:DOI>
      <dc:creator>Christof Sch\"otz</dc:creator>
    </item>
    <item>
      <title>Risk-based framework to determine climate-informed design storms for road drainage infrastructure</title>
      <link>https://arxiv.org/abs/2508.10183</link>
      <description>arXiv:2508.10183v1 Announce Type: cross 
Abstract: Climate change is amplifying extreme precipitation events in many regions and imposes substantial challenges for the resilience of road drainage infrastructure. Conventional design storm methodologies, which rely on historical trends of rainfall data under a stationarity assumption, may not adequately account for future climate variability. This study introduces a risk-based framework for determining climate-informed design storms tailored to road drainage systems. The proposed framework integrates climate model projections with risk assessment to quantify the potential impacts of future extreme rainfall on drainage performance and adjust the future design storm, with a focus on the province of Ontario, Canada. Projected precipitation changes for mid- and late-century time horizons are quantified using statistically downscaled CMIP6 General Circulation Models. The risk level is defined as a function of hazard and vulnerability, where hazard combines both physiographic and meteorological factors. Vulnerability is comprised of socioeconomic, transportation, and environmental considerations. To systematically integrate these components, a weighting scheme is developed based on a sensitivity analysis of the criteria, which provides flexibility in assigning relative importance to each factor. The estimated risk level is then applied to adjust the projected design storm accordingly. The proposed workflow is demonstrated through both province-wide and site-specific applications across Ontario's road network to better highlight its scalability and adaptability. The findings signify the necessity of shifting from static, stationarity-based design methodologies to dynamic, risk-informed approaches that enhance the long-term resilience of transportation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10183v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Fereshtehpour, Rashid Bashir, Neil F. Tandon</dc:creator>
    </item>
    <item>
      <title>Examining the Association between Estimated Prevalence and Diagnostic Test Accuracy using Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2508.10207</link>
      <description>arXiv:2508.10207v1 Announce Type: cross 
Abstract: There have been reports of correlation between estimates of prevalence and test accuracy across studies included in diagnostic meta-analyses. It has been hypothesized that this unexpected association arises because of certain biases commonly found in diagnostic accuracy studies. A theoretical explanation has not been studied systematically. In this work, we introduce directed acyclic graphs to illustrate common structures of bias in diagnostic test accuracy studies and to define the resulting data-generating mechanism behind a diagnostic meta-analysis. Using simulation studies, we examine how these common biases can produce a correlation between estimates of prevalence and index test accuracy and what factors influence its magnitude and direction. We found that an association arises either in the absence of a perfect reference test or in the presence of a covariate that simultaneously causes spectrum effect and is associated with the prevalence (confounding). We also show that the association between prevalence and accuracy can be removed by appropriate statistical methods. In the risk of bias evaluation in diagnostic meta-analyses, an observed association between estimates of prevalence and accuracy should be explored to understand its source and to adjust for latent or observed variables if possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10207v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yang Lu, Robert Platt, Nandini Dendukuri</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Autoregressions for High Dimensional Matrix-Valued Time Series</title>
      <link>https://arxiv.org/abs/2508.10291</link>
      <description>arXiv:2508.10291v1 Announce Type: cross 
Abstract: Motivated by predicting intraday trading volume curves, we consider two spatio-temporal autoregressive models for matrix time series, in which each column may represent daily trading volume curve of one asset, and each row captures synchronized 5-minute volume intervals across multiple assets. While traditional matrix time series focus mainly on temporal evolution, our approach incorporates both spatial and temporal dynamics, enabling simultaneous analysis of interactions across multiple dimensions. The inherent endogeneity in spatio-temporal autoregressive models renders ordinary least squares estimation inconsistent. To overcome this difficulty while simultaneously estimating two distinct weight matrices with banded structure, we develop an iterated generalized Yule-Walker estimator by adapting a generalized method of moments framework based on Yule-Walker equations. Moreover, unlike conventional models that employ a single bandwidth parameter, the dual-bandwidth specification in our framework requires a new two-step, ratio-based sequential estimation procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10291v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baojun Dou, Jing He, Sudhir Tiwari, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Dynamic Skewness in Stochastic Volatility Models: A Penalized Prior Approach</title>
      <link>https://arxiv.org/abs/2508.10778</link>
      <description>arXiv:2508.10778v1 Announce Type: cross 
Abstract: Financial time series often exhibit skewness and heavy tails, making it essential to use models that incorporate these characteristics to ensure greater reliability in the results. Furthermore, allowing temporal variation in the skewness parameter can bring significant gains in the analysis of this type of series. However, for more robustness, it is crucial to develop models that balance flexibility and parsimony. In this paper, we propose dynamic skewness stochastic volatility models in the SMSN family (DynSSV-SMSN), using priors that penalize model complexity. Parameter estimation was carried out using the Hamiltonian Monte Carlo (HMC) method via the \texttt{RStan} package. Simulation results demonstrated that penalizing priors present superior performance in several scenarios compared to the classical choices. In the empirical application to returns of cryptocurrencies, models with heavy tails and dynamic skewness provided a better fit to the data according to the DIC, WAIC, and LOO-CV information criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10778v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno E. Holtz, Ricardo S. Ehlers, Adriano K. Suzuki, Francisco Louzada</dc:creator>
    </item>
    <item>
      <title>Does fertility affect woman's labor force participation in low- and middle-income settings? Findings from a Bayesian nonparametric analysis</title>
      <link>https://arxiv.org/abs/2508.10787</link>
      <description>arXiv:2508.10787v1 Announce Type: cross 
Abstract: Estimating the causal effect of fertility on women's employment is challenging because fertility and labor decisions are jointly determined. The difficulty is amplified in low- and middle-income countries, where longitudinal data are scarce. In this study, we propose a novel approach to estimating the causal effect of fertility on employment using widely available Demographic and Health Survey (DHS) observational data. Using infecundity as an instrument for family size, our approach combines principal stratification with Bayesian Additive Regression Trees to flexibly account for covariate-dependent instrument validity, work with count-valued intermediate variables, and produce estimates of causal effects and effect heterogeneity, i.e., how effects vary with covariates in the survey population. We apply the approach to DHS data from Nigeria, Senegal, and Kenya. We find in the survey sample and general population that an additional child significantly reduces employment among women in Nigeria but has no clear average effect in Senegal or Kenya. Across all three countries, however, there is strong evidence of effect heterogeneity: younger, less-educated women experience large employment penalties, while older or more advantaged women are largely unaffected. Robustness checks confirm that these findings are not sensitive to key modeling assumptions. While limitations remain due to the cross-sectional nature of the DHS data, our results illustrate how flexible non-parametric models can uncover important effect variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10787v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Post-clustering Inference under Dependence</title>
      <link>https://arxiv.org/abs/2310.11822</link>
      <description>arXiv:2310.11822v2 Announce Type: replace-cross 
Abstract: Recent work by Gao et al. (JASA 2022) has laid the foundations for post-clustering inference, establishing a theoretical framework allowing to test for differences between means of estimated clusters. Additionally, they studied the estimation of unknown parameters while controlling the selective type I error. However, their theory was developed for independent observations identically distributed as $p$-dimensional Gaussian variables, where the parameter estimation could only be performed for spherical covariance matrices. Here, we aim at extending this framework to a more convenient scenario for practical applications, where arbitrary dependence structures between observations and features are allowed. We establish sufficient conditions for extending the setting presented by Gao et al. to the general dependence framework. Moreover, we assess theoretical conditions allowing the compatible estimation of a covariance matrix. The theory is developed for hierarchical agglomerative clustering algorithms with several types of linkages, and for the $k$-means algorithm. We illustrate our method with synthetic data and real data of protein structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11822v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Javier Gonz\'alez-Delgado, Mathis Deronzier, Juan Cort\'es, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Online Distributional Regression</title>
      <link>https://arxiv.org/abs/2407.08750</link>
      <description>arXiv:2407.08750v3 Announce Type: replace-cross 
Abstract: Large-scale streaming data are common in modern machine learning applications and have led to the development of online learning algorithms. Many fields, such as supply chain management, weather and meteorology, energy markets, and finance, have pivoted towards using probabilistic forecasts. This results in the need not only for accurate learning of the expected value but also for learning the conditional heteroskedasticity and conditional moments. Against this backdrop, we present a methodology for online estimation of regularized, linear distributional models. The proposed algorithm is based on a combination of recent developments for the online estimation of LASSO models and the well-known GAMLSS framework. We provide a case study on day-ahead electricity price forecasting, in which we show the competitive performance of the incremental estimation combined with strongly reduced computational effort. Our algorithms are implemented in a computationally efficient Python package ondil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08750v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Hirsch, Jonathan Berrisch, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>DAmodel: Hierarchical Bayesian Modelling of DA White Dwarfs for Spectrophotometric Calibration</title>
      <link>https://arxiv.org/abs/2412.08809</link>
      <description>arXiv:2412.08809v3 Announce Type: replace-cross 
Abstract: We use hierarchical Bayesian modelling to calibrate a network of 32 all-sky faint DA white dwarf (DA WD) spectrophotometric standards ($16.5 &lt; V &lt; 19.5$) alongside three CALSPEC standards, from 912 \r{A} to 32 $\mu$m. The framework is the first of its kind to jointly infer photometric zeropoints and WD parameters (surface gravity $\log g$, effective temperature $T_{\text{eff}}$, extinction $A_V$, dust relation parameter $R_V$) by simultaneously modelling both photometric and spectroscopic data. We model panchromatic Hubble Space Telescope Wide Field Camera 3 (HST/WFC3) UVIS and IR photometry, HST/STIS UV spectroscopy and ground-based optical spectroscopy to sub-percent precision. Photometric residuals for the sample are the lowest yet yielding $&lt;0.004$ mag RMS on average from the UV to the NIR, achieved by jointly inferring time-dependent changes in system sensitivity and WFC3/IR count-rate nonlinearity. Our GPU-accelerated implementation enables efficient sampling via Hamiltonian Monte Carlo, critical for exploring the high-dimensional posterior space. The hierarchical nature of the model enables population analysis of intrinsic WD and dust parameters. Inferred spectral energy distributions from this model will be essential for calibrating the James Webb Space Telescope as well as next-generation surveys, including Vera Rubin Observatory's Legacy Survey of Space and Time and the Nancy Grace Roman Space Telescope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08809v3</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.SR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/mnras/staf629</arxiv:DOI>
      <arxiv:journal_reference>Monthly Notices of the Royal Astronomical Society, Volume 540, Issue 1, June 2025, Pages 385-415</arxiv:journal_reference>
      <dc:creator>Benjamin M. Boyd, Gautham Narayan, Kaisey S. Mandel, Matthew Grayling, Abhijit Saha, Tim Axelrod, Thomas Matheson, Edward W. Olszewski, Annalisa Calamida, Aaron Do, Ralph C. Bohlin, Jay B. Holberg, Ivan Hubeny, Susana Deustua, Armin Rest, Christopher W. Stubbs, Aidan Berres, Mai Li, John W. Mackenty, Elena Sabbi</dc:creator>
    </item>
    <item>
      <title>Inequality Restricted Minimum Density Power Divergence Estimation in Panel Count Data</title>
      <link>https://arxiv.org/abs/2503.21534</link>
      <description>arXiv:2503.21534v2 Announce Type: replace-cross 
Abstract: Analysis of panel count data has garnered a considerable amount of attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most of the works focus on leveraging estimating equations-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present the robust density power divergence estimation for panel count data arising from nonhomogeneous Poisson processes, correlated through a latent frailty variable. In order to cope with real-world incidents, it is often desired to impose certain inequality constraints on the parameter space, giving rise to the restricted minimum density power divergence estimator. The significant contribution of this study lies in deriving its asymptotic properties. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21534v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Udita Goswami, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v3 Announce Type: replace-cross 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Chang, John Kairalla, Arkaprava Roy</dc:creator>
    </item>
  </channel>
</rss>
