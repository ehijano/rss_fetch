<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 May 2025 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rod Bustall: In Memoriam</title>
      <link>https://arxiv.org/abs/2505.06456</link>
      <description>arXiv:2505.06456v1 Announce Type: new 
Abstract: This is an obituary of Rod Burstall, written in his honour. Rod was a prominent computer scientist whose contributions span over forty years. Most of his career was spent at Edinburgh University. He lead the team programming Freddy, the first hand-eye assembly robot, with much of his effort being devoted to the development of the POP-2 programming language. He became interested in a mathematical approach to software development: he recognised the central role of structural induction; his work on reasoning about mutable data structures was an influential precursor of separation logic; he was the first to point out the connection between program proof and modal logic; and he was responsible for the idea that stores are mappings from locations to their contents.
  As part of his quest for correctness of programs, Rod, with John Darlington, undertook the first major work on program transformation. His interest in novel programming languages continued with the experimental language HOPE, developed with Don Sannella and David MacQueen. Robin Milner's Standard ML and its relatives integrated ideas from Hope, and Rod was an active member of the Standard ML design team. Rod pioneered the use of algebraic and categorical techniques in programming. He and Joseph Goguen proposed the first algebraic specification language. Rod was an early user of categorical ideas, in particular developing computational category theory with David Rydeheard.
  The interplay between programming and correctness proofs is again evident in Rod's contributions to automated proof support systems. For example, he led Randy Pollack, Zhaohui Luo, and others in the Lego proof assistant, which implemented type systems supporting interactive proof development. Finally, Rod, with James McKinna, investigated notions of programs packaged with proofs of their correctness, anticipating the topic of proof-carrying code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06456v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731974.</arxiv:DOI>
      <dc:creator>J Strother Moore, Gordon Plotkin, David Rydeheard, Don Sannella</dc:creator>
    </item>
    <item>
      <title>A Formally Verified Robustness Certifier for Neural Networks (Extended Version)</title>
      <link>https://arxiv.org/abs/2505.06958</link>
      <description>arXiv:2505.06958v1 Announce Type: new 
Abstract: Neural networks are often susceptible to minor perturbations in input that cause them to misclassify. A recent solution to this problem is the use of globally-robust neural networks, which employ a function to certify that the classification of an input cannot be altered by such a perturbation. Outputs that pass this test are called certified robust. However, to the authors' knowledge, these certification functions have not yet been verified at the implementation level. We demonstrate how previous unverified implementations are exploitably unsound in certain circumstances. Moreover, they often rely on approximation-based algorithms, such as power iteration, that (perhaps surprisingly) do not guarantee soundness. To provide assurance that a given output is robust, we implemented and formally verified a certification function for globally-robust neural networks in Dafny. We describe the program, its specifications, and the important design decisions taken for its implementation and verification, as well as our experience applying it in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06958v1</guid>
      <category>cs.PL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Tobler, Hira Taqdees Syeda, Toby Murray</dc:creator>
    </item>
    <item>
      <title>Verified Purely Functional Catenable Real-Time Deques</title>
      <link>https://arxiv.org/abs/2505.07681</link>
      <description>arXiv:2505.07681v1 Announce Type: new 
Abstract: We present OCaml and Rocq implementations of Kaplan and Tarjan's purely functional, real-time catenable deques. The correctness of our Rocq implementation is machine-checked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07681v1</guid>
      <category>cs.PL</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jules Viennot, Arthur Wendling, Arma\"el Gu\'eneau, Fran\c{c}ois Pottier</dc:creator>
    </item>
    <item>
      <title>LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers</title>
      <link>https://arxiv.org/abs/2403.11522</link>
      <description>arXiv:2403.11522v3 Announce Type: replace 
Abstract: While polyhedral compilers have shown success in implementing advanced code transformations, they still face challenges in selecting the ones that lead to the most profitable speedups. This has motivated the use of machine learning based cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of such an approach. While promising, this approach still faces significant limitations. State-of-the-art polyhedral compilers that use a deep learning cost model only support a small subset of affine transformations, limiting their ability to explore complex code transformations. Furthermore, their applicability does not scale beyond simple programs, thus excluding many program classes from their scope, such as those with non-rectangular iteration domains or multiple loop nests. These limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. In this paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a deep learning based cost model and covers a large space of affine transformations and programs. LOOPer allows the optimization of an extensive set of programs while being effective at applying complex sequences of polyhedral transformations. We implement and evaluate LOOPer and show that it achieves competitive speedups over the state-of-the-art. On the PolyBench benchmarks, LOOPer achieves a geometric mean speedup of 1.84x over Tiramisu and 1.42x over Pluto, two state-of-the-art polyhedral autoschedulers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11522v3</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islem Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, Riyadh Baghdadi</dc:creator>
    </item>
    <item>
      <title>CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization</title>
      <link>https://arxiv.org/abs/2407.10424</link>
      <description>arXiv:2407.10424v5 Announce Type: replace 
Abstract: The design flow of processors, particularly in hardware description languages (HDL) like Verilog and Chisel, is complex and costly. While recent advances in large language models (LLMs) have significantly improved coding tasks in software languages such as Python, their application in HDL generation remains limited due to the scarcity of high-quality HDL data. Traditional methods of adapting LLMs for hardware design rely on synthetic HDL datasets, which often suffer from low quality because even advanced LLMs like GPT perform poorly in the HDL domain. Moreover, these methods focus solely on chat tasks and the Verilog language, limiting their application scenarios.
  In this paper, we observe that: (1) HDL code collected from the real world is of higher quality than code generated by LLMs. (2) LLMs like GPT-3.5 excel in summarizing HDL code rather than generating it. (3) An explicit language tag can help LLMs better adapt to the target language when there is insufficient data. Based on these observations, we propose an efficient LLM fine-tuning pipeline for HDL generation that integrates a multi-level summarization data synthesis process with a novel Chat-FIM-Tag supervised fine-tuning method. The pipeline enhances the generation of HDL code from natural language descriptions and enables the handling of various tasks such as chat and infilling incomplete code. Utilizing this pipeline, we introduce CodeV, a series of HDL generation LLMs. Among them, CodeV-All not only possesses a more diverse range of language abilities, i.e. Verilog and Chisel, and a broader scope of tasks, i.e. Chat and fill-in-middle (FIM), but it also achieves performance on VerilogEval that is comparable to or even surpasses that of CodeV-Verilog fine-tuned on Verilog only, making them the first series of open-source LLMs designed for multi-scenario HDL generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10424v5</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Muxin Song, Yinan Xu, Ziyuan Nan, Mingju Gao, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Xing Hu</dc:creator>
    </item>
    <item>
      <title>XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</title>
      <link>https://arxiv.org/abs/2411.15100</link>
      <description>arXiv:2411.15100v3 Announce Type: replace-cross 
Abstract: The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15100v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen</dc:creator>
    </item>
    <item>
      <title>PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices</title>
      <link>https://arxiv.org/abs/2504.03529</link>
      <description>arXiv:2504.03529v3 Announce Type: replace-cross 
Abstract: Variational quantum algorithms (VQA) based on Hamiltonian simulation represent a specialized class of quantum programs well-suited for near-term quantum computing applications due to its modest resource requirements in terms of qubits and circuit depth. Unlike the conventional single-qubit (1Q) and two-qubit (2Q) gate sequence representation, Hamiltonian simulation programs are essentially composed of disciplined subroutines known as Pauli exponentiations (Pauli strings with coefficients) that are variably arranged. To capitalize on these distinct program features, this study introduces PHOENIX, a highly effective compilation framework that primarily operates at the high-level Pauli-based intermediate representation (IR) for generic Hamiltonian simulation programs. PHOENIX exploits global program optimization opportunities to the greatest extent, compared to existing SOTA methods despite some of them also utilizing similar IRs. PHOENIX employs the binary symplectic form (BSF) to formally describe Pauli strings and reformulates IR synthesis as reducing the column weights of BSF by appropriate Clifford transformations. It comes with a heuristic BSF simplification algorithm that searches for the most appropriate 2Q Clifford operators in sequence to maximally simplify the BSF at each step, until the BSF can be directly synthesized by basic 1Q and 2Q gates. PHOENIX further performs a global ordering strategy in a Tetris-like fashion for these simplified IR groups, carefully balancing optimization opportunities for gate cancellation, minimizing circuit depth, and managing qubit routing overhead. Experimental results demonstrate that PHOENIX outperforms SOTA VQA compilers across diverse program categories, backend ISAs, and hardware topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03529v3</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaohui Yang, Dawei Ding, Chenghong Zhu, Jianxin Chen, Yuan Xie</dc:creator>
    </item>
  </channel>
</rss>
