<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 04:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Machine-Learning Prediction of Virus-like Particle Stoichiometry and Stability using Persistent Topological Laplacians</title>
      <link>https://arxiv.org/abs/2507.21417</link>
      <description>arXiv:2507.21417v1 Announce Type: new 
Abstract: Understanding the stoichiometry and associated stability of virus-like particles (VLPs) is crucial for optimizing their assembly efficiency and immunogenic properties, which are essential for advancing biotechnology, vaccine design, and drug delivery. However, current experimental methods for determining VLP stoichiometry are labor-intensive, and time consuming. Machine learning approaches have hardly been applied to the study of VLPs. To address this challenge, we introduce a novel persistent Laplacian-based machine learning (PLML) mode that leverages both harmonic and non-harmonic spectra to capture intricate topological and geometric features of VLP structures. This approach achieves superior performance on the VLP200 dataset compared to existing methods. To further assess robustness and generalizability, we collected a new dataset, VLP706, containing 706 VLP samples with expanded stoichiometry diversity. Our PLML model maintains strong predictive accuracy on VLP706. Additionally, through random sequence perturbative mutation analysis, we found that 60-mers and 180-mers exhibit greater stability than 240-mers and 420-mers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21417v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Liu, Xuefei Huang, Guo-Wei Wei</dc:creator>
    </item>
    <item>
      <title>Multi-state Protein Design with DynamicMPNN</title>
      <link>https://arxiv.org/abs/2507.21938</link>
      <description>arXiv:2507.21938v1 Announce Type: cross 
Abstract: Structural biology has long been dominated by the one sequence, one structure, one function paradigm, yet many critical biological processes - from enzyme catalysis to membrane transport - depend on proteins that adopt multiple conformational states. Existing multi-state design approaches rely on post-hoc aggregation of single-state predictions, achieving poor experimental success rates compared to single-state design. We introduce DynamicMPNN, an inverse folding model explicitly trained to generate sequences compatible with multiple conformations through joint learning across conformational ensembles. Trained on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated using AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13% on structure-normalized RMSD across our challenging multi-state protein benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21938v1</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Abrudan, Sebastian Pujalte Ojeda, Chaitanya K. Joshi, Matthew Greenig, Felipe Engelberger, Alena Khmelinskaia, Jens Meiler, Michele Vendruscolo, Tuomas P. J. Knowles</dc:creator>
    </item>
    <item>
      <title>AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model</title>
      <link>https://arxiv.org/abs/2507.08920</link>
      <description>arXiv:2507.08920v2 Announce Type: replace 
Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08920v2</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changze Lv, Jiang Zhou, Siyu Long, Lihao Wang, Jiangtao Feng, Dongyu Xue, Yu Pei, Hao Wang, Zherui Zhang, Yuchen Cai, Zhiqiang Gao, Ziyuan Ma, Jiakai Hu, Chaochen Gao, Jingjing Gong, Yuxuan Song, Shuyi Zhang, Xiaoqing Zheng, Deyi Xiong, Lei Bai, Wanli Ouyang, Ya-Qin Zhang, Wei-Ying Ma, Bowen Zhou, Hao Zhou</dc:creator>
    </item>
  </channel>
</rss>
