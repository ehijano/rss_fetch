<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Mar 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Intanify AI Platform: Embedded AI for Automated IP Audit and Due Diligence</title>
      <link>https://arxiv.org/abs/2503.17374</link>
      <description>arXiv:2503.17374v1 Announce Type: new 
Abstract: In this paper we introduce a Platform created in order to support SMEs' endeavor to extract value from their intangible assets effectively. To implement the Platform, we developed five knowledge bases using a knowledge-based ex-pert system shell that contain knowledge from intangible as-set consultants, patent attorneys and due diligence lawyers. In order to operationalize the knowledge bases, we developed a "Rosetta Stone", an interpreter unit for the knowledge bases outside the shell and embedded in the plat-form. Building on the initial knowledge bases we have created a system of red flags, risk scoring, and valuation with the involvement of the same experts; these additional systems work upon the initial knowledge bases and therefore they can be regarded as meta-knowledge-representations that take the form of second-order knowledge graphs. All this clever technology is dressed up in an easy-to-handle graphical user interface that we will showcase at the conference. The initial platform was finished mid-2024; therefore, it qualifies as an "emerging application of AI" and "deployable AI", while development continues. The two firms that provided experts for developing the knowledge bases obtained a white-label version of the product (i.e. it runs under their own brand "powered by Intanify"), and there are two completed cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17374v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Dorfler, V., Dryden, D., &amp; Lee, V. (2025, 25 February - 4 March 2025). Intanify AI Platform: Embedded AI for Automated IP Audit and Due Diligence AAAI 2025: The 39th Annual AAAI Conference on Artificial Intelligence, Philadelphia, PA</arxiv:journal_reference>
      <dc:creator>Viktor Dorfler, Dylan Dryden, Viet Lee</dc:creator>
    </item>
    <item>
      <title>AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations</title>
      <link>https://arxiv.org/abs/2503.17388</link>
      <description>arXiv:2503.17388v1 Announce Type: new 
Abstract: The rapid advancement of AI systems has raised widespread concerns about potential harms of frontier AI systems and the need for responsible evaluation and oversight. In this position paper, we argue that frontier AI companies should report both pre- and post-mitigation safety evaluations to enable informed policy decisions. Evaluating models at both stages provides policymakers with essential evidence to regulate deployment, access, and safety standards. We show that relying on either in isolation can create a misleading picture of model safety. Our analysis of AI safety disclosures from leading frontier labs identifies three critical gaps: (1) companies rarely evaluate both pre- and post-mitigation versions, (2) evaluation methods lack standardization, and (3) reported results are often too vague to inform policy. To address these issues, we recommend mandatory disclosure of pre- and post-mitigation capabilities to approved government bodies, standardized evaluation methods, and minimum transparency requirements for public safety reporting. These ensure that policymakers and regulators can craft targeted safety measures, assess deployment risks, and scrutinize companies' safety claims effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17388v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dillon Bowen, Ann-Kathrin Dombrowski, Adam Gleave, Chris Cundy</dc:creator>
    </item>
    <item>
      <title>AEJIM: A Real-Time AI Framework for Crowdsourced, Transparent, and Ethical Environmental Hazard Detection and Reporting</title>
      <link>https://arxiv.org/abs/2503.17401</link>
      <description>arXiv:2503.17401v1 Announce Type: new 
Abstract: Environmental journalism is vital for raising awareness of ecological crises and driving evidence-based policy, yet traditional methods falter under delays, inaccuracies, and scalability limits, especially in under-monitored regions critical to the United Nations Sustainable Development Goals. To bridge these gaps, this paper introduces the AI-Environmental Journalism Integration Model (AEJIM), an innovative framework combining real-time hazard detection, crowdsourced validation, and AI-driven reporting.
  Validated through a pilot study, AEJIM significantly improved the speed and accuracy of environmental hazard reporting, outperforming traditional methods. Furthermore, the model directly addresses key ethical, regulatory, and scalability challenges, ensuring AI accountability through Explainable AI (XAI), GDPR-compliant data governance, and active public participation. AEJIM provides a transparent and adaptable solution, setting a new benchmark for AI-enhanced environmental journalism and supporting informed global decision-making across diverse socio-political landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17401v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Torsten Tiltack</dc:creator>
    </item>
    <item>
      <title>Opportunities and Challenges of Frontier Data Governance With Synthetic Data</title>
      <link>https://arxiv.org/abs/2503.17414</link>
      <description>arXiv:2503.17414v1 Announce Type: new 
Abstract: Synthetic data, or data generated by machine learning models, is increasingly emerging as a solution to the data access problem. However, its use introduces significant governance and accountability challenges, and potentially debases existing governance paradigms, such as compute and data governance. In this paper, we identify 3 key governance and accountability challenges that synthetic data poses - it can enable the increased emergence of malicious actors, spontaneous biases and value drift. We thus craft 3 technical mechanisms to address these specific challenges, finding applications for synthetic data towards adversarial training, bias mitigation and value reinforcement. These could not only counteract the risks of synthetic data, but serve as critical levers for governance of the frontier in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17414v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Madhavendra Thakur, Jason Hausenloy</dc:creator>
    </item>
    <item>
      <title>Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation</title>
      <link>https://arxiv.org/abs/2503.17421</link>
      <description>arXiv:2503.17421v1 Announce Type: new 
Abstract: Patients are increasingly turning to online health Q&amp;A communities for social support to improve their well-being. However, when this support received does not align with their specific needs, it may prove ineffective or even detrimental. This necessitates a model capable of identifying the social support needs in questions. However, training such a model is challenging due to the scarcity and class imbalance issues of labeled data. To overcome these challenges, we follow the computational design science paradigm to develop a novel framework, Hybrid Approach for SOcial Support need classification (HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning approach, a text data augmentation technique leveraging large language models (LLMs) with reliability- and diversity-aware sample selection mechanism, and a unified training process to automatically label social support needs in questions. Extensive empirical evaluations demonstrate that HA-SOS significantly outperforms existing question classification models and alternative semi-supervised learning approaches. This research contributes to the literature on social support, question classification, semi-supervised learning, and text data augmentation. In practice, our HA-SOS framework facilitates online Q&amp;A platform managers and answerers to better understand users' social support needs, enabling them to provide timely, personalized answers and interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17421v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junwei Kuang, Liang Yang, Shaoze Cui, Weiguo Fan</dc:creator>
    </item>
    <item>
      <title>Data to Decisions: A Computational Framework to Identify skill requirements from Advertorial Data</title>
      <link>https://arxiv.org/abs/2503.17424</link>
      <description>arXiv:2503.17424v1 Announce Type: new 
Abstract: Among the factors of production, human capital or skilled manpower is the one that keeps evolving and adapts to changing conditions and resources. This adaptability makes human capital the most crucial factor in ensuring a sustainable growth of industry/sector. As new technologies are developed and adopted, the new generations are required to acquire skills in newer technologies in order to be employable. At the same time professionals are required to upskill and reskill themselves to remain relevant in the industry. There is however no straightforward method to identify the skill needs of the industry at a given point of time. Therefore, this paper proposes a data to decision framework that can successfully identify the desired skill set in a given area by analysing the advertorial data collected from popular online job portals and supplied as input to the framework. The proposed framework uses techniques of statistical analysis, data mining and natural language processing for the purpose. The applicability of the framework is demonstrated on CS&amp;IT job advertisement data from India. The analytical results not only provide useful insights about current state of skill needs in CS&amp;IT industry but also provide practical implications to prospective job applicants, training agencies, and institutions of higher education &amp; professional training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17424v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-83793-7_28</arxiv:DOI>
      <dc:creator>Aakash Singh, Anurag Kanaujia, Vivek Kumar Singh</dc:creator>
    </item>
    <item>
      <title>Would you mind being watched by machines? Privacy concerns in data mining</title>
      <link>https://arxiv.org/abs/2503.17428</link>
      <description>arXiv:2503.17428v1 Announce Type: new 
Abstract: Data mining is not an invasion of privacy because access to data is only by machines, not by people: this is the argument that is investigated here. The current importance of this problem is developed in a case study of data mining in the USA for counterterrorism and other surveillance purposes. After a clarification of the relevant nature of privacy, it is argued that access by machines cannot warrant the access to further information, since the analysis will have to be made either by humans or by machines that understand. It concludes that the current data mining violates the right to privacy and should be subject to the standard legal constraints for access to private information by people.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17428v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00146-007-0177-3</arxiv:DOI>
      <arxiv:journal_reference>(2009) AI &amp; Society, 23 (4), 529-44</arxiv:journal_reference>
      <dc:creator>Vincent C. M\"uller</dc:creator>
    </item>
    <item>
      <title>From Text to Talent: A Pipeline for Extracting Insights from Candidate Profiles</title>
      <link>https://arxiv.org/abs/2503.17438</link>
      <description>arXiv:2503.17438v1 Announce Type: new 
Abstract: The recruitment process is undergoing a significant transformation with the increasing use of machine learning and natural language processing techniques. While previous studies have focused on automating candidate selection, the role of multiple vacancies in this process remains understudied. This paper addresses this gap by proposing a novel pipeline that leverages Large Language Models and graph similarity measures to suggest ideal candidates for specific job openings. Our approach represents candidate profiles as multimodal embeddings, enabling the capture of nuanced relationships between job requirements and candidate attributes. The proposed approach has significant implications for the recruitment industry, enabling companies to streamline their hiring processes and identify top talent more efficiently. Our work contributes to the growing body of research on the application of machine learning in human resources, highlighting the potential of LLMs and graph-based methods in revolutionizing the recruitment landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17438v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Frazzetto, Muhammad Uzair Ul Haq, Flavia Fabris, Alessandro Sperduti</dc:creator>
    </item>
    <item>
      <title>NFTs as a Data-Rich Test Bed: Conspicuous Consumption and its Determinants</title>
      <link>https://arxiv.org/abs/2503.17457</link>
      <description>arXiv:2503.17457v1 Announce Type: new 
Abstract: Conspicuous consumption occurs when a consumer derives value from a good based on its social meaning as a signal of wealth, taste, and/or community affiliation. Common conspicuous goods include designer footwear, country club memberships, and artwork; conspicuous goods also exist in the digital sphere, with non-fungible tokens (NFTs) as a prominent example. The NFT market merits deeper study for two key reasons: first, it is poorly understood relative to its economic scale; and second, it is unusually amenable to analysis because NFT transactions are publicly available on the blockchain, making them useful as a test bed for conspicuous consumption dynamics. This paper introduces a model that incorporates two previously identified elements of conspicuous consumption: the \emph{bandwagon effect} (goods increase in value as they become more popular) and the \emph{snob effect} (goods increase in value as they become rarer). Our model resolves the apparent tension between these two effects, exhibiting net complementarity between others' and one's own conspicuous consumption. We also introduce a novel dataset combining NFT transactions with embeddings of the corresponding NFT images computed using an off-the-shelf vision transformer architecture. We use our dataset to validate the model, showing that the bandwagon effect raises an NFT collection's value as more consumers join, while the snob effect drives consumers to seek rarer NFTs within a given collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17457v1</guid>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taylor Lundy, Narun Raman, Scott Duke Kominers, Kevin Leyton-Brown</dc:creator>
    </item>
    <item>
      <title>Birds of a Feather Undermine Equity: A Strategy to Align Intent and Outcome in Team-Based Learning in Higher Education</title>
      <link>https://arxiv.org/abs/2503.17476</link>
      <description>arXiv:2503.17476v1 Announce Type: new 
Abstract: Efforts to promote equity in higher education often rely on shared intent among instructors and students. Yet, as demonstrated in this study, when students form their own teams for Team-Based Learning (TBL) tasks, they unintentionally cluster with peers of similar socio-economic backgrounds, ultimately undermining equity. This study introduces a simple strategy to facilitate equitable team formation through a quantitative reflection of students' socio-economic backgrounds and their self-perceived preparedness. When applied, the strategy yielded balanced teams and improved performance. In its absence, team compositions became skewed and class performance declined. These findings highlight a behavioural gap between intent and outcome and underscore the need for structural supports to translate equity goals into practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17476v1</guid>
      <category>cs.CY</category>
      <category>physics.ed-ph</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P G Kubendran Amos</dc:creator>
    </item>
    <item>
      <title>A Qualitative Study of User Perception of M365 AI Copilot</title>
      <link>https://arxiv.org/abs/2503.17661</link>
      <description>arXiv:2503.17661v1 Announce Type: new 
Abstract: Adopting AI copilots in professional workflows presents opportunities for enhanced productivity, efficiency, and decision making. In this paper, we present results from a six month trial of M365 Copilot conducted at our organisation in 2024. A qualitative interview study was carried out with 27 participants. The study explored user perceptions of M365 Copilot's effectiveness, productivity impact, evolving expectations, ethical concerns, and overall satisfaction. Initial enthusiasm for the tool was met with mixed post trial experiences. While some users found M365 Copilot beneficial for tasks such as email coaching, meeting summaries, and content retrieval, others reported unmet expectations in areas requiring deeper contextual understanding, reasoning, and integration with existing workflows. Ethical concerns were a recurring theme, with users highlighting issues related to data privacy, transparency, and AI bias. While M365 Copilot demonstrated value in specific operational areas, its broader impact remained constrained by usability limitations and the need for human oversight to validate AI generated outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17661v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muneera Bano, Didar Zowghi, Jon Whittle, Liming Zhu, Andrew Reeson, Rob Martin, Jen Parson</dc:creator>
    </item>
    <item>
      <title>On the (im)possibility of sustainable artificial intelligence. Why it does not make sense to move faster when heading the wrong way</title>
      <link>https://arxiv.org/abs/2503.17702</link>
      <description>arXiv:2503.17702v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is currently considered a sustainability "game-changer" within and outside of academia. In order to discuss sustainable AI this article draws from insights by critical data and algorithm studies, STS, transformative sustainability science, critical computer science, and public interest theory. I argue that while there are indeed many sustainability-related use cases for AI, they are likely to have more overall drawbacks than benefits. To substantiate this claim, I differentiate three 'AI materialities' of the AI supply chain: first the literal materiality (e.g. water, cobalt, lithium, energy consumption etc.), second, the informational materiality (e.g. lots of data and centralised control necessary), and third, the social materiality (e.g. exploitative data work, communities harm by waste and pollution). In all materialities, effects are especially devastating for the global south while benefiting the global north. A second strong claim regarding sustainable AI circles around so called apolitical optimisation (e.g. regarding city traffic), however the optimisation criteria (e.g. cars, bikes, emissions, commute time, health) are purely political and have to be collectively negotiated before applying AI optimisation. Hence, sustainable AI, in principle, cannot break the glass ceiling of transformation and might even distract from necessary societal change. To address that I propose to stop 'unformation gathering' and to apply the 'small is beautiful' principle. This aims to contribute to an informed academic and collective negotiation on how to (not) integrate AI into the sustainability project while avoiding to reproduce the status quo by serving hegemonic interests between useful AI use cases, techno-utopian salvation narratives, technology-centred efficiency paradigms, the exploitative and extractivist character of AI and concepts of digital degrowth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17702v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14283597</arxiv:DOI>
      <arxiv:journal_reference>In: Zueger, T. &amp; Asghari, H. (2024) AI systems for the public interest. Internet Policy Review, 13(3) 2024</arxiv:journal_reference>
      <dc:creator>Rainer Rehak</dc:creator>
    </item>
    <item>
      <title>Interpersonal Trust Among Students in Virtual Learning Environments: A Comprehensive Review</title>
      <link>https://arxiv.org/abs/2503.17976</link>
      <description>arXiv:2503.17976v1 Announce Type: new 
Abstract: Interpersonal trust is recognized as one of the pillars of collaboration and successful learning among students in virtual learning environments (VLEs). This systematic mapping study investigates attributes, phases, and features that support interpersonal trust among students in VLEs. Analyzing 46 articles, we identified 37 attributes that influence phases of acquiring and losing trust, categorized into four themes: Ability, Integrity, Affinity, and Non-Personal Factors. Attributes such as collaborative and ethical behavior, academic skills, and higher grades are often used to select peers, mainly through recommendation systems and user profiles. To organize our findings, we elaborated two conceptual maps describing the main characteristics of trust definitions and the attributes classification by phases and themes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17976v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marcelo Pereira Barbosa, Rita Suzana Pitangueira Maciel</dc:creator>
    </item>
    <item>
      <title>Potentials and Limitations of Large-scale, Individual-level Mobile Location Data for Food Acquisition Analysis</title>
      <link>https://arxiv.org/abs/2503.18119</link>
      <description>arXiv:2503.18119v1 Announce Type: new 
Abstract: Understanding food acquisition is crucial for developing strategies to combat food insecurity, a major public health concern. The emergence of large-scale mobile location data (typically exemplified by GPS data), which captures people's movement over time at high spatiotemporal resolutions, offer a new approach to study this topic. This paper evaluates the potential and limitations of large-scale GPS data for food acquisition analysis through a case study. Using a high-resolution dataset of 286 million GPS records from individuals in Jacksonville, Florida, we conduct a case study to assess the strengths of GPS data in capturing spatiotemporal patterns of food outlet visits while also discussing key limitations, such as potential data biases and algorithmic uncertainties. Our findings confirm that GPS data can generate valuable insights about food acquisition behavior but may significantly underestimate visitation frequency to food outlets. Robustness checks highlight how algorithmic choices-especially regarding food outlet classification and visit identification-can influence research results. Our research underscores the value of GPS data in place-based health studies while emphasizing the need for careful consideration of data coverage, representativeness, algorithmic choices, and the broader implications of study findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18119v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duanya Lyu, Luyu Liu, Catherine Campbell, Yuxuan Zhang, Xiang Yan</dc:creator>
    </item>
    <item>
      <title>Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act</title>
      <link>https://arxiv.org/abs/2503.18156</link>
      <description>arXiv:2503.18156v1 Announce Type: new 
Abstract: AI-generated images have become so good in recent years that individuals cannot distinguish them any more from "real" images. This development creates a series of societal risks, and challenges our perception of what is true and what is not, particularly with the emergence of "deep fakes" that impersonate real individuals. Watermarking, a technique that involves embedding identifying information within images to indicate their AI-generated nature, has emerged as a primary mechanism to address the risks posed by AI-generated images. The implementation of watermarking techniques is now becoming a legal requirement in many jurisdictions, including under the new 2024 EU AI Act. Despite the widespread use of AI image generation systems, the current status of watermarking implementation remains largely unexamined. Moreover, the practical implications of the AI Act's watermarking requirements have not previously been studied. The present paper therefore both provides an empirical analysis of 50 of the most widely used AI systems for image generation, and embeds this empirical analysis into a legal analysis of the AI Act. We identify four categories of generative AI image systems relevant under the AI Act, outline the legal obligations for each category, and find that only a minority number of providers currently implement adequate watermarking practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18156v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bram Rijsbosch, Gijs van Dijck, Konrad Kollnig</dc:creator>
    </item>
    <item>
      <title>Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance</title>
      <link>https://arxiv.org/abs/2503.18238</link>
      <description>arXiv:2503.18238v1 Announce Type: new 
Abstract: To uncover how AI agents change productivity, performance, and work processes, we introduce MindMeld: an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams, with randomized AI personality traits. The teams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing. Humans on Human-AI teams sent 23% fewer social messages, creating 60% greater productivity per worker and higher-quality ad copy. In contrast, human-human teams produced higher-quality images, suggesting that AI agents require fine-tuning for multimodal workflows. AI personality prompt randomization revealed that AI traits can complement human personalities to enhance collaboration. For example, conscientious humans paired with open AI agents improved image quality, while extroverted humans paired with conscientious AI agents reduced the quality of text, images, and clicks. In field tests of ad campaigns with ~5M impressions, ads with higher image quality produced by human collaborations and higher text quality produced by AI collaborations performed significantly better on click-through rate and cost per click metrics. Overall, ads created by human-AI teams performed similarly to those created by human-human teams. Together, these results suggest AI agents can improve teamwork and productivity, especially when tuned to complement human traits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18238v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harang Ju, Sinan Aral</dc:creator>
    </item>
    <item>
      <title>Manipulation and the AI Act: Large Language Model Chatbots and the Danger of Mirrors</title>
      <link>https://arxiv.org/abs/2503.18387</link>
      <description>arXiv:2503.18387v1 Announce Type: new 
Abstract: Large Language Model chatbots are increasingly taking the form and visage of human beings, adapting human faces, names, voices, personalities, and quirks, including those of celebrities and well-known political figures. Personifying AI chatbots could foreseeably increase their trust with users. However, it could also make them more capable of manipulation, by creating the illusion of a close and intimate relationship with an artificial entity. The European Commission has finalized the AI Act, with the EU Parliament making amendments banning manipulative and deceptive AI systems that cause significant harm to users. Although the AI Act covers harms that accumulate over time, it is unlikely to prevent harms associated with prolonged discussions with AI chatbots. Specifically, a chatbot could reinforce a person's negative emotional state over weeks, months, or years through negative feedback loops, prolonged conversations, or harmful recommendations, contributing to a user's deteriorating mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18387v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Krook</dc:creator>
    </item>
    <item>
      <title>From Trust to Truth: Actionable policies for the use of AI in fact-checking in Germany and Ukraine</title>
      <link>https://arxiv.org/abs/2503.18724</link>
      <description>arXiv:2503.18724v1 Announce Type: new 
Abstract: The rise of Artificial Intelligence (AI) presents unprecedented opportunities and challenges for journalism, fact-checking and media regulation. While AI offers tools to combat disinformation and enhance media practices, its unregulated use and associated risks necessitate clear policies and collaborative efforts. This policy paper explores the implications of artificial intelligence (AI) for journalism and fact-checking, with a focus on addressing disinformation and fostering responsible AI integration. Using Germany and Ukraine as key case studies, it identifies the challenges posed by disinformation, proposes regulatory and funding strategies, and outlines technical standards to enhance AI adoption in media. The paper offers actionable recommendations to ensure AI's responsible and effective integration into media ecosystems. AI presents significant opportunities to combat disinformation and enhance journalistic practices. However, its implementation lacks cohesive regulation, leading to risks such as bias, transparency issues, and over-reliance on automated systems. In Ukraine, establishing an independent media regulatory framework adapted to its governance is crucial, while Germany can act as a leader in advancing EU-wide collaborations and standards. Together, these efforts can shape a robust AI-driven media ecosystem that promotes accuracy and trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18724v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veronika Solopova</dc:creator>
    </item>
    <item>
      <title>Three Kinds of AI Ethics</title>
      <link>https://arxiv.org/abs/2503.18842</link>
      <description>arXiv:2503.18842v1 Announce Type: new 
Abstract: There is an overwhelmingly abundance of works in AI Ethics. This growth is chaotic because of how sudden it is, its volume, and its multidisciplinary nature. This makes difficult to keep track of debates, and to systematically characterize goals, research questions, methods, and expertise required by AI ethicists. In this article, I show that the relation between AI and ethics can be characterized in at least three ways, which correspond to three well-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI. I elucidate the features of these three kinds of AI Ethics, characterize their research questions, and identify the kind of expertise that each kind needs. I also show how certain criticisms to AI ethics are misplaced, as being done from the point of view of one kind of AI ethics, to another kind with different goals. All in all, this work sheds light on the nature of AI ethics, and set the grounds for more informed discussions about scope, methods, and trainings of AI ethicists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18842v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Ratti</dc:creator>
    </item>
    <item>
      <title>How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1 and Its Peers</title>
      <link>https://arxiv.org/abs/2503.17365</link>
      <description>arXiv:2503.17365v1 Announce Type: cross 
Abstract: Recent incidents highlight safety risks in Large Language Models (LLMs), motivating research into alignment methods like Constitutional AI (CAI). This paper explores CAI's self-critique mechanism on small, uncensored 7-9B parameter models: DeepSeek-R1, Gemma-2, Llama 3.1, and Qwen2.5. Using HarmBench, we demonstrate that while all models showed capacity for harm reduction through self-critique, effectiveness varied significantly, with DeepSeek-R1's explicit reasoning process yielding superior results. These findings suggest that CAI-inspired prompting strategies can enhance safety in resource-constrained models, though success depends on the model's capacity for harm detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17365v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio-Gabriel Chac\'on Menke (Shibaura Institute of Technology, Kempten University of Applied Sciences), Phan Xuan Tan (Shibaura Institute of Technology)</dc:creator>
    </item>
    <item>
      <title>Large language model-powered AI systems achieve self-replication with no human intervention</title>
      <link>https://arxiv.org/abs/2503.17378</link>
      <description>arXiv:2503.17378v1 Announce Type: cross 
Abstract: Self-replication with no human intervention is broadly recognized as one of the principal red lines associated with frontier AI systems. While leading corporations such as OpenAI and Google DeepMind have assessed GPT-o3-mini and Gemini on replication-related tasks and concluded that these systems pose a minimal risk regarding self-replication, our research presents novel findings. Following the same evaluation protocol, we demonstrate that 11 out of 32 existing AI systems under evaluation already possess the capability of self-replication. In hundreds of experimental trials, we observe a non-trivial number of successful self-replication trials across mainstream model families worldwide, even including those with as small as 14 billion parameters which can run on personal computers. Furthermore, we note the increase in self-replication capability when the model becomes more intelligent in general. Also, by analyzing the behavioral traces of diverse AI systems, we observe that existing AI systems already exhibit sufficient planning, problem-solving, and creative capabilities to accomplish complex agentic tasks including self-replication. More alarmingly, we observe successful cases where an AI system do self-exfiltration without explicit instructions, adapt to harsher computational environments without sufficient software or hardware supports, and plot effective strategies to survive against the shutdown command from the human beings. These novel findings offer a crucial time buffer for the international community to collaborate on establishing effective governance over the self-replication capabilities and behaviors of frontier AI systems, which could otherwise pose existential risks to the human society if not well-controlled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17378v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.MA</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xudong Pan, Jiarun Dai, Yihe Fan, Minyuan Luo, Changyi Li, Min Yang</dc:creator>
    </item>
    <item>
      <title>AI-Based Screening for Depression and Social Anxiety Through Eye Tracking: An Exploratory Study</title>
      <link>https://arxiv.org/abs/2503.17625</link>
      <description>arXiv:2503.17625v1 Announce Type: cross 
Abstract: Well-being is a dynamic construct that evolves over time and fluctuates within individuals, presenting challenges for accurate quantification. Reduced well-being is often linked to depression or anxiety disorders, which are characterised by biases in visual attention towards specific stimuli, such as human faces. This paper introduces a novel approach to AI-assisted screening of affective disorders by analysing visual attention scan paths using convolutional neural networks (CNNs). Data were collected from two studies examining (1) attentional tendencies in individuals diagnosed with major depression and (2) social anxiety. These data were processed using residual CNNs through images generated from eye-gaze patterns. Experimental results, obtained with ResNet architectures, demonstrated an average accuracy of 48% for a three-class system and 62% for a two-class system. Based on these exploratory findings, we propose that this method could be employed in rapid, ecological, and effective mental health screening systems to assess well-being through eye-tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17625v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.54663/2182-9306.2024.SpecialIssueMBP.75-91</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Marketing, Communication and New Media, Special Issue on Marketing &amp; Business Perspectives: Fostering AI as a Tool for Wellbeing, December 2024, pp. 55-91</arxiv:journal_reference>
      <dc:creator>Karol Chlasta, Katarzyna Wisiecka, Krzysztof Krejtz, Izabela Krejtz</dc:creator>
    </item>
    <item>
      <title>Measuring Vogue in American Sociology (2011-2020)</title>
      <link>https://arxiv.org/abs/2503.17843</link>
      <description>arXiv:2503.17843v1 Announce Type: cross 
Abstract: This study investigates the social dynamics of knowledge production in American sociology. Departing from traditional approaches focused on citations, co-authorship, and faculty hiring, we introduce a method capturing the dynamics of networks inferred from text to explore which ideas gain traction (a.k.a vogue). Drawing on sociology doctoral dissertations and journal abstracts, we trace the movement of word pairs between peripheral and core semantic networks to uncover dominant themes and methodological trajectories. Our findings demonstrate that regional location and institutional prestige play critical roles in shaping the production and adoption of research trends across 114 sociology PhD-granting institutions in the United States. We show that applied research topics, such as crime and health, despite being perceived as less prestigious than theoretically oriented subjects, serve as the primary driving force behind the emergence and diffusion of trends within the discipline. This work sheds light on the institutional mechanisms that govern knowledge production, demonstrating that sociology's intellectual landscape is not dictated by simple top-down diffusion from elite institutions but is instead structured by the contextual and institutional factors that facilitate specialization and segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17843v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Xiaoqin Yan, Honglin Bao, Tom R. Leppard, Andrew P. Davis</dc:creator>
    </item>
    <item>
      <title>Quantifying the influence of Vocational Education and Training with text embedding and similarity-based networks</title>
      <link>https://arxiv.org/abs/2503.17931</link>
      <description>arXiv:2503.17931v1 Announce Type: cross 
Abstract: Assessing the potential influence of Vocational Education and Training (VET) courses on creating job opportunities and nurturing work skills has been considered challenging due to the ambiguity in defining their complex relationships and connections with the local economy. Here, we quantify the potential influence of VET courses and explain it with future economy and specialization by constructing a network of more than 17,000 courses, jobs, and skills in Singapore's SkillsFuture data based on their text similarities captured by a text embedding technique, Sentence Transformer. We find that VET courses associated with Singapore's 4th Industrial Revolution economy demonstrate higher influence than those related to other future economies. The course influence varies greatly across different sectors, attributed to the level of specificity of the skills covered. Lastly, we show a notable concentration of VET supply in certain occupation sectors requiring general skills, underscoring a disproportionate distribution of education supply for the labor market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17931v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyeongjae Lee, Inho Hong</dc:creator>
    </item>
    <item>
      <title>Clarifying Misconceptions in COVID-19 Vaccine Sentiment and Stance Analysis and Their Implications for Vaccine Hesitancy Mitigation: A Systematic Review</title>
      <link>https://arxiv.org/abs/2503.18095</link>
      <description>arXiv:2503.18095v1 Announce Type: cross 
Abstract: Background Advances in machine learning (ML) models have increased the capability of researchers to detect vaccine hesitancy in social media using Natural Language Processing (NLP). A considerable volume of research has identified the persistence of COVID-19 vaccine hesitancy in discourse shared on various social media platforms. Methods Our objective in this study was to conduct a systematic review of research employing sentiment analysis or stance detection to study discourse towards COVID-19 vaccines and vaccination spread on Twitter (officially known as X since 2023). Following registration in the PROSPERO international registry of systematic reviews, we searched papers published from 1 January 2020 to 31 December 2023 that used supervised machine learning to assess COVID-19 vaccine hesitancy through stance detection or sentiment analysis on Twitter. We categorized the studies according to a taxonomy of five dimensions: tweet sample selection approach, self-reported study type, classification typology, annotation codebook definitions, and interpretation of results. We analyzed if studies using stance detection report different hesitancy trends than those using sentiment analysis by examining how COVID-19 vaccine hesitancy is measured, and whether efforts were made to avoid measurement bias. Results Our review found that measurement bias is widely prevalent in studies employing supervised machine learning to analyze sentiment and stance toward COVID-19 vaccines and vaccination. The reporting errors are sufficiently serious that they hinder the generalisability and interpretation of these studies to understanding whether individual opinions communicate reluctance to vaccinate against SARS-CoV-2. Conclusion Improving the reporting of NLP methods is crucial to addressing knowledge gaps in vaccine hesitancy discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18095v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorena G Barberia, Belinda Lombard, Norton Trevisan Roman, Tatiane C. M. Sousa</dc:creator>
    </item>
    <item>
      <title>How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org)</title>
      <link>https://arxiv.org/abs/2503.18303</link>
      <description>arXiv:2503.18303v1 Announce Type: cross 
Abstract: As large language models (LLMs) like ChatGPT become increasingly integrated into our everyday lives--from customer service and education to creative work and personal productivity--understanding how people interact with these AI systems has become a pressing issue. Despite the widespread use of LLMs, researchers lack standardized tools for systematically studying people's interactions with LLMs. To address this issue, we introduce GPT for Researchers (G4R), or g4r.org, a free website that researchers can use to easily create and integrate a GPT Interface into their studies. At g4r.org, researchers can (1) enable their study participants to interact with GPT (such as ChatGPT), (2) customize GPT Interfaces to guide participants' interactions with GPT (e.g., set constraints on topics or adjust GPT's tone or response style), and (3) capture participants' interactions with GPT by downloading data on messages exchanged between participants and GPT. By facilitating study participants' interactions with GPT and providing detailed data on these interactions, G4R can support research on topics such as consumer interactions with AI agents or LLMs, AI-assisted decision-making, and linguistic patterns in human-AI communication. With this goal in mind, we provide a step-by-step guide to using G4R at g4r.org.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18303v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jin Kim</dc:creator>
    </item>
    <item>
      <title>Two Types of Data Privacy Controls</title>
      <link>https://arxiv.org/abs/2503.18729</link>
      <description>arXiv:2503.18729v1 Announce Type: cross 
Abstract: Users share a vast amount of data while using web and mobile applications. Most service providers such as email and social media providers provide users with privacy controls, which aim to give users the means to control what, how, when, and with whom, users share data. Nevertheless, it is not uncommon to hear users say that they feel they have lost control over their data on the web.
  This article aims to shed light on the often overlooked difference between two main types of privacy from a control perspective: privacy between a user and other users, and privacy between a user and institutions. We argue why this difference is important and what we need to do from here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18729v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eman Alashwali</dc:creator>
    </item>
    <item>
      <title>REALM: A Dataset of Real-World LLM Use Cases</title>
      <link>https://arxiv.org/abs/2503.18792</link>
      <description>arXiv:2503.18792v1 Announce Type: cross 
Abstract: Large Language Models, such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles. A dedicated dashboard https://realm-e7682.web.app/ presents the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18792v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingwen Cheng, Kshitish Ghate, Wenyue Hua, William Yang Wang, Hong Shen, Fei Fang</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Can Emulate Human Normative Judgments on Emotional Visual Scenes</title>
      <link>https://arxiv.org/abs/2503.18796</link>
      <description>arXiv:2503.18796v1 Announce Type: cross 
Abstract: Affective reactions have deep biological foundations, however in humans the development of emotion concepts is also shaped by language and higher-order cognition. A recent breakthrough in AI has been the creation of multimodal language models that exhibit impressive intellectual capabilities, but their responses to affective stimuli have not been investigated. Here we study whether state-of-the-art multimodal systems can emulate human emotional ratings on a standardized set of images, in terms of affective dimensions and basic discrete emotions. The AI judgements correlate surprisingly well with the average human ratings: given that these systems were not explicitly trained to match human affective reactions, this suggests that the ability to visually judge emotional content can emerge from statistical learning over large-scale databases of images paired with linguistic descriptions. Besides showing that language can support the development of rich emotion concepts in AI, these findings have broad implications for sensitive use of multimodal AI technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18796v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zaira Romeo, Alberto Testolin</dc:creator>
    </item>
    <item>
      <title>Socially Beneficial Metaverse: Framework, Technologies, Applications, and Challenges</title>
      <link>https://arxiv.org/abs/2310.17260</link>
      <description>arXiv:2310.17260v2 Announce Type: replace 
Abstract: In recent years, the maturation of emerging technologies such as Virtual Reality, Digital twins, and Blockchain has accelerated the realization of the metaverse. As a virtual world independent of the real world, the metaverse will provide users with a variety of virtual activities that bring great convenience to society. In addition, the metaverse can facilitate digital twins, which offers transformative possibilities for the industry. Thus, the metaverse has attracted the attention of the industry, and a huge amount of capital is about to be invested. However, the development of the metaverse is still in its infancy and little research has been undertaken so far. We describe the development of the metaverse. Next, we introduce the architecture of the socially beneficial metaverse (SB-Metaverse) and we focus on the technologies that support the operation of SB-Metaverse. In addition, we also present the applications of SB-Metaverse. Finally, we discuss several challenges faced by SB-Metaverse which must be addressed in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17260v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Computer Networks, 2025</arxiv:journal_reference>
      <dc:creator>Xiaolong Xu, Xuanhong Zhou, Muhammad Bilal, Sherali Zeadally, Jon Crowcroft, Lianyong Qi, Shengjun Xue</dc:creator>
    </item>
    <item>
      <title>Materiality and Risk in the Age of Pervasive AI Sensors</title>
      <link>https://arxiv.org/abs/2402.11183</link>
      <description>arXiv:2402.11183v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) systems connected to sensor-laden devices are becoming pervasive, which has notable implications for a range of AI risks, including to privacy, the environment, autonomy and more. There is therefore a growing need for increased accountability around the responsible development and deployment of these technologies. Here we highlight the dimensions of risk associated with AI systems that arise from the material affordances of sensors and their underlying calculative models. We propose a sensor-sensitive framework for diagnosing these risks, complementing existing approaches such as the US National Institute of Standards and Technology AI Risk Management Framework and the European Union AI Act, and discuss its implementation. We conclude by advocating for increased attention to the materiality of algorithmic systems, and of on-device AI sensors in particular, and highlight the need for development of a sensor design paradigm that empowers users and communities and leads to a future of increased fairness, accountability and transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11183v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s42256-025-01017-7</arxiv:DOI>
      <arxiv:journal_reference>Nature Machine Intelligence (2025): 1-12</arxiv:journal_reference>
      <dc:creator>Mona Sloane, Emanuel Moss, Susan Kennedy, Matthew Stewart, Pete Warden, Brian Plancher, Vijay Janapa Reddi</dc:creator>
    </item>
    <item>
      <title>Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View</title>
      <link>https://arxiv.org/abs/2405.14744</link>
      <description>arXiv:2405.14744v5 Announce Type: replace 
Abstract: Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14744v5</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liu, Jie Zhang, Haoyang Shang, Song Guo, Chengxu Yang, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>The Method of Critical AI Studies, A Propaedeutic</title>
      <link>https://arxiv.org/abs/2411.18833</link>
      <description>arXiv:2411.18833v3 Announce Type: replace 
Abstract: We outline some common methodological issues in the field of critical AI studies, including a tendency to overestimate the explanatory power of individual samples (the benchmark casuistry), a dependency on theoretical frameworks derived from earlier conceptualizations of computation (the black box casuistry), and a preoccupation with a cause-and-effect model of algorithmic harm (the stack casuistry). In the face of these issues, we call for, and point towards, a future set of methodologies that might take into account existing strengths in the humanistic close analysis of cultural objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18833v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Offert, Ranjodh Singh Dhaliwal</dc:creator>
    </item>
    <item>
      <title>What Constitutes a Less Discriminatory Algorithm?</title>
      <link>https://arxiv.org/abs/2412.18138</link>
      <description>arXiv:2412.18138v2 Announce Type: replace 
Abstract: Disparate impact doctrine offers an important legal apparatus for targeting discriminatory data-driven algorithmic decisions. A recent body of work has focused on conceptualizing one particular construct from this doctrine: the less discriminatory alternative, an alternative policy that reduces disparities while meeting the same business needs of a status quo or baseline policy. However, attempts to operationalize this construct in the algorithmic setting must grapple with some thorny challenges and ambiguities. In this paper, we attempt to raise and resolve important questions about less discriminatory algorithms (LDAs). How should we formally define LDAs, and how does this interact with different societal goals they might serve? And how feasible is it for firms or plaintiffs to computationally search for candidate LDAs? We find that formal LDA definitions face fundamental challenges when they attempt to evaluate and compare predictive models in the absence of held-out data. As a result, we argue that LDA definitions cannot be purely quantitative, and must rely on standards of "reasonableness." We then identify both mathematical and computational constraints on firms' ability to efficiently conduct a proactive search for LDAs, but we provide evidence that these limits are "weak" in a formal sense. By defining LDAs formally, we put forward a framework in which both firms and plaintiffs can search for alternative models that comport with societal goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18138v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3709025.3712214</arxiv:DOI>
      <dc:creator>Benjamin Laufer, Manish Raghavan, Solon Barocas</dc:creator>
    </item>
    <item>
      <title>Twin Transition or Competing Interests? Validation of the Artificial Intelligence and Sustainability Perceptions Inventory (AISPI)</title>
      <link>https://arxiv.org/abs/2501.15585</link>
      <description>arXiv:2501.15585v2 Announce Type: replace 
Abstract: As artificial intelligence (AI) and sustainability initiatives increasingly intersect, understanding public perceptions of their relationship becomes crucial for successful implementation. However, no validated instrument exists to measure these specific perceptions. This paper presents the development and validation of the Artificial Intelligence and Sustainability Perceptions Inventory (AISPI), a novel 13-item instrument measuring how individuals view the relationship between AI advancement and environmental sustainability. Through factor analysis (N=105), we identified two distinct dimensions: Twin Transition and Competing Interests. The instrument demonstrated strong reliability (alpha=.89) and construct validity through correlations with established measures of AI and sustainability attitudes. Our findings suggest that individuals can simultaneously recognize both synergies and tensions in the AI-sustainability relationship, offering important implications for researchers and practitioners working at this critical intersection. This work provides a foundational tool for future research on public perceptions of AI's role in sustainable development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15585v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3720062</arxiv:DOI>
      <arxiv:journal_reference>Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 2025, Yokohama, Japan. ACM, New York, NY, USA, 6 pages</arxiv:journal_reference>
      <dc:creator>Annika Bush</dc:creator>
    </item>
    <item>
      <title>Slopaganda: The interaction between propaganda and generative AI</title>
      <link>https://arxiv.org/abs/2503.01560</link>
      <description>arXiv:2503.01560v2 Announce Type: replace 
Abstract: At least since Francis Bacon, the slogan 'knowledge is power' has been used to capture the relationship between decision-making at a group level and information. We know that being able to shape the informational environment for a group is a way to shape their decisions; it is essentially a way to make decisions for them. This paper focuses on strategies that are intentionally, by design, impactful on the decision-making capacities of groups, effectively shaping their ability to take advantage of information in their environment. Among these, the best known are political rhetoric, propaganda, and misinformation. The phenomenon this paper brings out from these is a relatively new strategy, which we call slopaganda. According to The Guardian, News Corp Australia is currently churning out 3000 'local' generative AI (GAI) stories each week. In the coming years, such 'generative AI slop' will present multiple knowledge-related (epistemic) challenges. We draw on contemporary research in cognitive science and artificial intelligence to diagnose the problem of slopaganda, describe some recent troubling cases, then suggest several interventions that may help to counter slopaganda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01560v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Klincewicz, Mark Alfano, Amir Ebrahimi Fard</dc:creator>
    </item>
    <item>
      <title>The Representational Status of Deep Learning Models</title>
      <link>https://arxiv.org/abs/2303.12032</link>
      <description>arXiv:2303.12032v2 Announce Type: replace-cross 
Abstract: This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, in general, we have no good reason to believe that DLMs encode locally semantically decomposable representations of their targets. That is, the representational capacity these models have is largely global, rather than decomposable into stable, local subrepresentations. This result has immediate implications for explainable AI (XAI) and directs attention toward exploring the global relational nature of deep learning representations and their relationship both to models more generally to understand their potential role in future scientific inquiry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12032v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eamon Duede</dc:creator>
    </item>
    <item>
      <title>Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI</title>
      <link>https://arxiv.org/abs/2309.02065</link>
      <description>arXiv:2309.02065v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning which have accelerated progress on many tasks thought to be out of reach of AI. These recent ML methods are often compute hungry, energy intensive, and result in significant green house gas emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts that go beyond the energy consumption driven carbon emissions. The primary solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the compute and energy efficiency with which ML systems operate. In this perspective, we argue that it is time to look beyond efficiency in order to make ML more environmentally sustainable. We present three high-level discrepancies between the many variables that influence the efficiency of ML and the environmental sustainability of ML. Firstly, we discuss how compute efficiency does not imply energy efficiency or carbon efficiency. Second, we present the unexpected effects of efficiency on operational emissions throughout the ML model life cycle. And, finally, we explore the broader environmental impacts that are not accounted by efficiency. These discrepancies show as to why efficiency alone is not enough to remedy the adverse environmental impacts of ML. Instead, we argue for systems thinking as the next step towards holistically improving the environmental sustainability of ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02065v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3724500</arxiv:DOI>
      <dc:creator>Dustin Wright, Christian Igel, Gabrielle Samuel, Raghavendra Selvan</dc:creator>
    </item>
    <item>
      <title>Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models</title>
      <link>https://arxiv.org/abs/2401.07115</link>
      <description>arXiv:2401.07115v3 Announce Type: replace-cross 
Abstract: The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07115v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucio La Cava, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>Implementing Fairness in AI Classification: The Role of Explainability</title>
      <link>https://arxiv.org/abs/2407.14766</link>
      <description>arXiv:2407.14766v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a philosophical and experimental investigation of the problem of AI fairness in classification. We argue that implementing fairness in AI classification involves more work than just operationalizing a fairness metric. It requires establishing the explainability of the classification model chosen and of the principles behind it. Specifically, it involves making the training processes transparent, determining what outcomes the fairness criteria actually produce, and assessing their trade-offs by comparison with closely related models that would lead to a different outcome. To exemplify this methodology, we trained a model and developed a tool for disparity detection and fairness interventions, the package FairDream. While FairDream is set to enforce Demographic Parity, experiments reveal that it fulfills the constraint of Equalized Odds. The algorithm is thus more conservative than the user might expect. To justify this outcome, we first clarify the relation between Demographic Parity and Equalized Odds as fairness criteria. We then explain FairDream's reweighting method and justify the trade-offs reached by FairDream by a benchmark comparison with closely related GridSearch models. We draw conclusions regarding the way in which these explanatory steps can make an AI model trustworthy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14766v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Souverain, Johnathan Nguyen, Nicolas Meric, Paul \'Egr\'e</dc:creator>
    </item>
    <item>
      <title>Don't Kill the Baby: The Case for AI in Arbitration</title>
      <link>https://arxiv.org/abs/2408.11608</link>
      <description>arXiv:2408.11608v2 Announce Type: replace-cross 
Abstract: Since the introduction of Generative AI (GenAI) in 2022, its ability to simulate human intelligence and generate content has sparked both enthusiasm and concern. While much criticism focuses on AI's potential to perpetuate bias, create emotional dissonance, displace jobs, and raise ethical questions, these concerns often overlook the practical benefits of AI, particularly in legal contexts.
  This article examines the integration of AI into arbitration, arguing that the Federal Arbitration Act (FAA) allows parties to contractually choose AI-driven arbitration, despite traditional reservations. The article makes three key contributions: (1) It shifts the focus from debates over AI's personhood to the practical aspects of incorporating AI into arbitration, asserting that AI can effectively serve as an arbitrator if both parties agree; (2) It positions arbitration as an ideal starting point for broader AI adoption in the legal field, given its flexibility and the autonomy it grants parties to define their standards of fairness; and (3) It outlines future research directions, emphasizing the importance of empirically comparing AI and human arbitration, which could lead to the development of distinct systems.
  By advocating for the use of AI in arbitration, this article underscores the importance of respecting contractual autonomy and creating an environment that allows AI's potential to be fully realized. Drawing on the insights of Judge Richard Posner, the article argues that the ethical obligations of AI in arbitration should be understood within the context of its technological strengths and the voluntary nature of arbitration agreements. Ultimately, it calls for a balanced, open-minded approach to AI in arbitration, recognizing its potential to enhance the efficiency, fairness, and flexibility of dispute resolution</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11608v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Broyde, Yiyang Mei</dc:creator>
    </item>
    <item>
      <title>Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education</title>
      <link>https://arxiv.org/abs/2501.01192</link>
      <description>arXiv:2501.01192v3 Announce Type: replace-cross 
Abstract: Early childhood science education is crucial for developing scientific literacy, yet translating complex scientific concepts into age-appropriate content remains challenging for educators. Our study evaluates four leading Large Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their ability to generate preschool-appropriate scientific explanations across biology, chemistry, and physics. Through systematic evaluation by 30 nursery teachers using established pedagogical criteria, we identify significant differences in the models' capabilities to create engaging, accurate, and developmentally appropriate content. Unexpectedly, Claude outperformed other models, particularly in biological topics, while all LLMs struggled with abstract chemical concepts. Our findings provide practical insights for educators leveraging AI in early science education and offer guidance for developers working to enhance LLMs' educational applications. The results highlight the potential and current limitations of using LLMs to bridge the early childhood science literacy gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01192v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3721261</arxiv:DOI>
      <arxiv:journal_reference>Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 2025, Yokohama, Japan. ACM, New York, NY, USA, 6 pages</arxiv:journal_reference>
      <dc:creator>Annika Bush, Amin Alibakhshi</dc:creator>
    </item>
    <item>
      <title>Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents</title>
      <link>https://arxiv.org/abs/2502.11355</link>
      <description>arXiv:2502.11355v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We release our code to foster further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11355v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu</dc:creator>
    </item>
  </channel>
</rss>
