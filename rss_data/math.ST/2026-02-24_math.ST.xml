<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 02:45:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Selection Premium Decomposition for the Expected Maximum of Random Walks</title>
      <link>https://arxiv.org/abs/2602.19481</link>
      <description>arXiv:2602.19481v1 Announce Type: new 
Abstract: When $K$ models are evaluated on the same validation set of size $n$, the selected winner's apparent performance is biased upward. Suppose $K$ models are evaluated on a shared sequence of i.i.d. observations $X_1,\dots, X_n$, where model $k$ achieves response $f_k(X_i)$ with mean $\mu_k = \mathbb E[f_k(X)]$. Writing $Y_{i,k} = f_k(X_i)-\mu_k$ for the centered increment and $S_{n,k} = \sum_{i=1}^n Y_{i,k}$ for the centered cumulative score, the expected maximum satisfies $0\le\mathbb E\bigl[\max_k S_{n,k}\bigr] = \sum_{i=1}^n \mathbb E\bigl[\varphi_K(S_{i-1})\bigr]$ where $\varphi_K(u) = \mathbb{E}\bigl[\max_k(u_k + Y_k)\bigr] - \max_k u_k$, $u\in \mathbb R^K$, is the selection premium function. This formula corresponds to the null hypothesis case (all models are equal in the sense that they have the same mean), which clarifies that the bias arises from selection. While this decomposition follows from elementary conditioning and telescoping, we develop the analytical consequences in five directions. (i) structural properties of $\varphi_K$; (ii) extension to stopping times, recovering Wald's equation at $K=1$; (iii) a winner's curse decomposition for heterogeneous means; (iv) a universal bias concentration law showing that the first $\alpha$-fraction of observations generates a $\sqrt\alpha$-fraction of total bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19481v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor H. de la Pena, Fangyuan Lin, Victor K. de la Pena</dc:creator>
    </item>
    <item>
      <title>On Expectation Propagation and the Probabilistic Editor in some simple mixture problems</title>
      <link>https://arxiv.org/abs/2602.19709</link>
      <description>arXiv:2602.19709v1 Announce Type: new 
Abstract: As for other latent-variable problems, exact Bayesian analysis is typically not practicable for mixture problems and approximate methods have been developed. Variational Bayes tends to produce approximate posterior distributions for parameters that are too tightly concentrated in having variances that are too small. The paper identifies a few mixture problems in which Expectation Propagation and variations thereof lead to approximate posterior distributions that asymptotically exhibit `correct' variances and therefore stand to provide reliable interval estimates for the unknown parameter or parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19709v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Lid Hjort, Mike Titterington</dc:creator>
    </item>
    <item>
      <title>From Asymptotic to Finite-Sample Minimax Robust Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2602.19803</link>
      <description>arXiv:2602.19803v1 Announce Type: new 
Abstract: This paper establishes a formal connection between finite-sample and asymptotically minimax robust hypothesis testing under distributional uncertainty. It is shown that, whenever a finite-sample minimax robust test exists, it coincides with the solution of the corresponding asymptotic minimax problem. This result enables the analytical derivation of finite-sample minimax robust tests using asymptotic theory, bypassing the need for heuristic constructions. The total variation distance and band model are examined as representative uncertainty classes. For each, the least favorable distributions and corresponding robust likelihood ratio functions are derived in parametric form. In the total variation case, the new derivation generalizes earlier results by allowing unequal robustness parameters. The theory also explains and systematizes previously heuristic designs. Simulations are provided to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19803v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"okhan G\"ul</dc:creator>
    </item>
    <item>
      <title>Addressing parity blindness of data-driven Sobolev tests on the hypersphere</title>
      <link>https://arxiv.org/abs/2602.19839</link>
      <description>arXiv:2602.19839v1 Announce Type: new 
Abstract: We study the asymptotic behavior of the data-driven Sobolev test for testing uniformity on the (hyper)sphere. We show that it can be blind to certain contiguous alternatives and propose a simple modification of the test statistic. This adapted test retains consistency under fixed alternatives and achieves non-trivial asymptotic power against contiguous alternatives for which the original test fails. Simulation results support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19839v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcio Reverbel</dc:creator>
    </item>
    <item>
      <title>Order Dependence in the Moving-Range Sigma Estimator: A Total-Variance Decomposition</title>
      <link>https://arxiv.org/abs/2602.20007</link>
      <description>arXiv:2602.20007v1 Announce Type: new 
Abstract: In Individuals and Moving Range (I-MR) charts, the process standard deviation is often estimated by the span-2 average moving range, scaled by the usual constant $d_2$. Unlike the sample standard deviation, this estimator depends on the observation order: permuting the values can change the average moving range. We make this dependence explicit by modeling the order as an independent uniformly random permutation. A direct application of the law of total variance then decomposes its variance into a component due to ordering and a component due to the realized values. Averaging over all permutations yields a simple order-invariant baseline for the moving-range estimator: the sample Gini mean difference divided by $d_2$. Simulations quantify the resulting fraction of variance attributable to ordering under i.i.d. Normal sampling, and two NIST examples illustrate a typical ordering and an ordering with strong serial structure relative to random permutations of the same values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20007v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew T. Karl</dc:creator>
    </item>
    <item>
      <title>Estimators of different delta coefficients based on the unbiased estimator of the expected proportions of agreements</title>
      <link>https://arxiv.org/abs/2602.20071</link>
      <description>arXiv:2602.20071v1 Announce Type: new 
Abstract: To measure the degree of agreement between two observers that independently classify $n$ subjects within $K$ categories, it is common to use different kappa type coefficients, the most common of which is the $\kappa_C$ coefficient (Cohen's kappa). As $\kappa_C$ has some weaknesses -such as its poor performance with highly unbalanced marginal distributions-, the $\Delta$ coefficient is sometimes used, based on the $delta$ response model. This model allows us to obtain other parameters like: (a) the $\alpha_i$ contribution of each $i$ category to the value of the global agreement $\Delta=\sum \alpha_i$; and (b) the consistency $\mathcal{S}_i$ in the category $i$ (degree of agreement in the category $i$), a more appropriate parameter than the kappa value obtained by collapsing the data into the category $i$. It has recently been shown that the classic estimator $\hat{\kappa}_C$ underestimates $\kappa_C$, having obtained a new estimator $\hat{\kappa}_{CU}$ which is less biased. This article demonstrates that something similar happens to the known estimators $\hat{\Delta}$, $\hat{\alpha}_i$, and $\hat{\mathcal{S}}_i$ of $\Delta$, $\alpha_i$ and $\mathcal{S}_i$ (respectively), proposes new and less biased estimators $\hat{\Delta}_U$, $\hat{\alpha}_{iU}$, and $\hat{\mathcal{S}}_{iU}$, determines their variances, analyses the behaviour of all estimators, and concludes that the new estimators should be used when $n$ or $K$ are small (at least when $n\leq 50$ or $K\leq 3$). Additionally, the case where one of the raters is a gold standard is contemplated, in which situation two new parameters arise: the $conformity$ (the rater's capability to recognize a subject in the category $i$) and the $predictivity$ (the reliability of a response $i$ by the rater).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20071v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>A. Mart\'in Andr\'es, M. \'Alvarez Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Compound decisions and empirical Bayes via Bayesian nonparametrics</title>
      <link>https://arxiv.org/abs/2602.20115</link>
      <description>arXiv:2602.20115v1 Announce Type: new 
Abstract: We study the Gaussian sequence compound decision problem and analyze a Bayesian nonparametric estimator from an empirical Bayes, regret-based perspective. Motivated by sharp results for the classical nonparametric maximum likelihood estimator (NPMLE), we ask whether an analogous guarantee can be obtained using a standard Bayesian nonparametric prior. We show that a Dirichlet-process-based Bayesian procedure achieves near-optimal regret bounds. Our main results are stated in the compound decision framework, where the mean vector is treated as fixed, while we also provide parallel guarantees under a hierarchical model in which the means are drawn from a true unknown prior distribution. The posterior mean Bayes rule is, a fortiori, admissible, whereas we show that the NPMLE plug-in rule is inadmissible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20115v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Sid Kankanala</dc:creator>
    </item>
    <item>
      <title>Minimally Discrete and Minimally Randomized p-Values</title>
      <link>https://arxiv.org/abs/2602.18656</link>
      <description>arXiv:2602.18656v1 Announce Type: cross 
Abstract: In meta analysis, multiple hypothesis testing and many other methods, p-values are utilized as inputs and assumed to be uniformly distributed over the unit interval under the null hypotheses. If data used to generate p-values have discrete distributions then either natural, mid- or randomized p-values are typically utilized. Natural and mid-p-values can allow for valid, albeit conservative, downstream methods since under the null hypothesis they are dominated by uniform distributions in the stochastic and convex order, respectively. Randomized p-values need not lead to conservative procedures since they permit a uniform distributions under the null hypotheses through the generation of independent auxiliary variates. However, the auxiliary variates necessarily add variation to procedures. This manuscript introduces and studies ``minimally discrete'' (MD) natural p-values, MD mid-p-values and ``minimally randomized'' (MR) p-values. It is shown that MD p-values dominate their non-MD counterparts in the stochastic and convex order, and hence lead to less conservative, yet still valid, downstream methods. Likewise, MR p-values dominate their non-MR counterparts in that they are still uniformly distributed under the null hypotheses, but the added variation attributable to the independently generated auxiliary variate is smaller. It is anticipated that results here will facilitate the construction of new meta-analysis and multiple testing methods via more efficient p-value construction, and facilitate theoretical study of existing and new methods by establishing gold standards for addressing the unavoidable detrimental ``discreteness effect''.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18656v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Habiger, Pratyaydipta Rudra</dc:creator>
    </item>
    <item>
      <title>Optimal and Structure-Adaptive CATE Estimation with Kernel Ridge Regression</title>
      <link>https://arxiv.org/abs/2602.18958</link>
      <description>arXiv:2602.18958v2 Announce Type: cross 
Abstract: We propose an optimal algorithm for estimating conditional average treatment effects (CATEs) when response functions lie in a reproducing kernel Hilbert space (RKHS). We study settings in which the contrast function is structurally simpler than the nuisance functions: (i) it lies in a lower-complexity RKHS with faster eigenvalue decay, (ii) it satisfies a source condition relative to the nuisance kernel, or (iii) it depends on a known low-dimensional covariate representation. We develop a unified two-stage kernel ridge regression (KRR) method that attains minimax rates governed by the complexity of the contrast function rather than the nuisance class, in terms of both sample size and overlap. We also show that a simple model-selection step over candidate contrast spaces and regularization levels yields an oracle inequality, enabling adaptation to unknown CATE regularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18958v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seok-Jin Kim</dc:creator>
    </item>
    <item>
      <title>Robust Predictive Uncertainty and Double Descent in Contaminated Bayesian Random Features</title>
      <link>https://arxiv.org/abs/2602.19126</link>
      <description>arXiv:2602.19126v1 Announce Type: cross 
Abstract: We propose a robust Bayesian formulation of random feature (RF) regression that accounts explicitly for prior and likelihood misspecification via Huber-style contamination sets. Starting from the classical equivalence between ridge-regularized RF training and Bayesian inference with Gaussian priors and likelihoods, we replace the single prior and likelihood with $\epsilon$- and $\eta$-contaminated credal sets, respectively, and perform inference using pessimistic generalized Bayesian updating. We derive explicit and tractable bounds for the resulting lower and upper posterior predictive densities. These bounds show that, when contamination is moderate, prior and likelihood ambiguity effectively acts as a direct contamination of the posterior predictive distribution, yielding uncertainty envelopes around the classical Gaussian predictive. We introduce an Imprecise Highest Density Region (IHDR) for robust predictive uncertainty quantification and show that it admits an efficient outer approximation via an adjusted Gaussian credible interval. We further obtain predictive variance bounds (under a mild truncation approximation for the upper bound) and prove that they preserve the leading-order proportional-growth asymptotics known for RF models. Together, these results establish a robustness theory for Bayesian random features: predictive uncertainty remains computationally tractable, inherits the classical double-descent phase structure, and is improved by explicit worst-case guarantees under bounded prior and likelihood misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19126v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Caprio, Katerina Papagiannouli, Siu Lun Chau, Sayan Mukherjee</dc:creator>
    </item>
    <item>
      <title>Distributional Discontinuity Design</title>
      <link>https://arxiv.org/abs/2602.19290</link>
      <description>arXiv:2602.19290v1 Announce Type: cross 
Abstract: Regression discontinuity and kink designs are typically analyzed through mean effects, even when treatment changes the shape of the entire outcome distribution. To address this, we introduce distributional discontinuity designs, a framework for estimating causal effects for a scalar outcome at the boundary of a discontinuity in treatment assignment. Our estimand is the Wasserstein distance between limiting conditional outcome distributions; a single scale-interpretable measure of distribution shift. We show that this weakly bounds the average treatment effect, where equality holds if and only if the treatment effect is purely additive; thus, departure from equality measures effect heterogeneity. To further encode effect heterogeneity we show that the Wasserstein distance admits an orthogonal decomposition into squared differences in $L$-moments, thereby quantifying the contribution from location, scale, skewness, and higher-order shape components to the overall distributional distance. Next, we extend this framework to distributional kink designs by evaluating the Wasserstein derivative at a policy kink; this describes the flow of probability mass through the kink. In the case of fuzzy kink designs, we derive new identification results. Finally, we apply our methods on real data by re-analyzing two natural experiments to compare our distributional effects to traditional causal estimands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19290v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Zero Variance Portfolio</title>
      <link>https://arxiv.org/abs/2602.19462</link>
      <description>arXiv:2602.19462v1 Announce Type: cross 
Abstract: When the number of assets is larger than the sample size, the minimum variance portfolio interpolates the training data, delivering pathological zero in-sample variance. We show that if the weights of the zero variance portfolio are learned by a novel ``Ridgelet'' estimator, in a new test data this portfolio enjoys out-of-sample generalizability. It exhibits the double descent phenomenon and can achieve optimal risk in the overparametrized regime when the number of assets dominates the sample size. In contrast, a ``Ridgeless'' estimator which invokes the pseudoinverse fails in-sample interpolation and diverges away from out-of-sample optimality. Extensive simulations and empirical studies demonstrate that the Ridgelet method performs competitively in high-dimensional portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19462v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yi Ding, Zhentao Shi, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>The generalized underlap coefficient with an application in clustering</title>
      <link>https://arxiv.org/abs/2602.19473</link>
      <description>arXiv:2602.19473v1 Announce Type: cross 
Abstract: Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables. We establish key properties of UNL and provide an explicit connection to the total variation. We further interpret the UNL as a dependence measure between a group label and variables of interest and compare it with mutual information. We propose an importance sampling estimator of the UNL that can be combined with flexible density estimators. The utility of the UNL for assessing partition-covariate dependence in clustering is highlighted in detail, where it is particularly useful for evaluating the single-weights assumption in covariate-dependent mixture models. Finally we illustrate the application of the UNL in clustering using two real world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19473v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxi Zhang, Vanda Inacio, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Optimality of the Half-Order Exponent in the Turing-Good Identities for Bayes Factors</title>
      <link>https://arxiv.org/abs/2602.19838</link>
      <description>arXiv:2602.19838v1 Announce Type: cross 
Abstract: Bayes factors are widely computed by Monte Carlo, yet heavy-tailed sampling distributions can make numerical validation unreliable. The Turing--Good identities provide exact moment equalities for powers of a Bayes factor (a density ratio). When these identities are used as Good-check diagnostics, the power choice becomes a statistical design parameter. We develop a nonasymptotic variance theory for Monte Carlo evaluation of the identities and show that the half-order (square-root) power is uniquely minimax-stable: it equalizes variability across the two model orientations and is the only choice that guarantees finite second moments in a distribution-free worst-case sense over all mutually absolutely continuous model pairs. This yields a balanced two-sample half-order diagnostic that is symmetric in model labeling and has a uniform variance bound at fixed computational budget; in small-overlap regimes it is guaranteed to be no less efficient than the standard one-sided Turing check. Simulations for binomial Bayes factor workflows illustrate stable finite-sample behavior and sensitivity to simulator--evaluator mismatches. We further connect the half-order overlap viewpoint to stable primitives for normalizing-constant ratios and importance-sampling degeneracy summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19838v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kensuke Okada</dc:creator>
    </item>
    <item>
      <title>Adaptation to Intrinsic Dependence in Diffusion Language Models</title>
      <link>https://arxiv.org/abs/2602.20126</link>
      <description>arXiv:2602.20126v1 Announce Type: cross 
Abstract: Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\widetilde O(\mathsf{TC}/K)$ and $\widetilde O(\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\mathsf{TC}$ and $\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K&lt;L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20126v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunxiao Zhao, Changxiao Cai</dc:creator>
    </item>
    <item>
      <title>Conformal Risk Control for Non-Monotonic Losses</title>
      <link>https://arxiv.org/abs/2602.20151</link>
      <description>arXiv:2602.20151v1 Announce Type: cross 
Abstract: Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20151v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos</dc:creator>
    </item>
    <item>
      <title>Convergence rates for estimating multivariate scale mixtures of uniform densities</title>
      <link>https://arxiv.org/abs/2410.10251</link>
      <description>arXiv:2410.10251v2 Announce Type: replace 
Abstract: The Grenander estimator is a well-studied procedure for univariate nonparametric density estimation. It is usually defined as the Maximum Likelihood Estimator (MLE) over the class of all non-increasing densities on the positive real line. It can also be seen as the MLE over the class of all scale mixtures of uniform densities. Using the latter viewpoint, Pavlides and Wellner~\cite{pavlides2012nonparametric} proposed a multivariate extension of the Grenander estimator as the nonparametric MLE over the class of all multivariate scale mixtures of uniform densities. We prove that this multivariate estimator achieves the univariate cube root rate of convergence with only a logarithmic multiplicative factor that depends on the dimension. The usual curse of dimensionality is therefore avoided to some extent for this multivariate estimator. This result positively resolves a conjecture of Pavlides and Wellner~\cite{pavlides2012nonparametric} under an additional lower bound assumption. Our proof proceeds via a general accuracy result for the Hellinger accuracy of MLEs over convex classes of densities. We also provide algorithms for computing the estimator, and illustrate performance on real and simulated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10251v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/25-EJS2426</arxiv:DOI>
      <arxiv:journal_reference>Electron. J. Statist. 19(2): 3771-3834 (2025)</arxiv:journal_reference>
      <dc:creator>Arlene K. H. Kim, Gil Kur, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Estimating quantile treatments without strict overlap</title>
      <link>https://arxiv.org/abs/2506.18215</link>
      <description>arXiv:2506.18215v2 Announce Type: replace 
Abstract: We consider the problem of estimating quantile treatment effects without assuming strict overlap , i.e., we do not assume that the propensity score is bounded away from zero. More specifically, we consider an inverse probability weighting (IPW) approach for estimating quantiles in the potential outcomes framework and pay special attention to scenarios where the propensity scores can tend to zero as a regularly varying function. Our approach effectively considers a heavy-tailed objective function for estimating the quantile process. We introduce a truncated IPW estimator that is shown to outperform the standard quantile IPW estimator when strict overlap does not hold. We show that the limiting distribution of the estimated quantile process follows an infinitely divisible law and converges at the rate $n^{1-1/\gamma}$, where $\gamma&gt;1$ is the tail index of the propensity scores when they tend to zero. We propose a practical, data-driven procedure for selecting the truncation parameter, grounded in our asymptotic theory. The performance of our estimators is illustrated in numerical experiments and in a dataset that exhibits the presence of extreme propensity scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18215v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Avella-Medina, Richard Davis, Gennady Samorodnitsky</dc:creator>
    </item>
    <item>
      <title>On the Study of Weighted Fractional Cumulative Residual Inaccuracy and its Dynamical Version with Applications</title>
      <link>https://arxiv.org/abs/2506.22975</link>
      <description>arXiv:2506.22975v2 Announce Type: replace 
Abstract: In recent years, there has been a growing interest in information measures that quantify inaccuracy and uncertainty in systems. In this paper, we introduce a novel concept called the Weighted Fractional Cumulative Residual Inaccuracy (WFCRI). We develop several fundamental properties of WFCRI and establish important bounds that reveal its analytical behavior. Further, we examine the behavior of WFCRI under a mixture hazard model. A dynamic version of WFCRI also proposed and studied its behavior under proportional hazard rate model. An empirical estimation method for WFCRI under the proportional hazard rate model framework is also proposed, and its performance is evaluated through simulation studies. Finally, we demonstrate the utility of WFCRI measure in characterizing chaotic dynamics by applying it to the Ricker and cubic maps. The proposed measure is also applied to real data to assess the uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22975v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Pandey, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>Post-reduction inference for confidence sets of models</title>
      <link>https://arxiv.org/abs/2507.10373</link>
      <description>arXiv:2507.10373v2 Announce Type: replace 
Abstract: Sparsity in a regression context makes the model itself an object of interest, pointing to a confidence set of models as the appropriate presentation of evidence. A difficulty in areas such as genomics, where the number of candidate variables is vast, arises from the need for preliminary reduction prior to the assessment of models. The present paper considers a resolution using inferential separations fundamental to the Fisherian approach to conditional inference, namely, the sufficiency/co-sufficiency separation, and the ancillary/co-ancillary separation. The advantage of these separations is that no direction for departure from any hypothesised model is needed, avoiding issues that would otherwise arise from using the same data for reduction and for model assessment. In idealised cases with no nuisance parameters, the separations extract all the information in the data solely for the purpose for which it is useful, without loss or redundancy. The extent to which estimation of nuisance parameters affects the idealised information extraction is illustrated in detail for the normal-theory linear regression model, extending immediately to a log-normal accelerated-life model for time-to-event outcomes. This idealised analysis provides insight into when sample-splitting is likely to perform as well as, or better than, the co-sufficient or ancillary tests, and when it may be unreliable. The considerations involved in extending the detailed implementation to canonical exponential-family and more general regression models are briefly discussed. As part of the analysis for the Gaussian model, we introduce a modified version of the refitted cross-validation estimator of Fan et al. (2012), whose distribution theory is tractable in the appropriate conditional sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10373v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heather Battey, Daniel Garcia Rasines, Yanbo Tang</dc:creator>
    </item>
    <item>
      <title>A New Class of Asymptotically Distribution-Free Smooth Tests</title>
      <link>https://arxiv.org/abs/2508.01973</link>
      <description>arXiv:2508.01973v3 Announce Type: replace 
Abstract: This article demonstrates how recent developments in the theory of empirical processes allow us to construct a new family of asymptotically distribution-free smooth tests. Their distribution-free property is preserved even when the parameters are estimated, model selection is performed, and the sample size is only moderately large. A computationally efficient alternative to the classical parametric bootstrap is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01973v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyu Zhang, Sara Algeri</dc:creator>
    </item>
    <item>
      <title>Composite goodness-of-fit test with the Kernel Stein Discrepancy and a bootstrap for degenerate U-statistics with estimated parameters</title>
      <link>https://arxiv.org/abs/2510.22792</link>
      <description>arXiv:2510.22792v2 Announce Type: replace 
Abstract: This paper formally derives the asymptotic distribution of a goodness-of-fit test based on the Kernel Stein Discrepancy introduced in (Oscar Key et al., "Composite Goodness-of-fit Tests with Kernels", Journal of Machine Learning Research 26.51 (2025), pp. 1-60). The test enables the simultaneous estimation of the optimal parameter within a parametric family of candidate models. Its asymptotic distribution is shown to be a weighted sum of infinitely many $\chi^2$-distributed random variables plus an additional disturbance term, which is due to the parameter estimation. Further, we provide a general framework to bootstrap degenerate parameter-dependent $U$-statistics and use it to derive a new Kernel Stein Discrepancy composite goodness-of-fit test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22792v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Br\"uck, Veronika Reimoser, Fabian Baier</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Asymptotics of Differentially Private PCA</title>
      <link>https://arxiv.org/abs/2511.07270</link>
      <description>arXiv:2511.07270v3 Announce Type: replace 
Abstract: In differential privacy, random noise is introduced to privatize summary statistics of a sensitive dataset before releasing them. The noise level determines the privacy loss, which quantifies how easily an adversary can detect a target individual's presence in the dataset using the published statistic. Most privacy analyses provide upper bounds on the privacy loss. Sometimes, these bounds offer weak privacy guarantees unless the noise level is so high that it overwhelms the meaningful signal. It is unclear whether such high noise levels are necessary or a limitation of loose and pessimistic privacy bounds. This paper explores whether it is possible to obtain sharp privacy characterizations that determine the exact privacy loss of a mechanism on a given dataset. We study this problem in the context of differentially private principal component analysis (PCA), where the goal is to privatize the leading principal components of a dataset with $n$ samples and $p$ features. We analyze the exponential mechanism in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p \rightarrow \infty$). We show that in high dimensions, detecting a target individual's presence using privatized PCs is exactly as hard as distinguishing between two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with Le Cam's contiguity arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07270v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngjoo Yun, Rishabh Dudeja</dc:creator>
    </item>
    <item>
      <title>Concentration bounds for intrinsic dimension estimation using Gaussian kernels</title>
      <link>https://arxiv.org/abs/2512.04861</link>
      <description>arXiv:2512.04861v2 Announce Type: replace 
Abstract: We prove finite-sample concentration and anti-concentration bounds for dimension estimation using Gaussian kernel sums. Our bounds provide explicit dependence on sample size, bandwidth, and local geometric and distributional parameters, characterizing precisely how regularity conditions influence statistical performance. We also propose a bandwidth selection heuristic using derivative information, supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04861v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Andersson</dc:creator>
    </item>
    <item>
      <title>Dynamic covariate balancing: estimating treatment effects over time with potential local projections</title>
      <link>https://arxiv.org/abs/2103.01280</link>
      <description>arXiv:2103.01280v5 Announce Type: replace-cross 
Abstract: This paper studies the estimation and inference of treatment effects in panel data settings when treatments change dynamically over time.
  We propose a balancing method that allows for (i) treatments to be assigned dynamically over time based on high-dimensional covariates, past outcomes, and treatments; (ii) outcomes and time-varying covariates to depend on the trajectory of all past treatments; (iii) heterogeneity of treatment effects.
  Our approach recursively projects potential outcomes' expectations on past histories. It then controls the bias arising from the non-experimental and sequential nature of this setting by balancing dynamically observable characteristics over time. We establish inferential guarantees of the proposed method even when the number of observable characteristics significantly exceeds the sample size. We study numerical properties of the estimator and illustrate the benefits of the procedure in an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.01280v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davide Viviano, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>A quickest detection problem with false negatives</title>
      <link>https://arxiv.org/abs/2210.01844</link>
      <description>arXiv:2210.01844v3 Announce Type: replace-cross 
Abstract: We formulate and solve a variant of the quickest detection problem which features false negatives. A standard Brownian motion acquires a drift at an independent exponential random time which is not directly observable. Based on the observation in continuous time of the sample path of the process, an optimizer must detect the drift as quickly as possible after it has appeared. The optimizer can inspect the system multiple times upon payment of a fixed cost per inspection. If a test is performed on the system before the drift has appeared then, naturally, the test will return a negative outcome. However, if a test is performed after the drift has appeared, then the test may fail to detect it and return a false negative with probability $\epsilon\in(0,1)$. The optimisation ends when the drift is eventually detected. The problem is formulated mathematically as an optimal multiple stopping problem, and it is shown to be equivalent to a recursive optimal stopping problem. Exploiting such connection and free boundary methods we find explicit formulae for the expected cost and the optimal strategy. We also show that when $\epsilon = 0$ our expected cost is an affine transformation of the one in Shiryaev's classical optimal detection problem with a rescaled model parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.01844v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiziano De Angelis, Jhanvi Garg, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>Optimizing High-Dimensional Oblique Splits</title>
      <link>https://arxiv.org/abs/2503.14381</link>
      <description>arXiv:2503.14381v2 Announce Type: replace-cross 
Abstract: Evidence suggests that oblique splits can significantly enhance the performance of decision trees. This paper explores the optimization of high-dimensional oblique splits for decision tree construction, establishing the Sufficient Impurity Decrease (SID) convergence that takes into account $s_0$-sparse oblique splits. We demonstrate that the SID function class expands as sparsity parameter $s_0$ increases, enabling the model to capture complex data-generating processes such as the $s_0$-dimensional XOR function. Thus, $s_0$ represents the unknown potential complexity of the underlying data-generating function. Furthermore, we establish that learning these complex functions necessitates greater computational resources. This highlights a fundamental trade-off between statistical accuracy, which is governed by the $s_0$-dependent size of the SID function class, and computational cost. Particularly, for challenging problems, the required candidate oblique split set can become prohibitively large, rendering standard ensemble approaches computationally impractical. To address this, we propose progressive trees that optimize oblique splits through an iterative refinement process rather than a single-step optimization. These splits are integrated alongside traditional orthogonal splits into ensemble models like Random Forests to enhance finite-sample performance. The effectiveness of our approach is validated through simulations and real-data experiments, where it consistently outperforms various existing oblique tree models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14381v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chien-Ming Chi</dc:creator>
    </item>
    <item>
      <title>Empirical Measures and Strong Laws of Large Numbers in Categorical Probability</title>
      <link>https://arxiv.org/abs/2503.21576</link>
      <description>arXiv:2503.21576v2 Announce Type: replace-cross 
Abstract: The Glivenko--Cantelli theorem is a uniform version of the strong law of large numbers. It states that for every IID sequence of random variables, the empirical measure converges to the underlying distribution (in the sense of uniform convergence of the CDF). In this work, we provide tools to study such limits of empirical measures in categorical probability. We propose two axioms, namely permutation invariance and empirical adequacy, that a morphism of type $X^{\mathbb{N}} \to X$ should satisfy to be interpretable as taking an infinite sequence as input and producing a sample from its empirical measure as output. Since not all sequences have a well-defined empirical measure, such \emph{empirical sampling morphisms} live in quasi-Markov categories, which, unlike Markov categories, allow for partial morphisms.
  Given an empirical sampling morphism and a few other properties, we prove representability as well as abstract versions of the de Finetti theorem, the Glivenko--Cantelli theorem and the strong law of large numbers.
  We provide several concrete constructions of empirical sampling morphisms as partially defined Markov kernels on standard Borel spaces. Instantiating our abstract results then recovers the standard Glivenko--Cantelli theorem and the strong law of large numbers for random variables with finite first moment. Our work thus provides a joint proof of these two theorems in conjunction with the de Finetti theorem from first principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21576v2</guid>
      <category>math.PR</category>
      <category>cs.LO</category>
      <category>math.CT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Fritz, Tom\'a\v{s} Gonda, Antonio Lorenzin, Paolo Perrone, Areeb Shah Mohammed</dc:creator>
    </item>
    <item>
      <title>Foundations of Top-$k$ Decoding For Language Models</title>
      <link>https://arxiv.org/abs/2505.19371</link>
      <description>arXiv:2505.19371v2 Announce Type: replace-cross 
Abstract: Top-$k$ decoding is a widely used method for sampling from LLMs: at each token, only the largest $k$ next-token-probabilities are kept, and the next token is sampled after re-normalizing them to sum to unity. Top-$k$ and other sampling methods are motivated by the intuition that true next-token distributions are sparse, and the noisy LLM probabilities need to be truncated. However, to our knowledge, a precise theoretical motivation for the use of top-$k$ decoding is missing. In this work, we develop a theoretical framework that both explains and generalizes top-$k$ decoding. We view decoding at a fixed token as the recovery of a sparse probability distribution. We consider \emph{Bregman decoders} obtained by minimizing a separable Bregman divergence (for both the \emph{primal} and \emph{dual} cases) with a sparsity-inducing $\ell_0$ regularization. Despite the combinatorial nature of the objective, we show how to optimize it efficiently for a large class of divergences. We show that the optimal decoding strategies are greedy, and further that the loss function is discretely convex in $k$, so that binary search provably and efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a special case for the KL divergence, and identify new decoding strategies that have distinct behaviors (e.g., non-linearly up-weighting larger probabilities after re-normalization).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19371v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgy Noarov, Soham Mallick, Tao Wang, Sunay Joshi, Yan Sun, Yangxinyu Xie, Mengxin Yu, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?</title>
      <link>https://arxiv.org/abs/2507.11891</link>
      <description>arXiv:2507.11891v2 Announce Type: replace-cross 
Abstract: We study A/B experiments that are designed to compare the performance of two recommendation algorithms. Prior work has observed that the stable unit treatment value assumption (SUTVA) often does not hold in large-scale recommendation systems, and hence the estimate for the global treatment effect (GTE) is biased. Specifically, units under the treatment and control algorithms contribute to a shared pool of data that subsequently train both algorithms, resulting in interference between the two groups. In this paper, we investigate when such interference may affect our decision making on which algorithm is better. We formalize this insight under a multi-armed bandit framework and theoretically characterize when the sign of the difference-in-means estimator of the GTE under data sharing aligns with or contradicts the sign of the true GTE. Our analysis identifies the level of exploration versus exploitation as a key determinant of how data sharing impacts decision making, and we propose a detection procedure based on ramp-up experiments to signal incorrect algorithm comparison in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11891v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuangning Li, Chonghuan Wang, Jingyan Wang</dc:creator>
    </item>
    <item>
      <title>A Martingale Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2510.11853</link>
      <description>arXiv:2510.11853v2 Announce Type: replace-cross 
Abstract: The Maximum Mean Discrepancy (MMD) is a widely used multivariate distance metric for two-sample testing. The standard MMD test statistic has an intractable null distribution typically requiring costly resampling or permutation approaches for calibration. In this work we leverage a martingale interpretation of the estimated squared MMD to propose martingale MMD (mMMD), a quadratic-time statistic which has a limiting standard Gaussian distribution under the null. Moreover we show that the test is consistent against any fixed alternative and for large sample sizes, mMMD offers substantial computational savings over the standard MMD test, with only a minor loss in power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11853v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>The Gravitational Aspect of Information: The Physical Reality of Asymmetric "Distance"</title>
      <link>https://arxiv.org/abs/2510.22664</link>
      <description>arXiv:2510.22664v4 Announce Type: replace-cross 
Abstract: We show that when a Brownian bridge is physically constrained to satisfy a canonical condition, its time evolution exactly coincides with an m-geodesic on the statistical manifold of Gaussian distributions. This identification provides a direct physical realization of a geometric concept in information geometry. It implies that purely random processes evolve along informationally straight trajectories, analogous to geodesics in general relativity. Our findings suggest that the asymmetry of informational ``distance" (divergence) plays a fundamental physical role, offering a concrete step toward an equivalence principle for information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22664v4</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>gr-qc</category>
      <category>hep-ph</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>quant-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoi Koide, Armin van de Venn</dc:creator>
    </item>
    <item>
      <title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
      <link>https://arxiv.org/abs/2512.13123</link>
      <description>arXiv:2512.13123v5 Announce Type: replace-cross 
Abstract: The problem of stopping stochastic gradient descent (SGD) in an online manner, based solely on the observed trajectory, is a challenging theoretical problem with significant consequences for applications. While SGD is routinely monitored as it runs, the classical theory of SGD provides guarantees only at pre-specified iteration horizons and offers no valid way to decide, based on the observed trajectory, when further computation is justified. We address this longstanding gap by developing anytime-valid confidence sequences for stochastic gradient methods, which remain valid under continuous monitoring and directly induce statistically valid, trajectory-dependent stopping rules: stop as soon as the current upper confidence bound on an appropriate performance measure falls below a user-specified tolerance. The confidence sequences are constructed using nonnegative supermartingales, are time-uniform, and depend only on observable quantities along the SGD trajectory, without requiring prior knowledge of the optimization horizon. In convex optimization, this yields anytime-valid certificates for weighted suboptimality of projected SGD under general stepsize schedules, without assuming smoothness or strong convexity. In nonconvex optimization, it yields time-uniform certificates for weighted first-order stationarity under smoothness assumptions. We further characterize the stopping-time complexity of the resulting stopping rules under standard stepsize schedules. To the best of our knowledge, this is the first framework that provides statistically valid, time-uniform stopping rules for SGD across both convex and nonconvex settings based solely on its observed trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13123v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Distributional Limits for Eigenvalues of Graphon Kernel Matrices</title>
      <link>https://arxiv.org/abs/2601.04584</link>
      <description>arXiv:2601.04584v4 Announce Type: replace-cross 
Abstract: We study the fluctuation behavior of individual eigenvalues of kernel matrices arising from dense graphon-based random graphs. Under minimal integrability and boundedness assumptions on the graphon, we establish distributional limits for simple, well-separated eigenvalues of the associated integral operator. A sharp probabilistic dichotomy emerges: in the non-degenerate regime, the properly normalized empirical eigenvalue satisfies a central limit theorem with an explicit variance, whereas in the degenerate regime the leading stochastic term vanishes and the centered eigenvalue converges to a weighted chi-square law determined by the operator spectrum.
  The analysis requires no smoothness or Lipschitz conditions on the kernel. Prior work under comparable assumptions established only operator convergence and eigenspace consistency; the present results characterize the full distributional behavior of individual eigenvalues, extending fluctuation theory beyond the reach of classical operator-level arguments. The proofs combine second-order perturbation expansions, concentration bounds for kernel matrices, and Hoeffding decompositions for symmetric statistics, revealing that at the $\sqrt{n}$ scale the dominant randomness arises from latent-position sampling rather than Bernoulli edge noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04584v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Behzad Aalipur</dc:creator>
    </item>
    <item>
      <title>Low-Dimensional Adaptation of Rectified Flow: A Diffusion and Stochastic Localization Perspective</title>
      <link>https://arxiv.org/abs/2601.15500</link>
      <description>arXiv:2601.15500v3 Announce Type: replace-cross 
Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\varepsilon)$ (up to log factors), where $\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15500v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Roy, Alessandro Rinaldo, Purnamrita Sarkar</dc:creator>
    </item>
  </channel>
</rss>
