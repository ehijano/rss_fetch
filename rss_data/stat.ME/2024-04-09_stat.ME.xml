<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust Nonparametric Stochastic Frontier Analysis</title>
      <link>https://arxiv.org/abs/2404.04301</link>
      <description>arXiv:2404.04301v1 Announce Type: new 
Abstract: Benchmarking tools, including stochastic frontier analysis (SFA), data envelopment analysis (DEA), and its stochastic extension (StoNED) are core tools in economics used to estimate an efficiency envelope and production inefficiencies from data. The problem appears in a wide range of fields -- for example, in global health the frontier can quantify efficiency of interventions and funding of health initiatives. Despite their wide use, classic benchmarking approaches have key limitations that preclude even wider applicability. Here we propose a robust non-parametric stochastic frontier meta-analysis (SFMA) approach that fills these gaps. First, we use flexible basis splines and shape constraints to model the frontier function, so specifying a functional form of the frontier as in classic SFA is no longer necessary. Second, the user can specify relative errors on input datapoints, enabling population-level analyses. Third, we develop a likelihood-based trimming strategy to robustify the approach to outliers, which otherwise break available benchmarking methods. We provide a custom optimization algorithm for fast and reliable performance. We implement the approach and algorithm in an open source Python package `sfma'. Synthetic and real examples show the new capabilities of the method, and are used to compare SFMA to state of the art benchmarking packages that implement DEA, SFA, and StoNED.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04301v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Zheng, Nahom Worku, Marlena Bannick, Joseph Dielemann, Marcia Weaver, Christopher Murray, Aleksandr Aravkin</dc:creator>
    </item>
    <item>
      <title>Multi-way contingency tables with uniform margins</title>
      <link>https://arxiv.org/abs/2404.04343</link>
      <description>arXiv:2404.04343v1 Announce Type: new 
Abstract: We study the problem of transforming a multi-way contingency table into an equivalent table with uniform margins and same dependence structure. Such a problem relates to recent developments in copula modeling for discrete random vectors. Here, we focus on three-way binary tables and show that, even in such a simple case, the situation is quite different than for two-way tables. Many more constraints are needed to ensure a unique solution to the problem. Therefore, the uniqueness of the transformed table is subject to arbitrary choices of the practitioner. We illustrate the theory through some examples, and conclude with a discussion on the topic and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04343v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elisa Perrone, Roberto Fontana, Fabio Rapallo</dc:creator>
    </item>
    <item>
      <title>Bayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards</title>
      <link>https://arxiv.org/abs/2404.04398</link>
      <description>arXiv:2404.04398v1 Announce Type: new 
Abstract: Measuring the impact of an environmental point source exposure on the risk of disease, like cancer or childhood asthma, is well-developed. Modeling how an environmental health hazard that is extensive in space, like a wastewater canal, is not. We propose a novel Bayesian generative semiparametric model for characterizing the cumulative spatial exposure to an environmental health hazard that is not well-represented by a single point in space. The model couples a dose-response model with a log-Gaussian Cox process integrated against a distance kernel with an unknown length-scale. We show that this model is a well-defined Bayesian inverse model, namely that the posterior exists under a Gaussian process prior for the log-intensity of exposure, and that a simple integral approximation adequately controls the computational error. We quantify the finite-sample properties and the computational tractability of the discretization scheme in a simulation study. Finally, we apply the model to survey data on household risk of childhood diarrheal illness from exposure to a system of wastewater canals in Mezquital Valley, Mexico.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04398v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Trangucci, Jesse Contreras, Jon Zelner, Joseph N. S. Eisenberg, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow Modeling</title>
      <link>https://arxiv.org/abs/2404.04403</link>
      <description>arXiv:2404.04403v1 Announce Type: new 
Abstract: Tensor clustering has become an important topic, specifically in spatio-temporal modeling, due to its ability to cluster spatial modes (e.g., stations or road segments) and temporal modes (e.g., time of the day or day of the week). Our motivating example is from subway passenger flow modeling, where similarities between stations are commonly found. However, the challenges lie in the innate high-dimensionality of tensors and also the potential existence of anomalies. This is because the three tasks, i.e., dimension reduction, clustering, and anomaly decomposition, are inter-correlated to each other, and treating them in a separate manner will render a suboptimal performance. Thus, in this work, we design a tensor-based subspace clustering and anomaly decomposition technique for simultaneously outlier-robust dimension reduction and clustering for high-dimensional tensors. To achieve this, a novel low-rank robust subspace clustering decomposition model is proposed by combining Tucker decomposition, sparse anomaly decomposition, and subspace clustering. An effective algorithm based on Block Coordinate Descent is proposed to update the parameters. Prudent experiments prove the effectiveness of the proposed framework via the simulation study, with a gain of +25% clustering accuracy than benchmark methods in a hard case. The interrelations of the three tasks are also analyzed via ablation studies, validating the interrelation assumption. Moreover, a case study in the station clustering based on real passenger flow data is conducted, with quite valuable insights discovered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04403v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiuyun Hu, Ziyue Li, Chen Zhang, Fugee Tsung, Hao Yan</dc:creator>
    </item>
    <item>
      <title>Optimality-based reward learning with applications to toxicology</title>
      <link>https://arxiv.org/abs/2404.04406</link>
      <description>arXiv:2404.04406v1 Announce Type: new 
Abstract: In toxicology research, experiments are often conducted to determine the effect of toxicant exposure on the behavior of mice, where mice are randomized to receive the toxicant or not. In particular, in fixed interval experiments, one provides a mouse reinforcers (e.g., a food pellet), contingent upon some action taken by the mouse (e.g., a press of a lever), but the reinforcers are only provided after fixed time intervals. Often, to analyze fixed interval experiments, one specifies and then estimates the conditional state-action distribution (e.g., using an ANOVA). This existing approach, which in the reinforcement learning framework would be called modeling the mouse's "behavioral policy," is sensitive to misspecification. It is likely that any model for the behavioral policy is misspecified; a mapping from a mouse's exposure to their actions can be highly complex. In this work, we avoid specifying the behavioral policy by instead learning the mouse's reward function. Specifying a reward function is as challenging as specifying a behavioral policy, but we propose a novel approach that incorporates knowledge of the optimal behavior, which is often known to the experimenter, to avoid specifying the reward function itself. In particular, we define the reward as a divergence of the mouse's actions from optimality, where the representations of the action and optimality can be arbitrarily complex. The parameters of the reward function then serve as a measure of the mouse's tolerance for divergence from optimality, which is a novel summary of the impact of the exposure. The parameter itself is scalar, and the proposed objective function is differentiable, allowing us to benefit from typical results on consistency of parametric estimators while making very few assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04406v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel J. Weisenthal, Matthew Eckard, Askhan Ertefaie, Marissa Sobolewski, Sally W. Thurston</dc:creator>
    </item>
    <item>
      <title>Sample size planning for estimating the global win probability with assurance and precision</title>
      <link>https://arxiv.org/abs/2404.04415</link>
      <description>arXiv:2404.04415v1 Announce Type: new 
Abstract: Most clinical trials conducted in drug development contain multiple endpoints in order to collectively assess the intended effects of the drug on various disease characteristics. Focusing on the estimation of the global win probability, defined as the average win probability (WinP) across endpoints that a treated participant would have a better outcome than a control participant, we propose a closed-form sample size formula incorporating pre-specified precision and assurance, with precision denoted by the lower limit of confidence interval and assurance denoted by the probability of achieving that lower limit. We make use of the equivalence of the WinP and the area under the receiver operating characteristic curve (AUC) and adapt a formula originally developed for the difference between two AUCs to handle the global WinP. Unequal variance is allowed. Simulation results suggest that the method performs very well. We illustrate the proposed formula using a Parkinson's disease clinical trial design example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04415v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Shu, Guangyong Zou</dc:creator>
    </item>
    <item>
      <title>Bounding Causal Effects with Leaky Instruments</title>
      <link>https://arxiv.org/abs/2404.04446</link>
      <description>arXiv:2404.04446v1 Announce Type: new 
Abstract: Instrumental variables (IVs) are a popular and powerful tool for estimating causal effects in the presence of unobserved confounding. However, classical approaches rely on strong assumptions such as the $\textit{exclusion criterion}$, which states that instrumental effects must be entirely mediated by treatments. This assumption often fails in practice. When IV methods are improperly applied to data that do not meet the exclusion criterion, estimated causal effects may be badly biased. In this work, we propose a novel solution that provides $\textit{partial}$ identification in linear models given a set of $\textit{leaky instruments}$, which are allowed to violate the exclusion criterion to some limited degree. We derive a convex optimization objective that provides provably sharp bounds on the average treatment effect under some common forms of information leakage, and implement inference procedures to quantify the uncertainty of resulting estimates. We demonstrate our method in a set of experiments with simulated data, where it performs favorably against the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04446v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Watson, Jordan Penn, Lee M. Gunderson, Gecia Bravo-Hermsdorff, Afsaneh Mastouri, Ricardo Silva</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference in Ultrahigh Dimensional Partially Linear Single-Index Models</title>
      <link>https://arxiv.org/abs/2404.04471</link>
      <description>arXiv:2404.04471v1 Announce Type: new 
Abstract: This paper is concerned with estimation and inference for ultrahigh dimensional partially linear single-index models. The presence of high dimensional nuisance parameter and nuisance unknown function makes the estimation and inference problem very challenging. In this paper, we first propose a profile partial penalized least squares estimator and establish the sparsity, consistency and asymptotic representation of the proposed estimator in ultrahigh dimensional setting. We then propose an $F$-type test statistic for parameters of primary interest and show that the limiting null distribution of the test statistic is $\chi^2$ distribution, and the test statistic can detect local alternatives, which converge to the null hypothesis at the root-$n$ rate. We further propose a new test for the specification testing problem of the nonparametric function. The test statistic is shown to be asymptotically normal. Simulation studies are conducted to examine the finite sample performance of the proposed estimators and tests. A real data example is used to illustrate the proposed procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04471v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Xu Guo, Zhe Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamic Treatment Regimes with Replicated Observations Available for Error-prone Covariates: a Q-learning Approach</title>
      <link>https://arxiv.org/abs/2404.04696</link>
      <description>arXiv:2404.04696v1 Announce Type: new 
Abstract: Dynamic treatment regimes (DTRs) have received an increasing interest in recent years. DTRs are sequences of treatment decision rules tailored to patient-level information. The main goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that yields the best expected clinical outcome. Q-learning has been considered as one of the most popular regression-based methods to estimate the optimal DTR. However, it is rarely studied in an error-prone setting, where the patient information is contaminated with measurement error. In this paper, we study the effect of covariate measurement error on Q-learning and propose a correction method to correct the measurement error in Q-learning. Simulation studies are conducted to assess the performance of the proposed method in Q-learning. We illustrate the use of the proposed method in an application to the sequenced treatment alternatives to relieve depression data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04696v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Liu, Wenqing He</dc:creator>
    </item>
    <item>
      <title>Q-learning in Dynamic Treatment Regimes with Misclassified Binary Outcome</title>
      <link>https://arxiv.org/abs/2404.04697</link>
      <description>arXiv:2404.04697v1 Announce Type: new 
Abstract: The study of precision medicine involves dynamic treatment regimes (DTRs), which are sequences of treatment decision rules recommended by taking patient-level information as input. The primary goal of the DTR study is to identify an optimal DTR, a sequence of treatment decision rules that leads to the best expected clinical outcome. Statistical methods have been developed in recent years to estimate an optimal DTR, including Q-learning, a regression-based method in the DTR literature. Although there are many studies concerning Q-learning, little attention has been given in the presence of noisy data, such as misclassified outcomes. In this paper, we investigate the effect of outcome misclassification on Q-learning and propose a correction method to accommodate the misclassification effect. Simulation studies are conducted to demonstrate the satisfactory performance of the proposed method. We illustrate the proposed method in two examples from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study and the smoking cessation program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04697v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Liu, Wenqing He</dc:creator>
    </item>
    <item>
      <title>Topological data analysis for random sets and its application in detecting outliers and goodness of fit testing</title>
      <link>https://arxiv.org/abs/2404.04702</link>
      <description>arXiv:2404.04702v1 Announce Type: new 
Abstract: In this paper we present the methodology for detecting outliers and testing the goodness-of-fit of random sets using topological data analysis. We construct the filtration from level sets of the signed distance function and consider various summary functions of the persistence diagram derived from the obtained persistence homology. The outliers are detected using functional depths for the summary functions. Global envelope tests using the summary statistics as test statistics were used to construct the goodness-of-fit test. The procedures were justified by a simulation study using germ-grain random set models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04702v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vesna Gotovac {\DJ}oga\v{s}, Marcela Mandari\'c</dc:creator>
    </item>
    <item>
      <title>Change Point Detection in Dynamic Graphs with Generative Model</title>
      <link>https://arxiv.org/abs/2404.04719</link>
      <description>arXiv:2404.04719v1 Announce Type: new 
Abstract: This paper proposes a simple generative model to detect change points in time series of graphs. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that can generate dynamic graphs from the latent representations. The informative prior distributions in the latent spaces are learned from observed data as empirical Bayes, and the expressive power of a generative model is exploited to assist change point detection. Specifically, the model parameters are learned via maximum approximate likelihood, with a Group Fused Lasso regularization. The optimization problem is then solved via Alternating Direction Method of Multipliers (ADMM), and Langevin Dynamics are recruited for posterior inference. Experiments in simulated and real data demonstrate the ability of the generative model in supporting change point detection with good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04719v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Jialiang Li, Hangjian Li, Yanzhen Chen, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Bipartite causal inference with interference, time series data, and a random network</title>
      <link>https://arxiv.org/abs/2404.04775</link>
      <description>arXiv:2404.04775v1 Announce Type: new 
Abstract: In bipartite causal inference with interference there are two distinct sets of units: those that receive the treatment, termed interventional units, and those on which the outcome is measured, termed outcome units. Which interventional units' treatment can drive which outcome units' outcomes is often depicted in a bipartite network. We study bipartite causal inference with interference from observational data across time and with a changing bipartite network. Under an exposure mapping framework, we define causal effects specific to each outcome unit, representing average contrasts of potential outcomes across time. We establish unconfoundedness of the exposure received by the outcome units based on unconfoundedness assumptions on the interventional units' treatment assignment and the random graph, hence respecting the bipartite structure of the problem. By harvesting the time component of our setting, causal effects are estimable while controlling only for temporal trends and time-varying confounders. Our results hold for binary, continuous, and multivariate exposure mappings. In the case of a binary exposure, we propose three matching algorithms to estimate the causal effect based on matching exposed to unexposed time periods for the same outcome unit, and we show that the bias of the resulting estimators is bounded. We illustrate our approach with an extensive simulation study and an application on the effect of wildfire smoke on transportation by bicycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04775v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhaoyan Song, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>A Deep Learning Approach to Nonparametric Propensity Score Estimation with Optimized Covariate Balance</title>
      <link>https://arxiv.org/abs/2404.04794</link>
      <description>arXiv:2404.04794v1 Announce Type: new 
Abstract: This paper proposes a novel propensity score weighting analysis. We define two sufficient and necessary conditions for a function of the covariates to be the propensity score. The first is "local balance", which ensures the conditional independence of covariates and treatment assignment across a dense grid of propensity score values. The second condition, "local calibration", guarantees that a balancing score is a propensity score. Using three-layer feed-forward neural networks, we develop a nonparametric propensity score model that satisfies these conditions, effectively circumventing the issue of model misspecification and optimizing covariate balance to minimize bias and stabilize the inverse probability weights. Our proposed method performed substantially better than existing methods in extensive numerical studies of both real and simulated benchmark datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04794v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maosen Peng, Yan Li, Chong Wu, Liang Li</dc:creator>
    </item>
    <item>
      <title>Review for Handling Missing Data with special missing mechanism</title>
      <link>https://arxiv.org/abs/2404.04905</link>
      <description>arXiv:2404.04905v1 Announce Type: new 
Abstract: Missing data poses a significant challenge in data science, affecting decision-making processes and outcomes. Understanding what missing data is, how it occurs, and why it is crucial to handle it appropriately is paramount when working with real-world data, especially in tabular data, one of the most commonly used data types in the real world. Three missing mechanisms are defined in the literature: Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), each presenting unique challenges in imputation. Most existing work are focused on MCAR that is relatively easy to handle. The special missing mechanisms of MNAR and MAR are less explored and understood. This article reviews existing literature on handling missing values. It compares and contrasts existing methods in terms of their ability to handle different missing mechanisms and data types. It identifies research gap in the existing literature and lays out potential directions for future research in the field. The information in this review will help data analysts and researchers to adopt and promote good practices for handling missing data in real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04905v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youran Zhou, Sunil Aryal, Mohamed Reda Bouadjenek</dc:creator>
    </item>
    <item>
      <title>Dir-SPGLM: A Bayesian semiparametric GLM with data-driven reference distribution</title>
      <link>https://arxiv.org/abs/2404.05060</link>
      <description>arXiv:2404.05060v1 Announce Type: new 
Abstract: The recently developed semi-parametric generalized linear model (SPGLM) offers more flexibility as compared to the classical GLM by including the baseline or reference distribution of the response as an additional parameter in the model. However, some inference summaries are not easily generated under existing maximum-likelihood based inference (ML-SPGLM). This includes uncertainty in estimation for model-derived functionals such as exceedance probabilities. The latter are critical in a clinical diagnostic or decision-making setting. In this article, by placing a Dirichlet prior on the baseline distribution, we propose a Bayesian model-based approach for inference to address these important gaps. We establish consistency and asymptotic normality results for the implied canonical parameter. Simulation studies and an illustration with data from an aging research study confirm that the proposed method performs comparably or better in comparison with ML-SPGLM. The proposed Bayesian framework is most attractive for inference with small sample training data or in sparse-data scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05060v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Entejar Alam, Peter M\"uller, Paul J. Rathouz</dc:creator>
    </item>
    <item>
      <title>BayesPPDSurv: An R Package for Bayesian Sample Size Determination Using the Power and Normalized Power Prior for Time-To-Event Data</title>
      <link>https://arxiv.org/abs/2404.05118</link>
      <description>arXiv:2404.05118v1 Announce Type: new 
Abstract: The BayesPPDSurv (Bayesian Power Prior Design for Survival Data) R package supports Bayesian power and type I error calculations and model fitting using the power and normalized power priors incorporating historical data with for the analysis of time-to-event outcomes. The package implements the stratified proportional hazards regression model with piecewise constant hazard within each stratum. The package allows the historical data to inform the treatment effect parameter, parameter effects for other covariates in the regression model, as well as the baseline hazard parameters. The use of multiple historical datasets is supported. A novel algorithm is developed for computationally efficient use of the normalized power prior. In addition, the package supports the use of arbitrary sampling priors for computing Bayesian power and type I error rates, and has built-in features that semi-automatically generate sampling priors from the historical data. We demonstrate the use of BayesPPDSurv in a comprehensive case study for a melanoma clinical trial design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05118v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueqi Shen, Matthew A. Psioda, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Generalized Criterion for Identifiability of Additive Noise Models Using Majorization</title>
      <link>https://arxiv.org/abs/2404.05148</link>
      <description>arXiv:2404.05148v1 Announce Type: new 
Abstract: The discovery of causal relationships from observational data is very challenging. Many recent approaches rely on complexity or uncertainty concepts to impose constraints on probability distributions, aiming to identify specific classes of directed acyclic graph (DAG) models. In this paper, we introduce a novel identifiability criterion for DAGs that places constraints on the conditional variances of additive noise models. We demonstrate that this criterion extends and generalizes existing identifiability criteria in the literature that employ (conditional) variances as measures of uncertainty in (conditional) distributions. For linear Structural Equation Models, we present a new algorithm that leverages the concept of weak majorization applied to the diagonal elements of the Cholesky factor of the covariance matrix to learn a topological ordering of variables. Through extensive simulations and the analysis of bank connectivity data, we provide evidence of the effectiveness of our approach in successfully recovering DAGs. The code for reproducing the results in this paper is available in Supplementary Materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05148v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aramayis Dallakyan, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Assessing the causes of continuous effects by posterior effects of causes</title>
      <link>https://arxiv.org/abs/2404.05246</link>
      <description>arXiv:2404.05246v1 Announce Type: new 
Abstract: To evaluate a single cause of a binary effect, Dawid et al. (2014) defined the probability of causation, while Pearl (2015) defined the probabilities of necessity and sufficiency. For assessing the multiple correlated causes of a binary effect, Lu et al. (2023) defined the posterior causal effects based on post-treatment variables. In many scenarios, outcomes are continuous, simply binarizing them and applying previous methods may result in information loss or biased conclusions. To address this limitation, we propose a series of posterior causal estimands for retrospectively evaluating multiple correlated causes from a continuous effect, including posterior intervention effects, posterior total causal effects, and posterior natural direct effects. Under the assumptions of sequential ignorability, monotonicity, and perfect positive rank, we show that the posterior causal estimands of interest are identifiable and present the corresponding identification equations. We also provide a simple but effective estimation procedure and establish the asymptotic properties of the proposed estimators. An artificial hypertension example and a real developmental toxicity dataset are employed to illustrate our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05246v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shanshan Luo, Yixuan Yu, Chunchen Liu, Feng Xie, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Unsupervised Training of Convex Regularizers using Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2404.05445</link>
      <description>arXiv:2404.05445v1 Announce Type: new 
Abstract: Unsupervised learning is a training approach in the situation where ground truth data is unavailable, such as inverse imaging problems. We present an unsupervised Bayesian training approach to learning convex neural network regularizers using a fixed noisy dataset, based on a dual Markov chain estimation method. Compared to classical supervised adversarial regularization methods, where there is access to both clean images as well as unlimited to noisy copies, we demonstrate close performance on natural image Gaussian deconvolution and Poisson denoising tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05445v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Ziruo Cai, Marcelo Pereyra, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Bayesian Inverse Ising Problem with Three-body Interactions</title>
      <link>https://arxiv.org/abs/2404.05671</link>
      <description>arXiv:2404.05671v1 Announce Type: new 
Abstract: In this paper, we solve the inverse Ising problem with three-body interaction. Using the mean-field approximation, we find a tractable expansion of the normalizing constant. This facilitates estimation, which is known to be quite challenging for the Ising model. We then develop a novel hybrid MCMC algorithm that integrates Adaptive Metropolis Hastings (AMH), Hamiltonian Monte Carlo (HMC), and the Manifold-Adjusted Langevin Algorithm (MALA), which converges quickly and mixes well. We demonstrate the robustness of our algorithm using data simulated with a structure under which parameter estimation is known to be challenging, such as in the presence of a phase transition and at the critical point of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05671v1</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Godwin Osabutey, Robert Richardson, Garritt L. Page</dc:creator>
    </item>
    <item>
      <title>On the estimation of complex statistics combining different surveys</title>
      <link>https://arxiv.org/abs/2404.05702</link>
      <description>arXiv:2404.05702v1 Announce Type: new 
Abstract: The importance of exploring a potential integration among surveys has been acknowledged in order to enhance effectiveness and minimize expenses. In this work, we employ the alignment method to combine information from two different surveys for the estimation of complex statistics. The derivation of the alignment weights poses challenges in case of complex statistics due to their non-linear form. To overcome this, we propose to use a linearized variable associated with the complex statistic under consideration. Linearized variables have been widely used to derive variance estimates, thus allowing for the estimation of the variance of the combined complex statistics estimates. Simulations conducted show the effectiveness of the proposed approach, resulting to the reduction of the variance of the combined complex statistics estimates. Also, in some cases, the usage of the alignment weights derived using the linearized variable associated with a complex statistic, could result in a further reduction of the variance of the combined estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05702v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasilis Chasiotis, Dimitris Karlis</dc:creator>
    </item>
    <item>
      <title>Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer</title>
      <link>https://arxiv.org/abs/2404.04399</link>
      <description>arXiv:2404.04399v1 Announce Type: cross 
Abstract: We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04399v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Shirakawa, Yi Li, Yulun Wu, Sky Qiu, Yuxuan Li, Mingduo Zhao, Hiroyasu Iso, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression</title>
      <link>https://arxiv.org/abs/2404.04498</link>
      <description>arXiv:2404.04498v1 Announce Type: cross 
Abstract: The remarkable generalization performance of overparameterized models has challenged the conventional wisdom of statistical learning theory. While recent theoretical studies have shed light on this behavior in linear models or nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression remains lacking. This paper explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of adaptive prior based on the intrinsic spectral structure of the data. We establish posterior contraction for single-neuron models with Lipschitz continuous activation functions and for generalized linear models, demonstrating that our approach achieves consistent predictions in the overparameterized regime. Moreover, our Bayesian framework allows for uncertainty estimation of the predictions. The proposed method is validated through numerical simulations and a real data application, showcasing its ability to achieve accurate predictions and reliable uncertainty estimates. Our work advances the theoretical understanding of the blessing of overparameterization and offers a principled Bayesian approach for prediction in large nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04498v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama</dc:creator>
    </item>
    <item>
      <title>New methods for computing the generalized chi-square distribution</title>
      <link>https://arxiv.org/abs/2404.05062</link>
      <description>arXiv:2404.05062v1 Announce Type: cross 
Abstract: We present several exact and approximate mathematical methods and open-source software to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution, which appears in Bayesian classification problems. Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index $d'$ between multinormals. We compare the accuracy and speed of these methods against the best existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05062v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhranil Das</dc:creator>
    </item>
    <item>
      <title>Evaluating Interventional Reasoning Capabilities of Large Language Models</title>
      <link>https://arxiv.org/abs/2404.05545</link>
      <description>arXiv:2404.05545v1 Announce Type: cross 
Abstract: Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. Our analysis on four LLMs highlights that while GPT- 4 models show promising accuracy at predicting the intervention effects, they remain sensitive to distracting factors in the prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05545v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejas Kasetty, Divyat Mahajan, Gintare Karolina Dziugaite, Alexandre Drouin, Dhanya Sridhar</dc:creator>
    </item>
    <item>
      <title>How to Evaluate Entity Resolution Systems: An Entity-Centric Framework with Application to Inventor Name Disambiguation</title>
      <link>https://arxiv.org/abs/2404.05622</link>
      <description>arXiv:2404.05622v1 Announce Type: cross 
Abstract: Entity resolution (record linkage, microclustering) systems are notoriously difficult to evaluate. Looking for a needle in a haystack, traditional evaluation methods use sophisticated, application-specific sampling schemes to find matching pairs of records among an immense number of non-matches. We propose an alternative that facilitates the creation of representative, reusable benchmark data sets without necessitating complex sampling schemes. These benchmark data sets can then be used for model training and a variety of evaluation tasks. Specifically, we propose an entity-centric data labeling methodology that integrates with a unified framework for monitoring summary statistics, estimating key performance metrics such as cluster and pairwise precision and recall, and analyzing root causes for errors. We validate the framework in an application to inventor name disambiguation and through simulation studies. Software: https://github.com/OlivierBinette/er-evaluation/</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05622v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Binette, Youngsoo Baek, Siddharth Engineer, Christina Jones, Abel Dasylva, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Donor's Deferral and Return Behavior: Partial Identification from a Regression Discontinuity Design with Manipulation</title>
      <link>https://arxiv.org/abs/1910.02170</link>
      <description>arXiv:1910.02170v3 Announce Type: replace 
Abstract: Volunteer labor can temporarily yield lower benefits to charities than its costs. In such instances, organizations may wish to defer volunteer donations to a later date. Exploiting a discontinuity in blood donations' eligibility criteria, we show that deferring donors reduces their future volunteerism. In our setting, medical staff manipulates donors' reported hemoglobin levels over a threshold to facilitate donation. Such manipulation invalidates standard regression discontinuity design. To circumvent this issue, we propose a procedure for obtaining partial identification bounds where manipulation is present. Our procedure is applicable in various regression discontinuity settings where the running variable is manipulated and discrete.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.02170v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Rosenman, Karthik Rajkumar, Romain Gauriot, Robert Slonim</dc:creator>
    </item>
    <item>
      <title>Scale Reliant Inference</title>
      <link>https://arxiv.org/abs/2201.03616</link>
      <description>arXiv:2201.03616v4 Announce Type: replace 
Abstract: Scientific fields such as genomics, ecology, and political science often collect multivariate count data. In these fields, the data are often sufficiently noisy such that inferences regarding the total size of the measured systems have substantial uncertainty. This uncertainty can hinder downstream analyses, such as differential analysis in case-control studies. There have historically been two approaches to this problem: one considers the data as compositional and the other as counts that can be normalized. In this article, we use the framework of partially identified models to rigorously study the types of scientific questions (estimands) that can be answered (estimated) using these data. We prove that satisfying Frequentist inferential criteria is impossible for many estimation problems. In contrast, we find that the criteria for Bayesian inference can be satisfied, yet it requires a particular type of model called a Bayesian partially identified model. We introduce Scale Simulation Random Variables as a flexible and computationally efficient form of Bayesian partially identified models for analyzing these data. We use simulations and data analysis to validate our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.03616v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Pistner Nixon, Kyle C. McGovern, Jeffrey Letourneau, Lawrence A. David, Nicole A. Lazar, Sayan Mukherjee, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>Randomness of Shapes and Statistical Inference on Shapes via the Smooth Euler Characteristic Transform</title>
      <link>https://arxiv.org/abs/2204.12699</link>
      <description>arXiv:2204.12699v4 Announce Type: replace 
Abstract: In this article, we establish the mathematical foundations for modeling the randomness of shapes and conducting statistical inference on shapes using the smooth Euler characteristic transform. Based on these foundations, we propose two chi-squared statistic-based algorithms for testing hypotheses on random shapes. Simulation studies are presented to validate our mathematical derivations and to compare our algorithms with state-of-the-art methods to demonstrate the utility of our proposed framework. As real applications, we analyze a data set of mandibular molars from four genera of primates and show that our algorithms have the power to detect significant shape differences that recapitulate known morphological variation across suborders. Altogether, our discussions bridge the following fields: algebraic and computational topology, probability theory and stochastic processes, Sobolev spaces and functional analysis, analysis of variance for functional data, and geometric morphometrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.12699v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Meng, Jinyu Wang, Lorin Crawford, Ani Eloyan</dc:creator>
    </item>
    <item>
      <title>Sparse-group boosting -- Unbiased group and variable selection</title>
      <link>https://arxiv.org/abs/2206.06344</link>
      <description>arXiv:2206.06344v2 Announce Type: replace 
Abstract: In the presence of grouped covariates, we propose a framework for boosting that allows to enforce sparsity within and between groups. By using component-wise and group-wise gradient boosting at the same time with adjusted degrees of freedom, a model with similar properties as the sparse group lasso can be fitted through boosting. We show that within-group and between-group sparsity can be controlled by a mixing parameter and discuss similarities and differences to the mixing parameter in the sparse group lasso. With simulations, gene data as well as agricultural data we show the effectiveness and predictive competitiveness of this estimator. The data and simulations suggest, that in the presence of grouped variables the use of sparse group boosting is associated with less biased variable selection and higher predictability compared to component-wise boosting. Additionally, we propose a way of reducing bias in component-wise boosting through the degrees of freedom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06344v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Obster, Christian Heumann</dc:creator>
    </item>
    <item>
      <title>Modern Statistical Models and Methods for Estimating Fatigue-Life and Fatigue-Strength Distributions from Experimental Data</title>
      <link>https://arxiv.org/abs/2212.04550</link>
      <description>arXiv:2212.04550v3 Announce Type: replace 
Abstract: Engineers and scientists have been collecting and analyzing fatigue data since the 1800s to ensure the reliability of life-critical structures. Applications include (but are not limited to) bridges, building structures, aircraft and spacecraft components, ships, ground-based vehicles, and medical devices. Engineers need to estimate S-N relationships (Stress or Strain versus Number of cycles to failure), typically with a focus on estimating small quantiles of the fatigue-life distribution. Estimates from this kind of model are used as input to models (e.g., cumulative damage models) that predict failure-time distributions under varying stress patterns. Also, design engineers need to estimate lower-tail quantiles of the closely related fatigue-strength distribution. The history of applying incorrect statistical methods is nearly as long and such practices continue to the present. Examples include treating the applied stress (or strain) as the response and the number of cycles to failure as the explanatory variable in regression analyses (because of the need to estimate strength distributions) and ignoring or otherwise mishandling censored observations (known as runouts in the fatigue literature). The first part of the paper reviews the traditional modeling approach where a fatigue-life model is specified. We then show how this specification induces a corresponding fatigue-strength model. The second part of the paper presents a novel alternative modeling approach where a fatigue-strength model is specified and a corresponding fatigue-life model is induced. We explain and illustrate the important advantages of this new modeling approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04550v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Q. Meeker, Luis A. Escobar, Francis G. Pascual, Yili Hong, Peng Liu, Wayne M. Falk, Balajee Ananthasayanam</dc:creator>
    </item>
    <item>
      <title>Optimal Priors for the Discounting Parameter of the Normalized Power Prior</title>
      <link>https://arxiv.org/abs/2302.14230</link>
      <description>arXiv:2302.14230v2 Announce Type: replace 
Abstract: The power prior is a popular class of informative priors for incorporating information from historical data. It involves raising the likelihood for the historical data to a power, which acts as discounting parameter. When the discounting parameter is modelled as random, the normalized power prior is recommended. In this work, we prove that the marginal posterior for the discounting parameter for generalized linear models converges to a point mass at zero if there is any discrepancy between the historical and current data, and that it does not converge to a point mass at one when they are fully compatible. In addition, we explore the construction of optimal priors for the discounting parameter in a normalized power prior. In particular, we are interested in achieving the dual objectives of encouraging borrowing when the historical and current data are compatible and limiting borrowing when they are in conflict. We propose intuitive procedures for eliciting the shape parameters of a beta prior for the discounting parameter based on two minimization criteria, the Kullback-Leibler divergence and the mean squared error. Based on the proposed criteria, the optimal priors derived are often quite different from commonly used priors such as the uniform prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.14230v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueqi Shen, Luiz M. Carvalho, Matthew A. Psioda, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Cumulative differences between paired samples</title>
      <link>https://arxiv.org/abs/2305.11323</link>
      <description>arXiv:2305.11323v2 Announce Type: replace 
Abstract: The simplest, most common paired samples consist of observations from two populations, with each observed response from one population corresponding to an observed response from the other population at the same value of an ordinal covariate. The pair of observed responses (one from each population) at the same value of the covariate is known as a "matched pair" (with the matching based on the value of the covariate). A graph of cumulative differences between the two populations reveals differences in responses as a function of the covariate. Indeed, the slope of the secant line connecting two points on the graph becomes the average difference over the wide interval of values of the covariate between the two points; i.e., slope of the graph is the average difference in responses. ("Average" refers to the weighted average if the samples are weighted.) Moreover, a simple statistic known as the Kuiper metric summarizes into a single scalar the overall differences over all values of the covariate. The Kuiper metric is the absolute value of the total difference in responses between the two populations, totaled over the interval of values of the covariate for which the absolute value of the total is greatest. The total should be normalized such that it becomes the (weighted) average over all values of the covariate when the interval over which the total is taken is the entire range of the covariate (i.e., the sum for the total gets divided by the total number of observations, if the samples are unweighted, or divided by the total weight, if the samples are weighted). This cumulative approach is fully nonparametric and uniquely defined (with only one right way to construct the graphs and scalar summary statistics), unlike traditional methods such as reliability diagrams or parametric or semi-parametric regressions, which typically obscure significant differences due to their parameter settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11323v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isabel Kloumann, Hannah Korevaar, Chris McConnell, Mark Tygert, Jessica Zhao</dc:creator>
    </item>
    <item>
      <title>Gibbs Sampling using Anti-correlation Gaussian Data Augmentation, with Applications to L1-ball-type Models</title>
      <link>https://arxiv.org/abs/2309.09371</link>
      <description>arXiv:2309.09371v2 Announce Type: replace 
Abstract: L1-ball-type priors are a recent generalization of the spike-and-slab priors. By transforming a continuous precursor distribution to the L1-ball boundary, it induces exact zeros with positive prior and posterior probabilities. With great flexibility in choosing the precursor and threshold distributions, we can easily specify models under structured sparsity, such as those with dependent probability for zeros and smoothness among the non-zeros. Motivated to significantly accelerate the posterior computation, we propose a new data augmentation that leads to a fast block Gibbs sampling algorithm. The latent variable, named ``anti-correlation Gaussian'', cancels out the quadratic exponent term in the latent Gaussian distribution, making the parameters of interest conditionally independent so that they can be updated in a block. Compared to existing algorithms such as the No-U-Turn sampler, the new blocked Gibbs sampler has a very low computing cost per iteration and shows rapid mixing of Markov chains. We establish the geometric ergodicity guarantee of the algorithm in linear models. Further, we show useful extensions of our algorithm for posterior estimation of general latent Gaussian models, such as those involving multivariate truncated Gaussian or latent Gaussian process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09371v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Zheng, Leo L. Duan</dc:creator>
    </item>
    <item>
      <title>Automated threshold selection and associated inference uncertainty for univariate extremes</title>
      <link>https://arxiv.org/abs/2310.17999</link>
      <description>arXiv:2310.17999v3 Announce Type: replace 
Abstract: Threshold selection is a fundamental problem in any threshold-based extreme value analysis. While models are asymptotically motivated, selecting an appropriate threshold for finite samples is difficult and highly subjective through standard methods. Inference for high quantiles can also be highly sensitive to the choice of threshold. Too low a threshold choice leads to bias in the fit of the extreme value model, while too high a choice leads to unnecessary additional uncertainty in the estimation of model parameters. We develop a novel methodology for automated threshold selection that directly tackles this bias-variance trade-off. We also develop a method to account for the uncertainty in the threshold estimation and propagate this uncertainty through to high quantile inference. Through a simulation study, we demonstrate the effectiveness of our method for threshold selection and subsequent extreme quantile estimation, relative to the leading existing methods, and show how the method's effectiveness is not sensitive to the tuning parameters. We apply our method to the well-known, troublesome example of the River Nidd dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17999v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Murphy, Jonathan A. Tawn, Zak Varty</dc:creator>
    </item>
    <item>
      <title>Interaction Decomposition of prediction function</title>
      <link>https://arxiv.org/abs/2402.08239</link>
      <description>arXiv:2402.08239v2 Announce Type: replace 
Abstract: This paper discusses the foundation of methods for accurately grasping interaction effects. The partial dependence (PD) and accumulated local effects (ALE) methods, which capture interaction effects as terms, are known as global model-agnostic methods in the interpretable machine learning field. ALE provides a functional decomposition of the prediction function. In the present study, we propose and mathematically formalize the requirements of an interaction decomposition (ID) that decomposes a prediction function into its main and interaction effect terms. We also present a theorem by which a decomposition method meeting these requirements can be generated. Furthermore, we confirm that ALE is an ID but PD is not. Finally, we present examples of decomposition methods that meet the requirements of ID, using both existing methods and methods that differ from the existing ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08239v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirokazu Iwasawa, Yoshihiro Matsumori</dc:creator>
    </item>
    <item>
      <title>Applying Non-negative Matrix Factorization with Covariates to the Longitudinal Data as Growth Curve Model</title>
      <link>https://arxiv.org/abs/2403.05359</link>
      <description>arXiv:2403.05359v2 Announce Type: replace 
Abstract: Using Non-negative Matrix Factorization (NMF), the observed matrix can be approximated by the product of the basis and coefficient matrices. Moreover, if the coefficient vectors are explained by the covariates for each individual, the coefficient matrix can be written as the product of the parameter matrix and the covariate matrix, and additionally described in the framework of Non-negative Matrix tri-Factorization (tri-NMF) with covariates. Consequently, this is equal to the mean structure of the Growth Curve Model (GCM). The difference is that the basis matrix for GCM is given by the analyst, whereas that for NMF with covariates is unknown and optimized. In this study, we applied NMF with covariance to longitudinal data and compared it with GCM. We have also published an R package that implements this method, and we show how to use it through examples of data analyses including longitudinal measurement, spatiotemporal data and text data. In particular, we demonstrate the usefulness of Gaussian kernel functions as covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05359v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>Application-Driven Learning: A Closed-Loop Prediction and Optimization Approach Applied to Dynamic Reserves and Demand Forecasting</title>
      <link>https://arxiv.org/abs/2102.13273</link>
      <description>arXiv:2102.13273v5 Announce Type: replace-cross 
Abstract: Forecasting and decision-making are generally modeled as two sequential steps with no feedback, following an open-loop approach. In this paper, we present application-driven learning, a new closed-loop framework in which the processes of forecasting and decision-making are merged and co-optimized through a bilevel optimization problem. We present our methodology in a general format and prove that the solution converges to the best estimator in terms of the expected cost of the selected application. Then, we propose two solution methods: an exact method based on the KKT conditions of the second-level problem and a scalable heuristic approach suitable for decomposition methods. The proposed methodology is applied to the relevant problem of defining dynamic reserve requirements and conditional load forecasts, offering an alternative approach to current ad hoc procedures implemented in industry practices. We benchmark our methodology with the standard sequential least-squares forecast and dispatch planning process. We apply the proposed methodology to an illustrative system and to a wide range of instances, from dozens of buses to large-scale realistic systems with thousands of buses. Our results show that the proposed methodology is scalable and yields consistently better performance than the standard open-loop approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.13273v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joaquim Dias Garcia, Alexandre Street, Tito Homem-de-Mello, Francisco D. Mu\~noz</dc:creator>
    </item>
    <item>
      <title>A Non-Gaussian Bayesian Filter Using Power and Generalized Logarithmic Moments</title>
      <link>https://arxiv.org/abs/2211.13383</link>
      <description>arXiv:2211.13383v4 Announce Type: replace-cross 
Abstract: In this paper, we aim to propose a consistent non-Gaussian Bayesian filter of which the system state is a continuous function. The distributions of the true system states, and those of the system and observation noises, are only assumed Lebesgue integrable with no prior constraints on what function classes they fall within. This type of filter has significant merits in both theory and practice, which is able to ameliorate the curse of dimensionality for the particle filter, a popular non-Gaussian Bayesian filter of which the system state is parameterized by discrete particles and the corresponding weights. We first propose a new type of statistics, called the generalized logarithmic moments. Together with the power moments, they are used to form a density surrogate, parameterized as an analytic function, to approximate the true system state. The map from the parameters of the proposed density surrogate to both the power moments and the generalized logarithmic moments is proved to be a diffeomorphism, establishing the fact that there exists a unique density surrogate which satisfies both moment conditions. This diffeomorphism also allows us to use gradient methods to treat the convex optimization problem in determining the parameters. Last but not least, simulation results reveal the advantage of using both sets of moments for estimating mixtures of complicated types of functions. A robot localization simulation is also given, as an engineering application to validate the proposed filtering scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13383v4</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guangyu Wu, Anders Lindquist</dc:creator>
    </item>
    <item>
      <title>Skewed Bernstein-von Mises theorem and skew-modal approximations</title>
      <link>https://arxiv.org/abs/2301.03038</link>
      <description>arXiv:2301.03038v3 Announce Type: replace-cross 
Abstract: Gaussian approximations are routinely employed in Bayesian statistics to ease inference when the target posterior is intractable. Although these approximations are asymptotically justified by Bernstein-von Mises type results, in practice the expected Gaussian behavior may poorly represent the shape of the posterior, thus affecting approximation accuracy. Motivated by these considerations, we derive an improved class of closed-form approximations of posterior distributions which arise from a new treatment of a third-order version of the Laplace method yielding approximations in a tractable family of skew-symmetric distributions. Under general assumptions which account for misspecified models and non-i.i.d. settings, this family of approximations is shown to have a total variation distance from the target posterior whose rate of convergence improves by at least one order of magnitude the one established by the classical Bernstein-von Mises theorem. Specializing this result to the case of regular parametric models shows that the same improvement in approximation accuracy can be also derived for polynomially bounded posterior functionals. Unlike other higher-order approximations, our results prove that it is possible to derive closed-form and valid densities which are expected to provide, in practice, a more accurate, yet similarly-tractable, alternative to Gaussian approximations of the target posterior, while inheriting its limiting frequentist properties. We strengthen such arguments by developing a practical skew-modal approximation for both joint and marginal posteriors that achieves the same theoretical guarantees of its theoretical counterpart by replacing the unknown model parameters with the corresponding MAP estimate. Empirical studies confirm that our theoretical results closely match the remarkable performance observed in practice, even in finite, possibly small, sample regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.03038v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Durante, Francesco Pozza, Botond Szabo</dc:creator>
    </item>
    <item>
      <title>Estimators for multivariate allometric regression model</title>
      <link>https://arxiv.org/abs/2402.11219</link>
      <description>arXiv:2402.11219v3 Announce Type: replace-cross 
Abstract: In a regression model with multiple response variables and multiple explanatory variables, if the difference of the mean vectors of the response variables for different values of explanatory variables is always in the direction of the first principal eigenvector of the covariance matrix of the response variables, then it is called a multivariate allometric regression model. This paper studies the estimation of the first principal eigenvector in the multivariate allometric regression model. A class of estimators that includes conventional estimators is proposed based on weighted sum-of-squares matrices of regression sum-of-squares matrix and residual sum-of-squares matrix. We establish an upper bound of the mean squared error of the estimators contained in this class, and the weight value minimizing the upper bound is derived. Sufficient conditions for the consistency of the estimators are discussed in weak identifiability regimes under which the difference of the largest and second largest eigenvalues of the covariance matrix decays asymptotically and in "large $p$, large $n$" regimes, where $p$ is the number of response variables and $n$ is the sample size. Several numerical results are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11219v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koji Tsukuda, Shun Matsuura</dc:creator>
    </item>
    <item>
      <title>WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology</title>
      <link>https://arxiv.org/abs/2403.15238</link>
      <description>arXiv:2403.15238v2 Announce Type: replace-cross 
Abstract: Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI). Weakly supervised learning of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading). In this context, there is a need for improved spatial interpretability of predictions from such models. We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation. It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label. We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology. WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15238v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abhinav Sharma, Bojing Liu, Mattias Rantalainen</dc:creator>
    </item>
  </channel>
</rss>
