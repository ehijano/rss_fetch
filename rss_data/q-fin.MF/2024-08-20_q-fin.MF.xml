<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.MF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.MF</link>
    <description>q-fin.MF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.MF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 07:33:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal stopping and divestment timing under scenario ambiguity and learning</title>
      <link>https://arxiv.org/abs/2408.09349</link>
      <description>arXiv:2408.09349v1 Announce Type: new 
Abstract: Aiming to analyze the impact of environmental transition on the value of assets and on asset stranding, we study optimal stopping and divestment timing decisions for an economic agent whose future revenues depend on the realization of a scenario from a given set of possible futures. Since the future scenario is unknown and the probabilities of individual prospective scenarios are ambiguous, we adopt the smooth model of decision making under ambiguity aversion of Klibanoff et al (2005), framing the optimal divestment decision as an optimal stopping problem with learning under ambiguity aversion. We then prove a minimax result reducing this problem to a series of standard optimal stopping problems with learning. The theory is illustrated with two examples: the problem of optimally selling a stock with ambigous drift, and the problem of optimal divestment from a coal-fired power plant under transition scenario ambiguity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09349v1</guid>
      <category>q-fin.MF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Mazzon, Peter Tankov</dc:creator>
    </item>
    <item>
      <title>Periodic Trading Activities in Financial Markets: Mean-field Liquidation Game with Major-Minor Players</title>
      <link>https://arxiv.org/abs/2408.09505</link>
      <description>arXiv:2408.09505v1 Announce Type: new 
Abstract: Motivated by recent empirical findings on the periodic phenomenon of aggregated market volumes in equity markets, we aim to understand the causes and consequences of periodic trading activities through a game-theoretic perspective, examining market interactions among different types of participants. Specifically, we introduce a new mean-field liquidation game involving major and minor traders, where the major trader evaluates her strategy against a periodic targeting strategy while a continuum of minor players trade against her. We establish the existence and uniqueness of an open-loop Nash equilibrium. In addition, we prove an O(1/sqrt N) approximation rate of the mean-field solution to the Nash equilibrium in a major-minor game with N minor players. In equilibrium, minor traders exhibit front-running behaviors in both the periodic and trend components of their strategies, reducing the major trader's profit. Such strategic interactions diminish the strength of periodicity in both overall trading volume and asset prices. Our model rationalizes observed periodic trading activities in the market and offers new insights into market dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09505v1</guid>
      <category>q-fin.MF</category>
      <category>econ.TH</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yufan Chen, Lan Wu, Renyuan Xu, Ruixun Zhang</dc:creator>
    </item>
    <item>
      <title>Learning to Optimally Stop a Diffusion Process</title>
      <link>https://arxiv.org/abs/2408.09242</link>
      <description>arXiv:2408.09242v1 Announce Type: cross 
Abstract: We study optimal stopping for a diffusion process with unknown model primitives within the continuous-time reinforcement learning (RL) framework developed by Wang et al. (2020). By penalizing its variational inequality, we transform the stopping problem into a stochastic optimal control problem with two actions. We then randomize control into Bernoulli distributions and add an entropy regularizer to encourage exploration. We derive a semi-analytical optimal Bernoulli distribution, based on which we devise RL algorithms using the martingale approach established in Jia and Zhou (2022a) and prove a policy improvement theorem. Finally, we demonstrate the effectiveness of the algorithms in examples of pricing finite-horizon American put options and solving Merton's problem with transaction costs, and show that both the offline and online algorithms achieve high accuracy in learning the value functions and characterizing the associated free boundaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09242v1</guid>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <category>q-fin.PR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Min Dai, Yu Sun, Zuo Quan Xu, Xun Yu Zhou</dc:creator>
    </item>
    <item>
      <title>Exploratory Optimal Stopping: A Singular Control Formulation</title>
      <link>https://arxiv.org/abs/2408.09335</link>
      <description>arXiv:2408.09335v1 Announce Type: cross 
Abstract: This paper explores continuous-time and state-space optimal stopping problems from a reinforcement learning perspective. We begin by formulating the stopping problem using randomized stopping times, where the decision maker's control is represented by the probability of stopping within a given time--specifically, a bounded, non-decreasing, c\`adl\`ag control process. To encourage exploration and facilitate learning, we introduce a regularized version of the problem by penalizing it with the cumulative residual entropy of the randomized stopping time. The regularized problem takes the form of an (n+1)-dimensional degenerate singular stochastic control with finite-fuel. We address this through the dynamic programming principle, which enables us to identify the unique optimal exploratory strategy. For the specific case of a real option problem, we derive a semi-explicit solution to the regularized problem, allowing us to assess the impact of entropy regularization and analyze the vanishing entropy limit. Finally, we propose a reinforcement learning algorithm based on policy iteration. We show both policy improvement and policy convergence results for our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09335v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>q-fin.MF</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jodi Dianetti, Giorgio Ferrari, Renyuan Xu</dc:creator>
    </item>
    <item>
      <title>Set-valued stochastic integrals for convoluted L\'{e}vy processes</title>
      <link>https://arxiv.org/abs/2312.01730</link>
      <description>arXiv:2312.01730v2 Announce Type: replace-cross 
Abstract: In this paper we study set-valued Volterra-type stochastic integrals driven by L\'{e}vy processes. Upon extending the classical definitions of set-valued stochastic integral functionals to convoluted integrals with square-integrable kernels, set-valued convoluted stochastic integrals are defined by taking the closed decomposable hull of the integral functionals for generic time. We show that, aside from well-established results for set-valued It\^{o} integrals, while set-valued stochastic integrals with respect to a finite-variation Poisson random measure are guaranteed to be integrably bounded for bounded integrands, this is not true when the random measure is of infinite variation. For indefinite integrals, we prove that it is a mutual effect of kernel singularity and jumps that the set-valued convoluted integrals are possibly explosive and take extended vector values. These results have some important implications on how set-valued fractional dynamical systems are to be constructed in general. Two classes of set-monotone processes are studied for practical interests in economic and financial modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01730v2</guid>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixuan Xia</dc:creator>
    </item>
  </channel>
</rss>
