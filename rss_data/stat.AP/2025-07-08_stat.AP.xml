<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 01:41:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Random time-shift approximation enables hierarchical Bayesian inference of mechanistic within-host viral dynamics models on large datasets</title>
      <link>https://arxiv.org/abs/2507.02884</link>
      <description>arXiv:2507.02884v1 Announce Type: new 
Abstract: Mechanistic mathematical models of within-host viral dynamics are tools for understanding how a virus' biology and its interaction with the immune system shape the infectivity of a host. The biology of the process is encoded by the structure and parameters of the model that can be inferred statistically by fitting to viral load data. The main drawback of mechanistic models is that this inference is computationally expensive because the model must be repeatedly solved. This limits the size of the datasets that can be considered or the complexity of the models fitted. In this paper we develop a much cheaper inference method by implementing a novel approximation of the model dynamics that uses a combination of random and deterministic processes. This approximation also properly accounts for process noise early in the infection when cell and virion numbers are small, which is important for the viral dynamics but often overlooked. Our method runs on a consumer laptop and is fast enough to facilitate a full hierarchical Bayesian treatment of the problem with sharing of information to allow for individual level parameter differences. We apply our method to simulated data and a reanalysis of COVID-19 monitoring data in an National Basketball Association cohort of 163 individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02884v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan J. Morris, Lauren Kennedy, Andrew J. Black</dc:creator>
    </item>
    <item>
      <title>A Novel Method to Manage Production on Industry 4.0: Forecasting Overall Equipment Efficiency by Time Series with Topological Features</title>
      <link>https://arxiv.org/abs/2507.02890</link>
      <description>arXiv:2507.02890v1 Announce Type: new 
Abstract: Purpose: Overall equipment efficiency (OEE) is a key manufacturing KPI, but its volatile nature complicates short-term forecasting. This study presents a novel framework combining time series decomposition and topological data analysis to improve OEE prediction across various equipment, such as hydraulic press systems.
  Methods: The approach begins by decomposing hourly OEE data into trend, seasonal, and residual components. The residual, capturing short-term variability, is modeled using a seasonal ARIMA with exogenous variables (SARIMAX). These exogenous features include statistical descriptors and topological summaries from related time series. To manage the high-dimensional input space, we propose a hybrid feature selection strategy using recursive feature elimination based on statistically significant SARIMAX predictors, coupled with BIC-guided particle swarm optimization. The framework is evaluated on real-world datasets from multiple production systems.
  Results: The proposed model consistently outperforms conventional time series models and advanced transformer-based approaches, achieving significantly lower mean absolute error and mean absolute percentage error.
  Conclusion: Integrating classical forecasting with topological data analysis enhances OEE prediction accuracy, enabling proactive maintenance and informed production decisions in complex manufacturing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02890v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Korkut Anapa, \.Ismail G\"uzel, Ceylan Yozgatl{\i}gil</dc:creator>
    </item>
    <item>
      <title>TaxaPLN: a taxonomy-aware augmentation strategy for microbiome-trait classification including metadata</title>
      <link>https://arxiv.org/abs/2507.03588</link>
      <description>arXiv:2507.03588v1 Announce Type: new 
Abstract: The gut microbiome plays a crucial role in human health, making it a corner stone of modern biomedical research. To study its structure and dynamics, machine learning models are increasingly used to identify key microbial patterns associated with disease and environmental factors. However, microbiome data present unique challenges due to their compositionality, high-dimensionality, sparsity, and high variability, which can obscure meaningful signals. Besides, the effectiveness of machine learning models is often constrained by limited sample sizes, as microbiome data collection remains costly and time consuming. In this context, data augmentation has emerged as a promising strategy to enhance model robustness and predictive performance by generating artificial microbiome data. The aim of this study is to improve predictive modeling from microbiome data by introducing a model-based data augmentation approach that incorporates both taxonomic relationships and covariate information. To that end, we propose TaxaPLN, a data augmentation method built on PLN-Tree generative models, which leverages the taxonomy and a data-driven sampler to generate realistic synthetic microbiome compositions. We further introduce a conditional extension based on feature-wise linear modulation, enabling covariate-aware generation. Experiments on high-quality curated microbiome datasets show that TaxaPLN preserves ecological properties and generally improves or maintains predictive performances, particularly with non-linear classifiers, outperforming state-of-the-art baselines. Besides, TaxaPLN conditional augmentation establishes a novel benchmark for covariate-aware microbiome augmentation. The MIT-licensed source code is available at https://github.com/ AlexandreChaussard/PLNTree-package along with the datasets used in our experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03588v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Chaussard, Anna Bonnet, Sylvain Le Corff, Harry Sokol</dc:creator>
    </item>
    <item>
      <title>Forecasting the U.S. Renewable-Energy Mix with an ALR-BDARMA Compositional Time-Series Framework</title>
      <link>https://arxiv.org/abs/2507.04087</link>
      <description>arXiv:2507.04087v2 Announce Type: new 
Abstract: Accurate forecasts of the US renewable-generation mix are critical for planning transmission upgrades, sizing storage, and setting balancing-market rules. We present a Bayesian Dirichlet ARMA (BDARMA) model for monthly shares of hydro, geothermal, solar, wind, wood, municipal waste, and biofuels from January 2010 to January 2025. The mean vector follows a parsimonious VAR(2) in additive-log-ratio space, while the Dirichlet concentration parameter combines an intercept with ten Fourier harmonics, letting predictive dispersion expand or contract with the seasons.
  A 61-split rolling-origin study generates twelve-month density forecasts from January 2019 to January 2024. Relative to three benchmarks, a Gaussian VAR(2) in transform space, a seasonal naive copy of last year's proportions, and a drift-free additive-log-ratio random walk, BDARMA lowers the mean continuous ranked probability score by fifteen to sixty percent, achieves component-wise ninety percent interval coverage close to nominal, and matches Gaussian VAR point accuracy through eight months with a maximum loss of 0.02 Aitchison units thereafter. BDARMA therefore delivers sharp, well-calibrated probabilistic forecasts of multivariate renewable-energy shares without sacrificing point precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04087v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz</dc:creator>
    </item>
    <item>
      <title>Multi-country forecasts of age distribution of deaths: Life expectancy and annuity valuation</title>
      <link>https://arxiv.org/abs/2507.04303</link>
      <description>arXiv:2507.04303v1 Announce Type: new 
Abstract: We investigate two transformations within the framework of compositional data analysis for forecasting the age distribution of death counts. Drawing on age-specific period life-table death counts from 24 countries in the Human Mortality Database, we assess and compare the point and interval forecast accuracy of the two transformations. Enhancing the forecast accuracy of period life-table death counts holds significant value for demographers, who rely on such forecasts to estimate survival probabilities and life expectancy, and for actuaries, who use them to price temporary annuities across various entry ages and maturities. While our primary focus is on temporary annuities, we also consider long-term contracts that, particularly at higher entry ages, approximate lifetime annuities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04303v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Variance-based variable selection in sensor calibration with strong interferents -- application to air pollution monitoring with a carbon nanotube sensor array</title>
      <link>https://arxiv.org/abs/2507.05001</link>
      <description>arXiv:2507.05001v1 Announce Type: new 
Abstract: Air and water pollution are major threats to public health, highlighting the need for reliable environmental monitoring. Low-cost multisensor systems are promising but suffer from limited selectivity, because their responses are influenced by non-target variables (interferents) such as temperature and humidity. This complicates pollutant detection, especially in data-driven models with noisy, correlated inputs. We propose a method for selecting the most relevant interferents for sensor calibration, balancing performance and cost. Including too many variables can lead to overfitting, while omitting key variables reduces accuracy. Our approach evaluates numerous models using a bias-variance trade-off and variance analysis. The method is first validated on simulated data to assess strengths and limitations, then applied to a carbon nanotube-based sensor array deployed outdoors to characterize its sensitivity to air pollutants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05001v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marine Dumon, Berengere Lebental, Guillaume Perrin</dc:creator>
    </item>
    <item>
      <title>Statistical-Spatial Model for Motor Potentials Evoked Through Transcranial Magnetic Stimulation for the Development of Closed-Loop Procedures</title>
      <link>https://arxiv.org/abs/2507.03416</link>
      <description>arXiv:2507.03416v1 Announce Type: cross 
Abstract: The primary motor cortex appears to be in the center of transcranial magnetic stimulation (TMS). It is one of few locations that provide directly observable responses, and its physiology serves as model or reference for almost all other TMS targets, e.g., through the motor threshold and spatial targeting relative to its position. It furthermore sets the safety limits for the entire brain. Its easily detectable responses have led to closed-loop methods for a range of aspects, e.g., for automated thresholding, amplitude tracking, and targeting. The high variability of brain stimulation methods would substantially benefit from fast unbiased closed-loop methods. However, the development of more potent methods would early on in the design phase require proper models that allowed tuning and testing with sufficient without a high number of experiments, which are time-consuming and expensive or even impossible at the needed scale. On the one hand, theoretical researchers without access to experiments miss realistic spatial response models of brain stimulation to develop better methods. On the other hand, subjects should potentially not be exposed to early closed-loop-methods without sufficient prior testing as not yet well tuned feed-back as needed for closed-loop operation is known to erratic behavior.
  To bridge this gap, we developed a digital-twin-style population model that generates motor evoked potentials in response to virtual stimuli and includes statistical information on spatial (coil position and orientation) as well as recruitment in the population to represent inter- and intra-individual variability. The model allows users to simulate different subjects and millions of runs for software-in-the loop testing. The model includes all code to stimulate further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03416v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Farahmandrad, Stefan Goetz</dc:creator>
    </item>
    <item>
      <title>Nonparametric regression for cost-effectiveness analyses with observational data -- a tutorial</title>
      <link>https://arxiv.org/abs/2507.03511</link>
      <description>arXiv:2507.03511v1 Announce Type: cross 
Abstract: Healthcare decision-making often requires selecting among treatment options under budget constraints, particularly when one option is more effective but also more costly. Cost-effectiveness analysis (CEA) provides a framework for evaluating whether the health benefits of a treatment justify its additional costs. A key component of CEA is the estimation of treatment effects on both health outcomes and costs, which becomes challenging when using observational data, due to potential confounding. While advanced causal inference methods exist for use in such circumstances, their adoption in CEAs remains limited, with many studies relying on overly simplistic methods such as linear regression or propensity score matching. We believe that this is mainly due to health economists being generally unfamiliar with superior methodology. In this paper, we address this gap by introducing cost-effectiveness researchers to modern nonparametric regression models, with a particular focus on Bayesian Additive Regression Trees (BART). We provide practical guidance on how to implement BART in CEAs, including code examples, and discuss its advantages in producing more robust and credible estimates from observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03511v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Esser, Mateus Maia, Judith Bosmans, Johanna van Dongen</dc:creator>
    </item>
    <item>
      <title>Generating Synthetic Relational Tabular Data via Structural Causal Models</title>
      <link>https://arxiv.org/abs/2507.03528</link>
      <description>arXiv:2507.03528v1 Announce Type: cross 
Abstract: Synthetic tabular data generation has received increasing attention in recent years, particularly with the emergence of foundation models for tabular data. The breakthrough success of TabPFN (Hollmann et al.,2025), which leverages vast quantities of synthetic tabular datasets derived from structural causal models (SCMs), demonstrates the critical role synthetic data plays in developing powerful tabular foundation models. However, most real-world tabular data exists in relational formats spanning multiple interconnected tables - a structure not adequately addressed by current generation methods. In this work, we extend the SCM-based approach by developing a novel framework that generates realistic synthetic relational tabular data including causal relationships across tables. Our experiments confirm that this framework is able to construct relational datasets with complex inter-table dependencies mimicking real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03528v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Hoppe, Astrid Franz, Lars Kleinemeier, Udo G\"obel</dc:creator>
    </item>
    <item>
      <title>Multivariate MRP</title>
      <link>https://arxiv.org/abs/2507.03652</link>
      <description>arXiv:2507.03652v1 Announce Type: cross 
Abstract: Measuring public opinion at subnational geographies is critical to many theories in political science. Multilevel regression and post-stratification (MRP) is a popular tool for doing so, although existing work is limited to measuring opinion on a single survey question. We provide a framework for estimating the joint distribution of opinion on multiple questions ("Multivariate MRP"). To do so, we derive a novel method for variational inference in multinomial logistic regression with many random effects. This requires performing variational inference with high-dimensional fixed effects, but we show that this can be done at a low computational cost. We validate this procedure by estimating public opinion by party in the United States and show that existing methods can be improved considerably by adding contextual covariates on the prior levels of party identification. Substantively, we show how the output of multivariate MRP can be used to study representation across multiple policy issues simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03652v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Goplerud, Michael Auslen</dc:creator>
    </item>
    <item>
      <title>Regularizing Log-Linear Cost Models for Inpatient Stays by Merging ICD-10 Codes</title>
      <link>https://arxiv.org/abs/2507.03843</link>
      <description>arXiv:2507.03843v1 Announce Type: cross 
Abstract: Cost models in healthcare research must balance interpretability, accuracy, and parameter consistency. However, interpretable models often struggle to achieve both accuracy and consistency. Ordinary least squares (OLS) models for high-dimensional regression can be accurate but fail to produce stable regression coefficients over time when using highly granular ICD-10 diagnostic codes as predictors. This instability arises because many ICD-10 codes are infrequent in healthcare datasets. While regularization methods such as Ridge can address this issue, they risk discarding important predictors. Here, we demonstrate that reducing the granularity of ICD-10 codes is an effective regularization strategy within OLS while preserving the representation of all diagnostic code categories. By truncating ICD-10 codes from seven characters (e.g., T67.0XXA, T67.0XXD) to six (e.g., T67.0XX) or fewer, we reduce the dimensionality of the regression problem while maintaining model interpretability and consistency. Mathematically, the merging of predictors in OLS leads to increased trace of the Hessian matrix, which reduces the variance of coefficient estimation. Our findings explain why broader diagnostic groupings like DRGs and HCC codes are favored over highly granular ICD-10 codes in real-world risk adjustment and cost models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03843v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi-Ken Lu, David Alonge, Nicole Richardson, Bruno Richard</dc:creator>
    </item>
    <item>
      <title>Adaptive Designs in Fast-Track Registration Processes</title>
      <link>https://arxiv.org/abs/2507.04092</link>
      <description>arXiv:2507.04092v1 Announce Type: cross 
Abstract: Fast-track procedures play an important role in the context of conditional registration of health products, such as conditional approval processes and listing processes for digital health applications. Fast-track procedures offer the potential for earlier patient access to innovative products. They involve two registration steps. The applicants can apply first for conditional registration. A successful conditional registration provides a limited funding or approval period and time to prepare the application for permanent registration, which is the second step of the registration process. For conditional registration, typically, products have to fulfil only a part of the requirements necessary for permanent registration. There is interest in valid and efficient study designs for fast-track procedures. This will be addressed in this paper. A motivating example of the paper is the German fast-track registration process of digital health applications (DiGA) for reimbursement by statutory health insurances. The main focus of the paper is the systematic investigation of the utility of adaptive designs in the context of fast-track registration processes. The paper also covers a careful discussion of the different requirements found in the guidelines and their consequences. We demonstrate that the use of adaptive designs in the context of fast-track processes like the DiGA registration process is, in most cases, much more efficient than the current standard of two separate studies. The results presented in this paper are based on numerical calculations supported by mathematical arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04092v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liane Kluge, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking</title>
      <link>https://arxiv.org/abs/2507.04116</link>
      <description>arXiv:2507.04116v1 Announce Type: cross 
Abstract: This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04116v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fred Lydeard, Bashar I. Ahmad, Simon Godsill</dc:creator>
    </item>
    <item>
      <title>Predicting Air Pollution in Cork, Ireland Using Machine Learning</title>
      <link>https://arxiv.org/abs/2507.04196</link>
      <description>arXiv:2507.04196v1 Announce Type: cross 
Abstract: Air pollution poses a critical health threat in cities worldwide, with nitrogen dioxide levels in Cork, Ireland exceeding World Health Organization safety standards by up to $278\%$. This study leverages artificial intelligence to predict air pollution with unprecedented accuracy, analyzing nearly ten years of data from five monitoring stations combined with 30 years of weather records. We evaluated 17 machine learning algorithms, with Extra Trees emerging as the optimal solution, achieving $77\%$ prediction accuracy and significantly outperforming traditional forecasting methods. Our analysis reveals that meteorological conditions particularly temperature, wind speed, and humidity are the primary drivers of pollution levels, while traffic patterns and seasonal changes create predictable pollution cycles. Pollution exhibits dramatic seasonal variations, with winter levels nearly double those of summer, and daily rush-hour peaks reaching $120\%$ above normal levels. While Cork's air quality shows concerning violations of global health standards, our models detected an encouraging $31\%$ improvement from 2014 to 2022. This research demonstrates that intelligent forecasting systems can provide city planners and environmental officials with powerful prediction tools, enabling life-saving early warning systems and informed urban planning decisions. The technology exists today to transform urban air quality management. All research materials and code are freely available at: https://github.com/MdRashidunnabi/Air-Pollution-Analysis.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04196v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Rashidunnabi, Fahmida Faiza Ananna, Kailash Hambarde, Bruno Gabriel Nascimento Andrade, Dean Venables, Hugo Proenca</dc:creator>
    </item>
    <item>
      <title>Normalizing Flow to Augmented Posterior: Conditional Density Estimation with Interpretable Dimension Reduction for High Dimensional Data</title>
      <link>https://arxiv.org/abs/2507.04216</link>
      <description>arXiv:2507.04216v1 Announce Type: cross 
Abstract: The conditional density characterizes the distribution of a response variable $y$ given other predictor $x$, and plays a key role in many statistical tasks, including classification and outlier detection. Although there has been abundant work on the problem of Conditional Density Estimation (CDE) for a low-dimensional response in the presence of a high-dimensional predictor, little work has been done for a high-dimensional response such as images. The promising performance of normalizing flow (NF) neural networks in unconditional density estimation acts a motivating starting point. In this work, we extend NF neural networks when external $x$ is present. Specifically, they use the NF to parameterize a one-to-one transform between a high-dimensional $y$ and a latent $z$ that comprises two components \([z_P,z_N]\). The $z_P$ component is a low-dimensional subvector obtained from the posterior distribution of an elementary predictive model for $x$, such as logistic/linear regression. The $z_N$ component is a high-dimensional independent Gaussian vector, which explains the variations in $y$ not or less related to $x$. Unlike existing CDE methods, the proposed approach, coined Augmented Posterior CDE (AP-CDE), only requires a simple modification on the common normalizing flow framework, while significantly improving the interpretation of the latent component, since $z_P$ represents a supervised dimension reduction. In image analytics applications, AP-CDE shows good separation of $x$-related variations due to factors such as lighting condition and subject id, from the other random variations. Further, the experiments show that an unconditional NF neural network, based on an unsupervised model of $z$, such as Gaussian mixture, fails to generate interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04216v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cheng Zeng, George Michailidis, Hitoshi Iyatomi, Leo L Duan</dc:creator>
    </item>
    <item>
      <title>Inverse Probability Weighting for Recurrent Event Models</title>
      <link>https://arxiv.org/abs/2507.04567</link>
      <description>arXiv:2507.04567v1 Announce Type: cross 
Abstract: Recurrent events are common and important clinical trial endpoints in many disease areas, e.g., cardiovascular hospitalizations in heart failure, relapses in multiple sclerosis, or exacerbations in asthma. During a trial, patients may experience intercurrent events, that is, events after treatment assignment which affect the interpretation or existence of the outcome of interest. In many settings, a treatment effect in the scenario in which the intercurrent event would not occur is of clinical interest. A proper estimation method of such a hypothetical treatment effect has to account for all confounders of the recurrent event process and the intercurrent event. In this paper, we propose estimators targeting hypothetical estimands in recurrent events with proper adjustments of baseline and internal time-varying covariates. Specifically, we apply inverse probability weighting (IPW) to the commonly used Lin-Wei-Yang-Ying (LWYY) and negative binomial (NB) models in recurrent event analysis. Simulation studies demonstrate that our approach outperforms alternative analytical methods in terms of bias and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04567v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tobias Mutze, Richard Cook, Tianmeng Lyu</dc:creator>
    </item>
    <item>
      <title>Practical considerations for Gaussian Process modeling for causal inference quasi-experimental studies with panel data</title>
      <link>https://arxiv.org/abs/2507.05128</link>
      <description>arXiv:2507.05128v1 Announce Type: cross 
Abstract: Estimating causal effects in quasi-experiments with spatio-temporal panel data often requires adjusting for unmeasured confounding that varies across space and time. Gaussian Processes (GPs) offer a flexible, nonparametric modeling approach that can account for such complex dependencies through carefully chosen covariance kernels. In this paper, we provide a practical and interpretable framework for applying GPs to causal inference in panel data settings. We demonstrate how GPs generalize popular methods such as synthetic control and vertical regression, and we show that the GP posterior mean can be represented as a weighted average of observed outcomes, where the weights reflect spatial and temporal similarity. To support applied use, we explore how different kernel choices impact both estimation performance and interpretability, offering guidance for selecting between separable and nonseparable kernels. Through simulations and application to Hurricane Katrina mortality data, we illustrate how GP models can be used to estimate counterfactual outcomes and quantify treatment effects. All code and materials are made publicly available to support reproducibility and encourage adoption. Our results suggest that GPs are a promising and interpretable tool for addressing unmeasured spatio-temporal confounding in quasi-experimental studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05128v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia L. Vega, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Blind Targeting: Personalization under Third-Party Privacy Constraints</title>
      <link>https://arxiv.org/abs/2507.05175</link>
      <description>arXiv:2507.05175v1 Announce Type: cross 
Abstract: Major advertising platforms recently increased privacy protections by limiting advertisers' access to individual-level data. Instead of providing access to granular raw data, the platforms only allow a limited number of aggregate queries to a dataset, which is further protected by adding differentially private noise. This paper studies whether and how advertisers can design effective targeting policies within these restrictive privacy preserving data environments. To achieve this, I develop a probabilistic machine learning method based on Bayesian optimization, which facilitates dynamic data exploration. Since Bayesian optimization was designed to sample points from a function to find its maximum, it is not applicable to aggregate queries and to targeting. Therefore, I introduce two innovations: (i) integral updating of posteriors which allows to select the best regions of the data to query rather than individual points and (ii) a targeting-aware acquisition function that dynamically selects the most informative regions for the targeting task. I identify the conditions of the dataset and privacy environment that necessitate the use of such a "smart" querying strategy. I apply the strategic querying method to the Criteo AI Labs dataset for uplift modeling (Diemert et al., 2018) that contains visit and conversion data from 14M users. I show that an intuitive benchmark strategy only achieves 33% of the non-privacy-preserving targeting potential in some cases, while my strategic querying method achieves 97-101% of that potential, and is statistically indistinguishable from Causal Forest (Athey et al., 2019): a state-of-the-art non-privacy-preserving machine learning targeting method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05175v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anya Shchetkina</dc:creator>
    </item>
    <item>
      <title>Constraining the Milky Way Halo Accretion History With Simulated Stellar Halos: Designing the HALO7D-X Survey</title>
      <link>https://arxiv.org/abs/2507.05239</link>
      <description>arXiv:2507.05239v1 Announce Type: cross 
Abstract: We present the design for HALO7D-X, a survey of the stellar halo to investigate the accretion history of the Milky Way. The survey will use a combination of Hubble Space Telescope (HST) and Gaia data for sky position and proper motions of faint stars (18&lt;G&lt;21.5 mag), while line-of-sight velocity, distance, [Fe/H], and [alpha/Fe] will be measured using follow-up Keck spectroscopy. The survey will cover 30 lines of sight, made up of multiple HST archival fields and optimized for Keck DEIMOS spectroscopy. We use mock survey observations of the Bullock and Johnston stellar halo simulations to investigate the sensitivity of HALO7D-X to constrain the basic parameters of the accretion history of our Galaxy's stellar halo. We find that we are sensitive to the mass distribution and accretion timeline of the stellar halo progenitors, but not their orbital circularity. We find that the simulated halos fall into three different groups based on the similarities in their distributions of the observable dimensions of our survey. These groups are also distinct from each other in the mass distribution and accretion timeline of their progenitor satellites, showing that by using similarities in our observables among halos, we are able to identify similarities in their accretion histories. With HALO7D-X we will compare real Milky Way data with simulated halos and use this connection between observables and progenitor mass and accretion timeline to learn about the formation of our Galaxy's stellar halo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05239v1</guid>
      <category>astro-ph.GA</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miranda Apfel, Kevin A. McKinnon, Constance M. Rockosi, Puragra Guhathakurta, Kathryn V. Johnston</dc:creator>
    </item>
    <item>
      <title>Implementing Response-Adaptive Randomisation in Stratified Rare-disease Trials: Design Challenges and Practical Solutions</title>
      <link>https://arxiv.org/abs/2410.03346</link>
      <description>arXiv:2410.03346v2 Announce Type: replace 
Abstract: Although response-adaptive randomisation (RAR) has gained substantial attention in the literature, it still has limited use in clinical trials. Amongst other reasons, the implementation of RAR in real world trials raises important practical questions, often neglected in the technical literature. Motivated by an innovative phase-II stratified RAR rare-disease trial, this paper addresses two challenges: (1) How to ensure that RAR allocations are desirable i.e. both acceptable and faithful to the intended probabilities, particularly in small samples? and (2) What adaptations to trigger after interim analyses in the presence of missing data? To answer (1), we propose a Mapping strategy that discretises the randomisation probabilities into a vector of allocation ratios, resulting in improved frequentist errors. Under the implementation of Mapping, we answer (2) by analysing the impact of missing data on operating characteristics in selected scenarios. Finally, we discuss additional concerns including: pooling data across trial strata, analysing the level of blinding in the trial, and reporting safety results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03346v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajenki Das, Nina Deliu, Mark Toshner, Sof\'ia S Villar</dc:creator>
    </item>
    <item>
      <title>Identifying good forecasters via adaptive cognitive tests</title>
      <link>https://arxiv.org/abs/2411.11126</link>
      <description>arXiv:2411.11126v2 Announce Type: replace 
Abstract: Assessing forecasting performance is a time intensive activity, often requiring months or years before we know whether or not the reported forecasts were accurate. Cognitive tests can be quickly administered and are predictive of forecasting performance, but it is unclear which and how many tests are optimal. In this study, we develop adaptive cognitive tests that optimize the selection and efficiency of cognitive tests to assess forecasters of different skill levels. The tests are based on item response models and the adaptive testing procedures commonly used in educational testing. We show how the procedures can select highly informative cognitive tests from a larger battery of tests, thereby reducing the time taken to administer the tests. We use a second, independent dataset to show that the selected tests yield scores that are highly related to out-of-sample forecasting performance. The approach enables real-time, adaptive testing, providing immediate insights into forecasting talent in practical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11126v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edgar C. Merkle, Nikolay Petrov, Sophie Ma Zhu, Ezra Karger, Philip E. Tetlock, Mark Himmelstein</dc:creator>
    </item>
    <item>
      <title>On misconceptions about the Brier score in binary prediction models</title>
      <link>https://arxiv.org/abs/2504.04906</link>
      <description>arXiv:2504.04906v4 Announce Type: replace 
Abstract: The Brier score is a widely used metric evaluating overall performance of probabilistic predictions for binary outcomes in clinical research. However, its interpretation can be complex, as it does not align with commonly taught concepts in medical statistics. Consequently, the Brier score is often misinterpreted, sometimes to a significant extent, a fact that has not been adequately addressed in the literature. We aim to explore prevalent misconceptions surrounding the Brier score and elucidate the understanding and interpretation of Brier scores for practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04906v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linard Hoessly</dc:creator>
    </item>
    <item>
      <title>On the retraining frequency of global forecasting models</title>
      <link>https://arxiv.org/abs/2505.00356</link>
      <description>arXiv:2505.00356v2 Announce Type: replace 
Abstract: In an era of increasing computational capabilities and growing environmental consciousness, organizations face a critical challenge in balancing the accuracy of forecasting models with computational efficiency and sustainability. Global forecasting models, lowering the computational time, have gained significant attention over the years. However, the common practice of retraining these models with new observations raises important questions about the costs of forecasting. Using ten different machine learning and deep learning models, we analyzed various retraining scenarios, ranging from continuous updates to no retraining at all, across two large retail datasets. We showed that less frequent retraining strategies maintain the forecast accuracy while reducing the computational costs, providing a more sustainable approach to large-scale forecasting. We also found that machine learning models are a marginally better choice to reduce the costs of forecasting when coupled with less frequent model retraining strategies as the frequency of the data increases. Our findings challenge the conventional belief that frequent retraining is essential for maintaining forecasting accuracy. Instead, periodic retraining offers a good balance between predictive performance and efficiency, both in the case of point and probabilistic forecasting. These insights provide actionable guidelines for organizations seeking to optimize forecasting pipelines while reducing costs and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00356v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
    <item>
      <title>Coherent Track Before Detect: Detection via simultaneous trajectory estimation and long time integration</title>
      <link>https://arxiv.org/abs/1709.00310</link>
      <description>arXiv:1709.00310v4 Announce Type: replace-cross 
Abstract: In this work, we consider the detection of manoeuvring small objects with radars. Such objects induce low signal to noise ratio (SNR) reflections in the received signal. We consider both co-located and separated transmitter/receiver pairs, i.e., mono-static and bi-static configurations, respectively, as well as multi-static settings involving both types. We propose coherent track before detect: A detection approach which is capable of coherently integrating these reflections within a coherent processing interval (CPI) in all these configurations and continuing integration for an arbitrarily long time across consecutive CPIs. {We estimate the complex value of the reflection coefficients for integration while simultaneously estimating the object trajectory. Compounded with these computations is the estimation of the unknown time reference shift of the separated transmitters necessary for coherent processing.} Detection is made by using the resulting integration value in a Neyman-Pearson test against a constant false alarm rate threshold. We demonstrate the efficacy of our approach in a simulation example with a very low SNR object which cannot be detected with conventional techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:1709.00310v4</guid>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kimin Kim, Murat Uney, Bernard Mulgrew</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Context: A Comprehensive Bibliography of GWR and MGWR</title>
      <link>https://arxiv.org/abs/2404.16209</link>
      <description>arXiv:2404.16209v4 Announce Type: replace-cross 
Abstract: Local spatial models such as Geographically Weighted Regression (GWR) and Multiscale Geographically Weighted Regression (MGWR) serve as instrumental tools to capture intrinsic contextual effects through the estimates of the local intercepts and behavioral contextual effects through estimates of the local slope parameters. GWR and MGWR provide simple implementation yet powerful frameworks that could be extended to various disciplines that handle spatial data. This bibliography aims to serve as a comprehensive compilation of peer-reviewed papers that have utilized GWR or MGWR as a primary analytical method to conduct spatial analyses and acts as a useful guide to anyone searching the literature for previous examples of local statistical modeling in a wide variety of application fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16209v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Stewart Fotheringham, Chen-Lun Kao, Hanchen Yu, Sarah Bardin, Taylor Oshan, Ziqi Li, Mehak Sachdeva, Wei Luo</dc:creator>
    </item>
    <item>
      <title>Multicalibration for Modeling Censored Survival Data with Universal Adaptability</title>
      <link>https://arxiv.org/abs/2405.15948</link>
      <description>arXiv:2405.15948v4 Announce Type: replace-cross 
Abstract: Traditional statistical and machine learning methods typically assume that the training and test data follow the same distribution. However, this assumption is frequently violated in real-world applications, where the training data in the source domain may under-represent specific subpopulations in the test data of the target domain. This paper addresses target-independent learning under covariate shift, focusing on multicalibration for survival probability and restricted mean survival time. A black-box post-processing boosting algorithm specifically designed for censored survival data is introduced. By leveraging pseudo-observations, our method produces a multicalibrated predictor that is competitive with inverse propensity score weighting in predicting the survival outcome in an unlabeled target domain, ensuring not only overall accuracy but also fairness across diverse subpopulations. Our theoretical analysis of pseudo-observations builds upon the functional delta method and the $p$-variational norm. The algorithm's sample complexity, convergence properties, and multicalibration guarantees for post-processed predictors are provided. Our results establish a fundamental connection between multicalibration and universal adaptability, demonstrating that our calibrated function is comparable to, or outperforms, the inverse propensity score weighting estimator. Extensive numerical simulations and a real-world case study on cardiovascular disease risk prediction using two large prospective cohort studies validate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15948v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanxuan Ye, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v5 Announce Type: replace-cross 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Competitive balance in the UEFA Champions League group stage: novel measures show no evidence of decline</title>
      <link>https://arxiv.org/abs/2406.19222</link>
      <description>arXiv:2406.19222v5 Announce Type: replace-cross 
Abstract: Competitive balance, which refers to the level of control teams have over a sports competition, is a crucial indicator for tournament organisers. According to previous studies, competitive balance has significantly declined in the UEFA Champions League group stage over the recent decades. Our paper introduces alternative indices to investigate this issue. Two ex ante measures are based on Elo ratings, and four dynamic concentration indicators compare the final group ranking to reasonable benchmarks. Using these indices, we find no evidence of any long-run trend in the competitive balance of the UEFA Champions League group stage between the 2003/04 and 2023/24 seasons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19222v5</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
    <item>
      <title>Surrogate modeling with functional nonlinear autoregressive models (F-NARX)</title>
      <link>https://arxiv.org/abs/2410.07293</link>
      <description>arXiv:2410.07293v2 Announce Type: replace-cross 
Abstract: We propose a novel functional approach to surrogate modeling of dynamical systems with exogenous inputs. This approach, named Functional Nonlinear AutoRegressive with eXogenous inputs (F-NARX), approximates the system response based on temporal features of the exogenous inputs and the system response. This marks a major step away from the discrete-time-centric approach of classical NARX models, which determines the relationship between selected time steps of the input/output time series. By modeling the system in a time-feature space, F-NARX takes advantage of the temporal smoothness of the process being modeled, providing more stable predictions and reducing the dependence of model performance on the discretization of the time axis. In this work, we introduce an F-NARX implementation based on principal component analysis and polynomial regression. To further improve prediction accuracy, we also introduce a modified hybrid least angle regression approach to identify a sparse model structure and minimize the expected forecast error, rather than the one-step-ahead prediction error. We investigate the behavior and capabilities of our F-NARX implementation on two case studies: an eight-story building under wind loading and a three-story steel frame under seismic loading. Our results demonstrate that F-NARX has several favorable properties that make it well-suited to surrogate modeling applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07293v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ress.2025.111276</arxiv:DOI>
      <arxiv:journal_reference>Reliability Engineering &amp; System Safety, Volume 264, Part A, December 2025, 111276</arxiv:journal_reference>
      <dc:creator>Styfen Sch\"ar, Stefano Marelli, Bruno Sudret</dc:creator>
    </item>
    <item>
      <title>Sensor-fusion based Prognostics for Deep-space Habitats Exhibiting Multiple Unlabeled Failure Modes</title>
      <link>https://arxiv.org/abs/2411.12159</link>
      <description>arXiv:2411.12159v3 Announce Type: replace-cross 
Abstract: Deep-space habitats are complex systems that must operate autonomously over extended durations without ground-based maintenance. These systems are vulnerable to multiple, often unknown, failure modes that affect different subsystems and sensors in mode-specific ways. Developing accurate remaining useful life (RUL) prognostics is challenging, especially when failure labels are unavailable and sensor relevance varies by failure mode. In this paper, we propose an unsupervised prognostics framework that jointly identifies latent failure modes and selects informative sensors using only unlabeled training data. The methodology consists of two phases. In the offline phase, we model system failure times using a mixture of Gaussian regressions and apply a novel Expectation-Maximization algorithm to cluster degradation trajectories and select mode-specific sensors. In the online phase, we extract low-dimensional features from the selected sensors to diagnose the active failure mode and predict RUL using a weighted regression model. We demonstrate the effectiveness of our approach on a simulated dataset that reflects deep-space telemetry characteristics and on a real-world engine degradation dataset, showing improved accuracy and interpretability over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12159v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benjamin Peters, Ayush Mohanty, Xiaolei Fang, Stephen K. Robinson, Nagi Gebraeel</dc:creator>
    </item>
    <item>
      <title>Assessing treatment efficacy for interval-censored endpoints using multistate semi-Markov models fit to multiple data streams</title>
      <link>https://arxiv.org/abs/2501.14097</link>
      <description>arXiv:2501.14097v2 Announce Type: replace-cross 
Abstract: We introduce a computationally efficient and general approach for utilizing multiple, possibly interval-censored, data streams to study complex biomedical endpoints using multistate semi-Markov models. Our motivating application is the REGEN-2069 trial, which investigated the protective efficacy (PE) of the monoclonal antibody combination REGEN-COV against SARS-CoV-2 when administered prophylactically to individuals in households at high risk of secondary transmission. Using data on symptom onset, episodic RT-qPCR sampling, and serological testing, we estimate the PE of REGEN-COV for asymptomatic infection, its effect on seroconversion following infection, and the duration of viral shedding. We find that REGEN-COV reduced the risk of asymptomatic infection and the duration of viral shedding, and led to lower rates of seroconversion among asymptomatically infected participants. Our algorithm for fitting semi-Markov models to interval-censored data employs a Monte Carlo expectation maximization (MCEM) algorithm combined with importance sampling to efficiently address the intractability of the marginal likelihood when data are intermittently observed. Our algorithm provide substantial computational improvements over existing methods and allows us to fit semi-parametric models despite complex coarsening of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14097v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, C. Jason Liang, Allyson Mateja, Dean A. Follmann, Meagan P. O'Brien, Chenguang Wang, Jonathan Fintzi</dc:creator>
    </item>
    <item>
      <title>A Multi-Tiered Bayesian Network Coastal Compound Flood Analysis Framework</title>
      <link>https://arxiv.org/abs/2505.15520</link>
      <description>arXiv:2505.15520v2 Announce Type: replace-cross 
Abstract: Coastal compound floods (CCFs) are triggered by the interaction of multiple mechanisms, such as storm surges, storm rainfall, tides, and river flow. These events can bring significant damage to communities, and there is an increasing demand for accurate and efficient probabilistic analyses of CCFs to support risk assessments and decision-making. In this study, a multi-tiered Bayesian network (BN) CCF analysis framework is established. In this framework, conceptual designs of multiple tiers of BN models with varying complexities are developed for application with varying levels of data availability and resources. A case study is conducted in New Orleans, LA, with three tiers of BN models constructed to demonstrate this framework. In the Tier-1 BN model, storm surges and river flow are incorporated based on hydrodynamic simulations. A seasonality node is used to capture the dependence between concurrent river flow and tropical cyclone (TC) parameters. In the Tier-2 BN model, joint distribution models of TC parameters are built for separate TC intensity categories. TC-induced rainfall is modeled as input to hydraulic simulations. In the Tier-3 BN model, potential variations of meteorological conditions are incorporated by quantifying their effects on TC activity and coastal water level. Flood antecedent conditions are also incorporated to more completely represent the conditions contributing to flood severity. In this case study, a series of joint distribution, numerical, machine learning, and experimental models are used to compute conditional probability tables needed for the BNs. A series of probabilistic analyses is performed based on these BN models, including CCF hazard curve construction and CCF deaggregation. The results of the analysis demonstrate the promise of this framework in performing CCF hazard analysis under varying levels of resource availability and project needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15520v2</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Liu, Meredith L. Carr, Norberto C. Nadal-Caraballo, Luke A. Aucoin, Madison C. Yawn, Michelle T. Bensi</dc:creator>
    </item>
    <item>
      <title>Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction</title>
      <link>https://arxiv.org/abs/2506.05391</link>
      <description>arXiv:2506.05391v2 Announce Type: replace-cross 
Abstract: Autoregressive models are often employed to learn distributions of image data by decomposing the $D$-dimensional density function into a product of one-dimensional conditional distributions. Each conditional depends on preceding variables (pixels, in the case of image data), making the order in which variables are processed fundamental to the model performance. In this paper, we study the problem of observing a small subset of image pixels (referred to as a pixel patch) to predict the unobserved parts of the image. As our prediction mechanism, we propose a generalized version of the convolutional neural autoregressive distribution estimation (ConvNADE) model adapted for real-valued and color images. Moreover, we investigate the quality of image reconstruction when observing both random pixel patches and low-discrepancy pixel patches inspired by quasi-Monte Carlo theory. Experiments on benchmark datasets demonstrate that, where design permits, pixels sampled or stored to preserve uniform coverage improves reconstruction fidelity and test performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05391v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ambrose Emmett-Iwaniw, Nathan Kirk</dc:creator>
    </item>
  </channel>
</rss>
