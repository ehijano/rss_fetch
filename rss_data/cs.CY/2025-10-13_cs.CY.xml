<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Oct 2025 03:09:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Assurance of Frontier AI Built for National Security</title>
      <link>https://arxiv.org/abs/2510.08792</link>
      <description>arXiv:2510.08792v1 Announce Type: new 
Abstract: This memorandum presents four recommendations aimed at strengthening the principles of AI model reliability and AI model governability, as DoW, ODNI, NIST, and CAISI refine AI assurance frameworks under the AI Action Plan. Our focus concerns the open scientific problem of misalignment and its implications on AI model behavior. Specifically, misalignment and scheming capabilities can be a red flag indicating AI model insufficient reliability and governability. To address the national security threats arising from misalignment, we recommend that DoW and the IC strategically leverage existing testing and evaluation pipelines and their OT authority to future proof the principles of AI model reliability and AI model governability through a suite of scheming and control evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08792v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Pistillo, Charlotte Stix</dc:creator>
    </item>
    <item>
      <title>Rethinking How We Discuss the Guidance of Student Researchers in Computing</title>
      <link>https://arxiv.org/abs/2510.08885</link>
      <description>arXiv:2510.08885v1 Announce Type: new 
Abstract: Computing faculty at research universities are often expected to guide the work of undergraduate and graduate student researchers. This guidance is typically called advising or mentoring, but these terms belie the complexity of the relationship, which includes several related but distinct roles. I examine the guidance of student researchers in computing (abbreviated to research guidance or guidance throughout) within a facet framework, creating an inventory of roles that faculty members can hold. By expanding and disambiguating the language of guidance, this approach reveals the full breadth of faculty responsibilities toward student researchers, and it facilitates discussing conflicts between those responsibilities. Additionally, the facet framework permits greater flexibility for students seeking guidance, allowing them a robust support network without implying inadequacy in an individual faculty member's skills. I further argue that an over-reliance on singular terms like advising or mentoring for the guidance of student researchers obscures the full scope of faculty responsibilities and interferes with improvement of those as skills. Finally, I provide suggestions for how the facet framework can be utilized by faculty and institutions, and how parts of it can be discussed with students for their benefit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08885v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shomir Wilson</dc:creator>
    </item>
    <item>
      <title>GBA-UBF : A Large-Scale and Fine-Grained Building Function Classification Dataset in the Greater Bay Area</title>
      <link>https://arxiv.org/abs/2510.08921</link>
      <description>arXiv:2510.08921v1 Announce Type: new 
Abstract: Rapid urbanization in the Guangdong-Hong Kong-Macao Greater Bay Area (GBA) has created urgent demand for high-resolution, building-level functional data to support sustainable spatial planning. Existing land use datasets suffer from coarse granularity and difficulty in capturing intra-block heterogeneity. To this end, we present the Greater Bay Area Urban Building Function Dataset (GBA-UBF), a large-scale, fine-grained dataset that assigns one of five functional categories to nearly four million buildings across six core GBA cities. We proposed a Multi-level Building Function Optimization (ML-BFO) method by integrating Points of Interest (POI) records and building footprints through a three-stage pipeline: (1) candidate label generation using spatial overlay with proximity weighting, (2) iterative refinement based on neighborhood label autocorrelation, and (3) function-related correction informed by High-level POI buffers. To quantitatively validate results, we design the Building Function Matching Index (BFMI), which jointly measures categorical consistency and distributional similarity against POI-derived probability heatmaps. Comparative experiments demonstrate that GBA-UBF achieves significantly higher accuracy, with a BMFI of 0.58. This value markedly exceeds that of the baseline dataset and exhibits superior alignment with urban activity patterns. Field validation further confirms the dataset's semantic reliability and practical interpretability. The GBA-UBF dataset establishes a reproducible framework for building-level functional classification, bridging the gap between coarse land use maps and fine-grained urban analytics. The dataset is accessible at https://github.com/chenchs0629/GBA-UBF, and the data will undergo continuous improvement and updates based on feedback from the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08921v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunsong Chen, Yichen Hou, Huan Chen, Junlin Li, Rong Fu, Qiushen Lai, Yiping Chen, Ting Han</dc:creator>
    </item>
    <item>
      <title>AI and Human Oversight: A Risk-Based Framework for Alignment</title>
      <link>https://arxiv.org/abs/2510.09090</link>
      <description>arXiv:2510.09090v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) technologies continue to advance, protecting human autonomy and promoting ethical decision-making are essential to fostering trust and accountability. Human agency (the capacity of individuals to make informed decisions) should be actively preserved and reinforced by AI systems. This paper examines strategies for designing AI systems that uphold fundamental rights, strengthen human agency, and embed effective human oversight mechanisms. It discusses key oversight models, including Human-in-Command (HIC), Human-in-the-Loop (HITL), and Human-on-the-Loop (HOTL), and proposes a risk-based framework to guide the implementation of these mechanisms. By linking the level of AI model risk to the appropriate form of human oversight, the paper underscores the critical role of human involvement in the responsible deployment of AI, balancing technological innovation with the protection of individual values and rights. In doing so, it aims to ensure that AI technologies are used responsibly, safeguarding individual autonomy while maximizing societal benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09090v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laxmiraju Kandikatla, Branislav Radeljic</dc:creator>
    </item>
    <item>
      <title>Non-traditional data in pandemic preparedness and response: identifying and addressing first and last-mile challenges</title>
      <link>https://arxiv.org/abs/2510.09145</link>
      <description>arXiv:2510.09145v1 Announce Type: new 
Abstract: The pandemic served as an important test case of complementing traditional public health data with non-traditional data (NTD) such as mobility traces, social media activity, and wearables data to inform decision-making. Drawing on an expert workshop and a targeted survey of European modelers, we assess the promise and persistent limitations of such data in pandemic preparedness and response. We distinguish between "first-mile" (accessing and harmonizing data) and "last-mile" challenges (translating insights into actionable interventions). The expert workshop held in 2024 brought together participants from public health, academia, policymakers, and industry to reflect on lessons learned and define strategies for translating NTD insights into policy making. The survey offers evidence of the barriers faced during COVID-19 and highlights key data unavailability and underuse. Our findings reveal ongoing issues with data access, quality, and interoperability, as well as institutional and cognitive barriers to evidence-based decision-making. Around 66% of datasets suffered access problem, with data sharing reluctance for NTD being double that of traditional data (30% vs 15%). Only 10% reported they could use all the data they needed. We propose a set of recommendations: for first-mile challenges, solutions focus on technical and legal frameworks for data access.; for last-mile challenges, we recommend fusion centers, decision accelerator labs, and networks of scientific ambassadors to bridge the gap between analysis and action. Realizing the full value of NTD requires a sustained investment in institutional readiness, cross-sectoral collaboration, and a shift toward a culture of data solidarity. Grounded in the lessons of COVID-19, the article can be used to design a roadmap for using NTD to confront a broader array of public health emergencies, from climate shocks to humanitarian crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09145v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattia Mazzoli, Irma Varela-Lasheras, Sonia Namorado, Constantino Pereira Caetano, Andreia Leite, Lisa Hermans, Niel Hens, Polen T\"urkmen, Kyriaki Kalimeri, Leo Ferres, Ciro Cattuto, Daniela Paolotti, Stefaan Verhulst</dc:creator>
    </item>
    <item>
      <title>Federated Data Analytics for Cancer Immunotherapy: A Privacy-Preserving Collaborative Platform for Patient Management</title>
      <link>https://arxiv.org/abs/2510.09155</link>
      <description>arXiv:2510.09155v1 Announce Type: new 
Abstract: Connected health is a multidisciplinary approach focused on health management, prioritizing pa-tient needs in the creation of tools, services, and treatments. This paradigm ensures proactive and efficient care by facilitating the timely exchange of accurate patient information among all stake-holders in the care continuum. The rise of digital technologies and process innovations promises to enhance connected health by integrating various healthcare data sources. This integration aims to personalize care, predict health outcomes, and streamline patient management, though challeng-es remain, particularly in data architecture, application interoperability, and security. Data analytics can provide critical insights for informed decision-making and health co-creation, but solutions must prioritize end-users, including patients and healthcare professionals. This perspective was explored through an agile System Development Lifecycle in an EU-funded project aimed at developing an integrated AI-generated solution for managing cancer patients undergoing immunotherapy. This paper contributes with a collaborative digital framework integrating stakeholders across the care continuum, leveraging federated big data analytics and artificial intelligence for improved decision-making while ensuring privacy. Analytical capabilities, such as treatment recommendations and adverse event predictions, were validated using real-life data, achieving 70%-90% accuracy in a pilot study with the medical partners, demonstrating the framework's effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09155v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mira Raheem, Michael Papazoglou, Bernd Kr\"amer, Neamat El-Tazi, Amal Elgammal</dc:creator>
    </item>
    <item>
      <title>Student Development Agent: Risk-free Simulation for Evaluating AIED Innovations</title>
      <link>https://arxiv.org/abs/2510.09183</link>
      <description>arXiv:2510.09183v1 Announce Type: new 
Abstract: In the age of AI-powered educational (AIED) innovation, evaluating the developmental consequences of novel designs before they are exposed to students has become both essential and challenging. Since such interventions may carry irreversible effects, it is critical to anticipate not only potential benefits but also possible harms. This study proposes a student development agent framework based on large language models (LLMs), designed to simulate how students with diverse characteristics may evolve under different educational settings without administering them to real students. By validating the approach through a case study on a multi-agent learning environment (MAIC), we demonstrate that the agent's predictions align with real student outcomes in non-cognitive developments. The results suggest that LLM-based simulations hold promise for evaluating AIED innovations efficiently and ethically. Future directions include enhancing profile structures, incorporating fine-tuned or small task-specific models, validating effects of empirical findings, interpreting simulated data and optimizing evaluation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09183v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianxiao Jiang, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>Exploring User Risk Factors and Target Groups for Phishing Victimization in Pakistan</title>
      <link>https://arxiv.org/abs/2510.09249</link>
      <description>arXiv:2510.09249v1 Announce Type: new 
Abstract: Phishing attacks pose a significant cybersecurity threat globally. This study investigates phishing susceptibility within the Pakistani population, examining the influence of demographic factors, technological aptitude and usage, previous phishing victimization, and email characteristics. Data was collected through convenient sampling; a total of 164 people completed the questionnaire. Contrary to some assumptions, the results indicate that men, individuals over 25, employed persons and frequent online shoppers have relatively high phishing susceptibility. The characteristics of email significantly affected phishing victimization, with authority and urgency signaling increasing susceptibility, while risk cues sometimes improved vigilance. In particular, users were more susceptible to emails from communication services such as Gmail and LinkedIn compared to government or social media sources. These findings highlight the need for targeted security awareness interventions tailored to specific demographics and email types. A multifaceted approach combining technology and education is crucial to combat phishing attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09249v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javara A. Bukhsh, Maya Daneva, Marten van Sinderen</dc:creator>
    </item>
    <item>
      <title>Challenges in designing ethical rules for Infrastructures in Internet of Vehicles</title>
      <link>https://arxiv.org/abs/2510.09374</link>
      <description>arXiv:2510.09374v1 Announce Type: new 
Abstract: Vehicular Ad-hoc Networks (VANETs) have seen significant advancements in technology. Innovation in connectivity and communication has brought substantial capabilities to various components of VANETs such as vehicles, infrastructures, passengers, drivers and affiliated environmental sensors. Internet of Things (IoT) has brought the notion of Internet of Vehicles (IoV) to VANETs where each component of VANET is connected directly or indirectly to the Internet. Vehicles and infrastructures are key components of a VANET system that can greatly augment the overall experience of the network by integrating the competencies of Vehicle to Vehicle (V2V), Vehicle to Pedestrian (V2P), Vehicle to Sensor (V2S), Vehicle to Infrastructure (V2I) and Infrastructure to Infrastructure (I2I). Internet connectivity in Vehicles and Infrastructures has immensely expanded the potential of developing applications for VANETs under the broad spectrum of IoV. Advent in the use of technology in VANETs requires considerable efforts in scheming the ethical rules for autonomous systems. Currently, there is a gap in literature that focuses on the challenges involved in designing ethical rules or policies for infrastructures, sometimes referred to as Road Side Units (RSUs) for IoVs. This paper highlights the key challenges entailing the design of ethical rules for RSUs in IoV systems. Furthermore, the article also proposes major ethical principles for RSUs in IoV systems that would set foundation for modeling future IoV architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09374v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razi Iqbal</dc:creator>
    </item>
    <item>
      <title>Demystifying and Navigating AI Ethics in Power Electronics</title>
      <link>https://arxiv.org/abs/2510.09439</link>
      <description>arXiv:2510.09439v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is rapidly transforming power electronics, with AI-related publications in IEEE Power Electronics Society selected journals increasing more than fourfold from 2020 to 2025. However, the ethical dimensions of this transformation have received limited attention. This article underscores the urgent need for an ethical framework to guide responsible AI integration in power electronics, not only to prevent AI-related incidents but also to comply with legal and regulatory responsibilities. In this context, this article identifies four core pillars of AI ethics in power electronics: Security &amp; Safety, Explainability &amp; Transparency, Energy Sustainability, and Evolving Roles of Engineers. Each pillar is supported by practical and actionable insights to ensure that ethical principles are embedded in algorithm design, system deployment, and workforce development. The authors advocate for power electronics engineers to lead the ethical discourse, given their deep technical understanding of both AI systems and power conversion technologies. The paper concludes by calling on the IEEE Power Electronics Society to spearhead the establishment of ethical standards and best practices that ensure AI innovations are not only technically advanced but also trustworthy, safe, and sustainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09439v1</guid>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fanfan Lin, Peter Wilson, Xinze Li, Alan Mantooth</dc:creator>
    </item>
    <item>
      <title>Exploring Teachers' Perceptions of ChatGPT Through Prompt Engineering</title>
      <link>https://arxiv.org/abs/2510.08634</link>
      <description>arXiv:2510.08634v1 Announce Type: cross 
Abstract: Artificial Intelligence and especially Large Language Models (LLM), such as ChatGPT has revolutionized the way educators work. The results we get from LLMs depend on how we ask them to help us. The process and the technique behind an effective input is called prompt engineering. The aim of this study is to investigate whether science educators in secondary education improve their attitude toward ChatGPT as a learning assistant after appropriate training in prompt engineering. The results of the pilot study presented in this paper show an improvement in the previously mentioned teachers perceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08634v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitrios Gousopoulos</dc:creator>
    </item>
    <item>
      <title>A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data</title>
      <link>https://arxiv.org/abs/2510.08663</link>
      <description>arXiv:2510.08663v1 Announce Type: cross 
Abstract: Psychological assessments typically rely on structured rating scales, which cannot incorporate the rich nuance of a respondent's natural language. This study leverages recent LLM advances to harness qualitative data within a novel conceptual framework, combining LLM-scored text and traditional rating-scale items to create an augmented test. We demonstrate this approach using depression as a case study, developing and assessing the framework on a real-world sample of upper secondary students (n=693) and corresponding synthetic dataset (n=3,000). On held-out test sets, augmented tests achieved statistically significant improvements in measurement precision and accuracy. The information gain from the LLM items was equivalent to adding between 6.3 (real data) and 16.0 (synthetic data) items to the original 19-item test. Our approach marks a conceptual shift in automated scoring that bypasses its typical bottlenecks: instead of relying on pre-labelled data or complex expert-created rubrics, we empirically select the most informative LLM scoring instructions based on calculations of item information. This framework provides a scalable approach for leveraging the growing stream of transcribed text to enhance traditional psychometric measures, and we discuss its potential utility in clinical health and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08663v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joe Watson, Ivan O'Conner, Chia-Wen Chen, Luning Sun, Fang Luo, David Stillwell</dc:creator>
    </item>
    <item>
      <title>SoK: Scope and Mission of CS&amp;Law</title>
      <link>https://arxiv.org/abs/2510.08723</link>
      <description>arXiv:2510.08723v1 Announce Type: cross 
Abstract: We systematize the intellectual scope of the ACM Computer Science and Law Symposium (CS&amp;Law). In particular, we address the meaning and importance of the word ''and'' in the name of the symposium. We identify previously published papers (from CS&amp;Law and other forums) that exemplify different aspects of the CS&amp;Law scope and note that the scope is expected to evolve as the symposium and the community grow and change. To round out our systematization of the still nascent research area, we also discuss the mission of CS&amp;Law: What might the symposium seek to accomplish beyond providing a forum for intellectual exchange and community formation?</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08723v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joan Feigenbaum, Daniel J. Weitzner</dc:creator>
    </item>
    <item>
      <title>McMining: Automated Discovery of Misconceptions in Student Code</title>
      <link>https://arxiv.org/abs/2510.08827</link>
      <description>arXiv:2510.08827v1 Announce Type: cross 
Abstract: When learning to code, students often develop misconceptions about various programming language concepts. These can not only lead to bugs or inefficient code, but also slow down the learning of related concepts. In this paper, we introduce McMining, the task of mining programming misconceptions from samples of code from a student. To enable the training and evaluation of McMining systems, we develop an extensible benchmark dataset of misconceptions together with a large set of code samples where these misconceptions are manifested. We then introduce two LLM-based McMiner approaches and through extensive evaluations show that models from the Gemini, Claude, and GPT families are effective at discovering misconceptions in student code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08827v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erfan Al-Hossami, Razvan Bunescu</dc:creator>
    </item>
    <item>
      <title>Green Grid: Smart Tech Meets E-Waste</title>
      <link>https://arxiv.org/abs/2510.08888</link>
      <description>arXiv:2510.08888v1 Announce Type: cross 
Abstract: Electronic waste (e-waste) is a rapidly growing global problem caused by shorter device lifecycles and rising consumption. India ranks third globally in e-waste generation, producing over 1.7 million tonnes in 2023-24, of which less than half is formally processed. To address this, we propose Green Grid, an integrated AI-powered e-waste management platform combining IoT-enabled smart collection, AI-based device classification, blockchain-based traceability, and gamified citizen engagement. The system features smart recycling bins with sensors for real-time monitoring, deep learning models for device identification and sorting, a blockchain ledger for tamper-proof tracking, and a reward-based mobile or web app to encourage user participation. Additionally, Green Grid offers analytics dashboards and an eco-marketplace to support policymakers and recyclers. By bridging technology, sustainability, and community participation, the platform enhances transparency, increases formal recycling rates, and advances India's transition toward a circular economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08888v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yashodip Dharmendra Jagtap, Aaditya Ganesh Bagul</dc:creator>
    </item>
    <item>
      <title>Creation of the Chinese Adaptive Policy Communication Corpus</title>
      <link>https://arxiv.org/abs/2510.08986</link>
      <description>arXiv:2510.08986v1 Announce Type: cross 
Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central Government) Corpus, the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories, building on Ang's theory of adaptive policy communication. Spanning 1949-2023, this corpus includes national laws, administrative regulations, and ministerial rules issued by China's top authorities. Each document is segmented into paragraphs, producing a total of 3.3 million units. Alongside the corpus, we release comprehensive metadata, a two-round labeling framework, and a gold-standard annotation set developed by expert and trained coders. Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive labels, indicating high reliability for supervised modeling. We provide baseline classification results with several large language models (LLMs), together with our annotation codebook, and describe patterns from the dataset. This release aims to support downstream tasks and multilingual NLP research in policy communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08986v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bolun Sun, Charles Chang, Yuen Yuen Ang, Pingxu Hao, Ruotong Mu, Yuchen Xu, Zhengxin Zhang</dc:creator>
    </item>
    <item>
      <title>Auto-scaling Continuous Memory for GUI Agent</title>
      <link>https://arxiv.org/abs/2510.09038</link>
      <description>arXiv:2510.09038v1 Announce Type: cross 
Abstract: We study how to endow GUI agents with scalable memory that help generalize across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress past trajectories into text tokens, which balloons context length and misses decisive visual cues (e.g., exact widget size and position). We propose a continuous memory that encodes each GUI trajectory into a fixed-length sequence of continuous embeddings using the VLM itself as an encoder; these embeddings are plugged directly into the backbone's input layer, sharply reducing context cost while preserving fine-grained visual information. As memory size and retrieval depth increase, performance improves monotonically, unlike text memories that degrade with long prompts. To grow memory at low cost, we introduce an auto-scaling data flywheel that (i) discovers new environments via search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out trajectories with the agent, and (iv) verifies success with the same VLM. Using this pipeline, we collect 100k+ trajectories for about \$4000 and fine-tune only the memory encoder (LoRA on a Q-Former, 1.2\% parameters) with 1,500 samples. On real-world GUI benchmarks, our memory-augmented agent consistently improves success rates under long horizons and distribution shifts. Notably, Qwen-2.5-VL-7B + continuous memory achieves performance comparable to state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09038v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenyi Wu, Kun Zhou, Ruoxin Yuan, Vivian Yu, Stephen Wang, Zhiting Hu, Biwei Huang</dc:creator>
    </item>
    <item>
      <title>Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics</title>
      <link>https://arxiv.org/abs/2510.09082</link>
      <description>arXiv:2510.09082v2 Announce Type: cross 
Abstract: Learning complex network dynamics is fundamental to understanding, modelling and controlling real-world complex systems. There are two main problems in the task of predicting the dynamic evolution of complex networks: on the one hand, existing methods usually use simple graphs to describe the relationships in complex networks; however, this approach can only capture pairwise relationships, while there may be rich non-pairwise structured relationships in the network. First-order GNNs have difficulty in capturing dynamic non-pairwise relationships. On the other hand, theoretical prediction models lack accuracy and data-driven prediction models lack interpretability. To address the above problems, this paper proposes a higher-order network dynamics identification method for long-term dynamic prediction of complex networks. Firstly, to address the problem that traditional graph machine learning can only deal with pairwise relations, dynamic hypergraph learning is introduced to capture the higher-order non-pairwise relations among complex networks and improve the accuracy of complex network modelling. Then, a dual-driven dynamic prediction module for physical data is proposed. The Koopman operator theory is introduced to transform the nonlinear dynamical differential equations for the dynamic evolution of complex networks into linear systems for solving. Meanwhile, the physical information neural differential equation method is utilised to ensure that the dynamic evolution conforms to the physical laws. The dual-drive dynamic prediction module ensures both accuracy and interpretability of the prediction. Validated on public datasets and self-built industrial chain network datasets, the experimental results show that the method in this paper has good prediction accuracy and long-term prediction performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09082v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bicheng Wang, Junping Wang, Yibo Xue</dc:creator>
    </item>
    <item>
      <title>Dr. Bias: Social Disparities in AI-Powered Medical Guidance</title>
      <link>https://arxiv.org/abs/2510.09162</link>
      <description>arXiv:2510.09162v1 Announce Type: cross 
Abstract: With the rapid progress of Large Language Models (LLMs), the general public now has easy and affordable access to applications capable of answering most health-related questions in a personalized manner. These LLMs are increasingly proving to be competitive, and now even surpass professionals in some medical capabilities. They hold particular promise in low-resource settings, considering they provide the possibility of widely accessible, quasi-free healthcare support. However, evaluations that fuel these motivations highly lack insights into the social nature of healthcare, oblivious to health disparities between social groups and to how bias may translate into LLM-generated medical advice and impact users. We provide an exploratory analysis of LLM answers to a series of medical questions spanning key clinical domains, where we simulate these questions being asked by several patient profiles that vary in sex, age range, and ethnicity. By comparing natural language features of the generated responses, we show that, when LLMs are used for medical advice generation, they generate responses that systematically differ between social groups. In particular, Indigenous and intersex patients receive advice that is less readable and more complex. We observe these trends amplify when intersectional groups are considered. Considering the increasing trust individuals place in these models, we argue for higher AI literacy and for the urgent need for investigation and mitigation by AI developers to ensure these systemic differences are diminished and do not translate to unjust patient support. Our code is publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09162v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Kondrup, Anne Imouza</dc:creator>
    </item>
    <item>
      <title>Locally Optimal Private Sampling: Beyond the Global Minimax</title>
      <link>https://arxiv.org/abs/2510.09485</link>
      <description>arXiv:2510.09485v1 Announce Type: cross 
Abstract: We study the problem of sampling from a distribution under local differential privacy (LDP). Given a private distribution $P \in \mathcal{P}$, the goal is to generate a single sample from a distribution that remains close to $P$ in $f$-divergence while satisfying the constraints of LDP. This task captures the fundamental challenge of producing realistic-looking data under strong privacy guarantees. While prior work by Park et al. (NeurIPS'24) focuses on global minimax-optimality across a class of distributions, we take a local perspective. Specifically, we examine the minimax risk in a neighborhood around a fixed distribution $P_0$, and characterize its exact value, which depends on both $P_0$ and the privacy level. Our main result shows that the local minimax risk is determined by the global minimax risk when the distribution class $\mathcal{P}$ is restricted to a neighborhood around $P_0$. To establish this, we (1) extend previous work from pure LDP to the more general functional LDP framework, and (2) prove that the globally optimal functional LDP sampler yields the optimal local sampler when constrained to distributions near $P_0$. Building on this, we also derive a simple closed-form expression for the locally minimax-optimal samplers which does not depend on the choice of $f$-divergence. We further argue that this local framework naturally models private sampling with public data, where the public data distribution is represented by $P_0$. In this setting, we empirically compare our locally optimal sampler to existing global methods, and demonstrate that it consistently outperforms global minimax samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09485v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hrad Ghoukasian, Bonwoo Lee, Shahab Asoodeh</dc:creator>
    </item>
    <item>
      <title>From Birdwatch to Community Notes, from Twitter to X: four years of community-based content moderation</title>
      <link>https://arxiv.org/abs/2510.09585</link>
      <description>arXiv:2510.09585v1 Announce Type: cross 
Abstract: Community Notes (formerly known as Birdwatch) is the first large-scale crowdsourced content moderation initiative that was launched by X (formerly known as Twitter) in January 2021. As the Community Notes model gains momentum across other social media platforms, there is a growing need to assess its underlying dynamics and effectiveness. This Resource paper provides (a) a systematic review of the literature on Community Notes, and (b) a major curated dataset and accompanying source code to support future research on Community Notes. We parsed Notes and Ratings data from the first four years of the program and conducted language detection across all Notes. Focusing on English-language Notes, we extracted embedded URLs and identified discussion topics in each Note. Additionally, we constructed monthly interaction networks among the Contributors. Together with the literature review, these resources offer a robust foundation for advancing research on the Community Notes system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09585v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeedeh Mohammadi, Narges Chinichian, Hannah Doyal, Kristina Skutilova, Hao Cui, Michele d'Errico, Siobhan Grayson, Taha Yasseri</dc:creator>
    </item>
    <item>
      <title>Student-AI Interaction in an LLM-Empowered Learning Environment: A Cluster Analysis of Engagement Profiles</title>
      <link>https://arxiv.org/abs/2503.01694</link>
      <description>arXiv:2503.01694v3 Announce Type: replace 
Abstract: Integrating Large Language Models (LLMs) into educational practice enables personalized learning by accommodating diverse learner behaviors. This study explored diverse learner profiles within a multi-agent, LLM-empowered learning environment. Data was collected from 312 undergraduate students at a university in China as they participated in a six-module course. Based on hierarchical cluster analyses of system profiles and student-AI interactive dialogues, we found that students exhibit varied behavioral, cognitive, and emotional engagement tendencies. This analysis allowed us to identify two types of dropouts (early dropouts and stagnating interactors) and three completer profiles (active questioners, responsive navigators, and lurkers). The results showed that high levels of interaction do not always equate to productive learning and vice versa. Prior knowledge significantly influenced interaction patterns and short-term learning benefits. Further analysis of the human-AI dialogues revealed that some students actively engaged in knowledge construction, while others displayed a high frequency of regulatory behaviors. Notably, both groups of students achieved comparable learning gains, demonstrating the effectiveness of the multi-agent learning environment in supporting personalized learning. These results underscore the complex and multifaceted nature of engagement in human-AI collaborative learning and provide practical implications for the design of adaptive educational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01694v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanxin Hao, Jianxiao Jiang, Jifan Yu, Zhiyuan Liu, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement</title>
      <link>https://arxiv.org/abs/2509.19088</link>
      <description>arXiv:2509.19088v2 Announce Type: replace 
Abstract: Digital representations of individuals ("digital twins") promise to transform social science and decision-making. Yet it remains unclear whether such twins truly mirror the people they emulate. We conducted 19 preregistered studies with a representative U.S. panel and their digital twins, each constructed from rich individual-level data, enabling direct comparisons between human and twin behavior across a wide range of domains and stimuli (including never-seen-before ones). Twins reproduced individual responses with 75% accuracy and seemingly low correlation with human answers (approximately 0.2). However, this apparently high accuracy was no higher than that achieved by generic personas based on demographics only. In contrast, correlation improved when twins incorporated detailed personal information, even outperforming traditional machine learning benchmarks that require additional data. Twins exhibited systematic strengths and weaknesses - performing better in social and personality domains, but worse in political ones - and were more accurate for participants with higher education, higher income, and moderate political views and religious attendance. Together, these findings delineate both the promise and the current limits of digital twins: they capture some relative differences among individuals but not yet the unique judgments of specific people. All data and code are publicly available to support the further development and evaluation of digital twin pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19088v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Akshit Kumar, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>Teaching Data Science Students to Sketch Privacy Designs through Heuristics (Extended Technical Report)</title>
      <link>https://arxiv.org/abs/2504.04734</link>
      <description>arXiv:2504.04734v2 Announce Type: replace-cross 
Abstract: Recent studies reveal that experienced data practitioners often draw sketches to facilitate communication around privacy design concepts. However, there is limited understanding of how we can help novice students develop such communication skills. This paper studies methods for lowering novice data science students' barriers to creating high-quality privacy sketches. We first conducted a need-finding study (N=12) to identify barriers students face when sketching privacy designs. We then used a human-centered design approach to guide the method development, culminating in three simple, text-based heuristics. Our user studies with 24 data science students revealed that simply presenting three heuristics to the participants at the beginning of the study can enhance the coverage of privacy-related design decisions in sketches, reduce the mental effort required for creating sketches, and improve the readability of the final sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04734v2</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SP61157.2025.00147</arxiv:DOI>
      <dc:creator>Jinhe Wen, Yingxi Zhao, Wenqian Xu, Yaxing Yao, Haojian Jin</dc:creator>
    </item>
    <item>
      <title>Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation</title>
      <link>https://arxiv.org/abs/2505.03105</link>
      <description>arXiv:2505.03105v2 Announce Type: replace-cross 
Abstract: Human-AI scientific collaboration has evolved from tool-user relationships into co-evolutionary partnerships. When AlphaFold improved protein structure prediction, researchers engaged with an epistemic partner that transformed their approach to structure-function problems. Yet existing frameworks position AI as either sophisticated tool or potential risk, overlooking how scientific understanding emerges through recursive interaction. We introduce Cognitio Emergens (CE), a framework that captures the co-evolutionary nature of human-AI epistemic partnerships.
  Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE integrates three components: Agency Configurations modeling how authority distributes through Directed, Contributory, and Partnership modes, with partnerships oscillating dynamically rather than following linear progression; Epistemic Dimensions capturing six capabilities along Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide strategic development; and Partnership Dynamics identifying evolutionary forces including epistemic alienation, where researchers lose interpretive control over knowledge they formally endorse.
  The framework equips researchers to diagnose dimensional imbalances, institutional leaders to design governance structures supporting multiple agency configurations, and policymakers to develop evaluations beyond simple performance metrics. By reconceptualizing human-AI collaboration as fundamentally co-evolutionary, CE provides conceptual tools for cultivating partnerships that preserve epistemic integrity while enabling transformative breakthroughs neither humans nor AI could achieve independently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03105v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xule Lin</dc:creator>
    </item>
    <item>
      <title>Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective</title>
      <link>https://arxiv.org/abs/2506.19028</link>
      <description>arXiv:2506.19028v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19028v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy</dc:creator>
    </item>
    <item>
      <title>Individual utilities of life satisfaction reveal inequality aversion unrelated to political alignment</title>
      <link>https://arxiv.org/abs/2509.07793</link>
      <description>arXiv:2509.07793v3 Announce Type: replace-cross 
Abstract: How should well-being be prioritised in society, and what trade-offs are people willing to make between fairness and personal well-being? We investigate these questions using a stated preference experiment with a nationally representative UK sample (n = 300), in which participants evaluated life satisfaction outcomes for both themselves and others under conditions of uncertainty. Individual-level utility functions were estimated using an Expected Utility Maximisation (EUM) framework and tested for sensitivity to the overweighting of small probabilities, as characterised by Cumulative Prospect Theory (CPT). A majority of participants displayed concave (risk-averse) utility curves and showed stronger aversion to inequality in societal life satisfaction outcomes than to personal risk. These preferences were unrelated to political alignment, suggesting a shared normative stance on fairness in well-being that cuts across ideological boundaries. The results challenge use of average life satisfaction as a policy metric, and support the development of nonlinear utility-based alternatives that more accurately reflect collective human values. Implications for public policy, well-being measurement, and the design of value-aligned AI systems are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07793v3</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Crispin Cooper, Ana Fredrich, Tommaso Reggiani, Wouter Poortinga</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</title>
      <link>https://arxiv.org/abs/2510.01644</link>
      <description>arXiv:2510.01644v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer's policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01644v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Hawkins, Aditya Pramar, Rodney Beard, Rohitash Chandra</dc:creator>
    </item>
  </channel>
</rss>
