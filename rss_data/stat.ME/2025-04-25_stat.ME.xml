<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Apr 2025 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Sensitivity Analysis Framework for Quantifying Confidence in Decisions in the Presence of Data Uncertainty</title>
      <link>https://arxiv.org/abs/2504.17043</link>
      <description>arXiv:2504.17043v1 Announce Type: new 
Abstract: Nearly all statistical analyses that inform policy-making are based on imperfect data. As examples, the data may suffer from measurement errors, missing values, sample selection bias, or record linkage errors. Analysts have to decide how to handle such data imperfections, e.g., analyze only the complete cases or impute values for the missing items via some posited model. Their choices can influence estimates and hence, ultimately, policy decisions. Thus, it is prudent for analysts to evaluate the sensitivity of estimates and policy decisions to the assumptions underlying their choices. To facilitate this goal, we propose that analysts define metrics and visualizations that target the sensitivity of the ultimate decision to the assumptions underlying their approach to handling the data imperfections. Using these visualizations, the analyst can assess their confidence in the policy decision under their chosen analysis. We illustrate metrics and corresponding visualizations with two examples, namely considering possible measurement error in the inputs of predictive models of presidential vote share and imputing missing values when evaluating the percentage of children exposed to high levels of lead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17043v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adway S. Wadekar, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Conditional-Marginal Nonparametric Estimation for Stage Waiting Times from Multi-Stage Models under Dependent Right Censoring</title>
      <link>https://arxiv.org/abs/2504.17089</link>
      <description>arXiv:2504.17089v1 Announce Type: new 
Abstract: We investigate two population-level quantities (corresponding to complete data) related to uncensored stage waiting times in a progressive multi-stage model, conditional on a prior stage visit. We show how to estimate these quantities consistently using right-censored data. The first quantity is the stage waiting time distribution (survival function), representing the proportion of individuals who remain in stage j within time t after entering stage j. The second quantity is the cumulative incidence function, representing the proportion of individuals who transition from stage j to stage j' within time t after entering stage j. To estimate these quantities, we present two nonparametric approaches. The first uses an inverse probability of censoring weighting (IPCW) method, which reweights the counting processes and the number of individuals at risk (the at-risk set) to address dependent right censoring. The second method utilizes the notion of fractional observations (FRE) that modifies the at-risk set by incorporating probabilities of individuals (who might have been censored in a prior stage) eventually entering the stage of interest in the uncensored or full data experiment. Neither approach is limited to the assumption of independent censoring or Markovian multi-stage frameworks. Simulation studies demonstrate satisfactory performance for both sets of estimators, though the IPCW estimator generally outperforms the FRE estimator in the setups considered in our simulations. These estimations are further illustrated through applications to two real-world datasets: one from patients undergoing bone marrow transplants and the other from patients diagnosed with breast cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17089v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxi Zhang, Peihua Qiu, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>Target trial emulation without matching: a more efficient approach for evaluating vaccine effectiveness using observational data</title>
      <link>https://arxiv.org/abs/2504.17104</link>
      <description>arXiv:2504.17104v1 Announce Type: new 
Abstract: Real-world vaccine effectiveness has increasingly been studied using matching-based approaches, particularly in observational cohort studies following the target trial emulation framework. Although matching is appealing in its simplicity, it suffers important limitations in terms of clarity of the target estimand and the efficiency or precision with which is it estimated. Scientifically justified causal estimands of vaccine effectiveness may be difficult to define owing to the fact that vaccine uptake varies over calendar time when infection dynamics may also be rapidly changing. We propose a causal estimand of vaccine effectiveness that summarizes vaccine effectiveness over calendar time, similar to how vaccine efficacy is summarized in a randomized controlled trial. We describe the identification of our estimand, including its underlying assumptions, and propose simple-to-implement estimators based on two hazard regression models. We apply our proposed estimator in simulations and in a study to assess the effectiveness of the Pfizer-BioNTech COVID-19 vaccine to prevent infections with SARS-CoV2 in children 5-11 years old. In both settings, we find that our proposed estimator yields similar scientific inferences while providing significant efficiency gains over commonly used matching-based estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17104v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily Wu, Elizabeth Rogawski McQuade, Mats Stensrud, Razieh Nabi, David Benkeser</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for the Average Treatment Effect in a Score-Explained Heterogeneous Treatment Effect Model</title>
      <link>https://arxiv.org/abs/2504.17126</link>
      <description>arXiv:2504.17126v1 Announce Type: new 
Abstract: In many practical situations, randomly assigning treatments to subjects is uncommon due to feasibility constraints. For example, economic aid programs and merit-based scholarships are often restricted to those meeting specific income or exam score thresholds. In these scenarios, traditional approaches to estimating treatment effects typically focus solely on observations near the cutoff point, thereby excluding a significant portion of the sample and potentially leading to information loss. Moreover, these methods generally achieve a non-parametric convergence rate. While some approaches, e.g., Mukherjee et al. (2021), attempt to tackle these issues, they commonly assume that treatment effects are constant across individuals, an assumption that is often unrealistic in practice. In this study, we propose a differencing and matching-based estimator of the average treatment effect on the treated (ATT) in the presence of heterogeneous treatment effects, utilizing all available observations. We establish the asymptotic normality of our estimator and illustrate its effectiveness through various synthetic and real data analyses. Additionally, we demonstrate that our method yields non-parametric estimates of the conditional average treatment effect (CATE) and individual treatment effect (ITE) as a byproduct.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17126v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Christian Wibisono, Debarghya Mukherjee, Moulinath Banerjee, Ya'acov Ritov</dc:creator>
    </item>
    <item>
      <title>A Delayed Acceptance Auxiliary Variable MCMC for Spatial Models with Intractable Likelihood Function</title>
      <link>https://arxiv.org/abs/2504.17147</link>
      <description>arXiv:2504.17147v1 Announce Type: new 
Abstract: A large class of spatial models contains intractable normalizing functions, such as spatial lattice models, interaction spatial point processes, and social network models. Bayesian inference for such models is challenging since the resulting posterior distribution is doubly intractable. Although auxiliary variable MCMC (AVM) algorithms are known to be the most practical, they are computationally expensive due to the repeated auxiliary variable simulations. To address this, we propose delayed-acceptance AVM (DA-AVM) methods, which can reduce the number of auxiliary variable simulations. The first stage of the kernel uses a cheap surrogate to decide whether to accept or reject the proposed parameter value. The second stage guarantees detailed balance with respect to the posterior. The auxiliary variable simulation is performed only on the parameters accepted in the first stage. We construct various surrogates specifically tailored for doubly intractable problems, including subsampling strategy, Gaussian process emulation, and frequentist estimator-based approximation. We validate our method through simulated and real data applications, demonstrating its practicality for complex spatial models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17147v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jong Hyeon Lee, Jongmin Kim, Heesang Lee, Jaewoo Park</dc:creator>
    </item>
    <item>
      <title>A general approach to modeling environmental mixtures with multivariate outcomes</title>
      <link>https://arxiv.org/abs/2504.17195</link>
      <description>arXiv:2504.17195v1 Announce Type: new 
Abstract: An important goal of environmental health research is to assess the health risks posed by mixtures of multiple environmental exposures. In these mixtures analyses, flexible models like Bayesian kernel machine regression and multiple index models are appealing because they allow for arbitrary non-linear exposure-outcome relationships. However, this flexibility comes at the cost of low power, particularly when exposures are highly correlated and the health effects are weak, as is typical in environmental health studies. We propose an adaptive index modelling strategy that borrows strength across exposures and outcomes by exploiting similar mixture component weights and exposure-response relationships. In the special case of distributed lag models, in which exposures are measured repeatedly over time, we jointly encourage co-clustering of lag profiles and exposure-response curves to more efficiently identify critical windows of vulnerability and characterize important exposure effects. We then extend the proposed approach to the multivariate index model setting where the true index structure -- the number of indices and their composition -- is unknown, and introduce variable importance measures to quantify component contributions to mixture effects. Using time series data from the National Morbidity, Mortality and Air Pollution Study, we demonstrate the proposed methods by jointly modelling three mortality outcomes and two cumulative air pollution measurements with a maximum lag of 14 days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17195v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen McGee, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>A New Look at the Odds Ratio in Logistic Regression</title>
      <link>https://arxiv.org/abs/2504.17205</link>
      <description>arXiv:2504.17205v1 Announce Type: new 
Abstract: The standard odds ratio of logistic regression is foundational but limited to individual explanatory variables. This work derives a multivariable odds ratio that applies to all the explanatory variables in all their combinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17205v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e Ra\'ul Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Independence via Density Ratio Regression</title>
      <link>https://arxiv.org/abs/2504.17322</link>
      <description>arXiv:2504.17322v1 Announce Type: new 
Abstract: This paper develops a conditional independence (CI) test from a conditional density ratio (CDR) for weakly dependent data. The main contribution is presenting a closed-form expression for the estimated conditional density ratio function with good finite-sample performance. The key idea is exploiting the linear sieve combined with the quadratic norm. Matsushita et al. (2022) exploited the linear sieve to estimate the unconditional density ratio. We must exploit the linear sieve twice to estimate the conditional density ratio. First, we estimate an unconditional density ratio with an unweighted sieve least-squares regression, as done in Matsushita et al. (2022), and then the conditional density ratio with a weighted sieve least-squares regression, where the weights are the estimated unconditional density ratio. The proposed test has several advantages over existing alternatives. First, the test statistic is invariant to the monotone transformation of the data distribution and has a closed-form expression that enhances computational speed and efficiency. Second, the conditional density ratio satisfies the moment restrictions. The estimated ratio satisfies the empirical analog of those moment restrictions. As a result, the estimated density ratio is unlikely to have extreme values. Third, the proposed test can detect all deviations from conditional independence at rates arbitrarily close to $n^{-1/2}$ , and the local power loss is independent of the data dimension. A small-scale simulation study indicates that the proposed test outperforms the alternatives in various dependence structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17322v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunrong Ai, Zixuan Xu, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>Practical aspects of the virtual noise convex optimum design approach for correlated responses</title>
      <link>https://arxiv.org/abs/2504.17651</link>
      <description>arXiv:2504.17651v1 Announce Type: new 
Abstract: In this paper we present several practically-oriented extensions and considerations for the virtual noise method in optimal design under correlation. First we introduce a slightly modified virtual noise representation which further illuminates the parallels to the classical design approach for uncorrelated observations. We suggest more efficient algorithms to obtain the design measures. Furthermore, we show that various convex relaxation methods used for sensor selection are special cases of our approach and can be solved within our framework. Finally, we provide practical guidelines on how to generally approach a design problem with correlated observations and demonstrate how to utilize the virtual noise method in this context in a meaningful way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17651v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Hainy, Werner G. M\"uller, Andrej P\'azman</dc:creator>
    </item>
    <item>
      <title>An introduction to R package `mvs`</title>
      <link>https://arxiv.org/abs/2504.17546</link>
      <description>arXiv:2504.17546v1 Announce Type: cross 
Abstract: In biomedical science, a set of objects or persons can often be described by multiple distinct sets of features obtained from different data sources or modalities (called "multi-view data"). Classical machine learning methods ignore the multi-view structure of such data, limiting model interpretability and performance. The R package `mvs` provides methods that were designed specifically for dealing with multi-view data, based on the multi-view stacking (MVS) framework. MVS is a form of supervised (machine) learning used to train multi-view classification or prediction models. MVS works by training a learning algorithm on each view separately, estimating the predictive power of each view-specific model through cross-validation, and then using another learning algorithm to assign weights to the view-specific models based on their estimated predictions. MVS is a form of ensemble learning, dividing the large multi-view learning problem into smaller sub-problems. Most of these sub-problems can be solved in parallel, making it computationally attractive. Additionally, the number of features of the sub-problems is greatly reduced compared with the full multi-view learning problem. This makes MVS especially useful when the total number of features is larger than the number of observations (i.e., high-dimensional data). MVS can still be applied even if the sub-problems are themselves high-dimensional by adding suitable penalty terms to the learning algorithms. Furthermore, MVS can be used to automatically select the views which are most important for prediction. The R package `mvs` makes fitting MVS models, including such penalty terms, easily and openly accessible. `mvs` allows for the fitting of stacked models with any number of levels, with different penalty terms, different outcome distributions, and provides several options for missing data handling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17546v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter van Loon</dc:creator>
    </item>
    <item>
      <title>Some Results on Generalized Familywise Error Rate Controlling Procedures under Dependence</title>
      <link>https://arxiv.org/abs/2504.17611</link>
      <description>arXiv:2504.17611v1 Announce Type: cross 
Abstract: The topic of multiple hypotheses testing now has a potpourri of novel theories and ubiquitous applications in diverse scientific fields. However, the universal utility of this field often hinders the possibility of having a generalized theory that accommodates every scenario. This tradeoff is better reflected through the lens of dependence, a central piece behind the theoretical and applied developments of multiple testing. Although omnipresent in many scientific avenues, the nature and extent of dependence vary substantially with the context and complexity of the particular scenario. Positive dependence is the norm in testing many treatments versus a single control or in spatial statistics. On the contrary, negative dependence arises naturally in tests based on split samples and in cyclical, ordered comparisons. In GWAS, the SNP markers are generally considered to be weakly dependent. Generalized familywise error rate (k-FWER) control has been one of the prominent frequentist approaches in simultaneous inference. However, the performances of k-FWER controlling procedures are yet unexplored under different dependencies. This paper revisits the classical testing problem of normal means in different correlated frameworks. We establish upper bounds on the generalized familywise error rates under each dependence, consequently giving rise to improved testing procedures. Towards this, we present improved probability inequalities, which are of independent theoretical interest</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17611v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>Bayesian Function-on-Function Regression for Spatial Functional Data</title>
      <link>https://arxiv.org/abs/2401.08175</link>
      <description>arXiv:2401.08175v3 Announce Type: replace 
Abstract: Spatial functional data arise in many settings, such as particulate matter curves observed at monitoring stations and age population curves at each areal unit. Most existing functional regression models have limited applicability because they do not consider spatial correlations. Although functional kriging methods can predict the curves at unobserved spatial locations, they are based on variogram fittings rather than constructing hierarchical statistical models. In this manuscript, we propose a Bayesian framework for spatial function-on-function regression that can carry out parameter estimations and predictions. However, the proposed model has computational and inferential challenges because the model needs to account for within and between-curve dependencies. Furthermore, high-dimensional and spatially correlated parameters can lead to the slow mixing of Markov chain Monte Carlo algorithms. To address these issues, we first utilize a basis transformation approach to simplify the covariance and apply projection methods for dimension reduction. We also develop a simultaneous band score for the proposed model to detect the significant region in the regression function. We apply our method to both areal and point-level spatial functional data, showing the proposed method is computationally efficient and provides accurate estimations and predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08175v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heesang Lee, Dagun Oh, Sunhwa Choi, Jaewoo Park</dc:creator>
    </item>
    <item>
      <title>Analysis of Broken Randomized Experiments by Principal Stratification</title>
      <link>https://arxiv.org/abs/2405.16780</link>
      <description>arXiv:2405.16780v2 Announce Type: replace 
Abstract: Although randomized controlled trials have long been regarded as the ``gold standard'' for evaluating treatment effects, there is no natural prevention from post-treatment events. For example, non-compliance makes the actual treatment different from the assigned treatment, truncation-by-death renders the outcome undefined or ill-defined, and missingness prevents the outcomes from being measured. In this paper, we develop a statistical analysis framework using principal stratification to investigate the treatment effect in broken randomized experiments. The average treatment effect in compliers and always-survivors is adopted as the target causal estimand. We establish the asymptotic property for the estimator. To relax the identification assumptions, we also propose an interventionist estimand defined in compliers by adjusting for baseline covariates. We apply the framework to study the effect of training on earnings in the Job Corps study and find that the training program improves employment and earnings in the long term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16780v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinqing Liu, Xiang Peng, Tao Zhang, Yuhao Deng</dc:creator>
    </item>
    <item>
      <title>A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions</title>
      <link>https://arxiv.org/abs/2409.01444</link>
      <description>arXiv:2409.01444v3 Announce Type: replace 
Abstract: Prediction models need reliable predictive performance as they inform clinical decisions, aiding in diagnosis, prognosis, and treatment planning. The predictive performance of these models is typically assessed through discrimination and calibration. Changes in the distribution of the data impact model performance and there may be important changes between a model's current application and when and where its performance was last evaluated. In health-care, a typical change is a shift in case-mix. For example, for cardiovascular risk management, a general practitioner sees a different mix of patients than a specialist in a tertiary hospital.
  This work introduces a novel framework that differentiates the effects of case-mix shifts on discrimination and calibration based on the causal direction of the prediction task. When prediction is in the causal direction (often the case for prognosis predictions), calibration remains stable under case-mix shifts, while discrimination does not. Conversely, when predicting in the anti-causal direction (often with diagnosis predictions), discrimination remains stable, but calibration does not.
  A simulation study and empirical validation using cardiovascular disease prediction models demonstrate the implications of this framework. The causal case-mix framework provides insights for developing, evaluating and deploying prediction models across different clinical settings, emphasizing the importance of understanding the causal structure of the prediction task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01444v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter A. C. van Amsterdam</dc:creator>
    </item>
    <item>
      <title>Prior Sensitivity Analysis without Model Re-fit</title>
      <link>https://arxiv.org/abs/2409.19729</link>
      <description>arXiv:2409.19729v2 Announce Type: replace 
Abstract: Prior sensitivity analysis is a fundamental method to check the effects of prior distributions on the posterior distribution in Bayesian inference. Exploring the posteriors under several alternative priors can be computationally intensive, particularly for complex latent variable models. To address this issue, we propose a novel method for quantifying the prior sensitivity that does not require model re-fit. Specifically, we present a method to compute the Hellinger and Kullback-Leibler distances between two posterior distributions with base and alternative priors, using Monte Carlo integration based only on the base posterior distribution, through novel integral expressions of the two distances. We also extend the above approach for assessing the influence of hyperpriors in general latent variable models. We demonstrate the proposed method through examples of a simple normal distribution model, hierarchical binomial-beta model, and Gaussian process regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19729v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Model selection tests for truncated vine copulas under nested hypotheses</title>
      <link>https://arxiv.org/abs/2501.13304</link>
      <description>arXiv:2501.13304v2 Announce Type: replace 
Abstract: Vine copulas, constructed using bivariate copulas as building blocks, provide a flexible framework for modeling multi-dimensional dependencies. However, this flexibility is accompanied by rapidly increasing complexity as dimensionality grows, necessitating appropriate truncation to manage this challenge. While use of Vuong's model selection test has been proposed as a method to determine the optimal truncation level, its application to vine copulas has been heuristic, assuming only strictly non-nested hypotheses. This assumption conflicts with the inherent nesting within truncated vine copula structures. In this paper, we systematically apply Vuong's model selection tests to distinguish competing models of truncated vine copulas under both nested and strictly non-nested hypotheses. Through extensive simulation studies, we characterize the conditions under which the nested hypotheses provide improved discernibility and demonstrate that the strictly non-nested framework can still yield valid distinctions in certain settings. This broader perspective on model comparison contributes to both methodological clarity and practical guidance for vine copula truncation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13304v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ichiro Nishi, Yoshinori Kawasaki</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling</title>
      <link>https://arxiv.org/abs/2501.18577</link>
      <description>arXiv:2501.18577v2 Announce Type: replace 
Abstract: Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses. For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in downstream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research. However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid. Prior work uses what we call the Predict-Then-Debias estimator to give valid confidence intervals when machine learning algorithms impute missing variables, assuming a small complete sample from the population of interest. We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an arbitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations. We prove that these confidence intervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18577v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Exact Simulation of Longitudinal Data from Marginal Structural Models</title>
      <link>https://arxiv.org/abs/2502.07991</link>
      <description>arXiv:2502.07991v3 Announce Type: replace 
Abstract: Simulating longitudinal data from specified marginal structural models is a crucial but challenging task for evaluating causal inference methods and informing study design. While data generation typically proceeds in a fully conditional manner using structural equations according to a temporal ordering, it is difficult to ensure alignment between conditional distributions and the target marginal causal effects, which presents a fundamental challenge. To address this, we propose a flexible and efficient algorithm for simulating longitudinal data that adheres exactly to a specified marginal structural model. Our approach accommodates time-to-event outcomes and extends naturally to survival settings, which are prevalent in applied research. Compared to existing approaches, it offers several advantages: it enables exact simulation from a known causal model rather than relying on approximations; avoids restrictive assumptions about the data-generating process; and remains computationally efficient by requiring only the evaluation of analytical expressions, rather than Monte Carlo methods or numerical integration. Through simulation studies replicating realistic scenarios, we validate the method's accuracy and utility. Our method will facilitate researchers in effectively simulating data with target causal structures for their specific scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07991v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Lin, Daniel de Vassimon Manela, Chase Mathis, Jens Magelund Tarp, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>Comparison of Bayesian methods for extrapolation of treatment effects: a large scale simulation study</title>
      <link>https://arxiv.org/abs/2504.01949</link>
      <description>arXiv:2504.01949v2 Announce Type: replace 
Abstract: Extrapolating treatment effects from related studies is a promising strategy for designing and analyzing clinical trials in situations where achieving an adequate sample size is challenging. Bayesian methods are well-suited for this purpose, as they enable the synthesis of prior information through the use of prior distributions. While the operating characteristics of Bayesian approaches for borrowing data from control arms have been extensively studied, methods that borrow treatment effects -- quantities derived from the comparison between two arms -- remain less well understood. In this paper, we present the findings of an extensive simulation study designed to address this gap. We evaluate the frequentist operating characteristics of these methods, including the probability of success, mean squared error, bias, precision, and credible interval coverage. Our results provide insights into the strengths and limitations of existing methods in the context of confirmatory trials. In particular, we show that the Conditional Power Prior and the Robust Mixture Prior perform better overall, while the test-then-pool variants and the p-value-based power prior display suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01949v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Fauvel, Julien Tanniou, Pascal Godbillot, Marie G\'enin, Billy Amzal</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixtures Models with Repulsive and Attractive Atoms</title>
      <link>https://arxiv.org/abs/2302.09034</link>
      <description>arXiv:2302.09034v4 Announce Type: replace-cross 
Abstract: The study of almost surely discrete random probability measures is an active line of research in Bayesian nonparametrics. The idea of assuming interaction across the atoms of the random probability measure has recently spurred significant interest in the context of Bayesian mixture models. This allows the definition of priors that encourage well-separated and interpretable clusters. In this work, we provide a unified framework for the construction and the Bayesian analysis of random probability measures with interacting atoms, encompassing both repulsive and attractive behaviours. Specifically, we derive closed-form expressions for the posterior distribution, the marginal and predictive distributions, which were not previously available except for the case of measures with i.i.d. atoms. We show how these quantities are fundamental both for prior elicitation and to develop new posterior simulation algorithms for hierarchical mixture models. Our results are obtained without any assumption on the finite point process that governs the atoms of the random measure. Their proofs rely on analytical tools borrowed from the Palm calculus theory, which might be of independent interest. We specialise our treatment to the classes of Poisson, Gibbs, and determinantal point processes, as well as in the case of shot-noise Cox processes. Finally, we illustrate the performance of different modelling strategies on simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09034v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Raffaele Argiento, Federico Camerlenghi, Alessandra Guglielmi</dc:creator>
    </item>
    <item>
      <title>Effective Bayesian Causal Inference via Structural Marginalisation and Autoregressive Orders</title>
      <link>https://arxiv.org/abs/2402.14781</link>
      <description>arXiv:2402.14781v3 Announce Type: replace-cross 
Abstract: The traditional two-stage approach to causal inference first identifies a single causal model (or equivalence class of models), which is then used to answer causal queries. However, this neglects any epistemic model uncertainty. In contrast, Bayesian causal inference does incorporate epistemic uncertainty into query estimates via Bayesian marginalisation (posterior averaging) over all causal models. While principled, this marginalisation over entire causal models, i.e., both causal structures (graphs) and mechanisms, poses a tremendous computational challenge. In this work, we address this challenge by decomposing structure marginalisation into the marginalisation over (i) causal orders and (ii) directed acyclic graphs (DAGs) given an order. We can marginalise the latter in closed form by limiting the number of parents per variable and utilising Gaussian processes to model mechanisms. To marginalise over orders, we use a sampling-based approximation, for which we devise a novel auto-regressive distribution over causal orders (ARCO). Our method outperforms state-of-the-art in structure learning on simulated non-linear additive noise benchmarks, and yields competitive results on real-world data. Furthermore, we can accurately infer interventional distributions and average causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14781v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Toth, Christian Knoll, Franz Pernkopf, Robert Peharz</dc:creator>
    </item>
    <item>
      <title>Prediction Sets and Conformal Inference with Interval Outcomes</title>
      <link>https://arxiv.org/abs/2501.10117</link>
      <description>arXiv:2501.10117v3 Announce Type: replace-cross 
Abstract: Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10117v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiguang Liu, \'Aureo de Paula, Elie Tamer</dc:creator>
    </item>
    <item>
      <title>regMMD: An R package for parametric estimation and regression with maximum mean discrepancy</title>
      <link>https://arxiv.org/abs/2503.05297</link>
      <description>arXiv:2503.05297v2 Announce Type: replace-cross 
Abstract: The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for nonparametric tests and estimation. Recently, it has also been studied as an objective function for parametric estimation, as it has been shown to yield robust estimators. We have implemented MMD minimization for parameter inference in a wide range of statistical models, including various regression models, within an R package called regMMD. This paper provides an introduction to the regMMD package. We describe the available kernels and optimization procedures, as well as the default settings. Detailed applications to simulated and real data are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05297v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Alquier, Mathieu Gerber</dc:creator>
    </item>
  </channel>
</rss>
