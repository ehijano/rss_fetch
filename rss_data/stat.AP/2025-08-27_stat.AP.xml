<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Aug 2025 01:29:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Analytical Approach to Privacy and Performance Trade-Offs in Healthcare Data Sharing</title>
      <link>https://arxiv.org/abs/2508.18513</link>
      <description>arXiv:2508.18513v1 Announce Type: new 
Abstract: The secondary use of healthcare data is vital for research and clinical innovation, but it raises concerns about patient privacy. This study investigates how to balance privacy preservation and data utility in healthcare data sharing, considering the perspectives of both data providers and data users. Using a dataset of adult patients hospitalized between 2013 and 2015, we predict whether sepsis was present at admission or developed during the hospital stay. We identify sub-populations, such as older adults, frequently hospitalized patients, and racial minorities, that are especially vulnerable to privacy attacks due to their unique combinations of demographic and healthcare utilization attributes. These groups are also critical for machine learning (ML) model performance. We evaluate three anonymization methods-$k$-anonymity, the technique by Zheng et al., and the MO-OBAM model-based on their ability to reduce re-identification risk while maintaining ML utility. Results show that $k$-anonymity offers limited protection. The methods of Zheng et al. and MO-OBAM provide stronger privacy safeguards, with MO-OBAM yielding the best utility outcomes: only a 2% change in precision and recall compared to the original dataset. This work provides actionable insights for healthcare organizations on how to share data responsibly. It highlights the need for anonymization methods that protect vulnerable populations without sacrificing the performance of data-driven models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18513v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusi Wei, Hande Y. Benson, Muge Capan</dc:creator>
    </item>
    <item>
      <title>Dynamic Count Models with Flexible Innovation Processes for Irregular Maritime Migration</title>
      <link>https://arxiv.org/abs/2508.18716</link>
      <description>arXiv:2508.18716v1 Announce Type: new 
Abstract: Motivated by the dynamics of weekly sea border crossings in the Mediterranean (2015-2025) and the English Channel (2018-2025), we develop a Bayesian dynamic framework for modeling potentially heteroskedastic count time series. Building on theoretical considerations and empirical stylized facts, our approach specifies a latent log-intensity that follows a random walk driven by either heavy-tailed or stochastic volatility innovations, incorporating an explicit mechanism to separate structural from sampling zeros. Posterior inference is carried out via a straightforward Markov chain Monte Carlo algorithm. We compare alternative innovation specifications through a comprehensive out-of-sample density forecasting exercise, evaluating each model using log predictive scores and empirical coverage up to the 99th percentile of the predictive distribution. The results of two case studies reveal strong evidence for stochastic volatility in sea migration innovations, with stochastic volatility models producing particularly well-calibrated forecasts even at extreme quantiles. The model can be used to develop risk indicators and has direct policy implications for improving governance and preparedness for sea migration surges. The presented methodology readily extends to other zero-inflated non-stationary count time series applications, including epidemiological surveillance and public safety incident monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18716v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Zens, Jakub Bijak</dc:creator>
    </item>
    <item>
      <title>Separating Intent from Execution: A Probabilistic Approach to Pitch Location Accuracy</title>
      <link>https://arxiv.org/abs/2508.19184</link>
      <description>arXiv:2508.19184v1 Announce Type: new 
Abstract: Control has long been recognized as a critical component of pitcher performance, reflecting a pitcher's ability to execute pitches in alignment with his intended targets. However, accurately inferring a pitcher's intentions presents a persistent challenge. Traditional metrics typically rely on uniformity assumptions, inferring intent based on the behavior of a ``typical'' pitcher across similar situations. In this study, we propose an alternative, individualized approach to measuring control, one that eschews such assumptions in favor of personalized inference. We estimate a pitcher's intended location on a pitch-by-pitch basis, conditioning on both individual tendencies and specific game contexts. This allows us to assess control by comparing the actual pitch location to the inferred intended target, thereby aligning measurement more closely with the unique strategies of each pitcher. We introduce xCTRL, a novel metric that quantifies control as the distance between a pitch's actual location and its estimated intended location. We find that xCTRL exhibits strong stability and greater predictive power than existing control metrics. By capturing pitcher-specific intent, xCTRL enhances our understanding of control and offers a more intuitive and accurate representation of pitching performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19184v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Ludwig, Ryan S. Brill, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Mapping beyond diseases: Controlled variable selection for secondary phenotypes using tilted knockoffs</title>
      <link>https://arxiv.org/abs/2508.18548</link>
      <description>arXiv:2508.18548v1 Announce Type: cross 
Abstract: Researchers in biomedical studies often work with samples that are not selected uniformly at random from the population of interest, a major example being a case-control study. While these designs are motivated by specific scientific questions, it is often of interest to use the data collected to pursue secondary lines of investigations. In these cases, ignoring the fact that observations are not sampled uniformly at random can lead to spurious results. For example, in a case-control study, one might identify a spurious association between an exposure and a secondary phenotype when both affect the case-control status. This phenomenon is known as collider bias in the causal inference literature. While tests of independence under biased sampling are available, these methods typically do not apply when the number of variables is large.
  Here, we are interested in using the biased sample to select important exposures among a multitude of possible variables with replicability guarantees. While the model-X knockoff framework has been developed to test conditional independence hypotheses with False Discovery Rate (FDR) control, we show that its naive application fails to control FDR in the presence of biased sampling. We show how tilting the population distribution with the selection probability and constructing knockoff variables according to this tilted distribution instead leads to selection with FDR control. We study the FDR and power of the tilted knockoff method using simulated examples, and apply it to identify genetic underpinning of endophenotypes in a case-control study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18548v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qian Zhao, Susan Service, Carrie E. Bearden, Carlos Lopez-Jaramillo, Freimer Nelson, Chiara Sabatti</dc:creator>
    </item>
    <item>
      <title>Lightweight posterior construction for gravitational-wave catalogs with the Kolmogorov-Arnold network</title>
      <link>https://arxiv.org/abs/2508.18698</link>
      <description>arXiv:2508.18698v1 Announce Type: cross 
Abstract: Neural density estimation has seen widespread applications in the gravitational-wave (GW) data analysis, which enables real-time parameter estimation for compact binary coalescences and enhances rapid inference for subsequent analysis such as population inference. In this work, we explore the application of using the Kolmogorov-Arnold network (KAN) to construct efficient and interpretable neural density estimators for lightweight posterior construction of GW catalogs. By replacing conventional activation functions with learnable splines, KAN achieves superior interpretability, higher accuracy, and greater parameter efficiency on related scientific tasks. Leveraging this feature, we propose a KAN-based neural density estimator, which ingests megabyte-scale GW posterior samples and compresses them into model weights of tens of kilobytes. Subsequently, analytic expressions requiring only several kilobytes can be further distilled from these neural network weights with minimal accuracy trade-off. In practice, GW posterior samples with fidelity can be regenerated rapidly using the model weights or analytic expressions for subsequent analysis. Our lightweight posterior construction strategy is expected to facilitate user-level data storage and transmission, paving a path for efficient analysis of numerous GW events in the next-generation GW detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18698v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenshuai Liu, Yiming Dong, Ziming Wang, Lijing Shao</dc:creator>
    </item>
    <item>
      <title>Think before you fit: parameter identifiability, sensitivity and uncertainty in systems biology models</title>
      <link>https://arxiv.org/abs/2508.18853</link>
      <description>arXiv:2508.18853v1 Announce Type: cross 
Abstract: Reliable predictions from systems biology models require knowing whether parameters can be estimated from available data, and with what certainty. Identifiability analysis reveals whether parameters are learnable in principle (structural identifiability) and in practice (practical identifiability). We introduce the core ideas using linear models, highlighting how experimental design and output sensitivity shape identifiability. In nonlinear models, identifiability can vary with parameter values, motivating global and simulation-based approaches. We summarise computational methods for assessing identifiability noting that weakly identifiable parameters can undermine predictions beyond the calibration dataset. Strategies to improve identifiability include measuring different outputs, refining model structure, and adding prior knowledge. Far from a technical afterthought, identifiability determines the limits of inference and prediction. Recognising and addressing identifiability is essential for building models that are not only well-fitted to data, but also capable of delivering predictions with robust, quantifiable uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18853v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon P. Preston, Richard D. Wilkinson, Richard H. Clayton, Mike J. Chappell, Gary R. Mirams</dc:creator>
    </item>
    <item>
      <title>Power new generalized class of Kavya-Manoharan distributions with an application to exponential distribution</title>
      <link>https://arxiv.org/abs/2508.18930</link>
      <description>arXiv:2508.18930v1 Announce Type: cross 
Abstract: Recently, Verma et al. (2025) introduced a novel generalized class of Kavya-Manoharan distributions, which have demonstrated significant utility in reliability analysis and the modeling of lifetime data. This paper proposes an extension of this class by applying the power generalization technique, thereby enhancing more flexibility and applicability. We take the exponential distribution as the baseline distribution to introduce a new model capable of accommodating both monotonic and non-monotonic hazard rate functions. Our model includes eleven submodels. We present several statistical properties of the introduced model, including moments, generating and characteristic functions, mean deviations, quantile function, mean residual life function, R\'enyi entropy, order statistics, and reliability. To estimate the unknown model parameters, we use the maximum likelihood approach. A simulation study is conducted to assess the validity of the maximum likelihood estimator. The superiority of the new distribution is demonstrated through the use of a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18930v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lazhar Benkhelifa</dc:creator>
    </item>
    <item>
      <title>Preliminary Study on Space Utilization and Emergent Behaviors of Group vs. Single Pedestrians in Real-World Trajectories</title>
      <link>https://arxiv.org/abs/2508.18939</link>
      <description>arXiv:2508.18939v1 Announce Type: cross 
Abstract: This study presents an initial framework for distinguishing group and single pedestrians based on real-world trajectory data, with the aim of analyzing their differences in space utilization and emergent behavioral patterns. By segmenting pedestrian trajectories into fixed time bins and applying a Transformer-based pair classification model, we identify cohesive groups and isolate single pedestrians over a structured sequence-based filtering process. To prepare for deeper analysis, we establish a comprehensive metric framework incorporating both spatial and behavioral dimensions. Spatial utilization metrics include convex hull area, smallest enclosing circle radius, and heatmap-based spatial densities to characterize how different pedestrian types occupy and interact with space. Behavioral metrics such as velocity change, motion angle deviation, clearance radius, and trajectory straightness are designed to capture local adaptations and responses during interactions. Furthermore, we introduce a typology of encounter types-single-to-single, single-to-group, and group-to-group to categorize and later quantify different interaction scenarios. Although this version focuses primarily on the classification pipeline and dataset structuring, it establishes the groundwork for scalable analysis across different sequence lengths 60, 100, and 200 frames. Future versions will incorporate complete quantitative analysis of the proposed metrics and their implications for pedestrian simulation and space design validation in crowd dynamics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18939v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amartaivan Sanjjamts, Morita Hiroshi</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Modeling of Zero-Inflated Longitudinal Data and Survival with a Cure Fraction: Application to AIDS Data</title>
      <link>https://arxiv.org/abs/2508.19001</link>
      <description>arXiv:2508.19001v1 Announce Type: cross 
Abstract: We propose a comprehensive Bayesian joint modeling framework for zero-inflated longitudinal count data and time-to-event outcomes, explicitly incorporating a cure fraction to account for subjects who never experience the event. The longitudinal sub-model employs a flexible mixed-effects Hurdle model, with distributional options including zero-inflated Poisson and zero-inflated negative binomial, accommodating excess zeros and overdispersion common in count data. The survival component is modeled using a Cox proportional hazards model combined with a mixture cure model to distinguish cured from susceptible individuals. To link the longitudinal and survival processes, we include a linear combination of current longitudinal values as predictors in the survival model. Inference is performed via Hamiltonian Monte Carlo, enabling efficient and robust parameter estimation. The joint model supports dynamic predictions, facilitating real-time risk assessment and personalized medicine. Model performance and estimation accuracy are validated through simulation studies. Finally, we illustrate the methodology using a real-world HIV cohort dataset, demonstrating its practical utility in predicting patient survival outcomes and supporting personalized treatment decisions. Our results highlight the benefits of integrating complex longitudinal count data with survival information in clinical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19001v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taban Baghfalaki, Mojtaba Ganjali</dc:creator>
    </item>
    <item>
      <title>A Quick Estimation of Fr\'echet Quantizers for a Dynamic Solution to Flood Risk Management Problems</title>
      <link>https://arxiv.org/abs/2508.19045</link>
      <description>arXiv:2508.19045v1 Announce Type: cross 
Abstract: Multi-stage stochastic optimization is a well-known quantitative tool for decision-making under uncertainty. It is broadly used in financial and investment planning, inventory control, and also natural disaster risk management. Theoretical solutions of multi-stage stochastic programs can be found explicitly only in very exceptional cases due to their variational form and interdependency of uncertainty in time. Nevertheless, numerical solutions are often inaccurate, as they rely on Monte-Carlo sampling, which requires the Law of Large Numbers to hold for the approximation quality. In this article, we introduce a new approximation scheme, which computes and groups together stage-wise optimal quantizers of conditional Fr\'echet distributions for optimal weighting of value functions in the dynamic programming. We consider optimality of scenario quantization methods in the sense of minimal Kantorovich-Wasserstein distance at each stage of the scenario tree. By this, we bound the approximation error with convergence guarantees. We also provide global solution guarantees under convexity and monotonicity conditions on the value function. We apply the developed methods to the governmental budget allocation problem for risk management of flood events in Austria. For this, we propose an extremely efficient way to approximate optimal quantizers for conditional Fr\'echet distributions. Our approach allows to enhance the overall efficiency of dynamic programming via the use of different parameter estimation methods for different groups of quantizers. The groups are distinguished by a particular risk threshold and are able to differentiate between higher- and lower-impact flood events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19045v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anna Timonina-Farkas</dc:creator>
    </item>
    <item>
      <title>Replicability: Terminology, Measuring Success, and Strategy</title>
      <link>https://arxiv.org/abs/2508.19070</link>
      <description>arXiv:2508.19070v1 Announce Type: cross 
Abstract: Empirical science needs to be based on facts and claims that can be reproduced. This calls for replicating the studies that proclaim the claims, but practice in most fields still fails to implement this idea. When such studies emerged in the past decade, the results were generally disappointing. There have been an overwhelming number of papers addressing the ``reproducibility crisis'' in the last 20 years. Nevertheless, terminology is not yet settled, and there is no consensus about when a replication should be called successful. This paper intends to clarify such issues. A fundamental problem in empirical science is that usual claims only state that effects are non-zero, and such statements are scientifically void. An effect must have a \emph{relevant} size to become a reasonable item of knowledge. Therefore, estimation of an effect, with an indication of precision, forms a substantial scientific task, whereas testing it against zero does not. A relevant effect is one that is shown to exceed a relevance threshold. This paradigm has implications for the judgement on replication success.
  A further issue is the unavoidable variability between studies, called heterogeneity in meta-analysis. Therefore, it is of little value, again, to test for zero difference between an original effect and its replication, but exceedance of a corresponding relevance threshold should be tested. In order to estimate the degree of heterogeneity, more than one replication is needed, and an appropriate indication of the precision of an estimated effect requires such an estimate.
  These insights, which are discussed in the paper, show the complexity of obtaining solid scientific results, implying the need for a strategy to make replication happen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19070v1</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Werner A. Stahel (ETH Zurich, Switzerland)</dc:creator>
    </item>
    <item>
      <title>VPPE: Application of Scaled Vecchia Approximations to Parallel Partial Emulation</title>
      <link>https://arxiv.org/abs/2508.19144</link>
      <description>arXiv:2508.19144v1 Announce Type: cross 
Abstract: Computer models or simulators are widely used across scientific fields, but are computationally expensive limiting their use to explore possible scenarios/outcomes. Gaussian process emulators are statistical surrogates that can rapidly approximate the outputs of computer models at untested inputs and enable uncertainty quantification studies. The parallel partial emulation (PPE) was developed to model simulators with vector-valued outputs. While the PPE is adept at fitting simulator data with multidimensional outputs, the time to fit the PPE increases quickly as the number of training runs increases. The Scaled Vecchia approximation, a fast approximation to multivariate Gaussian likelihoods, makes fitting Gaussian process emulators with large training datasets tractable. Here we introduce the Vecchia Parallel Partial Emulation (VPPE) that utilizes the Scaled Vecchia approximation within the PPE framework to allow for parallel partial emulation with larger training datasets. The VPPE is applied to three computer experiments, a synthetic data set, a hydrology model, and a volcanic flow model, yielding comparable predictive accuracy to the PPE at a fraction of the runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19144v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josh Seidman, Elaine T. Spiller</dc:creator>
    </item>
    <item>
      <title>An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes</title>
      <link>https://arxiv.org/abs/2403.07554</link>
      <description>arXiv:2403.07554v3 Announce Type: replace 
Abstract: Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system. This can be used to enhance the accuracy of maintenance policies and increase the effectiveness of the equipment. In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables involved in a production process. The method is based on an Input-Output Hidden Markov Model (IO-HMM), in which the parameters of interest are the state transition probabilities and the parameters of the observations' joint density. The ultimate goal of the method is to predict operational process times in the near future, which enables the identification of hidden losses and the location of improvement areas in the process. The input stream in the IO-HMM model includes past values of the response variables and other process features, such as calendar variables, that can have an impact on the model's parameters. The discrete part of the IO-HMM models the operational mode of the process. The state transition probabilities are supposed to change over time and are updated using Bayesian principles. The continuous part of the IO-HMM models the joint density of the response variables. The estimate of the continuous model parameters is recursively computed through an adaptive algorithm that also admits a Bayesian interpretation. The adaptive algorithm allows for efficient updating of the current parameter estimates as soon as new information is available. We evaluate the method's performance using a real data set obtained from a company in a particular sector, and the results are compared with a collection of benchmark models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07554v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/asmb.70016</arxiv:DOI>
      <arxiv:journal_reference>Applied Stochastic Models in Business and Industry: Volume 41, Issue 3 May/June 2025, e70016</arxiv:journal_reference>
      <dc:creator>Fernando Miguelez, Josu Doncel, Maria Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Causal Feedback Discovery using Convergence Cross Mapping from Sea Ice Data</title>
      <link>https://arxiv.org/abs/2505.09001</link>
      <description>arXiv:2505.09001v2 Announce Type: replace 
Abstract: Identifying causal relationships in climate systems remains challenging due to nonlinear, coupled dynamics that limit the effectiveness of linear and stochastic causal discovery approaches. This study benchmarks Convergence Cross Mapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both synthetic datasets with ground truth causal links and 41 years of Arctic climate data</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09001v2</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Nji, Seraj Al Mahmud Mostafa, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>Subjective Perspectives within Learned Representations Predict High-Impact Innovation</title>
      <link>https://arxiv.org/abs/2506.04616</link>
      <description>arXiv:2506.04616v2 Announce Type: replace-cross 
Abstract: Existing studies of innovation emphasize the power of social structures to shape innovation capacity. Emerging machine learning approaches, however, enable us to model innovators' personal perspectives and interpersonal innovation opportunities as a function of their prior experience. We theorize and then quantify subjective perspectives and their interaction based on innovator positions within the geometric space of concepts inscribed by dynamic machine-learned language representations. Using data on millions of scientists, inventors, screenplay writers, entrepreneurs, and Wikipedia contributors across their respective creative domains, here we show that measured subjective perspectives predict which ideas individuals and groups will creatively attend to and successfully combine in the future. Across all cases and time periods we examine, when perspective diversity is decomposed as the difference between collaborators' perspectives on their creation, and background diversity as the difference between their experiences, the former consistently anticipates creative achievement while the latter portends its opposite. We analyze a natural experiment and simulate creative collaborations between AI agents designed with various perspective and background diversity, which support our observational findings. We explore mechanisms underlying these findings and identify how successful collaborators leverage common language to weave together diverse experiences obtained through trajectories of prior work. These perspectives converge and provoke one another to innovate. We examine the significance of these findings for team formation and research policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04616v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likun Cao, Rui Pan, James Evans</dc:creator>
    </item>
  </channel>
</rss>
