<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.AS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.AS</link>
    <description>eess.AS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.AS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 05:06:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Synth4Kws: Synthesized Speech for User Defined Keyword Spotting in Low Resource Environments</title>
      <link>https://arxiv.org/abs/2407.16840</link>
      <description>arXiv:2407.16840v1 Announce Type: new 
Abstract: One of the challenges in developing a high quality custom keyword spotting (KWS) model is the lengthy and expensive process of collecting training data covering a wide range of languages, phrases and speaking styles. We introduce Synth4Kws - a framework to leverage Text to Speech (TTS) synthesized data for custom KWS in different resource settings. With no real data, we found increasing TTS phrase diversity and utterance sampling monotonically improves model performance, as evaluated by EER and AUC metrics over 11k utterances of the speech command dataset. In low resource settings, with 50k real utterances as a baseline, we found using optimal amounts of TTS data can improve EER by 30.1% and AUC by 46.7%. Furthermore, we mix TTS data with varying amounts of real data and interpolate the real data needed to achieve various quality targets. Our experiments are based on English and single word utterances but the findings generalize to i18n languages and other keyword types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16840v1</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pai Zhu, Dhruuv Agarwal, Jacob W. Bartel, Kurt Partridge, Hyun Jin Park, Quan Wang</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Based Ensemble Learning For Speech Classification</title>
      <link>https://arxiv.org/abs/2407.17009</link>
      <description>arXiv:2407.17009v1 Announce Type: new 
Abstract: Speech classification has attracted increasing attention due to its wide applications, particularly in classifying physical and mental states. However, these tasks are challenging due to the high variability in speech signals. Ensemble learning has shown promising results when multiple classifiers are combined to improve performance. With recent advancements in hardware development, combining several models is not a limitation in deep learning research and applications. In this paper, we propose an uncertainty-based ensemble learning approach for speech classification. Specifically, we train a set of base features on the same classifier and quantify the uncertainty of their predictions. The predictions are combined using variants of uncertainty calculation to produce the final prediction. The visualization of the effect of uncertainty and its ensemble learning results show potential improvements in speech classification tasks. The proposed method outperforms single models and conventional ensemble learning methods in terms of unweighted accuracy or weighted accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17009v1</guid>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bagus Tris Atmaja, Felix Burkhardt</dc:creator>
    </item>
    <item>
      <title>Automatic Detection and Annotation of Sperm Whale Codas</title>
      <link>https://arxiv.org/abs/2407.17119</link>
      <description>arXiv:2407.17119v1 Announce Type: new 
Abstract: A key technology in sperm whale (Physeter macrocephalus) monitoring is the identification of sperm whale communication signals, known as codas. In this paper we present the first automatic coda detector and annotator. The main innovation in our detector is graph-based clustering, which utilizes the expected similarity between the clicks that make up the coda. Results show detection and accurate annotation at low signal-to-noise ratios, separation between codas and echolocation clicks, and discrimination between codas from simultaneously emitting whales. Using this automatic annotator, insights into the characterization of sperm whale communication are presented. The results include new types of coda signals, analyzes of the distribution of coda types among different whales and for different years, and evidence for synchronization between communicating whales in terms of coda type and coda transmission time. These results indicate a high degree of complexity in the communication system of this cetacean species. To ensure traceability, we share the implementation code of our coda detector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17119v1</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Gubnitsky, Yaly Mevorach, Shane Gero, David F. Gruber, Roee Diamant</dc:creator>
    </item>
    <item>
      <title>Reduction of Nonlinear Distortion in Condenser Microphones Using a Simple Post-Processing Technique</title>
      <link>https://arxiv.org/abs/2407.17250</link>
      <description>arXiv:2407.17250v1 Announce Type: new 
Abstract: In this paper, we introduce a novel approach for effectively reducing nonlinear distortion in single back-plate condenser microphones, i.e., most MEMS microphones, studio recording condenser microphones, and laboratory measurement microphones. This simple post-processing technique can be easily integrated on an external hardware such as an analog circuit, microcontroller, audio codec, DSP unit, or within the ASIC chip in a case of MEMS microphones. It significantly reduces microphone distortion across its frequency and dynamic range. It relies on a single parameter, which can be derived from either the microphone's physical parameters or a straightforward measurement presented in this paper. An optimal estimate of this parameter achieves the best distortion reduction, whereas overestimating it never increases distortion beyond the original level. The technique was tested on a MEMS microphone. Our findings indicate that for harmonic excitation the proposed technique reduces the second harmonic by approximately 40 dB, leading to a significant reduction in the Total Harmonic Distortion (THD). The efficiency of the distortion reduction technique for more complex signals is demonstrated through two-tone and multitone experiments, where second-order intermodulation products are reduced by at least 20 dB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17250v1</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petr Honz\'ik, Antonin Novak</dc:creator>
    </item>
    <item>
      <title>Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification</title>
      <link>https://arxiv.org/abs/2407.17416</link>
      <description>arXiv:2407.17416v1 Announce Type: new 
Abstract: This study investigates discriminative patterns learned by neural networks for accurate speech classification, with a specific focus on vowel classification tasks. By examining the activations and features of neural networks for vowel classification, we gain insights into what the networks "see" in spectrograms. Through the use of class activation mapping, we identify the frequencies that contribute to vowel classification and compare these findings with linguistic knowledge. Experiments on a American English dataset of vowels showcases the explainability of neural networks and provides valuable insights into the causes of misclassifications and their characteristics when differentiating them from unvoiced speech. This study not only enhances our understanding of the underlying acoustic cues in vowel classification but also offers opportunities for improving speech recognition by bridging the gap between abstract representations in neural networks and established linguistic knowledge</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17416v1</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesin James, Balamurali B. T., Binu Abeysinghe, Junchen Liu</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Review and Taxonomy of Audio-Visual Synchronization Techniques for Realistic Speech Animation</title>
      <link>https://arxiv.org/abs/2407.17430</link>
      <description>arXiv:2407.17430v1 Announce Type: new 
Abstract: In many applications, synchronizing audio with visuals is crucial, such as in creating graphic animations for films or games, translating movie audio into different languages, and developing metaverse applications. This review explores various methodologies for achieving realistic facial animations from audio inputs, highlighting generative and adaptive models. Addressing challenges like model training costs, dataset availability, and silent moment distributions in audio data, it presents innovative solutions to enhance performance and realism. The research also introduces a new taxonomy to categorize audio-visual synchronization methods based on logistical aspects, advancing the capabilities of virtual assistants, gaming, and interactive digital media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17430v1</guid>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jose Geraldo Fernandes, Sinval Nascimento, Daniel Dominguete, Andr\'e Oliveira, Lucas Rotsen, Gabriel Souza, Gabriel Lara, Mateus Vilela, Pedro Mapa, David Brochero, Hebert Costa, Frederico Coelho, Ant\^onio P. Braga</dc:creator>
    </item>
    <item>
      <title>Towards better visualizations of urban sound environments: insights from interviews</title>
      <link>https://arxiv.org/abs/2407.16889</link>
      <description>arXiv:2407.16889v1 Announce Type: cross 
Abstract: Urban noise maps and noise visualizations traditionally provide macroscopic representations of noise levels across cities. However, those representations fail at accurately gauging the sound perception associated with these sound environments, as perception highly depends on the sound sources involved. This paper aims at analyzing the need for the representations of sound sources, by identifying the urban stakeholders for whom such representations are assumed to be of importance. Through spoken interviews with various urban stakeholders, we have gained insight into current practices, the strengths and weaknesses of existing tools and the relevance of incorporating sound sources into existing urban sound environment representations. Three distinct use of sound source representations emerged in this study: 1) noise-related complaints for industrials and specialized citizens, 2) soundscape quality assessment for citizens, and 3) guidance for urban planners. Findings also reveal diverse perspectives for the use of visualizations, which should use indicators adapted to the target audience, and enable data accessibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16889v1</guid>
      <category>cs.CY</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>INTERNOISE 2024, Aug 2024, Nantes (France), France</arxiv:journal_reference>
      <dc:creator>Modan Tailleur (LS2N), Pierre Aumond (UMRAE), Vincent Tourre (AAU), Mathieu Lagrange (LS2N)</dc:creator>
    </item>
    <item>
      <title>A Framework for AI assisted Musical Devices</title>
      <link>https://arxiv.org/abs/2407.16899</link>
      <description>arXiv:2407.16899v1 Announce Type: cross 
Abstract: In this paper we present a novel framework for the study and design of AI assisted musical devices (AIMEs). Initially, we present a taxonomy of these devices and illustrate it with a set of scenarios and personas. Later, we propose a generic architecture for the implementation of AIMEs and present some examples from the scenarios. We show that the proposed framework and architecture are a valid tool for the study of intelligent musical devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16899v1</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5772/intechopen.108898</arxiv:DOI>
      <arxiv:journal_reference>IntechOpen (2023)</arxiv:journal_reference>
      <dc:creator>Miguel Civit, Luis Munoz Saavedra, Francisco Jose Cuadrado, Maria J. Escalona</dc:creator>
    </item>
    <item>
      <title>Long-Term, Store-Front Robotics: Interactive Music for Robotic Arm, Caxixi and Frame Drums</title>
      <link>https://arxiv.org/abs/2407.16956</link>
      <description>arXiv:2407.16956v1 Announce Type: cross 
Abstract: This paper presents an innovative exploration into the integration of interactive robotic musicianship within a commercial retail environment, specifically through a three-week-long in-store installation featuring a UR3 robotic arm, custom-built frame drums, and an adaptive music generation system. Situated in a prominent storefront in one of the world's largest cities, this project aimed to enhance the shopping experience by creating dynamic, engaging musical interactions that respond to the store's ambient soundscape. Key contributions include the novel application of industrial robotics in artistic expression, the deployment of interactive music to enrich retail ambiance, and the demonstration of continuous robotic operation in a public setting over an extended period. Challenges such as system reliability, variation in musical output, safety in interactive contexts, and brand alignment were addressed to ensure the installation's success. The project not only showcased the technical feasibility and artistic potential of robotic musicianship in retail spaces but also offered insights into the practical implications of such integration, including system reliability, the dynamics of human-robot interaction, and the impact on store operations. This exploration opens new avenues for enhancing consumer retail experiences through the intersection of technology, music, and interactive art, suggesting a future where robotic musicianship contributes meaningfully to public and commercial spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16956v1</guid>
      <category>cs.RO</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Computer Music Conference 2024, Seoul, South Korea</arxiv:journal_reference>
      <dc:creator>Richard Savery, Fouad Sukkar</dc:creator>
    </item>
    <item>
      <title>Collaboration Between Robots, Interfaces and Humans: Practice-Based and Audience Perspectives</title>
      <link>https://arxiv.org/abs/2407.16966</link>
      <description>arXiv:2407.16966v1 Announce Type: cross 
Abstract: This paper provides an analysis of a mixed-media experimental musical work that explores the integration of human musical interaction with a newly developed interface for the violin, manipulated by an improvising violinist, interactive visuals, a robotic drummer and an improvised synthesised orchestra. We first present a detailed technical overview of the systems involved including the design and functionality of each component. We then conduct a practice-based review examining the creative processes and artistic decisions underpinning the work, focusing on the challenges and breakthroughs encountered during its development. Through this introspective analysis, we uncover insights into the collaborative dynamics between the human performer and technological agents, revealing the complexities of blending traditional musical expressiveness with artificial intelligence and robotics. To gauge public reception and interpretive perspectives, we conducted an online survey, sharing a video of the performance with a diverse audience. The feedback collected from this survey offers valuable viewpoints on the accessibility, emotional impact, and perceived artistic value of the work. Respondents' reactions underscore the transformative potential of integrating advanced technologies in musical performance, while also highlighting areas for further exploration and refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16966v1</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Computer Music Conference 2024, Seoul, South Korea</arxiv:journal_reference>
      <dc:creator>Anna Savery, Richard Savery</dc:creator>
    </item>
    <item>
      <title>Zero-Shot vs. Few-Shot Multi-Speaker TTS Using Pre-trained Czech SpeechT5 Model</title>
      <link>https://arxiv.org/abs/2407.17167</link>
      <description>arXiv:2407.17167v1 Announce Type: cross 
Abstract: In this paper, we experimented with the SpeechT5 model pre-trained on large-scale datasets. We pre-trained the foundation model from scratch and fine-tuned it on a large-scale robust multi-speaker text-to-speech (TTS) task. We tested the model capabilities in a zero- and few-shot scenario. Based on two listening tests, we evaluated the synthetic audio quality and the similarity of how synthetic voices resemble real voices. Our results showed that the SpeechT5 model can generate a synthetic voice for any speaker using only one minute of the target speaker's data. We successfully demonstrated the high quality and similarity of our synthetic voices on publicly known Czech politicians and celebrities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17167v1</guid>
      <category>cs.SD</category>
      <category>cs.CL</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jan Lehe\v{c}ka, Zden\v{e}k Hanzl\'i\v{c}ek, Jind\v{r}ich Matou\v{s}ek, Daniel Tihelka</dc:creator>
    </item>
    <item>
      <title>Speech Editing -- a Summary</title>
      <link>https://arxiv.org/abs/2407.17172</link>
      <description>arXiv:2407.17172v1 Announce Type: cross 
Abstract: With the rise of video production and social media, speech editing has become crucial for creators to address issues like mispronunciations, missing words, or stuttering in audio recordings. This paper explores text-based speech editing methods that modify audio via text transcripts without manual waveform editing. These approaches ensure edited audio is indistinguishable from the original by altering the mel-spectrogram. Recent advancements, such as context-aware prosody correction and advanced attention mechanisms, have improved speech editing quality. This paper reviews state-of-the-art methods, compares key metrics, and examines widely used datasets. The aim is to highlight ongoing issues and inspire further research and innovation in speech editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17172v1</guid>
      <category>cs.SD</category>
      <category>cs.CL</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias K\"assmann, Yining Liu, Danni Liu</dc:creator>
    </item>
    <item>
      <title>EMO-Codec: A Depth Look at Emotion Preservation Capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations</title>
      <link>https://arxiv.org/abs/2407.15458</link>
      <description>arXiv:2407.15458v3 Announce Type: replace 
Abstract: The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15458v3</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenze Ren, Yi-Cheng Lin, Huang-Cheng Chou, Haibin Wu, Yi-Chiao Wu, Chi-Chun Lee, Hung-yi Lee, Yu Tsao</dc:creator>
    </item>
    <item>
      <title>EmoFake: An Initial Dataset for Emotion Fake Audio Detection</title>
      <link>https://arxiv.org/abs/2211.05363</link>
      <description>arXiv:2211.05363v4 Announce Type: replace-cross 
Abstract: Many datasets have been designed to further the development of fake audio detection, such as datasets of the ASVspoof and ADD challenges. However, these datasets do not consider a situation that the emotion of the audio has been changed from one to another, while other information (e.g. speaker identity and content) remains the same. Changing the emotion of an audio can lead to semantic changes. Speech with tampered semantics may pose threats to people's lives. Therefore, this paper reports our progress in developing such an emotion fake audio detection dataset involving changing emotion state of the origin audio named EmoFake. The fake audio in EmoFake is generated by open source emotion voice conversion models. Furthermore, we proposed a method named Graph Attention networks using Deep Emotion embedding (GADE) for the detection of emotion fake audio. Some benchmark experiments are conducted on this dataset. The results show that our designed dataset poses a challenge to the fake audio detection model trained with the LA dataset of ASVspoof 2019. The proposed GADE shows good performance in the face of emotion fake audio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05363v4</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhao, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Xiaohui Zhang, Yongfeng Dong</dc:creator>
    </item>
    <item>
      <title>Detecting Throat Cancer from Speech Signals using Machine Learning: A Scoping Literature Review</title>
      <link>https://arxiv.org/abs/2307.09230</link>
      <description>arXiv:2307.09230v2 Announce Type: replace-cross 
Abstract: Introduction: Cases of throat cancer are rising worldwide. With survival decreasing significantly at later stages, early detection is vital. Artificial intelligence (AI) and machine learning (ML) have the potential to detect throat cancer from patient speech, facilitating earlier diagnosis and reducing the burden on overstretched healthcare systems. However, no comprehensive review has explored the use of AI and ML for detecting throat cancer from speech. This review aims to fill this gap by evaluating how these technologies perform and identifying issues that need to be addressed in future research. Materials and Methods: We conducted a scoping literature review across three databases: Scopus,Web of Science, and PubMed. We included articles that classified speech using machine learning and specified the inclusion of throat cancer patients in their data. Articles were categorized based on whether they performed binary or multi-class classification. Results: We found 27 articles fitting our inclusion criteria, 12 performing binary classification, 13 performing multi-class classification, and two that do both binary and multiclass classification. The most common classification method used was neural networks, and the most frequently extracted feature was mel-spectrograms. We also documented pre-processing methods and classifier performance. We compared each article against the TRIPOD-AI checklist, which showed a significant lack of open science, with only one article sharing code and only three using open-access data. Conclusion: Open-source code is essential for external validation and further development in this field. Our review indicates that no single method or specific feature consistently outperforms others in detecting throat cancer from speech. Future research should focus on standardizing methodologies and improving the reproducibility of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09230v2</guid>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Paterson, James Moor, Luisa Cutillo</dc:creator>
    </item>
    <item>
      <title>Multi-Convformer: Extending Conformer with Multiple Convolution Kernels</title>
      <link>https://arxiv.org/abs/2407.03718</link>
      <description>arXiv:2407.03718v2 Announce Type: replace-cross 
Abstract: Convolutions have become essential in state-of-the-art end-to-end Automatic Speech Recognition~(ASR) systems due to their efficient modelling of local context. Notably, its use in Conformers has led to superior performance compared to vanilla Transformer-based ASR systems. While components other than the convolution module in the Conformer have been reexamined, altering the convolution module itself has been far less explored. Towards this, we introduce Multi-Convformer that uses multiple convolution kernels within the convolution module of the Conformer in conjunction with gating. This helps in improved modeling of local dependencies at varying granularities. Our model rivals existing Conformer variants such as CgMLP and E-Branchformer in performance, while being more parameter efficient. We empirically compare our approach with Conformer and its variants across four different datasets and three different modelling paradigms and show up to 8% relative word error rate~(WER) improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03718v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Darshan Prabhu, Yifan Peng, Preethi Jyothi, Shinji Watanabe</dc:creator>
    </item>
    <item>
      <title>On the Utility of Speech and Audio Foundation Models for Marmoset Call Analysis</title>
      <link>https://arxiv.org/abs/2407.16417</link>
      <description>arXiv:2407.16417v2 Announce Type: replace-cross 
Abstract: Marmoset monkeys encode vital information in their calls and serve as a surrogate model for neuro-biologists to understand the evolutionary origins of human vocal communication. Traditionally analyzed with signal processing-based features, recent approaches have utilized self-supervised models pre-trained on human speech for feature extraction, capitalizing on their ability to learn a signal's intrinsic structure independently of its acoustic domain. However, the utility of such foundation models remains unclear for marmoset call analysis in terms of multi-class classification, bandwidth, and pre-training domain. This study assesses feature representations derived from speech and general audio domains, across pre-training bandwidths of 4, 8, and 16 kHz for marmoset call-type and caller classification tasks. Results show that models with higher bandwidth improve performance, and pre-training on speech or general audio yields comparable results, improving over a spectral baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16417v2</guid>
      <category>cs.SD</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eklavya Sarkar, Mathew Magimai. -Doss</dc:creator>
    </item>
    <item>
      <title>Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning</title>
      <link>https://arxiv.org/abs/2407.16564</link>
      <description>arXiv:2407.16564v2 Announce Type: replace-cross 
Abstract: Text-to-music models allow users to generate nearly realistic musical audio with textual commands. However, editing music audios remains challenging due to the conflicting desiderata of performing fine-grained alterations on the audio while maintaining a simple user interface. To address this challenge, we propose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feedthese features into the internal layers of AudioLDM2, a diffusion-based text-to-music model. With 22M trainable parameters, AP-Adapter empowers users to harness both global (e.g., genre and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP-Adapter on three tasks: timbre transfer, genre transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16564v2</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fang-Duo Tsai, Shih-Lun Wu, Haven Kim, Bo-Yu Chen, Hao-Chung Cheng, Yi-Hsuan Yang</dc:creator>
    </item>
  </channel>
</rss>
