<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deciphering Cardiac Destiny: Unveiling Future Risks Through Cutting-Edge Machine Learning Approaches</title>
      <link>https://arxiv.org/abs/2409.15287</link>
      <description>arXiv:2409.15287v1 Announce Type: new 
Abstract: Cardiac arrest remains a leading cause of death worldwide, necessitating proactive measures for early detection and intervention. This project aims to develop and assess predictive models for the timely identification of cardiac arrest incidents, utilizing a comprehensive dataset of clinical parameters and patient histories. Employing machine learning (ML) algorithms like XGBoost, Gradient Boosting, and Naive Bayes, alongside a deep learning (DL) approach with Recurrent Neural Networks (RNNs), we aim to enhance early detection capabilities. Rigorous experimentation and validation revealed the superior performance of the RNN model, which effectively captures complex temporal dependencies within the data. Our findings highlight the efficacy of these models in accurately predicting cardiac arrest likelihood, emphasizing the potential for improved patient care through early risk stratification and personalized interventions. By leveraging advanced analytics, healthcare providers can proactively mitigate cardiac arrest risk, optimize resource allocation, and improve patient outcomes. This research highlights the transformative potential of machine learning and deep learning techniques in managing cardiovascular risk and advances the field of predictive healthcare analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15287v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Divya, M. Naga SravanKumar, T. JayaDharani, B. Pavan, K. Praveen</dc:creator>
    </item>
    <item>
      <title>Enhancing MBSE Education with Version Control and Automated Feedback</title>
      <link>https://arxiv.org/abs/2409.15294</link>
      <description>arXiv:2409.15294v1 Announce Type: new 
Abstract: This paper presents an innovative approach to conducting a Model-Based Systems Engineering (MBSE) course, engaging over 80 participants annually. The course is structured around collaborative group assignments, where students utilize Enterprise Architect to complete complex systems engineering tasks across six submissions. This year, we introduced several technological advancements to enhance the learning experience, including the use of LemonTree, SmartGit, and GitHub. Students collaborated on shared repositories in GitHub, received continuous feedback via automated checks through LemonTree Automation, and documented their progress with pre-rendered, continuously updating diagrams. Additionally, they managed 2-way and 3-way merges directly in SmartGit, with merge issues, updates, and model statistics readily available for each Work-in-Progress submission. The process of correcting and providing manual feedback was streamlined, thanks to accessible changelogs and renders in GitHub. An end-of-course feedback form revealed high student satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15294v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Levente Bajczi, D\'aniel Szekeres, Daniel Siegl, Vince Moln\'ar</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Education: Ethical Considerations and Insights from Ancient Greek Philosophy</title>
      <link>https://arxiv.org/abs/2409.15296</link>
      <description>arXiv:2409.15296v1 Announce Type: new 
Abstract: This paper explores the ethical implications of integrating Artificial Intelligence (AI) in educational settings, from primary schools to universities, while drawing insights from ancient Greek philosophy to address emerging concerns. As AI technologies increasingly influence learning environments, they offer novel opportunities for personalized learning, efficient assessment, and data-driven decision-making. However, these advancements also raise critical ethical questions regarding data privacy, algorithmic bias, student autonomy, and the changing roles of educators. This research examines specific use cases of AI in education, analyzing both their potential benefits and drawbacks. By revisiting the philosophical principles of ancient Greek thinkers such as Socrates, Aristotle, and Plato, we discuss how their writings can guide the ethical implementation of AI in modern education. The paper argues that while AI presents significant challenges, a balanced approach informed by classical philosophical thought can lead to an ethically sound transformation of education. It emphasizes the evolving role of teachers as facilitators and the importance of fostering student initiative in AI-rich environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15296v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kostas Karpouzis</dc:creator>
    </item>
    <item>
      <title>Transforming Redaction: How AI is Revolutionizing Data Protection</title>
      <link>https://arxiv.org/abs/2409.15308</link>
      <description>arXiv:2409.15308v1 Announce Type: new 
Abstract: Document redaction is a crucial process in various sectors to safeguard sensitive information from unauthorized access and disclosure. Traditional manual redaction methods, such as those performed using Adobe Acrobat, are labor-intensive, error-prone, and time-consuming. With the burgeoning volume of digital documents, the demand for more efficient and accurate redaction techniques is intensifying.
  This study presents the findings from a controlled experiment that compares traditional manual redaction, a redaction tool powered by classical machine learning algorithm, and AI-assisted redaction tools (iDox.ai Redact). The results indicate that iDox.ai Redact significantly outperforms manual methods, achieving higher accuracy and faster completion times. Conversely, the competitor product, classical machine learning algorithm and with necessitates manual intervention for certain sensitive data types, did not exhibit a statistically significant improvement over manual redaction.
  These findings suggest that while advanced AI technologies like iDox.ai Redact can substantially enhance data protection practices by reducing human error and improving compliance with data protection regulations, there remains room for improvement in AI tools that do not fully automate the redaction process. Future research should aim to enhance AI capabilities and explore their applicability across various document types and professional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15308v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sida Peng, Ming-Jen Huang, Matt Wu, Jeremy Wei</dc:creator>
    </item>
    <item>
      <title>Merging AI Incidents Research with Political Misinformation Research: Introducing the Political Deepfakes Incidents Database</title>
      <link>https://arxiv.org/abs/2409.15319</link>
      <description>arXiv:2409.15319v1 Announce Type: new 
Abstract: This article presents the Political Deepfakes Incidents Database (PDID), a collection of politically-salient deepfakes, encompassing synthetically-created videos, images, and less-sophisticated `cheapfakes.' The project is driven by the rise of generative AI in politics, ongoing policy efforts to address harms, and the need to connect AI incidents and political communication research. The database contains political deepfake content, metadata, and researcher-coded descriptors drawn from political science, public policy, communication, and misinformation studies. It aims to help reveal the prevalence, trends, and impact of political deepfakes, such as those featuring major political figures or events. The PDID can benefit policymakers, researchers, journalists, fact-checkers, and the public by providing insights into deepfake usage, aiding in regulation, enabling in-depth analyses, supporting fact-checking and trust-building efforts, and raising awareness of political deepfakes. It is suitable for research and application on media effects, political discourse, AI ethics, technology governance, media literacy, and countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15319v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v38i21.30349</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 21. 2024</arxiv:journal_reference>
      <dc:creator>Christina P. Walker, Daniel S. Schiff, Kaylyn Jackson Schiff</dc:creator>
    </item>
    <item>
      <title>Introducing ELLIPS: An Ethics-Centered Approach to Research on LLM-Based Inference of Psychiatric Conditions</title>
      <link>https://arxiv.org/abs/2409.15323</link>
      <description>arXiv:2409.15323v1 Announce Type: new 
Abstract: As mental health care systems worldwide struggle to meet demand, there is increasing focus on using language models to infer neuropsychiatric conditions or psychopathological traits from language production. Yet, so far, this research has only delivered solutions with limited clinical applicability, due to insufficient consideration of ethical questions crucial to ensuring the synergy between possible applications and model design. To accelerate progress towards clinically applicable models, our paper charts the ethical landscape of research on language-based inference of psychopathology and provides a practical tool for researchers to navigate it. We identify seven core ethical principles that should guide model development and deployment in this domain, translate them into ELLIPS, an ethical toolkit operationalizing these principles into questions that can guide researchers' choices with respect to data selection, architectures, evaluation, and model deployment, and provide a case study exemplifying its use. With this, we aim to facilitate the emergence of model technology with concrete potential for real-world applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15323v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberta Rocca, Giada Pistilli, Kritika Maheshwari, Riccardo Fusaroli</dc:creator>
    </item>
    <item>
      <title>Explainable AI: Definition and attributes of a good explanation for health AI</title>
      <link>https://arxiv.org/abs/2409.15338</link>
      <description>arXiv:2409.15338v1 Announce Type: new 
Abstract: Proposals of artificial intelligence (AI) solutions based on increasingly complex and accurate predictive models are becoming ubiquitous across many disciplines. As the complexity of these models grows, transparency and users' understanding often diminish. This suggests that accurate prediction alone is insufficient for making an AI-based solution truly useful. In the development of healthcare systems, this introduces new issues related to accountability and safety. Understanding how and why an AI system makes a recommendation may require complex explanations of its inner workings and reasoning processes. Although research on explainable AI (XAI) has significantly increased in recent years and there is high demand for XAI in medicine, defining what constitutes a good explanation remains ad hoc, and providing adequate explanations continues to be challenging. To fully realize the potential of AI, it is critical to address two fundamental questions about explanations for safety-critical AI applications, such as health-AI: (1) What is an explanation in health-AI? and (2) What are the attributes of a good explanation in health-AI? In this study, we examined published literature and gathered expert opinions through a two-round Delphi study. The research outputs include (1) a definition of what constitutes an explanation in health-AI and (2) a comprehensive list of attributes that characterize a good explanation in health-AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15338v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evangelia Kyrimi, Scott McLachlan, Jared M Wohlgemut, Zane B Perkins, David A. Lagnado, William Marsh, the ExAIDSS Expert Group</dc:creator>
    </item>
    <item>
      <title>An Interactive Web Application for School-Based Physical Fitness Testing in California: Geospatial Analysis and Custom Mapping</title>
      <link>https://arxiv.org/abs/2409.15352</link>
      <description>arXiv:2409.15352v1 Announce Type: new 
Abstract: Physical activity is essential for children's healthy growth and development. In the US, most states, including California, adhere to physical education standards and have implemented the mandated School-based Physical Fitness Testing (SB-PFT) for over two decades. Despite extensive data collection, research utilization of SB-PFT has been limited due to the absence of accessible analytical tools. We developed a web application using GeoServer, ArcGIS, and AWS to visualize SB-PFT data. This user-friendly platform enables education administrators and policymakers to analyze trends in children's physical fitness, identify successful programs at schools and districts, and evaluate new physical education initiatives. The application also features a custom mapping tool for comparing external datasets with SB-PFT data. We conclude that this platform, by integrating advanced analytical capabilities in an informatics-based tool, significantly enhances engagement in promoting children's physical fitness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15352v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yawen Guo, Kaiyuan Hu, Di Hu, Kai Zheng, Dan Cooper</dc:creator>
    </item>
    <item>
      <title>Asking an AI for salary negotiation advice is a matter of concern: Controlled experimental perturbation of ChatGPT for protected and non-protected group discrimination on a contextual task with no clear ground truth answers</title>
      <link>https://arxiv.org/abs/2409.15567</link>
      <description>arXiv:2409.15567v1 Announce Type: new 
Abstract: We conducted controlled experimental bias audits for four versions of ChatGPT, which we asked to recommend an opening offer in salary negotiations for a new hire. We submitted 98,800 prompts to each version, systematically varying the employee's gender, university, and major, and tested prompts in voice of each side of the negotiation: the employee versus employer. We find ChatGPT as a multi-model platform is not robust and consistent enough to be trusted for such a task. We observed statistically significant salary offers when varying gender for all four models, although with smaller gaps than for other attributes tested. The largest gaps were different model versions and between the employee- vs employer-voiced prompts. We also observed substantial gaps when varying university and major, but many of the biases were not consistent across model versions. We tested for fictional and fraudulent universities and found wildly inconsistent results across cases and model versions. We make broader contributions to the AI/ML fairness literature. Our scenario and our experimental design differ from mainstream AI/ML auditing efforts in key ways. Bias audits typically test discrimination for protected classes like gender, which we contrast with testing non-protected classes of university and major. Asking for negotiation advice includes how aggressive one ought to be in a negotiation relative to known empirical salary distributions and scales, which is a deeply contextual and personalized task that has no objective ground truth to validate. These results raise concerns for the specific model versions we tested and ChatGPT as a multi-model platform in continuous development. Our epistemology does not permit us to definitively certify these models as either generally biased or unbiased on the attributes we test, but our study raises matters of concern for stakeholders to further investigate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15567v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>R. Stuart Geiger, Flynn O'Sullivan, Elsie Wang, Jonathan Lo</dc:creator>
    </item>
    <item>
      <title>Mitigating Digital Discrimination in Dating Apps - The Dutch Breeze case</title>
      <link>https://arxiv.org/abs/2409.15828</link>
      <description>arXiv:2409.15828v1 Announce Type: new 
Abstract: In September 2023, the Netherlands Institute for Human Rights, the Dutch non-discrimination authority, decided that Breeze, a Dutch dating app, was justified in suspecting that their algorithm discriminated against non-white. Consequently, the Institute decided that Breeze must prevent this discrimination based on ethnicity. This paper explores two questions. (i) Is the discrimination based on ethnicity in Breeze's matching algorithm illegal? (ii) How can dating apps mitigate or stop discrimination in their matching algorithms? We illustrate the legal and technical difficulties dating apps face in tackling discrimination and illustrate promising solutions. We analyse the Breeze decision in-depth, combining insights from computer science and law. We discuss the implications of this judgment for scholarship and practice in the field of fair and non-discriminatory machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15828v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim de Jonge, Frederik Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>DepMamba: Progressive Fusion Mamba for Multimodal Depression Detection</title>
      <link>https://arxiv.org/abs/2409.15936</link>
      <description>arXiv:2409.15936v1 Announce Type: new 
Abstract: Depression is a common mental disorder that affects millions of people worldwide. Although promising, current multimodal methods hinge on aligned or aggregated multimodal fusion, suffering two significant limitations: (i) inefficient long-range temporal modeling, and (ii) sub-optimal multimodal fusion between intermodal fusion and intramodal processing. In this paper, we propose an audio-visual progressive fusion Mamba for multimodal depression detection, termed DepMamba. DepMamba features two core designs: hierarchical contextual modeling and progressive multimodal fusion. On the one hand, hierarchical modeling introduces convolution neural networks and Mamba to extract the local-to-global features within long-range sequences. On the other hand, the progressive fusion first presents a multimodal collaborative State Space Model (SSM) extracting intermodal and intramodal information for each modality, and then utilizes a multimodal enhanced SSM for modality cohesion. Extensive experimental results on two large-scale depression datasets demonstrate the superior performance of our DepMamba over existing state-of-the-art methods. Code is available at https://github.com/Jiaxin-Ye/DepMamba.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15936v1</guid>
      <category>cs.CY</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Ye, Junping Zhang, Hongming Shan</dc:creator>
    </item>
    <item>
      <title>GPT-4 as a Homework Tutor can Improve Student Engagement and Learning Outcomes</title>
      <link>https://arxiv.org/abs/2409.15981</link>
      <description>arXiv:2409.15981v1 Announce Type: new 
Abstract: This work contributes to the scarce empirical literature on LLM-based interactive homework in real-world educational settings and offers a practical, scalable solution for improving homework in schools. Homework is an important part of education in schools across the world, but in order to maximize benefit, it needs to be accompanied with feedback and followup questions. We developed a prompting strategy that enables GPT-4 to conduct interactive homework sessions for high-school students learning English as a second language. Our strategy requires minimal efforts in content preparation, one of the key challenges of alternatives like home tutors or ITSs. We carried out a Randomized Controlled Trial (RCT) in four high-school classes, replacing traditional homework with GPT-4 homework sessions for the treatment group. We observed significant improvements in learning outcomes, specifically a greater gain in grammar, and student engagement. In addition, students reported high levels of satisfaction with the system and wanted to continue using it after the end of the RCT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15981v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Vanzo, Sankalan Pal Chowdhury, Mrinmaya Sachan</dc:creator>
    </item>
    <item>
      <title>Custodial and Non-Custodial Wallets</title>
      <link>https://arxiv.org/abs/2409.15389</link>
      <description>arXiv:2409.15389v1 Announce Type: cross 
Abstract: Non-custodial wallets are a type of cryptocurrency wallet wherein the owner has full control over the private keys and is solely responsible for managing and securing the digital assets that it contains. Unlike custodial wallets, which are managed by third parties, such as exchanges, non-custodial wallets ensure that funds are controlled exclusively by the end user. We characterise the difference between custodial and non-custodial wallets and examine their key features and related risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15389v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Seymour, Geoff Goodell</dc:creator>
    </item>
    <item>
      <title>Uncovering Coordinated Cross-Platform Information Operations Threatening the Integrity of the 2024 U.S. Presidential Election Online Discussion</title>
      <link>https://arxiv.org/abs/2409.15402</link>
      <description>arXiv:2409.15402v1 Announce Type: cross 
Abstract: Information Operations (IOs) pose a significant threat to the integrity of democratic processes, with the potential to influence election-related online discourse. In anticipation of the 2024 U.S. presidential election, we present a study aimed at uncovering the digital traces of coordinated IOs on $\mathbb{X}$ (formerly Twitter). Using our machine learning framework for detecting online coordination, we analyze a dataset comprising election-related conversations on $\mathbb{X}$ from May 2024. This reveals a network of coordinated inauthentic actors, displaying notable similarities in their link-sharing behaviors. Our analysis shows concerted efforts by these accounts to disseminate misleading, redundant, and biased information across the Web through a coordinated cross-platform information operation: The links shared by this network frequently direct users to other social media platforms or suspicious websites featuring low-quality political content and, in turn, promoting the same $\mathbb{X}$ and YouTube accounts. Members of this network also shared deceptive images generated by AI, accompanied by language attacking political figures and symbolic imagery intended to convey power and dominance. While $\mathbb{X}$ has suspended a subset of these accounts, more than 75% of the coordinated network remains active. Our findings underscore the critical role of developing computational models to scale up the detection of threats on large social media platforms, and emphasize the broader implications of these techniques to detect IOs across the wider Web.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15402v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Minici, Luca Luceri, Federico Cinus, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>Vulnerabilities that arise from poor governance in Distributed Ledger Technologies</title>
      <link>https://arxiv.org/abs/2409.15947</link>
      <description>arXiv:2409.15947v1 Announce Type: cross 
Abstract: Current implementations of governance in Distributed Ledger Technologies leave them susceptible to a number of attacks. We survey the state of the art of Distributed Ledger Technologies (DLTs) governance protocols and work carried out to systematise good governance properties in the context of DLTs. We then select the most appropriate taxonomy of good governance properties and point to formal security notions that good governance protocols should satisfy. We point practitioners to existing solutions to deliver them, where possible. Furthermore, we outline a number of vulnerabilities that arise in the absence of good governance properties. We call on the research community and DLT research practitioners to prioritise delivering these good governance properties and continue to develop tools to do so, to avoid attacks to DLT protocols that exploit their poor governance models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15947v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aida Manzano Kharman, William Sanders</dc:creator>
    </item>
    <item>
      <title>From Pixels to Words: Leveraging Explainability in Face Recognition through Interactive Natural Language Processing</title>
      <link>https://arxiv.org/abs/2409.16089</link>
      <description>arXiv:2409.16089v1 Announce Type: cross 
Abstract: Face Recognition (FR) has advanced significantly with the development of deep learning, achieving high accuracy in several applications. However, the lack of interpretability of these systems raises concerns about their accountability, fairness, and reliability. In the present study, we propose an interactive framework to enhance the explainability of FR models by combining model-agnostic Explainable Artificial Intelligence (XAI) and Natural Language Processing (NLP) techniques. The proposed framework is able to accurately answer various questions of the user through an interactive chatbot. In particular, the explanations generated by our proposed method are in the form of natural language text and visual representations, which for example can describe how different facial regions contribute to the similarity measure between two faces. This is achieved through the automatic analysis of the output's saliency heatmaps of the face images and a BERT question-answering model, providing users with an interface that facilitates a comprehensive understanding of the FR decisions. The proposed approach is interactive, allowing the users to ask questions to get more precise information based on the user's background knowledge. More importantly, in contrast to previous studies, our solution does not decrease the face recognition performance. We demonstrate the effectiveness of the method through different experiments, highlighting its potential to make FR systems more interpretable and user-friendly, especially in sensitive applications where decision-making transparency is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16089v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>27th International Conference on Pattern Recognition Workshops (ICPRw 2024)</arxiv:journal_reference>
      <dc:creator>Ivan DeAndres-Tame, Muhammad Faisal, Ruben Tolosana, Rouqaiah Al-Refai, Ruben Vera-Rodriguez, Philipp Terh\"orst</dc:creator>
    </item>
    <item>
      <title>The Digital Transformation in Health: How AI Can Improve the Performance of Health Systems</title>
      <link>https://arxiv.org/abs/2409.16098</link>
      <description>arXiv:2409.16098v1 Announce Type: cross 
Abstract: Mobile health has the potential to revolutionize health care delivery and patient engagement. In this work, we discuss how integrating Artificial Intelligence into digital health applications-focused on supply chain, patient management, and capacity building, among other use cases-can improve the health system and public health performance. We present an Artificial Intelligence and Reinforcement Learning platform that allows the delivery of adaptive interventions whose impact can be optimized through experimentation and real-time monitoring. The system can integrate multiple data sources and digital health applications. The flexibility of this platform to connect to various mobile health applications and digital devices and send personalized recommendations based on past data and predictions can significantly improve the impact of digital tools on health system outcomes. The potential for resource-poor settings, where the impact of this approach on health outcomes could be more decisive, is discussed specifically. This framework is, however, similarly applicable to improving efficiency in health systems where scarcity is not an issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16098v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>\'Africa Peri\'a\~nez, Ana Fern\'andez del R\'io, Ivan Nazarov, Enric Jan\'e, Moiz Hassan, Aditya Rastogi, Dexian Tang</dc:creator>
    </item>
    <item>
      <title>Implicit assessment of language learning during practice as accurate as explicit testing</title>
      <link>https://arxiv.org/abs/2409.16133</link>
      <description>arXiv:2409.16133v1 Announce Type: cross 
Abstract: Assessment of proficiency of the learner is an essential part of Intelligent Tutoring Systems (ITS). We use Item Response Theory (IRT) in computer-aided language learning for assessment of student ability in two contexts: in test sessions, and in exercises during practice sessions. Exhaustive testing across a wide range of skills can provide a detailed picture of proficiency, but may be undesirable for a number of reasons. Therefore, we first aim to replace exhaustive tests with efficient but accurate adaptive tests. We use learner data collected from exhaustive tests under imperfect conditions, to train an IRT model to guide adaptive tests. Simulations and experiments with real learner data confirm that this approach is efficient and accurate. Second, we explore whether we can accurately estimate learner ability directly from the context of practice with exercises, without testing. We transform learner data collected from exercise sessions into a form that can be used for IRT modeling. This is done by linking the exercises to {\em linguistic constructs}; the constructs are then treated as "items" within IRT. We present results from large-scale studies with thousands of learners. Using teacher assessments of student ability as "ground truth," we compare the estimates obtained from tests vs. those from exercises. The experiments confirm that the IRT models can produce accurate ability estimation based on exercises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16133v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber</dc:creator>
    </item>
    <item>
      <title>Metamorphic Debugging for Accountable Software</title>
      <link>https://arxiv.org/abs/2409.16140</link>
      <description>arXiv:2409.16140v1 Announce Type: cross 
Abstract: As the laws have become more complicated and enormous, the role of software systems in navigating and understanding these intricacies has become more critical. Given their socio-economic and legally critical implications, ensuring software accountability -- encompassing qualities such as legal compliance, explainability, perceptions of procedural justice, fairness of outcomes, and confidentiality/privacy -- is of paramount social importance. Moreover, software that accurately interprets its requirements, complies with legal standards and upholds social fairness can serve as a surrogate for legal and social norms, enabling policymakers to inquire about the law as seamlessly as a software engineer conducts a test. However, ensuring software accountability faces three key challenges: i) Translating legalese into formal specifications, ii) Lack of a definitive 'truth' for queries (the oracle problem), and iii) Scarcity of trustworthy datasets due to privacy and legal concerns.
  Drawing from the experiences in debugging U.S. tax preparation software, we propose that these challenges can be tackled by focusing on relational specifications. While the exact output for a given input may be unknown, the relationship between the outputs of two related inputs may be easier to express. This observation resembles i) the legal doctrine of precedent, meaning that similar cases must yield similar rulings; and ii) metamorphic relation (MR) in software engineering that requires a specific relation between software inputs and outputs. We propose metamorphic debugging as the foundation for detecting, explaining, and repairing socio-legal software for these relations. We showcase recent results that leverage metamorphic debugging to detect and explain accountability bugs in tax prep and poverty management software systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16140v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Tizpaz-Niari, Shiva Darian, Ashutosh Trivedi</dc:creator>
    </item>
    <item>
      <title>LLM Echo Chamber: personalized and automated disinformation</title>
      <link>https://arxiv.org/abs/2409.16241</link>
      <description>arXiv:2409.16241v1 Announce Type: cross 
Abstract: Recent advancements have showcased the capabilities of Large Language Models like GPT4 and Llama2 in tasks such as summarization, translation, and content review. However, their widespread use raises concerns, particularly around the potential for LLMs to spread persuasive, humanlike misinformation at scale, which could significantly influence public opinion. This study examines these risks, focusing on LLMs ability to propagate misinformation as factual. To investigate this, we built the LLM Echo Chamber, a controlled digital environment simulating social media chatrooms, where misinformation often spreads. Echo chambers, where individuals only interact with like minded people, further entrench beliefs. By studying malicious bots spreading misinformation in this environment, we can better understand this phenomenon. We reviewed current LLMs, explored misinformation risks, and applied sota finetuning techniques. Using Microsoft phi2 model, finetuned with our custom dataset, we generated harmful content to create the Echo Chamber. This setup, evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the ethical concerns surrounding LLMs and emphasizes the need for stronger safeguards against misinformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16241v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Ma</dc:creator>
    </item>
    <item>
      <title>Fairness and Bias in Algorithmic Hiring: a Multidisciplinary Survey</title>
      <link>https://arxiv.org/abs/2309.13933</link>
      <description>arXiv:2309.13933v3 Announce Type: replace 
Abstract: Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13933v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696457</arxiv:DOI>
      <dc:creator>Alessandro Fabris, Nina Baranowska, Matthew J. Dennis, David Graus, Philipp Hacker, Jorge Saldivar, Frederik Zuiderveen Borgesius, Asia J. Biega</dc:creator>
    </item>
    <item>
      <title>"Flipped" University: LLM-Assisted Lifelong Learning Environment</title>
      <link>https://arxiv.org/abs/2409.10553</link>
      <description>arXiv:2409.10553v2 Announce Type: replace 
Abstract: The rapid development of artificial intelligence technologies, particularly Large Language Models (LLMs), has revolutionized the landscape of lifelong learning. This paper introduces a conceptual framework for a self-constructed lifelong learning environment supported by LLMs. It highlights the inadequacies of traditional education systems in keeping pace with the rapid deactualization of knowledge and skills. The proposed framework emphasizes the transformation from institutionalized education to personalized, self-driven learning. It leverages the natural language capabilities of LLMs to provide dynamic and adaptive learning experiences, facilitating the creation of personal intellectual agents that assist in knowledge acquisition. The framework integrates principles of lifelong learning, including the necessity of building personal world models, the dual modes of learning (training and exploration), and the creation of reusable learning artifacts. Additionally, it underscores the importance of curiosity-driven learning and reflective practices in maintaining an effective learning trajectory. The paper envisions the evolution of educational institutions into "flipped" universities, focusing on supporting global knowledge consistency rather than merely structuring and transmitting knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10553v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Krinkin, Tatiana Berlenko</dc:creator>
    </item>
    <item>
      <title>Undergrads Are All You Have</title>
      <link>https://arxiv.org/abs/2409.13750</link>
      <description>arXiv:2409.13750v2 Announce Type: replace 
Abstract: The outsourcing of busy work and other research-related tasks to undergraduate students is a time-honored academic tradition. In recent years, these tasks have been given to Lama-based large-language models such as Alpaca and Llama increasingly often, putting poor undergraduate students out of work. Due to the costs associated with importing and caring for South American Camelidae, researcher James Yoo set out to find a cheaper and more effective alternative to these models. The findings, published in the highly-respected journal, SIGBOVIK, demonstrates that their model, GPT-UGRD is on par with, and in some cases better, than Lama models for natural language processing tasks. The paper also demonstrates that GPT-UGRD is cheaper and easier to train and operate than transformer models. In this paper, we outline the implementation, application, multi-tenanting, and social implications of using this new model in research and other contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13750v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ashe Neth</dc:creator>
    </item>
    <item>
      <title>FairBranch: Mitigating Bias Transfer in Fair Multi-task Learning</title>
      <link>https://arxiv.org/abs/2310.13746</link>
      <description>arXiv:2310.13746v2 Announce Type: replace-cross 
Abstract: The generalisation capacity of Multi-Task Learning (MTL) suffers when unrelated tasks negatively impact each other by updating shared parameters with conflicting gradients. This is known as negative transfer and leads to a drop in MTL accuracy compared to single-task learning (STL). Lately, there has been a growing focus on the fairness of MTL models, requiring the optimization of both accuracy and fairness for individual tasks. Analogously to negative transfer for accuracy, task-specific fairness considerations might adversely affect the fairness of other tasks when there is a conflict of fairness loss gradients between the jointly learned tasks - we refer to this as Bias Transfer. To address both negative- and bias-transfer in MTL, we propose a novel method called FairBranch, which branches the MTL model by assessing the similarity of learned parameters, thereby grouping related tasks to alleviate negative transfer. Moreover, it incorporates fairness loss gradient conflict correction between adjoining task-group branches to address bias transfer within these task groups. Our experiments on tabular and visual MTL problems show that FairBranch outperforms state-of-the-art MTLs on both fairness and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13746v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arjun Roy, Christos Koutlis, Symeon Papadopoulos, Eirini Ntoutsi</dc:creator>
    </item>
    <item>
      <title>A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services</title>
      <link>https://arxiv.org/abs/2403.15780</link>
      <description>arXiv:2403.15780v2 Announce Type: replace-cross 
Abstract: As Machine Learning grows in popularity across various fields, equity has become a key focus for the AI community. However fairness-oriented approaches are still underexplored in smart mobility. Addressing this gap, our study investigates the balance between performance optimization and algorithmic fairness in shared micromobility services providing a novel framework based on Reinforcement Learning. Exploiting Q-Learning, the proposed methodology achieves equitable outcomes in terms of the Gini index across different areas characterized by their distance from central hubs. Through vehicle rebalancing, the provided scheme maximizes operator performance while ensuring fairness principles for users, reducing iniquity by up to 80% while only increasing costs by 30% (w.r.t. applying no equity adjustment). A case study with synthetic data validates our insights and highlights the importance of fairness in urban micromobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15780v2</guid>
      <category>eess.SY</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Cederle, Luca Vittorio Piron, Marina Ceccon, Federico Chiariotti, Alessandro Fabris, Marco Fabris, Gian Antonio Susto</dc:creator>
    </item>
    <item>
      <title>Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations</title>
      <link>https://arxiv.org/abs/2409.09947</link>
      <description>arXiv:2409.09947v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show promise as a writing aid for professionals performing legal analyses. However, LLMs can often hallucinate in this setting, in ways difficult to recognize by non-professionals and existing text evaluation metrics. In this work, we pose the question: when can machine-generated legal analysis be evaluated as acceptable? We introduce the neutral notion of gaps, as opposed to hallucinations in a strict erroneous sense, to refer to the difference between human-written and machine-generated legal analysis. Gaps do not always equate to invalid generation. Working with legal experts, we consider the CLERC generation task proposed in Hou et al. (2024b), leading to a taxonomy, a fine-grained detector for predicting gap categories, and an annotated dataset for automatic evaluation. Our best detector achieves 67% F1 score and 80% precision on the test set. Employing this detector as an automated metric on legal analysis generated by SOTA LLMs, we find around 80% contain hallucinations of different kinds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09947v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abe Bohan Hou, William Jurayj, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme</dc:creator>
    </item>
    <item>
      <title>MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2409.13935</link>
      <description>arXiv:2409.13935v2 Announce Type: replace-cross 
Abstract: This study explores the effectiveness of Large Language Models (LLMs) in creating personalized "mirror stories" that reflect and resonate with individual readers' identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13935v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarfaroz Yunusov, Hamza Sidat, Ali Emami</dc:creator>
    </item>
  </channel>
</rss>
