<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 04:01:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the optimization of discrepancy measures</title>
      <link>https://arxiv.org/abs/2508.04926</link>
      <description>arXiv:2508.04926v1 Announce Type: new 
Abstract: Points in the unit cube with low discrepancy can be constructed using algebra or, more recently, by direct computational optimization of a criterion. The usual $L_\infty$ star discrepancy is a poor criterion for this because it is computationally expensive and lacks differentiability. Its usual replacement, the $L_2$ star discrepancy, is smooth but exhibits other pathologies shown by J. Matou\v{s}ek. In an attempt to address these problems, we introduce the \textit{average squared discrepancy} which averages over $2^d$ versions of the $L_2$ star discrepancy anchored in the different vertices of $[0,1]^d$. Not only can this criterion be computed in $O(dn^2)$ time, like the $L_2$ star discrepancy, but also we show that it is equivalent to a weighted symmetric $L_2$ criterion of Hickernell's by a constant factor. We compare this criterion with a wide range of traditional discrepancy measures, and show that only the average squared discrepancy avoids the problems raised by Matou\v{s}ek. Furthermore, we present a comprehensive numerical study showing in particular that optimizing for the average squared discrepancy leads to strong performance for the $L_2$ star discrepancy, whereas the converse does not hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04926v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement, Nathan Kirk, Art B. Owen, T. Konstantin Rusch</dc:creator>
    </item>
    <item>
      <title>Toroidal area-preserving parameterizations of genus-one closed surfaces</title>
      <link>https://arxiv.org/abs/2508.05111</link>
      <description>arXiv:2508.05111v1 Announce Type: new 
Abstract: We consider the problem of computing toroidal area-preserving parameterizations of genus-one closed surfaces. We propose four algorithms based on Riemannian geometry: the projected gradient descent method, the projected conjugate gradient method, the Riemannian gradient method, and the Riemannian conjugate gradient method. Our objective function is based on the stretch energy functional, and the minimization is constrained on a power manifold of ring tori embedded in three-dimensional Euclidean space. Numerical experiments on several mesh models demonstrate the effectiveness of the proposed framework. Finally, we show how to use the proposed algorithms in the context of surface registration and texture mapping applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05111v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Sutti, Mei-Heng Yueh</dc:creator>
    </item>
    <item>
      <title>An asymptotic-preserving active flux scheme for the hyperbolic heat equation in the diffusive scaling</title>
      <link>https://arxiv.org/abs/2508.05166</link>
      <description>arXiv:2508.05166v1 Announce Type: new 
Abstract: The Active Flux (AF) method is a compact, high-order finite volume scheme that enhances flexibility by introducing point values at cell interfaces as additional degrees of freedom alongside cell averages. The method of lines is employed here for temporal discretization. A common approach for updating point values relies on the Jacobian Splitting (JS) method, which incorporates upwinding. A key advantage of the AF method over standard finite volume schemes is its structure-preserving property, motivating the investigation of its asymptotic-preserving (AP) behavior in the diffusive scaling. We show that the JS-based AF method without any modification is AP for solving the hyperbolic heat equation, in the sense that the limit scheme is a discretization of the limit heat equation. We use formal asymptotic analysis, discrete Fourier analysis, and numerical experiments to illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05166v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junming Duan, Wasilij Barsukow, Christian Klingenberg</dc:creator>
    </item>
    <item>
      <title>An Investigation into the Distribution of Ratios of Particle Solver-based Likelihoods</title>
      <link>https://arxiv.org/abs/2508.05303</link>
      <description>arXiv:2508.05303v1 Announce Type: new 
Abstract: We investigate the use of the Metropolis-Hastings algorithm to sample posterior distribution in a Bayesian inverse problem, where the likelihood function is random. Concretely, we consider the case where one has full field observations of a PDE solution, in case a one-dimensional diffusion equation, subject to a Gaussian observation error. Assuming one uses a particle-based Monte Carlo simulation when approximating the likelihood function, one gets an approximate likelihood with additive Gaussian noise in the log-likelihood. We study how these two Gaussian distributions affect the distribution of ratios of approximate likelihood evaluations, as required when evaluating acceptance probabilities in the Metropolis-Hastings algorithm. We do so through both theoretical analysis and numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05303v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emil L{\o}vbak, Sebastian Krumscheid</dc:creator>
    </item>
    <item>
      <title>A low-rank solver for the Stokes-Darcy model with random hydraulic conductivity and Beavers-Joseph condition</title>
      <link>https://arxiv.org/abs/2508.05328</link>
      <description>arXiv:2508.05328v1 Announce Type: new 
Abstract: This paper proposes, analyzes, and demonstrates an efficient low-rank solver for the stochastic Stokes-Darcy interface model with a random hydraulic conductivity both in the porous media domain and on the interface. We consider three interface conditions with randomness, including the Beavers-Joseph interface condition with the random hydraulic conductivity, on the interface between the free flow and the porous media flow. Our solver employs a novel generalized low-rank approximation of the large-scale stiffness matrices, which can significantly cut down the computational costs and memory requirements associated with matrix inversion without losing accuracy. Therefore, by adopting a suitable data compression ratio, the low-rank solver can maintain a high numerical precision with relatively low computational and space complexities. We also propose a strategy to determine the best choice of data compression ratios. Furthermore, we carry out the error analysis of the generalized low-rank matrix approximation algorithm and the low-rank solver. Finally, numerical experiments are conducted to validate the proposed algorithms and the theoretical conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05328v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujun Zhu, Yulan Ning, Zhipeng Yang, Xiaoming He, Ju Ming</dc:creator>
    </item>
    <item>
      <title>The domain-of-dependence stabilization for cut-cell meshes is fully discretely stable</title>
      <link>https://arxiv.org/abs/2508.05372</link>
      <description>arXiv:2508.05372v1 Announce Type: new 
Abstract: We present a fully discrete stability analysis of the domain-of-dependence stabilization for hyperbolic problems. The method aims to address issues caused by small cut cells by redistributing mass around the neighborhood of a small cut cell at a semi-discrete level. Our analysis is conducted for the linear advection model problem in one spatial dimension. We demonstrate that fully discrete stability can be achieved under a time step restriction that does not depend on the arbitrarily small cells, using an operator norm estimate. Additionally, this analysis offers a detailed understanding of the stability mechanism and highlights some challenges associated with higher-order polynomials. We also propose a way to mitigate these issues to derive a feasible CFL-like condition. The analytical findings, as well as the proposed solution are verified numerically in one- and two-dimensional simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05372v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Petri, Gunnar Birke, Christian Engwer, Hendrik Ranocha</dc:creator>
    </item>
    <item>
      <title>Inverse inequalities for kernel-based approximation on bounded domains and Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2508.05376</link>
      <description>arXiv:2508.05376v1 Announce Type: new 
Abstract: This paper establishes inverse inequalities for kernel-based approximation spaces defined on bounded Lipschitz domains in $\mathbb{R}^d$ and compact Riemannian manifolds. While inverse inequalities are well-studied for polynomial spaces, their extension to kernel-based trial spaces poses significant challenges. For bounded Lipschitz domains, we extend prior Bernstein inequalities, which only apply to a limited range of Sobolev orders, to all orders on the lower bound and $L_2$ on the upper, and derive Nikolskii inequalities that bound $L_\infty$ norms by $L_2$ norms. Our theory achieves the desired form but may require slightly more smoothness on the kernel than the regular $&gt;d/2$ assumption. For compact Riemannian manifolds, we focus on restricted kernels, which are defined as the restriction of positive definite kernels from the ambient Euclidean space to the manifold, and prove their counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05376v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengjie Sun, Leevan Ling</dc:creator>
    </item>
    <item>
      <title>Randomized Krylov-Schur eigensolver with deflation</title>
      <link>https://arxiv.org/abs/2508.05400</link>
      <description>arXiv:2508.05400v1 Announce Type: new 
Abstract: This work introduces a novel algorithm to solve large-scale eigenvalue problems and seek a small set of eigenpairs. The method, called randomized Krylov-Schur (rKS), has a simple implementation and benefits from fast and efficient operations in low-dimensional spaces, such as sketch-orthogonalization processes and stable reordering of Schur factorizations. It also includes a practical deflation technique for converged eigenpairs, enabling the computation of the eigenspace associated with a given part of the spectrum. Numerical experiments are provided to demonstrate the scalability and accuracy of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05400v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Guillaume de Damas, Laura Grigori</dc:creator>
    </item>
    <item>
      <title>A unified framework for the analysis, numerical approximation and model reduction of linear operator equations, Part I: Well-posedness in space and time</title>
      <link>https://arxiv.org/abs/2508.05407</link>
      <description>arXiv:2508.05407v1 Announce Type: new 
Abstract: We present a unified framework to construct well-posed formulations for large classes of linear operator equations including elliptic, parabolic and hyperbolic partial differential equations. This general approach incorporates known weak variational formulations as well as novel space-time variational forms of the hyperbolic wave equation. The main concept is completion and extension of operators starting from the strong form of the problem.
  This paper lays the theoretical foundation for a unified approach towards numerical approximation methods and also model reduction of parameterized linear operator equations which will be the subject of the following parts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05407v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Feuerle, Richard L\"oscher, Olaf Steinbach, Karsten Urban</dc:creator>
    </item>
    <item>
      <title>Learning Geometric-Aware Quadrature Rules for Functional Minimization</title>
      <link>https://arxiv.org/abs/2508.05445</link>
      <description>arXiv:2508.05445v1 Announce Type: new 
Abstract: Accurate numerical integration over non-uniform point clouds is a challenge for modern mesh-free machine learning solvers for partial differential equations (PDEs) using variational principles. While standard Monte Carlo (MC) methods are not capable of handling a non-uniform point cloud, modern neural network architectures can deal with permutation-invariant inputs, creating quadrature rules for any point cloud. In this work, we introduce QuadrANN, a Graph Neural Network (GNN) architecture designed to learn optimal quadrature weights directly from the underlying geometry of point clouds. The design of the model exploits a deep message-passing scheme where the initial layer encodes rich local geometric features from absolute and relative positions as well as an explicit local density measure. In contrast, the following layers incorporate a global context vector. These architectural choices allow the QuadrANN to generate a data-driven quadrature rule that is permutation-invariant and adaptive to both local point density and the overall domain shape. We test our methodology on a series of challenging test cases, including integration on convex and non-convex domains and estimating the solution of the Heat and Fokker-Planck equations. Across all the tests, QuadrANN reduces the variance of the integral estimation compared to standard Quasi-Monte Carlo methods by warping the point clouds to be more dense in critical areas where the integrands present certain singularities. This enhanced stability in critical areas of the domain at hand is critical for the optimization of energy functionals, leading to improved deep learning-based variational solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05445v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Costas Smaragdakis</dc:creator>
    </item>
    <item>
      <title>Numerical analysis of the stochastic Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2508.05564</link>
      <description>arXiv:2508.05564v1 Announce Type: new 
Abstract: The developments over the last five decades concerning numerical discretisations of the incompressible Navier--Stokes equations have lead to reliable tools for their approximation: those include stable methods to properly address the incompressibility constraint, stable discretisations to account for convection dominated problems, efficient time (splitting) methods, and methods to tackle their nonlinear character. While these tools may successfully be applied to reliably simulate even more complex fluid flow PDE models, their understanding requires a fundamental revision in the case of stochastic fluid models, which are gaining increased importance nowadays.
  This work motivates and surveys optimally convergent numerical methods for the stochastic Stokes and Navier--Stokes equations that were obtained in the last decades. Furtheremore, we computationally illustrate the failure of some of those methods from the deterministic setting, if they are straight-forwardly applied to the stochastic case. In fact, we explain why some of these deterministic methods perform sub-optimally by highlighting crucial analytical differences between the deterministic and stochastic equations -- and how modifications of the deterministic methods restore their optimal performance if they properly address the probabilistic nature of the stochastic problem.
  Next to the numerical analysis of schemes, we propose a general benchmark of prototypic fluid flow problems driven by different types of noise to also compare new algorithms by simulations in terms of complexities, efficiencies, and possible limitations. The driving motivation is to reach a better comparison of simulations for new schemes in terms of accuracy and complexities, and to also complement theoretical performance studies for restricted settings of data by more realistic ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05564v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Breit, Andreas Prohl, J\"orn Wichman</dc:creator>
    </item>
    <item>
      <title>Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos</title>
      <link>https://arxiv.org/abs/2508.04853</link>
      <description>arXiv:2508.04853v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the memory and compute costs of modern deep neural networks, including large language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as GPTQ-has emerged as a leading method due to its computational efficiency and strong empirical performance. Despite its widespread adoption, however, OPTQ lacks rigorous quantitative theoretical guarantees. This paper presents the first quantitative error bounds for both deterministic and stochastic variants of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ algorithm. We analyze how OPTQ's iterative procedure induces quantization error and derive non-asymptotic 2-norm error bounds that depend explicitly on the calibration data and a regularization parameter that OPTQ uses. Our analysis provides theoretical justification for several practical design choices, including the widely used heuristic of ordering features by decreasing norm, as well as guidance for selecting the regularization parameter. For the stochastic variant, we establish stronger infinity-norm error bounds, which enable control over the required quantization alphabet and are particularly useful for downstream layers and nonlinearities. Finally, we extend our analysis to Qronos, providing new theoretical bounds, for both its deterministic and stochastic variants, that help explain its empirical advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04853v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Zhang, Shihao Zhang, Ian Colbert, Rayan Saab</dc:creator>
    </item>
    <item>
      <title>Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition</title>
      <link>https://arxiv.org/abs/2508.04917</link>
      <description>arXiv:2508.04917v1 Announce Type: cross 
Abstract: Sparse linear systems are typically solved using preconditioned iterative methods, but applying preconditioners via sparse triangular solves introduces bottlenecks due to irregular memory accesses and data dependencies. This work leverages fine-grained domain decomposition to adapt triangular solves to the GPU architecture. We develop a fine-grained domain decomposition strategy that generates non-overlapping subdomains, increasing parallelism in the application of preconditioner at the expense of a modest increase in the iteration count for convergence. Each subdomain is assigned to a thread block and is sized such that the subdomain vector fits in the GPU shared memory, eliminating the need for inter-block synchronization and reducing irregular global memory accesses. Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$ software stack, we achieve a 10.7$\times$ speedup for triangular solves and a 3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized (BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04917v1</guid>
      <category>cs.PF</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atharva Gondhalekar, Kjetil Haugen, Thomas Gibson, Wu-chun Feng</dc:creator>
    </item>
    <item>
      <title>Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms</title>
      <link>https://arxiv.org/abs/2508.05141</link>
      <description>arXiv:2508.05141v1 Announce Type: cross 
Abstract: This paper establishes a comprehensive approximation result for deep fully-connected neural networks with commonly-used and general activation functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the $W^{m,p}$-norm for $m &lt; n$ and $1\le p \le \infty$. The derived rates surpass those of classical numerical approximation techniques, such as finite element and spectral methods, exhibiting a phenomenon we refer to as \emph{super-convergence}. Our analysis shows that deep networks with general activations can approximate weak solutions of partial differential equations (PDEs) with superior accuracy compared to traditional numerical methods at the approximation level. Furthermore, this work closes a significant gap in the error-estimation theory for neural-network-based approaches to PDEs, offering a unified theoretical foundation for their use in scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05141v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yahong Yang, Juncai He</dc:creator>
    </item>
    <item>
      <title>Modulation of the Monokinetic Limit for Models of Collective Dynamics</title>
      <link>https://arxiv.org/abs/2508.05478</link>
      <description>arXiv:2508.05478v1 Announce Type: cross 
Abstract: In this work, we perform modulation analysis of monokinetic limits from the kinetic Cucker- Smale model to the pressureless Euler alignment system. Two regimes are considered -- a strong Fokker- Planck force with vanishing noise and Knudsen number, and a pure noiseless Vlasov scheme. In the former case, we demonstrate convergence of the modulated profile to the standard Gaussian distribution, while in the latter case, the distribution converges to a profile satisfying an explicit transport equation along limiting characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05478v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alina Chertock, Roman Shvydkoy, Trevor Teolis</dc:creator>
    </item>
    <item>
      <title>Non-degenerate Rigid Alignment in a Patch Framework</title>
      <link>https://arxiv.org/abs/2303.11620</link>
      <description>arXiv:2303.11620v4 Announce Type: replace 
Abstract: Given a set of overlapping local views (patches) of a dataset, we consider the problem of finding a rigid alignment of the views that minimizes a $2$-norm based alignment error. In general, the views are noisy and a perfect alignment may not exist. In this work, we characterize the non-degeneracy of an alignment in the noisy setting based on the kernel and positivity of a certain matrix. This leads to a polynomial time algorithm for testing the non-degeneracy of a given alignment. Subsequently, we focus on Riemannian gradient descent for minimizing the alignment error, providing a sufficient condition on an alignment for the algorithm to converge (locally) linearly to it. \revadd{Additionally, we provide an exact recovery and noise stability analysis of the algorithm}. In the case of noiseless views, a perfect alignment exists, resulting in a realization of the points that respects the geometry of the views. Under a mild condition on the views, we show that a non-degenerate perfect alignment \revadd{characterizes the infinitesimally rigidity of a realization, and thus the local rigidity of a generic realization}. By specializing the non-degeneracy conditions to the noiseless case, we derive necessary and sufficient conditions on the overlapping structure of the views for \revadd{a perfect alignment to be non-degenerate and equivalently, for the resulting realization to be infinitesimally rigid}. Similar results are also derived regarding the uniqueness of a perfect alignment and global rigidity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11620v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Kohli, Gal Mishne, Alexander Cloninger</dc:creator>
    </item>
    <item>
      <title>A convergence analysis of Lawson's iteration for computing polynomial and rational minimax approximations</title>
      <link>https://arxiv.org/abs/2401.00778</link>
      <description>arXiv:2401.00778v3 Announce Type: replace 
Abstract: Lawson's iteration is a classical and effective method for solving the linear (polynomial) minimax approximation problem in the complex plane. Extension of Lawson's iteration for the rational minimax approximation problem with both computationally high efficiency and theoretical guarantee is challenging. A recent work [L.-H. Zhang, L. Yang, W. H. Yang and Y.-N. Zhang, A convex dual problem for the rational minimax approximation and Lawson's iteration, Math. Comp., 94(2025), 2457-2494.] reveals that Lawson's iteration can be viewed as a method for solving the dual problem of the original rational minimax approximation problem, and a new type of Lawson's iteration, namely, d-Lawson, was proposed, which reduces to the classical Lawson's iteration for the linear minimax approximation problem. For the rational case, such a dual problem is guaranteed to obtain the original minimax solution under Ruttan's sufficient condition, and numerically, d-Lawson was observed to converge monotonically with respect to the dual objective function. In this paper, we present a theoretical convergence analysis of d-Lawson for both the linear and rational minimax approximation problems. In particular, we show that (i) for the linear minimax approximation problem, $\beta=1$ is a near-optimal Lawson exponent in Lawson's iteration, and (ii) for the rational minimax approximation problem, under certain conditions, d-Lawson converges monotonically with respect to the dual objective function for any sufficiently small $\beta&gt;0$, and the limiting approximant satisfies the complementary slackness condition: any node associated with positive weight either is an interpolation point or has a constant error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00778v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei-Hong Zhang, Shanheng Han</dc:creator>
    </item>
    <item>
      <title>Robustness of data-driven approaches in limited angle tomography</title>
      <link>https://arxiv.org/abs/2403.11350</link>
      <description>arXiv:2403.11350v3 Announce Type: replace 
Abstract: The limited angle Radon transform is notoriously difficult to invert due to its ill-posedness. In this work, we give a mathematical explanation that data-driven approaches can stably reconstruct more information compared to traditional methods like filtered backprojection. In addition, we use experiments based on the U-Net neural network to validate our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11350v3</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiran Wang, Yimin Zhong</dc:creator>
    </item>
    <item>
      <title>Polytopal mesh agglomeration via geometrical deep learning for three-dimensional heterogeneous domains</title>
      <link>https://arxiv.org/abs/2406.10587</link>
      <description>arXiv:2406.10587v2 Announce Type: replace 
Abstract: Agglomeration techniques can be successfully employed to reduce the computational costs of numerical simulations and stand at the basis of multilevel algebraic solvers. To automatically perform mesh agglomeration, we propose a novel Geometrical Deep Learning-based algorithm that can exploit the geometrical and physical information of the underlying computational domain to construct the agglomerated grid and -- simultaneously -- guarantee the agglomerated grid's quality. In particular, we propose a bisection model based on Graph Neural Networks (GNNs) to partition a suitable connectivity graph of computational three-dimensional meshes. The new approach has a high online inference speed. It can simultaneously process the graph structure of the mesh, the geometrical information of the mesh (e.g., elements' volumes, centers' coordinates), and the physical information of the domain (e.g., physical parameters). Taking advantage of this new approach, our algorithm can agglomerate meshes of a domain composed of heterogeneous media, automatically respecting the underlying heterogeneities. The proposed GNN approach is compared with the k-means algorithm and METIS, which are widely employed approaches for graph partitioning and are meant to process only the connectivity information on the mesh. We demonstrate that the performance of our algorithms outperforms the k-means and METIS algorithms in terms of quality metrics and runtimes. Moreover, we demonstrate that our algorithm also shows a good level of generalization when applied to complex geometries, such as three-dimensional geometries reconstructed from medical images. Finally, the model's capability to perform agglomeration in heterogeneous domains is evaluated when integrated into a polytopal discontinuous Galerkin finite element solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10587v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paola F. Antonietti, Mattia Corti, Gabriele Martinelli</dc:creator>
    </item>
    <item>
      <title>Acceleration of convergence in approximate solutions of Urysohn integral equations with Green's kernels</title>
      <link>https://arxiv.org/abs/2409.01784</link>
      <description>arXiv:2409.01784v2 Announce Type: replace 
Abstract: Consider a non-linear operator equation $x - K(x) = f$, where $f$ is a given function and $K$ is a Urysohn integral operator with Green's function type kernel defined on $L^\infty [0, 1]$. We apply approximation methods based on interpolatory projections onto the approximating space $\mathcal{X}_n$, which is the space of piecewise polynomials of even degree with respect to a uniform partition of $[0, 1]$. The approximate solutions obtained from these methods demonstrate enhanced accuracy compared to the classical collocation solution for the same equation. Numerical examples are given to support our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01784v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2025.07.044</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, 240: 681-697 (2026)</arxiv:journal_reference>
      <dc:creator>Shashank K. Shukla, Gobinda Rakshit</dc:creator>
    </item>
    <item>
      <title>A simple linear convergence analysis of the randomized reshuffling Kaczmarz method</title>
      <link>https://arxiv.org/abs/2410.01140</link>
      <description>arXiv:2410.01140v3 Announce Type: replace 
Abstract: The random reshuffling Kaczmarz (RRK) method enjoys the simplicity and efficiency in solving linear systems as a Kaczmarz-type method, whereas it also inherits the practical improvements of the stochastic gradient descent (SGD) with random reshuffling (RR) over original SGD. However, the current studies on RRK do not characterize its convergence comprehensively. In this paper, we present a novel analysis of the RRK method and prove its linear convergence towards the unique least-norm solution of the linear system. Furthermore, the convergence upper bound is tight and does not depend on the dimension of the coefficient matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01140v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deren Han, Jiaxin Xie</dc:creator>
    </item>
    <item>
      <title>On the choice of optimization norm for Anderson acceleration of the Picard iteration for Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2505.07650</link>
      <description>arXiv:2505.07650v2 Announce Type: replace 
Abstract: Recently developed convergence theory for Anderson acceleration (AA) assumes that the AA optimization norm matches the norm of the Hilbert space that the fixed point function is defined on. While this seems a natural assumption, it may not be the optimal choice in terms of convergence of the iteration or computational efficiency. For the Picard iteration for the Navier-Stokes equations (NSE), the associated Hilbert space norm is $H^1_0(\Omega),$ which is inefficient to implement in a large scale HPC setting since it requires multiplication of global coefficient vectors by the stiffness matrix. Motivated by recent numerical tests that show using the $\ell^2$ norm produces similar convergence behavior as $H^1_0$ does, we revisit the convergence theory of [Pollock et al, {\it SINUM} 2019] and find that i) it can be improved with a sharper treatment of the nonlinear terms; and ii) in the case that the AA optimization norm is changed to $L^2$ (and by extension $\ell^2$ or $L^2$ using a diagonally lumped mass matrix), a new convergence theory is developed that provides an essentially equivalent estimate as the $H^1_0$ case. Several numerical tests illustrate the new theory, and the theory and tests reveal that one can interchangeably use the norms $H^1_0$, $L^2$, $\ell^2$ or $L^2$ with diagonally lumped mass matrix for the AA optimization problem without significantly affecting the overall convergence behavior. Thus, one is justified to use $\ell^2$ or diagonally lumped $L^2$ for the AA optimization norm in Anderson accelerated Picard iterations for large scale NSE problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07650v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabeth Hawkins, Leo Rebholz</dc:creator>
    </item>
    <item>
      <title>On a Continuum Model for Random Genetic Drift: A Dynamic Boundary Condition Approach</title>
      <link>https://arxiv.org/abs/2309.09484</link>
      <description>arXiv:2309.09484v3 Announce Type: replace-cross 
Abstract: We propose a new continuum model for random genetic drift by employing a dynamic boundary condition approach. The model can be viewed as a regularized version of the Kimura equation and admits a continuous solution. We establish the existence and uniqueness of a strong solution to the regularized system. Numerical experiments illustrate that, for sufficiently small regularization parameters, the model can capture key phenomena of the original Kimura equation, such as gene fixation and conservation of the first moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09484v3</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chun Liu, Jan-Eric Sulzbach, Yiwei Wang</dc:creator>
    </item>
    <item>
      <title>A Structure-Preserving Framework for Solving Parabolic Partial Differential Equations with Neural Networks</title>
      <link>https://arxiv.org/abs/2504.10273</link>
      <description>arXiv:2504.10273v2 Announce Type: replace-cross 
Abstract: Solving partial differential equations (PDEs) with neural networks (NNs) has shown great potential in various scientific and engineering fields. However, most existing NN solvers mainly focus on satisfying the given PDE formulas in the strong or weak sense, without explicitly considering some intrinsic physical properties, such as mass and momentum conservation, or energy dissipation. This limitation may result in nonphysical or unstable numerical solutions, particularly in long-term simulations. To address this issue, we propose ``Sidecar'', a novel framework that enhances the physical consistency of existing NN solvers for solving parabolic PDEs. Inspired by the time-dependent spectral renormalization approach, our Sidecar framework introduces a small network as a copilot, guiding the primary function-learning NN solver to respect the structure-preserving properties. Our framework is highly flexible, allowing the preservation of various physical quantities for different PDEs to be incorporated into a wide range of NN solvers. Experimental results on some benchmark problems demonstrate significant improvements brought by the proposed framework to both accuracy and structure preservation of existing NN solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10273v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaohang Chen, Lili Ju, Zhonghua Qiao</dc:creator>
    </item>
  </channel>
</rss>
