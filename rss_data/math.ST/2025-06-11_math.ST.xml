<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 04:03:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Identifiability and Estimation in High-Dimensional Nonparametric Latent Structure Models</title>
      <link>https://arxiv.org/abs/2506.09165</link>
      <description>arXiv:2506.09165v1 Announce Type: new 
Abstract: This paper studies the problems of identifiability and estimation in high-dimensional nonparametric latent structure models. We introduce an identifiability theorem that generalizes existing conditions, establishing a unified framework applicable to diverse statistical settings. Our results rigorously demonstrate how increased dimensionality, coupled with diversity in variables, inherently facilitates identifiability. For the estimation problem, we establish near-optimal minimax rate bounds for the high-dimensional nonparametric density estimation under latent structures with smooth marginals. Contrary to the conventional curse of dimensionality, our sample complexity scales only polynomially with the dimension. Additionally, we develop a perturbation theory for component recovery and propose a recovery procedure based on simultaneous diagonalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09165v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichen Lyu, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Semi-supervised Community Detection using Glauber Dynamics for an Ising Model</title>
      <link>https://arxiv.org/abs/2506.09223</link>
      <description>arXiv:2506.09223v1 Announce Type: new 
Abstract: We consider graphs with two communities and analyze an algorithm for learning the community labels when the edges of the graph and only a small fraction of the labels are known in advance. The algorithm is based on the Glauber dynamics for an Ising model where the energy function includes a quadratic penalty on the magnetization. The analysis focuses on graphs sampled from a Stochastic Block Model (SBM) with slowly growing mean degree. We derive a mean-field limit for the magnetization of each community, which can be used to choose the run-time of the algorithm to obtain a target accuracy level. We further prove that almost exact recovery is achieved in a number of iterations that is quasi-linear in the number of nodes. As a special case, our results provide the first rigorous analysis of the label propagation algorithm in the SBM with slowly diverging mean degree. We complement our theoretical results with several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09223v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantin Avrachenkov, Diego Goldsztajn</dc:creator>
    </item>
    <item>
      <title>Consistent Infill Estimability of the Regression Slope Between Gaussian Random Fields Under Spatial Confounding</title>
      <link>https://arxiv.org/abs/2506.09267</link>
      <description>arXiv:2506.09267v1 Announce Type: new 
Abstract: The problem of estimating the slope parameter in regression between two spatial processes under confounding by an unmeasured spatial process has received widespread attention in the recent statistical literature. Yet, a fundamental question remains unsolved: when is this slope consistently estimable under spatial confounding, with existing insights being largely empirical or estimator-specific. In this manuscript, we characterize conditions for consistent estimability of the regression slope between Gaussian random fields (GRFs). Under fixed-domain (infill) asymptotics, we give sufficient conditions for consistent estimability using a novel characterization of the regression slope as the ratio of principal irregular terms of covariances, dictating the relative local behavior of the exposure and confounder processes. When estimability holds, we provide consistent estimators of the slope using local differencing (taking discrete differences or Laplacians of the processes of suitable order). Using functional analysis results on Paley-Wiener spaces, we then provide an easy-to-verify necessary condition for consistent estimability of the slope in terms of the relative spectral tail decays of the confounder and exposure. As a by-product, we establish a novel and general spectral condition on the equivalence of measures on the paths of multivariate GRFs with component fields of varying smoothnesses, a result of independent importance. We show that for the Mat\'ern, power-exponential, generalized Cauchy, and coregionalization families, the necessary and sufficient conditions become identical, thereby providing a complete characterization of consistent estimability of the slope under spatial confounding. The results are extended to accommodate measurement error using local-averaging-and-differencing based estimators. Finite sample behavior is explored via numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09267v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhirup Datta, Michael L. Stein</dc:creator>
    </item>
    <item>
      <title>Almost-Optimal Local-Search Methods for Sparse Tensor PCA</title>
      <link>https://arxiv.org/abs/2506.09959</link>
      <description>arXiv:2506.09959v1 Announce Type: new 
Abstract: Local-search methods are widely employed in statistical applications, yet interestingly, their theoretical foundations remain rather underexplored, compared to other classes of estimators such as low-degree polynomials and spectral methods. Of note, among the few existing results recent studies have revealed a significant "local-computational" gap in the context of a well-studied sparse tensor principal component analysis (PCA), where a broad class of local Markov chain methods exhibits a notable underperformance relative to other polynomial-time algorithms. In this work, we propose a series of local-search methods that provably "close" this gap to the best known polynomial-time procedures in multiple regimes of the model, including and going beyond the previously studied regimes in which the broad family of local Markov chain methods underperforms. Our framework includes: (1) standard greedy and randomized greedy algorithms applied to the (regularized) posterior of the model; and (2) novel random-threshold variants, in which the randomized greedy algorithm accepts a proposed transition if and only if the corresponding change in the Hamiltonian exceeds a random Gaussian threshold-rather that if and only if it is positive, as is customary. The introduction of the random thresholds enables a tight mathematical analysis of the randomized greedy algorithm's trajectory by crucially breaking the dependencies between the iterations, and could be of independent interest to the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09959v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Lovig, Conor Sheehan, Konstantinos Tsirkas, Ilias Zadik</dc:creator>
    </item>
    <item>
      <title>Adversarial Surrogate Risk Bounds for Binary Classification</title>
      <link>https://arxiv.org/abs/2506.09348</link>
      <description>arXiv:2506.09348v1 Announce Type: cross 
Abstract: A central concern in classification is the vulnerability of machine learning models to adversarial attacks. Adversarial training is one of the most popular techniques for training robust classifiers, which involves minimizing an adversarial surrogate risk. Recent work characterized when a minimizing sequence of an adversarial surrogate risk is also a minimizing sequence of the adversarial classification risk for binary classification -- a property known as adversarial consistency. However, these results do not address the rate at which the adversarial classification risk converges to its optimal value for such a sequence of functions that minimize the adversarial surrogate. This paper provides surrogate risk bounds that quantify that convergence rate. Additionally, we derive distribution-dependent surrogate risk bounds in the standard (non-adversarial) learning setting, that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09348v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie S. Frank</dc:creator>
    </item>
    <item>
      <title>Sufficient digits and density estimation: A Bayesian nonparametric approach using generalized finite P\'olya trees</title>
      <link>https://arxiv.org/abs/2506.09437</link>
      <description>arXiv:2506.09437v1 Announce Type: cross 
Abstract: This paper proposes a novel approach for statistical modelling of a continuous random variable $X$ on $[0, 1)$, based on its digit representation $X=.X_1X_2\ldots$. In general, $X$ can be coupled with a random variable $N$ so that if a prior of $N$ is imposed, $(X_1,\ldots,X_N)$ becomes a sufficient statistics and $.X_{N+1}X_{N+2}\ldots$ is uniformly distributed. In line with this fact, and focusing on binary digits for simplicity, we propose a family of generalized finite P{\'o}lya trees that induces a random density for a sample, which becomes a flexible tool for density estimation. Here, the digit system may be random and learned from the data. We provide a detailed Bayesian analysis, including closed form expression for the posterior distribution which sidesteps the need of MCMC methods for posterior inference. We analyse the frequentist properties as the sample size increases, and provide sufficient conditions for consistency of the posterior distributions of the random density and $N$. We consider an extension to data spanning multiple orders of magnitude, and propose a prior distribution that encodes the so-called extended Newcomb-Benford law. Such a model shows promising results for density estimation of human-activity data. Our methodology is illustrated on several synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09437v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Jesper M{\o}ller</dc:creator>
    </item>
    <item>
      <title>Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds</title>
      <link>https://arxiv.org/abs/2506.09681</link>
      <description>arXiv:2506.09681v1 Announce Type: cross 
Abstract: Generative modeling aims to produce new random examples from an unknown target distribution, given access to a finite collection of examples. Among the leading approaches, denoising diffusion probabilistic models (DDPMs) construct such examples by mapping a Brownian motion via a diffusion process driven by an estimated score function. In this work, we first provide empirical evidence that DDPMs are robust to constant-variance noise in the score evaluations. We then establish finite-sample guarantees in Wasserstein-2 distance that exhibit two key features: (i) they characterize and quantify the robustness of DDPMs to noisy score estimates, and (ii) they achieve faster convergence rates than previously known results. Furthermore, we observe that the obtained rates match those known in the Gaussian case, implying their optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09681v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vahan Arsenyan, Elen Vardanyan, Arnak Dalalyan</dc:creator>
    </item>
    <item>
      <title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
      <link>https://arxiv.org/abs/2506.09853</link>
      <description>arXiv:2506.09853v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09853v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang</dc:creator>
    </item>
    <item>
      <title>Optimal Adjustment Sets for Nonparametric Estimation of Weighted Controlled Direct Effect</title>
      <link>https://arxiv.org/abs/2506.09871</link>
      <description>arXiv:2506.09871v1 Announce Type: cross 
Abstract: The weighted controlled direct effect (WCDE) generalizes the standard controlled direct effect (CDE) by averaging over the mediator distribution, providing a robust estimate when treatment effects vary across mediator levels. This makes the WCDE especially relevant in fairness analysis, where it isolates the direct effect of an exposure on an outcome, independent of mediating pathways. This work establishes three fundamental advances for WCDE in observational studies: First, we establish necessary and sufficient conditions for the unique identifiability of the WCDE, clarifying when it diverges from the CDE. Next, we consider nonparametric estimation of the WCDE and derive its influence function, focusing on the class of regular and asymptotically linear estimators. Lastly, we characterize the optimal covariate adjustment set that minimizes the asymptotic variance, demonstrating how mediator-confounder interactions introduce distinct requirements compared to average treatment effect estimation. Our results offer a principled framework for efficient estimation of direct effects in complex causal systems, with practical applications in fairness and mediation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09871v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruiyang Lin (University of Science,Technology of China), Yongyi Guo (University of Wisconsin-Madison), Kyra Gan (Cornell Tech)</dc:creator>
    </item>
    <item>
      <title>Learning single-index models via harmonic decomposition</title>
      <link>https://arxiv.org/abs/2506.09887</link>
      <description>arXiv:2506.09887v1 Announce Type: cross 
Abstract: We study the problem of learning single-index models, where the label $y \in \mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through an unknown one-dimensional projection $\langle \boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under Gaussian inputs, the statistical and computational complexity of recovering $\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function. In this paper, we propose a new perspective: we argue that "spherical harmonics" -- rather than "Hermite polynomials" -- provide the natural basis for this problem, as they capture its intrinsic "rotational symmetry". Building on this insight, we characterize the complexity of learning single-index models under arbitrary spherically symmetric input distributions. We introduce two families of estimators -- based on tensor unfolding and online SGD -- that respectively achieve either optimal sample complexity or optimal runtime, and argue that estimators achieving both may not exist in general. When specialized to Gaussian inputs, our theory not only recovers and clarifies existing results but also reveals new phenomena that had previously been overlooked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09887v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmit Joshi, Hugo Koubbi, Theodor Misiakiewicz, Nathan Srebro</dc:creator>
    </item>
    <item>
      <title>Constrained Denoising, Empirical Bayes, and Optimal Transport</title>
      <link>https://arxiv.org/abs/2506.09986</link>
      <description>arXiv:2506.09986v1 Announce Type: cross 
Abstract: In the statistical problem of denoising, Bayes and empirical Bayes methods can "overshrink" their output relative to the latent variables of interest. This work is focused on constrained denoising problems which mitigate such phenomena. At the oracle level, i.e., when the latent variable distribution is assumed known, we apply tools from the theory of optimal transport to characterize the solution to (i) variance-constrained, (ii) distribution-constrained, and (iii) general-constrained denoising problems. At the empirical level, i.e., when the latent variable distribution is not known, we use empirical Bayes methodology to estimate these oracle denoisers. Our approach is modular, and transforms any suitable (unconstrained) empirical Bayes denoiser into a constrained empirical Bayes denoiser. We prove explicit rates of convergence for our proposed methodologies, which both extend and sharpen existing asymptotic results that have previously considered only variance constraints. We apply our methodology in two applications: one in astronomy concerning the relative chemical abundances in a large catalog of red-clump stars, and one in baseball concerning minor- and major league batting skill for rookie players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09986v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Quinn Jaffe, Nikolaos Ignatiadis, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>The envelope of a complex Gaussian random variable</title>
      <link>https://arxiv.org/abs/2305.03038</link>
      <description>arXiv:2305.03038v4 Announce Type: replace 
Abstract: The envelope of an elliptical Gaussian complex vector, or equivalently, the amplitude or norm of a bivariate normal random vector has application in many weather and signal processing contexts. We explicitly characterize its distribution in the general case through its probability density, cumulative distribution and moment generating function. Moments and limiting distributions are also derived. These derivations are exploited to also characterize the special cases where the bivariate Gaussian mean vector and covariance matrix have a simpler structure, providing new additional insights in many cases. Simulations illustrate the benefits of using our formulae over Monte Carlo methods. We also use our derivations to get a better initial characterization of the distribution of the observed values in structural Magnetic Resonance Imaging datasets, and of wind speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03038v4</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sattwik Ghosal, Ranjan Maitra</dc:creator>
    </item>
    <item>
      <title>Community detection with the Bethe-Hessian</title>
      <link>https://arxiv.org/abs/2411.02835</link>
      <description>arXiv:2411.02835v2 Announce Type: replace 
Abstract: The Bethe-Hessian matrix, introduced by Saade, Krzakala, and Zdeborov\'a (2014), is a Hermitian matrix designed for applying spectral clustering algorithms to sparse networks. Rather than employing a non-symmetric and high-dimensional non-backtracking operator, a spectral method based on the Bethe-Hessian matrix is conjectured to also reach the Kesten-Stigum detection threshold in the sparse stochastic block model (SBM). We provide the first rigorous analysis of the Bethe-Hessian spectral method in the SBM under both the bounded expected degree and the growing degree regimes. Specifically, we demonstrate that: (i) When the expected degree $d\geq 2$, the number of negative outliers of the Bethe-Hessian matrix can consistently estimate the number of blocks above the Kesten-Stigum threshold, thus confirming a conjecture from Saade, Krzakala, and Zdeborov\'a (2014) for $d\geq 2$. (ii) For sufficiently large $d$, its eigenvectors can be used to achieve weak recovery. (iii) As $d\to\infty$, we establish the concentration of the locations of its negative outlier eigenvalues, and weak consistency can be achieved via a spectral method based on the Bethe-Hessian matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02835v2</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ludovic Stephan, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Cumulative INAR($\infty$) Processes via Least-Squares</title>
      <link>https://arxiv.org/abs/2412.01569</link>
      <description>arXiv:2412.01569v4 Announce Type: replace 
Abstract: This paper investigates the cumulative Integer-Valued Autoregressive model of infinite order, denoted as INAR($\infty$), a class of processes crucial for modeling count time series and equivalent to discrete-time Hawkes processes. We propose a computationally efficient conditional least-squares (CLS) estimator to address the challenge of parameter inference in this infinite-dimensional setting. We establish the key theoretical properties of the estimator, including its consistency and asymptotic normality. A central contribution is the rigorous treatment of its large-sample distribution in a framework where the parameter dimension grows with the sample size, for which we derive the corresponding sandwich-form covariance matrix. The theoretical results are substantiated through comprehensive Monte Carlo simulations. These experiments demonstrate that the estimator's accuracy and stability systematically improve as the sample size increases, confirming its consistency. Furthermore, we show that the estimator's finite-sample distribution is well-approximated by a normal distribution, and this approximation becomes more robust with larger samples. Our work provides a complete and practical framework for statistical inference in cumulative INAR($\infty$) models. The code to reproduce the numerical experiments is publicly available at https://github.com/gagawjbytw/INAR_estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01569v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingli Wang, Xiaohong Duan, Ping He</dc:creator>
    </item>
    <item>
      <title>On the optimality of coin-betting for mean estimation</title>
      <link>https://arxiv.org/abs/2412.02640</link>
      <description>arXiv:2412.02640v2 Announce Type: replace 
Abstract: Confidence sequences are sequences of confidence sets that adapt to incoming data while maintaining validity. Recent advances have introduced an algorithmic formulation for constructing some of the tightest confidence sequences for the mean of bounded real random variables. These approaches use a coin-betting framework, where a player sequentially bets on differences between potential mean values and observed data. This work discusses the optimality of such coin-betting formulation among algorithmic frameworks building on e-variables methods to test and estimate the mean of bounded random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02640v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Clerico</dc:creator>
    </item>
    <item>
      <title>Theory and Applications of Kernel Stein's Method on Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2501.00695</link>
      <description>arXiv:2501.00695v2 Announce Type: replace 
Abstract: Distributional comparison is a fundamental problem in statistical data analysis with numerous applications in a variety of scientific and engineering fields. Numerous methods exist for distributional comparison but kernel Stein's method has gained significant popularity in recent times. In this paper, we first present a novel mathematically rigorous and consistent generalization of the Stein operator to Riemannian manifolds. Then we show that the kernel Stein discrepancy (KSD) defined via this operator is nearly as strong as the KSD in the Euclidean setting in terms of distinguishing the target distributions from the reference. We investigate the asymptotic properties of the minimum kernel Stein discrepancy estimator (MKSDE), apply it to goodness-of-fit testing, and compare it to the maximum likelihood estimator (MLE) experimentally. We present several examples of our theory applied to commonly encountered Riemannian manifolds in practice namely, the n-sphere, the Grassmann, Stiefel, the manifold of symmetric positive definite matrices and other Riemannian homogeneous spaces. On the aforementioned manifolds, we consider a variety of distributions with intractable normalization constants and derive closed form expressions for the KSD and MKSDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00695v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoda Qu, Baba C. Vemuri</dc:creator>
    </item>
    <item>
      <title>Classifying one-dimensional discrete models with maximum likelihood degree one</title>
      <link>https://arxiv.org/abs/2205.09547</link>
      <description>arXiv:2205.09547v3 Announce Type: replace-cross 
Abstract: We propose a classification of all one-dimensional discrete statistical models with maximum likelihood degree one based on their rational parametrization. We show how all such models can be constructed from members of a smaller class of 'fundamental models' using a finite number of simple operations. We introduce 'chipsplitting games', a class of combinatorial games on a grid which we use to represent fundamental models. This combinatorial perspective enables us to show that there are only finitely many fundamental models in the probability simplex $\Delta_n$ for $n\leq 4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.09547v3</guid>
      <category>math.CO</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arthur Bik, Orlando Marigliano</dc:creator>
    </item>
    <item>
      <title>Adaptive Projected Two-Sample Comparisons for Single-Cell Gene Expression Data</title>
      <link>https://arxiv.org/abs/2403.05679</link>
      <description>arXiv:2403.05679v2 Announce Type: replace-cross 
Abstract: We study high-dimensional two-sample mean comparison and address the curse of dimensionality through data-adaptive projections. Leveraging the low-dimensional and localized signal structures commonly seen in single-cell genomics data, our first proposed method identifies a sparse, informative low-dimensional subspace and then performs statistical inference restricted to this subspace. To address the double-dipping issue -- arising from using the same data for projection and inference -- we develop a debiased projected estimator using the semiparametric double-machine learning framework. The resulting inference not only has the usual frequentist validity but also provides useful information on the potential location of the signal due to the sparsity of the projection. Our second method uses a more flexible projection scheme to improve the power against the global null hypothesis and avoid the degeneracy issue commonly faced by existing methods. It is particularly useful when debiasing is practically challenging or when the informative signal is not well-captured by the subspace. Experiments on synthetic data and real datasets demonstrate the theoretical promise and interpretability of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05679v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Einstein from Noise: Statistical Analysis</title>
      <link>https://arxiv.org/abs/2407.05277</link>
      <description>arXiv:2407.05277v2 Announce Type: replace-cross 
Abstract: ``Einstein from noise" (EfN) is a prominent example of the model bias phenomenon: systematic errors in the statistical model that lead to spurious but consistent estimates. In the EfN experiment, one falsely believes that a set of observations contains noisy, shifted copies of a template signal (e.g., an Einstein image), whereas in reality, it contains only pure noise observations. To estimate the signal, the observations are first aligned with the template using cross-correlation, and then averaged. Although the observations contain nothing but noise, it was recognized early on that this process produces a signal that resembles the template signal! This pitfall was at the heart of a central scientific controversy about validation techniques in structural biology.
  This paper provides a comprehensive statistical analysis of the EfN phenomenon above. We show that the Fourier phases of the EfN estimator (namely, the average of the aligned noise observations) converge to the Fourier phases of the template signal, explaining the observed structural similarity. Additionally, we prove that the convergence rate is inversely proportional to the number of noise observations and, in the high-dimensional regime, to the Fourier magnitudes of the template signal. Moreover, in the high-dimensional regime, the Fourier magnitudes converge to a scaled version of the template signal's Fourier magnitudes. This work not only deepens the theoretical understanding of the EfN phenomenon but also highlights potential pitfalls in template matching techniques and emphasizes the need for careful interpretation of noisy observations across disciplines in engineering, statistics, physics, and biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05277v2</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amnon Balanov, Wasim Huleihel, Tamir Bendory</dc:creator>
    </item>
    <item>
      <title>Stochastic block models with many communities and the Kesten--Stigum bound</title>
      <link>https://arxiv.org/abs/2503.03047</link>
      <description>arXiv:2503.03047v2 Announce Type: replace-cross 
Abstract: We study the inference of communities in stochastic block models with a growing number of communities. For block models with $n$ vertices and a fixed number of communities $q$, it was predicted in Decelle et al. (2011) that there are computationally efficient algorithms for recovering the communities above the Kesten--Stigum (KS) bound and that efficient recovery is impossible below the KS bound. This conjecture has since stimulated a lot of interest, with the achievability side proven in a line of research that culminated in the work of Abbe and Sandon (2018). Conversely, recent work by Sohn and Wein (2025) provides evidence for the hardness part using the low-degree paradigm.
  In this paper we investigate community recovery in the regime $q=q_n \to \infty$ as $n\to\infty$ where no such predictions exist. We show that efficient inference of communities remains possible above the KS bound. Furthermore, we show that recovery of block models is low-degree hard below the KS bound when the number of communities satisfies $q\ll \sqrt{n}$. Perhaps surprisingly, we find that when $q \gg \sqrt{n}$, there is an efficient algorithm based on non-backtracking walks for recovery even below the KS bound. We identify a new threshold and ask if it is the threshold for efficient recovery in this regime. Finally, we show that detection is easy and identify (up to a constant) the information-theoretic threshold for community recovery as the number of communities $q$ diverges.
  Our low-degree hardness results also naturally have consequences for graphon estimation, improving results of Luo and Gao (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03047v2</guid>
      <category>math.PR</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Byron Chin, Elchanan Mossel, Youngtak Sohn, Alexander S. Wein</dc:creator>
    </item>
    <item>
      <title>Operator limit of Wigner matrices I</title>
      <link>https://arxiv.org/abs/2503.23940</link>
      <description>arXiv:2503.23940v2 Announce Type: replace-cross 
Abstract: We consider the Wigner matrix $W_{n}$ of dimension $n \times n$ as $n \to \infty$. The objective of this paper is two folds: first we construct an operator $\mathcal{W}$ on a suitable Hilbert space $\mathcal{H}$ and then define a suitable notion of convergence such that the matrices $W_{n}$ converge in that notion of convergence to $\mathcal{W}$. We further investigate some properties of $\mathcal{W}$ and $\mathcal{H}$. We show that $\mathcal{H}$ is a nontrivial extension of $L^{2}[0,1]$ with respect to the Lebesgue measure and the spectral measure of $\mathcal{W}$ at any function $f \in L^{2}[0,1]$ is almost surely the semicircular law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23940v2</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debapratim Banerjee</dc:creator>
    </item>
    <item>
      <title>Entropic Isoperimetric and Cram\'er--Rao Inequalities for R\'enyi--Fisher Information</title>
      <link>https://arxiv.org/abs/2504.01837</link>
      <description>arXiv:2504.01837v2 Announce Type: replace-cross 
Abstract: The de Bruijn identity states that Fisher information is equal to a half of the time-derivative of Shannon differential entropy along heat flow. In the same spirit, a generalized version of Fisher information, which we term the R\'enyi--Fisher information, is defined as a half of the time-derivative of R\'enyi differential entropy along heat flow. Based on this R\'enyi--Fisher information, we establish several sharp R\'enyi-entropic isoperimetric inequalities, which generalize the classic entropic isoperimetric inequality to the R\'enyi setting. Utilizing these isoperimetric inequalities, we extend the classical Cram\'er--Rao inequality from Fisher information to R\'enyi--Fisher information. We then use these generalized Cram\'er--Rao inequalities to determine the signs of derivatives of R\'enyi entropy along heat flow, strengthening existing results on the complete monotonicity of R\'enyi entropy. We lastly explore the implications of our R\'enyi-entropic isoperimetric inequalities for entropy power inequalities. We demonstrate that, unlike in the Shannon entropy case, the classic entropy power inequality does not admit a direct extension to R\'enyi entropy without introducing additional exponents or scaling factors. Furthermore, we establish a sharp R\'enyi entropy power inequality involving a scaling factor under the assumption that one of two independent random vectors is Gaussian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01837v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wu, Lei Yu</dc:creator>
    </item>
  </channel>
</rss>
