<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Jun 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Heavy tails and negative correlation in a binomial model for sports matches: applications to curling</title>
      <link>https://arxiv.org/abs/2406.18601</link>
      <description>arXiv:2406.18601v1 Announce Type: new 
Abstract: A binomial model for sports matches is developed making use of the maximum possible score $n$ in a game. In contrast to previous approaches the scores of the two teams are negatively correlated, abstracting from a scenario whereby teams cancel each other out. When $n$ is known, analytical results are possible via a Gaussian approximation. Model calibration is obtained via generalized linear modelling, enabling elementary econometric and strategic analysis to be performed. Inter alia this includes quantifying the Last Stone First End effect, analogous to the home-field advantage found in conventional sports. When $n$ is unknown the model behaviour is richer and leads to heavy-tailed non-Gaussian behaviour. We present an approximate analysis of this case based on the Variance Gamma distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18601v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Fry, Mark Austin, Silvio Fanzon</dc:creator>
    </item>
    <item>
      <title>Multi-level Phenotypic Models of Cardiovascular Disease and Obstructive Sleep Apnea Comorbidities: A Longitudinal Wisconsin Sleep Cohort Study</title>
      <link>https://arxiv.org/abs/2406.18602</link>
      <description>arXiv:2406.18602v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVDs) are notably prevalent among patients with obstructive sleep apnea (OSA), posing unique challenges in predicting CVD progression due to the intricate interactions of comorbidities. Traditional models typically lack the necessary dynamic and longitudinal scope to accurately forecast CVD trajectories in OSA patients. This study introduces a novel multi-level phenotypic model to analyze the progression and interplay of these conditions over time, utilizing data from the Wisconsin Sleep Cohort, which includes 1,123 participants followed for decades. Our methodology comprises three advanced steps: (1) Conducting feature importance analysis through tree-based models to underscore critical predictive variables like total cholesterol, low-density lipoprotein (LDL), and diabetes. (2) Developing a logistic mixed-effects model (LGMM) to track longitudinal transitions and pinpoint significant factors, which displayed a diagnostic accuracy of 0.9556. (3) Implementing t-distributed Stochastic Neighbor Embedding (t-SNE) alongside Gaussian Mixture Models (GMM) to segment patient data into distinct phenotypic clusters that reflect varied risk profiles and disease progression pathways. This phenotypic clustering revealed two main groups, with one showing a markedly increased risk of major adverse cardiovascular events (MACEs), underscored by the significant predictive role of nocturnal hypoxia and sympathetic nervous system activity from sleep data. Analysis of transitions and trajectories with t-SNE and GMM highlighted different progression rates within the cohort, with one cluster progressing more slowly towards severe CVD states than the other. This study offers a comprehensive understanding of the dynamic relationship between CVD and OSA, providing valuable tools for predicting disease onset and tailoring treatment approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18602v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duy Nguyen, Ca Hoang, Phat K. Huynh, Tien Truong, Dang Nguyen, Abhay Sharma, Trung Q. Le</dc:creator>
    </item>
    <item>
      <title>Confidence interval estimation of mixed oil length with conditional diffusion model</title>
      <link>https://arxiv.org/abs/2406.18603</link>
      <description>arXiv:2406.18603v1 Announce Type: new 
Abstract: Accurately estimating the mixed oil length plays a big role in the economic benefit for oil pipeline network. While various proposed methods have tried to predict the mixed oil length, they often exhibit an extremely high probability (around 50\%) of underestimating it. This is attributed to their failure to consider the statistical variability inherent in the estimated length of mixed oil. To address such issues, we propose to use the conditional diffusion model to learn the distribution of the mixed oil length given pipeline features. Subsequently, we design a confidence interval estimation for the length of the mixed oil based on the pseudo-samples generated by the learned diffusion model. To our knowledge, we are the first to present an estimation scheme for confidence interval of the oil-mixing length that considers statistical variability, thereby reducing the possibility of underestimating it. When employing the upper bound of the interval as a reference for excluding the mixed oil, the probability of underestimation can be as minimal as 5\%, a substantial reduction compared to 50\%. Furthermore, utilizing the mean of the generated pseudo samples as the estimator for the mixed oil length enhances prediction accuracy by at least 10\% compared to commonly used methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18603v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanfeng Yang, Lihong Zhang, Ziqi Chen, Miaomiao Yu, Lei Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Stochastic Predictions of Non-Gaussian Systems with Applications in Climate Change</title>
      <link>https://arxiv.org/abs/2406.18606</link>
      <description>arXiv:2406.18606v1 Announce Type: new 
Abstract: Climate change poses significant challenges for accurate climate modeling due to the complexity and variability of non-Gaussian climate systems. To address the complexities of non-Gaussian systems in climate modeling, this thesis proposes a Bayesian framework utilizing the Unscented Kalman Filter (UKF), Ensemble Kalman Filter (EnKF), and Unscented Particle Filter (UPF) for one-dimensional and two-dimensional stochastic climate models, evaluated with real-world temperature and sea level data. We study these methods under varying conditions, including measurement noise, sample sizes, and observed and hidden variables, to highlight their respective advantages and limitations. Our findings reveal that merely increasing data is insufficient for accurate predictions; instead, selecting appropriate methods is crucial. This research provides insights into issues related to information barrier, curse of dimensionality, prediction variability, and measurement noise quantification, thereby enhancing the application of these techniques in real-world climate scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18606v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunjin Tong</dc:creator>
    </item>
    <item>
      <title>Analysis of Full-scale Riser Responses in Field Conditions Based on Gaussian Mixture Model</title>
      <link>https://arxiv.org/abs/2406.18611</link>
      <description>arXiv:2406.18611v1 Announce Type: new 
Abstract: Offshore slender marine structures experience complex and combined load conditions from waves, current and vessel motions that may result in both wave frequency and vortex shedding response patterns. Field measurements often consist of records of environmental conditions and riser responses, typically with 30-minute intervals. These data can be represented in a high-dimensional parameter space. However, it is difficult to visualize and understand the structural responses, as they are affected by many of these parameters. It becomes easier to identify trends and key parameters if the measurements with the same characteristics can be grouped together. Cluster analysis is an unsupervised learning method, which groups the data based on their relative distance, density of the data space, intervals, or statistical distributions. In the present study, a Gaussian mixture model guided by domain knowledge has been applied to analyze field measurements. Using the 242 measurement events of the Helland-Hansen riser, it is demonstrated that riser responses can be grouped into 12 clusters by the identification of key environmental parameters. This results in an improved understanding of complex structure responses. Furthermore, the cluster results are valuable for evaluating the riser response prediction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18611v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jfluidstructs.2022.103793</arxiv:DOI>
      <arxiv:journal_reference>Journal of Fluids and Structures, Volume 116, 2023, 103793</arxiv:journal_reference>
      <dc:creator>Jie Wu, S{\o}lve Eidnes, Jingzhe Jin, Halvor Lie, Decao Yin, Elizabeth Passano, Svein S{\ae}vik, Signe Riemer-Sorensen</dc:creator>
    </item>
    <item>
      <title>Robust Distributed Learning of Functional Data From Simulators through Data Sketching</title>
      <link>https://arxiv.org/abs/2406.18751</link>
      <description>arXiv:2406.18751v1 Announce Type: new 
Abstract: In environmental studies, realistic simulations are essential for understanding complex systems. Statistical emulation with Gaussian processes (GPs) in functional data models have become a standard tool for this purpose. Traditional centralized processing of such models requires substantial computational and storage resources, leading to emerging distributed Bayesian learning algorithms that partition data into shards for distributed computations. However, concerns about the sensitivity of distributed inference to shard selection arise. Instead of using data shards, our approach employs multiple random matrices to create random linear projections, or sketches, of the dataset. Posterior inference on functional data models is conducted using random data sketches on various machines in parallel. These individual inferences are combined across machines at a central server. The aggregation of inference across random matrices makes our approach resilient to the selection of data sketches, resulting in robust distributed Bayesian learning. An important advantage is its ability to maintain the privacy of sampling units, as random sketches prevent the recovery of raw data. We highlight the significance of our approach through simulation examples and showcase the performance of our approach as an emulator using surrogates of the Sea, Lake, and Overland Surges from Hurricanes (SLOSH) simulator - an important simulator for government agencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18751v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R. Jacob Andros, Rajarshi Guhaniyogi, Devin Francom, Donatella Pasqualini</dc:creator>
    </item>
    <item>
      <title>Similarities among top one day batters: physics-based quantification</title>
      <link>https://arxiv.org/abs/2406.18617</link>
      <description>arXiv:2406.18617v1 Announce Type: cross 
Abstract: Assessment of the performance of a player in any sport is very much needed to determine the ranking of players and make a solid team with the best players. Besides these, fans, journalists, sports persons, and sports councils often analyse the performances of current and retired players to identify the best players of all time. Here, we study the performance of all-time top batters in one-day cricket using physics-based statistical methods. The batters are selected in this study who possess either higher total runs or a high number of centuries. It is found that the total runs increases linearly with the innings number at the later stage of the batter carrier, and the runs rate estimated from the linear regression analysis also increases linearly with the average runs. The probability of non-scoring innings is found to be a negligibly small number (i.e., $\leq 0.1$ ) for each batter. Furthermore, based on innings-wise runs, we have computed the six-dimensional probability distribution vector for each player. Two components of the probability distribution vector vary linearly with average runs. The component representing the probability of scoring runs less than 50 linearly decreases with the average runs. In contrast, the probability of scoring runs greater than or equal to 100 and less than 150 linearly increases with the average runs. We have also estimated the entropy to assess the diversity of a player. Interestingly, the entropy varies linearly with the average runs, giving rise to two clusters corresponding to the old and recent players. Furthermore, the angle between two probability vectors is calculated for each pair of players to measure the similarities among the players. It is found that some of the players are almost identical to each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18617v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dipak Patra</dc:creator>
    </item>
    <item>
      <title>MultiObjMatch: Matching with Optimal Tradeoffs between Multiple Objectives in R</title>
      <link>https://arxiv.org/abs/2406.18819</link>
      <description>arXiv:2406.18819v1 Announce Type: cross 
Abstract: In an observational study, matching aims to create many small sets of similar treated and control units from initial samples that may differ substantially in order to permit more credible causal inferences. The problem of constructing matched sets may be formulated as an optimization problem, but it can be challenging to specify a single objective function that adequately captures all the design considerations at work. One solution, proposed by \citet{pimentel2019optimal} is to explore a family of matched designs that are Pareto optimal for multiple objective functions. We present an R package, \href{https://github.com/ShichaoHan/MultiObjMatch}{\texttt{MultiObjMatch}}, that implements this multi-objective matching strategy using a network flow algorithm for several common design goals: marginal balance on important covariates, size of the matched sample, and average within-pair multivariate distances. We demonstrate the package's flexibility in exploring user-defined tradeoffs of interest via two case studies, a reanalysis of the canonical National Supported Work dataset and a novel analysis of a clinical dataset to estimate the impact of diabetic kidney disease on hospitalization costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18819v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shichao Han, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Credit Ratings: Heterogeneous Effect on Capital Structure</title>
      <link>https://arxiv.org/abs/2406.18936</link>
      <description>arXiv:2406.18936v1 Announce Type: cross 
Abstract: Why do companies choose particular capital structures? A compelling answer to this question remains elusive despite extensive research. In this article, we use double machine learning to examine the heterogeneous causal effect of credit ratings on leverage. Taking advantage of the flexibility of random forests within the double machine learning framework, we model the relationship between variables associated with leverage and credit ratings without imposing strong assumptions about their functional form. This approach also allows for data-driven variable selection from a large set of individual company characteristics, supporting valid causal inference. We report three findings: First, credit ratings causally affect the leverage ratio. Having a rating, as opposed to having none, increases leverage by approximately 7 to 9 percentage points, or 30\% to 40\% relative to the sample mean leverage. However, this result comes with an important caveat, captured in our second finding: the effect is highly heterogeneous and varies depending on the specific rating. For AAA and AA ratings, the effect is negative, reducing leverage by about 5 percentage points. For A and BBB ratings, the effect is approximately zero. From BB ratings onwards, the effect becomes positive, exceeding 10 percentage points. Third, contrary to what the second finding might imply at first glance, the change from no effect to a positive effect does not occur abruptly at the boundary between investment and speculative grade ratings. Rather, it is gradual, taking place across the granular rating notches ("+/-") within the BBB and BB categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18936v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helmut Wasserbacher, Martin Spindler</dc:creator>
    </item>
    <item>
      <title>Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes</title>
      <link>https://arxiv.org/abs/2406.19015</link>
      <description>arXiv:2406.19015v1 Announce Type: cross 
Abstract: Health monitoring, fault analysis, and detection are critical for the safe and sustainable operation of battery systems. We apply Gaussian process resistance models on lithium iron phosphate battery field data to effectively separate the time-dependent and operating point-dependent resistance. The data set contains 29 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 232 cells and 131 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes allow the quick processing of over a million data points, enabling advanced online monitoring and furthering the understanding of battery pack failure in the field. The analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how batteries degrade and fail in the field and demonstrate the potential of efficient online monitoring based on data. We open-source the code and publish the large data set upon completion of the review of this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19015v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joachim Schaeffer, Eric Lenz, Duncan Gulla, Martin Z. Bazant, Richard D. Braatz, Rolf Findeisen</dc:creator>
    </item>
    <item>
      <title>Benchmarking M6 Competitors: An Analysis of Financial Metrics and Discussion of Incentives</title>
      <link>https://arxiv.org/abs/2406.19105</link>
      <description>arXiv:2406.19105v1 Announce Type: cross 
Abstract: The M6 Competition assessed the performance of competitors using a ranked probability score and an information ratio (IR). While these metrics do well at picking the winners in the competition, crucial questions remain for investors with longer-term incentives. To address these questions, we compare the competitors' performance to a number of conventional (long-only) and alternative indices using standard industry metrics. We apply factor models to the competitors' returns and show the difficulty for any competitor to demonstrate a statistically significant value-add above industry-standard benchmarks within the short timeframe of the competition. We also uncover that most competitors generated lower risk-adjusted returns and lower maximum drawdowns than randomly selected portfolios, and that most competitors could not generate significant out-performance in raw returns. We further introduce two new strategies by picking the competitors with the best (Superstars) and worst (Superlosers) recent performance and show that it is challenging to identify skill amongst investment managers. Overall, our findings highlight the difference in incentives for competitors over professional investors, where the upside of winning the competition dwarfs the potential downside of not winning to maximize fees over an extended period of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19105v1</guid>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.34990.11847/1</arxiv:DOI>
      <dc:creator>Matthew J. Schneider, Rufus Rankin, Prabir Burman, Alexander Aue</dc:creator>
    </item>
    <item>
      <title>Mixture priors for replication studies</title>
      <link>https://arxiv.org/abs/2406.19152</link>
      <description>arXiv:2406.19152v1 Announce Type: cross 
Abstract: Replication of scientific studies is important for assessing the credibility of their results. However, there is no consensus on how to quantify the extent to which a replication study replicates an original result. We propose a novel Bayesian approach based on mixture priors. The idea is to use a mixture of the posterior distribution based on the original study and a non-informative distribution as the prior for the analysis of the replication study. The mixture weight then determines the extent to which the original and replication data are pooled.
  Two distinct strategies are presented: one with fixed mixture weights, and one that introduces uncertainty by assigning a prior distribution to the mixture weight itself. Furthermore, it is shown how within this framework Bayes factors can be used for formal testing of scientific hypotheses, such as tests regarding the presence or absence of an effect. To showcase the practical application of the methodology, we analyze data from three replication studies. Our findings suggest that mixture priors are a valuable and intuitive alternative to other Bayesian methods for analyzing replication studies, such as hierarchical models and power priors. We provide the free and open source R package repmix that implements the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19152v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Macr\`i Demartino, Leonardo Egidi, Leonhard Held, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic Survival Analysis in Triple-Negative Breast Cancer</title>
      <link>https://arxiv.org/abs/2406.19213</link>
      <description>arXiv:2406.19213v1 Announce Type: cross 
Abstract: This study aims to evaluate the performance of Cox regression with lasso penalty and adaptive lasso penalty in high-dimensional settings. Variable selection methods are necessary in this context to reduce dimensionality and make the problem feasible. Several weight calculation procedures for adaptive lasso are proposed to determine if they offer an improvement over lasso, as adaptive lasso addresses its inherent bias. These proposed weights are based on principal component analysis, ridge regression, univariate Cox regressions and random survival forest (RSF). The proposals are evaluated in simulated datasets.
  A real application of these methodologies in the context of genomic data is also carried out. The study consists of determining the variables, clinical and genetic, that influence the survival of patients with triple-negative breast cancer (TNBC), which is a type breast cancer with low survival rates due to its aggressive nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19213v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pilar Gonz\'alez-Barquero (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid), Rosa E. Lillo (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid, Department of Statistics, Universidad Carlos III de Madrid), \'Alvaro M\'endez-Civieta (uc3m-Santander Big Data Institute, Universidad Carlos III de Madrid, Department of Biostatistics, Columbia University, New York)</dc:creator>
    </item>
    <item>
      <title>The myth of declining competitive balance in the UEFA Champions League group stage</title>
      <link>https://arxiv.org/abs/2406.19222</link>
      <description>arXiv:2406.19222v1 Announce Type: cross 
Abstract: According to previous studies, competitive balance has significantly declined in the UEFA Champions League group stage over the recent decades. Our paper introduces six alternative indices for measuring ex ante and ex post competitive balance in order to explore the robustness of these results. The ex ante measures are based on Elo ratings, while the ex post measures compare the group ranking to reasonable benchmarks. We find no evidence of any trend in the competitive balance of the UEFA Champions League group stage between the 2003/04 and 2023/24 seasons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19222v1</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
    <item>
      <title>Eliciting prior information from clinical trials via calibrated Bayes factor</title>
      <link>https://arxiv.org/abs/2406.19346</link>
      <description>arXiv:2406.19346v1 Announce Type: cross 
Abstract: In the Bayesian framework power prior distributions are increasingly adopted in clinical trials and similar studies to incorporate external and past information, typically to inform the parameter associated to a treatment effect. Their use is particularly effective in scenarios with small sample sizes and where robust prior information is actually available. A crucial component of this methodology is represented by its weight parameter, which controls the volume of historical information incorporated into the current analysis. This parameter can be considered as either fixed or random. Although various strategies exist for its determination, eliciting the prior distribution of the weight parameter according to a full Bayesian approach remains a challenge. In general, this parameter should be carefully selected to accurately reflect the available prior information without dominating the posterior inferential conclusions. To this aim, we propose a novel method for eliciting the prior distribution of the weight parameter through a simulation-based calibrated Bayes factor procedure. This approach allows for the prior distribution to be updated based on the strength of evidence provided by the data: The goal is to facilitate the integration of historical data when it aligns with current information and to limit it when discrepancies arise in terms, for instance, of prior-data conflicts. The performance of the proposed method is tested through simulation studies and applied to real data from clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19346v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Macr\`i Demartino, Leonardo Egidi, Nicola Torelli, Ioannis Ntzoufras</dc:creator>
    </item>
    <item>
      <title>Review of Quasi-Randomization Approaches for Estimation from Non-probability Samples</title>
      <link>https://arxiv.org/abs/2312.05383</link>
      <description>arXiv:2312.05383v3 Announce Type: replace 
Abstract: The recent proliferation of computers and the internet have opened new opportunities for collecting and processing data. However, such data are often obtained without a well-planned probability survey design. Such non-probability based samples cannot be automatically regarded as representative of the population of interest. Several classes of methods for estimation and inferences from non-probability samples have been developed in recent years. The quasi-randomization methods assume that non-probability sample selection is governed by an underlying latent random mechanism. The basic idea is to use information collected from a probability ("reference") sample to uncover latent non-probability survey participation probabilities (also known as "propensity scores") and use them in estimation of target finite population parameters. In this paper, we review and compare theoretical properties of recently developed methods of estimation survey participation probabilities and study their relative performances in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05383v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladislav Beresovsky, Julie Gershunskaya, Terrance D. Savitsky</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Modeling and Inference for Mechanistic Systems in Industrial Hygiene</title>
      <link>https://arxiv.org/abs/2307.00450</link>
      <description>arXiv:2307.00450v2 Announce Type: replace-cross 
Abstract: A series of experiments in stationary and moving passenger rail cars were conducted to measure removal rates of particles in the size ranges of SARS-CoV-2 viral aerosols, and the air changes per hour provided by existing and modified air handling systems. Such methods for exposure assessments are customarily based on mechanistic models derived from physical laws of particle movement that are deterministic and do not account for measurement errors inherent in data collection. The resulting analysis compromises on reliably learning about mechanistic factors such as ventilation rates, aerosol generation rates and filtration efficiencies from field measurements. This manuscript develops a Bayesian state space modeling framework that synthesizes information from the mechanistic system as well as the field data. We derive a stochastic model from finite difference approximations of differential equations explaining particle concentrations. Our inferential framework trains the mechanistic system using the field measurements from the chamber experiments and delivers reliable estimates of the underlying physical process with fully model-based uncertainty quantification. Our application falls within the realm of Bayesian "melding" of mechanistic and statistical models and is of significant relevance to industrial hygienists and public health researchers working on assessment of exposure to viral aerosols in rail car fleets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00450v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyakanti Pan, Darpan Das, Gurumurthy Ramachandran, Sudipto Banerjee</dc:creator>
    </item>
  </channel>
</rss>
