<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 May 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simulation-based Inference of Developmental EEG Maturation with the Spectral Graph Model</title>
      <link>https://arxiv.org/abs/2405.02524</link>
      <description>arXiv:2405.02524v1 Announce Type: new 
Abstract: The spectral content of macroscopic neural activity evolves throughout development, yet how this maturation relates to underlying brain network formation and dynamics remains unknown. To gain mechanistic insights into this process, we evaluate developmental EEG spectral changes via Bayesian model inversion of the spectral graph model (SGM), a parsimonious model of whole-brain spatiospectral activity derived from linearized neural field models coupled by the structural connectome. Simulation-based inference was used to estimate age-varying SGM parameter posterior distributions from EEG spectra spanning the developmental period. We found this model-fitting approach accurately captures the developmental maturation of EEG spectra via a neurobiologically consistent progression of key neural parameters: long-range coupling, axonal conductance speed, and excitatory:inhibitory balance. These results suggest that spectral maturation of brain activity observed during normal development is supported by functional adaptations, specifically age-dependent tuning of localized neural dynamics and their long-range coupling within the macroscopic, structural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02524v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danilo Bernardo, Xihe Xie, Parul Verma, Jonathan Kim, Virginia Liu, Ye Wu, Pew-Thian Yap, Srikantan Nagarajan, Ashish Raj</dc:creator>
    </item>
    <item>
      <title>A causal inference approach of monosynapses from spike trains</title>
      <link>https://arxiv.org/abs/2405.02786</link>
      <description>arXiv:2405.02786v1 Announce Type: new 
Abstract: Neuroscientists have worked on the problem of estimating synaptic properties, such as connectivity and strength, from simultaneously recorded spike trains since the 1960s. Recent years have seen renewed interest in the problem, coinciding with rapid advances in the technology of high-density neural recordings and optogenetics, which can be used to calibrate causal hypotheses about functional connectivity. Here, a rigorous causal inference framework for pairwise excitatory and inhibitory monosynaptic effects between spike trains is developed. Causal interactions are identified by separating spike interactions in pairwise spike trains by their timescales. Fast algorithms for computing accurate estimates of associated quantities are also developed. Through the lens of this framework, the link between biophysical parameters and statistical definitions of causality between spike trains is examined across a spectrum of dynamical systems simulations. In an idealized setting, we demonstrate a correspondence between the synaptic causal metric developed here and the probabilities of causation developed by Tian and Pearl. Since the probabilities of causation are derived under distinct assumptions and include data from experimental randomization, this opens up the possibility of testing the synaptic inference framework's assumptions with juxtacellular or optogenetic stimulation. We simulate such an experiment with a biophysically detailed channelrhodopsin model and show that randomization is not achieved; strong confounding persists even with strong stimulations. A principal goal is to ask how carefully articulated causal assumptions might better inform the design of neural stimulation experiments and, in turn, support experimental tests of those assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02786v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zach Saccomano, Sam Mckenzie, Horacio Rotstein, Asohan Amarasingham</dc:creator>
    </item>
    <item>
      <title>Firing rate model for brain rhythms controlled by astrocytes</title>
      <link>https://arxiv.org/abs/2405.03601</link>
      <description>arXiv:2405.03601v1 Announce Type: new 
Abstract: We propose a new mean-field model of brain rhythms governed by astrocytes. This theoretical framework describes how astrocytes can regulate neuronal activity and contribute to the generation of brain rhythms. The model describes at the population level the interactions between two large groups of excitatory and inhibitory neurons. The excitatory population is governed by astrocytes via a so-called tripartite synapse. This approach allows us to describe how the interactions between different groups of neurons and astrocytes can give rise to various patterns of synchronized activity and transitions between them. Using methods of nonlinear analysis we show that astrocytic modulation can lead to a change in the period and amplitude of oscillations in the populations of neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03601v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergey V. Stasenko, Sergey M. Olenin, Eugene A. Grines, Tatiana A. Levanova</dc:creator>
    </item>
    <item>
      <title>One nose but two nostrils: Learn to align with sparse connections between two olfactory cortices</title>
      <link>https://arxiv.org/abs/2405.03602</link>
      <description>arXiv:2405.03602v1 Announce Type: new 
Abstract: The integration of neural representations in the two hemispheres is an important problem in neuroscience. Recent experiments revealed that odor responses in cortical neurons driven by separate stimulation of the two nostrils are highly correlated. This bilateral alignment points to structured inter-hemispheric connections, but detailed mechanism remains unclear. Here, we hypothesized that continuous exposure to environmental odors shapes these projections and modeled it as online learning with local Hebbian rule. We found that Hebbian learning with sparse connections achieves bilateral alignment, exhibiting a linear trade-off between speed and accuracy. We identified an inverse scaling relationship between the number of cortical neurons and the inter-hemispheric projection density required for desired alignment accuracy, i.e., more cortical neurons allow sparser inter-hemispheric projections. We next compared the alignment performance of local Hebbian rule and the global stochastic-gradient-descent (SGD) learning for artificial neural networks. We found that although SGD leads to the same alignment accuracy with modestly sparser connectivity, the same inverse scaling relation holds. We showed that their similar performance originates from the fact that the update vectors of the two learning rules align significantly throughout the learning process. This insight may inspire efficient sparse local learning algorithms for more complex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03602v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bo Liu, Shanshan Qin, Venkatesh Murthy, Yuhai Tu</dc:creator>
    </item>
    <item>
      <title>Leveraging the Human Ventral Visual Stream to Improve Neural Network Robustness</title>
      <link>https://arxiv.org/abs/2405.02564</link>
      <description>arXiv:2405.02564v1 Announce Type: cross 
Abstract: Human object recognition exhibits remarkable resilience in cluttered and dynamic visual environments. In contrast, despite their unparalleled performance across numerous visual tasks, Deep Neural Networks (DNNs) remain far less robust than humans, showing, for example, a surprising susceptibility to adversarial attacks involving image perturbations that are (almost) imperceptible to humans. Human object recognition likely owes its robustness, in part, to the increasingly resilient representations that emerge along the hierarchy of the ventral visual cortex. Here we show that DNNs, when guided by neural representations from a hierarchical sequence of regions in the human ventral visual stream, display increasing robustness to adversarial attacks. These neural-guided models also exhibit a gradual shift towards more human-like decision-making patterns and develop hierarchically smoother decision surfaces. Importantly, the resulting representational spaces differ in important ways from those produced by conventional smoothing methods, suggesting that such neural-guidance may provide previously unexplored robustness solutions. Our findings support the gradual emergence of human robustness along the ventral visual hierarchy and suggest that the key to DNN robustness may lie in increasing emulation of the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02564v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenan Shao, Linjian Ma, Bo Li, Diane M. Beck</dc:creator>
    </item>
    <item>
      <title>Can Brain Signals Reveal Inner Alignment with Human Languages?</title>
      <link>https://arxiv.org/abs/2208.06348</link>
      <description>arXiv:2208.06348v5 Announce Type: replace 
Abstract: Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide interpretations of the performance improvement: (1) feature distribution shows the effectiveness of the alignment module for discovering and encoding the relationship between EEG and language; (2) alignment weights show the influence of different language semantics as well as EEG frequency features; (3) brain topographical maps provide an intuitive demonstration of the connectivity in the brain regions. Our code is available at \url{https://github.com/Jason-Qiu/EEG_Language_Alignment}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.06348v5</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Han, Jielin Qiu, Jiacheng Zhu, Mengdi Xu, Douglas Weber, Bo Li, Ding Zhao</dc:creator>
    </item>
    <item>
      <title>Fibration symmetries and cluster synchronization in the Caenorhabditis elegans connectome</title>
      <link>https://arxiv.org/abs/2305.19367</link>
      <description>arXiv:2305.19367v2 Announce Type: replace 
Abstract: Capturing how the Caenorhabditis elegans connectome structure gives rise to its neuron functionality remains unclear. It is through fiber symmetries found in its neuronal connectivity that synchronization of a group of neurons can be determined. To understand these we investigate graph symmetries and search for such in the symmetrized versions of the forward and backward locomotive sub-networks of the Caenorhabditi elegans worm neuron network. The use of ordinarily differential equations simulations admissible to these graphs are used to validate the predictions of these fiber symmetries and are compared to the more restrictive orbit symmetries. Additionally fibration symmetries are used to decompose these graphs into their fundamental building blocks which reveal units formed by nested loops or multilayered fibers. It is found that fiber symmetries of the connectome can accurately predict neuronal synchronization even under not idealized connectivity as long as the dynamics are within stable regimes of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19367v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bryant Avila, Pedro Augusto, Manuel Zimmer, Matteo Serafino, Hern\'an A. Makse</dc:creator>
    </item>
    <item>
      <title>Mind-to-Image: Projecting Visual Mental Imagination of the Brain from fMRI</title>
      <link>https://arxiv.org/abs/2404.05468</link>
      <description>arXiv:2404.05468v4 Announce Type: replace 
Abstract: The reconstruction of images observed by subjects from fMRI data collected during visual stimuli has made strong progress in the past decade, thanks to the availability of extensive fMRI datasets and advancements in generative models for image generation. However, the application of visual reconstruction has remained limited. Reconstructing visual imagination presents a greater challenge, with potentially revolutionary applications ranging from aiding individuals with disabilities to verifying witness accounts in court. The primary hurdles in this field are the absence of data collection protocols for visual imagery and the lack of datasets on the subject. Traditionally, fMRI-to-image relies on data collected from subjects exposed to visual stimuli, which poses issues for generating visual imagery based on the difference of brain activity between visual stimulation and visual imagery. For the first time, we have compiled a substantial dataset (around 6h of scans) on visual imagery along with a proposed data collection protocol. We then train a modified version of an fMRI-to-image model and demonstrate the feasibility of reconstructing images from two modes of imagination: from memory and from pure imagination. The resulting pipeline we call Mind-to-Image marks a step towards creating a technology that allow direct reconstruction of visual imagery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05468v4</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Caselles-Dupr\'e, Charles Mellerio, Paul H\'erent, Aliz\'ee Lopez-Persem, Benoit B\'eranger, Mathieu Soularue, Pierre Fautrel, Gauthier Vernier, Matthieu Cord</dc:creator>
    </item>
    <item>
      <title>Simplicity in Complexity : Explaining Visual Complexity using Deep Segmentation Models</title>
      <link>https://arxiv.org/abs/2403.03134</link>
      <description>arXiv:2403.03134v3 Announce Type: replace-cross 
Abstract: The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite complex. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images. This suggests that the complexity of images can be surprisingly simple.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03134v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingke Shen, Surabhi S Nath, Aenne Brielmann, Peter Dayan</dc:creator>
    </item>
  </channel>
</rss>
