<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Jun 2024 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Determining the Number of Communities in Sparse and Imbalanced Settings</title>
      <link>https://arxiv.org/abs/2406.04423</link>
      <description>arXiv:2406.04423v1 Announce Type: new 
Abstract: Community structures represent a crucial aspect of network analysis, and various methods have been developed to identify these communities. However, a common hurdle lies in determining the number of communities K, a parameter that often requires estimation in practice. Existing approaches for estimating K face two notable challenges: the weak community signal present in sparse networks and the imbalance in community sizes or edge densities that result in unequal per-community expected degree. We propose a spectral method based on a novel network operator whose spectral properties effectively overcome both challenges. This operator is a refined version of the non-backtracking operator, adapted from a "centered" adjacency matrix. Its leading eigenvalues are more concentrated than those of the adjacency matrix for sparse networks, while they also demonstrate enhanced signal under imbalance scenarios, a benefit attributed to the centering step. This is justified, either theoretically or numerically, under the null model K = 1, in both dense and ultra-sparse settings. A goodness-of-fit test based on the leading eigenvalue can be applied to determine the number of communities K.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04423v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhixuan Shao, Can M. Le</dc:creator>
    </item>
    <item>
      <title>Bayesian Methods to Improve The Accuracy of Differentially Private Measurements of Constrained Parameters</title>
      <link>https://arxiv.org/abs/2406.04448</link>
      <description>arXiv:2406.04448v1 Announce Type: new 
Abstract: Formal disclosure avoidance techniques are necessary to ensure that published data can not be used to identify information about individuals. The addition of statistical noise to unpublished data can be implemented to achieve differential privacy, which provides a formal mathematical privacy guarantee. However, the infusion of noise results in data releases which are less precise than if no noise had been added, and can lead to some of the individual data points being nonsensical. Examples of this are estimates of population counts which are negative, or estimates of the ratio of counts which violate known constraints. A straightforward way to guarantee that published estimates satisfy these known constraints is to specify a statistical model and incorporate a prior on census counts and ratios which properly constrains the parameter space. We utilize rejection sampling methods for drawing samples from the posterior distribution and we show that this implementation produces estimates of population counts and ratios which maintain formal privacy, are more precise than the original unconstrained noisy measurements, and are guaranteed to satisfy prior constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04448v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Janicki, Scott H. Holan, Kyle M. Irimata, James Livsey, Andrew Raim</dc:creator>
    </item>
    <item>
      <title>Conformal Multi-Target Hyperrectangles</title>
      <link>https://arxiv.org/abs/2406.04498</link>
      <description>arXiv:2406.04498v1 Announce Type: new 
Abstract: We propose conformal hyperrectangular prediction regions for multi-target regression. We propose split conformal prediction algorithms for both point and quantile regression to form hyperrectangular prediction regions, which allow for easy marginal interpretation and do not require covariance estimation. In practice, it is preferable that a prediction region is balanced, that is, having identical marginal prediction coverage, since prediction accuracy is generally equally important across components of the response vector. The proposed algorithms possess two desirable properties, namely, tight asymptotic overall nominal coverage as well as asymptotic balance, that is, identical asymptotic marginal coverage, under mild conditions. We then compare our methods to some existing methods on both simulated and real data sets. Our simulation results and real data analysis show that our methods outperform existing methods while achieving the desired nominal coverage and good balance between dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04498v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Randomized Trials with Partial Clustering and Imbalanced Dependence Structures</title>
      <link>https://arxiv.org/abs/2406.04505</link>
      <description>arXiv:2406.04505v1 Announce Type: new 
Abstract: In many randomized trials, participants are grouped into clusters, such as neighborhoods or schools, and these clusters are assumed to be the independent unit. This assumption, however, might not reflect the underlying dependence structure, with serious consequences to statistical power. First, consider a cluster randomized trial where participants are artificially grouped together for the purposes of randomization. For intervention participants the groups are the basis for intervention delivery, but for control participants the groups are dissolved. Second, consider an individually randomized group treatment trial where participants are randomized and then post-randomization, intervention participants are grouped together for intervention delivery, while the control participants continue with the standard of care. In both trial designs, outcomes among intervention participants will be dependent within each cluster, while outcomes for control participants will be effectively independent. We use causal models to non-parametrically describe the data generating process for each trial design and formalize the conditional independence in the observed data distribution. For estimation and inference, we propose a novel implementation of targeted minimum loss-based estimation (TMLE) accounting for partial clustering and the imbalanced dependence structure. TMLE is a model-robust approach, leverages covariate adjustment and machine learning to improve precision, and facilitates estimation of a large set of causal effects. In finite sample simulations, TMLE achieved comparable or markedly higher statistical power than common alternatives. Finally, application of TMLE to real data from the SEARCH-IPT trial resulted in 20-57\% efficiency gains, demonstrating the real-world consequences of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04505v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua R. Nugent, Elijah Kakande, Gabriel Chamie, Jane Kabami, Asiphas Owaraganise, Diane V. Havlir, Moses Kamya, Laura B. Balzer</dc:creator>
    </item>
    <item>
      <title>A novel multivariate regression model for unbalanced binary data : a strong conjugacy under random effect approach</title>
      <link>https://arxiv.org/abs/2406.04518</link>
      <description>arXiv:2406.04518v1 Announce Type: new 
Abstract: In this paper, we deduce a new multivariate regression model designed to fit correlated binary data. The multivariate distribution is derived from a Bernoulli mixed model with a nonnormal random intercept on the marginal approach. The random effect distribution is assumed to be the generalized log-gamma (GLG) distribution by considering a particular parameter setting. The complement log-log function is specified to lead to strong conjugacy between the response variable and random effect. The new discrete multivariate distribution, named MBerGLG distribution, has location and dispersion parameters. The MBerGLG distribution leads to the MBerGLG regression (MBerGLGR) model, providing an alternative approach to fitting both unbalanced and balanced correlated response binary data. Monte Carlo simulation studies show that its maximum likelihood estimators are unbiased, efficient, and consistent asymptotically. The randomized quantile residuals are performed to identify possible departures from the proposal model and the data and detect atypical subjects. Finally, two applications are presented in the data analysis section.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04518v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lizandra C. Fabio, Vanessa Barros, Cristian Villegas, Jalmar M. F. Carrasco</dc:creator>
    </item>
    <item>
      <title>Imputation of Nonignorable Missing Data in Surveys Using Auxiliary Margins Via Hot Deck and Sequential Imputation</title>
      <link>https://arxiv.org/abs/2406.04599</link>
      <description>arXiv:2406.04599v1 Announce Type: new 
Abstract: Survey data collection often is plagued by unit and item nonresponse. To reduce reliance on strong assumptions about the missingness mechanisms, statisticians can use information about population marginal distributions known, for example, from censuses or administrative databases. One approach that does so is the Missing Data with Auxiliary Margins, or MD-AM, framework, which uses multiple imputation for both unit and item nonresponse so that survey-weighted estimates accord with the known marginal distributions. However, this framework relies on specifying and estimating a joint distribution for the survey data and nonresponse indicators, which can be computationally and practically daunting in data with many variables of mixed types. We propose two adaptations to the MD-AM framework to simplify the imputation task. First, rather than specifying a joint model for unit respondents' data, we use random hot deck imputation while still leveraging the known marginal distributions. Second, instead of sampling from conditional distributions implied by the joint model for the missing data due to item nonresponse, we apply multiple imputation by chained equations for item nonresponse before imputation for unit nonresponse. Using simulation studies with nonignorable missingness mechanisms, we demonstrate that the proposed approach can provide more accurate point and interval estimates than models that do not leverage the auxiliary information. We illustrate the approach using data on voter turnout from the U.S. Current Population Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04599v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjiao Yang, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Dynamical mixture modeling with fast, automatic determination of Markov chains</title>
      <link>https://arxiv.org/abs/2406.04653</link>
      <description>arXiv:2406.04653v1 Announce Type: new 
Abstract: Markov state modeling has gained popularity in various scientific fields due to its ability to reduce complex time series data into transitions between a few states. Yet, current frameworks are limited by assuming a single Markov chain describes the data, and they suffer an inability to discern heterogeneities. As a solution, this paper proposes a variational expectation-maximization algorithm that identifies a mixture of Markov chains in a time-series data set. The method is agnostic to the definition of the Markov states, whether data-driven (e.g. by spectral clustering) or based on domain knowledge. Variational EM efficiently and organically identifies the number of Markov chains and dynamics of each chain without expensive model comparisons or posterior sampling. The approach is supported by a theoretical analysis and numerical experiments, including simulated and observational data sets based on ${\tt Last.fm}$ music listening, ultramarathon running, and gene expression. The results show the new algorithm is competitive with contemporary mixture modeling approaches and powerful in identifying meaningful heterogeneities in time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04653v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher E. Miles, Robert J. Webber</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Spatial-temporal Non-Gaussian Data Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2406.04655</link>
      <description>arXiv:2406.04655v1 Announce Type: new 
Abstract: Analysing non-Gaussian spatial-temporal data typically requires introducing spatial dependence in generalised linear models through the link function of an exponential family distribution. However, unlike in Gaussian likelihoods, inference is considerably encumbered by the inability to analytically integrate out the random effects and reduce the dimension of the parameter space. Iterative estimation algorithms struggle to converge due to the presence of weakly identified parameters. We devise an approach that obviates these issues by exploiting generalised conjugate multivariate distribution theory for exponential families, which enables exact sampling from analytically available posterior distributions conditional upon some fixed process parameters. More specifically, we expand upon the Diaconis-Ylvisaker family of conjugate priors to achieve analytically tractable posterior inference for spatially-temporally varying regression models conditional on some kernel parameters. Subsequently, we assimilate inference from these individual posterior distributions over a range of values of these parameters using Bayesian predictive stacking. We evaluate inferential performance on simulated data, compare with fully Bayesian inference using Markov chain Monte Carlo and apply our proposed method to analyse spatially-temporally referenced avian count data from the North American Breeding Bird Survey database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04655v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyakanti Pan, Lu Zhang, Jonathan R. Bradley, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Robust Inference of Dynamic Covariance Using Wishart Processes and Sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2406.04796</link>
      <description>arXiv:2406.04796v1 Announce Type: new 
Abstract: Several disciplines, such as econometrics, neuroscience, and computational psychology, study the dynamic interactions between variables over time. A Bayesian nonparametric model known as the Wishart process has been shown to be effective in this situation, but its inference remains highly challenging. In this work, we introduce a Sequential Monte Carlo (SMC) sampler for the Wishart process, and show how it compares to conventional inference approaches, namely MCMC and variational inference. Using simulations we show that SMC sampling results in the most robust estimates and out-of-sample predictions of dynamic covariance. SMC especially outperforms the alternative approaches when using composite covariance functions with correlated parameters. We demonstrate the practical applicability of our proposed approach on a dataset of clinical depression (n=1), and show how using an accurate representation of the posterior distribution can be used to test for dynamics on covariance</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04796v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hester Huijsdens, David Leeftink, Linda Geerligs, Max Hinne</dc:creator>
    </item>
    <item>
      <title>Dynamic prediction of death risk given a renewal hospitalization process</title>
      <link>https://arxiv.org/abs/2406.04849</link>
      <description>arXiv:2406.04849v1 Announce Type: new 
Abstract: Predicting the risk of death for chronic patients is highly valuable for informed medical decision-making. This paper proposes a general framework for dynamic prediction of the risk of death of a patient given her hospitalization history, which is generally available to physicians. Predictions are based on a joint model for the death and hospitalization processes, thereby avoiding the potential bias arising from selection of survivors. The framework accommodates various submodels for the hospitalization process. In particular, we study prediction of the risk of death in a renewal model for hospitalizations, a common approach to recurrent event modelling. In the renewal model, the distribution of hospitalizations throughout the follow-up period impacts the risk of death. This result differs from prediction in the Poisson model, previously studied, where only the number of hospitalizations matters. We apply our methodology to a prospective, observational cohort study of 401 patients treated for COPD in one of six outpatient respiratory clinics run by the Respiratory Service of Galdakao University Hospital, with a median follow-up of 4.16 years. We find that more concentrated hospitalizations increase the risk of death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04849v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Telmo J. P\'erez-Izquierdo, Irantzu Barrio, Cristobal Esteban</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Computation with Deep Learning and Conformal prediction</title>
      <link>https://arxiv.org/abs/2406.04874</link>
      <description>arXiv:2406.04874v1 Announce Type: new 
Abstract: Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these "user-choices". In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).
  Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04874v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meili Baragatti, Bertrand Cloez, David M\'etivier, Isabelle Sanchez</dc:creator>
    </item>
    <item>
      <title>Unguided structure learning of DAGs for count data</title>
      <link>https://arxiv.org/abs/2406.04994</link>
      <description>arXiv:2406.04994v1 Announce Type: new 
Abstract: Mainly motivated by the problem of modelling directional dependence relationships for multivariate count data in high-dimensional settings, we present a new algorithm, called learnDAG, for learning the structure of directed acyclic graphs (DAGs). In particular, the proposed algorithm tackled the problem of learning DAGs from observational data in two main steps: (i) estimation of candidate parent sets; and (ii) feature selection. We experimentally compare learnDAG to several popular competitors in recovering the true structure of the graphs in situations where relatively moderate sample sizes are available. Furthermore, to make our algorithm is stronger, a validation of the algorithm is presented through the analysis of real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04994v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thi Kim Hue Nguyen, Monica Chiogna, Davide Risso</dc:creator>
    </item>
    <item>
      <title>Testing common invariant subspace of multilayer networks</title>
      <link>https://arxiv.org/abs/2406.05010</link>
      <description>arXiv:2406.05010v1 Announce Type: new 
Abstract: Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common invariant subspace of the networks, because such common invariant subspace could capture the fundamental structural patterns and interactions across all layers. Many methods have been proposed to estimate the common invariant subspace. However, whether real-world multilayer networks share the same common subspace remains unknown. In this paper, we first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same subspace, and under the alternative hypothesis, there exist at least two networks that do not have the same subspace. We propose a Weighted Degree Difference Test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05010v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mingao Yuan, Qianqian Yao</dc:creator>
    </item>
    <item>
      <title>TrendLSW: Trend and Spectral Estimation of Nonstationary Time Series in R</title>
      <link>https://arxiv.org/abs/2406.05012</link>
      <description>arXiv:2406.05012v1 Announce Type: new 
Abstract: The TrendLSW R package has been developed to provide users with a suite of wavelet-based techniques to analyse the statistical properties of nonstationary time series. The key components of the package are (a) two approaches for the estimation of the evolutionary wavelet spectrum in the presence of trend; and (b) wavelet-based trend estimation in the presence of locally stationary wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the calculation of associated pointwise confidence intervals. Lastly, the package directly implements boundary handling methods that enable the methods to be performed on data of arbitrary length, not just dyadic length as is common for wavelet-based methods, ensuring no pre-processing of data is necessary. The key functionality of the package is demonstrated through two data examples, arising from biology and activity monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05012v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Euan T. McGonigle, Rebecca Killick, Matthew A. Nunes</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of Latent Spectral Shapes</title>
      <link>https://arxiv.org/abs/2406.04915</link>
      <description>arXiv:2406.04915v1 Announce Type: cross 
Abstract: This paper proposes a hierarchical spatial-temporal model for modelling the spectrograms of animal calls. The motivation stems from analyzing recordings of the so-called grunt calls emitted by various lemur species. Our goal is to identify a latent spectral shape that characterizes each species and facilitates measuring dissimilarities between them. The model addresses the synchronization of animal vocalizations, due to varying time-lengths and speeds, with non-stationary temporal patterns and accounts for periodic sampling artifacts produced by the time discretization of analog signals. The former is achieved through a synchronization function, and the latter is modeled using a circular representation of time. To overcome the curse of dimensionality inherent in the model's implementation, we employ the Nearest Neighbor Gaussian Process, and posterior samples are obtained using the Markov Chain Monte Carlo method. We apply the model to a real dataset comprising sounds from 8 different species. We define a representative sound for each species and compare them using a simple distance measure. Cross-validation is used to evaluate the predictive capability of our proposal and explore special cases. Additionally, a simulation example is provided to demonstrate that the algorithm is capable of retrieving the true parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04915v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiu Ching Yip, Daria Valente, Enrico Bibbona, Olivier Friard, Gianluca Mastrantonio, Marco Gamba</dc:creator>
    </item>
    <item>
      <title>Time-lag bias induced by unobserved heterogeneity: comparing treated patients to controls with a different start of follow-up</title>
      <link>https://arxiv.org/abs/2105.07685</link>
      <description>arXiv:2105.07685v2 Announce Type: replace 
Abstract: In comparative effectiveness research, treated and control patients might have a different start of follow-up as treatment is often started later in the disease trajectory. This typically occurs when data from treated and controls are not collected within the same source. Only patients who did not yet experience the event of interest whilst in the control condition end up in the treatment data source. In case of unobserved heterogeneity, these treated patients will have a lower average risk than the controls. We illustrate how failing to account for this time-lag between treated and controls leads to bias in the estimated treatment effect. We define estimands and time axes, then explore five methods to adjust for this time-lag bias by utilising the time between diagnosis and treatment initiation in different ways. We conducted a simulation study to evaluate whether these methods reduce the bias and then applied the methods to a comparison between fertility patients treated with insemination and similar but untreated patients. We conclude that time-lag bias can be vast and that the time between diagnosis and treatment initiation should be taken into account in the analysis to respect the chronology of the disease and treatment trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.07685v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rik van Eekelen, Patrick M. M. Bossuyt, Nan van Geloven</dc:creator>
    </item>
    <item>
      <title>Bounded-memory adjusted scores estimation in generalized linear models with large data sets</title>
      <link>https://arxiv.org/abs/2307.07342</link>
      <description>arXiv:2307.07342v4 Announce Type: replace 
Abstract: The widespread use of maximum Jeffreys'-prior penalized likelihood in binomial-response generalized linear models, and in logistic regression, in particular, are supported by the results of Kosmidis and Firth (2021, Biometrika), who show that the resulting estimates are always finite-valued, even in cases where the maximum likelihood estimates are not, which is a practical issue regardless of the size of the data set. In logistic regression, the implied adjusted score equations are formally bias-reducing in asymptotic frameworks with a fixed number of parameters and appear to deliver a substantial reduction in the persistent bias of the maximum likelihood estimator in high-dimensional settings where the number of parameters grows asymptotically as a proportion of the number of observations. In this work, we develop and present two new variants of iteratively reweighted least squares for estimating generalized linear models with adjusted score equations for mean bias reduction and maximization of the likelihood penalized by a positive power of the Jeffreys-prior penalty, which eliminate the requirement of storing $O(n)$ quantities in memory, and can operate with data sets that exceed computer memory or even hard drive capacity. We achieve that through incremental QR decompositions, which enable IWLS iterations to have access only to data chunks of predetermined size. Both procedures can also be readily adapted to fit generalized linear models when distinct parts of the data is stored across different sites and, due to privacy concerns, cannot be fully transferred across sites. We assess the procedures through a real-data application with millions of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07342v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Zietkiewicz, Ioannis Kosmidis</dc:creator>
    </item>
    <item>
      <title>W-kernel and essential subspace for frequentist evaluation of Bayesian estimators</title>
      <link>https://arxiv.org/abs/2311.13017</link>
      <description>arXiv:2311.13017v4 Announce Type: replace 
Abstract: The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequentist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. Projections to the essential subspace realize dimensional reduction in the sensitivity analysis and frequentist evaluation. A key tool for treating frequentist properties is the recently proposed Bayesian infinitesimal jackknife approximation(Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequentist duality, naturally appeared from the kernel framework. Two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13017v4</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukito Iba</dc:creator>
    </item>
    <item>
      <title>A Connection Between Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials</title>
      <link>https://arxiv.org/abs/2401.11352</link>
      <description>arXiv:2401.11352v2 Announce Type: replace 
Abstract: The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using stratified (permutated block) randomization or in the analysis stage through covariate adjustment. This article makes a connection between covariate adjustment and stratified randomization in a general framework where all regular, asymptotically linear estimators are identified as augmented estimators. From a geometric perspective, covariate adjustment can be viewed as an attempt to approximate the optimal augmentation function, and stratified randomization improves a given approximation by moving it closer to the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to attaching an optimal augmentation term based on the stratification factor. Under stratified randomization, adjusting for the stratification factor only in data analysis is not expected to improve efficiency, and the key to efficient estimation is incorporating new prognostic information from other covariates. In designing a trial with stratified randomization, it is not essential to include all important covariates in the stratification, because their prognostic information can be incorporated through covariate adjustment. These observations are confirmed in a simulation study and illustrated using real clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11352v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiwei Zhang</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis for publication bias in meta-analysis of sparse data based on exact likelihood</title>
      <link>https://arxiv.org/abs/2404.06837</link>
      <description>arXiv:2404.06837v2 Announce Type: replace 
Abstract: Meta-analysis is a powerful tool to synthesize findings from multiple studies. The normal-normal random-effects model is widely used to account for between-study heterogeneity. However, meta-analysis of sparse data, which may arise when the event rate is low for binary or count outcomes, poses a challenge to the normal-normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study model may not be good. To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study model with an exact model. Publication bias is one of the most serious threats in meta-analysis. Several quantitative sensitivity analysis methods for evaluating the potential impacts of selective publication are available for the normal-normal random-effects model. We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the t-statistic selection function of Copas to several generalized linear mixed-effects models. Through applications of our proposed method to several real-world meta-analysis and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal-normal model. The proposed method would give useful guidance to address publication bias in meta-analysis of sparse data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06837v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taojun Hu, Yi Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer</title>
      <link>https://arxiv.org/abs/2406.02751</link>
      <description>arXiv:2406.02751v2 Announce Type: replace 
Abstract: This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple Monte Carlo algorithm that can be used to calculate the posterior distribution of a system's reliability. This algorithm is especially useful when a system's reliability is modeled through the reliability of its subcomponents, yet only system-level data is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02751v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Botts</dc:creator>
    </item>
    <item>
      <title>Uplift Modeling Under Limited Supervision</title>
      <link>https://arxiv.org/abs/2403.19289</link>
      <description>arXiv:2403.19289v3 Announce Type: replace-cross 
Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or treatment policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random, underlining the need for models that can generalize with limited supervision to reduce experimental risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19289v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun Pang</dc:creator>
    </item>
    <item>
      <title>Diffusion posterior sampling for simulation-based inference in tall data settings</title>
      <link>https://arxiv.org/abs/2404.07593</link>
      <description>arXiv:2404.07593v2 Announce Type: replace-cross 
Abstract: Determining which parameters of a non-linear model best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators. The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available to better infer the parameters of the model. The proposed method is built upon recent developments from the flourishing score-based diffusion literature and allows to estimate the tall data posterior distribution, while simply using information from a score network trained for a single context observation. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07593v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Linhart, Gabriel Victorino Cardoso, Alexandre Gramfort, Sylvain Le Corff, Pedro L. C. Rodrigues</dc:creator>
    </item>
    <item>
      <title>Kernel Three Pass Regression Filter</title>
      <link>https://arxiv.org/abs/2405.07292</link>
      <description>arXiv:2405.07292v2 Announce Type: replace-cross 
Abstract: We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07292v2</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rajveer Jat, Daanish Padha</dc:creator>
    </item>
  </channel>
</rss>
