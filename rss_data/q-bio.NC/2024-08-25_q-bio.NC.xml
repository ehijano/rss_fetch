<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Aug 2024 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Universal dimensions of visual representation</title>
      <link>https://arxiv.org/abs/2408.12804</link>
      <description>arXiv:2408.12804v1 Announce Type: new 
Abstract: Do neural network models of vision learn brain-aligned representations because they share architectural constraints and task objectives with biological vision or because they learn universal features of natural image processing? We characterized the universality of hundreds of thousands of representational dimensions from visual neural networks with varied construction. We found that networks with varied architectures and task objectives learn to represent natural images using a shared set of latent dimensions, despite appearing highly distinct at a surface level. Next, by comparing these networks with human brain representations measured with fMRI, we found that the most brain-aligned representations in neural networks are those that are universal and independent of a network's specific characteristics. Remarkably, each network can be reduced to fewer than ten of its most universal dimensions with little impact on its representational similarity to the human brain. These results suggest that the underlying similarities between artificial and biological vision are primarily governed by a core set of universal image representations that are convergently learned by diverse systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12804v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zirui Chen, Michael F. Bonner</dc:creator>
    </item>
    <item>
      <title>An Introduction to Cognidynamics</title>
      <link>https://arxiv.org/abs/2408.13112</link>
      <description>arXiv:2408.13112v1 Announce Type: new 
Abstract: This paper gives an introduction to \textit{Cognidynamics}, that is to the dynamics of cognitive systems driven by optimal objectives imposed over time when they interact either with a defined virtual or with a real-world environment. The proposed theory is developed in the general framework of dynamic programming which leads to think of computational laws dictated by classic Hamiltonian equations. Those equations lead to the formulation of a neural propagation scheme in cognitive agents modeled by dynamic neural networks which exhibits locality in both space and time, thus contributing the longstanding debate on biological plausibility of learning algorithms like Backpropagation. We interpret the learning process in terms of energy exchange with the environment and show the crucial role of energy dissipation and its links with focus of attention mechanisms and conscious behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13112v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Gori</dc:creator>
    </item>
    <item>
      <title>Multilevel Interpretability Of Artificial Neural Networks: Leveraging Framework And Methods From Neuroscience</title>
      <link>https://arxiv.org/abs/2408.12664</link>
      <description>arXiv:2408.12664v1 Announce Type: cross 
Abstract: As deep learning systems are scaled up to many billions of parameters, relating their internal structure to external behaviors becomes very challenging. Although daunting, this problem is not new: Neuroscientists and cognitive scientists have accumulated decades of experience analyzing a particularly complex system - the brain. In this work, we argue that interpreting both biological and artificial neural systems requires analyzing those systems at multiple levels of analysis, with different analytic tools for each level. We first lay out a joint grand challenge among scientists who study the brain and who study artificial neural networks: understanding how distributed neural mechanisms give rise to complex cognition and behavior. We then present a series of analytical tools that can be used to analyze biological and artificial neural systems, organizing those tools according to Marr's three levels of analysis: computation/behavior, algorithm/representation, and implementation. Overall, the multilevel interpretability framework provides a principled way to tackle neural system complexity; links structure, computation, and behavior; clarifies assumptions and research priorities at each level; and paves the way toward a unified effort for understanding intelligent systems, may they be biological or artificial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12664v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhonghao He, Jascha Achterberg, Katie Collins, Kevin Nejad, Danyal Akarca, Yinzhu Yang, Wes Gurnee, Ilia Sucholutsky, Yuhan Tang, Rebeca Ianov, George Ogden, Chole Li, Kai Sandbrink, Stephen Casper, Anna Ivanova, Grace W. Lindsay</dc:creator>
    </item>
    <item>
      <title>Bayesian Network Modeling of Causal Influence within Cognitive Domains and Clinical Dementia Severity Ratings for Western and Indian Cohorts</title>
      <link>https://arxiv.org/abs/2408.12669</link>
      <description>arXiv:2408.12669v1 Announce Type: cross 
Abstract: This study investigates the causal relationships between Clinical Dementia Ratings (CDR) and its six domain scores across two distinct aging datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Longitudinal Aging Study of India (LASI). Using Directed Acyclic Graphs (DAGs) derived from Bayesian network models, we analyze the dependencies among domain scores and their influence on the global CDR. Our approach leverages the PC algorithm to estimate the DAG structures for both datasets, revealing notable differences in causal relationships and edge strengths between the Western and Indian populations. The analysis highlights a stronger dependency of CDR scores on memory functions in both datasets, but with significant variations in edge strengths and node degrees. By contrasting these findings, we aim to elucidate population-specific differences and similarities in dementia progression, providing insights that could inform targeted interventions and improve understanding of dementia across diverse demographic contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12669v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wupadrasta Santosh Kumar, Sayali Rajendra Bhutare, Neelam Sinha, Thomas Gregor Issac</dc:creator>
    </item>
    <item>
      <title>Comparative prospects of imaging methods for whole-brain mammalian connectomics</title>
      <link>https://arxiv.org/abs/2405.10488</link>
      <description>arXiv:2405.10488v2 Announce Type: replace 
Abstract: Mammalian whole-brain connectomes at nanoscale synaptic resolution are a crucial ingredient for holistic understanding of brain function. Imaging these connectomes at sufficient resolution to densely reconstruct cellular morphology and synapses represents a longstanding goal in neuroscience. Although the technologies needed to reconstruct whole-brain connectomes have not yet reached full maturity, they are advancing rapidly enough that the mouse brain might be within reach in the near future. Detailed exploration of these technologies is warranted to help plan projects with varying goals and requirements. Whole-brain human connectomes remain a more distant goal yet are worthy of consideration to orient large-scale neuroscience program plans. Here, we quantitatively compare existing and emerging imaging technologies that have potential to enable whole-brain mammalian connectomics. We perform calculations on electron microscopy (EM) techniques and expansion microscopy coupled with light-sheet fluorescence microscopy (ExLSFM) methods. We consider techniques from the literature that have sufficiently high resolution to identify all synapses and sufficiently high speed to be relevant for whole mammalian brains. Each imaging modality comes with benefits and drawbacks, so we suggest that attacking the problem through multiple approaches could yield the best outcomes. We offer this analysis as a resource for those considering how to organize efforts towards imaging whole-brain mammalian connectomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10488v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Logan Thrasher Collins, Todd Huffman, Randal Koene</dc:creator>
    </item>
    <item>
      <title>Pseudo Channel: Time Embedding for Motor Imagery Decoding</title>
      <link>https://arxiv.org/abs/2405.15812</link>
      <description>arXiv:2405.15812v2 Announce Type: replace 
Abstract: Motor imagery (MI) based EEG represents a frontier in enabling direct neural control of external devices and advancing neural rehabilitation. This study introduces a novel time embedding technique, termed traveling-wave based time embedding, utilized as a pseudo channel to enhance the decoding accuracy of MI-EEG signals across various neural network architectures. Unlike traditional neural network methods that fail to account for the temporal dynamics in MI-EEG in individual difference, our approach captures time-related changes for different participants based on a priori knowledge. Through extensive experimentation with multiple participants, we demonstrate that this method not only improves classification accuracy but also exhibits greater adaptability to individual differences compared to position encoding used in Transformer architecture. Significantly, our results reveal that traveling-wave based time embedding crucially enhances decoding accuracy, particularly for participants typically considered "EEG-illiteracy". As a novel direction in EEG research, the traveling-wave based time embedding not only offers fresh insights for neural network decoding strategies but also expands new avenues for research into attention mechanisms in neuroscience and a deeper understanding of EEG signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15812v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengqing Miao, Meirong Zhao</dc:creator>
    </item>
    <item>
      <title>Processing, evaluating and understanding FMRI data with afni_proc.py</title>
      <link>https://arxiv.org/abs/2406.05248</link>
      <description>arXiv:2406.05248v3 Announce Type: replace 
Abstract: FMRI data are noisy, complicated to acquire, and typically go through many steps of processing before they are used in a study or clinical practice. Being able to visualize and understand the data from the start through the completion of processing, while being confident that each intermediate step was successful, is challenging. AFNI's afni_proc$.$py is a tool to create and run a processing pipeline for FMRI data. With its flexible features, afni_proc$.$py allows users to both control and evaluate their processing at a detailed level. It has been designed to keep users informed about all processing steps: it does not just process the data, but first outputs a fully commented processing script that the users can read, query, interpret and refer back to. Having this full provenance is important for being able to understand each step of processing; it also promotes transparency and reproducibility by keeping the record of individual-level processing and modeling specifics in a single, shareable place. Additionally, afni_proc$.$py creates pipelines that contain several automatic self-checks for potential problems during runtime. The output directory contains a dictionary of relevant quantities that can be programmatically queried for potential issues and a systematic, interactive quality control (QC) HTML. All of these features help users evaluate and understand their data and processing in detail. We describe these and other aspects of afni_proc$.$py here using a set of task-based and resting state FMRI example commands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05248v3</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard C. Reynolds, Daniel R. Glen, Gang Chen, Ziad S. Saad, Robert W. Cox, Paul A. Taylor</dc:creator>
    </item>
  </channel>
</rss>
