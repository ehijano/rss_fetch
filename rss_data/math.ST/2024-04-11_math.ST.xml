<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Apr 2024 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From naive trees to Random Forests: A general approach for proofing consistency of tree-based methods</title>
      <link>https://arxiv.org/abs/2404.06850</link>
      <description>arXiv:2404.06850v1 Announce Type: new 
Abstract: Tree-based methods such as Random Forests are learning algorithms that have become an integral part of the statistical toolbox. The last decade has shed some light on theoretical properties such as their consistency for regression tasks. However, the usual proofs assume normal error terms as well as an additive regression function and are rather technical. We overcome these issues by introducing a simple and catchy technique for proofing consistency under quite general assumptions. To this end, we introduce a new class of naive trees, which do the subspacing completely at random and independent of the data. We then give a direct proof of their consistency. Using them to bound the error of more complex tree-based approaches such as univariate and multivariate CARTs, Extra Randomized Trees, or Random Forests, we deduce the consistency of all of them. Since naive trees appear to be too simple for actual application, we further analyze their finite sample properties in a simulation and small benchmark study. We find a slow convergence speed and a rather poor predictive performance. Based on these results, we finally discuss to what extent consistency proofs help to justify the application of complex learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06850v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nico F\"oge, Markus Pauly, Lena Schmid, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>On weighted failure rate, its means and associated quantile version</title>
      <link>https://arxiv.org/abs/2404.06909</link>
      <description>arXiv:2404.06909v1 Announce Type: new 
Abstract: In this paper, we define weighted failure rate and their different means from the stand point of an application. We begin by emphasizing that the formation of n independent component series system having weighted failure rates with sum of weight functions being unity is same as a mixture of n distributions. We derive some parametric and non-parametric characterization results. We discuss on the form invariance property of baseline failure rate for a specific choice of weight function. Some bounds on means of aging functions are obtained. Here, we establish that weighted IFRA class is not closed under formation of coherent systems unlike the IFRA class. An interesting application of the present work is credited to the fact that the quantile version of means of failure rate is obtained as a special case of weighted means of failure rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06909v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subarna Bhattacharjee, S. M. Sunoj, Sabana Anwar</dc:creator>
    </item>
    <item>
      <title>A New Statistic for Testing Covariance Equality in High-Dimensional Gaussian Low-Rank Models</title>
      <link>https://arxiv.org/abs/2404.07100</link>
      <description>arXiv:2404.07100v1 Announce Type: new 
Abstract: In this paper, we consider the problem of testing equality of the covariance matrices of L complex Gaussian multivariate time series of dimension $M$ . We study the special case where each of the L covariance matrices is modeled as a rank K perturbation of the identity matrix, corresponding to a signal plus noise model. A new test statistic based on the estimates of the eigenvalues of the different covariance matrices is proposed. In particular, we show that this statistic is consistent and with controlled type I error in the high-dimensional asymptotic regime where the sample sizes $N_1,\ldots,N_L$ of each time series and the dimension $M$ both converge to infinity at the same rate, while $K$ and $L$ are kept fixed. We also provide some simulations on synthetic and real data (SAR images) which demonstrate significant improvements over some classical methods such as the GLRT, or other alternative methods relevant for the high-dimensional regime and the low-rank model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07100v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSP.2024.3382476</arxiv:DOI>
      <dc:creator>R\'emi Beisson, Pascal Vallet, Audrey Giremus, Guillaume Ginolhac</dc:creator>
    </item>
    <item>
      <title>Learning of deep convolutional network image classifiers via stochastic gradient descent and over-parametrization</title>
      <link>https://arxiv.org/abs/2404.07128</link>
      <description>arXiv:2404.07128v1 Announce Type: new 
Abstract: Image classification from independent and identically distributed random variables is considered. Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer. Here all the weights are learned by stochastic gradient descent. A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network. In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07128v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Kohler, Adam Krzyzak, Alisha S\"anger</dc:creator>
    </item>
    <item>
      <title>Lecture notes on rough paths and applications to machine learning</title>
      <link>https://arxiv.org/abs/2404.06583</link>
      <description>arXiv:2404.06583v1 Announce Type: cross 
Abstract: These notes expound the recent use of the signature transform and rough path theory in data science and machine learning. We develop the core theory of the signature from first principles and then survey some recent popular applications of this approach, including signature-based kernel methods and neural rough differential equations. The notes are based on a course given by the two authors at Imperial College London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06583v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Cass, Cristopher Salvi</dc:creator>
    </item>
    <item>
      <title>Covariance Regression with High-Dimensional Predictors</title>
      <link>https://arxiv.org/abs/2404.06701</link>
      <description>arXiv:2404.06701v1 Announce Type: cross 
Abstract: In the high-dimensional landscape, addressing the challenges of covariance regression with high-dimensional covariates has posed difficulties for conventional methodologies. This paper addresses these hurdles by presenting a novel approach for high-dimensional inference with covariance matrix outcomes. The proposed methodology is illustrated through its application in elucidating brain coactivation patterns observed in functional magnetic resonance imaging (fMRI) experiments and unraveling complex associations within anatomical connections between brain regions identified through diffusion tensor imaging (DTI). In the pursuit of dependable statistical inference, we introduce an integrative approach based on penalized estimation. This approach combines data splitting, variable selection, aggregation of low-dimensional estimators, and robust variance estimation. It enables the construction of reliable confidence intervals for covariate coefficients, supported by theoretical confidence levels under specified conditions, where asymptotic distributions are provided. Through various types of simulation studies, the proposed approach performs well for covariance regression in the presence of high-dimensional covariates. This innovative approach is applied to the Lifespan Human Connectome Project (HCP) Aging Study, which aims to uncover a typical aging trajectory and variations in the brain connectome among mature and older adults. The proposed approach effectively identifies brain networks and associated predictors of white matter integrity, aligning with established knowledge of the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06701v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng He, Changliang Zou, Yi Zhao</dc:creator>
    </item>
    <item>
      <title>A Copula Graphical Model for Multi-Attribute Data using Optimal Transport</title>
      <link>https://arxiv.org/abs/2404.06735</link>
      <description>arXiv:2404.06735v1 Announce Type: cross 
Abstract: Motivated by modern data forms such as images and multi-view data, the multi-attribute graphical model aims to explore the conditional independence structure among vectors. Under the Gaussian assumption, the conditional independence between vectors is characterized by blockwise zeros in the precision matrix. To relax the restrictive Gaussian assumption, in this paper, we introduce a novel semiparametric multi-attribute graphical model based on a new copula named Cyclically Monotone Copula. This new copula treats the distribution of the node vectors as multivariate marginals and transforms them into Gaussian distributions based on the optimal transport theory. Since the model allows the node vectors to have arbitrary continuous distributions, it is more flexible than the classical Gaussian copula method that performs coordinatewise Gaussianization. We establish the concentration inequalities of the estimated covariance matrices and provide sufficient conditions for selection consistency of the group graphical lasso estimator. For the setting with high-dimensional attributes, a {Projected Cyclically Monotone Copula} model is proposed to address the curse of dimensionality issue that arises from solving high-dimensional optimal transport problems. Numerical results based on synthetic and real data show the efficiency and flexibility of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06735v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Zhang, Bing Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>A new way to evaluate G-Wishart normalising constants via Fourier analysis</title>
      <link>https://arxiv.org/abs/2404.06803</link>
      <description>arXiv:2404.06803v1 Announce Type: cross 
Abstract: The G-Wishart distribution is an essential component for the Bayesian analysis of Gaussian graphical models as the conjugate prior for the precision matrix. Evaluating the marginal likelihood of such models usually requires computing high-dimensional integrals to determine the G-Wishart normalising constant. Closed-form results are known for decomposable or chordal graphs, while an explicit representation as a formal series expansion has been derived recently for general graphs. The nested infinite sums, however, do not lend themselves to computation, remaining of limited practical value. Borrowing techniques from random matrix theory and Fourier analysis, we provide novel exact results well suited to the numerical evaluation of the normalising constant for a large class of graphs beyond chordal graphs. Furthermore, they open new possibilities for developing more efficient sampling schemes for Bayesian inference of Gaussian graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06803v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ching Wong, Giusi Moffa, Jack Kuipers</dc:creator>
    </item>
    <item>
      <title>Quiver Laplacians and Feature Selection</title>
      <link>https://arxiv.org/abs/2404.06993</link>
      <description>arXiv:2404.06993v1 Announce Type: cross 
Abstract: The challenge of selecting the most relevant features of a given dataset arises ubiquitously in data analysis and dimensionality reduction. However, features found to be of high importance for the entire dataset may not be relevant to subsets of interest, and vice versa. Given a feature selector and a fixed decomposition of the data into subsets, we describe a method for identifying selected features which are compatible with the decomposition into subsets. We achieve this by re-framing the problem of finding compatible features to one of finding sections of a suitable quiver representation. In order to approximate such sections, we then introduce a Laplacian operator for quiver representations valued in Hilbert spaces. We provide explicit bounds on how the spectrum of a quiver Laplacian changes when the representation and the underlying quiver are modified in certain natural ways. Finally, we apply this machinery to the study of peak-calling algorithms which measure chromatin accessibility in single-cell data. We demonstrate that eigenvectors of the associated quiver Laplacian yield locally and globally compatible features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06993v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.RT</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Otto Sumray, Heather A. Harrington, Vidit Nanda</dc:creator>
    </item>
    <item>
      <title>To impute or not to? Testing multivariate normality on incomplete dataset: Revisiting the BHEP test</title>
      <link>https://arxiv.org/abs/2404.07136</link>
      <description>arXiv:2404.07136v1 Announce Type: cross 
Abstract: In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random. Our objective is twofold: first, to gain insight into the asymptotic behavior of BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of test statistic under these approaches. It is observed that under the imputation approach, the affine invariance of test statistics is not preserved. To address this issue, we propose an appropriate bootstrap algorithm for approximating p-values. Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07136v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danijel Aleksi\'c, Bojana Milo\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>High-dimensional copula-based Wasserstein dependence</title>
      <link>https://arxiv.org/abs/2404.07141</link>
      <description>arXiv:2404.07141v1 Announce Type: cross 
Abstract: We generalize 2-Wasserstein dependence coefficients to measure dependence between a finite number of random vectors. This generalization includes theoretical properties, and in particular focuses on an interpretation of maximal dependence and an asymptotic normality result for a proposed semi-parametric estimator under a Gaussian copula assumption. In addition, we discuss general axioms for dependence measures between multiple random vectors, other plausible normalizations, and various examples. Afterwards, we look into plug-in estimators based on penalized empirical covariance matrices in order to deal with high dimensionality issues and take possible marginal independencies into account by inducing (block) sparsity. The latter ideas are investigated via a simulation study, considering other dependence coefficients as well. We illustrate the use of the developed methods in two real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07141v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven De Keyser, Irene Gijbels</dc:creator>
    </item>
    <item>
      <title>Nearest neighbor empirical processes</title>
      <link>https://arxiv.org/abs/2110.15083</link>
      <description>arXiv:2110.15083v4 Announce Type: replace 
Abstract: In the regression framework, the empirical measure based on the responses resulting from the nearest neighbors, among the covariates, to a given point $x$ is introduced and studied as a central statistical quantity. First, the associated empirical process is shown to satisfy a uniform central limit theorem under a local bracketing entropy condition on the underlying class of functions reflecting the localizing nature of the nearest neighbor algorithm. Second a uniform non-asymptotic bound is established under a well-known condition, often referred to as Vapnik-Chervonenkis, on the uniform entropy numbers. The covariance of the Gaussian limit obtained in the uniform central limit theorem is simply equal to the conditional covariance operator given the covariate value. This suggests the possibility of using standard formulas to estimate the variance by using only the nearest neighbors instead of the full data. This is illustrated on two problems: the estimation of the conditional cumulative distribution function and local linear regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.15083v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Portier</dc:creator>
    </item>
    <item>
      <title>PDE characterisation of geometric distribution functions and quantiles</title>
      <link>https://arxiv.org/abs/2208.11551</link>
      <description>arXiv:2208.11551v5 Announce Type: replace 
Abstract: We show that in any Euclidean space, an arbitrary probability measure can be reconstructed explicitly by its geometric (or spatial) distribution function. The reconstruction takes the form of a (potentially fractional) linear PDE, where the differential operator is given in closed form. This result implies that, contrary to a common belief in the statistical depth community, geometric cdf's in principle provide exact control over the probability content of all depth regions. We present a comprehensive study of the regularity of the geometric cdf, and show that a continuous density in general does not give rise to a geometric cdf with enough regularity to reconstruct the density pointwise. Surprisingly, we prove that the reconstruction displays different behaviours in odd and even dimension: it is local in odd dimension and completely nonlocal in even dimension. We investigate this issue and provide a partial counterpart for even dimensions, and establish a general representation formula of the geometric cdf of spherically symmetric probability laws in odd dimensions. We provide explicit examples of the reconstruction of a density from its geometric cdf in dimension 2 and 3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11551v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dimitri Konen</dc:creator>
    </item>
    <item>
      <title>Trade-off between predictive performance and FDR control for high-dimensional Gaussian model selection</title>
      <link>https://arxiv.org/abs/2302.01831</link>
      <description>arXiv:2302.01831v3 Announce Type: replace 
Abstract: In the context of the high-dimensional Gaussian linear regression for ordered variables, we study the variable selection procedure via the minimization of the penalized least-squares criterion. We focus on model selection where the penalty function depends on an unknown multiplicative constant commonly calibrated for prediction. We propose a new proper calibration of this hyperparameter to simultaneously control predictive risk and false discovery rate. We obtain non-asymptotic bounds on the False Discovery Rate with respect to the hyperparameter and we provide an algorithm to calibrate it. This algorithm is based on quantities that can typically be observed in real data applications. The algorithm is validated in an extensive simulation study and is compared with some existing variable selection procedures. Finally, we study an extension of our approach to the case in which an ordering of the variables is not available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01831v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Perrine Lacroix, Marie-Laure Martin</dc:creator>
    </item>
    <item>
      <title>Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity</title>
      <link>https://arxiv.org/abs/2305.04174</link>
      <description>arXiv:2305.04174v2 Announce Type: replace 
Abstract: Treatment effect estimation under unconfoundedness is a fundamental task in causal inference. In response to the challenge of analyzing high-dimensional datasets collected in substantive fields such as epidemiology, genetics, economics, and social sciences, many methods for treatment effect estimation with high-dimensional nuisance parameters (the outcome regression and the propensity score) have been developed in recent years. However, it is still unclear what is the necessary and sufficient sparsity condition on the nuisance parameters for the treatment effect to be $\sqrt{n}$-estimable. In this paper, we propose a new Double-Calibration strategy that corrects the estimation bias of the nuisance parameter estimates computed by regularized high-dimensional techniques and demonstrate that the corresponding Doubly-Calibrated estimator achieves $1 / \sqrt{n}$-rate as long as one of the nuisance parameters is sparse with sparsity below $\sqrt{n} / \log p$, where $p$ denotes the ambient dimension of the covariates, whereas the other nuisance parameter can be arbitrarily complex and completely misspecified. The Double-Calibration strategy can also be applied to settings other than treatment effect estimation, e.g. regression coefficient estimation in the presence of diverging number of controls in a semiparametric partially linear model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04174v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Liu, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Partition-Insensitive Parallel ADMM Algorithm for High-dimensional Linear Models</title>
      <link>https://arxiv.org/abs/2308.14557</link>
      <description>arXiv:2308.14557v3 Announce Type: replace 
Abstract: The parallel alternating direction method of multipliers (ADMM) algorithms have gained popularity in statistics and machine learning due to their efficient handling of large sample data problems. However, the parallel structure of these algorithms, based on the consensus problem, can lead to an excessive number of auxiliary variables when applied to highdimensional data, resulting in large computational burden. In this paper, we propose a partition-insensitive parallel framework based on the linearized ADMM (LADMM) algorithm and apply it to solve nonconvex penalized high-dimensional regression problems. Compared to existing parallel ADMM algorithms, our algorithm does not rely on the consensus problem, resulting in a significant reduction in the number of variables that need to be updated at each iteration. It is worth noting that the solution of our algorithm remains largely unchanged regardless of how the total sample is divided, which is known as partition-insensitivity. Furthermore, under some mild assumptions, we prove the convergence of the iterative sequence generated by our parallel algorithm. Numerical experiments on synthetic and real datasets demonstrate the feasibility and validity of the proposed algorithm. We provide a publicly available R software package to facilitate the implementation of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14557v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofei Wu, Jiancheng Jiang, Zhimin Zhang</dc:creator>
    </item>
    <item>
      <title>A compromise criterion for weighted least squares estimates</title>
      <link>https://arxiv.org/abs/2404.00753</link>
      <description>arXiv:2404.00753v2 Announce Type: replace 
Abstract: When independent errors in a linear model have non-identity covariance, the ordinary least squares estimate of the model coefficients is less efficient than the weighted least squares estimate. However, the practical application of weighted least squares is challenging due to its reliance on the unknown error covariance matrix. Although feasible weighted least squares estimates, which use an approximation of this matrix, often outperform the ordinary least squares estimate in terms of efficiency, this is not always the case. In some situations, feasible weighted least squares can be less efficient than ordinary least squares. The comparison between these two estimates has significant implications for the application of regression analysis in varied fields, yet such a comparison remains an unresolved challenge despite its seemingly straightforward nature. In this study, we directly address this challenge by identifying the conditions under which feasible weighted least squares estimates using fixed weights demonstrate greater efficiency than the ordinary least squares estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how certain robust regression estimates behave with respect to the linear model with normal errors of unequal variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00753v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Bryan, Haibo Zhou, Didong Li</dc:creator>
    </item>
    <item>
      <title>Least Squares-Based Permutation Tests in Time Series</title>
      <link>https://arxiv.org/abs/2404.06238</link>
      <description>arXiv:2404.06238v2 Announce Type: replace 
Abstract: This paper studies permutation tests for regression parameters in a time series setting, where the time series is assumed stationary but may exhibit an arbitrary (but weak) dependence structure. In such a setting, it is perhaps surprising that permutation tests can offer any type of inference guarantees, since permuting of covariates can destroy its relationship with the response. Indeed, the fundamental assumption of exchangeability of errors required for the finite-sample exactness of permutation tests, can easily fail. However, we show that permutation tests may be constructed which are asymptotically valid for a wide class of stationary processes, but remain exact when exchangeability holds. We also consider the problem of testing for no monotone trend and we construct asymptotically valid permutation tests in this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06238v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>Permutation Testing for Monotone Trend</title>
      <link>https://arxiv.org/abs/2404.06239</link>
      <description>arXiv:2404.06239v2 Announce Type: replace 
Abstract: In this paper, we consider the fundamental problem of testing for monotone trend in a time series. While the term "trend" is commonly used and has an intuitive meaning, it is first crucial to specify its exact meaning in a hypothesis testing context. A commonly used well-known test is the Mann-Kendall test, which we show does not offer Type 1 error control even in large samples. On the other hand, by an appropriate studentization of the Mann-Kendall statistic, we construct permutation tests that offer asymptotic error control quite generally, but retain the exactness property of permutation tests for i.i.d. observations. We also introduce "local" Mann-Kendall statistics as a means of testing for local rather than global trend in a time series. Similar properties of permutation tests are obtained for these tests as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06239v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>On Regression in Extreme Regions</title>
      <link>https://arxiv.org/abs/2303.03084</link>
      <description>arXiv:2303.03084v2 Announce Type: replace-cross 
Abstract: The statistical learning problem consists in building a predictive function $\hat{f}$ based on independent copies of $(X,Y)$ so that $Y$ is approximated by $\hat{f}(X)$ with minimum (squared) error. Motivated by various applications, special attention is paid here to the case of extreme (i.e. very large) observations $X$. Because of their rarity, the contributions of such observations to the (empirical) error is negligible, and the predictive performance of empirical risk minimizers can be consequently very poor in extreme regions. In this paper, we develop a general framework for regression on extremes. Under appropriate regular variation assumptions regarding the pair $(X,Y)$, we show that an asymptotic notion of risk can be tailored to summarize appropriately predictive performance in extreme regions. It is also proved that minimization of an empirical and nonasymptotic version of this 'extreme risk', based on a fraction of the largest observations solely, yields good generalization capacity. In addition, numerical results providing strong empirical evidence of the relevance of the approach proposed are displayed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03084v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huet, Stephan Cl\'emen\c{c}on, Anne Sabourin</dc:creator>
    </item>
    <item>
      <title>A recursion formula for mixed trace moments of isotropic Wishart matrices and the Gaussian unitary/orthogonal ensembles</title>
      <link>https://arxiv.org/abs/2311.04003</link>
      <description>arXiv:2311.04003v2 Announce Type: replace-cross 
Abstract: Exact recursion formulas for mixed moments of four fundamental random matrix ensembles are derived. The reason such recursive formulas are possible is closely related to properties of polygon gluings studied by Harer and Zagier as well as Akhmedov and Shakirov. The proofs of the formulas are direct and written in such a way that they do not rely on understanding of polygon gluings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04003v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Deitmar</dc:creator>
    </item>
    <item>
      <title>Theory of Compression Channels for Post-selected Quantum Metrology</title>
      <link>https://arxiv.org/abs/2311.06679</link>
      <description>arXiv:2311.06679v2 Announce Type: replace-cross 
Abstract: Post-selected quantum metrological scheme is especially advantageous when the final measurements are either very noisy or expensive in practical experiments. In this work, we put forward a general theory on the compression channels in post-selected metrology. We define the basic notions characterizing the compression quality and illuminate the underlying structure of lossless compression channels. Previous experiments on post-selected optical phase estimation and weak-value amplification are shown to be particular cases of this general theory. Furthermore, for two categories of bipartite systems, we show that the compression loss can be made arbitrarily small even when the compression channel acts only on one subsystem. These findings can be employed to distribute quantum measurements so that the measurement noise and cost are dramatically reduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06679v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Yang</dc:creator>
    </item>
  </channel>
</rss>
