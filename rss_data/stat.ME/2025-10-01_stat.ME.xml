<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Oct 2025 04:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Remote Auditing: Design-based Tests of Randomization, Selection, and Missingness with Broadly Accessible Satellite Imagery</title>
      <link>https://arxiv.org/abs/2510.00128</link>
      <description>arXiv:2510.00128v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) are the benchmark for causal inference, yet field implementation can deviate. We here present a remote audit - a design-based, preregistrable diagnostic that uses only pre-treatment satellite imagery to test whether assignment is independent of local conditions. The conditional randomization test of the remote audit evaluates whether treatment assignment is more predictable from pre-treatment satellite features than expected under the experiment's registered mechanism, providing a finite-sample valid, design-based diagnostic that requires no parametric assumptions. The procedure is finite-sample valid, honors blocks and clusters, and controls multiplicity across image models and resolutions via a max-statistic. We illustrate with two RCTs: Uganda's Youth Opportunities Program, where the audit corroborates randomization and flags selection and missing-data risks; and a school-based trial in Bangladesh, where assignment is highly predictable from pre-treatment features relative to the stated design, consistent with independent concerns about irregularities. Remote audits complement balance tests, lower early-stage costs, and enable rapid design checks when baseline surveys are expensive or infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00128v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Dimension Reduction for Characterizing Sexual Dimorphism in Biomechanics of the Temporomandibular Joint</title>
      <link>https://arxiv.org/abs/2510.00217</link>
      <description>arXiv:2510.00217v1 Announce Type: new 
Abstract: Sexual dimorphism is a critical factor in many biological and medical research fields. In biomechanics and bioengineering, understanding sex differences is crucial for studying musculoskeletal conditions such as temporomandibular disorder (TMD). This paper focuses on the association between the craniofacial skeletal morphology and temporomandibular joint (TMJ) related masticatory muscle attachments to discern sex differences. Data were collected from 10 male and 11 female cadaver heads to investigate sex-specific relationships between the skull and muscles. We propose a conditional cross-covariance reduction (CCR) model, designed to examine the dynamic association between two sets of random variables conditioned on a third binary variable (e.g., sex), highlighting the most distinctive sex-related relationships between skull and muscle attachments in the human cadaver data. Under the CCR model, we employ a sparse singular value decomposition algorithm and introduce a sequential permutation for selecting sparsity (SPSS) method to select important variables and to determine the optimal number of selected variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00217v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sung Hee Park, Xin Zhang, Elizabeth Slate, Shuchun Sun, Hai Yao</dc:creator>
    </item>
    <item>
      <title>Assumption-lean Inference for Network-linked Data</title>
      <link>https://arxiv.org/abs/2510.00287</link>
      <description>arXiv:2510.00287v1 Announce Type: new 
Abstract: We consider statistical inference for network-linked regression problems, where covariates may include network summary statistics computed for each node. In settings involving network data, it is often natural to posit that latent variables govern connection probabilities in the graph. Since the presence of these latent features makes classical regression assumptions even less tenable, we propose an assumption-lean framework for linear regression with jointly exchangeable regression arrays. We establish an analog of the Aldous-Hoover representation for such arrays, which may be of independent interest. Moreover, we consider two different projection parameters as potential targets and establish conditions under which asymptotic normality and bootstrap consistency hold when commonly used network statistics, including local subgraph frequencies and spectral embeddings, are used as covariates. In the case of linear regression with local count statistics, we show that a bias-corrected estimator allows one to target a more natural inferential target under weaker sparsity conditions compared to the OLS estimator. Our inferential tools are illustrated using both simulated data and real data related to the academic climate of elementary schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00287v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Nilanjan Chakraborty, Robert Lunde</dc:creator>
    </item>
    <item>
      <title>Structural Refinement of Bayesian Networks for Efficient Model Parameterisation</title>
      <link>https://arxiv.org/abs/2510.00334</link>
      <description>arXiv:2510.00334v1 Announce Type: new 
Abstract: Many Bayesian network modelling applications suffer from the issue of data scarcity. Hence the use of expert judgement often becomes necessary to determine the parameters of the conditional probability tables (CPTs) throughout the network. There are usually a prohibitively large number of these parameters to determine, even when complementing any available data with expert judgements. To address this challenge, a number of CPT approximation methods have been developed that reduce the quantity and complexity of parameters needing to be determined to fully parameterise a Bayesian network. This paper provides a review of a variety of structural refinement methods that can be used in practice to efficiently approximate a CPT within a Bayesian network. We not only introduce and discuss the intrinsic properties and requirements of each method, but we evaluate each method through a worked example on a Bayesian network model of cardiovascular risk assessment. We conclude with practical guidance to help Bayesian network practitioners choose an alternative approach when direct parameterisation of a CPT is infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00334v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieran Drury, Martine J. Barons, Jim Q. Smith</dc:creator>
    </item>
    <item>
      <title>An Accurate Standard Error Estimation for Quadratic Exponential Logistic Regressions by Applying Generalized Estimating Equations to Pseudo-Likelihoods</title>
      <link>https://arxiv.org/abs/2510.00431</link>
      <description>arXiv:2510.00431v1 Announce Type: new 
Abstract: For a set of binary response variables, conditional mean models characterize the expected value of a response variable given the others and are popularly applied in longitudinal and network data analyses. The quadratic exponential binary distribution is a natural choice in this context. However, maximum likelihood estimation of this distribution is computationally demanding due to its intractable normalizing constant, while the pseudo-likelihood, though computationally convenient, tends to severely underestimate the standard errors. In this work, we investigate valid estimation methods for the quadratic exponential binary distribution and its regression counterpart. We show that, when applying the generalized estimating equations to the pseudo-likelihood, using the independence working correlation yields consistent estimates, whereas using dependent structures, such as compound symmetric or autoregressive correlations, may introduce non-ignorable biases. Theoretical properties are derived, supported by simulation studies. For illustration, we apply the proposed approach to the carcinogenic toxicity of chemicals data and the constitutional court opinion wringing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00431v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ong Wei Yong, Lee Shao-Man, Hsueh Chia-Ming, Chang Sheng-Mao</dc:creator>
    </item>
    <item>
      <title>Empirical partially Bayes two sample testing</title>
      <link>https://arxiv.org/abs/2510.00432</link>
      <description>arXiv:2510.00432v1 Announce Type: new 
Abstract: A common task in high-throughput biology is to test for differences in means between two samples across thousands of features (e.g., genes or proteins), often with only a handful of replicates per sample. Moderated t-tests handle this problem by assuming normality and equal variances, and by applying the empirical partially Bayes principle: a prior is posited and estimated for the nuisance parameters (variances) but not for the primary parameters (means). This approach has been highly successful in genomics, yet the equal variance assumption is often violated in practice. Meanwhile, Welch's unequal variance t-test with few replicates suffers from inflated type-I error and low power. Taking inspiration from moderated t-tests, we extend the empirical partially Bayes paradigm to two-sample testing with unequal variances. We develop two procedures: one that models the ratio of the two sample-specific variances and another that models the two variances jointly, with prior distributions estimated by nonparametric maximum likelihood. Our empirical partially Bayes methods yield p-values that are asymptotically uniform as the number of features grows while the number of replicates remains fixed, ensuring asymptotic type-I error control. Simulations and applications to genomic data demonstrate substantial gains in power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00432v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanyi Ling, Wufang Hong, Nikolaos Ignatiadis</dc:creator>
    </item>
    <item>
      <title>A Data-Adaptive Factor Model Using Composite Quantile Approach</title>
      <link>https://arxiv.org/abs/2510.00558</link>
      <description>arXiv:2510.00558v1 Announce Type: new 
Abstract: This paper proposes a data-adaptive factor model (DAFM), a novel framework for extracting common factors that explain the structures of high-dimensional data. DAFM adopts a composite quantile strategy to adaptively capture the full distributional structure of the data, thereby enhancing estimation accuracy and revealing latent patterns that are invisible to conventional factor models. In this paper, we develop a practical algorithm for estimating DAFM by minimizing an objective function based on a weighted average of check functions across quantiles. We also establish the theoretical properties of the estimators, including their consistency and convergence rates. Furthermore, we derive their asymptotic distributions by introducing approximated estimators from a kernel-smoothed objective function, and propose two consistent methods for determining the number of factors. Simulation studies demonstrate that DAFM outperforms existing factor models across different data distributions, and real data analyses on volatility and forecasting further validate its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00558v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seeun Park, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>A Weighted Regression Approach to Break-Point Detection in Panel Data</title>
      <link>https://arxiv.org/abs/2510.00598</link>
      <description>arXiv:2510.00598v1 Announce Type: new 
Abstract: New procedures for detecting a change in the cross-sectional mean of panel data are proposed. The procedures rely on estimating nuisance parameters using certain cross-sectional means across panels using a weighted least squares regression. In the case of weak cross-sectional dependence between panels, we show how test statistics can be constructed to have a limit null distribution not depending on any choice of bandwidths typically needed to estimate the long-run variances of the panel errors. The theoretical assertions are derived for general choices of the regression weights, and it is shown that consistent test procedures can be obtained from the proposed process. The theoretical results are extended to the case where strong cross-sectional dependence exist between panels. The paper concludes with a numerical study illustrating the behavior of several special cases of the test procedure in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00598v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Charl Pretorius, Heinrich Roodt</dc:creator>
    </item>
    <item>
      <title>Spatial Gaussian fields for complex areas with application to marine megafauna conservation</title>
      <link>https://arxiv.org/abs/2510.00611</link>
      <description>arXiv:2510.00611v1 Announce Type: new 
Abstract: Spatial Gaussian fields (SGFs) are widely employed in modeling the distributions of marine megafauna, yet they traditionally rely on assumptions of isotropy and stationarity, conditions that often prove unrealistic in complex ecological environments featuring coastlines, islands, and depth gradients acting as partial movement barriers. Existing spatial models typically treat these barriers as either fully impermeable, completely blocking species movement and dispersal, or entirely absent, which inadequately represents most real-world scenarios. To address this limitation, we introduce the Transparent Barrier Model, an extension of spatial Gaussian fields that explicitly incorporates barriers with varying levels of permeability. The model assigns spatially varying range parameters to distinct barrier regions, allowing ecological and geographical knowledge about barrier permeability to directly inform model specifications. This approach maintains computational efficiency by utilizing the integrated nested Laplace approximation (INLA) framework combined with stochastic partial differential equations (SPDEs), ensuring feasible application even in large, complex spatial domains.We demonstrate the practical utility and flexibility of the Transparent Barrier Model through its application to dugong (Dugong dugon) distribution data from the Red Sea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00611v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martina Le-Bert Heyl, Janet van Niekerk, Haavard Rue</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control via Bayesian Mirror Statistic</title>
      <link>https://arxiv.org/abs/2510.00875</link>
      <description>arXiv:2510.00875v1 Announce Type: new 
Abstract: Simultaneously performing variable selection and inference in high-dimensional models is an open challenge in statistics and machine learning. The increasing availability of vast amounts of variables requires the adoption of specific statistical procedures to accurately select the most important predictors in a high-dimensional space, while being able to control some form of selection error. In this work we adapt the Mirror Statistic approach to False Discovery Rate (FDR) control into a Bayesian modelling framework. The Mirror Statistic, developed in the classic frequentist statistical framework, is a flexible method to control FDR, which only requires mild model assumptions, but requires two sets of independent regression coefficient estimates, usually obtained after splitting the original dataset. Here we propose to rely on a Bayesian formulation of the model and use the posterior distributions of the coefficients of interest to build the Mirror Statistic and effectively control the FDR without the need to split the data. Moreover, the method is very flexible since it can be used with continuous and discrete outcomes and more complex predictors, such as with mixed models. We keep the approach scalable to high-dimensions by relying on Automatic Differentiation Variational Inference and fully continuous prior choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00875v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Molinari, Magne Thoresen</dc:creator>
    </item>
    <item>
      <title>How can the use of different modes of survey data collection introduce bias? A simple introduction to mode effects using directed acyclic graphs (DAGs)</title>
      <link>https://arxiv.org/abs/2510.00900</link>
      <description>arXiv:2510.00900v1 Announce Type: new 
Abstract: Survey data are self-reported data collected directly from respondents by a questionnaire or an interview and are commonly used in epidemiology. Such data are traditionally collected via a single mode (e.g. face-to-face interview alone), but use of mixed-mode designs (e.g. offering face-to-face interview or online survey) has become more common. This introduces two key challenges. First, individuals may respond differently to the same question depending on the mode; these differences due to measurement are known as 'mode effects'. Second, different individuals may participate via different modes; these differences in sample composition between modes are known as 'mode selection'. Where recognised, mode effects are often handled by straightforward approaches such as conditioning on survey mode. However, while reducing mode effects, this and other equivalent approaches may introduce collider bias in the presence of mode selection. The existence of mode effects and the consequences of na\"ive conditioning may be underappreciated in epidemiology. This paper offers a simple introduction to these challenges using directed acyclic graphs by exploring a range of possible data structures. We discuss the potential implications of using conditioning- or imputation-based approaches and outline the advantages of quantitative bias analyses for dealing with mode effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00900v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgia D Tomova, Richard J Silverwood, Peter WG Tennant, Liam Wright</dc:creator>
    </item>
    <item>
      <title>An alternative bootstrap procedure for factor-augmented regression models</title>
      <link>https://arxiv.org/abs/2510.00947</link>
      <description>arXiv:2510.00947v1 Announce Type: new 
Abstract: In this paper, we propose a novel bootstrap algorithm that is more efficient than existing methods for approximating the distribution of the factor-augmented regression estimator for a rotated parameter vector. The regression is augmented by $r$ factors extracted from a large panel of $N$ variables observed over $T$ time periods. We consider general weak factor (WF) models with $r$ signal eigenvalues that may diverge at different rates, $N^{\alpha _{k}}$, where $0&lt;\alpha _{k}\leq 1$ for $k=1,2,...,r$. We establish the asymptotic validity of our bootstrap method using not only the conventional data-dependent rotation matrix $\hat{\bH}$, but also an alternative data-dependent rotation matrix, $\hat{\bH}_q$, which typically exhibits smaller asymptotic bias and achieves a faster convergence rate. Furthermore, we demonstrate the asymptotic validity of the bootstrap under a purely signal-dependent rotation matrix ${\bH}$, which is unique and can be regarded as the population analogue of both $\hat{\bH}$ and $\hat{\bH}_q$. Experimental results provide compelling evidence that the proposed bootstrap procedure achieves superior performance relative to the existing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00947v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyun Jiang, Takashi Yamagata</dc:creator>
    </item>
    <item>
      <title>Local aggregate multiscale processes: A scalable, machine-learning-compatible spatial model</title>
      <link>https://arxiv.org/abs/2510.00968</link>
      <description>arXiv:2510.00968v1 Announce Type: new 
Abstract: This study develops the Local Aggregate Multiscale Process (LAMP), a scalable and machine-learning-compatible alternative to conventional spatial Gaussian processes (GPs, or kriging). Unlike conventional covariance-based spatial models, LAMP represents spatial processes by a multiscale ensemble of local models, inspired by geographically weighted regression. To ensure stable model training, larger-scale patterns that are easier to learn are modeled first, followed by smaller-scale patterns, with training terminated once the validation score stops improving. The training procedure, which is based on holdout validation, is easily integrated with other machine learning algorithms (e.g., random forests and neural networks). LAMP training is computationally efficient as it avoids explicit matrix inversion, a major computational bottleneck in conventional GPs. Comparative Monte Carlo experiments demonstrate that LAMP, as well as its integration with random forests, achieves superior predictive performance compared to existing models. Finally, we apply the proposed methods to an analysis of residential land prices in the Tokyo metropolitan area, Japan.
  The R code is available from available from https://github.com/dmuraka/spLAMP_dev_version/tree/main</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00968v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daisuke Murakami, Alexis Comber, Takahiro Yoshida, Narumasa Tsutsumida, Chris Brunsdon, Tomoki Nakaya</dc:creator>
    </item>
    <item>
      <title>Rapid Scaling of Compositional Uncertainty from Sample to Population Levels</title>
      <link>https://arxiv.org/abs/2510.00980</link>
      <description>arXiv:2510.00980v1 Announce Type: new 
Abstract: Understanding population composition is essential across ecological, evolutionary, conservation, and resource management contexts. Modern methods such as genetic stock identification (GSI) estimate the proportion of individuals from each subpopulation using genetic data. Ideally, these estimates are obtained through mixture analysis, which captures both sampling and genetic uncertainty. However, historical datasets often rely on individual assignment methods that only account for sample-level uncertainty, limiting the validity of population-level inferences. To address this, we propose a reverse Dirichlet-multinomial model and derive multiple variance estimators to propagate uncertainty from the sample to the population level. We extend this framework to genetic mark-recapture studies, assess performance via simulation, and apply our method to estimate the escapement of Sockeye Salmon (Oncorhynchus nerka) in the Taku River.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Wang, Martin Lysy, Audrey B\'eliveau</dc:creator>
    </item>
    <item>
      <title>Evaluating Informative Cluster Size in Cluster Randomized Trials</title>
      <link>https://arxiv.org/abs/2510.01127</link>
      <description>arXiv:2510.01127v1 Announce Type: new 
Abstract: In cluster randomized trials, the average treatment effect among individuals (i-ATE) can be different from the cluster average treatment effect (c-ATE) when informative cluster size is present, i.e., when treatment effects or participant outcomes depend on cluster size. In such scenarios, mixed-effects models and generalized estimating equations (GEEs) with exchangeable correlation structure are biased for both the i-ATE and c-ATE estimands, whereas GEEs with an independence correlation structure or analyses of cluster-level summaries are recommended in practice. However, when cluster size is non-informative, mixed-effects models and GEEs with exchangeable correlation structure can provide unbiased estimation and notable efficiency gains over other methods. Thus, hypothesis tests for informative cluster size would be useful to assess this key phenomenon under cluster randomization. In this work, we develop model-based, model-assisted, and randomization-based tests for informative cluster size in cluster randomized trials. We construct simulation studies to examine the operating characteristics of these tests, show they have appropriate Type I error control and meaningful power, and contrast them to existing model-based tests used in the observational study setting. The proposed tests are then applied to data from a recent cluster randomized trial, and practical recommendations for using these tests are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01127v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryan S. Blette, Zhe Chen, Brennan C. Kahan, Andrew Forbes, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Partial Identification Approach to Counterfactual Fairness Assessment</title>
      <link>https://arxiv.org/abs/2510.00163</link>
      <description>arXiv:2510.00163v1 Announce Type: cross 
Abstract: The wide adoption of AI decision-making systems in critical domains such as criminal justice, loan approval, and hiring processes has heightened concerns about algorithmic fairness. As we often only have access to the output of algorithms without insights into their internal mechanisms, it was natural to examine how decisions would alter when auxiliary sensitive attributes (such as race) change. This led the research community to come up with counterfactual fairness measures, but how to evaluate the measure from available data remains a challenging task. In many practical applications, the target counterfactual measure is not identifiable, i.e., it cannot be uniquely determined from the combination of quantitative data and qualitative knowledge. This paper addresses this challenge using partial identification, which derives informative bounds over counterfactual fairness measures from observational data. We introduce a Bayesian approach to bound unknown counterfactual fairness measures with high confidence. We demonstrate our algorithm on the COMPAS dataset, examining fairness in recidivism risk scores with respect to race, age, and sex. Our results reveal a positive (spurious) effect on the COMPAS score when changing race to African-American (from all others) and a negative (direct causal) effect when transitioning from young to old age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00163v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saeyoung Rho, Junzhe Zhang, Elias Bareinboim</dc:creator>
    </item>
    <item>
      <title>CINDES: Classification induced neural density estimator and simulator</title>
      <link>https://arxiv.org/abs/2510.00367</link>
      <description>arXiv:2510.00367v1 Announce Type: cross 
Abstract: Neural network-based methods for (un)conditional density estimation have recently gained substantial attention, as various neural density estimators have outperformed classical approaches in real-data experiments. Despite these empirical successes, implementation can be challenging due to the need to ensure non-negativity and unit-mass constraints, and theoretical understanding remains limited. In particular, it is unclear whether such estimators can adaptively achieve faster convergence rates when the underlying density exhibits a low-dimensional structure. This paper addresses these gaps by proposing a structure-agnostic neural density estimator that is (i) straightforward to implement and (ii) provably adaptive, attaining faster rates when the true density admits a low-dimensional composition structure. Another key contribution of our work is to show that the proposed estimator integrates naturally into generative sampling pipelines, most notably score-based diffusion models, where it achieves provably faster convergence when the underlying density is structured. We validate its performance through extensive simulations and a real-data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00367v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehao Dai, Jianqing Fan, Yihong Gu, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Robust Spatiotemporally Contiguous Anomaly Detection Using Tensor Decomposition</title>
      <link>https://arxiv.org/abs/2510.00460</link>
      <description>arXiv:2510.00460v1 Announce Type: cross 
Abstract: Anomaly detection in spatiotemporal data is a challenging problem encountered in a variety of applications, including video surveillance, medical imaging data, and urban traffic monitoring. Existing anomaly detection methods focus mainly on point anomalies and cannot deal with temporal and spatial dependencies that arise in spatio-temporal data. Tensor-based anomaly detection methods have been proposed to address this problem. Although existing methods can capture dependencies across different modes, they are primarily supervised and do not account for the specific structure of anomalies. Moreover, these methods focus mainly on extracting anomalous features without providing any statistical confidence. In this paper, we introduce an unsupervised tensor-based anomaly detection method that simultaneously considers the sparse and spatiotemporally smooth nature of anomalies. The anomaly detection problem is formulated as a regularized robust low-rank + sparse tensor decomposition where the total variation of the tensor with respect to the underlying spatial and temporal graphs quantifies the spatiotemporal smoothness of the anomalies. Once the anomalous features are extracted, we introduce a statistical anomaly scoring framework that accounts for local spatio-temporal dependencies. The proposed framework is evaluated on both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00460v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachita Mondal, Mert Indibi, Tapabrata Maiti, Selin Aviyente</dc:creator>
    </item>
    <item>
      <title>On the Adversarial Robustness of Learning-based Conformal Novelty Detection</title>
      <link>https://arxiv.org/abs/2510.00463</link>
      <description>arXiv:2510.00463v1 Announce Type: cross 
Abstract: This paper studies the adversarial robustness of conformal novelty detection. In particular, we focus on AdaDetect, a powerful learning-based framework for novelty detection with finite-sample false discovery rate (FDR) control. While AdaDetect provides rigorous statistical guarantees under benign conditions, its behavior under adversarial perturbations remains unexplored. We first formulate an oracle attack setting that quantifies the worst-case degradation of FDR, deriving an upper bound that characterizes the statistical cost of attacks. This idealized formulation directly motivates a practical and effective attack scheme that only requires query access to AdaDetect's output labels. Coupling these formulations with two popular and complementary black-box adversarial algorithms, we systematically evaluate the vulnerability of AdaDetect on synthetic and real-world datasets. Our results show that adversarial perturbations can significantly increase the FDR while maintaining high detection power, exposing fundamental limitations of current error-controlled novelty detection methods and motivating the development of more robust alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00463v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daofu Zhang, Mehrdad Pournaderi, Hanne M. Clifford, Yu Xiang, Pramod K. Varshney</dc:creator>
    </item>
    <item>
      <title>Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold</title>
      <link>https://arxiv.org/abs/2510.00569</link>
      <description>arXiv:2510.00569v1 Announce Type: cross 
Abstract: Recovering a low-CP-rank tensor from noisy linear measurements is a central challenge in high-dimensional data analysis, with applications spanning tensor PCA, tensor regression, and beyond. We exploit the intrinsic geometry of rank-one tensors by casting the recovery task as an optimization problem over the Segre manifold, the smooth Riemannian manifold of rank-one tensors. This geometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent (RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at every iteration. Under mild noise assumptions, we prove that RGD converges at a local linear rate, while RGN exhibits an initial local quadratic convergence phase that transitions to a linear rate as the iterates approach the statistical noise floor. Extensive synthetic experiments validate these convergence guarantees and demonstrate the practical effectiveness of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00569v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Xu, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>Syntax-Guided Diffusion Language Models with User-Integrated Personalization</title>
      <link>https://arxiv.org/abs/2510.01028</link>
      <description>arXiv:2510.01028v1 Announce Type: cross 
Abstract: Large language models have made revolutionary progress in generating human-like text, yet their outputs often tend to be generic, exhibiting insufficient structural diversity, which limits personalized expression. Recent advances in diffusion models have opened new opportunities for improving language generation beyond the limitations of autoregressive paradigms. In this work, we propose a syntax-guided diffusion language model that integrates structural supervision and personalized conditioning to enhance text quality, diversity, and controllability. We introduce a cascaded framework that generates syntactic guidance before conditional text generation, and further generalize it to a novel noncascaded architecture for better alignment between structure and content. By incorporating syntactic information in the generating process, the proposed model better captures the lexical and structural characteristics of stylistic sentence construction. To enable fine-grained personalization, we develop a shared representation mechanism that facilitates information integration across users, supporting both faithful stylistic generation and generalizable zero-shot inference. Extensive experiments on multiple tasks demonstrate the superiority of our approach in fluency, diversity, and stylistic fidelity. Further qualitative analyses highlight its interpretability and flexibility in learning personalized patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01028v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruqian Zhang, Yijiao Zhang, Juan Shen, Zhongyi Zhu, Annie Qu</dc:creator>
    </item>
    <item>
      <title>The causal structure of galactic astrophysics</title>
      <link>https://arxiv.org/abs/2510.01112</link>
      <description>arXiv:2510.01112v1 Announce Type: cross 
Abstract: Data-driven astrophysics currently relies on the detection and characterisation of correlations between objects' properties, which are then used to test physical theories that make predictions for them. This process fails to utilise information in the data that forms a crucial part of the theories' predictions, namely which variables are directly correlated (as opposed to accidentally correlated through others), the directions of these determinations, and the presence or absence of confounders that correlate variables in the dataset but are themselves absent from it. We propose to recover this information through causal discovery, a well-developed methodology for inferring the causal structure of datasets that is however almost entirely unknown to astrophysics. We develop a causal discovery algorithm suitable for astrophysical datasets and illustrate it on $\sim$5$\times10^5$ low-redshift galaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish physical mechanisms that are degenerate on the basis of correlations alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01112v1</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.CO</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Desmond, Joseph Ramsey</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v4 Announce Type: replace 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Box Confidence Depth: simulation-based inference with hyper-rectangles</title>
      <link>https://arxiv.org/abs/2502.11072</link>
      <description>arXiv:2502.11072v2 Announce Type: replace 
Abstract: This work presents a novel simulation-based approach for constructing confidence regions in parametric models, which is particularly suited for generative models and situations where limited data and conventional asymptotic approximations fail to provide accurate results. The method leverages the concept of data depth and depends on creating random hyper-rectangles, i.e. boxes, in the sample space generated through simulations from the model, varying the input parameters. A probabilistic acceptance rule allows to retrieve a Depth-Confidence Distribution for the model parameters from which point estimators as well as calibrated confidence sets can be read-off. The method is designed to address cases where both the parameters and test statistics are multivariate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11072v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Laura Ventura</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
      <link>https://arxiv.org/abs/2505.17961</link>
      <description>arXiv:2505.17961v2 Announce Type: replace 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17961v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khellaf R\'emi, Bellet Aur\'elien, Josse Julie</dc:creator>
    </item>
    <item>
      <title>Geometric medians on product manifolds</title>
      <link>https://arxiv.org/abs/2505.18844</link>
      <description>arXiv:2505.18844v3 Announce Type: replace 
Abstract: Product manifolds arise when heterogeneous geometric variables are jointly observed. While the Fr\'{e}chet mean on Riemannian manifolds separates cleanly across factors, the canonical geometric median couples them, and its behavior has remained largely unexplored. In this paper, we give the first systematic treatment of this problem. After formulating the coupled objective, we establish general existence and uniqueness results that the median is unique on any Hadamard product, and remains locally unique under sharp conditions on curvature and injectivity radius even when one or more factors have positive curvature. We then prove that the estimator enjoys Lipschitz stability to perturbations and the optimal breakdown point, extending classical robustness guarantees to the product-manifold setting. Two practical solvers are proposed, including a Riemannian subgradient method with global sublinear convergence and a product-aware Weiszfeld algorithm that achieves local linear convergence. Both algorithms update the factors independently while respecting the latent coupling term, enabling implementation with standard manifold primitives. Simulations on parameter spaces of univariate and multivariate Gaussian distributions endowed with the Bures-Wasserstein geometry show that the median is more resilient to contamination than the Fr\'{e}chet mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18844v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiewon Park, Kisung You</dc:creator>
    </item>
    <item>
      <title>Simultaneous estimation of the effective reproduction number and the time series of daily infections: Application to Covid-19</title>
      <link>https://arxiv.org/abs/2506.21027</link>
      <description>arXiv:2506.21027v2 Announce Type: replace 
Abstract: The time varying effective reproduction number is an important parameter for communication and policy decisions during an epidemic. In this paper, we present new statistical methods for estimating the reproduction number based on the popular model of \citet{cori2013new} which defines the effective reproduction number based on self-exciting dynamics of new infections. Such a model is conceptually simple and less susceptible to misspecifications than more complicated multi-compartment models. However, statistical inference is challenging, and the previous literature has either relied on proxy data and/or a two-step approach in which the number of infections are first estimated. In contrast, we present a coherent Bayesian method that approximates the joint posterior of daily new infections and reproduction numbers using a novel Markov chain Monte Carlo (MCMC) algorithm. Comparing our method to the state-of-the-art three-step estimation procedure of \citet{huisman2022estimation}, both using daily confirmed cases from Switzerland in the Covid-19 epidemic and simulated data, we find that our method is more accurate in terms of point estimates and uncertainty quantification, especially near the beginning and end of an observation period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21027v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hans R. K\"unsch, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>On Causal Inference for the Survivor Function</title>
      <link>https://arxiv.org/abs/2507.16691</link>
      <description>arXiv:2507.16691v3 Announce Type: replace 
Abstract: In this expository paper, we consider the problem of causal inference and efficient estimation for the counterfactual survivor function. This problem has previously been considered in the literature in several papers, each relying on the imposition of conditions meant to identify the desired estimand from the observed data. These conditions, generally referred to as either implying or satisfying coarsening at random, are inconsistently imposed across this literature and, in all cases, fail to imply coarsening at random. We establish the first general characterization of coarsening at random, and also sequential coarsening at random, for this estimation problem. Other contributions include the first general characterization of the set of all influence functions for the counterfactual survival probability under sequential coarsening at random, and the corresponding nonparametric efficient influence function. These characterizations are general in that neither impose continuity assumptions on either the underlying failure or censoring time distributions. We further show how the latter compares to alternative forms recently derived in the literature, including establishing the pointwise equivalence of the influence functions for our nonparametric efficient estimator and that recently given in Westling et al (2024, Journal of the American Statistical Association).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16691v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin R. Baer, Ashkan Ertefaie, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>A direct approach to tree-guided feature aggregation for high-dimensional regression</title>
      <link>https://arxiv.org/abs/2507.19650</link>
      <description>arXiv:2507.19650v2 Announce Type: replace 
Abstract: In high-dimensional linear models, sparsity is often exploited to reduce variability and achieve parsimony. Equi-sparsity, where one assumes that predictors can be aggregated into groups sharing the same effects, is an alternative parsimonious structure that can be more suitable in certain applications. Previous work has clearly demonstrated the benefits of exploiting equi-sparsity in the presence of ``rare features'' (Yan and Bien 2021). In this work, we propose a new tree-guided regularization scheme for simultaneous estimation and feature aggregation. Unlike existing methods, our estimator avoids synthetic overparameterization and its detrimental effects. Even though our penalty is applied to hierarchically overlapped groups, we show that its proximal operator can be solved with a one-pass, non-iterative algorithm. Novel techniques are developed to study the finite-sample error bound of this seminorm-induced regularizer under least squares and binomial deviance losses. Theoretically, compared to existing methods, the proposed method offers a faster or equivalent rate depending on the true equi-sparisty structure. Extensive simulation studies verify these findings. Finally, we illustrate the usefulness of the proposed method with an application to a microbiome dataset, where we conduct post-selection inference on the aggregated features' effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19650v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinwen Fu, Aaron J. Molstad, Hui Zou</dc:creator>
    </item>
    <item>
      <title>Bias Correction in Factor-Augmented Regression Models with Weak Factors</title>
      <link>https://arxiv.org/abs/2509.02066</link>
      <description>arXiv:2509.02066v2 Announce Type: replace 
Abstract: In this paper, we study the asymptotic bias of the factor-augmented regression estimator and its reduction, which is augmented by the $r$ factors extracted from a large number of $N$ variables with $T$ observations. In particular, we consider general weak latent factor models with $r$ signal eigenvalues that may diverge at different rates, $N^{\alpha _{k}}$, $0&lt;\alpha _{k}\leq 1$, $k=1,\dots,r$. In the existing literature, the bias has been derived using an approximation for the estimated factors with a specific data-dependent rotation matrix $\hat{H}$ for the model with $\alpha_{k}=1$ for all $k$, whereas we derive the bias for weak factor models. In addition, we derive the bias using the approximation with a different rotation matrix $\hat{H}_q$, which generally has a smaller bias than with $\hat{H}$. We also derive the bias using our preferred approximation with a purely signal-dependent rotation $H$, which is unique and can be regarded as the population version of $\hat{H}$ and $\hat{H}_q$. Since this bias is parametrically inestimable, we propose a split-panel jackknife bias correction, and theory shows that it successfully reduces the bias. The extensive finite-sample experiments suggest that the proposed bias correction works very well, and the empirical application illustrates its usefulness in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02066v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyun Jiang, Yoshimasa Uematsu, Takashi Yamagata</dc:creator>
    </item>
    <item>
      <title>Functional Regression with Nonstationarity and Error Contamination: Application to the Economic Impact of Climate Change</title>
      <link>https://arxiv.org/abs/2509.08591</link>
      <description>arXiv:2509.08591v3 Announce Type: replace 
Abstract: This paper studies a regression model with functional dependent and explanatory variables, both of which exhibit nonstationary dynamics. The model assumes that the nonstationary stochastic trends of the dependent variable are explained by those of the explanatory variables, and hence that there exists a stable long-run relationship between the two variables despite their nonstationary behavior. We also assume that the functional observations may be error-contaminated. We develop novel autocovariance-based estimation and inference methods for this model. The methodology is broadly applicable to economic and statistical functional time series with nonstationary dynamics. To illustrate our methodology and its usefulness, we apply it to evaluating the global economic impact of climate change, an issue of intrinsic importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08591v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyungsik Nam, Won-Ki Seo</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation of Unfactorizable Systematic Uncertainties</title>
      <link>https://arxiv.org/abs/2509.15500</link>
      <description>arXiv:2509.15500v2 Announce Type: replace 
Abstract: Accurate assessment of systematic uncertainties is an increasingly vital task in physics studies, where large, high-dimensional datasets, like those collected at the Large Hadron Collider, hold the key to new discoveries. Common approaches to assessing systematic uncertainties rely on simplifications, such as assuming that the impact of the various sources of uncertainty factorizes. In this paper, we provide realistic example scenarios in which this assumption fails. We introduce an algorithm that uses Gaussian process regression to estimate the impact of systematic uncertainties \textit{without} assuming factorization. The Gaussian process models are enhanced with derivative information, which increases the accuracy of the regression without increasing the number of samples. In addition, we present a novel sampling strategy based on Bayesian experimental design, which is shown to be more efficient than random and grid sampling in our example scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15500v2</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Romero, Kyle Cranmer, Daniel Whiteson</dc:creator>
    </item>
    <item>
      <title>RAPSEM: Identifying Latent Mediators Without Sequential Ignorability via a Rank-Preserving Structural Equation Model</title>
      <link>https://arxiv.org/abs/2509.23935</link>
      <description>arXiv:2509.23935v2 Announce Type: replace 
Abstract: The identification of latent mediator variables is typically conducted using standard structural equation models (SEMs). When SEM is applied to mediation analysis with a causal interpretation, valid inference relies on the strong assumption of no unmeasured confounding, that is, all relevant covariates must be included in the analysis. This assumption is often violated in empirical applications, leading to biased estimates of direct and indirect effects. We address this limitation by weakening the causal assumptions and proposing a procedure that combines g-estimation with a two-stage method of moments to incorporate latent variables, thereby enabling more robust mediation analysis in settings common to the social sciences. We establish consistency and asymptotic normality of the resulting estimator. Simulation studies demonstrate that the estimator is unbiased across a wide range of settings, robust to violations of its underlying no-effect-modifier assumption, and achieves reasonable power to detect medium to large effects for sample sizes above 500, with power increasing as the strength of treatment-covariate interactions grows. The code is available at https://github.com/PsychometricsMZ/RAPSEM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23935v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sofia Morelli, Roberto Faleh, Holger Brandt</dc:creator>
    </item>
    <item>
      <title>Extremal correlation coefficient for functional data</title>
      <link>https://arxiv.org/abs/2405.17318</link>
      <description>arXiv:2405.17318v3 Announce Type: replace-cross 
Abstract: We propose a coefficient that measures dependence in paired samples of functions. It has properties similar to the Pearson correlation, but differs in significant ways: (i) it is designed to measure dependence between curves, (ii) it focuses only on extreme curves. The new coefficient is derived within the framework of regular variation in Banach spaces. A consistent estimator is proposed and justified by an asymptotic analysis and a simulation study. The usefulness of the new coefficient is illustrated on financial and and climate functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17318v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihyun Kim, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>A Likelihood Based Approach to Distribution Regression Using Conditional Deep Generative Models</title>
      <link>https://arxiv.org/abs/2410.02025</link>
      <description>arXiv:2410.02025v2 Announce Type: replace-cross 
Abstract: In this work, we explore the theoretical properties of conditional deep generative models under the statistical framework of distribution regression where the response variable lies in a high-dimensional ambient space but concentrates around a potentially lower-dimensional manifold. More specifically, we study the large-sample properties of a likelihood-based approach for estimating these models. Our results lead to the convergence rate of a sieve maximum likelihood estimator (MLE) for estimating the conditional distribution (and its devolved counterpart) of the response given predictors in the Hellinger (Wasserstein) metric. Our rates depend solely on the intrinsic dimension and smoothness of the true conditional distribution. These findings provide an explanation of why conditional deep generative models can circumvent the curse of dimensionality from the perspective of statistical foundations and demonstrate that they can learn a broader class of nearly singular conditional distributions. Our analysis also emphasizes the importance of introducing a small noise perturbation to the data when they are supported sufficiently close to a manifold. Finally, in our numerical studies, we demonstrate the effective implementation of the proposed approach using both synthetic and real-world datasets, which also provide complementary validation to our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02025v2</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shivam Kumar, Yun Yang, Lizhen Lin</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v3 Announce Type: replace-cross 
Abstract: This study investigates minimax and Bayes optimal strategies in fixed-budget best-arm identification. We consider an adaptive procedure consisting of a sampling phase followed by a recommendation phase, and we design an adaptive experiment within this framework to efficiently identify the best arm, defined as the one with the highest expected outcome. In our proposed strategy, the sampling phase consists of two stages. The first stage is a pilot phase, in which we allocate each arm uniformly in equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, arms are allocated in proportion to the variances estimated during the first stage. After the sampling phase, the procedure enters the recommendation phase, where we select the arm with the highest sample mean as our estimate of the best arm. We prove that this single strategy is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that coincide exactly with our lower bounds, including the constant terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>The Gauss-Markov Adjunction Provides Categorical Semantics of Residuals in Supervised Learning</title>
      <link>https://arxiv.org/abs/2507.02442</link>
      <description>arXiv:2507.02442v2 Announce Type: replace-cross 
Abstract: Enhancing the intelligibility and interpretability of machine learning is a crucial task in responding to the demand for Explicability as an AI principle, and in promoting the better social implementation of AI. The aim of our research is to contribute to this improvement by reformulating machine learning models through the lens of category theory, thereby developing a semantic framework for structuring and understanding AI systems. Our categorical modeling in this paper clarifies and formalizes the structural interplay between residuals and parameters in supervised learning. The present paper focuses on the multiple linear regression model, which represents the most basic form of supervised learning. By defining two Lawvere-enriched categories corresponding to parameters and data, along with an adjoint pair of functors between them, we introduce our categorical formulation of supervised learning. We show that the essential structure of this framework is captured by what we call the Gauss-Markov Adjunction. Within this setting, the dual flow of information can be explicitly described as a correspondence between variations in parameters and residuals. The ordinary least squares estimator for the parameters and the minimum residual are related via the preservation of limits by the right adjoint functor. Furthermore, we position this formulation as an instance of extended denotational semantics for supervised learning, and propose applying a semantic perspective developed in theoretical computer science as a formal foundation for Explicability in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02442v2</guid>
      <category>cs.AI</category>
      <category>math.CT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moto Kamiura</dc:creator>
    </item>
    <item>
      <title>Transporting Predictions via Double Machine Learning: Predicting Partially Unobserved Students' Outcomes</title>
      <link>https://arxiv.org/abs/2509.12533</link>
      <description>arXiv:2509.12533v2 Announce Type: replace-cross 
Abstract: Educational policymakers often lack data on student outcomes in regions where standardized tests were not administered. Machine learning techniques can be used to predict unobserved outcomes in target populations by training models on data from a source population. However, differences between the source and target populations, particularly in covariate distributions, can reduce the transportability of these models, potentially reducing predictive accuracy and introducing bias. We propose using double machine learning for a covariate-shift weighted model. First, we estimate the overlap score-namely, the probability that an observation belongs to the source dataset given its covariates. Second, balancing weights, defined as the density ratio of target-to-source membership probabilities, are used to reweight the individual observations' contribution to the loss or likelihood function in the target outcome prediction model. This approach downweights source observations that are less similar to the target population, allowing predictions to rely more heavily on observations with greater overlap. As a result, predictions become more generalizable under covariate shift. We illustrate this framework in the context of uncertain data on students' standardized financial literacy scores (FLS). Using Bayesian Additive Regression Trees (BART), we predict missing FLS. We find minimal differences in predictive performance between the weighted and unweighted models, suggesting limited covariate shift in our empirical setting. Nonetheless, the proposed approach provides a principled framework for addressing covariate shift and is broadly applicable to predictive modeling in the social and health sciences, where differences between source and target populations are common.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12533v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Falco J. Bargagli-Stoffi, Emma Landry, Kevin P. Josey, Kenneth De Beckker, Joana E. Maldonado, Kristof De Witte</dc:creator>
    </item>
    <item>
      <title>Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence</title>
      <link>https://arxiv.org/abs/2509.16599</link>
      <description>arXiv:2509.16599v2 Announce Type: replace-cross 
Abstract: Background: Evidence synthesis facilitates evidence-based medicine. This task becomes increasingly difficult to accomplished with applying computational solutions, since the medical literature grows at astonishing rates. Objective: This study evaluates an information retrieval-driven workflow, CASMA, to enhance the efficiency, transparency, and reproducibility of systematic reviews. Endometriosis recurrence serves as the ideal case due to its complex and ambiguous literature. Methods: The hybrid approach integrates PRISMA guidelines with fuzzy matching and regular expression (regex) to facilitate semi-automated deduplication and filtered records before manual screening. The workflow synthesised evidence from randomised controlled trials on the efficacy of a subclass of gonadotropin-releasing hormone agonists (GnRH-a). A modified splitting method addressed unit-of-analysis errors in multi-arm trials. Results: The workflow sharply reduced the screening workload, taking only 11 days to fetch and filter 33,444 records. Seven eligible RCTs were synthesized (841 patients). The pooled random-effects model yielded a Risk Ratio (RR) of $0.64$ ($95\%$ CI $0.48$ to $0.86$), demonstrating a $36\%$ reduction in recurrence, with non-significant heterogeneity ($I^2=0.00\%$, $\tau^2=0.00$). The findings were robust and stable, as they were backed by sensitivity analyses. Conclusion: This study demonstrates an application of an information-retrieval-driven workflow for medical evidence synthesis. The approach yields valuable clinical results and a generalisable framework to scale up the evidence synthesis, bridging the gap between clinical research and computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16599v2</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandro Tsang</dc:creator>
    </item>
  </channel>
</rss>
