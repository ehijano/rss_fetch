<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 01:33:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Strategic analysis of hydrogen market dynamics across collaboration models</title>
      <link>https://arxiv.org/abs/2506.22690</link>
      <description>arXiv:2506.22690v1 Announce Type: cross 
Abstract: The global energy landscape is experiencing a transformative shift, with an increasing emphasis on sustainable and clean energy sources. Hydrogen remains a promising candidate for decarbonization, energy storage, and as an alternative fuel. This study explores the landscape of hydrogen pricing and demand dynamics by evaluating three collaboration scenarios: market-based pricing, cooperative integration, and coordinated decision-making. It incorporates price-sensitive demand, environmentally friendly production methods, and market penetration effects, to provide insights into maximizing market share, profitability, and sustainability within the hydrogen industry. This study contributes to understanding the complexities of collaboration by analyzing those structures and their role in a fast transition to clean hydrogen production by balancing economic viability and environmental goals. The findings reveal that the cooperative integration strategy is the most effective for sustainable growth, increasing green hydrogen's market share to 19.06 % and highlighting the potential for environmentally conscious hydrogen production. They also suggest that the coordinated decision-making approach enhances profitability through collaborative tariff contracts while balancing economic viability and environmental goals. This study also underscores the importance of strategic pricing mechanisms, policy alignment, and the role of hydrogen hubs in achieving sustainable growth in the hydrogen sector. By highlighting the uncertainties and potential barriers, this research offers actionable guidance for policymakers and industry players in shaping a competitive and sustainable energy marketplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22690v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rser.2024.115001</arxiv:DOI>
      <dc:creator>Mohammad Asghari, Hamid Afshari, Mohamad Y Jaber, Cory Searcy</dc:creator>
    </item>
    <item>
      <title>Zero-disparity Distribution Synthesis: Fast Exact Calculation of Chi-Squared Statistic Distribution for Discrete Uniform Histograms</title>
      <link>https://arxiv.org/abs/2506.23416</link>
      <description>arXiv:2506.23416v1 Announce Type: cross 
Abstract: Pearson's chi-squared test is widely used to assess the uniformity of discrete histograms, typically relying on a continuous chi-squared distribution to approximate the test statistic, since computing the exact distribution is computationally too costly. While effective in many cases, this approximation allegedly fails when expected bin counts are low or tail probabilities are needed. Here, Zero-disparity Distribution Synthesis is presented, a fast dynamic programming approach for computing the exact distribution, enabling detailed analysis of approximation errors. The results dispel some existing misunderstandings and also reveal subtle, but significant pitfalls in approximation that are only apparent with exact values. The Python source code is available at https://github.com/DiscreteTotalVariation/ChiSquared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23416v1</guid>
      <category>stat.ME</category>
      <category>cs.MS</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikola Bani\'c, Neven Elezovi\'c</dc:creator>
    </item>
    <item>
      <title>Online Multivariate Changepoint Detection: Leveraging Links With Computational Geometry</title>
      <link>https://arxiv.org/abs/2311.01174</link>
      <description>arXiv:2311.01174v3 Announce Type: replace 
Abstract: The increasing volume of data streams poses significant computational challenges for detecting changepoints online. Likelihood-based methods are effective, but a naive sequential implementation becomes impractical online due to high computational costs. We develop an online algorithm that exactly calculates the likelihood ratio test for a single changepoint in $p$-dimensional data streams by leveraging a fascinating connection with computational geometry. This connection straightforwardly allows us to exactly recover sparse likelihood ratio statistics: that is assuming only a subset of the dimensions are changing. Our algorithm is straightforward, fast, and apparently quasi-linear. A dyadic variant of our algorithm is provably quasi-linear, being $\mathcal{O}(n\log(n)^{p+1})$ for $n$ data points and $p$ less than $3$, but slower in practice. These algorithms are computationally impractical when $p$ is larger than $5$, and we provide an approximate algorithm suitable for such $p$ which is $\mathcal{O}(np\log(n)^{\tilde{p}+1}), $ for some user-specified $\tilde{p} \leq 5$. We derive statistical guarantees for the proposed procedures in the Gaussian case, and confirm the good computational and statistical performance, and usefulness, of the algorithms on both empirical data and NBA data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01174v3</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liudmila Pishchagina, Gaetano Romano, Paul Fearnhead, Vincent Runge, Guillem Rigaill</dc:creator>
    </item>
    <item>
      <title>Bayesian Strategies for Repulsive Spatial Point Processes</title>
      <link>https://arxiv.org/abs/2404.15133</link>
      <description>arXiv:2404.15133v2 Announce Type: replace 
Abstract: There is increasing interest to develop Bayesian inferential algorithms for point process models with intractable likelihoods. A purpose of this paper is to illustrate the utility of using simulation based strategies, including approximate Bayesian computation (ABC) and Markov chain Monte Carlo (MCMC) methods for this task. Shirota and Gelfand (2017) proposed an extended version of an ABC approach for repulsive spatial point processes, including the Strauss point process and the determinantal point process, but their algorithm was not correctly detailed. We explain that is, in general, intractable and therefore impractical to use, except in some restrictive situations. This motivates us to instead consider an ABC-MCMC algorithm developed by Fearnhead and Prangle (2012). We further explore the use of the exchange algorithm, together with the recently proposed noisy Metropolis-Hastings algorithm (Alquier et al., 2016). As an extension of the exchange algorithm, which requires a single simulation from the likelihood at each iteration, the noisy Metropolis-Hastings algorithm considers multiple draws from the same likelihood function. We find that both of these inferential approaches yield good performance for repulsive spatial point processes in both simulated and real data applications and should be considered as viable approaches for the analysis of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15133v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyi Lu, Nial Friel</dc:creator>
    </item>
    <item>
      <title>Best Linear Unbiased Estimate from Privatized Contingency Tables</title>
      <link>https://arxiv.org/abs/2409.04387</link>
      <description>arXiv:2409.04387v5 Announce Type: replace 
Abstract: In differential privacy (DP) mechanisms, it can be beneficial to release ``redundant'' outputs, where some quantities can be estimated in multiple ways by combining different privatized values. Indeed, the DP 2020 Decennial Census products published by the U.S. Census Bureau consist of such redundant noisy counts. When redundancy is present, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained using different noisy counts result in the same value), and we show that the minimum variance processing is a linear projection. However, standard projection algorithms require excessive computation and memory, making them impractical for large-scale applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two-step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. Finally, we apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04387v5</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Adam Edwards, Paul Bartholomew, Andrew Sillers</dc:creator>
    </item>
    <item>
      <title>Annealed Leap-Point Sampler for Multimodal Target Distributions</title>
      <link>https://arxiv.org/abs/2112.12908</link>
      <description>arXiv:2112.12908v2 Announce Type: replace-cross 
Abstract: In Bayesian statistics, exploring high-dimensional multimodal posterior distributions poses major challenges for existing MCMC approaches. This paper introduces the Annealed Leap-Point Sampler (ALPS), which augments the target distribution state space with modified annealed (cooled) distributions, in contrast to traditional tempering approaches. The coldest state is chosen such that its annealed density is well-approximated locally by a Laplace approximation. This allows for automated setup of a scalable mode-leaping independence sampler. ALPS requires an exploration component to search for the mode locations, which can either be run adaptively in parallel to improve these mode-jumping proposals, or else as a pre-computation step. A theoretical analysis shows that for a d-dimensional problem the coolest temperature level required only needs to be linear in dimension, $\mathcal{O}(d)$, implying that the number of iterations needed for ALPS to converge is $\mathcal{O}(d)$ (typically leading to overall complexity $\mathcal{O}(d^3)$ when computational cost per iteration is taken into account). ALPS is illustrated on several complex, multimodal distributions that arise from real-world applications. This includes a seemingly-unrelated regression (SUR) model of longitudinal data from U.S. manufacturing firms, as well as a spectral density model that is used in analytical chemistry for identification of molecular biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.12908v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas G. Tawn, Matthew T. Moores, Hugo Queniat, Gareth O. Roberts</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Privatized Data with Unknown Sample Size</title>
      <link>https://arxiv.org/abs/2406.06231</link>
      <description>arXiv:2406.06231v2 Announce Type: replace-cross 
Abstract: We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection. We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that Approximate Bayesian Computation (ABC)-type posterior distributions converge under similar assumptions. We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP. We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06231v2</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Andres Felipe Barrientos, Nianqiao Ju</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v5 Announce Type: replace-cross 
Abstract: We introduce the inverse Kalman filter, which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the inverse Kalman filter with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the proposed approach through applications in nonparametric estimation of particle interaction functions, using both simulations and cell trajectories from microscopy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Community Detection Analysis of Spatial Transcriptomics Data</title>
      <link>https://arxiv.org/abs/2503.12351</link>
      <description>arXiv:2503.12351v3 Announce Type: replace-cross 
Abstract: The spatial transcriptomics (ST) data produced by recent biotechnologies, such as CosMx and Xenium, contain huge amount of information about cancer tissue samples, which has great potential for cancer research via detection of community: a collection of cells with distinct cell-type composition and similar neighboring patterns. But existing clustering methods do not work well for community detection of CosMx ST data, and the commonly used kNN compositional data method shows lack of informative neighboring cell patterns for huge CosMx data. In this article, we propose a novel and more informative disk compositional data (DCD) method, which identifies neighboring patterns of each cell via taking into account of ST data features from recent new technologies. After initial processing ST data into DCD matrix, a new innovative and interpretable DCD-TMHC community detection method is proposed here. Extensive simulation studies and CosMx breast cancer data analysis clearly show that our proposed DCD-TMHC method is superior to other methods. Based on the communities detected by DCD-TMHC method for CosMx breast cancer data, the logistic regression analysis results demonstrate that DCD-TMHC method is clearly interpretable and superior, especially in terms of assessment for different stages of cancer. These suggest that our proposed novel, innovative, informative and interpretable DCD-TMHC method here will be helpful and have impact to future cancer research based on ST data, which can improve cancer diagnosis and monitor cancer treatment progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12351v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Zhao</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power</title>
      <link>https://arxiv.org/abs/2506.20523</link>
      <description>arXiv:2506.20523v2 Announce Type: replace-cross 
Abstract: Adaptive experiments such as multi-armed bandits offer efficiency gains over traditional randomized experiments but pose two major challenges: invalid inference on the Average Treatment Effect (ATE) due to adaptive sampling and low statistical power for sub-optimal treatments. We address both issues by extending the Mixture Adaptive Design framework (arXiv:2311.05794). First, we propose MADCovar, a covariate-adjusted ATE estimator that is unbiased and preserves anytime-valid inference guarantees while substantially improving ATE precision. Second, we introduce MADMod, which dynamically reallocates samples to underpowered arms, enabling more balanced statistical power across treatments without sacrificing valid inference. Both methods retain MAD's core advantage of constructing asymptotic confidence sequences (CSs) that allow researchers to continuously monitor ATE estimates and stop data collection once a desired precision or significance criterion is met. Empirically, we validate both methods using simulations and real-world data. In simulations, MADCovar reduces CS width by up to $60\%$ relative to MAD. In a large-scale political RCT with $\approx32,000$ participants, MADCovar achieves similar precision gains. MADMod improves statistical power and inferential precision across all treatment arms, particularly for suboptimal treatments. Simulations show that MADMod sharply reduces Type II error while preserving the efficiency benefits of adaptive allocation. Together, MADCovar and MADMod make adaptive experiments more practical, reliable, and efficient for applied researchers across many domains. Our proposed methods are implemented through an open-source software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20523v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Molitor, Samantha Gold</dc:creator>
    </item>
  </channel>
</rss>
