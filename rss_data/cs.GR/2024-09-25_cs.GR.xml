<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 10:16:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simplifying Triangle Meshes in the Wild</title>
      <link>https://arxiv.org/abs/2409.15458</link>
      <description>arXiv:2409.15458v1 Announce Type: new 
Abstract: This paper introduces a fast and robust method for simplifying surface triangle meshes in the wild while maintaining high visual quality. While previous methods achieve excellent results on manifold meshes by using the quadric error metric, they suffer from producing high-quality outputs for user-created meshes, which often contain non-manifold elements and multiple connected components. In this work, we begin by outlining the pitfalls of existing mesh simplification techniques and highlighting the discrepancy in their formulations with existing mesh data. We then propose a method for simplifying these (non-manifold) triangle meshes, while maintaining quality comparable to the existing methods for manifold inputs. Our key idea is to reformulate mesh simplification as a problem of decimating simplicial 2-complexes. This involves a novel construction to turn a triangle soup into a simplicial 2-complex, followed by iteratively collapsing 1-simplices (vertex pairs) with our modified quadric error metric tailored for topology changes. Besides, we also tackle textured mesh simplification. Instead of following existing strategies to preserve mesh UVs, we propose a novel perspective that only focuses on preserving texture colors defined on the surface, regardless of the layout in the texture UV space. This leads to a more robust method for textured mesh simplification that is free from the texture bleeding artifact. Our mesh simplification enables level-of-detail algorithms to operate on arbitrary triangle meshes in the wild. We demonstrate improvements over prior techniques through extensive qualitative and quantitative evaluations, along with user studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15458v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hsueh-Ti Derek Liu, Xiaoting Zhang, Cem Yuksel</dc:creator>
    </item>
    <item>
      <title>A Differentiable Material Point Method Framework for Shape Morphing</title>
      <link>https://arxiv.org/abs/2409.15746</link>
      <description>arXiv:2409.15746v1 Announce Type: new 
Abstract: We present a novel, physically-based morphing technique for elastic shapes, leveraging the differentiable material point method (MPM) with space-time control through per-particle deformation gradients to accommodate complex topology changes. This approach, grounded in MPM's natural handling of dynamic topologies, is enhanced by a chained iterative optimization technique, allowing for the creation of both succinct and extended morphing sequences that maintain coherence over time. Demonstrated across various challenging scenarios, our method is able to produce detailed elastic deformation and topology transitions, all grounded within our physics-based simulation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15746v1</guid>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael Xu, Chang-Yong Song, David I. W. Levin, David Hyde</dc:creator>
    </item>
    <item>
      <title>StyleReiser: Stylizing Video With Reinforced Structure Guide</title>
      <link>https://arxiv.org/abs/2409.15341</link>
      <description>arXiv:2409.15341v1 Announce Type: cross 
Abstract: We introduce StyleReiser, an example-based video stylization method that transfers style from a given keyframe to the entire video sequence while maintaining visual consistency even in distant frames where the scene structure may change significantly. Unlike previous keyframe-based methods, our approach considers consistency with the prescribed style and maintains fidelity to new structural elements appearing in the target video sequence. This combination can significantly improve the quality of the stylized sequence without the need to add more correction keyframes. We also demonstrate that our approach can notably enhance the output of text-driven video stylization methods by suppressing their structural instability and enabling the user to perform custom edits on the generated keyframes. Moreover, due to its capability to perform inference in real-time, our technique can also be applied in interactive scenarios, such as consistently stylized video calls, which are difficult to achieve using text-driven approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15341v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Radim Spetlik, David Futschik, Daniel Sykora</dc:creator>
    </item>
    <item>
      <title>Neural Control Variates with Automatic Integration</title>
      <link>https://arxiv.org/abs/2409.15394</link>
      <description>arXiv:2409.15394v1 Announce Type: cross 
Abstract: This paper presents a method to leverage arbitrary neural network architecture for control variates. Control variates are crucial in reducing the variance of Monte Carlo integration, but they hinge on finding a function that both correlates with the integrand and has a known analytical integral. Traditional approaches rely on heuristics to choose this function, which might not be expressive enough to correlate well with the integrand. Recent research alleviates this issue by modeling the integrands with a learnable parametric model, such as a neural network. However, the challenge remains in creating an expressive parametric model with a known analytical integral. This paper proposes a novel approach to construct learnable parametric control variates functions from arbitrary neural network architectures. Instead of using a network to approximate the integrand directly, we employ the network to approximate the anti-derivative of the integrand. This allows us to use automatic differentiation to create a function whose integration can be constructed by the antiderivative network. We apply our method to solve partial differential equations using the Walk-on-sphere algorithm. Our results indicate that this approach is unbiased and uses various network architectures to achieve lower variance than other control variate methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15394v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3641519.3657395</arxiv:DOI>
      <arxiv:journal_reference>SIGGRAPH Conference Papers 2024</arxiv:journal_reference>
      <dc:creator>Zilu Li, Guandao Yang, Qingqing Zhao, Xi Deng, Leonidas Guibas, Bharath Hariharan, Gordon Wetzstein</dc:creator>
    </item>
    <item>
      <title>Disentangled Generation and Aggregation for Robust Radiance Fields</title>
      <link>https://arxiv.org/abs/2409.15715</link>
      <description>arXiv:2409.15715v1 Announce Type: cross 
Abstract: The utilization of the triplane-based radiance fields has gained attention in recent years due to its ability to effectively disentangle 3D scenes with a high-quality representation and low computation cost. A key requirement of this method is the precise input of camera poses. However, due to the local update property of the triplane, a similar joint estimation as previous joint pose-NeRF optimization works easily results in local minima. To this end, we propose the Disentangled Triplane Generation module to introduce global feature context and smoothness into triplane learning, which mitigates errors caused by local updating. Then, we propose the Disentangled Plane Aggregation to mitigate the entanglement caused by the common triplane feature aggregation during camera pose updating. In addition, we introduce a two-stage warm-start training strategy to reduce the implicit constraints caused by the triplane generator. Quantitative and qualitative results demonstrate that our proposed method achieves state-of-the-art performance in novel view synthesis with noisy or unknown camera poses, as well as efficient convergence of optimization. Project page: https://gaohchen.github.io/DiGARR/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15715v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shihe Shen, Huachen Gao, Wangze Xu, Rui Peng, Luyang Tang, Kaiqiang Xiong, Jianbo Jiao, Ronggang Wang</dc:creator>
    </item>
    <item>
      <title>A Formalization of Image Vectorization by Region Merging</title>
      <link>https://arxiv.org/abs/2409.15940</link>
      <description>arXiv:2409.15940v1 Announce Type: cross 
Abstract: Image vectorization converts raster images into vector graphics composed of regions separated by curves. Typical vectorization methods first define the regions by grouping similar colored regions via color quantization, then approximate their boundaries by Bezier curves. In that way, the raster input is converted into an SVG format parameterizing the regions' colors and the Bezier control points. This compact representation has many graphical applications thanks to its universality and resolution-independence. In this paper, we remark that image vectorization is nothing but an image segmentation, and that it can be built by fine to coarse region merging. Our analysis of the problem leads us to propose a vectorization method alternating region merging and curve smoothing. We formalize the method by alternate operations on the dual and primal graph induced from any domain partition. In that way, we address a limitation of current vectorization methods, which separate the update of regional information from curve approximation. We formalize region merging methods by associating them with various gain functionals, including the classic Beaulieu-Goldberg and Mumford-Shah functionals. More generally, we introduce and compare region merging criteria involving region number, scale, area, and internal standard deviation. We also show that the curve smoothing, implicit in all vectorization methods, can be performed by the shape-preserving affine scale space. We extend this flow to a network of curves and give a sufficient condition for the topological preservation of the segmentation. The general vectorization method that follows from this analysis shows explainable behaviors, explicitly controlled by a few intuitive parameters. It is experimentally compared to state-of-the-art software and proved to have comparable or superior fidelity and cost efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15940v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roy Y. He, Sung Ha Kang, Jean-Michel Morel</dc:creator>
    </item>
    <item>
      <title>Semantics-Controlled Gaussian Splatting for Outdoor Scene Reconstruction and Rendering in Virtual Reality</title>
      <link>https://arxiv.org/abs/2409.15959</link>
      <description>arXiv:2409.15959v1 Announce Type: cross 
Abstract: Advancements in 3D rendering like Gaussian Splatting (GS) allow novel view synthesis and real-time rendering in virtual reality (VR). However, GS-created 3D environments are often difficult to edit. For scene enhancement or to incorporate 3D assets, segmenting Gaussians by class is essential. Existing segmentation approaches are typically limited to certain types of scenes, e.g., ''circular'' scenes, to determine clear object boundaries. However, this method is ineffective when removing large objects in non-''circling'' scenes such as large outdoor scenes. We propose Semantics-Controlled GS (SCGS), a segmentation-driven GS approach, enabling the separation of large scene parts in uncontrolled, natural environments. SCGS allows scene editing and the extraction of scene parts for VR. Additionally, we introduce a challenging outdoor dataset, overcoming the ''circling'' setup. We outperform the state-of-the-art in visual quality on our dataset and in segmentation quality on the 3D-OVS dataset. We conducted an exploratory user study, comparing a 360-video, plain GS, and SCGS in VR with a fixed viewpoint. In our subsequent main study, users were allowed to move freely, evaluating plain GS and SCGS. Our main study results show that participants clearly prefer SCGS over plain GS. We overall present an innovative approach that surpasses the state-of-the-art both technically and in user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15959v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Schieber, Jacob Young, Tobias Langlotz, Stefanie Zollmann, Daniel Roth</dc:creator>
    </item>
    <item>
      <title>Using Virtual Reality as a Simulation Tool for Augmented Reality Virtual Windows: Effects on Cognitive Workload and Task Performance</title>
      <link>https://arxiv.org/abs/2409.16037</link>
      <description>arXiv:2409.16037v1 Announce Type: cross 
Abstract: Virtual content in Augmented Reality (AR) applications can be constructed according to the designer's requirements, but real environments, are difficult to be accurate control or completely reproduce. This makes it difficult to prototype AR applications for certain real environments. One way to address this issue is to use Virtual Reality (VR) to simulate an AR system, enabling the design of controlled experiments and conducting usability evaluations. However, the effectiveness of using VR to simulate AR has not been well studied. In this paper, we report on a user study (N=20) conducted to investigate the impact of using an VR simulation of AR on participants' task performance and cognitive workload (CWL). Participants performed several office tasks in an AR scene with virtual monitors and then again in the VR-simulated AR scene. While using the interfaces CWL was measured with Electroencephalography (EEG) data and a subjective questionnaire. Results showed that frequent visual checks on the keyboard resulted in decreased task performance and increased cognitive workload. This study found that using AR centered on virtual monitor can be effectively simulated using VR. However, there is more research that can be done, so we also report on the study limitations and directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16037v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Liu, Weiping He, Mark Billinghurst</dc:creator>
    </item>
    <item>
      <title>Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking</title>
      <link>https://arxiv.org/abs/2409.16287</link>
      <description>arXiv:2409.16287v1 Announce Type: cross 
Abstract: Articulated object manipulation requires precise object interaction, where the object's axis must be carefully considered. Previous research employed interactive perception for manipulating articulated objects, but typically, open-loop approaches often suffer from overlooking the interaction dynamics. To address this limitation, we present a closed-loop pipeline integrating interactive perception with online axis estimation from segmented 3D point clouds. Our method leverages any interactive perception technique as a foundation for interactive perception, inducing slight object movement to generate point cloud frames of the evolving dynamic scene. These point clouds are then segmented using Segment Anything Model 2 (SAM2), after which the moving part of the object is masked for accurate motion online axis estimation, guiding subsequent robotic actions. Our approach significantly enhances the precision and efficiency of manipulation tasks involving articulated objects. Experiments in simulated environments demonstrate that our method outperforms baseline approaches, especially in tasks that demand precise axis-based control. Project Page: https://hytidel.github.io/video-tracking-for-axis-estimation/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16287v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Wang, Tianxing Chen, Qiaojun Yu, Tianling Xu, Zanxin Chen, Yiting Fu, Cewu Lu, Yao Mu, Ping Luo</dc:creator>
    </item>
    <item>
      <title>Efficient Nearest Neighbor Search Using Dynamic Programming</title>
      <link>https://arxiv.org/abs/2409.15023</link>
      <description>arXiv:2409.15023v2 Announce Type: replace-cross 
Abstract: When dealing with point clouds distributed on manifold surfaces in 3D space, or when the query point is far from the data, the efficiency of traditional nearest neighbor search algorithms (e.g., KD Tree and R Tree) may degrade. In extreme cases, the complexity of the query can approach O(n). In this paper, we propose a novel dynamic programming technique that precomputes a Directed Acyclic Graph (DAG) to enable more efficient nearest neighbor queries for 2D manifold data. By leveraging this structure, only a small number of distance comparisons between point pairs are required to accurately identify the nearest neighbor. Extensive experimental results demonstrate that our method achieves query speeds that are 1x-10x faster than traditional methods. Moreover, our algorithm exhibits significant potential. It achieves query efficiency comparable to KD-trees on uniformly distributed point clouds. Additionally, our algorithm supports nearest neighbor queries among the first k points. Coupled with our algorithm, a farthest point sampling algorithm with lower complexity can also be implemented. Furthermore, our method has the potential to support nearest neighbor queries with different types of primitives and distance metrics. We believe that the method proposed in this paper represents the most concise and straightforward exact nearest neighbor search algorithm currently available, and it will contribute significantly to advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15023v2</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Wang, Jiantao Song, Shiqing Xin, Shuangmin Chen, Changhe Tu, Wenping Wang, Jiaye Wang</dc:creator>
    </item>
  </channel>
</rss>
