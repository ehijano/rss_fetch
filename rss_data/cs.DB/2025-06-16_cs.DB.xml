<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 02:28:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Jelly: a fast and convenient RDF serialization format</title>
      <link>https://arxiv.org/abs/2506.11298</link>
      <description>arXiv:2506.11298v1 Announce Type: new 
Abstract: Existing RDF serialization formats such as Turtle, N-Triples, and JSON-LD are widely used for communication and storage in knowledge graph and Semantic Web applications. However, they suffer from limitations in performance, compression ratio, and lack of native support for RDF streams. To address these shortcomings, we introduce Jelly, a fast and convenient binary serialization format for RDF data that supports both batch and streaming use cases. Jelly is designed to maximize serialization throughput, reduce file size with lightweight streaming compression, and minimize compute resource usage. Built on Protocol Buffers, Jelly is easy to integrate with modern programming languages and RDF libraries. To maximize reusability, Jelly has an open protocol specification, open-source implementations in Java and Python integrated with popular RDF libraries, and a versatile command-line tool. To illustrate its usefulness, we outline concrete use cases where Jelly can provide tangible benefits. By combining practical usability with state-of-the-art efficiency, Jelly is an important contribution to the Semantic Web tool stack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11298v1</guid>
      <category>cs.DB</category>
      <category>cs.NI</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Sowinski, Karolina Bogacka, Anastasiya Danilenka, Nikita Kozlov</dc:creator>
    </item>
    <item>
      <title>OCPQ: Object-Centric Process Querying &amp; Constraints</title>
      <link>https://arxiv.org/abs/2506.11541</link>
      <description>arXiv:2506.11541v1 Announce Type: new 
Abstract: Process querying is used to extract information and insights from process execution data. Similarly, process constraints can be checked against input data, yielding information on which process instances violate them. Traditionally, such process mining techniques use case-centric event data as input. However, with the uptake of Object-Centric Process Mining (OCPM), existing querying and constraint checking techniques are no longer applicable. Object-Centric Event Data (OCED) removes the requirement to pick a single case notion (i.e., requiring that events belong to exactly one case) and can thus represent many real-life processes much more accurately. In this paper, we present a novel highly-expressive approach for object-centric process querying, called OCPQ. It supports a wide variety of applications, including OCED-based constraint checking and filtering. The visual representation of nested queries in OCPQ allows users to intuitively read and create queries and constraints. We implemented our approach using (1) a high-performance execution engine backend and (2) an easy-to-use editor frontend. Additionally, we evaluated our approach on a real-life dataset, showing the lack in expressiveness of prior work and runtime performance significantly better than the general querying solutions SQLite and Neo4j, as well as comparable to the performance-focused DuckDB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11541v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-92474-3_23</arxiv:DOI>
      <dc:creator>Aaron K\"usters, Wil M. P. van der Aalst</dc:creator>
    </item>
    <item>
      <title>LLM-based Dynamic Differential Testing for Database Connectors with Reinforcement Learning-Guided Prompt Selection</title>
      <link>https://arxiv.org/abs/2506.11870</link>
      <description>arXiv:2506.11870v1 Announce Type: new 
Abstract: Database connectors are critical components enabling applications to interact with underlying database management systems (DBMS), yet their security vulnerabilities often remain overlooked. Unlike traditional software defects, connector vulnerabilities exhibit subtle behavioral patterns and are inherently challenging to detect. Besides, nonstandardized implementation of connectors leaves potential risks (a.k.a. unsafe implementations) but is more elusive. As a result, traditional fuzzing methods are incapable of finding such vulnerabilities. Even for LLM-enable test case generation, due to a lack of domain knowledge, they are also incapable of generating test cases that invoke all interface and internal logic of connectors. In this paper, we propose reinforcement learning (RL)-guided LLM test-case generation for database connector testing. Specifically, to equip the LLM with sufficient and appropriate domain knowledge, a parameterized prompt template is composed which can be utilized to generate numerous prompts. Test cases are generated via LLM with a prompt, and are dynamically evaluated through differential testing across multiple connectors. The testing is iteratively conducted, with each round RL is adopted to select optimal prompt based on prior-round behavioral feedback, so as to maximize control flow coverage. We implement aforementioned methodology in a practical tool and evaluate it on two widely used JDBC connectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported 16 bugs, among them 10 are officially confirmed and the rest are acknowledged as unsafe implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11870v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ce Lyu, Minghao Zhao, Yanhao Wang, Liang Jie</dc:creator>
    </item>
    <item>
      <title>Data Science: a Natural Ecosystem</title>
      <link>https://arxiv.org/abs/2506.11010</link>
      <description>arXiv:2506.11010v1 Announce Type: cross 
Abstract: This manuscript provides a holistic (data-centric) view of what we term essential data science, as a natural ecosystem with challenges and missions stemming from the data universe with its multiple combinations of the 5D complexities (data structure, domain, cardinality, causality, and ethics) with the phases of the data life cycle. Data agents perform tasks driven by specific goals. The data scientist is an abstract entity that comes from the logical organization of data agents with their actions. Data scientists face challenges that are defined according to the missions. We define specific discipline-induced data science, which in turn allows for the definition of pan-data science, a natural ecosystem that integrates specific disciplines with the essential data science. We semantically split the essential data science into computational, and foundational. We claim that there is a serious threat of divergence between computational and foundational data science. Especially, if no approach is taken to rate whether a data universe discovery should be useful or not. We suggest that rigorous approaches to measure the usefulness of data universe discoveries might mitigate such a divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11010v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilio Porcu (LIGM), Roy El Moukari (LIGM), Laurent Najman (LIGM), Francisco Herrera (UGR), Horst Simon (ADIA)</dc:creator>
    </item>
    <item>
      <title>Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task</title>
      <link>https://arxiv.org/abs/2506.11986</link>
      <description>arXiv:2506.11986v1 Announce Type: cross 
Abstract: Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10\% improvement in filter accuracy compared to the existing method. Our code is available at https://github.com/hongWin/Schema-R1/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11986v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wuzhenghong Wen, Su Pan, yuwei Sun</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Fair Max-Min Diversification in $\mathbb{R}^d$</title>
      <link>https://arxiv.org/abs/2404.04713</link>
      <description>arXiv:2404.04713v3 Announce Type: replace 
Abstract: The task of extracting a diverse subset from a dataset, often referred to as maximum diversification, plays a pivotal role in various real-world applications that have far-reaching consequences. In this work, we delve into the realm of fairness-aware data subset selection, specifically focusing on the problem of selecting a diverse set of size $k$ from a large collection of $n$ data points (FairDiv).
  The FairDiv problem is well-studied in the data management and theory community. In this work, we develop the first constant approximation algorithm for FairDiv that runs in near-linear time using only linear space. In contrast, all previously known constant approximation algorithms run in super-linear time (with respect to $n$ or $k$) and use super-linear space. Our approach achieves this efficiency by employing a novel combination of the Multiplicative Weight Update method and advanced geometric data structures to implicitly and approximately solve a linear program. Furthermore, we improve the efficiency of our techniques by constructing a coreset. Using our coreset, we also propose the first efficient streaming algorithm for the FairDiv problem whose efficiency does not depend on the distribution of data points. Empirical evaluation on million-sized datasets demonstrates that our algorithm achieves the best diversity within a minute. All prior techniques are either highly inefficient or do not generate a good solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04713v3</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654940</arxiv:DOI>
      <arxiv:journal_reference>SIGMOD 2024</arxiv:journal_reference>
      <dc:creator>Yash Kurkure, Miles Shamo, Joseph Wiseman, Sainyam Galhotra, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable with DBOS</title>
      <link>https://arxiv.org/abs/2506.10886</link>
      <description>arXiv:2506.10886v2 Announce Type: replace 
Abstract: To meet the needs of a large pharmaceutical organization, we set out to create S3Mirror - an application for transferring large genomic sequencing datasets between S3 buckets quickly, reliably, and observably. We used the DBOS Transact durable execution framework to achieve these goals and benchmarked the performance and cost of the application. S3Mirror is an open source DBOS Python application that can run in a variety of environments, including DBOS Cloud Pro, where it runs as much as 40x faster than AWS DataSync at a fraction of the cost. Moreover, S3Mirror is resilient to failures and allows for real-time filewise observability of ongoing and past transfers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10886v2</guid>
      <category>cs.DB</category>
      <category>q-bio.GN</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Vasquez-Grinnell, Alex Poliakov</dc:creator>
    </item>
  </channel>
</rss>
