<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 May 2025 04:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Range conditions on distributions and their possible application to geometric calibration in 2D parallel and fan-beam geometries</title>
      <link>https://arxiv.org/abs/2505.08805</link>
      <description>arXiv:2505.08805v1 Announce Type: new 
Abstract: In tomography, range conditions or data consistency conditions (DCCs) on functions have proven useful for geometric self-calibration, which involves identifying geometric parameters of acquisition systems based only on acquired radiographic images. These self-calibration methods using range conditions on functions typically require non-truncated data. In this work, we derive range conditions on distributions and demonstrate their application in addressing data truncation issues during the calibration process. We propose a novel approach based on range conditions on distributions, employing Dirac distributions to model markers within the field-of-view of an X-ray system. Our calibration methods are based on the local geometric information from non-truncated projections of a marker set. By applying range conditions to projections of sums of Dirac distributions, combined with specific calibration marker sets, we derive analytical formulas that enable the identification of geometric calibration parameters. We aim to present DCCs on distributions in tomography and explore the potential of DCCs on distributions as a possible tool in calibration. This approach represents one possible application, demonstrating how DCCs on distributions can effectively address challenges such as data truncation and incomplete marker set information. We present results for the 2D parallel geometry (Radon transform) and the 2D fan-beam geometry with sources on a line.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08805v1</guid>
      <category>stat.AP</category>
      <category>eess.IV</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasia Konik, Laurent Desbat</dc:creator>
    </item>
    <item>
      <title>Causal Feedback Discovery using Convergence Cross Mapping from Sea Ice Data</title>
      <link>https://arxiv.org/abs/2505.09001</link>
      <description>arXiv:2505.09001v1 Announce Type: new 
Abstract: The Arctic region is experiencing accelerated warming, largely driven by complex and nonlinear interactions among time series atmospheric variables such as, sea ice extent, short-wave radiation, temperature, and humidity. These interactions significantly alter sea ice dynamics and atmospheric conditions, leading to increased sea ice loss. This loss further intensifies Arctic amplification and disrupts weather patterns through various feedback mechanisms. Although stochastic methods such as Granger causality, PCMCI, and VarLiNGAM estimate causal interactions among atmospheric variables, they are limited to unidirectional causal relationships and often miss weak causal interactions and feedback loops in nonlinear settings. In this study, we show that Convergent Cross Mapping (CCM) can effectively estimate nonlinear causal coupling, identify weak interactions and causal feedback loops among atmospheric variables. CCM employs state space reconstruction (SSR) which makes it suitable for complex nonlinear dynamic systems. While CCM has been successfully applied to a diverse range of systems, including fisheries and online social networks, its application in climate science is under-explored. Our results show that CCM effectively uncovers strong nonlinear causal feedback loops and weak causal interactions often overlooked by stochastic methods in complex nonlinear dynamic atmospheric systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09001v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Nji, Seraj Al Mahmud Mostafa, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.09026</link>
      <description>arXiv:2505.09026v1 Announce Type: new 
Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining grid stability and enabling efficient integration of renewable energy sources. Gaussian Process (GP) models offer a principled framework for quantifying uncertainty; however, conventional approaches rely on stationary kernels, which are inadequate for modeling the inherently non-stationary nature of wind speed and power output. We propose a non-stationary GP framework that incorporates the generalized spectral mixture (GSM) kernel, enabling the model to capture time-varying patterns and heteroscedastic behaviors in wind speed and wind power data. We evaluate the performance of the proposed model on real-world SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons. Compared to standard radial basis function and spectral mixture kernels, the GSM-based model outperforms, particularly in short-term forecasts. These results highlight the necessity of modeling non-stationarity in wind power forecasting and demonstrate the practical value of non-stationary GP models in operational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09026v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Domniki Ladopoulou, Dat Minh Hong, Petros Dellaportas</dc:creator>
    </item>
    <item>
      <title>Generalizing imaging biomarker repeatability studies using Bayesian inference: Applications in detecting heterogeneous treatment response in whole-body diffusion-weighted MRI of metastatic prostate cancer</title>
      <link>https://arxiv.org/abs/2505.09197</link>
      <description>arXiv:2505.09197v1 Announce Type: new 
Abstract: The assessment of imaging biomarkers is critical for advancing precision medicine and improving disease characterization. Despite the availability of methods to derive disease heterogeneity metrics in imaging studies, a robust framework for evaluating measurement uncertainty remains underdeveloped. To address this gap, we propose a novel Bayesian framework to assess the precision of disease heterogeneity measures in biomarker studies.
  Our approach extends traditional methods for evaluating biomarker precision by providing greater flexibility in statistical assumptions and enabling the analysis of biomarkers beyond univariate or multivariate normally-distributed variables. Using Hamiltonian Monte Carlo sampling, the framework supports both, for example, normally-distributed and Dirichlet-Multinomial distributed variables, enabling the derivation of posterior distributions for biomarker parameters under diverse model assumptions. Designed to be broadly applicable across various imaging modalities and biomarker types, the framework builds a foundation for generalizing reproducible and objective biomarker evaluation.
  To demonstrate utility, we apply the framework to whole-body diffusion-weighted MRI (WBDWI) to assess heterogeneous therapeutic responses in metastatic bone disease. Specifically, we analyze data from two patient studies investigating treatments for metastatic castrate-resistant prostate cancer (mCRPC). Our results reveal an approximately 70% response rate among individual tumors across both studies, objectively characterizing differential responses to systemic therapies and validating the clinical relevance of the proposed methodology.
  This Bayesian framework provides a powerful tool for advancing biomarker research across diverse imaging-based studies while offering valuable insights into specific clinical applications, such as mCRPC treatment response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09197v1</guid>
      <category>stat.AP</category>
      <category>eess.IV</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew D Blackledge, Konstantinos Zormpas-Petridis, Ricardo Donners, Antonio Candito, David J Collins, Johann de Bono, Chris Parker, Dow-Mu Koh, Nina Tunariu</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction</title>
      <link>https://arxiv.org/abs/2505.08821</link>
      <description>arXiv:2505.08821v1 Announce Type: cross 
Abstract: Accurate blood glucose prediction can enable novel interventions for type 1 diabetes treatment, including personalized insulin and dietary adjustments. Although recent advances in transformer-based architectures have demonstrated the power of attention mechanisms in complex multivariate time series prediction, their potential for blood glucose (BG) prediction remains underexplored. We present a comparative analysis of transformer models for multi-horizon BG prediction, examining forecasts up to 4 hours and input history up to 1 week. The publicly available DCLP3 dataset (n=112) was split (80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset (n=12) served as an external test set. We trained networks with point-wise, patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal data. For short-term blood glucose prediction, Crossformer, a patch-wise transformer architecture, achieved a superior 30-minute prediction of RMSE (15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h), PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6 mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used tokenization through patches demonstrated improved accuracy with larger input sizes, with the best results obtained with a one-week history. These findings highlight the promise of transformer-based architectures for BG prediction by capturing and leveraging seasonal patterns in multivariate time-series data to improve accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08821v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meryem Altin Karagoz, Marc D. Breton, Anas El Fathi</dc:creator>
    </item>
    <item>
      <title>EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development</title>
      <link>https://arxiv.org/abs/2505.09054</link>
      <description>arXiv:2505.09054v1 Announce Type: cross 
Abstract: The construction industry is a major contributor to global greenhouse gas emissions, with embodied carbon being a key component. This study develops EcoSphere, an innovative software designed to evaluate and balance embodied and operational carbon emissions with construction and environmental costs in urban planning. Using high-resolution data from the National Structure Inventory, combined with computer vision and natural language processing applied to Google Street View and satellite imagery, EcoSphere categorizes buildings by structural and material characteristics with a bottom-up approach, creating a baseline emissions dataset. By simulating policy scenarios and mitigation strategies, EcoSphere provides policymakers and non-experts with actionable insights for sustainable development in cities and provide them with a vision of the environmental and financial results of their decisions. Case studies in Chicago and Indianapolis showcase how EcoSphere aids in assessing policy impacts on carbon emissions and costs, supporting data-driven progress toward carbon neutrality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09054v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siavash Ghorbany, Ming Hu, Siyuan Yao, Matthew Sisk, Chaoli Wang</dc:creator>
    </item>
    <item>
      <title>A Bayesian Treatment Selection Design for Phase II Randomised Cancer Clinical Trials</title>
      <link>https://arxiv.org/abs/2505.09460</link>
      <description>arXiv:2505.09460v1 Announce Type: cross 
Abstract: It is crucial to design Phase II cancer clinical trials that balance the efficiency of treatment selection with clinical practicality. Sargent and Goldberg proposed a frequentist design that allow decision-making even when the primary endpoint is ambiguous. However, frequentist approaches rely on fixed thresholds and long-run frequency properties, which can limit flexibility in practical applications. In contrast, the Bayesian decision rule, based on posterior probabilities, enables transparent decision-making by incorporating prior knowledge and updating beliefs with new data, addressing some of the inherent limitations of frequentist designs. In this study, we propose a novel Bayesian design, allowing selection of a best-performing treatment. Specifically, concerning phase II clinical trials with a binary outcome, our decision rule employs posterior interval probability by integrating the joint distribution over all values, for which the 'success rate' of the bester-performing treatment is greater than that of the other(s). This design can then determine which a treatment should proceed to the next phase, given predefined decision thresholds. Furthermore, we propose two sample size determination methods to empower such treatment selection designs implemented in a Bayesian framework. Through simulation studies and real-data applications, we demonstrate how this approach can overcome challenges related to sample size constraints in randomised trials. In addition, we present a user-friendly R Shiny application, enabling clinicians to Bayesian designs. Both our methodology and the software application can advance the design and analysis of clinical trials for evaluating cancer treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09460v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moka Komaki, Satoru Shinoda, Haiyan Zheng, Kouji Yamamoto</dc:creator>
    </item>
    <item>
      <title>Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios</title>
      <link>https://arxiv.org/abs/2505.09516</link>
      <description>arXiv:2505.09516v1 Announce Type: cross 
Abstract: Cluster analysis, or clustering, plays a crucial role across numerous scientific and engineering domains. Despite the wealth of clustering methods proposed over the past decades, each method is typically designed for specific scenarios and presents certain limitations in practical applications. In this paper, we propose depth-based local center clustering (DLCC). This novel method makes use of data depth, which is known to produce a center-outward ordering of sample points in a multivariate space. However, data depth typically fails to capture the multimodal characteristics of {data}, something of the utmost importance in the context of clustering. To overcome this, DLCC makes use of a local version of data depth that is based on subsets of {data}. From this, local centers can be identified as well as clusters of varying shapes. Furthermore, we propose a new internal metric based on density-based clustering to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a flexible clustering approach that seems to overcome some limitations of traditional clustering methods, thereby enhancing data analysis capabilities across a wide range of application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09516v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Practical privacy metrics for synthetic data</title>
      <link>https://arxiv.org/abs/2406.16826</link>
      <description>arXiv:2406.16826v3 Announce Type: replace 
Abstract: This paper explains how the synthpop package for R has been extended to include functions to calculate measures of identity and attribute disclosure risk for synthetic data that measure risks for the records used to create the synthetic data. The basic function, disclosure, calculates identity disclosure for a set of quasi-identifiers (keys) and attribute disclosure for one variable specified as a target from the same set of keys. The second function, disclosure.summary, is a wrapper for the first and presents summary results for a set of targets. This short paper explains the measures of disclosure risk and documents how they are calculated. We recommend two measures: $RepU$ (replicated uniques) for identity disclosure and $DiSCO$ (Disclosive in Synthetic Correct Original) for attribute disclosure. Both are expressed a \% of the original records and each can be compared to similar measures calculated from the original data. Experience with using the functions on real data found that some apparent disclosures could be identified as coming from relationships in the data that would be expected to be known to anyone familiar with its features. We flag cases when this seems to have occurred and provide means of excluding them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16826v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gillian M Raab, Beata Nowok, Chris Dibben</dc:creator>
    </item>
    <item>
      <title>Machine learning bridging battery field data and laboratory data</title>
      <link>https://arxiv.org/abs/2505.05364</link>
      <description>arXiv:2505.05364v2 Announce Type: replace 
Abstract: Aiming at the dilemma that most laboratory data-driven diagnostic and prognostic methods cannot be applied to field batteries in passenger cars and energy storage systems, this paper proposes a method to bridge field data and laboratory data using machine learning. Only two field real impedances corresponding to a medium frequency and a high frequency are needed to predict laboratory real impedance curve, laboratory charge/discharge curve, and laboratory relaxation curve. Based on the predicted laboratory data, laboratory data-driven methods can be used for field battery diagnosis and prognosis. Compared with the field data-driven methods based on massive historical field data, the proposed method has the advantages of higher accuracy, lower cost, faster speed, readily available, and no use of private data. The proposed method is tested using two open-source datasets containing 249 NMC cells. For a test set containing 76 cells, the mean absolute percentage errors of laboratory real impedance curve, charge curve, and discharge curve prediction results are 0.85%, 4.72%, and 2.69%, respectively. This work fills the gap between laboratory data-driven diagnostic and prognostic methods and field battery applications, making all laboratory data-driven methods applicable to field battery diagnosis and prognosis. Furthermore, this work overturns the fixed path of developing field battery diagnostic and prognostic methods based on massive field historical data, opening up new research and breakthrough directions for field battery diagnosis and prognosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05364v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbin Zhao, Hao Liu, Zhihua Deng, Tong Li, Haoyi Jiang, Zhenfei Ling, Xingkai Wang, Lei Zhang, Xiaoping Ouyang</dc:creator>
    </item>
    <item>
      <title>Evacuation decisions in response to natural disasters: Insights from a large-scale social media survey</title>
      <link>https://arxiv.org/abs/2008.03665</link>
      <description>arXiv:2008.03665v2 Announce Type: replace-cross 
Abstract: Evacuation in response to natural disasters is a complex process involving multiple decision-makers at the personal, household, community, and government levels. Consequently, many disparate factors influence who evacuates, when, and how to respond to a nearby disaster. In this paper, we leverage a novel method of data collection through social media to explore the evacuation response decisions of people in areas affected by the 2019-2020 Australian bushfires. We explore the validity of this data collection method for generating plausible estimates of evacuation and its ability to supplement cell phone location data using survey responses. Ultimately, we identify several key factors influencing household decisions on evacuation, specifically focusing on the phenomenon of household members evacuating or returning from evacuation at different times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.03665v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paige Maas, Zack Almquist, Eugenia Giraudy, JW Schneider</dc:creator>
    </item>
    <item>
      <title>A Bayesian functional model with multilevel partition priors for group studies in neuroscience</title>
      <link>https://arxiv.org/abs/2312.16739</link>
      <description>arXiv:2312.16739v2 Announce Type: replace-cross 
Abstract: The statistical analysis of group studies in neuroscience is particularly challenging due to the complex spatio-temporal nature of the data, its multiple levels and the inter-individual variability in brain responses. In this respect, traditional ANOVA-based studies and linear mixed effects models typically provide only limited exploration of the dynamic of the group brain activity and variability of the individual responses potentially leading to overly simplistic conclusions and/or missing more intricate patterns. In this study we propose a novel Bayesian model-based clustering method for functional data to simultaneously assess group effects and individual deviations over the most important temporal features in the data. To this aim, we develop an innovative multilevel partition prior to model the functional scores of a functional Principal Components decomposition of neuroscientific recordings; this approach returns a thorough exploration of group differences and individual deviations without compromising on the spatio-temporal nature of the data. By means of a simulation study we demonstrate that the proposed model returns correct classification in different clustering scenarios under low and high noise levels in the data. Finally we consider a case study using Electroencephalogram data recorded during an object recognition task where our approach provides new insights into the underlying brain mechanisms generating the data and their variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16739v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicol\`o Margaritella, Vanda In\'acio, Ruth King</dc:creator>
    </item>
    <item>
      <title>Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</title>
      <link>https://arxiv.org/abs/2502.19086</link>
      <description>arXiv:2502.19086v3 Announce Type: replace-cross 
Abstract: We adopt Gaussian Processes (GPs) as latent functions for probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when making predictions. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which has both a point mass at zero and heavy tails, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19086v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Damato, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
  </channel>
</rss>
