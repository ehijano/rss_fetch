<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Assessing the impacts of convening experts: a bibliometric analysis of a research program spanning four decades</title>
      <link>https://arxiv.org/abs/2410.07608</link>
      <description>arXiv:2410.07608v1 Announce Type: new 
Abstract: Over the last few decades, research institutions and funders have begun policies and programs that incentivize large-scale collaboration across institutions on focal research areas. Yet, few studies have evaluated the impact of those programs on research, particularly on timelines longer than a few years. Using the Canadian Institute for Advanced Research (CIFAR) as a case study, we examined the impacts of supporting a research program that convened experts across intuitions and countries for 40+ years. In this study, we used the Scopus bibliometric database to analyse publishing and citation trends within this team since its formation in 1986 and used nearest neighbour matching to compare these trends against authors across the globe with similar career characteristics to measure how effectively the CIFAR program Gravity &amp; the Extreme Universe (CIFAR-GEU) has catalyzed collaborations and produced high quality research outputs. We found a greater degree of co-authorship within the CIFAR-GEU group compared to the Control group. We also found that the outputs generated by the CIFAR-GEU group had, overall, higher values for citation-based impact indicators (e.g., stronger metrics around citations, impact from international collaborations and reach beyond academia).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07608v1</guid>
      <category>cs.DL</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deborah M. Buehler, Mark J Daley, Kyle Demes</dc:creator>
    </item>
    <item>
      <title>PubMed knowledge graph 2.0: Connecting papers, patents, and clinical trials in biomedical science</title>
      <link>https://arxiv.org/abs/2410.07969</link>
      <description>arXiv:2410.07969v1 Announce Type: new 
Abstract: Papers, patents, and clinical trials are indispensable types of scientific literature in biomedicine, crucial for knowledge sharing and dissemination. However, these documents are often stored in disparate databases with varying management standards and data formats, making it challenging to form systematic, fine-grained connections among them. To address this issue, we introduce PKG2.0, a comprehensive knowledge graph dataset encompassing over 36 million papers, 1.3 million patents, and 0.48 million clinical trials in the biomedical field. PKG2.0 integrates these previously dispersed resources through various links, including biomedical entities, author networks, citation relationships, and research projects. Fine-grained biomedical entity extraction, high-performance author name disambiguation, and multi-source citation integration have played a crucial role in the construction of the PKG dataset. Additionally, project data from the NIH Exporter enriches the dataset with metadata of NIH-funded projects and their scholarly outputs. Data validation demonstrates that PKG2.0 excels in key tasks such as author disambiguation and biomedical entity recognition. This dataset provides valuable resources for biomedical researchers, bibliometric scholars, and those engaged in literature mining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07969v1</guid>
      <category>cs.DL</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jian Xu, Chao Yu, Jiawei Xu, Ying Ding, Vetle I. Torvik, Jaewoo Kang, Mujeen Sung, Min Song</dc:creator>
    </item>
    <item>
      <title>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders</title>
      <link>https://arxiv.org/abs/2405.17044</link>
      <description>arXiv:2405.17044v2 Announce Type: replace-cross 
Abstract: The rapid growth of scientific literature makes it challenging for researchers to identify novel and impactful ideas, especially across disciplines. Modern artificial intelligence (AI) systems offer new approaches, potentially inspiring ideas not conceived by humans alone. But how compelling are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, which uses 58 million research papers and a large-language model to generate research ideas. We conduct a large-scale evaluation in which over 100 research group leaders - from natural sciences to humanities - ranked more than 4,400 personalized ideas based on their interest. This data allows us to predict research interest using (1) supervised neural networks trained on human evaluations, and (2) unsupervised zero-shot ranking with large-language models. Our results demonstrate how future systems can help generating compelling research ideas and foster unforeseen interdisciplinary collaborations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17044v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuemei Gu, Mario Krenn</dc:creator>
    </item>
    <item>
      <title>Gymnasium: A Standard Interface for Reinforcement Learning Environments</title>
      <link>https://arxiv.org/abs/2407.17032</link>
      <description>arXiv:2407.17032v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) is a continuously growing field that has the potential to revolutionize many areas of artificial intelligence. However, despite its promise, RL research is often hindered by the lack of standardization in environment and algorithm implementations. This makes it difficult for researchers to compare and build upon each other's work, slowing down progress in the field. Gymnasium is an open-source library that provides a standard API for RL environments, aiming to tackle this issue. Gymnasium's main feature is a set of abstractions that allow for wide interoperability between environments and training algorithms, making it easier for researchers to develop and test RL algorithms. In addition, Gymnasium provides a collection of easy-to-use environments, tools for easily customizing environments, and tools to ensure the reproducibility and robustness of RL research. Through this unified framework, Gymnasium significantly streamlines the process of developing and testing RL algorithms, enabling researchers to focus more on innovation and less on implementation details. By providing a standardized platform for RL research, Gymnasium helps to drive forward the field of reinforcement learning and unlock its full potential. Gymnasium is available online at https://github.com/Farama-Foundation/Gymnasium</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17032v2</guid>
      <category>cs.LG</category>
      <category>cs.DL</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariel Kwiatkowski, Mark Towers, Jordan Terry, John U. Balis, Gianluca De Cola, Tristan Deleu, Manuel Goul\~ao, Andreas Kallinteris, Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierr\'e, Sander Schulhoff, Jun Jet Tai, Hannah Tan, Omar G. Younis</dc:creator>
    </item>
  </channel>
</rss>
