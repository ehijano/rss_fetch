<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 02:37:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Probably approximately correct high-dimensional causal effect estimation given a valid adjustment set</title>
      <link>https://arxiv.org/abs/2411.08141</link>
      <description>arXiv:2411.08141v1 Announce Type: new 
Abstract: Accurate estimates of causal effects play a key role in decision-making across applications such as healthcare, economics, and operations. In the absence of randomized experiments, a common approach to estimating causal effects uses \textit{covariate adjustment}. In this paper, we study covariate adjustment for discrete distributions from the PAC learning perspective, assuming knowledge of a valid adjustment set $\bZ$, which might be high-dimensional. Our first main result PAC-bounds the estimation error of covariate adjustment by a term that is exponential in the size of the adjustment set; it is known that such a dependency is unavoidable even if one only aims to minimize the mean squared error. Motivated by this result, we introduce the notion of an \emph{$\eps$-Markov blanket}, give bounds on the misspecification error of using such a set for covariate adjustment, and provide an algorithm for $\eps$-Markov blanket discovery; our second main result upper bounds the sample complexity of this algorithm. Furthermore, we provide a misspecification error bound and a constraint-based algorithm that allow us to go beyond $\eps$-Markov blankets to even smaller adjustment sets. Our third main result upper bounds the sample complexity of this algorithm, and our final result combines the first three into an overall PAC bound. Altogether, our results highlight that one does not need to perfectly recover causal structure in order to ensure accurate estimates of causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08141v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davin Choo, Chandler Squires, Arnab Bhattacharyya, David Sontag</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v1 Announce Type: cross 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, is a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent US Food \&amp; Drug Administration (FDA) and European Medical Agency (EMA) guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. In this paper, we show that an HOIF-motivated estimator for the treatment-specific mean has significantly improved statistical properties compared to popular adjusted estimators in practice when the number of baseline covariates $p$ is relatively large compared to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted estimator. Furthermore, we demonstrate that a novel debiased adjusted estimator proposed recently by Lu et al. is, in fact, another HOIF-motivated estimator under disguise. Finally, simulation studies are conducted to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Interaction Testing in Variation Analysis</title>
      <link>https://arxiv.org/abs/2411.08861</link>
      <description>arXiv:2411.08861v1 Announce Type: cross 
Abstract: Relationships of cause and effect are of prime importance for explaining scientific phenomena. Often, rather than just understanding the effects of causes, researchers also wish to understand how a cause $X$ affects an outcome $Y$ mechanistically -- i.e., what are the causal pathways that are activated between $X$ and $Y$. For analyzing such questions, a range of methods has been developed over decades under the rubric of causal mediation analysis. Traditional mediation analysis focuses on decomposing the average treatment effect (ATE) into direct and indirect effects, and therefore focuses on the ATE as the central quantity. This corresponds to providing explanations for associations in the interventional regime, such as when the treatment $X$ is randomized. Commonly, however, it is of interest to explain associations in the observational regime, and not just in the interventional regime. In this paper, we introduce \text{variation analysis}, an extension of mediation analysis that focuses on the total variation (TV) measure between $X$ and $Y$, written as $\mathrm{E}[Y \mid X=x_1] - \mathrm{E}[Y \mid X=x_0]$. The TV measure encompasses both causal and confounded effects, as opposed to the ATE which only encompasses causal (direct and mediated) variations. In this way, the TV measure is suitable for providing explanations in the natural regime and answering questions such as ``why is $X$ associated with $Y$?''. Our focus is on decomposing the TV measure, in a way that explicitly includes direct, indirect, and confounded variations. Furthermore, we also decompose the TV measure to include interaction terms between these different pathways. Subsequently, interaction testing is introduced, involving hypothesis tests to determine if interaction terms are significantly different from zero. If interactions are not significant, more parsimonious decompositions of the TV measure can be used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08861v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drago Plecko</dc:creator>
    </item>
    <item>
      <title>Selecting Penalty Parameters of High-Dimensional M-Estimators using Bootstrapping after Cross-Validation</title>
      <link>https://arxiv.org/abs/2104.04716</link>
      <description>arXiv:2104.04716v5 Announce Type: replace 
Abstract: We develop a new method for selecting the penalty parameter for $\ell_{1}$-penalized M-estimators in high dimensions, which we refer to as bootstrapping after cross-validation. We derive rates of convergence for the corresponding $\ell_1$-penalized M-estimator and also for the post-$\ell_1$-penalized M-estimator, which refits the non-zero entries of the former estimator without penalty in the criterion function. We demonstrate via simulations that our methods are not dominated by cross-validation in terms of estimation errors and can outperform cross-validation in terms of inference. As an empirical illustration, we revisit Fryer Jr (2019), who investigated racial differences in police use of force, and confirm his findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.04716v5</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Chetverikov, Jesper Riis-Vestergaard S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Strong Approximations for Empirical Processes Indexed by Lipschitz Functions</title>
      <link>https://arxiv.org/abs/2406.04191</link>
      <description>arXiv:2406.04191v2 Announce Type: replace 
Abstract: This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by possibly Lipschitz functions, improving on previous results in the literature. In the setting considered by Rio (1994), and if the function class is Lipschitzian, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\max\{d,2\}}$, up to a $\operatorname{polylog}(n)$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the rate $n^{-1/2}\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml\'os et al., 1975). Second, a uniform Gaussian strong approximation is established for multiplicative separable empirical processes indexed by possibly Lipschitz functions, which addresses some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). Finally, two other uniform Gaussian strong approximation results are presented when the function class is a sequence of Haar basis based on quasi-uniform partitions. Applications to nonparametric density and regression estimation are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04191v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Improved performance guarantees for Tukey's median</title>
      <link>https://arxiv.org/abs/2410.00219</link>
      <description>arXiv:2410.00219v2 Announce Type: replace 
Abstract: Is there a natural way to order data in dimension greater than one? The approach based on the notion of data depth, often associated with the name of John Tukey, is among the most popular. Tukey's depth has found applications in robust statistics, graph theory, and the study of elections and social choice. We present improved performance guarantees for empirical Tukey's median, a deepest point associated with the given sample, when the data-generating distribution is elliptically symmetric and possibly anisotropic. Some of our results remain valid in the wider class of affine equivariant estimators. As a corollary of our bounds, we show that the diameter of the set of all empirical Tukey's medians scales like $o(n^{-1/2})$ where $n$ is the sample size. Moreover, when the data are 2-dimensional, we prove that the diameter is of order $O(n^{-3/4}\log^{3/2}(n))$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00219v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislav Minsker, Yinan Shen</dc:creator>
    </item>
    <item>
      <title>Efficiency of estimators for locally asymptotically normal quantum statistical models</title>
      <link>https://arxiv.org/abs/2209.00832</link>
      <description>arXiv:2209.00832v2 Announce Type: replace-cross 
Abstract: We herein establish an asymptotic representation theorem for locally asymptotically normal quantum statistical models. This theorem enables us to study the asymptotic efficiency of quantum estimators such as quantum regular estimators and quantum minimax estimators, leading to a universal tight lower bound beyond the i.i.d. assumption. This formulation complements the theory of quantum contiguity developed in the previous paper [Fujiwara and Yamagata, Bernoulli 26 (2020) 2105-2141], providing a solid foundation of the theory of weak quantum local asymptotic normality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00832v2</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/23-AOS2285</arxiv:DOI>
      <arxiv:journal_reference>Ann. Statist. 51 (3) 1159 - 1182, June 2023</arxiv:journal_reference>
      <dc:creator>Akio Fujiwara, Koichi Yamagata</dc:creator>
    </item>
    <item>
      <title>Robust Quickest Change Detection in Non-Stationary Processes</title>
      <link>https://arxiv.org/abs/2310.09673</link>
      <description>arXiv:2310.09673v2 Announce Type: replace-cross 
Abstract: Optimal algorithms are developed for robust detection of changes in non-stationary processes. These are processes in which the distribution of the data after change varies with time. The decision-maker does not have access to precise information on the post-change distribution. It is shown that if the post-change non-stationary family has a distribution that is least favorable in a well-defined sense, then the algorithms designed using the least favorable distributions are robust and optimal. Non-stationary processes are encountered in public health monitoring and space and military applications. The robust algorithms are applied to real and simulated data to show their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09673v2</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingze Hou, Yousef Oleyaeimotlagh, Rahul Mishra, Hoda Bidkhori, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Predictive Inference in Multi-environment Scenarios</title>
      <link>https://arxiv.org/abs/2403.16336</link>
      <description>arXiv:2403.16336v2 Announce Type: replace-cross 
Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, potentially hierarchical data-generating scenarios. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets. Our contributions also include extensions for settings with non-real-valued responses, a theory of consistency for predictive inference in these general problems, and insights on the limits of conditional coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16336v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Discrete-Valued Signal Estimation via Low-Complexity Message Passing Algorithm for Highly Correlated Measurements</title>
      <link>https://arxiv.org/abs/2411.07558</link>
      <description>arXiv:2411.07558v2 Announce Type: replace-cross 
Abstract: This paper considers a discrete-valued signal estimation scheme based on a low-complexity Bayesian optimal message passing algorithm (MPA) for solving massive linear inverse problems under highly correlated measurements. Gaussian belief propagation (GaBP) can be derived by applying the central limit theorem (CLT)-based Gaussian approximation to the sum-product algorithm (SPA) operating on a dense factor graph (FG), while matched filter (MF)-expectation propagation (EP) can be obtained based on the EP framework tailored for the same FG. Generalized approximate message passing (GAMP) can be found by applying a rigorous approximation technique for both of them in the large-system limit, and these three MPAs perform signal detection using MF by assuming large-scale uncorrelated observations. However, each of them has a different inherent self-noise suppression mechanism, which makes a significant difference in the robustness against the correlation of the observations when we apply an annealed discrete denoiser (ADD) that adaptively controls its nonlinearity with the inverse temperature parameter corresponding to the number of iterations. In this paper, we unravel the mechanism of this interesting phenomenon, and further demonstrate the practical applicability of the low-complexity Bayesian optimal MPA with ADD under highly correlated measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07558v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoharu Furudoi, Takumi Takahashi, Shinsuke Ibi, Hideki Ochiai</dc:creator>
    </item>
  </channel>
</rss>
