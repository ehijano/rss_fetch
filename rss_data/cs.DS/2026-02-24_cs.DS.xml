<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 02:44:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact Algorithms for Resource Reallocation Under Budgetary Constraints</title>
      <link>https://arxiv.org/abs/2602.18438</link>
      <description>arXiv:2602.18438v1 Announce Type: new 
Abstract: Efficient resource (re-)allocation is a critical challenge in optimizing productivity and sustainability within multi-party supply networks. In this work, we introduce the \textsc{Red-Blue Reinforcement} (R-BR) problem, where a service provider under budgetary constraints must minimize client reallocations to reduce the required number of servers they should maintain by a specified amount. We conduct a systematic algorithmic study, providing three exact algorithms that scale well as the input grows (FPT), which could prove useful in practice. Our algorithms are efficient for topologies that model rural road networks (bounded distance to cluster), modern transportation systems (bounded modular-width), or have bounded clique-width, a parameter that is of great theoretical importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18438v1</guid>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arun Kumar Das, Sandip Das, Sweta Das, Foivos Fioravantes, Nikolaos Melissinos</dc:creator>
    </item>
    <item>
      <title>Strengths and Limitations of Greedy in Cup Games</title>
      <link>https://arxiv.org/abs/2602.18610</link>
      <description>arXiv:2602.18610v1 Announce Type: new 
Abstract: In the cup game, an adversary distributes 1 unit of water among $n$ cups every time step. The player then selects a single cup from which to remove 1 unit of water. In the bamboo trimming problem, the adversary must choose fixed rates for the cups, and the player is additionally allowed to empty the chosen cup entirely. Past work has shown that the optimal backlog in these two settings is $\Theta(\log n)$ and 2 respectively.
  The greedy algorithm has been shown in previous work to be exactly optimal in the general cup game and asymptotically optimal in the bamboo setting. The greedy algorithm has been conjectured [16] to achieve the exactly optimal backlog of 2 in the bamboo setting as well. In this paper, we prove a lower bound of $2.076$ for the backlog of the greedy algorithm, disproving the conjecture of [16]. We also introduce a new algorithm, a hybrid greedy/Deadline-Driven, which achieves backlog $O(\log n)$ in the general cup game, and remains exactly optimal for the bamboo trimming problem and the fixed-rate cup game -- this constitutes the first algorithm that achieves asymptotically optimal performance across all three settings.
  Additionally, we introduce a new model, the semi-oblivious cup game, in which the player is uncertain of the exact heights of each cup. We analyze the performance of the greedy algorithm in this setting, which can be viewed as selecting an arbitrary cup within a constant multiplicative factor of the fullest cup. We prove matching upper and lower bounds showing that the greedy algorithm achieves a backlog of $\Theta(n^{\frac{c-1}{c}})$ in the semi-oblivious cup game. We also establish matching upper and lower bounds of $2^{\Theta(\sqrt{\log n})}$ in the semi-oblivious cup flushing game. Finally, we show that in an additive error setting, greedy is actually able to achieve backlog $\Theta(\log n)$, via matching upper and lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18610v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kalina Jasi\'nska, John Kuszmaul, Gyudong Lee</dc:creator>
    </item>
    <item>
      <title>Dynamic data structures for twin-ordered matrices</title>
      <link>https://arxiv.org/abs/2602.18770</link>
      <description>arXiv:2602.18770v1 Announce Type: new 
Abstract: We present a dynamic data structure for representing binary $n\times n$ matrices that are $d$-twin-ordered, for a~fixed parameter $d$. Our structure supports cell queries and single-cell updates both in $\Oh(\log \log n)$ expected worst case time, while using $\Oh_d(n)$ memory; here, the $\Oh_d(\cdot)$ notation</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18770v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bart{\l}omiej Bosek, Jadwiga Czy\.zewska, Evangelos Kipouridis, Wojciech Nadara, Micha{\l} Pilipczuk, Karol W\k{e}grzycki, Anna Zych-Pawlewicz</dc:creator>
    </item>
    <item>
      <title>EdgeSketch: Efficient Analysis of Massive Graph Streams</title>
      <link>https://arxiv.org/abs/2602.18957</link>
      <description>arXiv:2602.18957v1 Announce Type: new 
Abstract: We introduce EdgeSketch, a compact graph representation for efficient analysis of massive graph streams. EdgeSketch provides unbiased estimators for key graph properties with controllable variance and supports implementing graph algorithms on the stored summary directly. It is constructed in a fully streaming manner, requiring a single pass over the edge stream, while offline analysis relies solely on the sketch. We evaluate the proposed approach on two representative applications: community detection via the Louvain method and graph reconstruction through node similarity estimation. Experiments demonstrate substantial memory savings and runtime improvements over both lossless representations and prior sketching approaches, while maintaining reliable accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18957v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Lemiesz, Dingqi Yang, Philippe Cudr\'e-Mauroux</dc:creator>
    </item>
    <item>
      <title>One Color Makes All the Difference in the Tractability of Partial Coloring in Semi-Streaming</title>
      <link>https://arxiv.org/abs/2602.18987</link>
      <description>arXiv:2602.18987v1 Announce Type: new 
Abstract: This paper investigates the semi-streaming complexity of \textit{$k$-partial coloring}, a generalization of proper graph coloring. For $k \geq 1$, a $k$-partial coloring requires that each vertex $v$ in an $n$-node graph is assigned a color such that at least $\min\{k, \deg(v)\}$ of its neighbors are assigned colors different from its own. This framework naturally extends classical coloring problems: specifically, $k$-partial $(k+1)$-coloring and $k$-partial $k$-coloring generalize $(\Delta+1)$-proper coloring and $\Delta$-proper coloring, respectively.
  Prior works of Assadi, Chen, and Khanna [SODA~2019] and Assadi, Kumar, and Mittal [TheoretiCS~2023] show that both $(\Delta+1)$-proper coloring and $\Delta$-proper coloring admit one-pass randomized semi-streaming algorithms. We explore whether these efficiency gains extend to their partial coloring generalizations and reveal a sharp computational threshold : while $k$-partial $(k+1)$-coloring admits a one-pass randomized semi-streaming algorithm, the $k$-partial $k$-coloring remains semi-streaming intractable, effectively demonstrating a ``dichotomy of one color'' in the streaming model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18987v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avinandan Das</dc:creator>
    </item>
    <item>
      <title>An efficient recursive decomposition algorithm for undirected graphs</title>
      <link>https://arxiv.org/abs/2602.19189</link>
      <description>arXiv:2602.19189v1 Announce Type: new 
Abstract: The decomposition of undirected graphs simplifies complex problems by breaking them into solvable subgraphs, following the philosophy of divide and conquer. This paper investigates the relationship between atom decomposition and the maximum cardinality search (MCS) ordering in general undirected graphs. Specifically, we prove that applying a convex extension to the node numbered $1$ and its neighborhood in an MCS ordering yields an atom in the graph. Furthermore, based on the MCS ordering, we introduce a recursive algorithm for decomposing an undirected graph into its atoms. This approach closely aligns with the results of chordal graph decomposition. As a result, minimal triangulation of the graph is no longer required, and the identification of clique minimal separators is avoided. In the experimental section, we combine the proposed decomposition algorithm with two existing convex expansion methods. The results show that both combinations significantly outperform the existing algorithms in terms of efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19189v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pei Heng, Yi Sun, Jianhua Guo</dc:creator>
    </item>
    <item>
      <title>On Identifying Critical Network Edges via Analyzing Changes in Shapes (Curvatures)</title>
      <link>https://arxiv.org/abs/2602.19328</link>
      <description>arXiv:2602.19328v1 Announce Type: new 
Abstract: In recent years extensions of manifold Ricci curvature to discrete combinatorial objects such as graphs and hypergraphs (popularly called as "network shapes"), have found a plethora of applications in a wide spectrum of research areas ranging over metabolic systems, transcriptional regulatory networks, protein-protein-interaction networks, social networks and brain networks to deep learning models and quantum computing but, in contrast, they have been looked at by relatively fewer researchers in the algorithms and computational complexity community. As an attempt to bring these network Ricci-curvature related problems under the lens of computational complexity and foster further inter-disciplinary interactions, we provide a formal framework for studying algorithmic and computational complexity issues for detecting critical edges in an undirected graph using Ollivier-Ricci curvatures and provide several algorithmic and inapproximability results for problems in this framework. Our results show some interesting connections between the exact perfect matching and perfect matching blocker problems for bipartite graphs and our problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19328v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bhaskar DasGupta, Katie Kruzan</dc:creator>
    </item>
    <item>
      <title>Variations on the Problem of Identifying Spectrum-Preserving String Sets</title>
      <link>https://arxiv.org/abs/2602.19408</link>
      <description>arXiv:2602.19408v1 Announce Type: new 
Abstract: In computational genomics, many analyses rely on efficient storage and traversal of $k$-mers, motivating compact representations such as spectrum-preserving string sets (SPSS), which store strings whose $k$-mer spectrum matches that of the input. Existing approaches, including Unitigs, Eulertigs and Matchtigs, model this task as a path cover problem on the deBruijn graph. We extend this framework from paths to branching structures by introducing necklace covers, which combine cycles and tree-like attachments (pendants). We present a greedy algorithm that constructs a necklace cover while guaranteeing, under certain conditions, optimality in the cumulative size of the final representation.
  Experiments on real genomic datasets indicate that the minimum necklace cover achieves smaller representations than Eulertigs and comparable compression to the Masked Superstrings approach, while maintaining exactness of the $k$-mer spectrum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19408v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sankardeep Chakraborty, Roberto Grossi, Ren Kimura, Giulia Punzi, Kunihiko Sadakane, Wiktor Zuba</dc:creator>
    </item>
    <item>
      <title>Covering a Polyomino-Shaped Stain with Non-Overlapping Identical Stickers</title>
      <link>https://arxiv.org/abs/2602.19525</link>
      <description>arXiv:2602.19525v1 Announce Type: new 
Abstract: You find a stain on the wall and decide to cover it with non-overlapping stickers of a single identical shape (rotation and reflection are allowed). Is it possible to find a sticker shape that fails to cover the stain? In this paper, we consider this problem under polyomino constraints and complete the classification of always-coverable stain shapes (polyominoes). We provide proofs for the maximal always-coverable polyominoes and construct concrete counterexamples for the minimal not always-coverable ones, demonstrating that such cases exist even among hole-free polyominoes. This classification consequently yields an algorithm to determine the always-coverability of any given stain. We also show that the problem of determining whether a given sticker can cover a given stain is $\NP$-complete, even though exact cover is not demanded. This result extends to the 1D case where the connectivity requirement is removed. As an illustration of the problem complexity, for a specific hexomino (6-cell) stain, the smallest sticker found in our search that avoids covering it has, although not proven minimum, a bounding box of $325 \times 325$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19525v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keigo Oka, Naoki Inaba, Akira Iino</dc:creator>
    </item>
    <item>
      <title>Minimizing Total Travel Time for Collaborative Package Delivery with Heterogeneous Drones</title>
      <link>https://arxiv.org/abs/2602.19535</link>
      <description>arXiv:2602.19535v1 Announce Type: new 
Abstract: Given a fleet of drones with different speeds and a set of package delivery requests, the collaborative delivery problem asks for a schedule for the drones to collaboratively carry out all package deliveries, with the objective of minimizing the total travel time of all drones. We show that the best non-preemptive schedule (where a package that is picked up at its source is immediately delivered to its destination by one drone) is within a factor of three of the best preemptive schedule (where several drones can participate in the delivery of a single package). Then, we present a constant-factor approximation algorithm for the problem of computing the best non-preemptive schedule. The algorithm reduces the problem to a tree combination problem and uses a primal-dual approach to solve the latter. We have implemented a version of the algorithm optimized for practical efficiency and report the results of experiments on large-scale instances with synthetic and real-world data, demonstrating that our algorithm is scalable and delivers schedules of excellent quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19535v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Erlebach, Kelin Luo, Wen Zhang</dc:creator>
    </item>
    <item>
      <title>Analyzing and Leveraging the $k$-Sensitivity of LZ77</title>
      <link>https://arxiv.org/abs/2602.19649</link>
      <description>arXiv:2602.19649v1 Announce Type: new 
Abstract: We study the sensitivity of the Lempel-Ziv 77 compression algorithm to edits, showing how modifying a string $w$ can deteriorate or improve its compression. Our first result is a tight upper bound for $k$ edits: $\forall w' \in B(w,k)$, we have $C_{\mathrm{LZ77}}(w') \leq 3 \cdot C_{\mathrm{LZ77}}(w) + 4k$. This result contrasts with Lempel-Ziv 78, where a single edit can significantly deteriorate compressibility, a phenomenon known as a *one-bit catastrophe*.
  We further refine this bound, focusing on the coefficient $3$ in front of $C_{\mathrm{LZ77}}(w)$, and establish a surprising trichotomy based on the compressibility of $w$. More precisely we prove the following bounds:
  - if $C_{\mathrm{LZ77}}(w) \lesssim k^{3/2}\sqrt{n}$, the compression may increase by up to a factor of $\approx 3$,
  - if $k^{3/2}\sqrt{n} \lesssim C_{\mathrm{LZ77}}(w) \lesssim k^{1/3}n^{2/3}$, this factor is at most $\approx 2$,
  - if $C_{\mathrm{LZ77}}(w) \gtrsim k^{1/3}n^{2/3}$, the factor is at most $\approx 1$.
  Finally, we present an $\varepsilon$-approximation algorithm to pre-edit a word $w$ with a budget of $k$ modifications to improve its compression. In favorable scenarios, this approach yields a total compressed size reduction by up to a factor of~$3$, accounting for both the LZ77 compression of the modified word and the cost of storing the edits, $C_{\mathrm{LZ77}}(w') + k \log |w|$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19649v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Bathie, Paul Huber, Guillaume Lagarde, Akka Zemmari</dc:creator>
    </item>
    <item>
      <title>Exploration of Always $S$-Connected Temporal Graphs</title>
      <link>https://arxiv.org/abs/2602.19657</link>
      <description>arXiv:2602.19657v1 Announce Type: new 
Abstract: \emph{Temporal graphs} are a generalisation of (static) graphs, defined by a sequence of \emph{snapshots}, each a static graph defined over a common set of vertices. \emph{Exploration} problems are one of the most fundamental and most heavily studied problems on temporal graphs, asking if a set of $m$ agents can visit every vertex in the graph, with each agent only allowed to traverse a single edge per snapshot. In this paper, we introduce and study \emph{always $S$-connected} temporal graphs, a generalisation of always connected temporal graphs where, rather than forming a single connected component in each snapshot, we have at most $\vert S \vert$ components, each defined by the connection to a single vertex in the set $S$. We use this formulation as a tool for exploring graphs admitting an \emph{$(r,b)$-division}, a partitioning of the vertex set into disconnected components, each of which is $S$-connected, where $\vert S \vert \leq b$.
  We show that an always $S$-connected temporal graph with $m = \vert S \vert$ and an average degree of $\Delta$ can be explored by $m$ agents in $O(n^{1.5} m^3 \Delta^{1.5}\log^{1.5}(n))$ snapshots. Using this as a subroutine, we show that any always-connected temporal graph with treewidth at most $k$ can be explored by a single agent in $O\left(n^{4/3} k^{5.5}\log^{2.5}(n)\right)$ snapshots, improving on the current state-of-the-art for small values of $k$. Further, we show that interval graph with only a small number of large cliques can be explored by a single agent in $O\left(n^{4/3} \log^{2.5}(n)\right)$ snapshots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19657v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duncan Adamson, Paul G Spirakis</dc:creator>
    </item>
    <item>
      <title>Servicing Matched Client Pairs with Facilities</title>
      <link>https://arxiv.org/abs/2602.19680</link>
      <description>arXiv:2602.19680v1 Announce Type: new 
Abstract: We study Facility Location with Matching, a Facility Location problem where, given additional information about which pair of clients is compatible to be matched, we need to match as many clients as possible and assign each matched client pair to a same open facility at minimum total cost. The problem is motivated by match-making services relevant in, for example, video games or social apps. It naturally generalizes two prominent combinatorial optimization problems -- Uncapacitated Facility Location and Minimum-cost Maximum Matching. Facility Location with Matching also generalizes the Even-constrained Facility Location problem studied by Kim, Shin, and An (Algorithmica 2023).
  We propose a linear programming (LP) relaxation for this problem, and present a 3.868-approximation algorithm. Our algorithm leverages the work on bifactor-approximation algorithms (Byrka and Aardal, SICOMP 2012); our main technical contribution is a rerouting subroutine that reroutes a fractional solution to be supported on a fixed maximum matching with only small additional cost. For a special case where all clients are matched, we provide a refined algorithm achieving an approximation ratio of 2.218. As our algorithms are based on rounding an optimal solution to the LP relaxation, these approximation results also give the same upper bounds on the integrality gap of the relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19680v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fateme Abbasi, Martin B\"ohm, Jaros{\l}aw Byrka, Matin Mohammadi, Yongho Shin</dc:creator>
    </item>
    <item>
      <title>Placing Green Bridges Optimally for Robust Habitat Reconnection</title>
      <link>https://arxiv.org/abs/2602.19834</link>
      <description>arXiv:2602.19834v1 Announce Type: new 
Abstract: We study the problem of robustly reconnecting habitats via the placement of green bridges at minimum total cost. Habitats are fragmented into patches and we seek to reconnect each habitat such that it remains connected even if any of its patches becomes unavailable. Formally, we are given an undirected graph with edge costs, a set of fixed green bridges represented as a subset of the graph's edges, a set of habitats represented as vertex subsets, and some budget. We decide whether there exists a subset of the graph's edges containing all fixed green bridges such that, for each habitat, the induced subgraph on the solution edges is 2-vertex-connected, and the total cost does not exceed the budget. We also study the 2-edge-connectivity variant, modeling the case where any single reconnecting green bridge may fail. We analyze the computational complexity of these problems, focusing on the boundary between NP-hardness and polynomial-time solvability when the maximum habitat size and maximum vertex degree are bounded by constants. We prove that for each constant maximum habitat size of at least four there exists a small constant maximum degree for which the problems are NP-hard, and complement this with polynomial-time algorithms yielding partial dichotomies for bounded habitat size and degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19834v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gero Ellmies, Till Fluschnik</dc:creator>
    </item>
    <item>
      <title>The Bidirected Cut Relaxation for Steiner Tree: Better Integrality Gap Bounds and the Limits of Moat Growing</title>
      <link>https://arxiv.org/abs/2602.19879</link>
      <description>arXiv:2602.19879v1 Announce Type: new 
Abstract: The Steiner Tree problem asks for the cheapest way of connecting a given subset of the vertices in an undirected graph. One of the most prominent linear programming relaxations for Steiner Tree is the Bidirected Cut Relaxation (BCR). Determining the integrality gap of this relaxation is a long-standing open question. For several decades, the best known upper bound was 2, which is achievable by standard techniques. Only very recently, Byrka, Grandoni, and Traub [FOCS 2024] showed that the integrality gap of BCR is strictly below 2.
  We prove that the integrality gap of BCR is at most 1.898, improving significantly on the previous bound of 1.9988. For the important special case where a terminal minimum spanning tree is an optimal Steiner tree, we show that the integrality gap is at most 12/7, by providing a tight analysis of the dual-growth procedure by Byrka et al. To obtain the general bound of 1.898 on the integrality gap, we generalize their dual growth procedure to a broad class of moat-growing algorithms. Moreover, we prove that no such moat-growing algorithm yields dual solutions certifying an integrality gap below 12/7.
  Finally, we observe an interesting connection to the Hypergraphic Relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19879v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Paschmanns, Vera Traub</dc:creator>
    </item>
    <item>
      <title>Fast and simple multiplication of bounded twin-width matrices</title>
      <link>https://arxiv.org/abs/2602.20023</link>
      <description>arXiv:2602.20023v1 Announce Type: new 
Abstract: Matrix multiplication is a fundamental task in almost all computational fields, including machine learning and optimization, computer graphics, signal processing, and graph algorithms (static and dynamic). Twin-width is a natural complexity measure of matrices (and more general structures) that has recently emerged as a unifying concept with important algorithmic applications. While the twin-width of a matrix is invariant to re-ordering rows and columns, most of its algorithmic applications to date assume that the input is given in a certain canonical ordering that yields a bounded twin-width contraction sequence. In general, efficiently finding such a sequence -- even for an approximate twin-width value -- remains a central and elusive open question.
  In this paper we show that a binary $n \times n$ matrix of twin-width $d$ can be preprocessed in $\widetilde{\mathcal{O}}_d(n^2)$ time, so that its product with any vector can be computed in $\widetilde{\mathcal{O}}_d(n)$ time. Notably, the twin-width of the input matrix need not be known and no particular ordering of its rows and columns is assumed. If a canonical ordering is available, i.e., if the input matrix is $d$-twin-ordered, then the runtime of preprocessing and matrix-vector products can be further reduced to $\mathcal{O}(n^2+dn)$ and $\mathcal{O}(dn)$.
  Consequently, we can multiply two $n \times n$ matrices in $\widetilde{\mathcal{O}}(n^2)$ time, when at least one of the matrices consists of 0/1 entries and has bounded twin-width. The results also extend to the case of bounded twin-width matrices with adversarial corruption. Our algorithms are significantly faster and simpler than earlier methods that involved first-order model checking and required both input matrices to be $d$-twin-ordered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20023v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'aszl\'o Kozma, Michal Opler</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Sequential Prediction with Abstentions</title>
      <link>https://arxiv.org/abs/2602.17918</link>
      <description>arXiv:2602.17918v1 Announce Type: cross 
Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $\mu$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $\mu$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17918v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Yu, Mo\"ise Blanchard</dc:creator>
    </item>
    <item>
      <title>Computational Complexity of Edge Coverage Problem for Constrained Control Flow Graphs</title>
      <link>https://arxiv.org/abs/2602.18774</link>
      <description>arXiv:2602.18774v1 Announce Type: cross 
Abstract: The article studies edge coverage for control flow graphs extended with explicit constraints. Achieving a given level of white-box coverage for a given code is a classic problem in software testing. We focus on designing test sets that achieve edge coverage \textit{while respecting additional constraints} between vertices. The paper analyzes how such constraints affect both the feasibility and computational complexity of edge coverage.
  The paper discusses five types of constraints. POSITIVE constraints require at least one test path where a given vertex precedes another. NEGATIVE constraints forbid any such test path. ONCE constraints require exactly one test path with a single occurrence of one vertex before another. MAX ONCE constraints allow such precedence in at most one test path. ALWAYS constraints require every test path containing a given vertex to also contain another vertex later on the same path. Each type models a different test requirement, such as mandatory flows, semantic exclusions, or execution cost limits.
  We investigate the computational complexity of finding a test set that achieves edge coverage and respects a given set of constraints. For POSITIVE constraints, the existence of an edge covering test set is decidable in polynomial time by extending standard edge coverage constructions with additional paths for each constraint. For NEGATIVE, MAX ONCE, ONCE, and ALWAYS constraints, the decision problem is NP-complete. The proofs rely on polynomial reductions from variants of SAT. The NP-completeness results hold even for restricted graph classes, including acyclic graphs, for all these four constraints.
  Finally, we study the fixed-parameter tractability of the NEGATIVE constraint. Although the general problem is NP-complete, the paper presents an FPT algorithm with respect to the number of constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18774v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.SE</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Ruszil, Artur Pola\'nski, Adam Roman, Jakub Zelek</dc:creator>
    </item>
    <item>
      <title>Quantum Sketches, Hashing, and Approximate Nearest Neighbors</title>
      <link>https://arxiv.org/abs/2602.19259</link>
      <description>arXiv:2602.19259v1 Announce Type: cross 
Abstract: Motivated by Johnson--Lindenstrauss dimension reduction, amplitude encoding, and the view of measurements as hash-like primitives, one might hope to compress an $n$-point approximate nearest neighbor (ANN) data structure into $O(\log n)$ qubits. We rule out this possibility in a broad quantum sketch model, the dataset $P$ is encoded as an $m$-qubit state $\rho_P$, and each query is answered by an arbitrary query-dependent measurement on a fresh copy of $\rho_P$. For every approximation factor $c\ge 1$ and constant success probability $p&gt;1/2$, we exhibit $n$-point instances in Hamming space $\{0,1\}^d$ with $d=\Theta(\log n)$ for which any such sketch requires $m=\Omega(n)$ qubits, via a reduction to quantum random access codes and Nayak's lower bound. These memory lower bounds coexist with potential quantum query-time gains and in candidate-scanning abstractions of hashing-based ANN, amplitude amplification yields a quadratic reduction in candidate checks, which is essentially optimal by Grover/BBBV-type bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19259v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajjad Hashemian</dc:creator>
    </item>
    <item>
      <title>The Sample Complexity of Replicable Realizable PAC Learning</title>
      <link>https://arxiv.org/abs/2602.19552</link>
      <description>arXiv:2602.19552v1 Announce Type: cross 
Abstract: In this paper, we consider the problem of replicable realizable PAC learning. We construct a particularly hard learning problem and show a sample complexity lower bound with a close to $(\log|H|)^{3/2}$ dependence on the size of the hypothesis class $H$. Our proof uses several novel techniques and works by defining a particular Cayley graph associated with $H$ and analyzing a suitable random walk on this graph by examining the spectral properties of its adjacency matrix.
  Furthermore, we show an almost matching upper bound for the lower bound instance, meaning if a stronger lower bound exists, one would have to consider a different instance of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19552v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kasper Green Larsen, Markus Engelund Mathiasen, Chirag Pabbaraju, Clement Svendsen</dc:creator>
    </item>
    <item>
      <title>GPU-Native Compressed Neighbor Lists with a Space-Filling-Curve Data Layout</title>
      <link>https://arxiv.org/abs/2602.19873</link>
      <description>arXiv:2602.19873v1 Announce Type: cross 
Abstract: We have developed a compressed neighbor list for short-range particle-particle interaction based on a space- filling curve (SFC) memory layout and particle clusters. The neighbor list can be constructed efficiently on GPUs, supporting NVIDIA and AMD hardware, and has a memory footprint of only 4 bytes per particle to store approximately 200 neighbors. Compared to the highly-optimized domain-specific neighbor list implementation of GROMACS, a molecular dynamics code, it has a comparable cluster overhead and delivers similar performance in a neighborhood pass. Thanks to the SFC-based data layout and the support for varying interaction radii per particle, our neighbor list performs well for systems with high density contrasts, such as those encountered in many astrophysical and cosmological applications. Due to the close relation between SFCs and octrees, our neighbor list seamlessly integrates with octree-based domain decomposition and multipole-based methods for long-range gravitational or electrostatic interactions. To demonstrate the coupling between long- and short-range forces, we simulate an Evrard collapse, a standard test case for the coupling between hydrodynamical and gravitational forces, on up to 1024 GPUs, and compare our results to the analytical solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19873v1</guid>
      <category>cs.CE</category>
      <category>astro-ph.IM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Felix Thaler, Sebastian Keller</dc:creator>
    </item>
    <item>
      <title>Comparing the Hardness of Online Minimization and Maximization Problems with Predictions</title>
      <link>https://arxiv.org/abs/2409.12694</link>
      <description>arXiv:2409.12694v3 Announce Type: replace 
Abstract: We build on the work of Berg, Boyar, Favrholdt, and Larsen, who developed a complexity theory for online problems with and without predictions (IJTCS-FAW, volume 15828 of LNCS, Springer, 2025) where they define a hierarchy of complexity classes that classifies online problems based on the competitiveness of best possible deterministic online algorithms for each problem. Their work focused on online minimization problems and we continue their work by considering online maximization problems.
  We compare the competitiveness of the base online minimization problem from Berg, Boyar, Favrholdt, and Larsen, Asymmetric String Guessing, to the competitiveness of Online Bounded Degree Independent Set. Formally, we show that there exists algorithms of any given competitiveness for Asymmetric String Guessing if and only if there exists algorithms of the same competitiveness for Online Bounded Degree Independent Set, while respecting that the competitiveness of algorithms is measured differently for minimization and maximization problems. Beyond this, we give several hardness preserving reductions between different online maximization problems, which imply new membership, hardness, and completeness results for the complexity classes. Finally, we show new positive and negative algorithmic results for (among others) Online Bounded Degree Independent Set, Online Interval Scheduling, Online Set Packing, and Online Bounded Degree Clique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12694v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Magnus Berg</dc:creator>
    </item>
    <item>
      <title>Faster Pseudo-Deterministic Minimum Cut</title>
      <link>https://arxiv.org/abs/2602.14550</link>
      <description>arXiv:2602.14550v2 Announce Type: replace 
Abstract: Pseudo-deterministic algorithms are randomized algorithms that, with high constant probability, output a fixed canonical solution. The study of pseudo-deterministic algorithms for the global minimum cut problem was recently initiated by Agarwala and Varma [ITCS'26], who gave a black-box reduction incurring an $O(\log n \log \log n)$ overhead. We introduce a natural graph-theoretic tie-breaking mechanism that uniquely selects a canonical minimum cut. Using this mechanism, we obtain: (i) A pseudo-deterministic minimum cut algorithm for weighted graphs running in $O(m\log^2 n)$ time, eliminating the $O(\log n \log \log n)$ overhead of prior work and matching existing randomized algorithms. (ii) The first pseudo-deterministic algorithm for maintaining a canonical minimum cut in a fully-dynamic unweighted graph, with $\mathrm{polylog}(n)$ update time and $\tilde{O}(n)$ query time. (iii) Improved pseudo-deterministic algorithms for unweighted graphs in the dynamic streaming and cut-query models of computation, matching the best randomized algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14550v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yotam Kenneth-Mordoch</dc:creator>
    </item>
    <item>
      <title>The NTU Partitioned Matching Game for International Kidney Exchange Programs</title>
      <link>https://arxiv.org/abs/2409.01452</link>
      <description>arXiv:2409.01452v2 Announce Type: replace-cross 
Abstract: Motivated by the real-world problem of international kidney exchange (IKEP), recent literature introduced a generalized transferable utility matching game featuring a partition of the vertex set of a graph into players, and analyzed its complexity. We explore the non-transferable utility (NTU) variant of the game, where the utility of players is given by the number of their matched vertices. Our motivation for studying this problem is twofold. First, the NTU version is arguably a more natural model of the international kidney exchange program, as the utility of a participating country mostly depends on how many of its patients receive a kidney, which is non-transferable by nature. Second, the special case where each player has two vertices, which we call the NTU matching game with couples, is interesting in its own right and has intriguing structural properties.
  We study the core of the NTU game, which suitably captures the notion of stability of an IKEP, as it precludes incentives to deviate from the proposed solution for any possible coalition of the players. We prove computational complexity results about the weak and strong cores under various assumptions on the players. In particular, we show that if every player has two vertices, then the weak core is always nonempty, and the existence of a strong core solution can be decided in polynomial time. Moreover, one can efficiently optimize on the strong core. In contrast, it is NP-hard to decide whether the strong core is empty when each player has three vertices. We also show that if the number of players is constant, then the non-emptiness of the weak and strong cores is polynomial-time decidable, and we can find a minimum-cost core solution in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01452v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gergely Cs\'aji, Tam\'as Kir\'aly, Zsuzsa M\'esz\'aros-Karkus</dc:creator>
    </item>
    <item>
      <title>Hypergraphs as Weighted Directed Self-Looped Graphs: Spectral Properties, Clustering, Cheeger Inequality</title>
      <link>https://arxiv.org/abs/2411.03331</link>
      <description>arXiv:2411.03331v2 Announce Type: replace-cross 
Abstract: Hypergraphs naturally arise when studying group relations and have been widely used in the field of machine learning. To the best of our knowledge, the recently proposed edge-dependent vertex weights (EDVW) modeling is one of the most generalized modeling methods of hypergraphs, i.e., most existing hypergraph conceptual modeling methods can be generalized as EDVW hypergraphs without information loss. However, the relevant algorithmic developments on EDVW hypergraphs remain nascent: compared to the spectral theories for graphs, its formulations are incomplete, the spectral clustering algorithms are not well-developed, and the hypergraph Cheeger Inequality is not well-defined. To this end, deriving a unified random walk-based formulation, we propose our definitions of hypergraph Rayleigh Quotient, NCut, boundary/cut, volume, and conductance, which are consistent with the corresponding definitions on graphs. Then, we prove that the normalized hypergraph Laplacian is associated with the NCut value, which inspires our proposed HyperClus-G algorithm for spectral clustering on EDVW hypergraphs. Finally, we prove that HyperClus-G can always find an approximately linearly optimal partitioning in terms of both NCut and conductance. Additionally, we provide extensive experiments to validate our theoretical findings from an empirical perspective. Code of HyperClus-G is available at https://github.com/iDEA-iSAIL-Lab-UIUC/HyperClus-G.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03331v2</guid>
      <category>cs.SI</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zihao Li, Dongqi Fu, Hengyu Liu, Jingrui He</dc:creator>
    </item>
    <item>
      <title>The Contiguous Art Gallery Problem is in {\Theta}(n log n)</title>
      <link>https://arxiv.org/abs/2511.02960</link>
      <description>arXiv:2511.02960v4 Announce Type: replace-cross 
Abstract: Recently, a natural variant of the Art Gallery problem, known as the \emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$, the goal is to partition its boundary $\partial P$ into the smallest number of contiguous segments such that each segment is completely visible from some point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this variant is polynomial-time solvable. At SoCG~2025, three independent works presented algorithms for this problem, each achieving a running time of $O(k n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution. Interestingly, these results were obtained using entirely different approaches, yet all led to roughly the same asymptotic complexity, suggesting that such a running time might be inherent to the problem.
  We show that this is not the case. In the real RAM-model, the prevalent model in computational geometry, we present an $O(n \log n)$-time algorithm, achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We also give a straightforward sorting-based lower bound by reducing from the set intersection problem. We thus show that the Contiguous Art Gallery problem is in $\Theta(n \log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02960v4</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarita de Berg, Jacobus Conradi, Ivor van der Hoog, Eva Rotenberg</dc:creator>
    </item>
    <item>
      <title>Graded Projection Recursion (GPR): A Framework for Controlling Bit-Complexity of Algebraic Packing</title>
      <link>https://arxiv.org/abs/2511.11988</link>
      <description>arXiv:2511.11988v4 Announce Type: replace-cross 
Abstract: We present Graded Projection Recursion (GPR), a framework for converting certain blocked recursive algorithms into model-honest (i.e., reflects full bit complexity) near-quadratic procedures under bounded intermediate budgets. Part I gives a proof-complete {\em integral specification} of a three-band packing identity and a two-round middle-band extractor, and shows how per-node centering and sqrt-free dyadic l2 normalization (recursive invariant amplification) gives a sufficient packing base that grows linearly with the scaling depth (i.e., logarithmic bit-growth) in exact arithmetic.
  On fixed-width hardware, exact evaluation of the packed recursion generally requires either an extended-precision path or digit-band (slice) staging} that emulates b(n) bits of mantissa precision using w-bit words, incurring only polylogarithmic overhead; This leads to a soft-quadratic bit cost O(n^2) when b(n)=\Theta(\log n) in the basic 2x2 recursion). Part II introduces execution-format comparators (e.g., IEEE-754), a drift ledger, and a decision-invariance theorem that supports commensurate-accuracy claims in floating arithmetic (and that cleanly accounts for any staged/truncated auxiliary drift). Part III provides case-study reductions (LUP/solve/det/inv, LDL^T, blocked QR, SOI/SPD functions, GSEVP, dense LP/SDP IPM kernels, Gaussian process regression, and representative semiring problems) showing how to export the kernel advantage without reintroducing uncontrolled intermediate growth. Part IV abstracts admissible packings and extractors via a master condition and an easily checkable BWBM sufficient condition, and sketches extensions to multilinear/multigraded kernels and non-rounding extractors (e.g., CRT and semiring bucket projections).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11988v4</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Uhlmann</dc:creator>
    </item>
    <item>
      <title>On the Hardness of Approximation of the Fair k-Center Problem</title>
      <link>https://arxiv.org/abs/2602.16688</link>
      <description>arXiv:2602.16688v2 Announce Type: replace-cross 
Abstract: In this work, we study the hardness of approximation of the fair $k$-center problem. In this problem, we are given a set of data points in a metric space that is partitioned into groups and the task is to choose a subset of $k$-data points, called centers, such that a prescribed number of data points from each group are chosen while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for fair $k$-center in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the classical unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, assuming $\mathsf{P} \neq \mathsf{NP}$, for any $\epsilon&gt;0$, no polynomial-time algorithm can approximate fair $k$-center to $(3-\epsilon)$-factor.
  Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16688v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suhas Thejaswi</dc:creator>
    </item>
  </channel>
</rss>
