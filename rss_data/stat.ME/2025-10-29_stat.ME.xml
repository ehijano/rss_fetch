<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Random Forest Inverse Probability Weighted Pseudo-Observation Framework for Alternating Recurrent Events</title>
      <link>https://arxiv.org/abs/2510.23764</link>
      <description>arXiv:2510.23764v1 Announce Type: new 
Abstract: Alternating recurrent events, where subjects experience two potentially correlated event types over time, are common in healthcare, social, and behavioral studies. Often there is a primary event of interest that, when triggered, initiates a period of treatment and recovery measured via a secondary time-to-event. For example, cancer patients can experience repeated blood clotting emergencies that require hospitalization followed by discharge, people with alcohol use disorder can have periods of addiction and sobriety, or care partners can experience periods of depression and recovery. Potential censoring of the data requires special handling. Overlaying this are the missing at-risk periods for the primary event type when individuals have initiated the primary event but not reached the subsequent secondary event. In this paper, we develop a framework for regression analysis of censored alternating recurrent events that uses a random forest inverse probability weighting strategy to avoid bias in the analysis of the time to the primary event due to informative missingness from the alternate secondary state. The proposed regression model estimates $\tau$-restricted mean time to the primary event of interest while taking into account complexities of censored. Simulations show good performance of our method when the alternate times-to-event are either independent or correlated. We analyze a mobile health study data to evaluate the impact of self-care push notifications on the mental state of caregivers of traumatic brain injury patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23764v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abigail Loe, Susan Murray, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>ETZ: A Modeling Principle for Confirmability of Drug-Development Studies</title>
      <link>https://arxiv.org/abs/2510.23799</link>
      <description>arXiv:2510.23799v1 Announce Type: new 
Abstract: Transitioning from Phase 2 to Phase 3 in drug development, at a rate of $\approx$40%, is the most stringent among phase transitions (Hay et al. (2014)). Yet, success rate at Phase 3 leading to approval is only $\approx$50% (Arrowsmith (2011b)). To improve Confirmability, we propose a methodological shift: replacing multiple hypothesis testing with inference based on confidence sets, and substituting conventional power and sample size calculations with a Confidently Bounded Quantile (CBQ) framework.
  Our confidence set inferences to answer the questions of whether to transition to a Confirmatory study as well as what to designate as the endpoint in that study. Construction of our directed confidence sets follows the Partitioning Principle, taking the best of each of Pivoting and Neyman Confidence Set Construction.
  Rooted in Tukey's Confidently Bounded Allowance (CBA) (Tukey (1994a)), our proposed CBQ makes the transitioning decision following the Correct and Useful Inference principle in Hsu (1996). CBQ removes from "power" the probability of rejecting for wrong reasons, eliminating the need for informal discounting in power calculation that has existed in the biopharmaceutical industry.
  ETZ, the modeling principle proposed in Wang et al. (2025), quantifies the impact of three variability components on confirmability. In repeated-measures RCTs, it separates within-subject and between-subject variability, further dividing the latter into baseline and trajectory components. This enables informed investment decisions for the sponsors on targeting variability reduction to improve confirmability. A Shiny-based Confirmability App supports all computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23799v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yujia Sun, Yang Han, Xingya Wang, Szu-Yu Tang, Yushi Liu, Jason C. Hsu</dc:creator>
    </item>
    <item>
      <title>Testing-driven Variable Selection in Bayesian Modal Regression</title>
      <link>https://arxiv.org/abs/2510.23831</link>
      <description>arXiv:2510.23831v1 Announce Type: new 
Abstract: We propose a Bayesian variable selection method in the framework of modal regression for heavy-tailed responses. An efficient expectation-maximization algorithm is employed to expedite parameter estimation. A test statistic is constructed to exploit the shape of the model error distribution to effectively separate informative covariates from unimportant ones. Through simulations, we demonstrate and evaluate the efficacy of the proposed method in identifying important covariates in the presence of non-Gaussian model errors. Finally, we apply the proposed method to analyze two datasets arising in genetic and epigenetic studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23831v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiasong Duan, Hongmei Zhang, Xianzheng Huang</dc:creator>
    </item>
    <item>
      <title>From Stochasticity to Signal: A Bayesian Latent State Model for Reliable Measurement with LLMs</title>
      <link>https://arxiv.org/abs/2510.23874</link>
      <description>arXiv:2510.23874v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used to automate classification tasks in business, such as analyzing customer satisfaction from text. However, the inherent stochasticity of LLMs, in terms of their tendency to produce different outputs for the same input, creates a significant measurement error problem that is often neglected with a single round of output, or addressed with ad-hoc methods like majority voting. Such naive approaches fail to quantify uncertainty and can produce biased estimates of population-level metrics. In this paper, we propose a principled solution by reframing LLM variability as a statistical measurement error problem and introducing a Bayesian latent state model to address it. Our model treats the true classification (e.g., customer dissatisfaction) as an unobserved latent variable and the multiple LLM ratings as noisy measurements of this state. This framework allows for the simultaneous estimation of the LLM's false positive and false negative error rates, the underlying base rate of the phenomenon in the population, the posterior probability of the true state for each individual observation, and the causal impact of a business intervention, if any, on the latent state. Through simulation studies, we demonstrate that our model accurately recovers true parameters where naive methods fail. We conclude that this methodology provides a general and reliable framework for converting noisy, probabilistic outputs from LLMs into accurate and actionable insights for scientific and business applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23874v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Ignacio Martinez</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification and Estimation of Ratios of Multi-Category Means under Preferential Sampling</title>
      <link>https://arxiv.org/abs/2510.23920</link>
      <description>arXiv:2510.23920v1 Announce Type: new 
Abstract: Multi-category data arise in diverse fields including marketing, chemistry, public policy, genomics, political science, and ecology. We consider the problem of estimating ratios of category-specific means in a fully nonparametric setting, allowing for both observational units and categories to be preferentially sampled. We consider covariate-adjusted and unadjusted estimands that are non-parametrically defined and straightforward to interpret. While identifiability for related models has been established through parametric distributions or restrictions on the conditional mean (e.g., log-linearity), we show that identifiability can be obtained through an independence assumption or a category constraint, such as a reference category or a centering function. We develop an efficient, doubly-robust targeted minimum loss based estimator with excellent finite-sample performance, including in the setting of a large number of infrequently observed categories. We contrast the performance of our method with related approaches via simulation, and apply it to identify bacteria that are differentially abundant in diarrheal cases compared to controls. Our work provides a general framework for studying parameter identifiability in compositional data settings without requiring parametric assumptions on the data distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23920v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Grant Hopkins, Sarah Teichman, Ellen Graham, Amy D Willis</dc:creator>
    </item>
    <item>
      <title>Illustrating implications of misaligned causal questions and statistics in settings with competing events and interest in treatment mechanisms</title>
      <link>https://arxiv.org/abs/2510.24018</link>
      <description>arXiv:2510.24018v1 Announce Type: new 
Abstract: In the presence of competing events, many investigators are interested in a direct treatment effect on the event of interest that does not capture treatment effects on competing events. Classical survival analysis methods that treat competing events like censoring events, at best, target a controlled direct effect: the effect of the treatment under a difficult to imagine and typically clinically irrelevant scenario where competing events are somehow eliminated. A separable direct effect, quantifying the effect of a future modified version of the treatment, is an alternative direct effect notion that may better align with an investigator's underlying causal question. In this paper, we provide insights into the implications of naively applying an estimator constructed for a controlled direct effect (i.e., "censoring by competing events") when the actual causal effect of interest is a separable direct effect. We illustrate the degree to which controlled and separable direct effects may take different values, possibly even different signs, and the degree to which these two different effects may be differentially impacted by violation and/or near violation of their respective identifying conditions under a range of data generating scenarios. Finally, we provide an empirical comparison of inverse probability of censoring weighting to an alternative weighted estimator specifically structured for a separable effect using data from a randomized trial of estrogen therapy and prostate cancer mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24018v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Kawahara, Sean McGrath, Jessica G Young</dc:creator>
    </item>
    <item>
      <title>Quantifying inconsistency in one-stage individual participant data meta-analyses of treatment-covariate interactions: a simulation study</title>
      <link>https://arxiv.org/abs/2510.24130</link>
      <description>arXiv:2510.24130v1 Announce Type: new 
Abstract: It is recommended that measures of between-study effect heterogeneity be reported when conducting individual-participant data meta-analyses (IPD-MA). Methods exist to quantify inconsistency between trials via I^2 (the percentage of variation in the treatment effect due to between-study heterogeneity) when conducting two-stage IPD-MA, and when conducting one-stage IPD-MA with approximately equal numbers of treatment and control group participants. We extend formulae to estimate I^2 when investigating treatment-covariate interactions with unequal numbers of participants across subgroups and/or continuous covariates. A simulation study was conducted to assess the agreement in values of I^2 between those derived from two-stage models using traditional methods and those derived from equivalent one-stage models. Fourteen scenarios differed by the magnitude of between-trial heterogeneity, the number of trials, and the average number of participants in each trial. Bias and precision of I^2 were similar between the one- and two-stage models. The mean difference in I^2 between equivalent models ranged between -1.0 and 0.0 percentage points across scenarios. However, disparities were larger in simulated datasets with smaller samples sizes with up to 19.4 percentage points difference between models. Thus, the estimates of I^2 derived from these extended methods can be interpreted similarly to those from existing formulae for two-stage models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24130v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myra B. McGuinness, Joanne E. McKenzie, Andrew Forbes, Flora Hui, Keith R. Martin, Robert J. Casson, Amalia Karahalios</dc:creator>
    </item>
    <item>
      <title>Intelligent n-Means Spatial Sampling</title>
      <link>https://arxiv.org/abs/2510.24183</link>
      <description>arXiv:2510.24183v1 Announce Type: new 
Abstract: Well-spread samples are desirable in many disciplines because they improve estimation when target variables exhibit spatial structure. This paper introduces an integrated methodological framework for spreading samples over the population's spatial coordinates. First, we propose a new, translation-invariant spreadness index that quantifies spatial balance with a clear interpretation. Second, we develop a clustering method that balances clusters with respect to an auxiliary variable; when the auxiliary variable is the inclusion probability, the procedure yields clusters whose totals are one, so that a single draw per cluster is, in principle, representative and produces units optimally spread along the population coordinates, an attractive feature for finite population sampling. Third, building on the graphical sampling framework, we design an efficient sampling scheme that further enhances spatial balance. At its core lies an intelligent, computationally efficient search layer that adapts to the population's spatial structure and inclusion probabilities, tailoring a design to each specific population to maximize spread. Across diverse spatial patterns and both equal- and unequal-probability regimes, this intelligent coupling consistently outperformed all rival spread-oriented designs on dispersion metrics, while the spreadness index remained informative and the clustering step improved representativeness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24183v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bardia Panahbehagh, Mehdi Mohebbi, Amir Mohammad HosseiniNasab</dc:creator>
    </item>
    <item>
      <title>A Frequency-Domain NonStationarity Test for dependent data</title>
      <link>https://arxiv.org/abs/2510.24319</link>
      <description>arXiv:2510.24319v1 Announce Type: new 
Abstract: Distinguishing long-memory behaviour from nonstationarity is challenging, as both produce slowly decaying sample autocovariances. Existing stationarity tests either fail to account for long-memory processes or exhibit poor empirical size, particularly near the boundary between stationarity and nonstationarity. We propose a new, parameter-free testing procedure based on the evaluation of periodograms across multiple epochs. The limiting distributions derived here are obtained under stationarity and nonstationarity assumptions and analytically tractable, expressed as finite sums of weighted independent $\chi^2$ random variables. Simulation studies indicate that the proposed method performs favorably compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24319v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamedou Ould Haye, Anne Philippe</dc:creator>
    </item>
    <item>
      <title>Pseudo-Bayesian Optimal Designs for Fitting Fractional Polynomial Response Surface Models</title>
      <link>https://arxiv.org/abs/2510.24349</link>
      <description>arXiv:2510.24349v1 Announce Type: new 
Abstract: Fractional polynomial models are potentially useful for response surfaces investigations. With the availability of routines for fitting nonlinear models in statistical packages they are increasingly being used. However, as in all experiments the design should be chosen such that the model parameters are estimated as efficiently as possible. The design choice for such models involves the known nonlinear models' design difficulties but \cite{gilmour_trinca_2012b} proposed a methodology capable of producing exact designs that makes use of the computing facilities available today. In this paper, we use this methodology to find Bayesian optimal exact designs for several fractional polynomial models. The optimum designs are compared to various standard designs in response surface problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24349v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luzia A. Trinca, Steven G. Gilmour</dc:creator>
    </item>
    <item>
      <title>Self-Normalized Quantile Empirical Saddlepoint Approximation</title>
      <link>https://arxiv.org/abs/2510.24352</link>
      <description>arXiv:2510.24352v1 Announce Type: new 
Abstract: We propose a density-free method for frequentist inference on population quantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation (SNQESA). The approach builds a self-normalized pivot from the indicator score for a fixed quantile threshold and then employs a constrained empirical saddlepoint approximation to obtain highly accurate tail probabilities. Inverting these tail areas yields confidence intervals and tests without estimating the unknown density at the target quantile, thereby eliminating bandwidth selection and the boundary issues that affect kernel-based Wald/Hall-Sheather intervals. Under mild local regularity, the resulting procedures attain higher-order tail accuracy and second-order coverage after inversion. Because the pivot is anchored in a bounded Bernoulli reduction, the method remains reliable for skewed and heavy-tailed distributions and for extreme quantiles. Extensive Monte Carlo experiments across light, heavy, and multimodal distributions demonstrate that SNQESA delivers stable coverage and competitive interval lengths in small to moderate samples while being orders of magnitude faster than large-B resampling schemes. An empirical study on Value-at-Risk with rolling windows further highlights the gains in tail performance and computational efficiency. The framework naturally extends to two-sample quantile differences and to regression-type settings, offering a practical, analytically transparent alternative to kernel, bootstrap, and empirical-likelihood methods for distribution-free quantile inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24352v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hou Jian, Meng Tan, Tian Maozai</dc:creator>
    </item>
    <item>
      <title>Comparison of Estimators for Multi-State Models in Potentially Non-Markov Processes</title>
      <link>https://arxiv.org/abs/2510.24453</link>
      <description>arXiv:2510.24453v1 Announce Type: new 
Abstract: Various estimators for modelling the transition probabilities in multi-state models have been proposed, e.g., the Aalen-Johansen estimator, the landmark Aalen-Johansen estimator, and a hybrid Aalen-Johansen estimator. While the Aalen-Johansen estimator is generally only consistent under the rather restrictive Markov assumption, the landmark Aalen-Johansen estimator can handle non-Markov multi-state models. However, the landmark Aalen-Johansen estimator leads to a strict data reduction and, thus, to an increased variance. The hybrid Aalen-Johansen estimator serves as a compromise by, firstly, checking with a log-rank-based test whether the Markov assumption is satisfied. Secondly, landmarking is only applied if the Markov assumption is rejected. In this work, we propose a new hybrid Aalen-Johansen estimator which uses a Cox model instead of the log-rank-based test to check the Markov assumption in the first step. Furthermore, we compare the four estimators in an extensive simulation study across Markov, semi-Markov, and distinct non-Markov settings. In order to get deep insights into the performance of the estimators, we consider four different measures: bias, variance, root mean squared error, and coverage rate. Additionally, further influential factors on the estimators such as the form and degree of non-Markov behaviour, the different transitions, and the starting time are analysed. The main result of the simulation study is that the hybrid Aalen-Johansen estimators yield favourable results across various measures and settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24453v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carolin Drenda, Dennis Dobler, Merle Munko, Andrew Titman</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric modeling of multivariate count data with an unknown number of traits</title>
      <link>https://arxiv.org/abs/2510.24526</link>
      <description>arXiv:2510.24526v1 Announce Type: new 
Abstract: Feature and trait allocation models are fundamental objects in Bayesian nonparametrics and play a prominent role in several applications. Existing approaches, however, typically assume full exchangeability of the data, which may be restrictive in settings characterized by heterogeneous but related groups. In this paper, we introduce a general and tractable class of Bayesian nonparametric priors for partially exchangeable trait allocation models, relying on completely random vectors. We provide a comprehensive theoretical analysis, including closed-form expressions for marginal and posterior distributions, and illustrate the tractability of our framework in the cases of binary and Poisson-distributed traits. A distinctive aspect of our approach is that the number of traits is a random quantity, thereby allowing us to model and estimate unobserved traits. Building on these results, we also develop a novel mixture model that infers the group partition structure from the data, effectively clustering trait allocations. This extension generalizes Bayesian nonparametric latent class models and avoids the systematic overclustering that arises when the number of traits is assumed to be fixed. We demonstrate the practical usefulness of our methodology through an application to the `Ndrangheta criminal network from the Operazione Infinito investigation, where our model provides insights into the organization of illicit activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24526v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Ghilotti, Federico Camerlenghi, Tommaso Rigon, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>Unbiased likelihood estimation of the Langevin diffusion for animal movement modelling</title>
      <link>https://arxiv.org/abs/2510.24539</link>
      <description>arXiv:2510.24539v1 Announce Type: new 
Abstract: The resource selection function provides a model for describing habitat suitability, which can be used to predict the spatial utilisation distribution of a species. Tracking data can be modelled as a point process, but this is made complicated by the presence of temporally irregular autocorrelation. One proposed model to handle this is the continuous-time Langevin diffusion. However, current estimation techniques obtain increasingly biased parameter estimates as the intervals between observations increase. In this paper, we address this issue using Brownian bridges in an importance sampling scheme to improve the likelihood approximation of the Langevin diffusion model. We show using a series of simulation studies that this approach effectively removes the bias in many scenarios. Furthermore, we show that the model actually performs better at lower sampling rates over a longer duration than shorter duration at a higher sampling frequency. This research broadens the applicability of Langevin diffusion models to telemetry data at coarser resolutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24539v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ron Ronald Togunov, Simen Knutsen Furset, Martin Emil Pettersen, Robert Brian O'Hara</dc:creator>
    </item>
    <item>
      <title>Machine-Learning-Assisted Comparison of Regression Functions</title>
      <link>https://arxiv.org/abs/2510.24714</link>
      <description>arXiv:2510.24714v1 Announce Type: new 
Abstract: We revisit the classical problem of comparing regression functions, a fundamental question in statistical inference with broad relevance to modern applications such as data integration, transfer learning, and causal inference. Existing approaches typically rely on smoothing techniques and are thus hindered by the curse of dimensionality. We propose a generalized notion of kernel-based conditional mean dependence that provides a new characterization of the null hypothesis of equal regression functions. Building on this reformulation, we develop two novel tests that leverage modern machine learning methods for flexible estimation. We establish the asymptotic properties of the test statistics, which hold under both fixed- and high-dimensional regimes. Unlike existing methods that often require restrictive distributional assumptions, our framework only imposes mild moment conditions. The efficacy of the proposed tests is demonstrated through extensive numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24714v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Yan, Zhuoxi Li, Yang Ning, Yong Chen</dc:creator>
    </item>
    <item>
      <title>Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling</title>
      <link>https://arxiv.org/abs/2510.23631</link>
      <description>arXiv:2510.23631v1 Announce Type: cross 
Abstract: Alignment of large language models (LLMs) has predominantly relied on pairwise preference optimization, where annotators select the better of two responses to a prompt. While simple, this approach overlooks the opportunity to learn from richer forms of human feedback, such as multiwise comparisons and top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a unified framework that bridges preference optimization with (ranked) choice modeling via maximum likelihood estimation. The framework is flexible, supporting both utility-based and rank-based choice models. It subsumes several existing pairwise methods (e.g., DPO, SimPO), while providing principled training objectives for richer feedback formats. We instantiate this framework with two representative ranked choice models (Multinomial Logit and Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms competitive baselines. RCPO shows how directly leveraging ranked preference data, combined with the right choice models, yields more effective alignment. It offers a versatile and extensible foundation for incorporating (ranked) choice modeling into LLM training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23631v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Tang, Yifan Feng</dc:creator>
    </item>
    <item>
      <title>Beyond Normality: Reliable A/B Testing with Non-Gaussian Data</title>
      <link>https://arxiv.org/abs/2510.23666</link>
      <description>arXiv:2510.23666v1 Announce Type: cross 
Abstract: A/B testing has become the cornerstone of decision-making in online markets, guiding how platforms launch new features, optimize pricing strategies, and improve user experience. In practice, we typically employ the pairwise $t$-test to compare outcomes between the treatment and control groups, thereby assessing the effectiveness of a given strategy. To be trustworthy, these experiments must keep Type I error (i.e., false positive rate) under control; otherwise, we may launch harmful strategies. However, in real-world applications, we find that A/B testing often fails to deliver reliable results. When the data distribution departs from normality or when the treatment and control groups differ in sample size, the commonly used pairwise $t$-test is no longer trustworthy. In this paper, we quantify how skewed, long tailed data and unequal allocation distort error rates and derive explicit formulas for the minimum sample size required for the $t$-test to remain valid. We find that many online feedback metrics require hundreds of millions samples to ensure reliable A/B testing. Thus we introduce an Edgeworth-based correction that provides more accurate $p$-values when the available sample size is limited. Offline experiments on a leading A/B testing platform corroborate the practical value of our theoretical minimum sample size thresholds and demonstrate that the corrected method substantially improves the reliability of A/B testing in real-world conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23666v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junpeng Gong, Chunkai Wang, Hao Li, Jinyong Ma, Haoxuan Li, Xu He</dc:creator>
    </item>
    <item>
      <title>Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes</title>
      <link>https://arxiv.org/abs/2510.23817</link>
      <description>arXiv:2510.23817v1 Announce Type: cross 
Abstract: Industrial processes generate complex data that challenge fault detection systems, often yielding opaque or underwhelming results despite advanced machine learning techniques. This study tackles such difficulties using the Tennessee Eastman Process, a well-established benchmark known for its intricate dynamics, to develop an innovative fault detection framework. Initial attempts with standard models revealed limitations in both performance and interpretability, prompting a shift toward a more tractable approach. By employing SHAP (SHapley Additive exPlanations), we transform the problem into a more manageable and transparent form, pinpointing the most critical process features driving fault predictions. This reduction in complexity unlocks the ability to apply causal analysis through Directed Acyclic Graphs, generated by multiple algorithms, to uncover the underlying mechanisms of fault propagation. The resulting causal structures align strikingly with SHAP findings, consistently highlighting key process elements-like cooling and separation systems-as pivotal to fault development. Together, these methods not only enhance detection accuracy but also provide operators with clear, actionable insights into fault origins, a synergy that, to our knowledge, has not been previously explored in this context. This dual approach bridges predictive power with causal understanding, offering a robust tool for monitoring complex manufacturing environments and paving the way for smarter, more interpretable fault detection in industrial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23817v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Cortes dos Santos, Matheus Becali Rocha, Renato A Krohling</dc:creator>
    </item>
    <item>
      <title>Machine learning approaches for interpretable antibody property prediction using structural data</title>
      <link>https://arxiv.org/abs/2510.23975</link>
      <description>arXiv:2510.23975v1 Announce Type: cross 
Abstract: Understanding the relationship between antibody sequence, structure and function is essential for the design of antibody-based therapeutics and research tools. Recently, machine learning (ML) models mostly based on the application of large language models to sequence information have been developed to predict antibody properties. Yet there are open directions to incorporate structural information, not only to enhance prediction but also to offer insights into the underlying molecular mechanisms. This chapter provides an overview of these approaches and describes two ML frameworks that integrate structural data (via graph representations) with neural networks to predict properties of antibodies: ANTIPASTI predicts binding affinity (a global property) whereas INFUSSE predicts residue flexibility (a local property). We survey the principles underpinning these models; the ways in which they encode structural knowledge; and the strategies that can be used to extract biologically relevant statistical signals that can help discover and disentangle molecular determinants of the properties of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23975v1</guid>
      <category>q-bio.QM</category>
      <category>physics.bio-ph</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Michalewicz, Mauricio Barahona, Barbara Bravi</dc:creator>
    </item>
    <item>
      <title>Streamlining business functions in official statistical production with Machine Learning</title>
      <link>https://arxiv.org/abs/2510.24394</link>
      <description>arXiv:2510.24394v1 Announce Type: cross 
Abstract: We provide a description of pilot and production experiences to streamline some business functions in the official statistical production process using statistical learning models. Our approach is quality-oriented searching for an improvement on accuracy, cost-efficiency, timeliness, granularity, response burden reduction, and frequency. Pilot experiences have been conducted with data from real surveys in Statistics Spain (INE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24394v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandra Barrag\'an, Adri\'an P\'erez-Bote, Carlos S\'aez, David Salgado, Luis Sanguiao-Sande</dc:creator>
    </item>
    <item>
      <title>Nearest Neighbor Matching as Least Squares Density Ratio Estimation and Riesz Regression</title>
      <link>https://arxiv.org/abs/2510.24433</link>
      <description>arXiv:2510.24433v1 Announce Type: cross 
Abstract: This study proves that Nearest Neighbor (NN) matching can be interpreted as an instance of Riesz regression for automatic debiased machine learning. Lin et al. (2023) shows that NN matching is an instance of density-ratio estimation with their new density-ratio estimator. Chernozhukov et al. (2024) develops Riesz regression for automatic debiased machine learning, which directly estimates the Riesz representer (or equivalently, the bias-correction term) by minimizing the mean squared error. In this study, we first prove that the density-ratio estimation method proposed in Lin et al. (2023) is essentially equivalent to Least-Squares Importance Fitting (LSIF) proposed in Kanamori et al. (2009) for direct density-ratio estimation. Furthermore, we derive Riesz regression using the LSIF framework. Based on these results, we derive NN matching from Riesz regression. This study is based on our work Kato (2025a) and Kato (2025b).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24433v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>An Economical Approach to Design with Precision Criteria</title>
      <link>https://arxiv.org/abs/2306.09476</link>
      <description>arXiv:2306.09476v4 Announce Type: replace 
Abstract: Estimation frameworks for statistical inference are preferred to hypothesis testing when quantifying uncertainty and precise estimation are more valuable than binary decisions about statistical significance. Study design for estimation-based investigations often uses precision criteria to select sample sizes that control the length of interval estimates with respect to a sampling distribution. In this paper, we formally define a distribution that characterizes the probability of obtaining a sufficiently narrow interval estimate as a function of the sample size. This distribution can be used to determine the smallest sample size needed to ensure an interval estimate is sufficiently narrow. We prove that this distribution is approximately normal in large-sample settings for many data generation processes. However, this approximate normality may not hold for studies with moderate sample sizes, particularly when incorporating prior information or obtaining asymmetric interval estimates. Thus, we also propose an efficient simulation-based approach to approximate the distribution for the sample size by estimating the sampling distribution of interval estimate lengths at only two sample sizes. Our methodology provides a unified framework for design with precision criteria in Bayesian and frequentist settings. We illustrate the broad applicability of this framework with several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09476v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>Cluster-Randomized Trials with Cross-Cluster Interference</title>
      <link>https://arxiv.org/abs/2310.18836</link>
      <description>arXiv:2310.18836v5 Announce Type: replace 
Abstract: The literature on cluster-randomized trials typically allows for interference within but not across clusters. This may be implausible when units are irregularly distributed across space without well-separated communities, as clusters in such cases may not align with significant geographic, social, or economic divisions. This paper develops methods for reducing bias due to cross-cluster interference. We first propose an estimation strategy that excludes units not surrounded by clusters assigned to the same treatment arm. We show that this substantially reduces bias relative to conventional difference-in-means estimators without significant cost to variance. Second, we formally establish a bias-variance trade-off in the choice of clusters: constructing fewer, larger clusters reduces bias due to interference but increases variance. We provide a rule for choosing the number of clusters to balance the asymptotic orders of the bias and variance of our estimator. Finally, we consider unsupervised learning for cluster construction and provide theoretical guarantees for $k$-medoids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18836v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Basis Selection for Functional Data Representation with Correlated Errors</title>
      <link>https://arxiv.org/abs/2405.20758</link>
      <description>arXiv:2405.20758v4 Announce Type: replace 
Abstract: Functional data analysis finds widespread application across various fields. While functional data are intrinsically infinite-dimensional, in practice, they are observed only at a finite set of points, typically over a dense grid. As a result, smoothing techniques are often used to approximate the observed data as functions. In this work, we propose a novel Bayesian approach for selecting basis functions for smoothing one or multiple curves simultaneously. Our method differentiates from other Bayesian approaches in two key ways: (i) by accounting for correlated errors and (ii) by developing a variational Expectation-Maximization (VEM) algorithm, which is faster than Markov chain Monte Carlo (MCMC) methods such as Gibbs sampling. Simulation studies demonstrate that our method effectively identifies the true underlying structure of the data across various scenarios, and it is applicable to different types of functional data. Our VEM algorithm not only recovers the basis coefficients and the correct set of basis functions but also estimates the existing within-curve correlation. When applied to the motorcycle, LIDAR (LIght Detection And Ranging) experiment and Canadian weather datasets, our method demonstrates comparable, and in some cases superior, performance in terms of adjusted R2 compared to regression splines, smoothing splines, least absolute shrinkage and selection operator (LASSO) and Bayesian LASSO. Our proposed method is implemented in R and codes are available at https://github.com/acarolcruz/VB-Bases-Selection</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20758v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Carolina da Cruz, Camila P. E. de Souza, Pedro H. T. O. Sousa</dc:creator>
    </item>
    <item>
      <title>Covariate-dependent hierarchical Dirichlet processes</title>
      <link>https://arxiv.org/abs/2407.02676</link>
      <description>arXiv:2407.02676v5 Announce Type: replace 
Abstract: Bayesian hierarchical modeling is a natural framework to effectively integrate data and borrow information across groups. In this paper, we address problems related to density estimation and identifying clusters across related groups, by proposing a hierarchical Bayesian approach that incorporates additional covariate information. To achieve flexibility, our approach builds on ideas from Bayesian nonparametrics, combining the hierarchical Dirichlet process with dependent Dirichlet processes. The proposed model is widely applicable, accommodating multiple and mixed covariate types through appropriate kernel functions as well as different output types through suitable component-specific likelihoods. This extends our ability to discern the relationship between covariates and clusters, while also effectively borrowing information and quantifying differences across groups. By employing a data augmentation trick, we are able to tackle the intractable normalized weights and construct a Markov chain Monte Carlo algorithm for posterior inference. The proposed method is illustrated on simulated data and two real data sets on single-cell RNA sequencing (scRNA-seq) and calcium imaging. For scRNA-seq data, we show that the incorporation of cell dynamics facilitates the discovery of additional cell subgroups. On calcium imaging data, our method identifies interpretable clusters of time frames with similar neural activity, aligning with the observed behavior of the animal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02676v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Huizi Zhang, Sara Wade, Natalia Bochkina</dc:creator>
    </item>
    <item>
      <title>Order of Addition in Mixture-Amount Experiments</title>
      <link>https://arxiv.org/abs/2410.04864</link>
      <description>arXiv:2410.04864v2 Announce Type: replace 
Abstract: In a mixture experiment, we study the behavior and properties of $m$ mixture components, where the primary focus is on the proportions of the components that make up the mixture rather than the total amount. Mixture-amount experiments are specialized types of mixture experiments where both the proportions of the components in the mixture and the total amount of the mixture are of interest. In this paper, we consider an Order-of-Addition (OofA) mixture-amount experiment in which the response depends on both the mixture amounts of components and their order of addition. Full mixture OofA designs are constructed to maintain orthogonality between the mixture-amount model terms and the effects of the order of addition. \answer{But the number of runs in such full OofA designs increases as $m$ increases. We employ the Threshold Accepting (TA) Algorithm to select an n-row subset from the full Order-of-Addition (OofA) mixture design that maximizes G-optimality while minimizing the number of experimental runs. Further, the G-efficiency criterion is used to assess how well the design supports the precise and unbiased estimation of the model parameters.} These designs enable the estimation of mixture-component model parameters and the order-of-addition effects. The Fraction of Design Space (FDS) plot is used to provide a visual assessment of the prediction capabilities of a design across the entire design space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04864v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taha Hasan, Touqeer Ahmad</dc:creator>
    </item>
    <item>
      <title>A semi-supervised framework for diverse multiple hypothesis testing scenarios</title>
      <link>https://arxiv.org/abs/2411.15771</link>
      <description>arXiv:2411.15771v3 Announce Type: replace 
Abstract: Standard multiple testing procedures are designed to report a list of discoveries, or suspected false null hypotheses, given the hypotheses' p-values or test scores. Recently there has been a growing interest in enhancing such procedures by combining additional information with the primary p-value or score. In line with this idea, we develop RESET (REScoring via Estimating and Training), which uses a unique data-splitting protocol that subsequently allows any semi-supervised learning approach to factor in the available side information while maintaining finite sample error rate control. Our practical implementation, RESET Ensemble, selects from an ensemble of classification algorithms so that it is compatible with a range of multiple testing scenarios without the need for the user to select the appropriate one. We apply RESET to both p-value and competition based multiple testing problems and show that RESET is (1) power-wise competitive, (2) fast compared to most tools and (3) able to uniquely achieve finite sample false discovery rate or false discovery exceedance control, depending on the user's preference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15771v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Freestone, William Stafford Noble, Uri Keich</dc:creator>
    </item>
    <item>
      <title>Regularisation of CART trees by summation of $p$-values</title>
      <link>https://arxiv.org/abs/2505.18769</link>
      <description>arXiv:2505.18769v2 Announce Type: replace 
Abstract: The standard procedure to decide on the complexity of a CART regression tree is to use cross-validation with the aim of obtaining a predictor that generalises well to unseen data. The randomness in the selection of folds implies that the selected CART regression tree is not a deterministic function of the data. Moreover, the cross-validation procedure may become time consuming and result in inefficient use of training data. We propose a simple deterministic in-sample method that can be used for stopping the growing of a CART regression tree based on node-wise statistical tests. This testing procedure is derived using a connection to change point detection, where the null hypothesis corresponds to no signal. The suggested $p$-value based procedure allows us to consider covariate vectors of arbitrary dimension and allows us to bound the $p$-value of an entire tree from above. Further, we show that the test detects a not too weak signal with a high probability, given a not too small sample size.
  We illustrate our methodology and the asymptotic results on both simulated and real world data. Additionally, we illustrate how the $p$-value based method can be used to construct a deterministic piece-wise constant auto-calibrated predictor based on a given black-box predictor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18769v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nils Engler, Mathias Lindholm, Filip Lindskog, Taariq Nazar</dc:creator>
    </item>
    <item>
      <title>Stratification-based Instrumental Variable Analysis Framework for Nonlinear Effect Analysis</title>
      <link>https://arxiv.org/abs/2507.07349</link>
      <description>arXiv:2507.07349v2 Announce Type: replace 
Abstract: Nonlinear causal effects are prevalent in many research scenarios involving continuous exposures, and instrumental variables (IVs) can be employed to investigate such effects, particularly in the presence of unmeasured confounders. However, common IV methods for nonlinear effect analysis, such as IV regression or the control-function method, have inherent limitations, leading to either low statistical power or potentially misleading conclusions. In this work, we propose an alternative IV framework for nonlinear effect analysis, which has recently emerged in genetic epidemiology and addresses many of the drawbacks of existing IV methods. This framework enables study of the effect function while avoiding unnecessary model assumptions. In particular, it facilitates the identification of change points or threshold values in causal effects. Through a wide variety of simulations, we demonstrate that our framework outperforms other representative nonlinear IV methods in predicting the effect shape when the instrument is weak and can accurately estimate the effect function as well as identify the change point and predict its value under various structural model and effect shape scenarios. We further apply our framework to assess the nonlinear effect of alcohol consumption on systolic blood pressure using a genetic instrument (i.e. Mendelian randomization) with UK Biobank data. Our analysis detects a threshold beyond which alcohol intake exhibits a clear causal effect on the outcome. Our results are consistent with published medical guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07349v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haodong Tian, Ashish Patel, Stephen Burgess</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v5 Announce Type: replace 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Chang, John Kairalla, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models</title>
      <link>https://arxiv.org/abs/2510.21661</link>
      <description>arXiv:2510.21661v2 Announce Type: replace 
Abstract: Functional data analysis (FDA) deals with high-resolution data recorded over a continuum, such as time, space or frequency. Device-based assessments of physical activity or sleep are objective yet still prone to measurement error. We present MECfda, an R package that (i) fits scalar-on-function, generalized scalar-on-function, and functional quantile regression models, and (ii) provides bias-corrected estimation when functional covariates are measured with error. By unifying these tools under a consistent syntax, MECfda enables robust inference for FDA applications that involve noisy functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21661v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyang Ji, Ufuk Beyaztas, Nicolas Escobar-Velasquez, Yuanyuan Luan, Xiwei Chen, Mengli Zhang, Roger Zoh, Lan Xue, Carmen Tekwe</dc:creator>
    </item>
    <item>
      <title>Optimal Spatial Anomaly Detection</title>
      <link>https://arxiv.org/abs/2510.22330</link>
      <description>arXiv:2510.22330v2 Announce Type: replace 
Abstract: There has been a growing interest in anomaly detection problems recently, whilst their focuses are mostly on anomalies taking place on the time index. In this work, we investigate a new anomaly-in-mean problem in multidimensional spatial lattice, that is, to detect the number and locations of anomaly ''spatial regions'' from the baseline. In addition to the classic minimisation over the cost function with a $L_0$ penalisation, we introduce an innovative penalty on the area of the minimum convex hull that covers the anomaly regions. We show that the proposed method yields a consistent estimation of the number of anomalies, and it achieves near optimal localisation error under the minimax framework. We also propose a dynamic programming algorithm to solve the double penalised cost minimisation approximately, and carry out large-scale Monte Carlo simulations to examine its numeric performance. The method has a wide range of applications in real-world problems. As an example, we apply it to detect the marine heatwaves using the sea surface temperature data from the European Space Agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22330v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baiyu Wang, Chao Zheng</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2310.13966</link>
      <description>arXiv:2310.13966v2 Announce Type: replace-cross 
Abstract: In recent years, transfer learning has garnered significant attention in the machine learning community. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the minimax optimal rate. Through extensive numerical experiments on synthetic data and real examples, we validate our theoretical findings and demonstrate the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13966v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Wang, Caixing Wang, Xin He, Xingdong Feng</dc:creator>
    </item>
    <item>
      <title>Conditional Rank-Rank Regression</title>
      <link>https://arxiv.org/abs/2407.06387</link>
      <description>arXiv:2407.06387v4 Announce Type: replace-cross 
Abstract: Rank-rank regression is commonly employed in economic research as a way of capturing the relationship between two economic variables. The slope of this regression is the Spearman rank correlation, a classical measure of association. However, in many applications it is common practice to include covariates to account for differences in association levels between groups as defined by the values of these covariates. This is either done by including the covariates or by modeling the residuals obtained after partialing out the impact of the covariates. In each of these instances the resulting rank-rank regression coefficients can be difficult to interpret. We propose the conditional rank-rank regression, which uses conditional ranks instead of unconditional ranks, to measure average within-group persistence. The coefficient of this new regression corresponds to the average Spearman rank correlation conditional on the covariates, a natural summary measure of within-group association. We develop a flexible estimation approach using distribution regression and establish a theoretical framework for large sample inference. An empirical study on intergenerational income mobility in Switzerland demonstrates the advantages of this approach. The study reveals stronger intergenerational persistence between fathers and sons compared to fathers and daughters, with the within-group persistence explaining 62% of the overall income persistence for sons and 52% for daughters. Smaller families and those with highly educated fathers exhibit greater persistence in economic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06387v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v3 Announce Type: replace-cross 
Abstract: Economists often estimate treatment effects in experiments using remotely sensed variables (RSVs), e.g., satellite images or mobile phone activity, in place of directly measured economic outcomes. A common practice is to use an observational sample to train a predictor of the economic outcome from the RSV, and then use these predictions as the outcomes in the experiment. We show that this method is biased whenever the RSV is a post-outcome variable, meaning that variation in the economic outcome causes variation in the RSV. For example, changes in poverty or environmental quality cause changes in satellite images, but not vice versa. As our main result, we nonparametrically identify the treatment effect by formalizing the intuition underlying common practice: the conditional distribution of the RSV given the outcome and treatment is stable across samples. Our identifying formula reveals that efficient inference requires predictions of three quantities from the RSV -- the outcome, treatment, and sample indicator -- whereas common practice only predicts the outcome. Valid inference does not require any rate conditions on RSV predictions, justifying the use of complex deep learning algorithms with unknown statistical properties. We reanalyze the effect of an anti-poverty program in India using satellite images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with Time-Varying Confounding</title>
      <link>https://arxiv.org/abs/2502.05295</link>
      <description>arXiv:2502.05295v2 Announce Type: replace-cross 
Abstract: Estimating causal effects from spatiotemporal observational data is essential in public health, environmental science, and policy evaluation, where randomized experiments are often infeasible. Existing approaches, however, either rely on strong structural assumptions or fail to handle key challenges such as interference, spatial confounding, temporal carryover, and time-varying confounding -- where covariates are influenced by past treatments and, in turn, affect future ones. We introduce GST-UNet (G-computation Spatio-Temporal UNet), a theoretically grounded neural framework that combines a U-Net-based spatiotemporal encoder with regression-based iterative G-computation to estimate location-specific potential outcomes under complex intervention sequences. GST-UNet explicitly adjusts for time-varying confounders and captures non-linear spatial and temporal dependencies, enabling valid causal inference from a single observed trajectory in data-scarce settings. We validate its effectiveness in synthetic experiments and in a real-world analysis of wildfire smoke exposure and respiratory hospitalizations during the 2018 California Camp Fire. Together, these results position GST-UNet as a principled and ready-to-use framework for spatiotemporal causal inference, advancing reliable estimation in policy-relevant and scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05295v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miruna Oprescu, David K. Park, Xihaier Luo, Shinjae Yoo, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Amortized variational transdimensional inference</title>
      <link>https://arxiv.org/abs/2506.04749</link>
      <description>arXiv:2506.04749v2 Announce Type: replace-cross 
Abstract: The expressiveness of flow-based models combined with stochastic variational inference (SVI) has expanded the application of optimization-based Bayesian inference to highly complex problems. However, despite the importance of multi-model Bayesian inference, defined over a transdimensional joint model and parameter space, flow-based SVI has been limited to problems defined over a fixed-dimensional parameter space. We introduce CoSMIC normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables use of a single amortized variational density for inference over a transdimensional (multi-model) conditional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using ideas from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments show the performance of VTI on challenging problems that scale to high-cardinality model spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04749v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurence Davies, Dan Mackinlay, Rafael Oliveira, Scott A. Sisson</dc:creator>
    </item>
  </channel>
</rss>
