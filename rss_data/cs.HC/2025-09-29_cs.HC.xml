<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>"I Don't Think RAI Applies to My Model'' -- Engaging Non-champions with Sticky Stories for Responsible AI Work</title>
      <link>https://arxiv.org/abs/2509.22858</link>
      <description>arXiv:2509.22858v1 Announce Type: new 
Abstract: Responsible AI (RAI) tools -- checklists, templates, and governance processes -- often engage RAI champions, individuals intrinsically motivated to advocate ethical practices, but fail to reach non-champions, who frequently dismiss them as bureaucratic tasks. To explore this gap, we shadowed meetings and interviewed data scientists at an organization, finding that practitioners perceived RAI as irrelevant to their work. Building on these insights and theoretical foundations, we derived design principles for engaging non-champions, and introduced sticky stories -- narratives of unexpected ML harms designed to be concrete, severe, surprising, diverse, and relevant, unlike widely circulated media to which practitioners are desensitized. Using a compound AI system, we generated and evaluated sticky stories through human and LLM assessments at scale, confirming they embodied the intended qualities. In a study with 29 practitioners, we found that, compared to regular stories, sticky stories significantly increased time spent on harm identification, broadened the range of harms recognized, and fostered deeper reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22858v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadia Nahar, Chenyang Yang, Yanxin Chen, Wesley Hanwen Deng, Ken Holstein, Motahhare Eslami, Christian K\"astner</dc:creator>
    </item>
    <item>
      <title>What If Moderation Didn't Mean Suppression? A Case for Personalized Content Transformation</title>
      <link>https://arxiv.org/abs/2509.22861</link>
      <description>arXiv:2509.22861v1 Announce Type: new 
Abstract: Centralized content moderation paradigm both falls short and over-reaches: 1) it fails to account for the subjective nature of harm, and 2) it acts with blunt suppression in response to content deemed harmful, even when such content can be salvaged. We first investigate this through formative interviews, documenting how seemingly benign content becomes harmful due to individual life experiences. Based on these insights, we developed DIY-MOD, a browser extension that operationalizes a new paradigm: personalized content transformation. Operating on a user's own definition of harm, DIY-MOD transforms sensitive elements within content in real-time instead of suppressing the content itself. The system selects the most appropriate transformation for a piece of content from a diverse palette--from obfuscation to artistic stylizing--to match the user's specific needs while preserving the content's informational value. Our two-session user study demonstrates that this approach increases users' sense of agency and safety, enabling them to engage with content and communities they previously needed to avoid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22861v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rayhan Rashed, Farnaz Jahanbakhsh</dc:creator>
    </item>
    <item>
      <title>Explicit modelling of subject dependency in BCI decoding</title>
      <link>https://arxiv.org/abs/2509.23247</link>
      <description>arXiv:2509.23247v1 Announce Type: new 
Abstract: Brain-Computer Interfaces (BCIs) suffer from high inter-subject variability and limited labeled data, often requiring lengthy calibration phases. In this work, we present an end-to-end approach that explicitly models the subject dependency using lightweight convolutional neural networks (CNNs) conditioned on the subject's identity. Our method integrates hyperparameter optimization strategies that prioritize class imbalance and evaluates two conditioning mechanisms to adapt pre-trained models to unseen subjects with minimal calibration data. We benchmark three lightweight architectures on a time-modulated Event-Related Potentials (ERP) classification task, providing interpretable evaluation metrics and explainable visualizations of the learned representations. Results demonstrate improved generalization and data-efficient calibration, highlighting the scalability and practicality of subject-adaptive BCIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23247v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Romani, Francesco Paissan, Andrea Foss\`a, Elisabetta Farella</dc:creator>
    </item>
    <item>
      <title>Debiasing the Influence of Demographic and Appearance Cues in Social Engineering via Role-Taking: Negative Results</title>
      <link>https://arxiv.org/abs/2509.23271</link>
      <description>arXiv:2509.23271v1 Announce Type: new 
Abstract: This study investigates the efficacy of role-taking and literacy-based interventions in reducing the influence of appearance cues, such as gender, age, ethnicity, and clothing style, on trust and risk-taking in social engineering contexts. A-4 (Group: Control, Literacy, Persuader, Persuadee) * 2 (Time: Pre, Post) mixed factorial design was implemented over two weeks with 139 participants. The control group received no material. The literacy group attended two sessions focused on how behavior can be similar regardless of appearance cues. The persuader group completed three sessions, learning how to use such cues to influence others. The persuadee group attended three sessions involving the selection, justification, and reflection on personas and scenarios. Scenarios centered on financial and rental advice. A one-week gap followed before post-intervention testing. In both pre- and post-tests, participants assessed personas combining appearance cues, offering mobile hotspots with potential risk. They rated trust and willingness to take the risk. Validated measures and scenarios were used, including word-of-mouth and issue involvement scales. It was expected that cue influence would diminish post-intervention. However, no significant within- or between-group differences emerged. Findings raise concerns about the effectiveness of debiasing efforts and call for reconsideration of approaches using literacy, role-taking, rehearsal, drama, and simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23271v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tourjana Islam Supti (College of Engineering, Qatar University, Doha, Qatar), Israa Abuelezz (College of Engineering, Qatar University, Doha, Qatar), Aya Muhanad (College of Engineering, Qatar University, Doha, Qatar), Mahmoud Barhmagi (College of Engineering, Qatar University, Doha, Qatar), Ala Yankouskaya (Department of Psychology, Bournemouth University, Poole, UK), Khaled M. Khan (College of Engineering, Qatar University, Doha, Qatar), Aiman Erbad (College of Engineering, Qatar University, Doha, Qatar), Raian Ali (College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar)</dc:creator>
    </item>
    <item>
      <title>Designing AI-Infused Interactive Systems for Online Communities: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2509.23309</link>
      <description>arXiv:2509.23309v1 Announce Type: new 
Abstract: AI-infused systems have demonstrated remarkable capabilities in addressing diverse human needs within online communities. Their widespread adoption has shaped user experiences and community dynamics at scale. However, designing such systems requires a clear understanding of user needs, careful design decisions, and robust evaluation. While research on AI-infused systems for online communities has flourished in recent years, a comprehensive synthesis of this space remains absent. In this work, we present a systematic review of 77 studies, analyzing the systems they propose through three lenses: the challenges they aim to address, their design functionalities, and the evaluation strategies employed. The first two dimensions are organized around four core aspects of community participation: contribution, consumption, mediation, and moderation. Our analysis identifies common design and evaluation patterns, distills key design considerations, and highlights opportunities for future research on AI-infused systems in online communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23309v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuanhao Zhang, Xiaoyu Wang, Jiaxiong Hu, Ziqi Pan, Zhenhui Peng, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>"Shall We Dig Deeper?": Designing and Evaluating Strategies for LLM Agents to Advance Knowledge Co-Construction in Asynchronous Online Discussions</title>
      <link>https://arxiv.org/abs/2509.23327</link>
      <description>arXiv:2509.23327v1 Announce Type: new 
Abstract: Asynchronous online discussions enable diverse participants to co-construct knowledge beyond individual contributions. This process ideally evolves through sequential phases, from superficial information exchange to deeper synthesis. However, many discussions stagnate in the early stages. Existing AI interventions typically target isolated phases, lacking mechanisms to progressively advance knowledge co-construction, and the impacts of different intervention styles in this context remain unclear and warrant investigation. To address these gaps, we conducted a design workshop to explore AI intervention strategies (task-oriented and/or relationship-oriented) throughout the knowledge co-construction process, and implemented them in an LLM-powered agent capable of facilitating progression while consolidating foundations at each phase. A within-subject study (N=60) involving five consecutive asynchronous discussions showed that the agent consistently promoted deeper knowledge progression, with different styles exerting distinct effects on both content and experience. These findings provide actionable guidance for designing adaptive AI agents that sustain more constructive online discussions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23327v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuanhao Zhang, Wenbo Li, Xiaoyu Wang, Kangyu Yuan, Shuai Ma, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>New Synthetic Goldmine: Hand Joint Angle-Driven EMG Data Generation Framework for Micro-Gesture Recognition</title>
      <link>https://arxiv.org/abs/2509.23359</link>
      <description>arXiv:2509.23359v1 Announce Type: new 
Abstract: Electromyography (EMG)-based gesture recognition has emerged as a promising approach for human-computer interaction. However, its performance is often limited by the scarcity of labeled EMG data, significant cross-user variability, and poor generalization to unseen gestures. To address these challenges, we propose SeqEMG-GAN, a conditional, sequence-driven generative framework that synthesizes high-fidelity EMG signals from hand joint angle sequences. Our method introduces a context-aware architecture composed of an angle encoder, a dual-layer context encoder featuring the novel Ang2Gist unit, a deep convolutional EMG generator, and a discriminator, all jointly optimized via adversarial learning. By conditioning on joint kinematic trajectories, SeqEMG-GAN is capable of generating semantically consistent EMG sequences, even for previously unseen gestures, thereby enhancing data diversity and physiological plausibility. Experimental results show that classifiers trained solely on synthetic data experience only a slight accuracy drop (from 57.77% to 55.71%). In contrast, training with a combination of real and synthetic data significantly improves accuracy to 60.53%, outperforming real-only training by 2.76%. These findings demonstrate the effectiveness of our framework,also achieves the state-of-art performance in augmenting EMG datasets and enhancing gesture recognition performance for applications such as neural robotic hand control, AI/AR glasses, and gesture-based virtual gaming systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23359v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nana Wang, Gen Li, Suli Wang, Pengfei Ren, Hao Su</dc:creator>
    </item>
    <item>
      <title>Bridging the Trust Gap in Crowdfunding: A Novel Expert-Based Evaluation Mechanism</title>
      <link>https://arxiv.org/abs/2509.23378</link>
      <description>arXiv:2509.23378v1 Announce Type: new 
Abstract: Crowdfunding has emerged as a vital alternative funding source, transforming how creative projects and startups secure financing by directly connecting creators to backers. However, persistent trust issues and information asymmetry between creators and backers significantly hinder its growth and development. Existing trust-enhancement mechanisms, such as third-party endorsements and basic expert validation often lack objectivity and robustness, leaving backers vulnerable to biased signals and project failures. This paper addresses these limitations by introducing a novel trust-enhancement mechanism, referred to as Double-Score Voting. This approach refines expert validation systems by integrating two critical dimensions: firstly, a granular score-based vote from experts on a project's potential, moving beyond simple binary approval; and secondly, a weighted score representing the expert's credibility and level of expertise. This dual-layered evaluation provides a more nuanced, objective, and reliable assessment of project viability. The mechanism is formalised mathematically, and its practical implementation is demonstrated through CertiFund, a prototype crowdfunding platform developed to test and validate the concept. The findings of this study demonstrate that the Double-Score Voting mechanism can significantly mitigate information asymmetry, thereby increasing the credibility of projects and fostering a more trustworthy ecosystem for both creators and backers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23378v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Issam Hosni, Omar Talbi</dc:creator>
    </item>
    <item>
      <title>NeuroBridge: Using Generative AI to Bridge Cross-neurotype Communication Differences through Neurotypical Perspective-taking</title>
      <link>https://arxiv.org/abs/2509.23434</link>
      <description>arXiv:2509.23434v1 Announce Type: new 
Abstract: Communication challenges between autistic and neurotypical individuals stem from a mutual lack of understanding of each other's distinct, and often contrasting, communication styles. Yet, autistic individuals are expected to adapt to neurotypical norms, making interactions inauthentic and mentally exhausting for them. To help redress this imbalance, we build NeuroBridge, an online platform that utilizes large language models (LLMs) to simulate: (a) an AI character that is direct and literal, a style common among many autistic individuals, and (b) four cross-neurotype communication scenarios in a feedback-driven conversation between this character and a neurotypical user. Through NeuroBridge, neurotypical individuals gain a firsthand look at autistic communication, and reflect on their role in shaping cross-neurotype interactions. In a user study with 12 neurotypical participants, we find that NeuroBridge improved their understanding of how autistic people may interpret language differently, with all describing autism as a social difference that "needs understanding by others" after completing the simulation. Participants valued its personalized, interactive format and described AI-generated feedback as "constructive", "logical" and "non-judgmental". Most perceived the portrayal of autism in the simulation as accurate, suggesting that users may readily accept AI-generated (mis)representations of disabilities. To conclude, we discuss design implications for disability representation in AI, the need for making NeuroBridge more personalized, and LLMs' limitations in modeling complex social scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23434v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3663547.3746337</arxiv:DOI>
      <dc:creator>Rukhshan Haroon, Kyle Wigdor, Katie Yang, Nicole Toumanios, Eileen T. Crehan, Fahad Dogar</dc:creator>
    </item>
    <item>
      <title>DraftMarks: Enhancing Transparency in Human-AI Co-Writing Through Interactive Skeuomorphic Process Traces</title>
      <link>https://arxiv.org/abs/2509.23505</link>
      <description>arXiv:2509.23505v1 Announce Type: new 
Abstract: As generative AI becomes part of everyday writing, questions of transparency and productive human effort are increasingly important. Educators, reviewers, and readers want to understand how AI shaped the process. Where was human effort focused? What role did AI play in the creation of the work? How did the interaction unfold? Existing approaches often reduce these dynamics to summary metrics or simplified provenance. We introduce DraftMarks, an augmented reading tool that surfaces the human-AI writing process through familiar physical metaphors. DraftMarks employs skeuomorphic encodings such as eraser crumbs to convey the intensity of revision, and masking tape or smudges to mark AI-generated content, simulating the process within the final written artifact. By using data from writer-AI interactions, DraftMarks' algorithm computes various collaboration metrics and writing traces. Through a formative study, we identified computational logic for different readership, and evaluated DraftMarks for its effectiveness in assessing AI co-authored writing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23505v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Momin N. Siddiqui, Nikki Nasseri, Adam Coscia, Roy Pea, Hari Subramonyam</dc:creator>
    </item>
    <item>
      <title>Exploring Collaboration Breakdowns Between Provider Teams and Patients in Post-Surgery Care</title>
      <link>https://arxiv.org/abs/2509.23509</link>
      <description>arXiv:2509.23509v1 Announce Type: new 
Abstract: Post-surgery care involves ongoing collaboration between provider teams and patients, which starts from post-surgery hospitalization through home recovery after discharge. While prior HCI research has primarily examined patients' challenges at home, less is known about how provider teams coordinate discharge preparation and care handoffs, and how breakdowns in communication and care pathways may affect patient recovery. To investigate this gap, we conducted semi-structured interviews with 13 healthcare providers and 4 patients in the context of gastrointestinal (GI) surgery. We found coordination boundaries between in- and out-patient teams, coupled with complex organizational structures within teams, impeded the "invisible work" of preparing patients' home care plans and triaging patient information. For patients, these breakdowns resulted in inadequate preparation for home transition and fragmented self-collected data, both of which undermine timely clinical decision-making. Based on these findings, we outline design opportunities to formalize task ownership and handoffs, contextualize co-temporal signals, and align care plans with home resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23509v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bingsheng Yao, Menglin Zhao, Zhan Zhang, Pengqi Wang, Emma G Chester, Changchang Yin, Tianshi Li, Varun Mishra, Lace Padilla, Odysseas Chatzipanagiotou, Timothy Pawlik, Ping Zhang, Weidan Cao, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>Eye-Tracking and BCI Integration for Assistive Communication in Locked-In Syndrome: Pilot Study with Healthy Participants</title>
      <link>https://arxiv.org/abs/2509.23518</link>
      <description>arXiv:2509.23518v1 Announce Type: new 
Abstract: Patients with Amyotrophic Lateral Sclerosis (ALS) progressively lose voluntary motor control, often leading to a Locked-In State (LIS), or in severe cases, a Completely Locked-in State (CLIS). Eye-tracking (ET) systems are common communication tools in early LIS but become ineffective as oculomotor function declines. EEG-based Brain-Computer Interfaces (BCIs) offer a non-muscular communication alternative, but delayed adoption may reduce performance due to diminished goal-directed thinking. This study presents a preliminary hybrid BCI framework combining ET and BCI to support a gradual transition between modalities. A group of five healthy participants tested a modified P300-based BCI. Gaze and EEG data were processed in real time, and an ET-BCI fusion algorithm was proposed to enhance detection of user intention. Results indicate that combining both modalities may maintain high accuracy and offers insights on how to potentially improve communication continuity for patients transitioning from LIS to CLIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23518v1</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Patr\'icia Pinto, Rute Bettencourt, Urbano J. Nunes, Gabriel Pires</dc:creator>
    </item>
    <item>
      <title>Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI Product Concepts</title>
      <link>https://arxiv.org/abs/2509.23525</link>
      <description>arXiv:2509.23525v1 Announce Type: new 
Abstract: AI creates and exacerbates privacy risks, yet practitioners lack effective resources to identify and mitigate these risks. We present Privy, a tool that guides practitioners through structured privacy impact assessments to: (i) identify relevant risks in novel AI product concepts, and (ii) propose appropriate mitigations. Privy was shaped by a formative study with 11 practitioners, which informed two versions -- one LLM-powered, the other template-based. We evaluated these two versions of Privy through a between-subjects, controlled study with 24 separate practitioners, whose assessments were reviewed by 13 independent privacy experts. Results show that Privy helps practitioners produce privacy assessments that experts deemed high quality: practitioners identified relevant risks and proposed appropriate mitigation strategies. These effects were augmented in the LLM-powered version. Practitioners themselves rated Privy as being useful and usable, and their feedback illustrates how it helps overcome long-standing awareness, motivation, and ability barriers in privacy work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23525v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hao-Ping Lee, Yu-Ju Yang, Matthew Bilik, Isadora Krsek, Thomas Serban von Davier, Kyzyl Monteiro, Jason Lin, Shivani Agarwal, Jodi Forlizzi, Sauvik Das</dc:creator>
    </item>
    <item>
      <title>Community Analysis of Social Virtual Reality Based on Large-Scale Log Data of a Commercial Metaverse Platform</title>
      <link>https://arxiv.org/abs/2509.23654</link>
      <description>arXiv:2509.23654v1 Announce Type: new 
Abstract: This study quantitatively analyzes the structural characteristics of user communities within Social Virtual Reality (Social VR) platforms supporting head-mounted displays (HMDs), based on large-scale log data. By detecting and evaluating community structures from data on substantial interactions (defined as prolonged co-presence in the same virtual space), we found that Social VR platforms tend to host numerous, relatively small communities characterized by strong internal cohesion and limited inter-community connections. This finding contrasts with the large-scale, broadly connected community structures typically observed in conventional Social Networking Services (SNS). Furthermore, we identified a user segment capable of mediating between communities, despite these users not necessarily having numerous direct connections. We term this user segment `community hoppers' and discuss their characteristics. These findings contribute to a deeper understanding of the community structures that emerge within the unique communication environment of Social VR and the roles users play within them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23654v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroto Tsutsui, Takefumi Hiraki, Yuichi Hiroi, Shoichi Hasegawa</dc:creator>
    </item>
    <item>
      <title>"Having Lunch Now": Understanding How Users Engage with a Proactive Agent for Daily Planning and Self-Reflection</title>
      <link>https://arxiv.org/abs/2509.24073</link>
      <description>arXiv:2509.24073v1 Announce Type: new 
Abstract: Conversational agents have been studied as tools to scaffold planning and self-reflection for productivity and well-being. While prior work has demonstrated positive outcomes, we still lack a clear understanding of what drives these results and how users behave and communicate with agents that act as coaches rather than assistants. Such understanding is critical for designing interactions in which agents foster meaningful behavioral change. We conducted a 14-day longitudinal study with 12 participants using a proactive agent that initiated regular check-ins to support daily planning and reflection. Our findings reveal diverse interaction patterns: participants accepted or negotiated suggestions, developed shared mental models, reported progress, and at times resisted or disengaged. We also identified problematic aspects of the agent's behavior, including rigidity, premature turn-taking, and overpromising. Our work contributes to understanding how people interact with a proactive, coach-like agent and offers design considerations for facilitating effective behavioral change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24073v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adnan Abbas, Caleb Wohn, Arnav Jagtap, Eugenia H Rho, Sang Won Lee</dc:creator>
    </item>
    <item>
      <title>WireBend-kit: A Computational Design and Fabrication Toolkit for Wirebending Custom 3D Wireframe Structures</title>
      <link>https://arxiv.org/abs/2509.24083</link>
      <description>arXiv:2509.24083v1 Announce Type: new 
Abstract: This paper introduces WireBend-kit, a desktop wirebending machine and computational design tool for creating 3D wireframe structures. Combined, they allow users to rapidly and inexpensively create custom 3D wireframe structures from aluminum wire. Our design tool is implemented in freely available software and allows users to generate virtual wireframe designs and assess their fabricability. A path-planning procedure automatically converts the wireframe design into fabrication instructions for our machine while accounting for material elasticity and kinematic error sources. The custom machine costs $293 in parts and can form aluminum wire into 3D wireframe structures through an ordered sequence of feed, bend, and rotate instructions. Our technical evaluation reveals our system's ability to overcome odometrically accumulating errors inherent to wirebending in order to produce accurate 3D structures from inexpensive hardware. Finally, we provide application examples demonstrating the design space enabled by Wirebend-kit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24083v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3745778.3766662</arxiv:DOI>
      <dc:creator>Faraz Faruqi, Josha Paonaskar, Riley Schuler, Aiden Prevey, Carson Taylor, Anika Tak, Anthony Guinto, Eeshani Shilamkar, Natarith Cheenaruenthong, Martin Nisser</dc:creator>
    </item>
    <item>
      <title>Exploring Opportunities to Support Novice Visual Artists' Inspiration and Ideation with Generative AI</title>
      <link>https://arxiv.org/abs/2509.24167</link>
      <description>arXiv:2509.24167v1 Announce Type: new 
Abstract: Recent generative AI advances present new possibilities for supporting visual art creation, but how such promise might assist novice artists during early-stage processes requires investigation. How novices adopt or resist these tools can shift the relationship between the art community and generative systems. We interviewed 13 artists to uncover needs in key dimensions during early stages of creation: (1) quicker and better access to references, (2) visualizations of reference combinations, (3) external artistic feedback, and (4) personalized support to learn new techniques and styles. Mapping such needs to state-of-the-art open-sourced advances, we developed a set of six interactive prototypes to expose emerging capabilities to novice artists. Afterward, we conducted co-design workshops with 13 novice visual artists through which artists articulated requirements and tensions for artist-centered AI development. Our work reveals opportunities to design novice-targeted tools that foreground artists' needs, offering alternative visions for generative AI to serve visual creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24167v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cindy Peng, Alice Qian, Linghao Jin, Jieneng Chen, Evans Xu Han, Paul Pu Liang, Hong Shen, Haiyi Zhu, Jane Hsieh</dc:creator>
    </item>
    <item>
      <title>Understanding Cognitive States from Head &amp; Hand Motion Data</title>
      <link>https://arxiv.org/abs/2509.24255</link>
      <description>arXiv:2509.24255v1 Announce Type: new 
Abstract: As virtual reality (VR) and augmented reality (AR) continue to gain popularity, head and hand motion data captured by consumer VR systems have become ubiquitous. Prior work shows that such telemetry can be highly identifying and reflect broad user traits, often aligning with intuitive "folk theories" of body language. However, it remains unclear to what extent motion kinematics encode more nuanced cognitive states, such as confusion, hesitation, and readiness, which lack clear correlates with motion. To investigate this, we introduce a novel dataset of head and hand motion with frame-level annotations of these states collected during structured decision-making tasks. Our findings suggest that deep temporal models can infer subtle cognitive states from motion alone, achieving comparable performance with human observers. This work demonstrates that standard VR telemetry contains strong patterns related to users' internal cognitive processes, which opens the door for a new generation of adaptive virtual environments. To enhance reproducibility and support future work, we will make our dataset and modeling framework publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24255v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiang Wen, Mark Roman Miller</dc:creator>
    </item>
    <item>
      <title>Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports</title>
      <link>https://arxiv.org/abs/2509.24298</link>
      <description>arXiv:2509.24298v1 Announce Type: new 
Abstract: The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24298v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.MM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changde Du, Yizhuo Lu, Zhongyu Huang, Yi Sun, Zisen Zhou, Shaozheng Qin, Huiguang He</dc:creator>
    </item>
    <item>
      <title>Exploring Similarity between Neural and LLM Trajectories in Language Processing</title>
      <link>https://arxiv.org/abs/2509.24307</link>
      <description>arXiv:2509.24307v1 Announce Type: new 
Abstract: Understanding the similarity between large language models (LLMs) and human brain activity is crucial for advancing both AI and cognitive neuroscience. In this study, we provide a multilinguistic, large-scale assessment of this similarity by systematically comparing 16 publicly available pretrained LLMs with human brain responses during natural language processing tasks in both English and Chinese. Specifically, we use ridge regression to assess the representational similarity between LLM embeddings and electroencephalography (EEG) signals, and analyze the similarity between the "neural trajectory" and the "LLM latent trajectory." This method captures key dynamic patterns, such as magnitude, angle, uncertainty, and confidence. Our findings highlight both similarities and crucial differences in processing strategies: (1) We show that middle-to-high layers of LLMs are central to semantic integration and correspond to the N400 component observed in EEG; (2) The brain exhibits continuous and iterative processing during reading, whereas LLMs often show discrete, stage-end bursts of activity, which suggests a stark contrast in their real-time semantic processing dynamics. This study could offer new insights into LLMs and neural processing, and also establish a critical framework for future investigations into the alignment between artificial intelligence and biological intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24307v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Xiao, Kaiwen Wei, Jiang Zhong, Dongshuo Yin, Yu Tian, Xuekai Wei, Mingliang Zhou</dc:creator>
    </item>
    <item>
      <title>TraitSpaces: Towards Interpretable Visual Creativity for Human-AI Co-Creation</title>
      <link>https://arxiv.org/abs/2509.24326</link>
      <description>arXiv:2509.24326v1 Announce Type: new 
Abstract: We introduce a psychologically grounded and artist-informed framework for modeling visual creativity across four domains: Inner, Outer, Imaginative, and Moral Worlds. Drawing on interviews with practicing artists and theories from psychology, we define 12 traits that capture affective, symbolic, cultural, and ethical dimensions of creativity.Using 20k artworks from the SemArt dataset, we annotate images with GPT 4.1 using detailed, theory-aligned prompts, and evaluate the learnability of these traits from CLIP image embeddings. Traits such as Environmental Dialogicity and Redemptive Arc are predicted with high reliability ($R^2 \approx 0.64 - 0.68$), while others like Memory Imprint remain challenging, highlighting the limits of purely visual encoding. Beyond technical metrics, we visualize a "creativity trait-space" and illustrate how it can support interpretable, trait-aware co-creation - e.g., sliding along a Redemptive Arc axis to explore works of adversity and renewal. By linking cultural-aesthetic insights with computational modeling, our work aims not to reduce creativity to numbers, but to offer shared language and interpretable tools for artists, researchers, and AI systems to collaborate meaningfully.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24326v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prerna Luthra</dc:creator>
    </item>
    <item>
      <title>Investigating the Task Load of Investigating the Task Load in Visualization Studies</title>
      <link>https://arxiv.org/abs/2509.24643</link>
      <description>arXiv:2509.24643v1 Announce Type: new 
Abstract: The NASA task load index (short: NASA-TLX) is a common metric to evaluate the workload of a user in a visualization study. Yet, it is rarely performed as initially intended, as the sources-of-workload evaluation is often omitted for various reasons. We conduct an online survey to investigate the task load of administering different versions of the NASA-TLX in a meta-study using the ReVISit framework. Our results show that it is not the slight increase in experiment time, but rather participants' frustration with the procedure, that contributes to the slight increase in task load when using the full version of the TLX compared to using a shortened version. However, we also show that the full version can shine a different and more faceted light on workload by adding a personal dimension to the data. We propose that a compact version of the sources-of-workload questionnaire can mitigate both time loss and frustration for study participants, while still providing the same data as the original procedure. The online study can be found and interactively explored on https://dpahr.github.io/tlxtlx/, and the source for the study, as well as the code for our analysis, can be found on https://github.com/dpahr/tlxtlx/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24643v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Pahr, Sara Di Bartolomeo</dc:creator>
    </item>
    <item>
      <title>A Robust Multi-Scale Framework with Test-Time Adaptation for sEEG-Based Speech Decoding</title>
      <link>https://arxiv.org/abs/2509.24700</link>
      <description>arXiv:2509.24700v1 Announce Type: new 
Abstract: Decoding speech from stereo-electroencephalography (sEEG) signals has emerged as a promising direction for brain-computer interfaces (BCIs). Its clinical applicability, however, is limited by the inherent non-stationarity of neural signals, which causes domain shifts between training and testing, undermining decoding reliability. To address this challenge, a two-stage framework is proposed for enhanced robustness. First, a multi-scale decomposable mixing (MDM) module is introduced to model the hierarchical temporal dynamics of speech production, learning stable multi-timescale representations from sEEG signals. Second, a source-free online test-time adaptation (TTA) method performs entropy minimization to adapt the model to distribution shifts during inference. Evaluations on the public DU-IN spoken word decoding benchmark show that the approach outperforms state-of-the-art models, particularly in challenging cases. This study demonstrates that combining invariant feature learning with online adaptation is a principled strategy for developing reliable BCI systems. Our code is available at https://github.com/lyyi599/MDM-TENT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24700v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suli Wang, Yang-yang Li, Siqi Cai, Haizhou Li</dc:creator>
    </item>
    <item>
      <title>Understanding Collaboration between Professional Designers and Decision-making AI: A Case Study in the Workplace</title>
      <link>https://arxiv.org/abs/2509.24718</link>
      <description>arXiv:2509.24718v1 Announce Type: new 
Abstract: The rapid development of artificial intelligence (AI) has fundamentally transformed creative work practices in the design industry. Existing studies have identified both opportunities and challenges for creative practitioners in their collaboration with generative AI and explored ways to facilitate effective human-AI co-creation. However, there is still a limited understanding of designers' collaboration with AI that supports creative processes distinct from generative AI. To address these gaps, this study focuses on understanding designers' collaboration with decision-making AI, which supports the convergence process in the creative workflow, as opposed to the divergent process supported by generative AI. Specifically, we conducted a case study at an online advertising design company to explore how professional graphic designers at the company perceive the impact of decision-making AI on their creative work practices. The case company incorporated an AI system that predicts the effectiveness of advertising design into the design workflow as a decision-making support tool. Findings from interviews with 12 designers identified how designers trust and rely on AI, its perceived benefits and challenges, and their strategies for navigating the challenges. Based on the findings, we discuss design recommendations for integrating decision-making AI into the creative design workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24718v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3757686</arxiv:DOI>
      <arxiv:journal_reference>PACMHCI, Vol. 9, No. 7 (CSCW), November 2025 Issue</arxiv:journal_reference>
      <dc:creator>Nami Ogawa, Yuki Okafuji, Yuji Hatada, Jun Baba</dc:creator>
    </item>
    <item>
      <title>Diamonds in the rough: Transforming SPARCs of imagination into a game concept by leveraging medium sized LLMs</title>
      <link>https://arxiv.org/abs/2509.24730</link>
      <description>arXiv:2509.24730v1 Announce Type: new 
Abstract: Recent research has demonstrated that large language models (LLMs) can support experts across various domains, including game design. In this study, we examine the utility of medium-sized LLMs, models that operate on consumer-grade hardware typically available in small studios or home environments. We began by identifying ten key aspects that contribute to a strong game concept and used ChatGPT to generate thirty sample game ideas. Three medium-sized LLMs, LLaMA 3.1, Qwen 2.5, and DeepSeek-R1, were then prompted to evaluate these ideas according to the previously identified aspects. A qualitative assessment by two researchers compared the models' outputs, revealing that DeepSeek-R1 produced the most consistently useful feedback, despite some variability in quality. To explore real-world applicability, we ran a pilot study with ten students enrolled in a storytelling course for game development. At the early stages of their own projects, students used our prompt and DeepSeek-R1 to refine their game concepts. The results indicate a positive reception: most participants rated the output as high quality and expressed interest in using such tools in their workflows. These findings suggest that current medium-sized LLMs can provide valuable feedback in early game design, though further refinement of prompting methods could improve consistency and overall effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24730v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Geheeb, Farhan Abid Ivan, Daniel Dyrda, Miriam Ansch\"utz, Georg Groh</dc:creator>
    </item>
    <item>
      <title>AIPOM: Agent-aware Interactive Planning for Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2509.24826</link>
      <description>arXiv:2509.24826v1 Announce Type: new 
Abstract: Large language models (LLMs) are being increasingly used for planning in orchestrated multi-agent systems. However, existing LLM-based approaches often fall short of human expectations and, critically, lack effective mechanisms for users to inspect, understand, and control their behaviors. These limitations call for enhanced transparency, controllability, and human oversight. To address this, we introduce AIPOM, a system supporting human-in-the-loop planning through conversational and graph-based interfaces. AIPOM enables users to transparently inspect, refine, and collaboratively guide LLM-generated plans, significantly enhancing user control and trust in multi-agent workflows. Our code and demo video are available at https://github.com/megagonlabs/aipom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24826v1</guid>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hannah Kim, Kushan Mitra, Chen Shen, Dan Zhang, Estevam Hruschka</dc:creator>
    </item>
    <item>
      <title>Rescue Operators' Perspectives on KIRETT Wearable Technology: A Qualitative Study</title>
      <link>https://arxiv.org/abs/2509.24831</link>
      <description>arXiv:2509.24831v1 Announce Type: new 
Abstract: In emergencies, treatment needs to be fast, accu-rate and patient-specific. For instance, in emergency scenarios, obstacles like treatment environments and medical difficulties can lead to bad outcomes for patients. Additionally, a drastic change of health vitals can force paramedics to shift to a different treatment in the ongoing treatment of the patient in order to save a patient's life. The KIRETT (engl.: 'Artificial intelligence in rescue operations') demonstrator is developed to provide a rescue operator with a wrist-worn device, enabling treatment recommendation (with the help of knowledge graph) with situation detection models to improve the emergency treatment of a patient. This paper aims to provide a qualitative evaluation of the 2-days testing in the KIRETT project with the focus of knowledge graphs, knowledge fusion, and user-experience-design (UX-design).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24831v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/smc54092.2024.10831087</arxiv:DOI>
      <dc:creator>Mubaris Nadeem, Johannes Zenkert, Lisa Bender, Christian Weber, Madjid Fathi</dc:creator>
    </item>
    <item>
      <title>Interface Design to Support Legal Reading and Writing: Insights from Interviews with Legal Experts</title>
      <link>https://arxiv.org/abs/2509.24854</link>
      <description>arXiv:2509.24854v1 Announce Type: new 
Abstract: Legal professionals spend significant time reading, writing, and interpreting complex documents, yet research has not fully captured how they approach these tasks or what they expect from skimming and writing-support tools. To examine practices and views on emerging tools, we interviewed 22 legal professionals about workflows, challenges, and technology use. In each session, we leveraged prior HCI-based skimming and writing prototypes that surface emergent cross-document relationships and support AI-resilient interaction (noticing, judging, and recovering from model errors or unexpected behavior); participants completed a contextual fit evaluation to assess whether and how they would use the tools, which document types, and at what stages in their work. Our analysis details limitations and challenges in workflows, domain-specific feedback on AI-resilient interfaces, and expert insights on legal tech design. These findings offer actionable guidance for technology designers developing reading and writing-support for legal professionals, and for legal professionals seeking peer-informed tool integration strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24854v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chelse Swoopes, Ziwei Gu, Elena L. Glassman</dc:creator>
    </item>
    <item>
      <title>Color, Gender, and Bias: Examining the Role of Stereotyped Colors in Visualization-Driven Pay Decisions</title>
      <link>https://arxiv.org/abs/2509.24999</link>
      <description>arXiv:2509.24999v1 Announce Type: new 
Abstract: We investigate the impact of stereotyped gender-color associations in a visualization-driven decision-making task. In the context of gender data visualization, the well-known "pink for girls and blue for boys" color assignment is associated with stereotypes that could bias readers and decision-makers. Understanding the effects of using stereotyped colors in visualizations for decision-making can help designers better choose colors in stereotype-prone contexts. We therefore explore the potential impact of stereotyped colors on compensation decision-making through two crowdsourced experiments. In these experiments, we evaluate how the association of color with gender (stereotyped vs non-stereotyped) affects the user's allocation decisions in the context of salary adjustments. Our results indicate that explicit expression of the color-gender associations, in the form of a legend on the data visualization, leads to in-group favoritism. However, in the absence of a legend, this in-group favoritism disappears, and a small effect of non-stereotyped colors is observed. A free copy of this paper with all supplemental materials is available at https://osf.io/d4q3v/?view_only=22b636d6f7bb4a7991d9576933b3aaad</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24999v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florent Cabric, Margret Vilborg Bjarnadottir, Petra Isenberg</dc:creator>
    </item>
    <item>
      <title>Fairness for niche users and providers: algorithmic choice and profile portability</title>
      <link>https://arxiv.org/abs/2509.22660</link>
      <description>arXiv:2509.22660v1 Announce Type: cross 
Abstract: Ensuring fair outcomes for multiple stakeholders in recommender systems has been studied mostly in terms of algorithmic interventions: building new models with better fairness properties, or using reranking to improve outcomes from an existing algorithm. What has rarely been studied is structural changes in the recommendation ecosystem itself. Our work explores the fairness impact of algorithmic pluralism, the idea that the recommendation algorithm is decoupled from the platform through which users access content, enabling user choice in algorithms. Prior work using a simulation approach has shown that niche consumers and (especially) niche providers benefit from algorithmic choice. In this paper, we use simulation to explore the question of profile portability, to understand how different policies regarding the handling of user profiles interact with fairness outcomes for consumers and providers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22660v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elizabeth McKinnie, Anas Buhayh, Clement Canel, Robin Burke</dc:creator>
    </item>
    <item>
      <title>Security Friction Quotient for Zero Trust Identity Policy with Empirical Validation</title>
      <link>https://arxiv.org/abs/2509.22663</link>
      <description>arXiv:2509.22663v1 Announce Type: cross 
Abstract: We define a practical method to quantify the trade-off between security and operational friction in modern identity-centric programs. We introduce the Security Friction Quotient (SFQ), a bounded composite index that combines a residual-risk estimator with empirically grounded friction terms (latency, failure rate, and helpdesk impact). We establish clarity properties (boundedness, monotonic response, and weight identifiability) with short proofs, then evaluate widely used Conditional Access policies over a 12-week horizon using Monte Carlo simulation (n = 2,000 runs per policy/scenario) with effect sizes and 95% confidence intervals. We further assess rank stability under 10,000 random weight draws, finding 95.5% preservation of policy ordering. Finally, we provide a 12-week passkey field observation from an enterprise-scale cohort (N = 1,200) that directionally aligns with the simulation's phishing-resistant MFA gains. The SFQ framework is designed to be reproducible, interpretable, and directly actionable for Zero Trust identity policy decisions, with artifacts and parameter ranges provided to support policy design, review, and continuous improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22663v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Youssef</dc:creator>
    </item>
    <item>
      <title>Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs</title>
      <link>https://arxiv.org/abs/2509.22711</link>
      <description>arXiv:2509.22711v1 Announce Type: cross 
Abstract: Partisan bias in LLMs has been evaluated to assess political leanings, typically through a broad lens and largely in Western contexts. We move beyond identifying general leanings to examine harmful, adversarial representational associations around political leaders and parties. To do so, we create datasets \textit{NeutQA-440} (non-adversarial prompts) and \textit{AdverQA-440} (adversarial prompts), which probe models for comparative plausibility judgments across the USA and India. Results show high susceptibility to biased partisan associations and pronounced asymmetries (e.g., substantially more favorable associations for U.S. Democrats than Republicans) alongside mixed-polarity concentration around India's BJP, highlighting systemic risks and motivating standardized, cross-cultural evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22711v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divyanshu Kumar, Ishita Gupta, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi</dc:creator>
    </item>
    <item>
      <title>A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification</title>
      <link>https://arxiv.org/abs/2509.22725</link>
      <description>arXiv:2509.22725v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking "what kind of impact should LLMs have in education?" Drawing on Biesta's tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22725v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayu Huang (Minhui), Ruoxin Ritter Wang (Minhui), Jen-Hao Liu (Minhui), Boming Xia (Minhui), Yue Huang (Minhui), Ruoxi Sun (Minhui),  Jason (Minhui),  Xue, Jinan Zou</dc:creator>
    </item>
    <item>
      <title>Student Engagement with GenAI's Tutoring Feedback: A Mixed Methods Study</title>
      <link>https://arxiv.org/abs/2509.22974</link>
      <description>arXiv:2509.22974v1 Announce Type: cross 
Abstract: How students utilize immediate tutoring feedback in programming education depends on various factors. Among them are the feedback quality, but also students' engagement, i.e., their perception, interpretation, and use of feedback. However, there is limited research on how students engage with various types of tutoring feedback. For this reason, we developed a learning environment that provides students with Python programming tasks and various types of immediate, AI-generated tutoring feedback. The feedback is displayed within four components. Using a mixed-methods approach (think-aloud study and eye-tracking), we conducted a study with 20 undergraduate students enrolled in an introductory programming course. Our research aims to: (1) identify what students think when they engage with the tutoring feedback components, and (2) explore the relations between the tutoring feedback components, students' visual attention, verbalized thoughts, and their immediate actions as part of the problem-solving process. The analysis of students' thoughts while engaging with 380 feedback components revealed four main themes: students express understanding or disagreement, additional information needed, and students explicitly judge the feedback. Exploring the relations between feedback, students' attention, thoughts, and actions showed a clear relationship. While expressions of understanding were associated with improvements, expressions of disagreement or need for additional information prompted students to collect another feedback component rather than act on the current information. These insights into students' engagement and decision-making processes contribute to an increased understanding of tutoring feedback and how students engage with it. Thereby, this work has implications for tool developers and educators facilitating feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22974v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sven Jacobs, Jan Haas, Natalie Kiesler</dc:creator>
    </item>
    <item>
      <title>LiDAR-based Human Activity Recognition through Laplacian Spectral Analysis</title>
      <link>https://arxiv.org/abs/2509.23255</link>
      <description>arXiv:2509.23255v1 Announce Type: cross 
Abstract: Human Activity Recognition supports applications in healthcare, manufacturing, and human-machine interaction. LiDAR point clouds offer a privacy-preserving alternative to cameras and are robust to illumination. We propose a HAR method based on graph spectral analysis. Each LiDAR frame is mapped to a proximity graph (epsilon-graph) and the Laplacian spectrum is computed. Eigenvalues and statistics of eigenvectors form pose descriptors, and temporal statistics over sliding windows yield fixed vectors for classification with support vector machines and random forests. On the MM-Fi dataset with 40 subjects and 27 activities, under a strict subject-independent protocol, the method reaches 94.4% accuracy on a 13-class rehabilitation set and 90.3% on all 27 activities. It also surpasses the skeleton-based baselines reported for MM-Fi. The contribution is a compact and interpretable feature set derived directly from point cloud geometry that provides an accurate and efficient alternative to end-to-end deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23255v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sasan Sharifipour, Constantino \'Alvarez Casado, Le Nguyen, Tharindu Ekanayake, Manuel Lage Ca\~nellas, Nhi Nguyen, Miguel Bordallo L\'opez</dc:creator>
    </item>
    <item>
      <title>Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics</title>
      <link>https://arxiv.org/abs/2509.23297</link>
      <description>arXiv:2509.23297v1 Announce Type: cross 
Abstract: Software visualization seeks to represent software artifacts graphical-ly in two or three dimensions, with the goal of enhancing comprehension, anal-ysis, maintenance, and evolution of the source code. In this context, visualiza-tions employ graphical forms such as dependency structures, treemaps, or time-lines that incorporate repository histories. These visualizations allow software engineers to identify structural patterns, detect complexity hotspots, and infer system behaviors that are difficult to perceive directly from source text. By adopting metaphor-based approaches, visualization tools provide macroscopic overviews while enabling focused inspection of specific program elements, thus offering an accessible means of understanding large-scale systems. The contri-bution of our work lies in three areas. First, we introduce a configurable group-ing mechanism that supports flexible organization of code elements based on arbitrary relationships. Second, we combine fine-grained and coarse-grained software metrics to provide a multi-level perspective on system properties. Third, we present an interactive visualization engine that allows developers to dynamically adjust rendering attributes. Collectively, these advances provide a more adaptable and insightful approach to source code comprehension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23297v1</guid>
      <category>cs.SE</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Savidis, Christos Vasilopoulos</dc:creator>
    </item>
    <item>
      <title>Modeling and Exploiting the Time Course of Chromatic Adaptation for Display Power Optimizations in Virtual Reality</title>
      <link>https://arxiv.org/abs/2509.23489</link>
      <description>arXiv:2509.23489v1 Announce Type: cross 
Abstract: We introduce a gaze-tracking--free method to reduce OLED display power consumption in VR with minimal perceptual impact. This technique exploits the time course of chromatic adaptation, the human visual system's ability to maintain stable color perception under changing illumination. To that end, we propose a novel psychophysical paradigm that models how human adaptation state changes with the scene illuminant. We exploit this model to compute an optimal illuminant shift trajectory, controlling the rate and extent of illumination change, to reduce display power under a given perceptual loss budget. Our technique significantly improves the perceptual quality over prior work that applies illumination shifts instantaneously. Our technique can also be combined with prior work on luminance dimming to reduce display power by 31% with no statistical loss of perceptual quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23489v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763294</arxiv:DOI>
      <dc:creator>Ethan Chen, Sushant Kondguli, Carl Marshall, Yuhao Zhu</dc:creator>
    </item>
    <item>
      <title>Dynamic Trust Calibration Using Contextual Bandits</title>
      <link>https://arxiv.org/abs/2509.23497</link>
      <description>arXiv:2509.23497v1 Announce Type: cross 
Abstract: Trust calibration between humans and Artificial Intelligence (AI) is crucial for optimal decision-making in collaborative settings. Excessive trust can lead users to accept AI-generated outputs without question, overlooking critical flaws, while insufficient trust may result in disregarding valuable insights from AI systems, hindering performance. Despite its importance, there is currently no definitive and objective method for measuring trust calibration between humans and AI. Current approaches lack standardization and consistent metrics that can be broadly applied across various contexts, and they don't distinguish between the formation of opinions and subsequent human decisions. In this work, we propose a novel and objective method for dynamic trust calibration, introducing a standardized trust calibration measure and an indicator. By utilizing Contextual Bandits-an adaptive algorithm that incorporates context into decision-making-our indicator dynamically assesses when to trust AI contributions based on learned contextual information. We evaluate this indicator across three diverse datasets, demonstrating that effective trust calibration results in significant improvements in decision-making performance, as evidenced by 10 to 38% increase in reward metrics. These findings not only enhance theoretical understanding but also provide practical guidance for developing more trustworthy AI systems supporting decisions in critical domains, for example, disease diagnoses and criminal justice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23497v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno M. Henrique, Eugene Santos Jr</dc:creator>
    </item>
    <item>
      <title>Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations</title>
      <link>https://arxiv.org/abs/2509.24250</link>
      <description>arXiv:2509.24250v1 Announce Type: cross 
Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24250v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Kim, Daniel He, Jorge Chao, Wiktor Rajca, Mohammed Amin, Nishant Malpani, Ruta Desai, Antti Oulasvirta, Bjoern Hartmann, Sanjit Seshia</dc:creator>
    </item>
    <item>
      <title>LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research</title>
      <link>https://arxiv.org/abs/2509.24294</link>
      <description>arXiv:2509.24294v1 Announce Type: cross 
Abstract: Grounded theory offers deep insights from qualitative data, but its reliance on expert-intensive manual coding presents a major scalability bottleneck. Current computational tools stop short of true automation, keeping researchers firmly in the loop. We introduce LOGOS, a novel, end-to-end framework that fully automates the grounded theory workflow, transforming raw text into a structured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic clustering, graph reasoning, and a novel iterative refinement process to build highly reusable codebooks. To ensure fair comparison, we also introduce a principled 5-dimensional metric and a train-test split protocol for standardized, unbiased evaluation. Across five diverse corpora, LOGOS consistently outperforms strong baselines and achieves a remarkable $88.2\%$ alignment with an expert-developed schema on a complex dataset. LOGOS demonstrates a powerful new path to democratize and scale qualitative research without sacrificing theoretical nuance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24294v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyu Pi, Qisen Yang, Chuong Nguyen</dc:creator>
    </item>
    <item>
      <title>Experience Paper: Adopting Activity Recognition in On-demand Food Delivery Business</title>
      <link>https://arxiv.org/abs/2509.24303</link>
      <description>arXiv:2509.24303v1 Announce Type: cross 
Abstract: This paper presents the first nationwide deployment of human activity recognition (HAR) technology in the on-demand food delivery industry. We successfully adapted the state-of-the-art LIMU-BERT foundation model to the delivery platform. Spanning three phases over two years, the deployment progresses from a feasibility study in Yangzhou City to nationwide adoption involving 500,000 couriers across 367 cities in China. The adoption enables a series of downstream applications, and large-scale tests demonstrate its significant operational and economic benefits, showcasing the transformative potential of HAR technology in real-world applications. Additionally, we share lessons learned from this deployment and open-source our LIMU-BERT pretrained with millions of hours of sensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24303v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680207.3765261</arxiv:DOI>
      <dc:creator>Huatao Xu, Yan Zhang, Wei Gao, Guobin Shen, Mo Li</dc:creator>
    </item>
    <item>
      <title>UI-UG: A Unified MLLM for UI Understanding and Generation</title>
      <link>https://arxiv.org/abs/2509.24361</link>
      <description>arXiv:2509.24361v1 Announce Type: cross 
Abstract: Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24361v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Yang, Weijie Qiu, Ru Zhang, Zhou Fang, Ruichao Mao, Xiaoyu Lin, Maji Huang, Zhaosong Huang, Teng Guo, Shuoyang Liu, Hai Rao</dc:creator>
    </item>
    <item>
      <title>DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability</title>
      <link>https://arxiv.org/abs/2509.24413</link>
      <description>arXiv:2509.24413v1 Announce Type: cross 
Abstract: The emergence of large pre-trained models based on natural language has breathed new life into robotics development. Extensive research has integrated large models with robots, utilizing the powerful semantic understanding and generation capabilities of large models to facilitate robot control through natural language instructions gradually. However, we found that robots that strictly adhere to human instructions, especially those containing misleading information, may encounter errors during task execution, potentially leading to safety hazards. This resembles the concept of counterfactuals in natural language processing (NLP), which has not yet attracted much attention in robotic research. In an effort to highlight this issue for future studies, this paper introduced directive counterfactuals (DCFs) arising from misleading human directives. We present DynaMIC, a framework for generating robot task flows to identify DCFs and relay feedback to humans proactively. This capability can help robots be sensitive to potential DCFs within a task, thus enhancing the reliability of the execution process. We conducted semantic-level experiments and ablation studies, showcasing the effectiveness of this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24413v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianqiang Yan, Ziqiao Lin, Sicheng Wang, Tianwei Zhang, Zhenglong Sun</dc:creator>
    </item>
    <item>
      <title>Position: Towards Bidirectional Human-AI Alignment</title>
      <link>https://arxiv.org/abs/2406.09264</link>
      <description>arXiv:2406.09264v4 Announce Type: replace 
Abstract: Recent advances in general-purpose AI underscore the urgent need to align AI systems with human goals and values. Yet, the lack of a clear, shared understanding of what constitutes "alignment" limits meaningful progress and cross-disciplinary collaboration. In this position paper, we argue that the research community should explicitly define and critically reflect on "alignment" to account for the bidirectional and dynamic relationship between humans and AI. Through a systematic review of over 400 papers spanning HCI, NLP, ML, and more, we examine how alignment is currently defined and operationalized. Building on this analysis, we introduce the Bidirectional Human-AI Alignment framework, which not only incorporates traditional efforts to align AI with human values but also introduces the critical, underexplored dimension of aligning humans with AI -- supporting cognitive, behavioral, and societal adaptation to rapidly advancing AI technologies. Our findings reveal significant gaps in current literature, especially in long-term interaction design, human value modeling, and mutual understanding. We conclude with three central challenges and actionable recommendations to guide future research toward more nuanced, reciprocal, and human-AI alignment approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09264v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith Ringel Morris, Paul Resnick, David Jurgens</dc:creator>
    </item>
    <item>
      <title>Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?</title>
      <link>https://arxiv.org/abs/2501.15463</link>
      <description>arXiv:2501.15463v3 Announce Type: replace 
Abstract: Existing research primarily evaluates the values of LLMs by examining their stated inclinations towards specific values. However, the "Value-Action Gap," a phenomenon rooted in environmental and social psychology, reveals discrepancies between individuals' stated values and their actions in real-world contexts. To what extent do LLMs exhibit a similar gap between their stated values and their actions informed by those values? This study introduces ValueActionLens, an evaluation framework to assess the alignment between LLMs' stated values and their value-informed actions. The framework encompasses the generation of a dataset comprising 14.8k value-informed actions across twelve cultures and eleven social topics, and two tasks to evaluate how well LLMs' stated value inclinations and value-informed actions align across three different alignment measures. Extensive experiments reveal that the alignment between LLMs' stated values and actions is sub-optimal, varying significantly across scenarios and models. Analysis of misaligned results identifies potential harms from certain value-action gaps. To predict the value-action gaps, we also uncover that leveraging reasoned explanations improves performance. These findings underscore the risks of relying solely on the LLMs' stated values to predict their behaviors and emphasize the importance of context-aware evaluations of LLM values and value-action gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15463v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hua Shen, Nicholas Clark, Tanushree Mitra</dc:creator>
    </item>
    <item>
      <title>A Design Space to Characterize Creativity in Visualization Design</title>
      <link>https://arxiv.org/abs/2504.02204</link>
      <description>arXiv:2504.02204v2 Announce Type: replace 
Abstract: Characterizing creativity in visualization design can lead to the design of more expressive representations and visualization authoring tools that prioritize human creativity. In this paper, we examine how creativity manifests itself in visualization design processes. Through a systematic review of 58 papers, we develop a design space to characterize creativity in visualization design. Our findings show that the prior literature predominantly used atypical designs through free-form drawings, infographics, pictorials, and data comics to define creative representations. However, creativity in visualization extends beyond visual representations to encompass design activities such as sketching, storyboarding, and discussion, and supporting other creative tasks (e.g., fiction writing). We discuss the implications of these findings for fostering innovation within established design paradigms and for developing more sophisticated visualization authoring systems. The full list of coded papers is available here: https://vizcreativity.notion.site/coded-papers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02204v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naimul Hoque, Zinat Ara, Safwat Ali Khan, Fanny Chevalier, Niklas Elmqvist</dc:creator>
    </item>
    <item>
      <title>Errors in Stereo Geometry Induce Distance Misperception</title>
      <link>https://arxiv.org/abs/2505.23685</link>
      <description>arXiv:2505.23685v2 Announce Type: replace 
Abstract: Stereoscopic head-mounted displays (HMDs) render and present binocular images to create an egocentric, 3D percept to the HMD user. Within this render and presentation pipeline there are potential rendering camera and viewing position errors that can induce deviations in the depth and distance that a user perceives compared to the underlying intended geometry. For example, rendering errors can arise when HMD render cameras are incorrectly positioned relative to the assumed centers of projections of the HMD displays and viewing errors can arise when users view stereo geometry from the incorrect location in the HMD eyebox. In this work we present a geometric framework that predicts errors in distance perception arising from inaccurate HMD perspective geometry and build an HMD platform to reliably simulate render and viewing error in a Quest 3 HMD with eye tracking to experimentally test these predictions. We present a series of five experiments to explore the efficacy of this geometric framework and show that errors in perspective geometry can induce both under- and over-estimations in perceived distance. We further demonstrate how real-time visual feedback can be used to dynamically recalibrate visuomotor mapping so that an accurate reach distance is achieved even if the perceived visual distance is negatively impacted by geometric error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23685v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raffles Xingqi Zhu, Charlie S. Burlingham, Olivier Mercier, Phillip Guan</dc:creator>
    </item>
    <item>
      <title>StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework</title>
      <link>https://arxiv.org/abs/2506.14159</link>
      <description>arXiv:2506.14159v2 Announce Type: replace 
Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14159v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shayan Talaei, Meijin Li, Kanu Grover, James Kent Hippler, Diyi Yang, Amin Saberi</dc:creator>
    </item>
    <item>
      <title>Designing for Disclosure in Data Visualizations</title>
      <link>https://arxiv.org/abs/2508.08383</link>
      <description>arXiv:2508.08383v2 Announce Type: replace 
Abstract: Visualizing data often entails data transformations that can reveal and hide information, operations we dub disclosure tactics. Whether designers hide information intentionally or as an implicit consequence of other design choices, tools and frameworks for visualization offer little explicit guidance on disclosure. To systematically characterize how visualizations can limit access to an underlying dataset, we contribute a content analysis of 425 examples of visualization techniques sampled from academic papers in the visualization literature, resulting in a taxonomy of disclosure tactics. Our taxonomy organizes disclosure tactics based on how they change the data representation underlying a chart, providing a systematic way to reason about design trade-offs in terms of what information is revealed, distorted, or hidden. We demonstrate the benefits of using our taxonomy by showing how it can guide reasoning in design scenarios where disclosure is a first-order consideration. Adopting disclosure as a framework for visualization research offers new perspective on authoring tools, literacy, uncertainty communication, personalization, and ethical design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08383v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krisha Mehta, Gordon Kindlmann, Alex Kale</dc:creator>
    </item>
    <item>
      <title>Understanding the Effects of Miscalibrated AI Confidence on User Trust, Reliance, and Decision Efficacy</title>
      <link>https://arxiv.org/abs/2402.07632</link>
      <description>arXiv:2402.07632v4 Announce Type: replace-cross 
Abstract: Providing well-calibrated AI confidence can help promote users' appropriate trust in and reliance on AI, which are essential for AI-assisted decision-making. However, calibrating AI confidence -- providing confidence score that accurately reflects the true likelihood of AI being correct -- is known to be challenging. To understand the effects of AI confidence miscalibration, we conducted our first experiment. The results indicate that miscalibrated AI confidence impairs users' appropriate reliance and reduces AI-assisted decision-making efficacy, and AI miscalibration is difficult for users to detect. Then, in our second experiment, we examined whether communicating AI confidence calibration levels could mitigate the above issues. We find that it helps users to detect AI miscalibration. Nevertheless, since such communication decreases users' trust in uncalibrated AI, leading to high under-reliance, it does not improve the decision efficacy. We discuss design implications based on these findings and future directions to address risks and ethical concerns associated with AI miscalibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07632v4</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingshu Li, Yitian Yang, Renwen Zhang, Q. Vera Liao, Tianqi Song, Zhengtao Xu, Yi-chieh Lee</dc:creator>
    </item>
    <item>
      <title>Attentive Dilated Convolution for Automatic Sleep Staging using Force-directed Layout</title>
      <link>https://arxiv.org/abs/2409.01962</link>
      <description>arXiv:2409.01962v2 Announce Type: replace-cross 
Abstract: Sleep stages play an important role in identifying sleep patterns and diagnosing sleep disorders. In this study, we present an automated sleep stage classifier called the Attentive Dilated Convolutional Neural Network (AttDiCNN), which uses deep learning methodologies to address challenges related to data heterogeneity, computational complexity, and reliable and automatic sleep staging. We employed a force-directed layout based on the visibility graph to capture the most significant information from the EEG signals, thereby representing the spatial-temporal features. The proposed network consists of three modules: the Localized Spatial Feature Extraction Network (LSFE), Spatio-Temporal-Temporal Long Retention Network (S2TLR), and Global Averaging Attention Network (G2A). The LSFE captures spatial information from sleep data, the S2TLR is designed to extract the most pertinent information in long-term contexts, and the G2A reduces computational overhead by aggregating information from the LSFE and S2TLR. We evaluated the performance of our model on three comprehensive and publicly accessible datasets, achieving state-of-the-art accuracies of 98.56%, 99.66%, and 99.08% for the EDFX, HMC, and NCH datasets, respectively, while maintaining a low computational complexity with 1.4 M parameters. Our proposed architecture surpasses existing methodologies in several performance metrics, thereby proving its potential as an automated tool for clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01962v2</guid>
      <category>eess.SP</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Jobayer, Md Mehedi Hasan Shawon, Tasfin Mahmud, Md. Borhan Uddin Antor, Arshad M. Chowdhury</dc:creator>
    </item>
    <item>
      <title>M6(GPT)3: Generating Multitrack Modifiable Multi-Minute MIDI Music from Text using Genetic algorithms, Probabilistic methods and GPT Models in any Progression and Time Signature</title>
      <link>https://arxiv.org/abs/2409.12638</link>
      <description>arXiv:2409.12638v3 Announce Type: replace-cross 
Abstract: This work introduces the M6(GPT)3 composer system, capable of generating complete, multi-minute musical compositions with complex structures in any time signature, in the MIDI domain from input descriptions in natural language. The system utilizes an autoregressive transformer language model to map natural language prompts to composition parameters in JSON format. The defined structure includes time signature, scales, chord progressions, and valence-arousal values, from which accompaniment, melody, bass, motif, and percussion tracks are created. We propose a genetic algorithm for the generation of melodic elements. The algorithm incorporates mutations with musical significance and a fitness function based on normal distribution and predefined musical feature values. The values adaptively evolve, influenced by emotional parameters and distinct playing styles. The system for generating percussion in any time signature utilises probabilistic methods, including Markov chains. Through both human and objective evaluations, we demonstrate that our music generation approach outperforms baselines on specific, musically meaningful metrics, offering a viable alternative to purely neural network-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12638v3</guid>
      <category>cs.SD</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICMEW68306.2025.11152218</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), Nantes, France, 2025, pp. 1-6</arxiv:journal_reference>
      <dc:creator>Jakub Po\'cwiardowski, Mateusz Modrzejewski, Marek S. Tatara</dc:creator>
    </item>
    <item>
      <title>GUI Agents: A Survey</title>
      <link>https://arxiv.org/abs/2412.13501</link>
      <description>arXiv:2412.13501v3 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13501v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, Seunghyun Yoon, Lina Yao, Branislav Kveton, Jihyung Kil, Thien Huu Nguyen, Trung Bui, Tianyi Zhou, Ryan A. Rossi, Franck Dernoncourt</dc:creator>
    </item>
    <item>
      <title>Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2412.15701</link>
      <description>arXiv:2412.15701v3 Announce Type: replace-cross 
Abstract: Recent advancements in language models (LMs) have sparked growing interest in developing LM agents. While fully autonomous agents could excel in many scenarios, numerous use cases inherently require them to collaborate with humans due to humans' latent preferences, domain expertise, or need for control. To facilitate the study of human-agent collaboration, we present Collaborative Gym (Co-Gym), a general framework enabling asynchronous, tripartite interaction among agents, humans, and task environments. We instantiate Co-Gym with three representative tasks in both simulated and real-world conditions, and propose an evaluation framework that assesses both the collaboration outcomes and processes. Our findings reveal that collaborative agents consistently outperform their fully autonomous counterparts in task performance within those delivered cases, achieving win rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related Work when evaluated by real users. However, our study also highlights significant challenges in developing collaborative agents, requiring advancements in core aspects of intelligence -- communication capabilities, situational awareness, and balancing autonomy and human control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15701v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, Diyi Yang</dc:creator>
    </item>
    <item>
      <title>Nirvana AI Governance: How AI Policymaking Is Committing Three Old Fallacies</title>
      <link>https://arxiv.org/abs/2501.10384</link>
      <description>arXiv:2501.10384v2 Announce Type: replace-cross 
Abstract: This research applies Harold Demsetz's concept of the nirvana approach to the realm of AI governance and debunks three common fallacies in various AI policy proposals--"the grass is always greener on the other side," "free lunch," and "the people could be different." Through this, I expose fundamental flaws in the current AI regulatory proposal. First, some commentators intuitively believe that people are more reliable than machines and that government works better in risk control than companies' self-regulation, but they do not fully compare the differences between the status quo and the proposed replacements. Second, when proposing some regulatory tools, some policymakers and researchers do not realize and even gloss over the fact that harms and costs are also inherent in their proposals. Third, some policy proposals are initiated based on a false comparison between the AI-driven world, where AI does lead to some risks, and an entirely idealized world, where no risk exists at all. However, the appropriate approach is to compare the world where AI causes risks to the real world where risks are everywhere, but people can live well with these risks. The prevalence of these fallacies in AI governance underscores a broader issue: the tendency to idealize potential solutions without fully considering their real-world implications. This idealization can lead to regulatory proposals that are not only impractical but potentially harmful to innovation and societal progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10384v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Zhang</dc:creator>
    </item>
    <item>
      <title>HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring</title>
      <link>https://arxiv.org/abs/2509.07260</link>
      <description>arXiv:2509.07260v3 Announce Type: replace-cross 
Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07260v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Wang, Ting Dang, Xinyu Zhang, Vassilis Kostakos, Michael J. Witbrock, Hong Jia</dc:creator>
    </item>
    <item>
      <title>Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media</title>
      <link>https://arxiv.org/abs/2509.16811</link>
      <description>arXiv:2509.16811v2 Announce Type: replace-cross 
Abstract: Creators struggle to edit long-form, narrative-rich videos not because of UI complexity, but due to the cognitive demands of searching, storyboarding, and sequencing hours of footage. Existing transcript- or embedding-based methods fall short for creative workflows, as models struggle to track characters, infer motivations, and connect dispersed events. We present a prompt-driven, modular editing system that helps creators restructure multi-hour content through free-form prompts rather than timelines. At its core is a semantic indexing pipeline that builds a global narrative via temporal segmentation, guided memory compression, and cross-granularity fusion, producing interpretable traces of plot, dialogue, emotion, and context. Users receive cinematic edits while optionally refining transparent intermediate outputs. Evaluated on 400+ videos with expert ratings, QA, and preference studies, our system scales prompt-driven editing, preserves narrative coherence, and balances automation with creator control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16811v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Ding, Xinyi Wang, Junlong Chen, Per Ola Kristensson, Junxiao Shen</dc:creator>
    </item>
  </channel>
</rss>
