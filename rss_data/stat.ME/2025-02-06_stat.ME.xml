<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Feb 2025 02:44:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Confidence intervals for functionals in constrained inverse problems via data-adaptive sampling-based calibration</title>
      <link>https://arxiv.org/abs/2502.02674</link>
      <description>arXiv:2502.02674v1 Announce Type: new 
Abstract: We address functional uncertainty quantification for ill-posed inverse problems where it is possible to evaluate a possibly rank-deficient forward model, the observation noise distribution is known, and there are known parameter constraints. We present four constraint-aware confidence intervals extending the work of Batlle et al. (2023) by making the intervals both computationally feasible and less conservative. Our approach first shrinks the potentially unbounded constraint set compact in a data-adaptive way, obtains samples of the relevant test statistic inside this set to estimate a quantile function, and then uses these computed quantities to produce the intervals. Our data-adaptive bounding approach is based on the approach by Berger and Boos (1994), and involves defining a subset of the constraint set where the true parameter exists with high probability. This probabilistic guarantee is then incorporated into the final coverage guarantee in the form of an uncertainty budget. We then propose custom sampling algorithms to efficiently sample from this subset, even when the parameter space is high-dimensional. Optimization-based interval methods formulate confidence interval computation as two endpoint optimizations, where the optimization constraints can be set to achieve different types of interval calibration while seamlessly incorporating parameter constraints. However, choosing valid optimization constraints has been elusive. We show that all four proposed intervals achieve nominal coverage for a particular functional both theoretically and in practice, with numerical examples demonstrating superior performance of our intervals over the OSB interval in terms of both coverage and expected length. In particular, we show the superior performance in a realistic unfolding simulation from high-energy physics that is severely ill-posed and involves a rank-deficient forward model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02674v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Stanley, Pau Batlle, Pratik Patil, Houman Owhadi, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Estimating Optimal Dynamic Treatment Regimes Using Irregularly Observed Data: A Target Trial Emulation and Bayesian Joint Modeling Approach</title>
      <link>https://arxiv.org/abs/2502.02736</link>
      <description>arXiv:2502.02736v1 Announce Type: new 
Abstract: An optimal dynamic treatment regime (DTR) is a sequence of decision rules aimed at providing the best course of treatments individualized to patients. While conventional DTR estimation uses longitudinal data, such data can also be irregular, where patient-level variables can affect visit times, treatment assignments and outcomes. In this work, we first extend the target trial framework - a paradigm to estimate statistical estimands specified under hypothetical randomized trials using observational data - to the DTR context; this extension allows treatment regimes to be defined with intervenable visit times. We propose an adapted version of G-computation marginalizing over random effects for rewards that encapsulate a treatment strategy's value. To estimate components of the G-computation formula, we then articulate a Bayesian joint model to handle correlated random effects between the outcome, visit and treatment processes. We show via simulation studies that, in the estimation of regime rewards, failure to account for the observational treatment and visit processes produces bias which can be removed through joint modeling. We also apply our proposed method on data from INSPIRE 2 and 3 studies to estimate optimal injection cycles of Interleukin 7 to treat HIV-infected individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02736v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Larry Dong, Eleanor Pullenayegum, Rodolphe Thi\'ebaut, Olli Saarela</dc:creator>
    </item>
    <item>
      <title>Sufficient dimension reduction for regression with spatially correlated errors: application to prediction</title>
      <link>https://arxiv.org/abs/2502.02781</link>
      <description>arXiv:2502.02781v1 Announce Type: new 
Abstract: In this paper, we address the problem of predicting a response variable in the context of both, spatially correlated and high-dimensional data. To reduce the dimensionality of the predictor variables, we apply the sufficient dimension reduction (SDR) paradigm, which reduces the predictor space while retaining relevant information about the response. To achieve this, we impose two different spatial models on the inverse regression: the separable spatial covariance model (SSCM) and the spatial autoregressive error model (SEM). For these models, we derive maximum likelihood estimators for the reduction and use them to predict the response via nonparametric rules for forward regression. Through simulations and real data applications, we demonstrate the effectiveness of our approach for spatial data prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02781v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liliana Forzani, Rodrigo Garc\'ia Arancibia, Antonella Gieco, Pamela Llop, Anne Yao</dc:creator>
    </item>
    <item>
      <title>Don't Let Your Likert Scales Grow Up To Be Visual Analog Scales: Understanding the Relationship Between Number of Response Categories and Measurement Error</title>
      <link>https://arxiv.org/abs/2502.02846</link>
      <description>arXiv:2502.02846v1 Announce Type: new 
Abstract: The use of Visual Analog Scales (VAS), which can be broadly conceptualized as items where the response scale is 0-100, has surged recently due to the convenience of digital assessments. However, there is no consensus as to whether the use of VAS scales is optimal in a measurement sense. Put differently, in the 90+ years since Likert introduced his eponymous scale, the field does not know how to determine the optimal number of response options for a given item. In the current work, we investigate the optimal number of response categories using a series of simulations. We find that when the measurement error of an item is not dependent on the number of response categories, there is no true optimum; rather, reliability increases with number of response options and then plateaus. However, under the more realistic assumption that the measurement error of an item increases with the number of response categories, we find a clear optimum that depends on the rate of that increase. If measurement error increases with the number of response categories, then conversion of any Likert scale item to VAS will result in a drastic decrease in reliability. Finally, if researchers do want to change the response scale of a validated measure, they must re-validate the new measure as the measurement error of the scale is likely to change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02846v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siqi Sun, Karen M. Schmidt, Teague R. Henry</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis for multivariable missing data using multiple imputation: a tutorial</title>
      <link>https://arxiv.org/abs/2502.02892</link>
      <description>arXiv:2502.02892v1 Announce Type: new 
Abstract: Multiple imputation is a popular method for handling missing data, with fully conditional specification (FCS) being one of the predominant imputation approaches for multivariable missingness. Unbiased estimation with standard implementations of multiple imputation depends on assumptions concerning the missingness mechanism (e.g. that data are "missing at random"). The plausibility of these assumptions can only be assessed using subject-matter knowledge, and not data alone. It is therefore important to perform sensitivity analyses to explore the robustness of results to violations of these assumptions (e.g. if the data are in fact "missing not at random"). In this tutorial, we provide a roadmap for conducting sensitivity analysis using the Not at Random Fully Conditional Specification (NARFCS) procedure for multivariate imputation. Using a case study from the Longitudinal Study of Australian Children, we work through the steps involved, from assessing the need to perform the sensitivity analysis, and specifying the NARFCS models and sensitivity parameters, through to implementing NARFCS using FCS procedures in R and Stata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02892v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cattram D Nguyen, Katherine J Lee, Ian R White, Stef van Buuren, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Bayesian Power and Sample Size Calculations for Bayes Factors in the Binomial Setting</title>
      <link>https://arxiv.org/abs/2502.02914</link>
      <description>arXiv:2502.02914v1 Announce Type: new 
Abstract: Bayesian design of experiments and sample size calculations usually rely on complex Monte Carlo simulations in practice. Obtaining bounds on Bayesian notions of the false-positive rate and power therefore often lack closed-form or approximate numerical solutions. In this paper, we focus on the sample size calculation in the binomial setting via Bayes factors, the predictive updating factor from prior to posterior odds. We discuss the drawbacks of sample size calculations via Monte Carlo simulations and propose a numerical root-finding approach which allows to determine the necessary sample size to obtain prespecified bounds of Bayesian power and type-I-error rate almost instantaneously. Real-world examples and applications in clinical trials illustrate the advantage of the proposed method. We focus on point-null versus composite and directional hypothesis tests, derive the corresponding Bayes factors, and discuss relevant aspects to consider when pursuing Bayesian design of experiments with the introduced approach. In summary, our approach allows for a Bayes-frequentist compromise by providing a Bayesian analogue to a frequentist power analysis for the Bayes factor in binomial settings. A case study from a Phase II trial illustrates the utility of our approach. The methods are implemented in our R package bfpwr.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02914v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Riko Kelter, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Data denoising with self consistency, variance maximization, and the Kantorovich dominance</title>
      <link>https://arxiv.org/abs/2502.02925</link>
      <description>arXiv:2502.02925v1 Announce Type: new 
Abstract: We introduce a new framework for data denoising, partially inspired by martingale optimal transport. For a given noisy distribution (the data), our approach involves finding the closest distribution to it among all distributions which 1) have a particular prescribed structure (expressed by requiring they lie in a particular domain), and 2) are self-consistent with the data. We show that this amounts to maximizing the variance among measures in the domain which are dominated in convex order by the data. For particular choices of the domain, this problem and a relaxed version of it, in which the self-consistency condition is removed, are intimately related to various classical approaches to denoising. We prove that our general problem has certain desirable features: solutions exist under mild assumptions, have certain robustness properties, and, for very simple domains, coincide with solutions to the relaxed problem.
  We also introduce a novel relationship between distributions, termed Kantorovich dominance, which retains certain aspects of the convex order while being a weaker, more robust, and easier-to-verify condition. Building on this, we propose and analyze a new denoising problem by substituting the convex order in the previously described framework with Kantorovich dominance. We demonstrate that this revised problem shares some characteristics with the full convex order problem but offers enhanced stability, greater computational efficiency, and, in specific domains, more meaningful solutions. Finally, we present simple numerical examples illustrating solutions for both the full convex order problem and the Kantorovich dominance problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02925v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Zoen-Git Hiew, Tongseok Lim, Brendan Pass, Marcelo Cruz de Souza</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of Unit-Weibull distribution based on dual generalized order statistics with application to the Cotton Production Data</title>
      <link>https://arxiv.org/abs/2502.02927</link>
      <description>arXiv:2502.02927v1 Announce Type: new 
Abstract: The Unit Weibull distribution with parameters $\alpha$ and $\beta$ is considered to study in the context of dual generalized order statistics. For the analysis purpose, Bayes estimators based on symmetric and asymmetric loss functions are obtained. The methods which are utilized for Bayesian estimation are approximation and simulation tools such as Lindley, Tierney-Kadane and Markov chain Monte Carlo methods. The authors have considered squared error loss function as symmetric and LINEX and general entropy loss function as asymmetric loss functions. After presenting the mathematical results, a simulation study is conducted to exhibit the performances of various derived estimators. As this study is considered for the dual generalized order statistics that is unification of models based distinct ordered random variable such as order statistics, record values, etc. This provides flexibility in our results and in continuation of this, the cotton production data of USA is analyzed for both submodels of ordered random variables: order statistics and record values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02927v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qazi J. Azhad, Abdul Nasir Khan, Bhagwati Devi, Jahangir Sabbir Khan, Ayush Tripathi</dc:creator>
    </item>
    <item>
      <title>Rank-Based Identification of High-dimensional Surrogate Markers: Application to Vaccinology</title>
      <link>https://arxiv.org/abs/2502.03030</link>
      <description>arXiv:2502.03030v1 Announce Type: new 
Abstract: In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies like RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE employs a non-parametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose post-vaccination expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes whose combined expression at 1 day post-injection appears to be a reasonable surrogate for the neutralising antibody titres at 28 days after vaccination. Pathways related to innate antiviral signalling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03030v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Hughes, Layla Parast, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>Adding covariates to bounds: What is the question?</title>
      <link>https://arxiv.org/abs/2502.03156</link>
      <description>arXiv:2502.03156v1 Announce Type: new 
Abstract: Symbolic nonparametric bounds for partial identification of causal effects now have a long history in the causal literature. Sharp bounds, bounds that use all available information to make the range of values as narrow as possible, are often the goal. For this reason, many publications have focused on deriving sharp bounds, but the concept of sharp bounds is nuanced and can be misleading. In settings with ancillary covariates, the situation becomes more complex. We provide clear definitions for pointwise and uniform sharpness of covariate-conditional bounds, that we then use to prove some general and some specific to the IV setting results about the relationship between these two concepts. As we demonstrate, general conditions are much more difficult to determine and thus, we urge authors to be clear when including ancillary covariates in bounds via conditioning about the setting of interest and the assumptions made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03156v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustav Jonzon, Erin E Gabriel, Arvid Sj\"olander, Michael C Sachs</dc:creator>
    </item>
    <item>
      <title>New technique for parameter estimation and improved fits to experimental data for a set of compound Poisson distributions</title>
      <link>https://arxiv.org/abs/2502.03237</link>
      <description>arXiv:2502.03237v1 Announce Type: new 
Abstract: Compound Poisson distributions have been employed by many authors to fit experimental data, typically via the method of moments or maximum likelihood estimation. We propose a new technique and apply it to several sets of published data. It yields better fits than those obtained by the original authors for a set of widely employed compound Poisson distributions (in some cases, significantly better). The technique employs the power spectrum (the absolute square of the characteristic function). The new idea is suggested as a useful addition to the tools for parameter estimation of compound Poisson distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03237v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. R. Mane</dc:creator>
    </item>
    <item>
      <title>Optimal design of experiments with quantitative-sequence factors</title>
      <link>https://arxiv.org/abs/2502.03241</link>
      <description>arXiv:2502.03241v1 Announce Type: new 
Abstract: A new type of experiment with joint considerations of quantitative and sequence factors is recently drawing much attention in medical science, bio-engineering, and many other disciplines. The input spaces of such experiments are semi-discrete and often very large. Thus, efficient and economical experimental designs are required. Based on the transformations and aggregations of good lattice point sets, we construct a new class of optimal quantitative-sequence (QS) designs that are marginally coupled, pair-balanced, space-filling, and asymptotically orthogonal. The proposed QS designs have a certain flexibility in run and factor sizes and are especially appealing for high-dimensional cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03241v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1360/SCM-2024-0039</arxiv:DOI>
      <arxiv:journal_reference>SCIENCE CHINA Mathematics, 2025, 55: 1-24 (in Chinese)</arxiv:journal_reference>
      <dc:creator>Yaping Wang, Sixu Liu, Qian Xiao</dc:creator>
    </item>
    <item>
      <title>Bayesian Covariate-Dependent Circadian Modeling of Rest-Activity Rhythms</title>
      <link>https://arxiv.org/abs/2502.03273</link>
      <description>arXiv:2502.03273v1 Announce Type: new 
Abstract: We propose a Bayesian covariate-dependent anti-logistic circadian model for analyzing activity data collected via wrist-worn wearable devices. The proposed approach integrates covariates into the modeling of the amplitude and phase parameters, facilitating cohort-level analysis with enhanced flexibility and interpretability. To promote model sparsity, we employ an l_1-ball projection prior, enabling precise control over complexity while identifying significant predictors. We assess performances on simulated data and then apply the method to real-world actigraphy data from people with epilepsy. Our results demonstrate the model's effectiveness in uncovering complex relationships among demographic, psychological, and medical factors influencing rest-activity rhythms, offering insights for personalized clinical assessments and healthcare interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03273v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beniamino Hadj-Amar, Vaishnav Krishnan, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Posterior SBC: Simulation-Based Calibration Checking Conditional on Data</title>
      <link>https://arxiv.org/abs/2502.03279</link>
      <description>arXiv:2502.03279v1 Announce Type: new 
Abstract: Simulation-based calibration checking (SBC) refers to the validation of an inference algorithm and model implementation through repeated inference on data simulated from a generative model. In the original and commonly used approach, the generative model uses parameters drawn from the prior, and thus the approach is testing whether the inference works for simulated data generated with parameter values plausible under that prior. This approach is natural and desirable when we want to test whether the inference works for a wide range of datasets we might observe. However, after observing data, we are interested in answering whether the inference works conditional on that particular data. In this paper, we propose posterior SBC and demonstrate how it can be used to validate the inference conditionally on observed data. We illustrate the utility of posterior SBC in three case studies: (1) A simple multilevel model; (2) a model that is governed by differential equations; and (3) a joint integrative neuroscience model which is approximated via amortized Bayesian inference with neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03279v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teemu S\"ailynoja, Marvin Schmitt, Paul B\"urkner, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Dealing with multiple intercurrent events using hypothetical and treatment policy strategies simultaneously</title>
      <link>https://arxiv.org/abs/2502.03329</link>
      <description>arXiv:2502.03329v1 Announce Type: new 
Abstract: To precisely define the treatment effect of interest in a clinical trial, the ICH E9 estimand addendum describes that relevant so-called intercurrent events should be identified and strategies specified to deal with them. Handling intercurrent events with different strategies leads to different estimands. In this paper, we focus on estimands that involve addressing one intercurrent event with the treatment policy strategy and another with the hypothetical strategy. We define these estimands using potential outcomes and causal diagrams, considering the possible causal relationships between the two intercurrent events and other variables. We show that there are different causal estimand definitions and assumptions one could adopt, each having different implications for estimation, which is demonstrated in a simulation study. The different considerations are illustrated conceptually using a diabetes trial as an example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03329v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camila Olarte Parra, Rhian M. Daniel, Jonathan W. Bartlett</dc:creator>
    </item>
    <item>
      <title>Estimating causal effects using difference-in-differences under network dependency and interference</title>
      <link>https://arxiv.org/abs/2502.03414</link>
      <description>arXiv:2502.03414v1 Announce Type: new 
Abstract: Differences-in-differences (DiD) is a causal inference method for observational longitudinal data that assumes parallel expected outcome trajectories between treatment groups under the (possible) counterfactual of receiving a specific treatment. In this paper DiD is extended to allow for (i) network dependency where outcomes, treatments, and covariates may exhibit between-unit latent correlation, and (ii) interference, where treatments can affect outcomes in neighboring units. In this setting, the causal estimand of interest is the average exposure effect among units with a specific exposure level, where the exposure is a function of treatments from potentially many units. Under a conditional parallel trends assumption and suitable network dependency conditions, a doubly robust estimator allowing for data-adaptive nuisance function estimation is proposed and shown to be consistent and asymptotically normal with variance reaching the semiparametric efficiency bound. The proposed methods are evaluated in simulations and applied to study the effects of adopting emission control technologies in coal power plants on county-level mortality due to cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03414v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jetsupphasuk, Didong Li, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Practically Effective Adjustment Variable Selection in Causal Inference</title>
      <link>https://arxiv.org/abs/2502.02701</link>
      <description>arXiv:2502.02701v1 Announce Type: cross 
Abstract: In the estimation of causal effects, one common method for removing the influence of confounders is to adjust the variables that satisfy the back-door criterion. However, it is not always possible to uniquely determine sets of such variables. Moreover, real-world data is almost always limited, which means it may be insufficient for statistical estimation. Therefore, we propose criteria for selecting variables from a list of candidate adjustment variables along with an algorithm to prevent accuracy degradation in causal effect estimation. We initially focus on directed acyclic graphs (DAGs) and then outlines specific steps for applying this method to completed partially directed acyclic graphs (CPDAGs). We also present and prove a theorem on causal effect computation possibility in CPDAGs. Finally, we demonstrate the practical utility of our method using both existing and artificial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02701v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-072X/ada861</arxiv:DOI>
      <arxiv:journal_reference>Journal of Physics: Complexity 6, 015001 (2025)</arxiv:journal_reference>
      <dc:creator>Atsushi Noda, Takashi Isozaki</dc:creator>
    </item>
    <item>
      <title>Parametric Scaling Law of Tuning Bias in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2502.03023</link>
      <description>arXiv:2502.03023v1 Announce Type: cross 
Abstract: Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional holdout set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03023v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zeng, Kangdao Liu, Bingyi Jing, Hongxin Wei</dc:creator>
    </item>
    <item>
      <title>A Bootstrap-based Method for Testing Network Similarity</title>
      <link>https://arxiv.org/abs/1911.06869</link>
      <description>arXiv:1911.06869v3 Announce Type: replace 
Abstract: This paper studies the matched network inference problem, where the goal is to determine if two networks, defined on a common set of nodes, exhibit a specific form of stochastic similarity. Two notions of similarity are considered: (i) equality, i.e., testing whether the networks arise from the same random graph model, and (ii) scaling, i.e., testing whether their probability matrices are proportional for some unknown scaling constant. We develop a testing framework based on a parametric bootstrap approach and a Frobenius norm-based test statistic. The proposed approach is highly versatile as it covers both the equality and scaling problems, and ensures adaptability under various model settings, including stochastic blockmodels, Chung-Lu models, and random dot product graph models. We establish theoretical consistency of the proposed tests and demonstrate their empirical performance through extensive simulations under a wide range of model classes. Our results establish the flexibility and computational efficiency of the proposed method compared to existing approaches. We also report a real-world application involving the Aarhus network dataset, which reveals meaningful sociological patterns across different communication layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:1911.06869v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somnath Bhadra, Kaustav Chakraborty, Srijan Sengupta, Soumendra Lahiri</dc:creator>
    </item>
    <item>
      <title>Causal survival analysis under competing risks using longitudinal modified treatment policies</title>
      <link>https://arxiv.org/abs/2202.03513</link>
      <description>arXiv:2202.03513v3 Announce Type: replace 
Abstract: Longitudinal modified treatment policies (LMTP) have been recently developed as a novel method to define and estimate causal parameters that depend on the natural value of treatment. LMTPs represent an important advancement in causal inference for longitudinal studies as they allow the non-parametric definition and estimation of the joint effect of multiple categorical, numerical, or continuous exposures measured at several time points. We extend the LMTP methodology to problems in which the outcome is a time-to-event variable subject to right-censoring and competing risks. We present identification results and non-parametric locally efficient estimators that use flexible data-adaptive regression techniques to alleviate model misspecification bias, while retaining important asymptotic properties such as $\sqrt{n}$-consistency. We present an application to the estimation of the effect of the time-to-intubation on acute kidney injury amongst COVID-19 hospitalized patients, where death by other causes is taken to be the competing event.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.03513v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iv\'an D\'iaz, Katherine L Hoffman, Nima S. Hejazi</dc:creator>
    </item>
    <item>
      <title>Calibrated inference: statistical inference that accounts for both sampling uncertainty and distributional uncertainty</title>
      <link>https://arxiv.org/abs/2202.11886</link>
      <description>arXiv:2202.11886v4 Announce Type: replace 
Abstract: How can we draw trustworthy scientific conclusions? One criterion is that a study can be replicated by independent teams. While replication is critically important, it is arguably insufficient. If a study is biased for some reason and other studies recapitulate the approach then findings might be consistently incorrect. It has been argued that trustworthy scientific conclusions require disparate sources of evidence. However, different methods might have shared biases, making it difficult to judge the trustworthiness of a result. We formalize this issue by introducing a "distributional uncertainty model", wherein dense distributional shifts emerge as the superposition of numerous small random changes. The distributional perturbation model arises under a symmetry assumption on distributional shifts and is strictly weaker than assuming that the data is i.i.d. from the target distribution. We show that a stability analysis on a single data set allows us to construct confidence intervals that account for both sampling uncertainty and distributional uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.11886v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujin Jeong, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Adaptive weighted approach for high-dimensional statistical learning and inference</title>
      <link>https://arxiv.org/abs/2206.02204</link>
      <description>arXiv:2206.02204v3 Announce Type: replace 
Abstract: We propose a new weighted average estimator for the high dimensional parameters under the distributed learning system, in which the weight assigned to each coordinate is precisely proportional to the inverse of the variance of the local estimates for that coordinate. This strategy empowers the new estimator to achieve a minimal mean squared error, comparable to the current state-of-the-art one-shot distributed learning methods. While at the same time, the new weighting approach maintains remarkably low communication costs, as each agent is required to transmit only two vectors to the central server. As a result, the newly proposed method achieves optimal statistical efficiency while significantly reducing communication overhead. We further demonstrate the effectiveness of the new estimator by investigating the error bound and the asymptotic properties of the estimation, as well as the numerical performance on some simulated examples and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02204v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jun Lu, Xiaoyu Mao, Mengyao Li, Chenping Hou</dc:creator>
    </item>
    <item>
      <title>How to Tell When a Result Will Replicate: Significance and Replication in Distributional Null Hypothesis Tests</title>
      <link>https://arxiv.org/abs/2211.02609</link>
      <description>arXiv:2211.02609v3 Announce Type: replace 
Abstract: There is a well-known problem in Null Hypothesis Significance Testing: many statistically significant results fail to replicate in subsequent experiments. We show that this problem arises because standard `point-form null' significance tests consider only within-experiment but ignore between-experiment variation, and so systematically underestimate the degree of random variation in results. We give an extension to standard significance testing that addresses this problem by analysing both within- and between-experiment variation. This `distributional null' approach does not underestimate experimental variability and so is not overconfident in identifying significance; because this approach addresses between-experiment variation, it gives mathematically coherent estimates for the probability of replication of significant results. Using a large-scale replication dataset (the first `Many Labs' project), we show that many experimental results that appear statistically significant in standard tests are in fact consistent with random variation when both within- and between-experiment variation are taken into account in this approach. Further, grouping experiments in this dataset into `predictor-target' pairs we show that the predicted replication probabilities for target experiments produced in this approach (given predictor experiment results and the sample sizes of the two experiments) are strongly correlated with observed replication rates. Distributional null hypothesis testing thus gives researchers a statistical tool for identifying statistically significant and reliably replicable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.02609v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fintan Costello, Paul Watts</dc:creator>
    </item>
    <item>
      <title>A Cheat Sheet for Bayesian Prediction</title>
      <link>https://arxiv.org/abs/2304.12218</link>
      <description>arXiv:2304.12218v2 Announce Type: replace 
Abstract: This paper reviews the growing field of Bayesian prediction. Bayes point and interval prediction are defined and exemplified and situated in statistical prediction more generally. Then, four general approaches to Bayes prediction are defined and we turn to predictor selection. This can be done predictively or non-predictively and predictors can be based on single models or multiple models. We call these latter cases unitary predictors and model average predictors, respectively. Then we turn to the most recent aspect of prediction to emerge, namely prediction in the context of large observational data sets and discuss three further classes of techniques. We conclude with a summary and statement of several current open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12218v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Statist. Sci. 2025, Vol. 40, 3-24</arxiv:journal_reference>
      <dc:creator>Bertrand Clarke, Yuling Yao</dc:creator>
    </item>
    <item>
      <title>Data-adaptive exposure thresholds for the Horvitz-Thompson estimator of the Average Treatment Effect in experiments with network interference</title>
      <link>https://arxiv.org/abs/2405.15887</link>
      <description>arXiv:2405.15887v2 Announce Type: replace 
Abstract: Randomized controlled trials often suffer from interference, a violation of the Stable Unit Treatment Values Assumption (SUTVA) in which a unit's treatment assignment affects the outcomes of its neighbors. This interference causes bias in naive estimators of the average treatment effect (ATE). A popular method to achieve unbiasedness is to pair the Horvitz-Thompson estimator of the ATE with a known exposure mapping: a function that identifies which units in a given randomization are not subject to interference. For example, an exposure mapping can specify that any unit with at least $h$-fraction of its neighbors having the same treatment status does not experience interference. However, this threshold $h$ is difficult to elicit from domain experts, and a misspecified threshold can induce bias. In this work, we propose a data-adaptive method to select the "$h$"-fraction threshold that minimizes the mean squared error of the Hortvitz-Thompson estimator. Our method estimates the bias and variance of the Horvitz-Thompson estimator under different thresholds using a linear dose-response model of the potential outcomes. We present simulations illustrating that our method improves upon non-adaptive choices of the threshold. We further illustrate the performance of our estimator by running experiments on a publicly-available Amazon product similarity graph. Furthermore, we demonstrate that our method is robust to deviations from the linear potential outcomes model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15887v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vydhourie Thiyageswaran, Tyler McCormick, Jennifer Brennan</dc:creator>
    </item>
    <item>
      <title>Latent Gaussian and H\"usler--Reiss Graphical Models with Golazo Penalty</title>
      <link>https://arxiv.org/abs/2408.12482</link>
      <description>arXiv:2408.12482v2 Announce Type: replace 
Abstract: The existence of latent variables in practical problems is common, for example when some variables are difficult or expensive to measure, or simply unknown. When latent variables are unaccounted for, structure learning for Gaussian graphical models can be blurred by additional correlation between the observed variables that is incurred by the latent variables. A standard approach for this problem is a latent version of the graphical lasso that splits the inverse covariance matrix into a sparse and a low-rank part that are penalized separately. This approach has recently been extended successfully to H\"usler--Reiss graphical models, which can be considered as an analogue of Gaussian graphical models in extreme value statistics. In this paper we propose a generalization of structure learning for Gaussian and H\"usler--Reiss graphical models via the flexible Golazo penalty. This allows us to introduce latent versions of for example the adaptive lasso, positive dependence constraints or predetermined sparsity patterns, and combinations of those. We develop algorithms for both latent graphical models with the Golazo penalty and demonstrate them on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12482v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ignacio Echave-Sustaeta Rodr\'iguez, Frank R\"ottger</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v3 Announce Type: replace 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by a factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simple hypothesis tests, as well as an algorithm to determine the necessary inflation factor for model parameter fits and Goodness of Fit tests and composite hypothesis tests. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v3</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Markov chain Monte Carlo without evaluating the target: an auxiliary variable approach</title>
      <link>https://arxiv.org/abs/2406.05242</link>
      <description>arXiv:2406.05242v3 Announce Type: replace-cross 
Abstract: In sampling tasks, it is common for target distributions to be known up to a normalizing constant. However, in many situations, even evaluating the unnormalized distribution can be costly or infeasible. This issue arises in scenarios such as sampling from the Bayesian posterior for tall datasets and the 'doubly-intractable' distributions. In this paper, we begin by observing that seemingly different Markov chain Monte Carlo (MCMC) algorithms, such as the exchange algorithm, PoissonMH, and TunaMH, can be unified under a simple common procedure. We then extend this procedure into a novel framework that allows the use of auxiliary variables in both the proposal and the acceptance-rejection step. Several new MCMC algorithms emerge from this framework that utilize estimated gradients to guide the proposal moves. They have demonstrated significantly better performance than existing methods on both synthetic and real datasets. Additionally, we develop the theory of the new framework and apply it to existing algorithms to simplify and extend their results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05242v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Yuan, Guanyang Wang</dc:creator>
    </item>
    <item>
      <title>RieszBoost: Gradient Boosting for Riesz Regression</title>
      <link>https://arxiv.org/abs/2501.04871</link>
      <description>arXiv:2501.04871v2 Announce Type: replace-cross 
Abstract: Answering causal questions often involves estimating linear functionals of conditional expectations, such as the average treatment effect or the effect of a longitudinal modified treatment policy. By the Riesz representation theorem, these functionals can be expressed as the expected product of the conditional expectation of the outcome and the Riesz representer, a key component in doubly robust estimation methods. Traditionally, the Riesz representer is estimated indirectly by deriving its explicit analytical form, estimating its components, and substituting these estimates into the known form (e.g., the inverse propensity score). However, deriving or estimating the analytical form can be challenging, and substitution methods are often sensitive to practical positivity violations, leading to higher variance and wider confidence intervals. In this paper, we propose a novel gradient boosting algorithm to directly estimate the Riesz representer without requiring its explicit analytical form. This method is particularly suited for tabular data, offering a flexible, nonparametric, and computationally efficient alternative to existing methods for Riesz regression. Through simulation studies, we demonstrate that our algorithm performs on par with or better than indirect estimation techniques across a range of functionals, providing a user-friendly and robust solution for estimating causal quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04871v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn J. Lee, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Dimension-Reduced Clustering for High-Dimensional Nonspherical Mixtures</title>
      <link>https://arxiv.org/abs/2502.02580</link>
      <description>arXiv:2502.02580v2 Announce Type: replace-cross 
Abstract: In mixture models, nonspherical (anisotropic) noise within each cluster is widely present in real-world data. We study both the minimax rate and optimal statistical procedure for clustering under high-dimensional nonspherical mixture models. In high-dimensional settings, we first establish the information-theoretic limits for clustering under Gaussian mixtures. The minimax lower bound unveils an intriguing informational dimension-reduction phenomenon: there exists a substantial gap between the minimax rate and the oracle clustering risk, with the former determined solely by the projected centers and projected covariance matrices in a low-dimensional space. Motivated by the lower bound, we propose a novel computationally efficient clustering method: Covariance Projected Spectral Clustering (COPO). Its key step is to project the high-dimensional data onto the low-dimensional space spanned by the cluster centers and then use the projected covariance matrices in this space to enhance clustering. We establish tight algorithmic upper bounds for COPO, both for Gaussian noise with flexible covariance and general noise with local dependence. Our theory indicates the minimax-optimality of COPO in the Gaussian case and highlights its adaptivity to a broad spectrum of dependent noise. Extensive simulation studies under various noise structures and real data analysis demonstrate our method's superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02580v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Yuqi Gu</dc:creator>
    </item>
  </channel>
</rss>
