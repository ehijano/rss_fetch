<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 01:28:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing</title>
      <link>https://arxiv.org/abs/2509.02767</link>
      <description>arXiv:2509.02767v1 Announce Type: new 
Abstract: The cloud computing technology uses datacenters, which require energy. Recent trends show that the required energy for these datacenters will rise over time, or at least remain constant. Hence, the scientific community developed different algorithms, architectures, and approaches for improving the energy efficiency of cloud datacenters, which are summarized under the umbrella term Green Cloud computing. In this paper, we use an economic approach - taxes - for reducing the energy consumption of datacenters. We developed a tax model called GreenCloud tax, which penalizes energy-inefficient datacenters while fostering datacenters that are energy-efficient. Hence, providers running energy-efficient datacenters are able to offer cheaper prices to consumers, which consequently leads to a shift of workloads from energy-inefficient datacenters to energy-efficient datacenters. The GreenCloud tax approach was implemented using the simulation environment CloudSim. We applied real data sets published in the SPEC benchmark for the executed simulation scenarios, which we used for evaluating the GreenCloud tax.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02767v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benedikt Pittl, Werner Mach, Erich Schikuta</dc:creator>
    </item>
    <item>
      <title>Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training</title>
      <link>https://arxiv.org/abs/2509.03018</link>
      <description>arXiv:2509.03018v1 Announce Type: new 
Abstract: Reliability is essential for ensuring efficiency in LLM training. However, many real-world reliability issues remain difficult to resolve, resulting in wasted resources and degraded model performance. Unfortunately, today's collective communication libraries operate as black boxes, hiding critical information needed for effective root cause analysis. We propose Mycroft, a lightweight distributed tracing and root cause analysis system designed to address previously hidden reliability issues in collective communication. Mycroft's key idea is to trace collective communication states and leverage internal control and data dependencies to resolve reliability problems in LLM training. Mycroft has been deployed at ByteDance for over six months to debug collective communication related issues at runtime. It detected anomalies within 15 seconds in 90% of cases and identified the root cause within 20 seconds in 60% of cases. We also conducted extensive fault injection experiments to demonstrate Mycroft's capability and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03018v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, Yang Bai, Shuguang Wang, Wencong Xiao, Jianxi Ye, Minlan Yu, Hong Xu</dc:creator>
    </item>
    <item>
      <title>FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs</title>
      <link>https://arxiv.org/abs/2509.03047</link>
      <description>arXiv:2509.03047v1 Announce Type: new 
Abstract: Large language models (LLMs) have made a profound impact across various fields due to their advanced capabilities. However, training these models at unprecedented scales requires extensive AI accelerator clusters and sophisticated parallelism strategies, which pose significant challenges in maintaining system reliability over prolonged training periods. A major concern is the substantial loss of training time caused by inevitable hardware and software failures. To address these challenges, we present FlashRecovery, a fast and low-cost failure recovery system comprising three core modules: (1) Active and real-time failure detection. This module performs continuous training state monitoring, enabling immediate identification of hardware and software failures within seconds, thus ensuring rapid incident response; (2) Scale-independent task restart. By employing different recovery strategies for normal and faulty nodes, combined with an optimized communication group reconstruction protocol, our approach ensures that the recovery time remains nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery within one step. Our novel recovery mechanism enables single-step restoration, completely eliminating dependence on traditional checkpointing methods and their associated overhead. Collectively, these innovations enable FlashRecovery to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective (RPO), substantially improving the reliability and efficiency of long-duration LLM training. Experimental results demonstrate that FlashRecovery system can achieve training restoration on training cluster with 4, 800 devices in 150 seconds. We also verify that the time required for failure recovery is nearly consistent for different scales of training tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03047v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haijun Zhang, Jinxiang Wang, Zhenhua Yu, Yanyong Zhang, Xuejie Ji, Kaining Mao, Jun Zhang, Yaqing Zhang, Ting Wu, Fei Jie, Xiemin Huang, Zhifang Cai, Junhua Cheng, Shuwei Wang, Wei Li, Xiaoming Bao, Hua Xu, Shixiong Zhao, Jun Li, Hongwei Sun, Ziyang Zhang, Yi Xiong, Chunsheng Li</dc:creator>
    </item>
    <item>
      <title>The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies</title>
      <link>https://arxiv.org/abs/2509.03104</link>
      <description>arXiv:2509.03104v1 Announce Type: new 
Abstract: Serverless computing is transforming cloud application development, but the performance-cost trade-offs of control plane designs remain poorly understood due to a lack of open, cross-platform benchmarks and detailed system analyses. In this work, we address these gaps by designing a serverless system that approximates the scaling behaviors of commercial providers, including AWS Lambda and Google Cloud Run. We systematically compare the performance and cost-efficiency of both synchronous and asynchronous autoscaling policies by replaying real-world workloads and varying key autoscaling parameters.
  We demonstrate that our open-source systems can closely replicate the operational characteristics of commercial platforms, enabling reproducible and transparent experimentation. By evaluating how autoscaling parameters affect latency, memory usage, and CPU overhead, we reveal several key findings. First, we find that serverless systems exhibit significant computational overhead due to instance churn equivalent to 10-40% of the CPU cycles spent on request handling, primarily originating from worker nodes. Second, we observe high memory allocation due to scaling policy: 2-10 times more than actively used. Finally, we demonstrate that reducing these overheads typically results in significant performance degradation in the current systems, underscoring the need for new, cost-efficient autoscaling strategies. Additionally, we employ a hybrid methodology that combines real control plane deployments with large-scale simulation to extend our evaluation closer to a production scale, thereby bridging the gap between small research clusters and real-world environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03104v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonid Kondrashov, Boxi Zhou, Hancheng Wang, Dmitrii Ustiugov</dc:creator>
    </item>
    <item>
      <title>Efficient and Secure Sleepy Model for BFT Consensus</title>
      <link>https://arxiv.org/abs/2509.03145</link>
      <description>arXiv:2509.03145v1 Announce Type: new 
Abstract: Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available systems face a critical challenge: balancing latency and security in fluctuating node participation. Existing solutions often require multiple rounds of voting per decision, leading to high latency or limited resilience to adversarial behavior. This paper presents a BFT protocol integrating a pre-commit mechanism with publicly verifiable secret sharing (PVSS) into message transmission. By binding users' identities to their messages through PVSS, our approach reduces communication rounds. Compared to other state-of-the-art methods, our protocol typically requires only four network delays (4$\Delta$) in common scenarios while being resilient to up to 1/2 adversarial participants. This integration enhances the efficiency and security of the protocol without compromising integrity. Theoretical analysis demonstrates the robustness of the protocol against Byzantine attacks. Experimental evaluations show that, compared to traditional BFT protocols, our protocol significantly prevents fork occurrences and improves chain stability. Furthermore, compared to longest-chain protocol, our protocol maintains stability and lower latency in scenarios with moderate participation fluctuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03145v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengkun Ren, Hai Dong, Zahir Tari, Pengcheng Zhang</dc:creator>
    </item>
    <item>
      <title>CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload</title>
      <link>https://arxiv.org/abs/2509.03394</link>
      <description>arXiv:2509.03394v1 Announce Type: new 
Abstract: Cloud platforms are increasingly relied upon to host diverse, resource-intensive workloads due to their scalability, flexibility, and cost-efficiency. In multi-tenant cloud environments, virtual machines are consolidated on shared physical servers to improve resource utilization. While virtualization guarantees resource partitioning for CPU, memory, and storage, it cannot ensure performance isolation. Competition for shared resources such as last-level cache, memory bandwidth, and network interfaces often leads to severe performance degradation. Existing management techniques, including VM scheduling and resource provisioning, require accurate performance prediction to mitigate interference. However, this remains challenging in public clouds due to the black-box nature of VMs and the highly dynamic nature of workloads. To address these limitations, we propose CloudFormer, a dual-branch Transformer-based model designed to predict VM performance degradation in black-box environments. CloudFormer jointly models temporal dynamics and system-level interactions, leveraging 206 system metrics at one-second resolution across both static and dynamic scenarios. This design enables the model to capture transient interference effects and adapt to varying workload conditions without scenario-specific tuning. Complementing the methodology, we provide a fine-grained dataset that significantly expands the temporal resolution and metric diversity compared to existing benchmarks. Experimental results demonstrate that CloudFormer consistently outperforms state-of-the-art baselines across multiple evaluation metrics, achieving robust generalization across diverse and previously unseen workloads. Notably, CloudFormer attains a mean absolute error (MAE) of just 7.8%, representing a substantial improvement in predictive accuracy and outperforming existing methods at least by 28%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03394v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza</dc:creator>
    </item>
    <item>
      <title>On the Optimization of Methods for Establishing Well-Connected Communities</title>
      <link>https://arxiv.org/abs/2509.02590</link>
      <description>arXiv:2509.02590v1 Announce Type: cross 
Abstract: Community detection plays a central role in uncovering meso scale structures in networks. However, existing methods often suffer from disconnected or weakly connected clusters, undermining interpretability and robustness. Well-Connected Clusters (WCC) and Connectivity Modifier (CM) algorithms are post-processing techniques that improve the accuracy of many clustering methods. However, they are computationally prohibitive on massive graphs. In this work, we present optimized parallel implementations of WCC and CM using the HPE Chapel programming language. First, we design fast and efficient parallel algorithms that leverage Chapel's parallel constructs to achieve substantial performance improvements and scalability on modern multicore architectures. Second, we integrate this software into Arkouda/Arachne, an open-source, high-performance framework for large-scale graph analytics. Our implementations uniquely enable well-connected community detection on massive graphs with more than 2 billion edges, providing a practical solution for connectivity-preserving clustering at web scale. For example, our implementations of WCC and CM enable community detection of the over 2-billion edge Open-Alex dataset in minutes using 128 cores, a result infeasible to compute previously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02590v1</guid>
      <category>cs.SI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Dindoost, Oliver Alvarado Rodriguez, Bartosz Bryg, Minhyuk Park, George Chacko, Tandy Warnow, David A. Bader</dc:creator>
    </item>
    <item>
      <title>\textit{In Silico} Benchmarking of Detectable Byzantine Agreement in Noisy Quantum Networks</title>
      <link>https://arxiv.org/abs/2509.02629</link>
      <description>arXiv:2509.02629v1 Announce Type: cross 
Abstract: Quantum communication resources offer significant advantages for fault-tolerant distributed protocols, particularly in Byzantine Agreement (BA), where reliability against adversarial interference is essential. Quantum Detectable Byzantine Agreement (QDBA) enables consensus protocols that surpass classical limitations by leveraging entangled quantum states. In this work, we focus on the practical realization of QDBA using Einstein-Podolsky-Rosen (EPR) pairs, the simplest maximally entangled quantum resources, making the protocol experimentally accessible across current quantum hardware platforms. We present a comprehensive computational study of the EPRQDBA protocol under realistic quantum network conditions, utilizing the Aliro Quantum Network Simulator to evaluate the performance and robustness of the protocol. Our simulations systematically explore the protocol's parameter space --including variations in network size, traitorous node count, the amount of entanglement consumed in the protocol, and physically motivated noise models tailored specifically for superconducting and photonic qubit technologies. Through extensive numerical experiments, we provide insights into how these physically realistic parameters impact protocol performance, establishing critical thresholds and optimal operational regimes for experimental implementations. This work bridges theoretical advances in quantum consensus protocols with practical network implementations, offering a concrete reference for experimentalists. Our findings serve as a guideline for evaluating and optimizing QDBA implementations in realistic, noisy environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02629v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayank Bhatia, Shaan Doshi, Daniel Winton, Brian Doolittle, Bruno Abreu, Santiago N\'u\~nez-Corrales</dc:creator>
    </item>
    <item>
      <title>Treasure Hunt in Anonymous Graphs with Quantum Pebbles by Oblivious Agents</title>
      <link>https://arxiv.org/abs/2509.02909</link>
      <description>arXiv:2509.02909v1 Announce Type: cross 
Abstract: We investigate the problem of finding a static treasure in anonymous graphs using oblivious agents and introduce a novel approach that leverages quantum information. In anonymous graphs, vertices are unlabelled, indistinguishable, and edges are locally labelled with port numbers. Agents typically rely on stationary classical pebbles placed by an oracle to guide their search. However, this classical approach is constrained by limited information transmission and high traversal complexity. Classical pebbles are not sufficient for search if the agents are oblivious. We propose the first use of quantum pebbles for search in anonymous graphs. Quantum pebbles periodically emit qubits in a fixed quantum state. Each pebble encodes the port number to the next node using a unique quantum state. The agent determines the correct path by performing measurements in multiple bases, exploiting the probabilistic nature of quantum measurement to distinguish states. We show that this strategy enables an oblivious agent to locate the treasure in $D$ steps using $D$ quantum pebbles, where $D$ is the length of the shortest path between the starting point and the treasure. Moreover, only $O((\log D + \log \Delta)/(\log 1/\delta))$ measurements per node are required to ensure high success probability in a graph with maximum degree $\Delta$ where $\delta = \cos^2(\frac{\pi}{2\Delta})$. We propose the use of quantum information as a guidance mechanism in anonymous graph search. We demonstrate that quantum pebbles can not only emulate the functionality of classical pebbles but can do so with improved efficiency, offering a promising direction for future quantum-enhanced distributed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02909v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Gaur, Barun Gorain, Rishi Ranjan Singh, Daya Gaur</dc:creator>
    </item>
    <item>
      <title>A description of the radio astronomy data processing tool DDF Pipeline</title>
      <link>https://arxiv.org/abs/2509.03075</link>
      <description>arXiv:2509.03075v1 Announce Type: cross 
Abstract: This paper presents the DDF Pipeline, a radio astronomy data processing tool initially designed for the LOw-Frequency ARray (LO- FAR) radio-telescope and a candidate for processing data from the Square Kilometre Array (SKA). This work describes the DDF Pipeline software and presents a coarse-grain profiling execution to characterize its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03075v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.DC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathis Certenais, Fran\c{c}ois Bodin, Laurent Morin</dc:creator>
    </item>
    <item>
      <title>DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling</title>
      <link>https://arxiv.org/abs/2509.03472</link>
      <description>arXiv:2509.03472v1 Announce Type: cross 
Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user privacy when using sensitive data to train neural networks. During training, converting model weights and activations into low-precision formats, i.e., quantization, can drastically reduce training times, energy consumption, and cost, and is thus a widely used technique. In this work, we demonstrate that quantization causes significantly higher accuracy degradation in DP-SGD compared to regular SGD. We observe that this is caused by noise injection in DP-SGD, which amplifies quantization variance, leading to disproportionately large accuracy degradation. To address this challenge, we present QPQuant, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. Our method combines two key ideas that effectively reduce quantization variance: (i) probabilistic sampling of the layers that rotates which layers are quantized every epoch, and (ii) loss-aware layer prioritization, which uses a differentially private loss sensitivity estimator to identify layers that can be quantized with minimal impact on model quality. This estimator consumes a negligible fraction of the overall privacy budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50, and DenseNet121 across a range of datasets demonstrate that DPQuant consistently outperforms static quantization baselines, achieving near Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical throughput improvements on low-precision hardware, with less than 2% drop in validation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03472v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yubo Gao, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar</dc:creator>
    </item>
    <item>
      <title>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</title>
      <link>https://arxiv.org/abs/2407.00079</link>
      <description>arXiv:2407.00079v4 Announce Type: replace 
Abstract: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00079v4</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, Xinran Xu</dc:creator>
    </item>
    <item>
      <title>Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids</title>
      <link>https://arxiv.org/abs/2508.08022</link>
      <description>arXiv:2508.08022v2 Announce Type: replace 
Abstract: Real-time monitoring of power consumption in cities and micro-grids through the Internet of Things (IoT) can help forecast future demand and optimize grid operations. But moving all consumer-level usage data to the cloud for predictions and analysis at fine time scales can expose activity patterns. Federated Learning~(FL) is a privacy-sensitive collaborative DNN training approach that retains data on edge devices, trains the models on private data locally, and aggregates the local models in the cloud. But key challenges exist: (i) clients can have non-independently identically distributed~(non-IID) data, and (ii) the learning should be computationally cheap while scaling to 1000s of (unseen) clients. In this paper, we develop and evaluate several optimizations to FL training across edge and cloud for time-series demand forecasting in micro-grids and city-scale utilities using DNNs to achieve a high prediction accuracy while minimizing the training cost. We showcase the benefit of using exponentially weighted loss while training and show that it further improves the prediction of the final model. Finally, we evaluate these strategies by validating over 1000s of clients for three states in the US from the OpenEIA corpus, and performing FL both in a pseudo-distributed setting and a Pi edge cluster. The results highlight the benefits of the proposed methods over baselines like ARIMA and DNNs trained for individual consumers, which are not scalable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08022v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roopkatha Banerjee, Sampath Koti, Gyanendra Singh, Anirban Chakraborty, Gurunath Gurrala, Bhushan Jagyasi, Yogesh Simmhan</dc:creator>
    </item>
    <item>
      <title>CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization</title>
      <link>https://arxiv.org/abs/2507.08406</link>
      <description>arXiv:2507.08406v2 Announce Type: replace-cross 
Abstract: As transistor counts in a single chip exceed tens of billions, the complexity of RTL-level simulation and verification has grown exponentially, often extending simulation campaigns to several months. In industry practice, RTL simulation is divided into two phases: functional debug and system validation. While system validation demands high simulation speed and is typically accelerated using FPGAs, functional debug relies on rapid compilation-rendering multi-core CPUs the primary choice. However, the limited simulation speed of CPUs has become a major bottleneck. To address this challenge, we propose CCSS, a scalable multi-core RTL simulation platform that achieves both fast compilation and high simulation throughput. CCSS accelerates combinational logic computation and sequential logic synchronization through specialized architecture and compilation strategies. It employs a balanced DAG partitioning method and efficient boolean computation cores for combinational logic, and adopts a low-latency network-on-chip (NoC) design to synchronize sequential states across cores efficiently. Experimental results show that CCSS delivers up to 12.9x speedup over state-of-the-art multi-core simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08406v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weigang Feng, Yijia Zhang, Zekun Wang, Zhengyang Wang, Yi Wang, Peijun Ma, Ningyi Xu</dc:creator>
    </item>
  </channel>
</rss>
