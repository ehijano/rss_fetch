<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Jan 2026 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Cache-Aware Hybrid Sieve Combining Segmentation and Bit-Packing for Fast Prime Generation</title>
      <link>https://arxiv.org/abs/2601.19909</link>
      <description>arXiv:2601.19909v1 Announce Type: new 
Abstract: Prime generation is a fundamental task in cryptography, number theory, and randomized algorithms. While the classical Sieve of Eratosthenes is simple and efficient in theory, its practical performance on modern central processing units is often limited by memory access inefficiencies. This paper introduces a cache-aware hybrid sieve that integrates segmentation, bit-packing, and cache-line-aligned block processing to optimize memory bandwidth and level one and level two cache locality.
  The proposed approach reduces memory usage by storing only odd numbers and using one bit per value. The sieve range is divided into cache-sized blocks to minimize cache misses, while primes up to the square root of the limit are reused across blocks. Experimental results demonstrate up to an eight times reduction in memory usage and runtime improvements of up to two point four times compared to the classical sieve and one point seven times compared to the segmented sieve. Benchmarks up to ten to the power of nine illustrate that architecture-aware algorithm design can yield substantial practical performance gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19909v1</guid>
      <category>cs.DS</category>
      <category>cs.AR</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathi Lakshmi Mani Thirdhana (National Institute of Technology Calicut, India)</dc:creator>
    </item>
    <item>
      <title>Node-Weighted Multicut in Planar Digraphs</title>
      <link>https://arxiv.org/abs/2601.20038</link>
      <description>arXiv:2601.20038v1 Announce Type: new 
Abstract: Kawarabayashi and Sidiropoulos [KS22] obtained an $O(\log^2 n)$-approximation algorithm for Multicut in planar digraphs via a natural LP relaxation, which also establishes a corresponding upper bound on the multicommodity flow-cut gap. Their result is in contrast to a lower bound of $\tilde{\Omega}(n^{1/7})$ on the flow-cut gap for general digraphs due to Chuzhoy and Khanna [CK09]. We extend the algorithm and analysis in [KS22] to the node-weighted Multicut problem. Unlike in general digraphs, node-weighted problems cannot be reduced to edge-weighted problems in a black box fashion due to the planarity restriction. We use the node-weighted problem as a vehicle to accomplish two additional goals: (i) to obtain a deterministic algorithm (the algorithm in [KS22] is randomized), and (ii) to simplify and clarify some aspects of the algorithm and analysis from [KS22]. The Multicut result, via a standard technique, implies an approximation for the Nonuniform Sparsest Cut problem with an additional logarithmic factor loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20038v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandra Chekuri, Rhea Jain</dc:creator>
    </item>
    <item>
      <title>Hypergraph Samplers: Typical and Worst Case Behavior</title>
      <link>https://arxiv.org/abs/2601.20039</link>
      <description>arXiv:2601.20039v1 Announce Type: new 
Abstract: We study the utility and limitations of using $k$-uniform hypergraphs $H = ([n], E)$ ($n \ge \mathrm{poly}(k)$) in the context of error reduction for randomized algorithms for decision problems with one- or two-sided error. Our error reduction idea is sampling a uniformly random hyperedge of $H$, and repeating the algorithm $k$ times using the hyperedge vertices as seeds. This is a general paradigm, which captures every pseudorandom method generating $k$ seeds without repetition. We show two results which imply a gap between the typical and the worst-case behavior of using $H$ for error-reduction.
  First, in the context of one-sided error reduction, if using a random hyperedge of $H$ decreases the error probability from $p$ to $p^k + \epsilon$, then $H$ cannot have too few edges, i.e., $|E| = \Omega(n k^{-1} \epsilon^{-1})$. Thus, the number of random bits needed for reducing the error from $p$ to $p^k + \epsilon$ cannot be reduced below $\lg n+\lg(\epsilon^{-1})-\lg k+O(1)$. This is also true for hypergraphs of average uniformity $k$. Our result implies new lower bounds for dispersers and vertex-expanders.
  Second, if the vertex degrees are reasonably distributed, we show that in a $(1-o(1))$-fraction of the cases, choosing $k$ pseudorandom seeds using $H$ will reduce the error probability to at most $o(1)$ above the error probability of using $k$ IID seeds, for both algorithms with one- or two-sided error. Thus, despite our lower bound, for a $(1-o(1))$-fraction of randomized algorithms (and inputs) for decision problems, the advantage of using IID samples over samples obtained from a uniformly random edge of a reasonable hypergraph is negligible. A similar statement holds true for randomized algorithms with two-sided error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20039v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vedat Levi Alev, Uriya A. First</dc:creator>
    </item>
    <item>
      <title>Dynamic framework for edge-connectivity maintenance of simple graphs</title>
      <link>https://arxiv.org/abs/2601.20137</link>
      <description>arXiv:2601.20137v1 Announce Type: new 
Abstract: We present a dynamic framework for maintaining $k$-edge-connectivity of undirected, simple graphs subject to structural updates, specifically single edge additions and removals. The required edge-connectivity $k$ is a chosen, constant parameter. Unlike standard dynamic graph problems, such as dynamic minimum-cut, which focus solely on reporting the value of the minimum cut, our approach actively modifies the graph $G$ to maintain the edge-connectivity invariant $\lambda(G) \ge k$. We address two fundamental maintenance tasks: redundancy elimination, which identifies and removes an existing edge rendered redundant for $k$-edge-connectivity by new edge insertion, and connectivity restoration, which computes and inserts a minimal set of augmenting edges to restore graph's $k$-edge-connectivity following an old edge deletion. To preclude trivial reversals, we strictly enforce that the eliminated edge is distinct from the inserted edge and that restoration excludes the already deleted edge. Our solution of the first problem integrates Nagamochi-Ibaraki sparse certificates [Nagamochi and Ibaraki 1992] with Link-Cut Trees [Sleator and Tarjan 1983] to remove redundant edges in $O(k \log n)$ amortized time. For restoration, we propose a localized augmentation strategy that exploits the residual graph structure to bridge the minimum cut. By executing Dinic's [Dinic 1970] algorithm on the sparsified input graph, we identify the minimal edge set required to reconnect the graph in $O(k \cdot n^{5/3})$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20137v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Blazej Wrobel</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Algorithms for Graph Spanners via Low-Diameter Router Decomposition</title>
      <link>https://arxiv.org/abs/2601.20718</link>
      <description>arXiv:2601.20718v1 Announce Type: new 
Abstract: A $t$-spanner of an undirected $n$-vertex graph $G$ is a sparse subgraph $H$ of $G$ that preserves all pairwise distances between its vertices to within multiplicative factor $t$, also called the \emph{stretch}. We investigate the problem of maintaining spanners in the fully dynamic setting with an adaptive adversary. Despite a long line of research, this problem is still poorly understood: no algorithm achieving a sublogarithmic stretch, a sublinear in $n$ update time, and a strongly subquadratic in $n$ spanner size is currently known.
  One of our main results is a deterministic algorithm, that, for any $512 \leq k \leq (\log n)^{1/49}$ and $1/k\leq \delta \leq 1/400$, maintains a spanner $H$ of a fully dynamic graph with stretch $poly(k)\cdot 2^{O(1/\delta^6)}$ and size $|E(H)|\leq O(n^{1+O(1/k)})$, with worst-case update time $n^{O(\delta)}$ and recourse $n^{O(1/k)}$. Our algorithm relies on a new technical tool that we develop, called low-diameter router decomposition. We design a deterministic algorithm that maintains a decomposition of a fully dynamic graph into edge-disjoint clusters with bounded vertex overlap, where each cluster $C$ is a bounded-diameter router, meaning that any reasonable multicommodity demand over the vertices of $C$ can be routed along short paths and with low congestion. A similar graph decomposition notion was introduced by [Haeupler et al., STOC 2022] and strengthened by [Haeupler et al., FOCS 2024]. However, in contrast to these and other prior works, the decomposition that our algorithm maintains is proper, ensuring that the routing paths between the pairs of vertices of each cluster $C$ are contained inside $C$, rather than in the entire graph $G$. We show additional applications of our router decomposition, including dynamic algorithms for fault-tolerant spanners and low-congestion spanners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20718v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Chuzhoy, Merav Parter</dc:creator>
    </item>
    <item>
      <title>Minimum-Cost Network Flow with Dual Predictions</title>
      <link>https://arxiv.org/abs/2601.20203</link>
      <description>arXiv:2601.20203v1 Announce Type: cross 
Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20203v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiyang Chen, Hailong Yao, Xia Yin</dc:creator>
    </item>
    <item>
      <title>Improved Computational Lower Bound of Estimation for Multi-Frequency Group Synchronization</title>
      <link>https://arxiv.org/abs/2601.20522</link>
      <description>arXiv:2601.20522v1 Announce Type: cross 
Abstract: We study the computational phase transition in a multi-frequency group synchronization problem, where pairwise relative measurements of group elements are observed across multiple frequency channels and corrupted by Gaussian noise. Using the framework of \emph{low-degree polynomial algorithms}, we analyze the task of estimating the structured signal in such observations. We show that, assuming the low-degree heuristic, in synchronization models over the circle group $\mathsf{SO}(2)$, a simple spectral method is computationally optimal among all polynomial-time estimators when the number of frequencies satisfies $L=n^{o(1)}$. This significantly extends prior work \cite{KBK24+}, which only applied to a fixed constant number of frequencies. Together with known upper bounds on the statistical threshold \cite{PWBM18a}, our results establish the existence of a \emph{statistical-to-computational gap} in this model when the number of frequencies is sufficiently large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20522v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Active Learning for Decision Trees with Provable Guarantees</title>
      <link>https://arxiv.org/abs/2601.20775</link>
      <description>arXiv:2601.20775v1 Announce Type: cross 
Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+\epsilon)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $\epsilon$ is close to optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20775v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arshia Soltani Moakhar, Tanapoom Laoaron, Faraz Ghahremani, Kiarash Banihashem, MohammadTaghi Hajiaghayi</dc:creator>
    </item>
    <item>
      <title>Breaching the 2 LMP Approximation Barrier for Facility Location with Applications to k-Median</title>
      <link>https://arxiv.org/abs/2207.05150</link>
      <description>arXiv:2207.05150v2 Announce Type: replace 
Abstract: The Uncapacitated Facility Location (UFL) problem is one of the most fundamental clustering problems: Given a set of clients $C$ and a set of facilities $F$ in a metric space $(C \cup F, dist)$ with facility costs $open : F \to \mathbb{R}^+$, the goal is to find a set of facilities $S \subseteq F$ to minimize the sum of the opening cost $open(S)$ and the connection cost $d(S) := \sum_{p \in C} \min_{c \in S} dist(p, c)$. An algorithm for UFL is called a Lagrangian Multiplier Preserving (LMP) $\alpha$ approximation if it outputs a solution $S\subseteq F$ satisfying $open(S) + d(S) \leq open(S^*) + \alpha d(S^*)$ for any $S^* \subseteq F$. The best-known LMP approximation ratio for UFL is at most $2$ by the JMS algorithm of Jain, Mahdian, and Saberi based on the Dual-Fitting technique.
  We present a (slightly) improved LMP approximation algorithm for UFL. This is achieved by combining the Dual-Fitting technique with Local Search, another popular technique to address clustering problems. From a conceptual viewpoint, our result gives a theoretical evidence that local search can be enhanced so as to avoid bad local optima by choosing the initial feasible solution with LP-based techniques. Using the framework of bipoint solutions, our result directly implies a (slightly) improved approximation for the $k$-Median problem from 2.6742 to 2.67059.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.05150v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Cohen-Addad, Fabrizio Grandoni, Euiwoong Lee, Chris Schwiegelshohn</dc:creator>
    </item>
    <item>
      <title>Parameterized Spanning Tree Congestion</title>
      <link>https://arxiv.org/abs/2410.08314</link>
      <description>arXiv:2410.08314v5 Announce Type: replace 
Abstract: In this paper we study the Spanning Tree Congestion problem, where we are given a graph $G=(V,E)$ and are asked to find a spanning tree $T$ of minimum maximum congestion. Here, the congestion of an edge $e\in T$ is the number of edges $uv\in E$ such that the (unique) path from $u$ to $v$ in $T$ traverses $e$. We consider this well-studied NP-hard problem from the point of view of (structural) parameterized complexity and obtain the following results.
  We resolve a natural open problem by showing that Spanning Tree Congestion is not FPT parameterized by treewidth (under standard assumptions). More strongly, we present a generic reduction which applies to (almost) any parameter of the form ``vertex-deletion distance to class $\mathcal{C}$'', thus obtaining W[1]-hardness for parameters more restricted than treewidth, including tree-depth plus feedback vertex set, or incomparable to treewidth, such as twin cover. Via a slight tweak of the same reduction we also show that the problem is NP-complete on graphs of modular-width $4$.
  Even though it is known that Spanning Tree Congestion remains NP-hard on instances with only one vertex of unbounded degree, it is currently open whether the problem remains hard on bounded-degree graphs. We resolve this question by showing NP-hardness on graphs of maximum degree 8.
  Complementing the problem's W[1]-hardness for treewidth...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08314v5</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis, Valia Mitsou, Edouard Nemery, Yota Otachi, Manolis Vasilakis, Daniel Vaz</dc:creator>
    </item>
    <item>
      <title>Simple Sublinear Algorithms for $(\Delta+1)$ Vertex Coloring via Asymmetric Palette Sparsification</title>
      <link>https://arxiv.org/abs/2502.17629</link>
      <description>arXiv:2502.17629v4 Announce Type: replace 
Abstract: The palette sparsification theorem (PST) of Assadi, Chen, and Khanna (SODA 2019) states that in every graph $G$ with maximum degree $\Delta$, sampling a list of $O(\log{n})$ colors from $\{1,\ldots,\Delta+1\}$ for every vertex independently and uniformly, with high probability, allows for finding a $(\Delta+1)$ vertex coloring of $G$ by coloring each vertex only from its sampled list. PST naturally leads to a host of sublinear algorithms for $(\Delta+1)$ vertex coloring, including in semi-streaming, sublinear time, and MPC models, which are all proven to be nearly optimal, and in the case of the former two are the only known sublinear algorithms for this problem.
  While being a quite natural and simple-to-state theorem, PST suffers from two drawbacks. Firstly, all its known proofs require technical arguments that rely on sophisticated graph decompositions and probabilistic arguments. Secondly, finding the coloring of the graph from the sampled lists in an efficient manner requires a considerably complicated algorithm.
  We show that a natural weakening of PST addresses both these drawbacks while still leading to sublinear algorithms of similar quality (up to polylog factors). In particular, we prove an asymmetric palette sparsification theorem (APST) that allows for list sizes of the vertices to have different sizes and only bounds the average size of these lists. The benefit of this weaker requirement is that we can now easily show the graph can be $(\Delta+1)$ colored from the sampled lists using the standard greedy coloring algorithm. This way, we can recover nearly-optimal bounds for $(\Delta+1)$ vertex coloring in all the aforementioned models using algorithms that are much simpler to implement and analyze.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17629v4</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.26.3</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 5 (2026), Article 3, 1-18</arxiv:journal_reference>
      <dc:creator>Sepehr Assadi, Helia Yazdanyar</dc:creator>
    </item>
    <item>
      <title>Improved bounds on the zeros of the chromatic polynomial of graphs and claw-free graphs</title>
      <link>https://arxiv.org/abs/2505.04366</link>
      <description>arXiv:2505.04366v2 Announce Type: replace-cross 
Abstract: We prove that for any graph $G$ the (complex) zeros of its chromatic polynomial, $\chi_G(x)$, lie inside the disk centered at $0$ of radius $4.25 \Delta(G)$, where $\Delta(G)$ denotes the maximum degree of $G$. This improves on a recent result of Jenssen, Patel and Regts, who proved a bound of $5.94\Delta(G)$. Moreover, we show that for graphs of sufficiently large girth we can replace $4.25$ by $3.60$ and for claw-free graphs we can replace $4.25$ by $3.81$.
  Our proofs add some substantially novel ideas to those developed by Jenssen, Patel, and Regts, while building on them. A key novel ingredient for claw-free graphs is to use a representation of the coefficients of the chromatic polynomial in terms of the number of certain partial acyclic orientations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04366v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ferenc Bencs, Guus Regts</dc:creator>
    </item>
  </channel>
</rss>
