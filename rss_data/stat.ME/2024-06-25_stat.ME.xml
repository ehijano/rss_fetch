<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jun 2024 01:50:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sparse Bayesian multidimensional scaling(s)</title>
      <link>https://arxiv.org/abs/2406.15573</link>
      <description>arXiv:2406.15573v1 Announce Type: new 
Abstract: Bayesian multidimensional scaling (BMDS) is a probabilistic dimension reduction tool that allows one to model and visualize data consisting of dissimilarities between pairs of objects. Although BMDS has proven useful within, e.g., Bayesian phylogenetic inference, its likelihood and gradient calculations require a burdensome order of $N^2$ floating-point operations, where $N$ is the number of data points. Thus, BMDS becomes impractical as $N$ grows large. We propose and compare two sparse versions of BMDS (sBMDS) that apply log-likelihood and gradient computations to subsets of the observed dissimilarity matrix data. Landmark sBMDS (L-sBMDS) extracts columns, while banded sBMDS (B-sBMDS) extracts diagonals of the data. These sparse variants let one specify a time complexity between $N^2$ and $N$. Under simplified settings, we prove posterior consistency for subsampled distance matrices. Through simulations, we examine the accuracy and computational efficiency across all models using both the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms. We observe approximately 3-fold, 10-fold and 40-fold speedups with negligible loss of accuracy, when applying the sBMDS likelihoods and gradients to 500, 1,000 and 5,000 data points with 50 bands (landmarks); these speedups only increase with the size of data considered. Finally, we apply the sBMDS variants to the phylogeographic modeling of multiple influenza subtypes to better understand how these strains spread through global air transportation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15573v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ami Sheth, Aaron Smith, Andrew J. Holbrook</dc:creator>
    </item>
    <item>
      <title>Graphical copula GARCH modeling with dynamic conditional dependence</title>
      <link>https://arxiv.org/abs/2406.15582</link>
      <description>arXiv:2406.15582v1 Announce Type: new 
Abstract: Modeling returns on large portfolios is a challenging problem as the number of parameters in the covariance matrix grows as the square of the size of the portfolio. Traditional correlation models, for example, the dynamic conditional correlation (DCC)-GARCH model, often ignore the nonlinear dependencies in the tail of the return distribution. In this paper, we aim to develop a framework to model the nonlinear dependencies dynamically, namely the graphical copula GARCH (GC-GARCH) model. Motivated from the capital asset pricing model, to allow modeling of large portfolios, the number of parameters can be greatly reduced by introducing conditional independence among stocks given some risk factors. The joint distribution of the risk factors is factorized using a directed acyclic graph (DAG) with pair-copula construction (PCC) to enhance the modeling of the tails of the return distribution while offering the flexibility of having complex dependent structures. The DAG induces topological orders to the risk factors, which can be regarded as a list of directions of the flow of information. The conditional distributions among stock returns are also modeled using PCC. Dynamic conditional dependence structures are incorporated to allow the parameters in the copulas to be time-varying. Three-stage estimation is used to estimate parameters in the marginal distributions, the risk factor copulas, and the stock copulas. The simulation study shows that the proposed estimation procedure can estimate the parameters and the underlying DAG structure accurately. In the investment experiment of the empirical study, we demonstrate that the GC-GARCH model produces more precise conditional value-at-risk prediction and considerably higher cumulative portfolio returns than the DCC-GARCH model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15582v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lupe Shun Hin Chan, Amanda Man Ying Chu, Mike Ka Pui So</dc:creator>
    </item>
    <item>
      <title>Nonparametric FBST for Validating Linear Models</title>
      <link>https://arxiv.org/abs/2406.15608</link>
      <description>arXiv:2406.15608v1 Announce Type: new 
Abstract: The Full Bayesian Significance Test (FBST) possesses many desirable aspects, such as not requiring a non-zero prior probability for hypotheses while also producing a measure of evidence for $H_0$. Still, few attempts have been made to bring the FBST to nonparametric settings, with the main drawback being the need to obtain the highest posterior density (HPD) in a function space. In this work, we use Gaussian processes to provide an analytically tractable FBST for hypotheses of the type $$ H_0: g(\boldsymbol{x}) = \boldsymbol{b}(\boldsymbol{x})\boldsymbol{\beta}, \quad \forall \boldsymbol{x} \in \mathcal{X}, \quad \boldsymbol{\beta} \in \mathbb{R}^k, $$ where $g(\cdot)$ is the regression function, $\boldsymbol{b}(\cdot)$ is a vector of linearly independent linear functions -- such as $\boldsymbol{b}(\boldsymbol{x}) = \boldsymbol{x}'$ -- and $\mathcal{X}$ is the covariates' domain. We also make use of pragmatic hypotheses to verify if the adherence of linear models may be approximately instead of exactly true, allowing for the inclusion of valuable information such as measurement errors and utility judgments. This contribution extends the theory of the FBST, allowing its application in nonparametric settings and providing a procedure that easily tests if linear models are adequate for the data and that can automatically perform variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15608v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo F. L. Lassance, Julio M. Stern, Rafael B. Stern</dc:creator>
    </item>
    <item>
      <title>Mixture of Directed Graphical Models for Discrete Spatial Random Fields</title>
      <link>https://arxiv.org/abs/2406.15700</link>
      <description>arXiv:2406.15700v1 Announce Type: new 
Abstract: Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). We introduce the concept of compatibility to show how an undirected graph can be used as a template for the structural dependencies between areal units to create sets of DAGs which, as a collection, preserve the structural dependencies represented in the template undirected graph. We then introduce three classes of compatible DAGs and corresponding algorithms for fitting MDGMs based on these classes. In addition, we compare MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15700v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Brandon Carter, Catherine A. Calder</dc:creator>
    </item>
    <item>
      <title>Bayesian modeling of multi-species labeling errors in ecological studies</title>
      <link>https://arxiv.org/abs/2406.15844</link>
      <description>arXiv:2406.15844v1 Announce Type: new 
Abstract: Ecological and conservation studies monitoring bird communities typically rely on species classification based on bird vocalizations. Historically, this has been based on expert volunteers going into the field and making lists of the bird species that they observe. Recently, machine learning algorithms have emerged that can accurately classify bird species based on audio recordings of their vocalizations. Such algorithms crucially rely on training data that are labeled by experts. Automated classification is challenging when multiple species are vocalizing simultaneously, there is background noise, and/or the bird is far from the microphone. In continuously monitoring different locations, the size of the audio data become immense and it is only possible for human experts to label a tiny proportion of the available data. In addition, experts can vary in their accuracy and breadth of knowledge about different species. This article focuses on the important problem of combining sparse expert annotations to improve bird species classification while providing uncertainty quantification. We additionally are interested in providing expert performance scores to increase their engagement and encourage improvements. We propose a Bayesian hierarchical modeling approach and evaluate this approach on a new community science platform developed in Finland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15844v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Wang, Patrik Lauha, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Clustering and Meta-Analysis Using a Mixture of Dependent Linear Tail-Free Priors</title>
      <link>https://arxiv.org/abs/2406.15912</link>
      <description>arXiv:2406.15912v1 Announce Type: new 
Abstract: We propose a novel nonparametric Bayesian approach for meta-analysis with event time outcomes. The model is an extension of linear dependent tail-free processes. The extension includes a modification to facilitate (conditionally) conjugate posterior updating and a hierarchical extension with a random partition of studies. The partition is formalized as a Dirichlet process mixture. The model development is motivated by a meta-analysis of cancer immunotherapy studies. The aim is to validate the use of relevant biomarkers in the design of immunotherapy studies. The hypothesis is about immunotherapy in general, rather than about a specific tumor type, therapy and marker. This broad hypothesis leads to a very diverse set of studies being included in the analysis and gives rise to substantial heterogeneity across studies</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15912v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernardo Flores, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>On the use of splines for representing ordered factors</title>
      <link>https://arxiv.org/abs/2406.15933</link>
      <description>arXiv:2406.15933v1 Announce Type: new 
Abstract: In the context of regression-type statistical models, the inclusion of some ordered factors among the explanatory variables requires the conversion of qualitative levels to numeric components of the linear predictor. The present note represent a follow-up of a methodology proposed by Azzalini (2023} for constructing numeric scores assigned to the factors levels. The aim of the present supplement it to allow additional flexibility of the mapping from ordered levels and numeric scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15933v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adelchi Azzalini</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Online Change Detection for Low-Rank Images</title>
      <link>https://arxiv.org/abs/2406.16136</link>
      <description>arXiv:2406.16136v1 Announce Type: new 
Abstract: We present a distribution-free CUSUM procedure designed for online change detection in a time series of low-rank images, particularly when the change causes a mean shift. We represent images as matrix data and allow for temporal dependence, in addition to inherent spatial dependence, before and after the change. The marginal distributions are assumed to be general, not limited to any specific parametric distribution. We propose new monitoring statistics that utilize the low-rank structure of the in-control mean matrix. Additionally, we study the properties of the proposed detection procedure, assessing whether the monitoring statistics effectively capture a mean shift and evaluating the rate of increase in average run length relative to the control limit in both in-control and out-of-control cases. The effectiveness of our procedure is demonstrated through simulated and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16136v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tingnan Gong, Seong-Hee Kim, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v1 Announce Type: new 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators are statistical win probability models, which fit the relationship between a binary win/loss outcome variable and certain game-state variables using data-driven regression or machine learning approaches. To illustrate just how difficult it is to accurately fit a statistical win probability model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don't achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of longitudinal treatment effects using difference-in-differences and machine learning</title>
      <link>https://arxiv.org/abs/2406.16234</link>
      <description>arXiv:2406.16234v1 Announce Type: new 
Abstract: Difference-in-differences is based on a parallel trends assumption, which states that changes over time in average potential outcomes are independent of treatment assignment, possibly conditional on covariates. With time-varying treatments, parallel trends assumptions can identify many types of parameters, but most work has focused on group-time average treatment effects and similar parameters conditional on the treatment trajectory. This paper focuses instead on identification and estimation of the intervention-specific mean - the mean potential outcome had everyone been exposed to a proposed intervention - which may be directly policy-relevant in some settings. Previous estimators for this parameter under parallel trends have relied on correctly-specified parametric models, which may be difficult to guarantee in applications. We develop multiply-robust and efficient estimators of the intervention-specific mean based on the efficient influence function, and derive conditions under which data-adaptive machine learning methods can be used to relax modeling assumptions. Our approach allows the parallel trends assumption to be conditional on the history of time-varying covariates, thus allowing for adjustment for time-varying covariates possibly impacted by prior treatments. Simulation results support the use of the proposed methods at modest sample sizes. As an example, we estimate the effect of a hypothetical federal minimum wage increase on self-rated health in the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16234v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Illenberger, Iv\'an D\'iaz, Audrey Renson</dc:creator>
    </item>
    <item>
      <title>Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data</title>
      <link>https://arxiv.org/abs/2406.16458</link>
      <description>arXiv:2406.16458v2 Announce Type: new 
Abstract: Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the "distance-based Chatterjee correlation", owing to the use here of the "distance transformed data" defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson's correlation when applied to "distance transformed" data. The next logical step is to compute Chatterjee's correlation on the same "distance transformed" data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16458v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</dc:creator>
    </item>
    <item>
      <title>Influence analyses of "designs" for evaluating inconsistency in network meta-analysis</title>
      <link>https://arxiv.org/abs/2406.16485</link>
      <description>arXiv:2406.16485v1 Announce Type: new 
Abstract: Network meta-analysis is an evidence synthesis method for comparative effectiveness analyses of multiple available treatments. To justify evidence synthesis, consistency is a relevant assumption; however, existing methods founded on statistical testing possibly have substantial limitations of statistical powers or several drawbacks in treating multi-arm studies. Besides, inconsistency is theoretically explained as design-by-treatment interactions, and the primary purpose of these analyses is prioritizing "designs" for further investigations to explore sources of biases and irregular issues that might influence the overall results. In this article, we propose an alternative framework for inconsistency evaluations using influence diagnostic methods that enable quantitative evaluations of the influences of individual designs to the overall results. We provide four new methods to quantify the influences of individual designs through a "leave-one-design-out" analysis framework. We also propose a simple summary measure, the O-value, for prioritizing designs and interpreting these influential analyses straightforwardly. Furthermore, we propose another testing approach based on the leave-one-design-out analysis framework. By applying the new methods to a network meta-analysis of antihypertensive drugs, we demonstrate the new methods located potential sources of inconsistency accurately. The proposed methods provide new insights into alternatives to existing test-based methods, especially quantifications of influences of individual designs on the overall network meta-analysis results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16485v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kotaro Sasaki, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Statistical ranking with dynamic covariates</title>
      <link>https://arxiv.org/abs/2406.16507</link>
      <description>arXiv:2406.16507v1 Announce Type: new 
Abstract: We consider a covariate-assisted ranking model grounded in the Plackett--Luce framework. Unlike existing works focusing on pure covariates or individual effects with fixed covariates, our approach integrates individual effects with dynamic covariates. This added flexibility enhances realistic ranking yet poses significant challenges for analyzing the associated estimation procedures. This paper makes an initial attempt to address these challenges. We begin by discussing the sufficient and necessary condition for the model's identifiability. We then introduce an efficient alternating maximization algorithm to compute the maximum likelihood estimator (MLE). Under suitable assumptions on the topology of comparison graphs and dynamic covariates, we establish a quantitative uniform consistency result for the MLE with convergence rates characterized by the asymptotic graph connectivity. The proposed graph topology assumption holds for several popular random graph models under optimal leading-order sparsity conditions. A comprehensive numerical study is conducted to corroborate our theoretical findings and demonstrate the application of the proposed model to real-world datasets, including horse racing and tennis competitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16507v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinjun Dong, Ruijian Han, Binyan Jiang, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>YEAST: Yet Another Sequential Test</title>
      <link>https://arxiv.org/abs/2406.16523</link>
      <description>arXiv:2406.16523v1 Announce Type: new 
Abstract: Large-scale randomised experiments have become a standard tool for developing products and improving user experience. To reduce losses from shipping harmful changes experimental results are, in practice, often checked repeatedly, which leads to inflated false alarm rates. To alleviate this problem, one can use sequential testing techniques as they control false discovery rates despite repeated checks. While multiple sequential testing methods exist in the literature, they either restrict the number of interim checks the experimenter can perform or have tuning parameters that require calibration. In this paper, we propose a novel sequential testing method that does not limit the number of interim checks and at the same time does not have any tuning parameters. The proposed method is new and does not stem from existing experiment monitoring procedures. It controls false discovery rates by ``inverting'' a bound on the threshold crossing probability derived from a classical maximal inequality. We demonstrate both in simulations and using real-world data that the proposed method outperforms current state-of-the-art sequential tests for continuous test monitoring. In addition, we illustrate the method's effectiveness with a real-world application on a major online fashion platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16523v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo</dc:creator>
    </item>
    <item>
      <title>EFECT -- A Method and Metric to Assess the Reproducibility of Stochastic Simulation Studies</title>
      <link>https://arxiv.org/abs/2406.16820</link>
      <description>arXiv:2406.16820v1 Announce Type: new 
Abstract: Reproducibility is a foundational standard for validating scientific claims in computational research. Stochastic computational models are employed across diverse fields such as systems biology, financial modelling and environmental sciences. Existing infrastructure and software tools support various aspects of reproducible model development, application, and dissemination, but do not adequately address independently reproducing simulation results that form the basis of scientific conclusions. To bridge this gap, we introduce the Empirical Characteristic Function Equality Convergence Test (EFECT), a data-driven method to quantify the reproducibility of stochastic simulation results. EFECT employs empirical characteristic functions to compare reported results with those independently generated by assessing distributional inequality, termed EFECT error, a metric to quantify the likelihood of equality. Additionally, we establish the EFECT convergence point, a metric for determining the required number of simulation runs to achieve an EFECT error value of a priori statistical significance, setting a reproducibility benchmark. EFECT supports all real-valued and bounded results irrespective of the model or method that produced them, and accommodates stochasticity from intrinsic model variability and random sampling of model inputs. We tested EFECT with stochastic differential equations, agent-based models, and Boolean networks, demonstrating its broad applicability and effectiveness. EFECT standardizes stochastic simulation reproducibility, establishing a workflow that guarantees reliable results, supporting a wide range of stakeholders, and thereby enhancing validation of stochastic simulation studies, across a model's lifecycle. To promote future standardization efforts, we are developing open source software library libSSR in diverse programming languages for easy integration of EFECT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16820v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. J. Sego, Matthias K\"onig, Luis L. Fonseca, Baylor Fain, Adam C. Knapp, Krishna Tiwari, Henning Hermjakob, Herbert M. Sauro, James A. Glazier, Reinhard C. Laubenbacher, Rahuman S. Malik-Sheriff</dc:creator>
    </item>
    <item>
      <title>Adjusting for Selection Bias Due to Missing Eligibility Criteria in Emulated Target Trials</title>
      <link>https://arxiv.org/abs/2406.16830</link>
      <description>arXiv:2406.16830v1 Announce Type: new 
Abstract: Target trial emulation (TTE) is a popular framework for observational studies based on electronic health records (EHR). A key component of this framework is determining the patient population eligible for inclusion in both a target trial of interest and its observational emulation. Missingness in variables that define eligibility criteria, however, presents a major challenge towards determining the eligible population when emulating a target trial with an observational study. In practice, patients with incomplete data are almost always excluded from analysis despite the possibility of selection bias, which can arise when subjects with observed eligibility data are fundamentally different than excluded subjects. Despite this, to the best of our knowledge, very little work has been done to mitigate this concern. In this paper, we propose a novel conceptual framework to address selection bias in TTE studies, tailored towards time-to-event endpoints, and describe estimation and inferential procedures via inverse probability weighting (IPW). Under an EHR-based simulation infrastructure, developed to reflect the complexity of EHR data, we characterize common settings under which missing eligibility data poses the threat of selection bias and investigate the ability of the proposed methods to address it. Finally, using EHR databases from Kaiser Permanente, we demonstrate the use of our method to evaluate the effect of bariatric surgery on microvascular outcomes among a cohort of severely obese patients with Type II diabetes mellitus (T2DM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Issa Dahabreh, Rui Wang, David Arterburn, Catherine Lee, Heidi Fischer, Susan Shortreed, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>On the extensions of the Chatterjee-Spearman test</title>
      <link>https://arxiv.org/abs/2406.16859</link>
      <description>arXiv:2406.16859v1 Announce Type: new 
Abstract: Chatterjee (2021) introduced a novel independence test that is rank-based, asymptotically normal and consistent against all alternatives. One limitation of Chatterjee's test is its low statistical power for detecting monotonic relationships. To address this limitation, in our previous work (Zhang, 2024, Commun. Stat. - Theory Methods), we proposed to combine Chatterjee's and Spearman's correlations into a max-type test and established the asymptotic joint normality. This work examines three key extensions of the combined test. First, motivated by its original asymmetric form, we extend the Chatterjee-Spearman test to a symmetric version, and derive the asymptotic null distribution of the symmetrized statistic. Second, we investigate the relationships between Chatterjee's correlation and other popular rank correlations, including Kendall's tau and quadrant correlation. We demonstrate that, under independence, Chatterjee's correlation and any of these rank correlations are asymptotically joint normal and independent. Simulation studies demonstrate that the Chatterjee-Kendall test has better power than the Chatterjee-Spearman test. Finally, we explore two possible extensions to the multivariate case. These extensions expand the applicability of the rank-based combined tests to a broader range of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16859v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Zhang</dc:creator>
    </item>
    <item>
      <title>How big does a population need to be before demographers can ignore individual-level randomness in demographic events?</title>
      <link>https://arxiv.org/abs/2406.15514</link>
      <description>arXiv:2406.15514v1 Announce Type: cross 
Abstract: When studying a national-level population, demographers can safely ignore the effect of individual-level randomness on age-sex structure. When studying a single community, or group of communities, however, the potential importance of individual-level randomness is less clear. We seek to measure the effect of individual-level randomness in births and deaths on standard summary indicators of age-sex structure, for populations of different sizes, focusing on on demographic conditions typical of historical populations. We conduct a microsimulation experiment where we simulate events and age-sex structure under a range of settings for demographic rates and population size. The experiment results suggest that individual-level randomness strongly affects age-sex structure for populations of about 100, but has a much smaller effect on populations of 1,000, and a negligible effect on populations of 10,000. Our conclusion is that analyses of age-sex structure in historical populations with sizes on the order 100 must account for individual-level randomness in demographic events. Analyses of populations with sizes on the order of 1,000 may need to make some allowance for individual-level variation, but other issues, such as measurement error, probably deserve more attention. Analyses of populations of 10,000 can safely ignore individual-level variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15514v1</guid>
      <category>physics.soc-ph</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Bryant (Bayesian Demography Limited, New Zealand), Tahu Kukutai (University of Waikato, New Zealand), Junni L. Zhang (Peking University, China)</dc:creator>
    </item>
    <item>
      <title>Hedging in Sequential Experiments</title>
      <link>https://arxiv.org/abs/2406.15867</link>
      <description>arXiv:2406.15867v1 Announce Type: cross 
Abstract: Experimentation involves risk. The investigator expends time and money in the pursuit of data that supports a hypothesis. In the end, the investigator may find that all of these costs were for naught and the data fail to reject the null. Furthermore, the investigator may not be able to test other hypotheses with the same data set in order to avoid false positives due to p-hacking. Therefore, there is a need for a mechanism for investigators to hedge the risk of financial and statistical bankruptcy in the business of experimentation.
  In this work, we build on the game-theoretic statistics framework to enable an investigator to hedge their bets against the null hypothesis and thus avoid ruin. First, we describe a method by which the investigator's test martingale wealth process can be capitalized by solving for the risk-neutral price. Then, we show that a portfolio that comprises the risky test martingale and a risk-free process is still a test martingale which enables the investigator to select a particular risk-return position using Markowitz portfolio theory. Finally, we show that a function that is derivative of the test martingale process can be constructed and used as a hedging instrument by the investigator or as a speculative instrument by a risk-seeking investor who wants to participate in the potential returns of the uncertain experiment wealth process. Together, these instruments enable an investigator to hedge the risk of ruin and they enable a investigator to efficiently hedge experimental risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15867v1</guid>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Cook, Patrick Flaherty</dc:creator>
    </item>
    <item>
      <title>Learning When the Concept Shifts: Confounding, Invariance, and Dimension Reduction</title>
      <link>https://arxiv.org/abs/2406.15904</link>
      <description>arXiv:2406.15904v1 Announce Type: cross 
Abstract: Practitioners often deploy a learned prediction model in a new environment where the joint distribution of covariate and response has shifted. In observational data, the distribution shift is often driven by unobserved confounding factors lurking in the environment, with the underlying mechanism unknown. Confounding can obfuscate the definition of the best prediction model (concept shift) and shift covariates to domains yet unseen (covariate shift). Therefore, a model maximizing prediction accuracy in the source environment could suffer a significant accuracy drop in the target environment. This motivates us to study the domain adaptation problem with observational data: given labeled covariate and response pairs from a source environment, and unlabeled covariates from a target environment, how can one predict the missing target response reliably? We root the adaptation problem in a linear structural causal model to address endogeneity and unobserved confounding. We study the necessity and benefit of leveraging exogenous, invariant covariate representations to cure concept shifts and improve target prediction. This further motivates a new representation learning method for adaptation that optimizes for a lower-dimensional linear subspace and, subsequently, a prediction model confined to that subspace. The procedure operates on a non-convex objective-that naturally interpolates between predictability and stability/invariance-constrained on the Stiefel manifold. We study the optimization landscape and prove that, when the regularization is sufficient, nearly all local optima align with an invariant linear subspace resilient to both concept and covariate shift. In terms of predictability, we show a model that uses the learned lower-dimensional subspace can incur a nearly ideal gap between target and source risk. Three real-world data sets are investigated to validate our method and theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15904v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kulunu Dharmakeerthi, YoonHaeng Hur, Tengyuan Liang</dc:creator>
    </item>
    <item>
      <title>Comparison of methods for mediation analysis with multiple correlated mediators</title>
      <link>https://arxiv.org/abs/2406.16174</link>
      <description>arXiv:2406.16174v1 Announce Type: cross 
Abstract: Various methods have emerged for conducting mediation analyses with multiple correlated mediators, each with distinct strengths and limitations. However, a comparative evaluation of these methods is lacking, providing the motivation for this paper. This study examines six mediation analysis methods for multiple correlated mediators that provide insights to the contributors for health disparities. We assessed the performance of each method in identifying joint or path-specific mediation effects in the context of binary outcome variables varying mediator types and levels of residual correlation between mediators. Through comprehensive simulations, the performance of six methods in estimating joint and/or path-specific mediation effects was assessed rigorously using a variety of metrics including bias, mean squared error, coverage and width of the 95$\%$ confidence intervals. Subsequently, these methods were applied to the REasons for Geographic And Racial Differences in Stroke (REGARDS) study, where differing conclusions were obtained depending on the mediation method employed. This evaluation provides valuable guidance for researchers grappling with complex multi-mediator scenarios, enabling them to select an optimal mediation method for their research question and dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16174v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mary Appah, D. Leann Long, George Howard, Melissa J. Smith</dc:creator>
    </item>
    <item>
      <title>F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data</title>
      <link>https://arxiv.org/abs/2406.16221</link>
      <description>arXiv:2406.16221v1 Announce Type: cross 
Abstract: Demand prediction is a crucial task for e-commerce and physical retail businesses, especially during high-stake sales events. However, the limited availability of historical data from these peak periods poses a significant challenge for traditional forecasting methods. In this paper, we propose a novel approach that leverages strategically chosen proxy data reflective of potential sales patterns from similar entities during non-peak periods, enriched by features learned from a graph neural networks (GNNs)-based forecasting model, to predict demand during peak events. We formulate the demand prediction as a meta-learning problem and develop the Feature-based First-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages proxy data from non-peak periods and GNN-generated relational metadata to learn feature-specific layer parameters, thereby adapting to demand forecasts for peak events. Theoretically, we show that by considering domain similarities through task-specific metadata, our model achieves improved generalization, where the excess risk decreases as the number of training tasks increases. Empirical evaluations on large-scale industrial datasets demonstrate the superiority of our approach. Compared to existing state-of-the-art models, our method demonstrates a notable improvement in demand prediction accuracy, reducing the Mean Absolute Error by 26.24% on an internal vending machine dataset and by 1.04% on the publicly accessible JD.com dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16221v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zexing Xu, Linjun Zhang, Sitan Yang, Rasoul Etesami, Hanghang Tong, Huan Zhang, Jiawei Han</dc:creator>
    </item>
    <item>
      <title>VICatMix: variational Bayesian clustering and variable selection for discrete biomedical data</title>
      <link>https://arxiv.org/abs/2406.16227</link>
      <description>arXiv:2406.16227v1 Announce Type: cross 
Abstract: Effective clustering of biomedical data is crucial in precision medicine, enabling accurate stratifiction of patients or samples. However, the growth in availability of high-dimensional categorical data, including `omics data, necessitates computationally efficient clustering algorithms. We present VICatMix, a variational Bayesian finite mixture model designed for the clustering of categorical data. The use of variational inference (VI) in its training allows the model to outperform competitors in term of efficiency, while maintaining high accuracy. VICatMix furthermore performs variable selection, enhancing its performance on high-dimensional, noisy data. The proposed model incorporates summarisation and model averaging to mitigate poor local optima in VI, allowing for improved estimation of the true number of clusters simultaneously with feature saliency. We demonstrate the performance of VICatMix with both simulated and real-world data, including applications to datasets from The Cancer Genome Atlas (TCGA), showing its use in cancer subtyping and driver gene discovery. We demonstrate VICatMix's utility in integrative cluster analysis with different `omics datasets, enabling the discovery of novel subtypes.
  \textbf{Availability:} VICatMix is freely available as an R package, incorporating C++ for faster computation, at \url{https://github.com/j-ackierao/VICatMix}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16227v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul D. W. Kirk, Jackie Rao</dc:creator>
    </item>
    <item>
      <title>Position: Benchmarking is Limited in Reinforcement Learning Research</title>
      <link>https://arxiv.org/abs/2406.16241</link>
      <description>arXiv:2406.16241v1 Announce Type: cross 
Abstract: Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16241v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott M. Jordan, Adam White, Bruno Castro da Silva, Martha White, Philip S. Thomas</dc:creator>
    </item>
    <item>
      <title>METRIK: Measurement-Efficient Randomized Controlled Trials using Transformers with Input Masking</title>
      <link>https://arxiv.org/abs/2406.16351</link>
      <description>arXiv:2406.16351v1 Announce Type: cross 
Abstract: Clinical randomized controlled trials (RCTs) collect hundreds of measurements spanning various metric types (e.g., laboratory tests, cognitive/motor assessments, etc.) across 100s-1000s of subjects to evaluate the effect of a treatment, but do so at the cost of significant trial expense. To reduce the number of measurements, trial protocols can be revised to remove metrics extraneous to the study's objective, but doing so requires additional human labor and limits the set of hypotheses that can be studied with the collected data. In contrast, a planned missing design (PMD) can reduce the amount of data collected without removing any metric by imputing the unsampled data. Standard PMDs randomly sample data to leverage statistical properties of imputation algorithms, but are ad hoc, hence suboptimal. Methods that learn PMDs produce more sample-efficient PMDs, but are not suitable for RCTs because they require ample prior data (150+ subjects) to model the data distribution. Therefore, we introduce a framework called Measurement EfficienT Randomized Controlled Trials using Transformers with Input MasKing (METRIK), which, for the first time, calculates a PMD specific to the RCT from a modest amount of prior data (e.g., 60 subjects). Specifically, METRIK models the PMD as a learnable input masking layer that is optimized with a state-of-the-art imputer based on the Transformer architecture. METRIK implements a novel sampling and selection algorithm to generate a PMD that satisfies the trial designer's objective, i.e., whether to maximize sampling efficiency or imputation performance for a given sampling budget. Evaluated across five real-world clinical RCT datasets, METRIK increases the sampling efficiency of and imputation performance under the generated PMD by leveraging correlations over time and across metrics, thereby removing the need to manually remove metrics from the RCT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16351v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayeri Lala (Princeton University, Princeton, USA), Niraj K. Jha (Princeton University, Princeton, USA)</dc:creator>
    </item>
    <item>
      <title>CLEAR: Can Language Models Really Understand Causal Graphs?</title>
      <link>https://arxiv.org/abs/2406.16605</link>
      <description>arXiv:2406.16605v1 Announce Type: cross 
Abstract: Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models' understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models' behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at https://github.com/OpenCausaLab/CLEAR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16605v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu</dc:creator>
    </item>
    <item>
      <title>CausalFormer: An Interpretable Transformer for Temporal Causal Discovery</title>
      <link>https://arxiv.org/abs/2406.16708</link>
      <description>arXiv:2406.16708v1 Announce Type: cross 
Abstract: Temporal causal discovery is a crucial task aimed at uncovering the causal relations within time series data. The latest temporal causal discovery methods usually train deep learning models on prediction tasks to uncover the causality between time series. They capture causal relations by analyzing the parameters of some components of the trained models, e.g., attention weights and convolution weights. However, this is an incomplete mapping process from the model parameters to the causality and fails to investigate the other components, e.g., fully connected layers and activation functions, that are also significant for causal discovery. To facilitate the utilization of the whole deep learning models in temporal causal discovery, we proposed an interpretable transformer-based causal discovery model termed CausalFormer, which consists of the causality-aware transformer and the decomposition-based causality detector. The causality-aware transformer learns the causal representation of time series data using a prediction task with the designed multi-kernel causal convolution which aggregates each input time series along the temporal dimension under the temporal priority constraint. Then, the decomposition-based causality detector interprets the global structure of the trained causality-aware transformer with the proposed regression relevance propagation to identify potential causal relations and finally construct the causal graph. Experiments on synthetic, simulated, and real datasets demonstrate the state-of-the-art performance of CausalFormer on discovering temporal causality. Our code is available at https://github.com/lingbai-kong/CausalFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16708v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingbai Kong, Wengen Li, Hanchen Yang, Yichao Zhang, Jihong Guan, Shuigeng Zhou</dc:creator>
    </item>
    <item>
      <title>Semiparametric Estimation of Treatment Effects in Observational Studies with Heterogeneous Partial Interference</title>
      <link>https://arxiv.org/abs/2107.12420</link>
      <description>arXiv:2107.12420v3 Announce Type: replace 
Abstract: In many observational studies in social science and medicine, subjects or units are connected, and one unit's treatment and attributes may affect another's treatment and outcome, violating the stable unit treatment value assumption (SUTVA) and resulting in interference. To enable feasible estimation and inference, many previous works assume exchangeability of interfering units (neighbors). However, in many applications with distinctive units, interference is heterogeneous and needs to be modeled explicitly. In this paper, we focus on the partial interference setting, and only restrict units to be exchangeable conditional on observable characteristics. Under this framework, we propose generalized augmented inverse propensity weighted (AIPW) estimators for general causal estimands that include heterogeneous direct and spillover effects. We show that they are semiparametric efficient and robust to heterogeneous interference as well as model misspecifications. We apply our methods to the Add Health dataset to study the direct effects of alcohol consumption on academic performance and the spillover effects of parental incarceration on adolescent well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.12420v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaonan Qu, Ruoxuan Xiong, Jizhou Liu, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Re-thinking Spatial Confounding in Spatial Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2301.05743</link>
      <description>arXiv:2301.05743v2 Announce Type: replace 
Abstract: In the last two decades, considerable research has been devoted to a phenomenon known as spatial confounding. Spatial confounding is thought to occur when there is multicollinearity between a covariate and the random effect in a spatial regression model. This multicollinearity is considered highly problematic when the inferential goal is estimating regression coefficients and various methodologies have been proposed to attempt to alleviate it. Recently, it has become apparent that many of these methodologies are flawed, yet the field continues to expand. In this paper, we offer a novel perspective of synthesizing the work in the field of spatial confounding. We propose that at least two distinct phenomena are currently conflated with the term spatial confounding. We refer to these as the ``analysis model'' and the ``data generation'' types of spatial confounding. We show that these two issues can lead to contradicting conclusions about whether spatial confounding exists and whether methods to alleviate it will improve inference. Our results also illustrate that in most cases, traditional spatial linear mixed models do help to improve inference on regression coefficients. Drawing on the insights gained, we offer a path forward for research in spatial confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05743v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kori Khan, Candace Berrett</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Distributed Estimation and Inference for Cox's Model</title>
      <link>https://arxiv.org/abs/2302.12111</link>
      <description>arXiv:2302.12111v3 Announce Type: replace 
Abstract: Motivated by multi-center biomedical studies that cannot share individual data due to privacy and ownership concerns, we develop communication-efficient iterative distributed algorithms for estimation and inference in the high-dimensional sparse Cox proportional hazards model. We demonstrate that our estimator, even with a relatively small number of iterations, achieves the same convergence rate as the ideal full-sample estimator under very mild conditions. To construct confidence intervals for linear combinations of high-dimensional hazard regression coefficients, we introduce a novel debiased method, establish central limit theorems, and provide consistent variance estimators that yield asymptotically valid distributed confidence intervals. In addition, we provide valid and powerful distributed hypothesis tests for any coordinate element based on a decorrelated score test. We allow time-dependent covariates as well as censored survival times. Extensive numerical experiments on both simulated and real data lend further support to our theory and demonstrate that our communication-efficient distributed estimators, confidence intervals, and hypothesis tests improve upon alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12111v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Bayle, Jianqing Fan, Zhipeng Lou</dc:creator>
    </item>
    <item>
      <title>Accommodating informative visit times for analysing irregular longitudinal data: a sensitivity analysis approach with balancing weights estimators</title>
      <link>https://arxiv.org/abs/2305.16018</link>
      <description>arXiv:2305.16018v2 Announce Type: replace 
Abstract: Irregular longitudinal data with informative visit times arise when patients' visits are partly driven by concurrent disease outcomes. However, existing methods such as inverse intensity weighting (IIW), often overlook or have not adequately assess the influence of informative visit times on estimation and inference. Based on novel balancing weights estimators, we propose a new sensitivity analysis approach to addressing informative visit times within the IIW framework. The balancing weights are obtained by balancing observed history variable distributions over time and including a selection function with specified sensitivity parameters to characterise the additional influence of the concurrent outcome on the visit process. A calibration procedure is proposed to anchor the range of the sensitivity parameters to the amount of variation in the visit process that could be additionally explained by the concurrent outcome given the observed history and time. Simulations demonstrate that our balancing weights estimators outperform existing weighted estimators for robustness and efficiency. We provide an R Markdown tutorial of the proposed methods and apply them to analyse data from a clinic-based cohort of psoriatic arthritis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16018v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean Yiu, Li Su</dc:creator>
    </item>
    <item>
      <title>Similarity-based Random Partition Distribution for Clustering Functional Data</title>
      <link>https://arxiv.org/abs/2308.01704</link>
      <description>arXiv:2308.01704v3 Announce Type: replace 
Abstract: Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extended generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP), to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production as well as incorporates pairwise similarity information to ensure accurate and meaningful grouping. The theoretical properties of the SGDP are studied. Then, SGDP-based random partition is applied to a real-world dataset of hourly population flow in $500\text{m}^2$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed SGDP will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01704v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa, Genya Kobayashi</dc:creator>
    </item>
    <item>
      <title>Scalable Composite Likelihood Estimation of Probit Models with Crossed Random Effects</title>
      <link>https://arxiv.org/abs/2308.15681</link>
      <description>arXiv:2308.15681v2 Announce Type: replace 
Abstract: Crossed random effects structures arise in many scientific contexts. They raise severe computational problems with likelihood computations scaling like $N^{3/2}$ or worse for $N$ data points. In this paper we develop a new composite likelihood approach for crossed random effects probit models. For data arranged in R rows and C columns, the likelihood function includes a very difficult R + C dimensional integral. The composite likelihood we develop uses the marginal distribution of the response along with two hierarchical models. The cost is reduced to $\mathcal{O}(N)$ and it can be computed with $R + C$ one dimensional integrals. We find that the commonly used Laplace approximation has a cost that grows superlinearly. We get consistent estimates of the probit slope and variance components from our composite likelihood algorithm. We also show how to estimate the covariance of the estimated regression coefficients. The algorithm scales readily to a data set of five million observations from Stitch Fix with $R + C &gt; 700{,}000$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15681v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruggero Bellio, Swarnadip Ghosh, Art B. Owen, Cristiano Varin</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis of Inverse Probability Weighting Estimators of Causal Effects in Observational Studies with Multivalued Treatments</title>
      <link>https://arxiv.org/abs/2308.15986</link>
      <description>arXiv:2308.15986v4 Announce Type: replace 
Abstract: One of the fundamental challenges in drawing causal inferences from observational studies is that the assumption of no unmeasured confounding is not testable from observed data. Therefore, assessing sensitivity to this assumption's violation is important to obtain valid causal conclusions in observational studies. Although several sensitivity analysis frameworks are available in the casual inference literature, very few of them are applicable to observational studies with multivalued treatments. To address this issue, we propose a sensitivity analysis framework for performing sensitivity analysis in multivalued treatment settings. Within this framework, a general class of additive causal estimands has been proposed. We demonstrate that the estimation of the causal estimands under the proposed sensitivity model can be performed very efficiently. Simulation results show that the proposed framework performs well in terms of bias of the point estimates and coverage of the confidence intervals when there is sufficient overlap in the covariate distributions. We illustrate the application of our proposed method by conducting an observational study that estimates the causal effect of fish consumption on blood mercury levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15986v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Abdul Basit, Mahbub A. H. M. Latif, Abdus S Wahed</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Bures-Wasserstein Flows</title>
      <link>https://arxiv.org/abs/2310.13764</link>
      <description>arXiv:2310.13764v2 Announce Type: replace 
Abstract: We develop a statistical framework for conducting inference on collections of time-varying covariance operators (covariance flows) over a general, possibly infinite dimensional, Hilbert space. We model the intrinsically non-linear structure of covariances by means of the Bures-Wasserstein metric geometry. We make use of the Riemmanian-like structure induced by this metric to define a notion of mean and covariance of a random flow, and develop an associated Karhunen-Lo\`eve expansion. We then treat the problem of estimation and construction of functional principal components from a finite collection of covariance flows, observed fully or irregularly.
  Our theoretical results are motivated by modern problems in functional data analysis, where one observes operator-valued random processes -- for instance when analysing dynamic functional connectivity and fMRI data, or when analysing multiple functional time series in the frequency domain. Nevertheless, our framework is also novel in the finite-dimensions (matrix case), and we demonstrate what simplifications can be afforded then. We illustrate our methodology by means of simulations and data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13764v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo V. Santoro, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Assessing the Unobserved: Enhancing Causal Inference in Sociology with Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2311.13410</link>
      <description>arXiv:2311.13410v2 Announce Type: replace 
Abstract: Explaining social events is a primary objective of applied data-driven sociology. To achieve that objective, many sociologists use statistical causal inference to identify causality using observational studies research context where the analyst does not control the data generating process. However, it is often challenging in observation studies to satisfy the unmeasured confounding assumption, namely, that there is no lurking third variable affecting the causal relationship of interest. In this article, we develop a framework enabling sociologists to employ a different strategy to enhance the quality of observational studies. Our framework builds on a surprisingly simple statistical approach, sensitivity analysis: a thought-experimental framework where the analyst imagines a lever, which they can pull for probing a variety of theoretically driven statistical magnitudes of posited unmeasured confounding which in turn distorts the causal effect of interest. By pulling that lever, the analyst can identify how strong an unmeasured confounder must be to wash away the estimated causal effect. Although each sensitivity analysis method requires its own assumptions, this sort of post-hoc analysis provides underutilized tools to bound causal quantities. Extending Lundberg et al, we develop a five-step approach to how applied sociological research can incorporate sensitivity analysis, empowering scholars to rejuvenate causal inference in observational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13410v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheng Lin, Jose M. Pena, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data</title>
      <link>https://arxiv.org/abs/2311.14889</link>
      <description>arXiv:2311.14889v2 Announce Type: replace 
Abstract: In this paper we review recent advances in statistical methods for the evaluation of the heterogeneity of treatment effects (HTE), including subgroup identification and estimation of individualized treatment regimens, from randomized clinical trials and observational studies. We identify several types of approaches using the features introduced in Lipkovich, Dmitrienko and D'Agostino (2017) that distinguish the recommended principled methods from basic methods for HTE evaluation that typically rely on rules of thumb and general guidelines (the methods are often referred to as common practices). We discuss the advantages and disadvantages of various principled methods as well as common measures for evaluating their performance. We use simulated data and a case study based on a historical clinical trial to illustrate several new approaches to HTE evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14889v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ilya Lipkovich, David Svensson, Bohdana Ratitch, Alex Dmitrienko</dc:creator>
    </item>
    <item>
      <title>Prediction of causal genes at GWAS loci with pleiotropic gene regulatory effects using sets of correlated instrumental variables</title>
      <link>https://arxiv.org/abs/2401.06261</link>
      <description>arXiv:2401.06261v2 Announce Type: replace 
Abstract: Multivariate Mendelian randomization (MVMR) is a statistical technique that uses sets of genetic instruments to estimate the direct causal effects of multiple exposures on an outcome of interest. At genomic loci with pleiotropic gene regulatory effects, that is, loci where the same genetic variants are associated to multiple nearby genes, MVMR can potentially be used to predict candidate causal genes. However, consensus in the field dictates that the genetic instruments in MVMR must be independent, which is usually not possible when considering a group of candidate genes from the same locus. We used causal inference theory to show that MVMR with correlated instruments satisfies the instrumental set condition. This is a classical result by Brito and Pearl (2002) for structural equation models that guarantees the identifiability of causal effects in situations where multiple exposures collectively, but not individually, separate a set of instrumental variables from an outcome variable. Extensive simulations confirmed the validity and usefulness of these theoretical results even at modest sample sizes. Importantly, the causal effect estimates remain unbiased and their variance small when instruments are highly correlated. We applied MVMR with correlated instrumental variable sets at risk loci from genome-wide association studies (GWAS) for coronary artery disease using eQTL data from the STARNET study. Our method predicts causal genes at twelve loci, each associated with multiple colocated genes in multiple tissues. However, the extensive degree of regulatory pleiotropy across tissues and the limited number of causal variants in each locus still require that MVMR is run on a tissue-by-tissue basis, and testing all gene-tissue pairs at a given locus in a single model to predict causal gene-tissue combinations remains infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06261v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mariyam Khan, Adriaan-Alexander Ludl, Sean Bankier, Johan Bjorkegren, Tom Michoel</dc:creator>
    </item>
    <item>
      <title>Informative Simultaneous Confidence Intervals for Graphical Test Procedures</title>
      <link>https://arxiv.org/abs/2402.13719</link>
      <description>arXiv:2402.13719v3 Announce Type: replace 
Abstract: Simultaneous confidence intervals (SCIs) that are compatible with a given closed test procedure are often non-informative. More precisely, for a one-sided null hypothesis, the bound of the SCI can stick to the border of the null hypothesis, irrespective of how far the point estimate deviates from the null hypothesis. This has been illustrated for the Bonferroni-Holm and fall-back procedures, for which alternative SCIs have been suggested, that are free of this deficiency. These informative SCIs are not fully compatible with the initial multiple test, but are close to it and hence provide similar power advantages. They provide a multiple hypothesis test with strong family-wise error rate control that can be used in replacement of the initial multiple test. The current paper extends previous work for informative SCIs to graphical test procedures. The information gained from the newly suggested SCIs is shown to be always increasing with increasing evidence against a null hypothesis. The new SCIs provide a compromise between information gain and the goal to reject as many hypotheses as possible. The SCIs are defined via a family of dual graphs and the projection method. A simple iterative algorithm for the computation of the intervals is provided. A simulation study illustrates the results for a complex graphical test procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13719v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Werner Brannath, Liane Kluge, Martin Scharpenberg</dc:creator>
    </item>
    <item>
      <title>Comparing statistical likelihoods with diagnostic probabilities based on directly observed proportions to help understand the replication crisis</title>
      <link>https://arxiv.org/abs/2403.16906</link>
      <description>arXiv:2403.16906v3 Announce Type: replace 
Abstract: Diagnosticians use an observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician might regard a continuous Gaussian probability distribution of possible numerical outcomes conditional on the information in the study methods and data as posterior probabilities. Similarly, they might regard the distribution of possible means based on a SEM as a posterior probability distribution too. If the converse likelihood distribution of the observed mean conditional on any hypothetical mean (e.g. the null hypothesis) is assumed to be the same as the above posterior distribution (as is customary) then by Bayes rule, the prior distribution of all possible hypothetical means is uniform. It follows that the probability Q of any theoretically true mean falling into a tail beyond a null hypothesis would be equal to that tails area as a proportion of the whole. It also follows that the P value (the probability of the observed mean or something more extreme conditional on the null hypothesis) is equal to Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. So, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying this double variance to achieve a power of 80%, the required number of observations is doubled compared to conventional approaches. If some replicating study is to achieve a P value of up to 0.025 yet again with a probability of 0.8, then this requires 3 times as many observations in the power calculation. This might explain the replication crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16906v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huw Llewelyn</dc:creator>
    </item>
    <item>
      <title>Generative Model for Change Point Detection in Dynamic Graphs</title>
      <link>https://arxiv.org/abs/2404.04719</link>
      <description>arXiv:2404.04719v2 Announce Type: replace 
Abstract: This paper proposes a generative model to detect change points in time series of graphs. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that can generate graphs from the latent representations. The informative prior distributions in the latent spaces are learned from the observed data as empirical Bayes, and the expressive power of generative model is exploited to assist multiple change point detection. Specifically, the model parameters are learned via maximum approximate likelihood, with a Group Fused Lasso regularization on the prior parameters. The optimization problem is then solved via Alternating Direction Method of Multipliers (ADMM), and Langevin Dynamics are recruited for posterior inference. Experiments in both simulated and real data demonstrate the ability of the generative model in supporting change point detection with good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04719v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Jialiang Li, Hangjian Li, Yanzhen Chen, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials</title>
      <link>https://arxiv.org/abs/2405.02529</link>
      <description>arXiv:2405.02529v4 Announce Type: replace 
Abstract: As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02529v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Utkarsh Chauhan, Daylen Mackey, John R. Mackey</dc:creator>
    </item>
    <item>
      <title>Causal Inference on Process Graphs, Part I: The Structural Equation Process Representation</title>
      <link>https://arxiv.org/abs/2305.11561</link>
      <description>arXiv:2305.11561v2 Announce Type: replace-cross 
Abstract: When dealing with time series data, causal inference methods often employ structural vector autoregressive (SVAR) processes to model time-evolving random systems. In this work, we rephrase recursive SVAR processes with possible latent component processes as a linear Structural Causal Model (SCM) of stochastic processes on a simple causal graph, the \emph{process graph}, that models every process as a single node. Using this reformulation, we generalise Wright's well-known path-rule for linear Gaussian SCMs to the newly introduced process SCMs and we express the auto-covariance sequence of an SVAR process by means of a generalised trek-rule. Employing the Fourier-Transformation, we derive compact expressions for causal effects in the frequency domain that allow us to efficiently visualise the causal interactions in a multivariate SVAR process. Finally, we observe that the process graph can be used to formulate graphical criteria for identifying causal effects and to derive algebraic relations with which these frequency domain causal effects can be recovered from the observed spectral density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11561v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas-Domenic Reiter, Andreas Gerhardus, Jonas Wahl, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>Convergence rates of non-stationary and deep Gaussian process regression</title>
      <link>https://arxiv.org/abs/2312.07320</link>
      <description>arXiv:2312.07320v3 Announce Type: replace-cross 
Abstract: The focus of this work is the convergence of non-stationary and deep Gaussian process regression. More precisely, we follow a Bayesian approach to regression or interpolation, where the prior placed on the unknown function $f$ is a non-stationary or deep Gaussian process, and we derive convergence rates of the posterior mean to the true function $f$ in terms of the number of observed training points. In some cases, we also show convergence of the posterior variance to zero. The only assumption imposed on the function $f$ is that it is an element of a certain reproducing kernel Hilbert space, which we in particular cases show to be norm-equivalent to a Sobolev space. Our analysis includes the case of estimated hyper-parameters in the covariance kernels employed, both in an empirical Bayes' setting and the particular hierarchical setting constructed through deep Gaussian processes. We consider the settings of noise-free or noisy observations on deterministic or random training points. We establish general assumptions sufficient for the convergence of deep Gaussian process regression, along with explicit examples demonstrating the fulfilment of these assumptions. Specifically, our examples require that the H\"older or Sobolev norms of the penultimate layer are bounded almost surely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07320v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Moriarty-Osborne, Aretha L. Teckentrup</dc:creator>
    </item>
    <item>
      <title>Discovering influential text using convolutional neural networks</title>
      <link>https://arxiv.org/abs/2406.10086</link>
      <description>arXiv:2406.10086v2 Announce Type: replace-cross 
Abstract: Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two datasets. The first enables direct validation of the model's ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10086v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Megan Ayers, Luke Sanford, Margaret Roberts, Eddie Yang</dc:creator>
    </item>
    <item>
      <title>Deep Optimal Experimental Design for Parameter Estimation Problems</title>
      <link>https://arxiv.org/abs/2406.14003</link>
      <description>arXiv:2406.14003v2 Announce Type: replace-cross 
Abstract: Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14003v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Shahriar Rahim Siddiqui, Arman Rahmim, Eldad Haber</dc:creator>
    </item>
  </channel>
</rss>
