<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Nov 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Marionette: Data Structure Description and Management for Heterogeneous Computing</title>
      <link>https://arxiv.org/abs/2511.04853</link>
      <description>arXiv:2511.04853v1 Announce Type: new 
Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might be extremely challenging, particularly when targeting heterogeneous platforms such as GPUs. Marionette is a C++17 library designed to address this by enabling flexible, efficient, and portable data structure definitions. It decouples data layout from the description of the interface, supports multiple memory management strategies, and provides efficient data transfers and conversions across devices, all of this with minimal runtime overhead due to the compile-time nature of its abstractions. By allowing interfaces to be augmented with arbitrary functions, Marionette maintains compatibility with existing code and offers a streamlined interface that supports both straightforward and advanced use cases. This paper outlines its design, usage, and performance, including a CUDA-based case study demonstrating its efficiency and flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04853v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nuno dos Santos Fernandes, Pedro Tom\'as, Nuno Roma, Frank Winklmeier, Patricia Conde-Mu\'i\~no</dc:creator>
    </item>
    <item>
      <title>Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs</title>
      <link>https://arxiv.org/abs/2511.05053</link>
      <description>arXiv:2511.05053v1 Announce Type: new 
Abstract: Machine learning based on neural networks has advanced rapidly, but the high energy consumption required for training and inference remains a major challenge. Hyperdimensional Computing (HDC) offers a lightweight, brain-inspired alternative that enables high parallelism but often suffers from lower accuracy on complex visual tasks. To overcome this, hybrid accelerators combining HDC and Convolutional Neural Networks (CNNs) have been proposed, though their adoption is limited by poor generalizability and programmability. The rise of open-source RISC-V architectures has created new opportunities for domain-specific GPU design. Unlike traditional proprietary GPUs, emerging RISC-V-based GPUs provide flexible, programmable platforms suitable for custom computation models such as HDC. In this study, we design and implement custom GPU instructions optimized for HDC operations, enabling efficient processing for hybrid HDC-CNN workloads. Experimental results using four types of custom HDC instructions show a performance improvement of up to 56.2 times in microbenchmark tests, demonstrating the potential of RISC-V GPUs for energy-efficient, high-performance computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05053v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wakuto Matsumi, Riaz-Ul-Haque Mian</dc:creator>
    </item>
    <item>
      <title>GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters</title>
      <link>https://arxiv.org/abs/2511.05067</link>
      <description>arXiv:2511.05067v1 Announce Type: new 
Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers and high-performance computing (HPC) systems, enabling the fast execution of compute-intensive applications, such as Convolutional Neural Networks (CNNs). However, sustained workloads can impose significant stress on GPU components, raising reliability concerns due to potential faults that corrupt the intermediate application computations, leading to incorrect results. Estimating the stress induced by an application is thus crucial to predict reliability (with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online telemetry parameters and hardware performance counters to assess GPU stress induced by different applications. The experimental results indicate the stress induced by a parallel workload can be estimated by combining telemetry data and Performance Counters that reveal the efficiency in the resource usage of the target workload. For this purpose the selected performance counters focus on measuring the i) throughput, ii) amount of issued instructions and iii) stall events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05067v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>20th International Conference on Design, Test &amp; Technology of Integrated Systems (DTTIS) 2025</arxiv:journal_reference>
      <dc:creator>Giuseppe Esposito, Juan-David Guerrero-Balaguera, Josie Esteban Rodriguez Condia, Matteo Sonza Reorda, Marco Barbiero, Rossella Fortuna</dc:creator>
    </item>
    <item>
      <title>The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective</title>
      <link>https://arxiv.org/abs/2511.04946</link>
      <description>arXiv:2511.04946v1 Announce Type: cross 
Abstract: Fully Homomorphic Encryption (FHE) allows computations to be performed on encrypted data, significantly enhancing user privacy. However, the I/O challenges associated with deploying FHE applications remains understudied. We analyze the impact of storage I/O on the performance of FHE applications and summarize key lessons from the status quo. Key results include that storage I/O can degrade the performance of ASICs by as much as 357$\times$ and reduce GPUs performance by up to 22$\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04946v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>Advanced Parallel Processing Technologies (2025) 337-351</arxiv:journal_reference>
      <dc:creator>Lei Chen, Erci Xu, Yiming Sun, Shengyu Fan, Xianglong Deng, Guiming Shi, Guang Fan, Liang Kong, Yilan Zhu, Shoumeng Yan, Mingzhe Zhang</dc:creator>
    </item>
    <item>
      <title>CUNQA: a Distributed Quantum Computing emulator for HPC</title>
      <link>https://arxiv.org/abs/2511.05209</link>
      <description>arXiv:2511.05209v1 Announce Type: cross 
Abstract: The challenge of scaling quantum computers to gain computational power is expected to lead to architectures with multiple connected quantum processing units (QPUs), commonly referred to as Distributed Quantum Computing (DQC). In parallel, there is a growing momentum toward treating quantum computers as accelerators, integrating them into the heterogeneous architectures of high-performance computing (HPC) environments. This work combines these two foreseeable futures in CUNQA, an open-source DQC emulator designed for HPC environments that allows testing, evaluating and studying DQC in HPC before it even becomes real. It implements the three DQC models of no-communication, classical-communication and quantum-communication; which will be examined in this work. Addressing programming considerations, explaining emulation and simulation details, and delving into the specifics of the implementation will be part of the effort. The well-known Quantum Phase Estimation (QPE) algorithm is used to demonstrate and analyze the emulation of the models. To the best of our knowledge, CUNQA is the first tool designed to emulate the three DQC schemes in an HPC environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05209v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jorge V\'azquez-P\'erez, Daniel Exp\'osito-Pati\~no, Marta Losada, \'Alvaro Carballido, Andr\'es G\'omez, Tom\'as F. Pena</dc:creator>
    </item>
    <item>
      <title>Almost Time-Optimal Loosely-Stabilizing Leader Election on Arbitrary Graphs Without Identifiers in Population Protocols</title>
      <link>https://arxiv.org/abs/2411.03902</link>
      <description>arXiv:2411.03902v2 Announce Type: replace 
Abstract: The population protocol model is a computational model for passive mobile agents. We address the leader election problem, which determines a unique leader on arbitrary communication graphs starting from any configuration. Unfortunately, self-stabilizing leader election is impossible to be solved without knowing the exact number of agents; thus, we consider loosely-stabilizing leader election, which converges to safe configurations in a relatively short time, and holds the specification (maintains a unique leader) for a relatively long time. When agents have unique identifiers, Sudo et al.(2019) proposed a protocol that, given an upper bound $N$ for the number of agents $n$, converges in $O(mN\log n)$ expected steps, where $m$ is the number of edges. When unique identifiers are not required, they also proposed a protocol that, using random numbers and given $N$, converges in $O(mN^2\log{N})$ expected steps. Both protocols have a holding time of $\Omega(e^{2N})$ expected steps and use $O(\log{N})$ bits of memory. They also showed that the lower bound of the convergence time is $\Omega(mN)$ expected steps for protocols with a holding time of $\Omega(e^N)$ expected steps given $N$.
  In this paper, we propose protocols that do not require unique identifiers. These protocols achieve convergence times close to the lower bound with increasing memory usage. Specifically, given $N$ and an upper bound $\Delta$ for the maximum degree, we propose two protocols whose convergence times are $O(mN\log n)$ and $O(mN\log N)$ both in expectation and with high probability. The former protocol uses random numbers, while the latter does not require them. Both protocols utilize $O(\Delta \log N)$ bits of memory and hold the specification for $\Omega(e^{2N})$ expected steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03902v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haruki Kanaya, Ryota Eguchi, Taisho Sasada, Michiko Inoue</dc:creator>
    </item>
    <item>
      <title>OptiLog: Assigning Roles in Byzantine Consensus</title>
      <link>https://arxiv.org/abs/2502.15428</link>
      <description>arXiv:2502.15428v2 Announce Type: replace 
Abstract: Byzantine Fault-Tolerant (BFT) protocols play an important role in blockchains. As the deployment of such systems extends to wide-area networks, the scalability of BFT protocols becomes a critical concern. Optimizations that assign specific roles to individual replicas can significantly improve the performance of BFT systems. However, such role assignment is highly sensitive to faults, potentially undermining the optimizations' effectiveness. To address these challenges, we present OptiLog, a logging framework for collecting and analyzing measurements that help to assign roles in globally distributed systems, despite the presence of faults. OptiLog presents local measurements in global data structures, to enable consistent decisions and hold replicas accountable if they do not perform according to their reported measurements. We demonstrate OptiLog's flexibility by applying it to two BFT protocols: (1) Aware, a highly optimized PBFT-like protocol, and (2) Kauri, a tree-based protocol designed for large-scale deployments. OptiLog detects and excludes replicas that misbehave during consensus and thus enables the system to operate in an optimized, low-latency configuration, even under adverse conditions. Experiments show that for tree overlays deployed across 73 worldwide cities, trees found by OptiLog display 39% lower latency than Kauri.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15428v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3767295.3769342</arxiv:DOI>
      <dc:creator>Hanish Gogada, Christian Berger, Leander Jehl, Hans P. Reiser, Hein Meling</dc:creator>
    </item>
    <item>
      <title>SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference</title>
      <link>https://arxiv.org/abs/2505.24095</link>
      <description>arXiv:2505.24095v2 Announce Type: replace 
Abstract: Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24095v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tian Xia, Ziming Mao, Jamison Kerney, Ethan J. Jackson, Zhifei Li, Jiarong Xing, Scott Shenker, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM</title>
      <link>https://arxiv.org/abs/2511.03293</link>
      <description>arXiv:2511.03293v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with Neural Processing Units (NPUs), yet the decode phase remains memory-intensive, limiting performance. Processing-in-Memory (PIM) offers a promising solution, but co-executing NPU-PIM systems face challenges such as data layout mismatches, bandwidth loss, and redundant storage. To address these issues, we propose UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major, tile-based layout and a configurable DRAM mapping strategy to ensure compatibility with NPU computation while maximizing PIM efficiency -- without introducing extra memory overhead or bandwidth loss. Comprehensive evaluations on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving end-to-end LLM inference efficiency on edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03293v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu</dc:creator>
    </item>
    <item>
      <title>LLM4FaaS: No-Code Application Development using LLMs and FaaS</title>
      <link>https://arxiv.org/abs/2502.14450</link>
      <description>arXiv:2502.14450v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) show great capabilities in generating code from natural language descriptions, bringing programming power closer to non-technical users. However, their lack of expertise in operating the generated code remains a key barrier to realizing customized applications. Function-as-a-Service (FaaS) platforms offer a high level of abstraction for code execution and deployment, allowing users to run LLM-generated code without requiring technical expertise or incurring operational overhead.
  In this paper, we present LLM4FaaS, a no-code application development approach that integrates LLMs and FaaS platforms to enable non-technical users to build and run customized applications using only natural language. By deploying LLM-generated code through FaaS, LLM4FaaS abstracts away infrastructure management and boilerplate code generation. We implement a proof-of-concept prototype based on an open-source FaaS platform, and evaluate it using real prompts from non-technical users. Experiments with GPT-4o show that LLM4FaaS can automatically build and deploy code in 71.47% of cases, outperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform at 14.55%, narrowing the gap to human performance at 88.99%. Further analysis of code quality, programming language diversity, latency, and consistency demonstrates a balanced performance in terms of efficiency, maintainability and availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14450v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghe Wang, Tobias Pfandzelter, Trever Schirmer, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Distributed Stochastic Momentum Tracking with Local Updates: Achieving Optimal Communication and Iteration Complexities</title>
      <link>https://arxiv.org/abs/2510.24155</link>
      <description>arXiv:2510.24155v2 Announce Type: replace-cross 
Abstract: We propose Local Momentum Tracking (LMT), a novel distributed stochastic gradient method for solving distributed optimization problems over networks. To reduce communication overhead, LMT enables each agent to perform multiple local updates between consecutive communication rounds. Specifically, LMT integrates local updates with the momentum tracking strategy and the Loopless Chebyshev Acceleration (LCA) technique. We demonstrate that LMT achieves linear speedup with respect to the number of local updates as well as the number of agents for minimizing smooth objective functions with and without the Polyak-{\L}ojasiewicz (PL) condition. Notably, with sufficiently many local updates $Q\geq Q^*$, LMT attains the optimal communication complexity. For a moderate number of local updates $Q\in[1,Q^*]$, LMT achieves the optimal iteration complexity. To our knowledge, LMT is the first distributed stochastic gradient method with local updates that enjoys such properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24155v2</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Huang, Shi Pu</dc:creator>
    </item>
    <item>
      <title>Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture</title>
      <link>https://arxiv.org/abs/2511.03359</link>
      <description>arXiv:2511.03359v2 Announce Type: replace-cross 
Abstract: We have developed a new version of the high-performance J\"ulich universal quantum computer simulator (JUQCS-50) that leverages key features of the GH200 superchips as used in the JUPITER supercomputer, enabling simulations of a 50-qubit universal quantum computer for the first time. JUQCS-50 achieves this through three key innovations: (1) extending usable memory beyond GPU limits via high-bandwidth CPU-GPU interconnects and LPDDR5 memory; (2) adaptive data encoding to reduce memory footprint with acceptable trade-offs in precision and compute effort; and (3) an on-the-fly network traffic optimizer. These advances result in an 11.4-fold speedup over the previous 48-qubit record on the K computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03359v2</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans De Raedt, Jiri Kraus, Andreas Herten, Vrinda Mehta, Mathis Bode, Markus Hrywniak, Kristel Michielsen, Thomas Lippert</dc:creator>
    </item>
  </channel>
</rss>
