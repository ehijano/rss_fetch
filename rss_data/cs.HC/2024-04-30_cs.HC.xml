<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deepfake Labels Restore Reality, Especially for Those Who Dislike the Speaker</title>
      <link>https://arxiv.org/abs/2404.17581</link>
      <description>arXiv:2404.17581v1 Announce Type: new 
Abstract: Deepfake videos create dangerous possibilities for public misinformation. In this experiment (N=204), we investigated whether labeling videos as containing actual or deepfake statements from US President Biden helps participants later differentiate between true and fake information. People accurately recalled 93.8% of deepfake videos and 84.2% of actual videos, suggesting that labeling videos can help combat misinformation. Individuals who identify as Republican and had lower favorability ratings of Biden performed better in distinguishing between actual and deepfake videos, a result explained by the elaboration likelihood model (ELM), which predicts that people who distrust a message source will more critically evaluate the message.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17581v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan L. Tenhundfeld, Ryan Weber, William I. MacKenzie, Hannah M. Barr, Candice Lanius</dc:creator>
    </item>
    <item>
      <title>Data Quality in Crowdsourcing and Spamming Behavior Detection</title>
      <link>https://arxiv.org/abs/2404.17582</link>
      <description>arXiv:2404.17582v1 Announce Type: new 
Abstract: As crowdsourcing emerges as an efficient and cost-effective method for obtaining labels for machine learning datasets, it is important to assess the quality of crowd-provided data, so as to improve analysis performance and reduce biases in subsequent machine learning tasks. Given the lack of ground truth in most cases of crowdsourcing, we refer to data quality as annotators' consistency and credibility. Unlike the simple scenarios where Kappa coefficient and intraclass correlation coefficient usually can apply, online crowdsourcing requires dealing with more complex situations. We introduce a systematic method for evaluating data quality and detecting spamming threats via variance decomposition, and we classify spammers into three categories based on their different behavioral patterns. A spammer index is proposed to assess entire data consistency and two metrics are developed to measure crowd worker's credibility by utilizing the Markov chain and generalized random effects models. Furthermore, we showcase the practicality of our techniques and their advantages by applying them on a face verification task with both simulation and real-world data collected from two crowdsourcing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17582v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Ba, Michelle V. Mancenido, Erin K. Chiou, Rong Pan</dc:creator>
    </item>
    <item>
      <title>NeuroNet: A Novel Hybrid Self-Supervised Learning Framework for Sleep Stage Classification Using Single-Channel EEG</title>
      <link>https://arxiv.org/abs/2404.17585</link>
      <description>arXiv:2404.17585v1 Announce Type: new 
Abstract: The classification of sleep stages is a pivotal aspect of diagnosing sleep disorders and evaluating sleep quality. However, the conventional manual scoring process, conducted by clinicians, is time-consuming and prone to human bias. Recent advancements in deep learning have substantially propelled the automation of sleep stage classification. Nevertheless, challenges persist, including the need for large datasets with labels and the inherent biases in human-generated annotations. This paper introduces NeuroNet, a self-supervised learning (SSL) framework designed to effectively harness unlabeled single-channel sleep electroencephalogram (EEG) signals by integrating contrastive learning tasks and masked prediction tasks. NeuroNet demonstrates superior performance over existing SSL methodologies through extensive experimentation conducted across three polysomnography (PSG) datasets. Additionally, this study proposes a Mamba-based temporal context module to capture the relationships among diverse EEG epochs. Combining NeuroNet with the Mamba-based temporal context module has demonstrated the capability to achieve, or even surpass, the performance of the latest supervised learning methodologies, even with a limited amount of labeled data. This study is expected to establish a new benchmark in sleep stage classification, promising to guide future research and applications in the field of sleep analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17585v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheol-Hui Lee, Hakseung Kim, Hyun-jee Han, Min-Kyung Jung, Byung C. Yoon, Dong-Joo Kim</dc:creator>
    </item>
    <item>
      <title>The Future of Scientific Publishing: Automated Article Generation</title>
      <link>https://arxiv.org/abs/2404.17586</link>
      <description>arXiv:2404.17586v1 Announce Type: new 
Abstract: This study introduces a novel software tool leveraging large language model (LLM) prompts, designed to automate the generation of academic articles from Python code a significant advancement in the fields of biomedical informatics and computer science. Selected for its widespread adoption and analytical versatility, Python served as a foundational proof of concept; however, the underlying methodology and framework exhibit adaptability across various GitHub repo's underlining the tool's broad applicability (Harper 2024). By mitigating the traditionally time-intensive academic writing process, particularly in synthesizing complex datasets and coding outputs, this approach signifies a monumental leap towards streamlining research dissemination. The development was achieved without reliance on advanced language model agents, ensuring high fidelity in the automated generation of coherent and comprehensive academic content. This exploration not only validates the successful application and efficiency of the software but also projects how future integration of LLM agents which could amplify its capabilities, propelling towards a future where scientific findings are disseminated more swiftly and accessibly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17586v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jeremy R. Harper</dc:creator>
    </item>
    <item>
      <title>Uncovering the Metaverse within Everyday Environments: a Coarse-to-Fine Approach</title>
      <link>https://arxiv.org/abs/2404.17587</link>
      <description>arXiv:2404.17587v1 Announce Type: new 
Abstract: The recent release of the Apple Vision Pro has reignited interest in the metaverse, showcasing the intensified efforts of technology giants in developing platforms and devices to facilitate its growth. As the metaverse continues to proliferate, it is foreseeable that everyday environments will become increasingly saturated with its presence. Consequently, uncovering links to these metaverse items will be a crucial first step to interacting with this new augmented world. In this paper, we address the problem of establishing connections with virtual worlds within everyday environments, especially those that are not readily discernible through direct visual inspection. We introduce a vision-based approach leveraging Artcode visual markers to uncover hidden metaverse links embedded in our ambient surroundings. This approach progressively localises the access points to the metaverse, transitioning from coarse to fine localisation, thus facilitating an exploratory interaction process. Detailed experiments are conducted to study the performance of the proposed approach, demonstrating its effectiveness in Artcode localisation and enabling new interaction opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17587v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liming Xu, Dave Towey, Andrew P. French, Steve Benford</dc:creator>
    </item>
    <item>
      <title>Exploring Vulnerabilities in Remote VR User Studies</title>
      <link>https://arxiv.org/abs/2404.17588</link>
      <description>arXiv:2404.17588v1 Announce Type: new 
Abstract: This position paper explores the possibilities and challenges of using Virtual Reality (VR) in remote user studies. Highlighting the immersive nature of VR, the paper identifies key vulnerabilities, including varying technical proficiency, privacy concerns, ethical considerations, and data security risks. To address these issues, proposed mitigation strategies encompass comprehensive onboarding, prioritized informed consent, implementing privacy-by-design principles, and adherence to ethical guidelines. Secure data handling, including encryption and disposal protocols, is advocated. In conclusion, while remote VR studies present unique opportunities, carefully considering and implementing mitigation strategies is essential to uphold reliability, ethical integrity, and security, ensuring responsible and effective use of VR in user research. Ongoing efforts are crucial for adapting to the evolving landscape of VR technology in user studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17588v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viktorija Paneva, Florian Alt</dc:creator>
    </item>
    <item>
      <title>TICE and normalisation, pour une r{\'e}novation universitaire dans les pays du Sud</title>
      <link>https://arxiv.org/abs/2404.17595</link>
      <description>arXiv:2404.17595v1 Announce Type: new 
Abstract: University renovation is a recurring fact generated by permanent technological innovations and new methods of organizing universities and training offers. Technological interoperability standards play a determining role, not only by providing added value in terms of saving space and time, but also by changing educational models and knowledge acquisition processes. The countries of sub-Saharan Africa present an institutional framework and a particular type of university organization which requires an in-depth rereading of their operating methods for a better recovery on the path to renovation through ICT. This document offers avenues for reflection and action frameworks that focus on the achievements of norms and interoperability standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17595v1</guid>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Colloque international " Les usages intelligents des technologies de l'information et de la communication dans la r{\'e}organisation universitaire '', Africampus; Universit{\'e} Omar Bongo, Jun 2008, Libreville, Gabon. pp.437-458</arxiv:journal_reference>
      <dc:creator>Mokhtar Ben Henda (MICA, ISD, GRESIC, ISIC, Chaire Unesco-ITEN)</dc:creator>
    </item>
    <item>
      <title>Human-AI Collaborative Big-Thick Data Collection</title>
      <link>https://arxiv.org/abs/2404.17602</link>
      <description>arXiv:2404.17602v1 Announce Type: new 
Abstract: By Big-Thick data, we mean large-scale sensor data (big data) which provides an objective view of reality, coupled with thick data, i.e., data generated by people, which describes their subjective view of the reality described by big data. Big-thick data enables a machine understanding of human behavior and activities, as well as the human interpretation of what they are doing, i.e., their own personal descriptions of the why, what, and how. The goal of this short paper is to provide a high-level description of a platform, called i-Log, that enables the collection of big-thick data. Its core components are: tools for collecting sensor data as well as the user feedback (e.g., user answers to machine questions), and a dashboard which provides visual qualitative and quantitative feedback on how things are evolving, as well as suitable notifications to the user.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17602v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Zhao, Ivan Kayongo, Leonardo Malcotti, Fausto Giunchiglia</dc:creator>
    </item>
    <item>
      <title>Exploring Remote Hands-on Support for Collaborative Embedded Systems Development</title>
      <link>https://arxiv.org/abs/2404.17604</link>
      <description>arXiv:2404.17604v1 Announce Type: new 
Abstract: Embedded systems development is a complex task that often requires team collaboration. Given the growing market of freelancers and the global shift to remote work, remote collaboration has become a necessity for many developers and clients. While existing communication and coordination tools help users share, discuss, and edit code collaboratively, these tools were specifically designed for software rather than hardware development. In this work, our goal is to explore the design space of remote support tools for embedded systems development. To do this, we interviewed 12 seasoned embedded systems developers regarding their current remote work practices, issues, and needs. We then conducted a user enactment study with a bespoke remote manipulation agent, Handy, as a hypothetical assistant to elicit the types of support developers desire from a collaborator. Our findings describe the scenarios and strategies in which remote work takes place; the support needs and information, coordination, and implementation challenges expressed by developers; and the privacy, control, and trust concerns that developers have when working on their projects with remote physical manipulation tools. This research contributes to the literature by bringing embedded systems development in line with remote, on-demand collaboration and help-seeking in software environments. The empirical basis of this work provides a rich foundation of documented needs, preferences, and desires that can ground future work on remote manipulation agents and enhance collaboration support in the domain of embedded systems development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17604v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Chen, Jasmine Jones</dc:creator>
    </item>
    <item>
      <title>VisAnywhere: Developing Multi-platform Scientific Visualization Applications</title>
      <link>https://arxiv.org/abs/2404.17619</link>
      <description>arXiv:2404.17619v1 Announce Type: new 
Abstract: Scientists often explore and analyze large-scale scientific simulation data by leveraging two- and three-dimensional visualizations. The data and tasks can be complex and therefore best supported using myriad display technologies, from mobile devices to large high-resolution display walls to virtual reality headsets. Using a simulation of neuron connections in the human brain, we present our work leveraging various web technologies to create a multi-platform scientific visualization application. Users can spread visualization and interaction across multiple devices to support flexible user interfaces and both co-located and remote collaboration. Drawing inspiration from responsive web design principles, this work demonstrates that a single codebase can be adapted to develop scientific visualization applications that operate everywhere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17619v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Marrinan, Madeleine Moeller, Alina Kanayinkal, Victor A. Mateevitsi, Michael E. Papka</dc:creator>
    </item>
    <item>
      <title>Seizing the Means of Production: Exploring the Landscape of Crafting, Adapting and Navigating Generative AI Models in the Visual Arts</title>
      <link>https://arxiv.org/abs/2404.17688</link>
      <description>arXiv:2404.17688v1 Announce Type: new 
Abstract: In this paper, we map out the landscape of options available to visual artists for creating personal artworks, including crafting, adapting and navigating deep generative models. Following that, we argue for revisiting model crafting, defined as the design and manipulation of generative models for creative goals, and motivate studying and designing for model crafting as a creative activity in its own right.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17688v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed M. Abuzuraiq, Philippe Pasquier</dc:creator>
    </item>
    <item>
      <title>SIM2VR: Towards Automated Biomechanical Testing in VR</title>
      <link>https://arxiv.org/abs/2404.17695</link>
      <description>arXiv:2404.17695v1 Announce Type: new 
Abstract: Automated biomechanical testing has great potential for the development of VR applications, as initial insights into user behaviour can be gained in silico early in the design process. In particular, it allows prediction of user movements and ergonomic variables, such as fatigue, prior to conducting user studies. However, there is a fundamental disconnect between simulators hosting state-of-the-art biomechanical user models and simulators used to develop and run VR applications. Existing user simulators often struggle to capture the intricacies and nuances of real-world VR applications, reducing ecological validity of user predictions. In this paper, we introduce SIM2VR, a system that aligns user simulation with a given VR application by establishing a continuous closed loop between the two processes. This, for the first time, enables training simulated users directly in the same VR application that real users interact with. We demonstrate that SIM2VR can predict differences in user performance, ergonomics and strategies in a fast-paced, dynamic arcade game. In order to expand the scope of automated biomechanical testing beyond simple visuomotor tasks, advances in cognitive models and reward function design will be needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17695v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florian Fischer, Aleksi Ikkala, Markus Klar, Arthur Fleig, Miroslav Bachinski, Roderick Murray-Smith, Perttu H\"am\"al\"ainen, Antti Oulasvirta, J\"org M\"uller</dc:creator>
    </item>
    <item>
      <title>"Actually I Can Count My Blessings": User-Centered Design of an Application to Promote Gratitude Among Young Adults</title>
      <link>https://arxiv.org/abs/2404.17698</link>
      <description>arXiv:2404.17698v1 Announce Type: new 
Abstract: Regular practice of gratitude has the potential to enhance psychological wellbeing and foster stronger social connections among young adults. However, there is a lack of research investigating user needs and expectations regarding gratitude-promoting applications. To address this gap, we employed a user-centered design approach to develop a mobile application that facilitates gratitude practice. Our formative study involved 20 participants who utilized an existing application, providing insights into their preferences for organizing expressions of gratitude and the significance of prompts for reflection and mood labeling after working hours. Building on these findings, we conducted a deployment study with 26 participants using our custom-designed application, which confirmed the positive impact of structured options to guide gratitude practice and highlighted the advantages of passive engagement with the application during busy periods. Our study contributes to the field by identifying key design considerations for promoting gratitude among young adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17698v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Bhattacharjee, Zichen Gong, Bingcheng Wang, Timothy James Luckcock, Emma Watson, Elena Allica Abellan, Leslie Gutman, Anne Hsu, Joseph Jay Williams</dc:creator>
    </item>
    <item>
      <title>Bridging the Social &amp; Technical Divide in Augmentative and Alternative Communication (AAC) Applications for Autistic Adults</title>
      <link>https://arxiv.org/abs/2404.17730</link>
      <description>arXiv:2404.17730v1 Announce Type: new 
Abstract: Natural Language Processing (NLP) techniques are being used more frequently to improve high-tech Augmentative and Alternative Communication (AAC), but many of these techniques are integrated without the inclusion of the users' perspectives. As many of these tools are created with children in mind, autistic adults are often neglected in the design of AAC tools to begin with. We conducted in-depth interviews with 12 autistic adults to find the pain points of current AAC and determine what general technological advances they would find helpful. We found that in addition to technological issues, there are many societal issues as well. We found 9 different categories of themes from our interviews: input options, output options, selecting or adapting AAC for a good fit, when to start or swap AAC, benefits (of use), access (to AAC), stumbling blocks for continued use, social concerns, and lack of control. In this paper, we go through these nine categories in depth and then suggest possible guidelines for the NLP community, AAC application makers, and policy makers to improve AAC use for autistic adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17730v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lara J. Martin, Malathy Nagalakshmi</dc:creator>
    </item>
    <item>
      <title>MedBike: A Cardiac Patient Monitoring System Enhanced through Gamification</title>
      <link>https://arxiv.org/abs/2404.17731</link>
      <description>arXiv:2404.17731v1 Announce Type: new 
Abstract: The "MedBike" is an innovative project in the field of pediatric cardiac rehabilitation. It is a 2D interactive game created specifically for children under the age of 18 who have cardiac conditions. This game is part of the MedBike system, a novel rehabilitation tool combining physical exercise with the spirit of gaming. The MedBike game provides children with a safe, controlled, and engaging environment in which to exercise and recover. It has three distinct levels of increasing intensity, each with its own set of environments and challenges that are tailored to different stages of rehabilitation. This report dives into the details of the MedBike game, highlighting its unique features and gameplay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17731v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tahmim Hossain, Faisal Sayed, Yugesh Rai, Kalpak Bansod, Md Nahid Sadik</dc:creator>
    </item>
    <item>
      <title>A Value-Oriented Investigation of Photoshop's Generative Fill</title>
      <link>https://arxiv.org/abs/2404.17781</link>
      <description>arXiv:2404.17781v1 Announce Type: new 
Abstract: The creative industry is both concerned and enthusiastic about how generative AI will reshape creativity. How might these tools interact with the workflow values of creative artists? In this paper, we adopt a value-sensitive design framework to examine how generative AI, particularly Photoshop's Generative Fill (GF), helps or hinders creative professionals' values. We obtained 566 unique posts about GF from online forums for creative professionals who use Photoshop in their current work practices. We conducted reflexive thematic analysis focusing on usefulness, ease of use, and user values. Users found GF useful in doing touch-ups, expanding images, and generating composite images. GF helped users' values of productivity by making work efficient but created a value tension around creativity: it helped reduce barriers to creativity but hindered distinguishing 'human' from algorithmic art. Furthermore, GF hindered lived experiences shaping creativity and hindered the honed prideful skills of creative work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17781v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian P. Swift, Debaleena Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>GPT for Games: A Scoping Review (2020-2023)</title>
      <link>https://arxiv.org/abs/2404.17794</link>
      <description>arXiv:2404.17794v1 Announce Type: new 
Abstract: This paper introduces a scoping review of 55 articles to explore GPT's potential for games, offering researchers a comprehensive understanding of the current applications and identifying both emerging trends and unexplored areas. We identify five key applications of GPT in current game research: procedural content generation, mixed-initiative game design, mixed-initiative gameplay, playing games, and game user research. Drawing from insights in each of these application areas, we propose directions for future research in each one. This review aims to lay the groundwork by illustrating the state of the art for innovative GPT applications in games, promising to enrich game development and enhance player experiences with cutting-edge AI innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17794v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daijin Yang, Erica Kleinman, Casper Harteveld</dc:creator>
    </item>
    <item>
      <title>GenAI Distortion: The Effect of GenAI Fluency and Positive Affect</title>
      <link>https://arxiv.org/abs/2404.17822</link>
      <description>arXiv:2404.17822v1 Announce Type: new 
Abstract: The introduction of generative artificial intelligence (GenAI) into educational practices has been transformative, yet it brings a crucial concern about the potential distortion of users' beliefs. Given the prevalence of GenAI among college students, examining the psychological mechanisms that lead to GenAI distortion from both technological factors and the individual's psychological processes is a critical priority. A mixed-methods approach is employed to test the proposed hypotheses. Study 1 (N = 10) revealed through qualitative analysis that GenAI's fluent outputs significantly engaged college students, eliciting positive emotional responses during an interaction. GenAI's tendency to conflate fact with fiction often led to presentations of unrealistic and exaggerated information, potentially distorting users' perceptions of reality-a phenomenon termed GenAI distortion. Following these insights, Study 2 (cross-sectional survey, N = 999) and Study 3 (experimental manipulation, N = 175) explored how GenAI fluency affects college students' GenAI distortion and examined the mediating effect of positive affect. The results indicated that GenAI fluency predicts GenAI distortion via the mediating role of positive affect. Our findings provide theoretical foundations and practical implications for understanding GenAI distortion among college students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17822v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiantong Yang, Mengmeng Zhang</dc:creator>
    </item>
    <item>
      <title>Empowering Mobility: Brain-Computer Interface for Enhancing Wheelchair Control for Individuals with Physical Disabilities</title>
      <link>https://arxiv.org/abs/2404.17895</link>
      <description>arXiv:2404.17895v1 Announce Type: new 
Abstract: The integration of brain-computer interfaces (BCIs) into the realm of smart wheelchair (SW) technology signifies a notable leap forward in enhancing the mobility and autonomy of individuals with physical disabilities. BCIs are a technology that enables direct communication between the brain and external devices. While BCIs systems offer remarkable opportunities for enhancing human-computer interaction and providing mobility solutions for individuals with disabilities, they also raise significant concerns regarding security, safety, and privacy that have not been thoroughly addressed by researchers on a large scale. Our research aims to enhance wheelchair control for individuals with physical disabilities by leveraging electroencephalography (EEG) signals for BCIs. We introduce a non-invasive BCI system that utilizes a neuro-signal acquisition headset to capture EEG signals. These signals are obtained from specific brain activities that individuals have been trained to produce, allowing for precise control of the wheelchair. EEG-based BCIs are instrumental in capturing the brain's electrical activity and translating these signals into actionable commands. The primary objective of our study is to demonstrate the system's capability to interpret EEG signals and decode specific thought patterns or mental commands issued by the user. By doing so, it aims to convert these into accurate control commands for the wheelchair. This process includes the recognition of navigational intentions, such as moving forward, backward, or executing turns, specifically tailored for wheelchair operation. Through this innovative approach, we aim to create a seamless interface between the user's cognitive intentions and the wheelchair's movements, enhancing autonomy and mobility for individuals with physical disabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17895v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiva Ghasemi, Denis Gracanin, Mohammad Azab</dc:creator>
    </item>
    <item>
      <title>Towards Intent-based User Interfaces: Charting the Design Space of Intent-AI Interactions Across Task Types</title>
      <link>https://arxiv.org/abs/2404.18196</link>
      <description>arXiv:2404.18196v1 Announce Type: new 
Abstract: Technological advances continue to redefine the dynamics of human-machine interactions, particularly in task execution. This proposal responds to the advancements in Generative AI by outlining a research plan that probes intent-AI interaction across a diverse set of tasks: fixed-scope content curation task, atomic creative tasks, and complex and interdependent tasks. This exploration aims to inform and contribute to the development of Intent-based User Interface (IUI). The study is structured in three phases: examining fixed-scope tasks through news headline generation, exploring atomic creative tasks via analogy generation, and delving into complex tasks through exploratory visual data analysis. Future work will focus on improving IUIs to better provide suggestions to encourage experienced users to express broad and exploratory intents, and detailed and structured guidance for novice users to iterate on analysis intents for high quality outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18196v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijian Ding</dc:creator>
    </item>
    <item>
      <title>Finding Understanding and Support: Navigating Online Communities to Share and Connect at the intersection of Abuse and Foster Care Experiences</title>
      <link>https://arxiv.org/abs/2404.18301</link>
      <description>arXiv:2404.18301v1 Announce Type: new 
Abstract: Many children in foster care experience trauma that is rooted in unstable family relationships. Other members of the foster care system like foster parents and social workers face secondary trauma. Drawing on 10 years of Reddit data, we used a mixed methods approach to analyze how different members of the foster care system find support and similar experiences at the intersection of two Reddit communities - foster care, and abuse. Users who cross this boundary focus on trauma experiences specific to different roles in foster care. While representing a small number of users, boundary crossing users contribute heavily to both communities, and, compared to matching users, receive higher scores and more replies. We explore the roles boundary crossing users have both in the online community and in the context of foster care. Finally, we present design recommendations that would support trauma survivors find communities more suited to their personal experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18301v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tawfiq Ammari, Eunhye Ahn, Astha Lakhankar, Joyce Lee</dc:creator>
    </item>
    <item>
      <title>Display in the Air: Balancing Distraction and Workload in AR Glasses Interfaces for Driving Navigation</title>
      <link>https://arxiv.org/abs/2404.18357</link>
      <description>arXiv:2404.18357v1 Announce Type: new 
Abstract: Augmented Reality (AR) navigation via Head-Mounted Displays (HMDs), particularly AR glasses, is revolutionizing the driving experience by integrating real-time routing information into the driver's field of view. Despite the potential of AR glasses, the question of how to display navigation information on the interface of these devices remains a valuable yet relatively unexplored research area. This study employs a mixed-method approach involving 32 participants, combining qualitative feedback from semi-structured interviews with quantitative data from usability questionnaires in both simulated and real-world scenarios. Highlighting the necessity of real-world testing, the research evaluates the impact of five icon placements on the efficiency and effectiveness of information perception in both environments. The experiment results indicate a preference for non-central icon placements, especially bottom-center in real world, which mostly balances distraction and workload for the driver. Moreover, these findings contribute to the formulation of four specific design implications for augmented reality interfaces and systems. This research advances the understanding of AR glasses in driving assistance and sets the stage for further developments in this emerging technology field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18357v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyang He, Keyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Equivalence: An analysis of artists' roles with Image Generative AI from Conceptual Art perspective through an interactive installation design practice</title>
      <link>https://arxiv.org/abs/2404.18385</link>
      <description>arXiv:2404.18385v1 Announce Type: new 
Abstract: Over the past year, the emergence of advanced text-to-image Generative AI models has significantly impacted the art world, challenging traditional notions of creativity and the role of artists. This study explores how artists interact with these technologies, using a 5P model (Purpose, People, Process, Product, and Press) based on Rhodes' creativity framework to compare the artistic processes behind Conceptual Art and Image Generative AI. To exemplify this framework, a practical case study titled "Equivalence", a multi-screen interactive installation that converts users' speech input into continuously evolving paintings developed based on Stable Diffusion and NLP algorithms, was developed. Through comprehensive analysis and the case study, this work aims to broaden our understanding of artists' roles and foster a deeper appreciation for the creative aspects inherent in artwork created with Image Generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18385v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan Li, Dan C. Baciu, Marcos Novak, George Legrady</dc:creator>
    </item>
    <item>
      <title>Understanding and Shaping Human-Technology Assemblages in the Age of Generative AI</title>
      <link>https://arxiv.org/abs/2404.18405</link>
      <description>arXiv:2404.18405v1 Announce Type: new 
Abstract: Generative AI capabilities are rapidly transforming how we perceive, interact with, and relate to machines. This one-day workshop invites HCI researchers, designers, and practitioners to imaginatively inhabit and explore the possible futures that might emerge from humans combining generative AI capabilities into everyday technologies at massive scale. Workshop participants will craft stories, visualisations, and prototypes through scenario-based design to investigate these possible futures, resulting in the production of an open-annotated scenario library and a journal or interactions article to disseminate the findings. We aim to gather the DIS community knowledge to explore, understand and shape the relations this new interaction paradigm is forging between humans, their technologies and the environment in safe, sustainable, enriching, and responsible ways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18405v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Andres, Chris Danta, Andrea Bianchi, Sungyeon Hong, Zhuying Li, Eduardo B. Sandoval, Charles Martin, Ned Cooper</dc:creator>
    </item>
    <item>
      <title>ChatGPT as an inventor: Eliciting the strengths and weaknesses of current large language models against humans in engineering design</title>
      <link>https://arxiv.org/abs/2404.18479</link>
      <description>arXiv:2404.18479v1 Announce Type: new 
Abstract: This study compares the design practices and performance of ChatGPT 4.0, a large language model (LLM), against graduate engineering students in a 48-hour prototyping hackathon, based on a dataset comprising more than 100 prototypes. The LLM participated by instructing two participants who executed its instructions and provided objective feedback, generated ideas autonomously and made all design decisions without human intervention. The LLM exhibited similar prototyping practices to human participants and finished second among six teams, successfully designing and providing building instructions for functional prototypes. The LLM's concept generation capabilities were particularly strong. However, the LLM prematurely abandoned promising concepts when facing minor difficulties, added unnecessary complexity to designs, and experienced design fixation. Communication between the LLM and participants was challenging due to vague or unclear descriptions, and the LLM had difficulty maintaining continuity and relevance in answers. Based on these findings, six recommendations for implementing an LLM like ChatGPT in the design process are proposed, including leveraging it for ideation, ensuring human oversight for key decisions, implementing iterative feedback loops, prompting it to consider alternatives, and assigning specific and manageable tasks at a subsystem level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18479v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Nyg{\aa}rd Ege, Henrik H. {\O}vreb{\o}, Vegar Stubberud, Martin Francis Berg, Christer Elverum, Martin Steinert, H{\aa}vard Vestad</dc:creator>
    </item>
    <item>
      <title>On the Evaluation of Procedural Level Generation Systems</title>
      <link>https://arxiv.org/abs/2404.18657</link>
      <description>arXiv:2404.18657v1 Announce Type: new 
Abstract: The evaluation of procedural content generation (PCG) systems for generating video game levels is a complex and contested topic. Ideally, the field would have access to robust, generalisable and widely accepted evaluation approaches that can be used to compare novel PCG systems to prior work, but consensus on how to evaluate novel systems is currently limited. We argue that the field can benefit from a structured analysis of how procedural level generation systems can be evaluated, and how these techniques are currently used by researchers. This analysis can then be used to both inform on the current state of affairs, and to provide data to justify changes to this practice. This work aims to provide this by first developing a novel taxonomy of PCG evaluation approaches, and then presenting the results of a survey of recent work in the field through the lens of this taxonomy. The results of this survey highlight several important weaknesses in current practice which we argue could be substantially mitigated by 1) promoting use of evaluation free system descriptions where appropriate, 2) promoting the development of diverse research frameworks, 3) promoting reuse of code and methodology wherever possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18657v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Withington, Michael Cook, Laurissa Tokarchuk</dc:creator>
    </item>
    <item>
      <title>How Deep Is Your Gaze? Leveraging Distance in Image-Based Gaze Analysis</title>
      <link>https://arxiv.org/abs/2404.18680</link>
      <description>arXiv:2404.18680v1 Announce Type: new 
Abstract: Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data. They are typically extracted with a constant size, approximating the foveated area. As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques. In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance. Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area. We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths. Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization. We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18680v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3649902.3653349</arxiv:DOI>
      <dc:creator>Maurice Koch, Nelusa Pathmanathan, Daniel Weiskopf, Kuno Kurzhals</dc:creator>
    </item>
    <item>
      <title>Feminist Interaction Techniques: Deterring Non-Consensual Screenshots with Interaction Techniques</title>
      <link>https://arxiv.org/abs/2404.18867</link>
      <description>arXiv:2404.18867v1 Announce Type: new 
Abstract: Non-consensual Intimate Media (NCIM) refers to the distribution of sexual or intimate content without consent. NCIM is common and causes significant emotional, financial, and reputational harm. We developed Hands-Off, an interaction technique for messaging applications that deters non-consensual screenshots. Hands-Off requires recipients to perform a hand gesture in the air, above the device, to unlock media -- which makes simultaneous screenshotting difficult. A lab study shows that Hands-Off gestures are easy to perform and reduce non-consensual screenshots by 67 percent. We conclude by generalizing this approach and introduce the idea of Feminist Interaction Techniques (FIT), interaction techniques that encode feminist values and speak to societal problems, and reflect on FIT's opportunities and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18867v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Qiwei, Francesca Lameiro, Shefali Patel,  Cristi-Isaula-Reyes, Eytan Adar, Eric Gilbert, Sarita Schoenebeck</dc:creator>
    </item>
    <item>
      <title>Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking</title>
      <link>https://arxiv.org/abs/2404.18881</link>
      <description>arXiv:2404.18881v1 Announce Type: new 
Abstract: Data augmentation techniques apply transformations to existing texts to generate additional data. The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension. Analyzing the synthetically generated texts and their corresponding labels is slow and demanding. To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique. INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling. INSPECTOR allows users to group related texts by their transformation provenance, i.e., the transformations applied to the original text, or feature provenance, the linguistic features of the original text. For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model. In a user study, INSPECTOR increases the number of texts with correct labels identified by 3X on a sentiment analysis task and by 4X on a hate speech detection task. The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique. Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful. Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort. This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18881v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong Jin Kang, Fabrice Harel-Canada, Muhammad Ali Gulzar, Violet Peng, Miryung Kim</dc:creator>
    </item>
    <item>
      <title>Hybrid 3D Human Pose Estimation with Monocular Video and Sparse IMUs</title>
      <link>https://arxiv.org/abs/2404.17837</link>
      <description>arXiv:2404.17837v1 Announce Type: cross 
Abstract: Temporal 3D human pose estimation from monocular videos is a challenging task in human-centered computer vision due to the depth ambiguity of 2D-to-3D lifting. To improve accuracy and address occlusion issues, inertial sensor has been introduced to provide complementary source of information. However, it remains challenging to integrate heterogeneous sensor data for producing physically rational 3D human poses. In this paper, we propose a novel framework, Real-time Optimization and Fusion (RTOF), to address this issue. We first incorporate sparse inertial orientations into a parametric human skeleton to refine 3D poses in kinematics. The poses are then optimized by energy functions built on both visual and inertial observations to reduce the temporal jitters. Our framework outputs smooth and biomechanically plausible human motion. Comprehensive experiments with ablation studies demonstrate its rationality and efficiency. On Total Capture dataset, the pose estimation error is significantly decreased compared to the baseline method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17837v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Bao, Xu Zhao, Dahong Qian</dc:creator>
    </item>
    <item>
      <title>Automating Customer Needs Analysis: A Comparative Study of Large Language Models in the Travel Industry</title>
      <link>https://arxiv.org/abs/2404.17975</link>
      <description>arXiv:2404.17975v1 Announce Type: cross 
Abstract: In the rapidly evolving landscape of Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools for many tasks, such as extracting valuable insights from vast amounts of textual data. In this study, we conduct a comparative analysis of LLMs for the extraction of travel customer needs from TripAdvisor posts. Leveraging a diverse range of models, including both open-source and proprietary ones such as GPT-4 and Gemini, we aim to elucidate their strengths and weaknesses in this specialized domain. Through an evaluation process involving metrics such as BERTScore, ROUGE, and BLEU, we assess the performance of each model in accurately identifying and summarizing customer needs. Our findings highlight the efficacy of opensource LLMs, particularly Mistral 7B, in achieving comparable performance to larger closed models while offering affordability and customization benefits. Additionally, we underscore the importance of considering factors such as model size, resource requirements, and performance metrics when selecting the most suitable LLM for customer needs analysis tasks. Overall, this study contributes valuable insights for businesses seeking to leverage advanced NLP techniques to enhance customer experience and drive operational efficiency in the travel industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17975v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Barandoni, Filippo Chiarello, Lorenzo Cascone, Emiliano Marrale, Salvatore Puccio</dc:creator>
    </item>
    <item>
      <title>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments</title>
      <link>https://arxiv.org/abs/2404.18021</link>
      <description>arXiv:2404.18021v1 Announce Type: cross 
Abstract: The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent's effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18021v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaixuan Huang, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, Le Cong</dc:creator>
    </item>
    <item>
      <title>MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot</title>
      <link>https://arxiv.org/abs/2404.18074</link>
      <description>arXiv:2404.18074v1 Announce Type: cross 
Abstract: Autonomous virtual agents are often limited by their singular mode of interaction with real-world environments, restricting their versatility. To address this, we propose the Multi-Modal Agent Collaboration framework (MMAC-Copilot), a framework utilizes the collective expertise of diverse agents to enhance interaction ability with operating systems. The framework introduces a team collaboration chain, enabling each participating agent to contribute insights based on their specific domain knowledge, effectively reducing the hallucination associated with knowledge domain gaps. To evaluate the performance of MMAC-Copilot, we conducted experiments using both the GAIA benchmark and our newly introduced Visual Interaction Benchmark (VIBench). VIBench focuses on non-API-interactable applications across various domains, including 3D gaming, recreation, and office scenarios. MMAC-Copilot achieved exceptional performance on GAIA, with an average improvement of 6.8\% over existing leading systems. Furthermore, it demonstrated remarkable capability on VIBench, particularly in managing various methods of interaction within systems and applications. These results underscore MMAC-Copilot's potential in advancing the field of autonomous virtual agents through its innovative approach to agent collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18074v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zirui Song, Yaohang Li, Meng Fang, Zhenhao Chen, Zecheng Shi, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Generative AI for Visualization: State of the Art and Future Directions</title>
      <link>https://arxiv.org/abs/2404.18144</link>
      <description>arXiv:2404.18144v1 Announce Type: cross 
Abstract: Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion model and large language model have also drastically increase the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI and generative algorithms. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18144v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, Wei Zeng</dc:creator>
    </item>
    <item>
      <title>Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement</title>
      <link>https://arxiv.org/abs/2404.18214</link>
      <description>arXiv:2404.18214v1 Announce Type: cross 
Abstract: Sequential recommendation is one of the important branches of recommender system, aiming to achieve personalized recommended items for the future through the analysis and prediction of users' ordered historical interactive behaviors. However, along with the growth of the user volume and the increasingly rich behavioral information, how to understand and disentangle the user's interactive multi-intention effectively also poses challenges to behavior prediction and sequential recommendation. In light of these challenges, we propose a Contrastive Learning sequential recommendation method based on Multi-Intention Disentanglement (MIDCL). In our work, intentions are recognized as dynamic and diverse, and user behaviors are often driven by current multi-intentions, which means that the model needs to not only mine the most relevant implicit intention for each user, but also impair the influence from irrelevant intentions. Therefore, we choose Variational Auto-Encoder (VAE) to realize the disentanglement of users' multi-intentions, and propose two types of contrastive learning paradigms for finding the most relevant user's interactive intention, and maximizing the mutual information of positive sample pairs, respectively. Experimental results show that MIDCL not only has significant superiority over most existing baseline methods, but also brings a more interpretable case to the research about intention-based prediction and recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18214v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Hu, Yuzhi Xiao, Tao Huang, Xuanrong Huo</dc:creator>
    </item>
    <item>
      <title>Evaluating Readability and Faithfulness of Concept-based Explanations</title>
      <link>https://arxiv.org/abs/2404.18533</link>
      <description>arXiv:2404.18533v1 Announce Type: cross 
Abstract: Despite the surprisingly high intelligence exhibited by Large Language Models (LLMs), we are somehow intimidated to fully deploy them into real-life applications considering their black-box nature. Concept-based explanations arise as a promising avenue for explaining what the LLMs have learned, making them more transparent to humans. However, current evaluations for concepts tend to be heuristic and non-deterministic, e.g. case study or human evaluation, hindering the development of the field. To bridge the gap, we approach concept-based explanation evaluation via faithfulness and readability. We first introduce a formal definition of concept generalizable to diverse concept-based explanations. Based on this, we quantify faithfulness via the difference in the output upon perturbation. We then provide an automatic measure for readability, by measuring the coherence of patterns that maximally activate a concept. This measure serves as a cost-effective and reliable substitute for human evaluation. Finally, based on measurement theory, we describe a meta-evaluation method for evaluating the above measures via reliability and validity, which can be generalized to other tasks as well. Extensive experimental analysis has been conducted to validate and inform the selection of concept evaluation measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18533v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Li, Haoran Jin, Ruixuan Huang, Zhihao Xu, Defu Lian, Zijia Lin, Di Zhang, Xiting Wang</dc:creator>
    </item>
    <item>
      <title>IncidentResponseGPT: Generating Traffic Incident Response Plans with Generative Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2404.18550</link>
      <description>arXiv:2404.18550v1 Announce Type: cross 
Abstract: Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion. Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle. This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans. By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents. The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems. Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents. The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management. By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18550v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artur Grigorev, Khaled Saleh, Yuming Ou</dc:creator>
    </item>
    <item>
      <title>Warmth and competence in human-agent cooperation</title>
      <link>https://arxiv.org/abs/2201.13448</link>
      <description>arXiv:2201.13448v3 Announce Type: replace 
Abstract: Interaction and cooperation with humans are overarching aspirations of artificial intelligence (AI) research. Recent studies demonstrate that AI agents trained with deep reinforcement learning are capable of collaborating with humans. These studies primarily evaluate human compatibility through "objective" metrics such as task performance, obscuring potential variation in the levels of trust and subjective preference that different agents garner. To better understand the factors shaping subjective preferences in human-agent cooperation, we train deep reinforcement learning agents in Coins, a two-player social dilemma. We recruit $N = 501$ participants for a human-agent cooperation study and measure their impressions of the agents they encounter. Participants' perceptions of warmth and competence predict their stated preferences for different agents, above and beyond objective performance metrics. Drawing inspiration from social science and biology research, we subsequently implement a new ``partner choice'' framework to elicit revealed preferences: after playing an episode with an agent, participants are asked whether they would like to play the next episode with the same agent or to play alone. As with stated preferences, social perception better predicts participants' revealed preferences than does objective performance. Given these results, we recommend human-agent interaction researchers routinely incorporate the measurement of social perception and subjective preferences into their studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.13448v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10458-024-09649-6</arxiv:DOI>
      <dc:creator>Kevin R. McKee, Xuechunzi Bai, Susan T. Fiske</dc:creator>
    </item>
    <item>
      <title>Has the Virtualization of the Face Changed Facial Perception? A Study of the Impact of Photo Editing and Augmented Reality on Facial Perception</title>
      <link>https://arxiv.org/abs/2303.00612</link>
      <description>arXiv:2303.00612v3 Announce Type: replace 
Abstract: Augmented reality and other photo editing filters are popular methods used to modify faces online. Considering the important role of facial perception in communication, how do we perceive this increasing number of modified faces? In this paper we present the results of six surveys that measure familiarity with different styles of facial filters, perceived strangeness of faces edited with different filters, and ability to discern whether images are filtered. Our results demonstrate that faces modified with more traditional face filters are perceived similarly to unmodified faces, and faces filtered with augmented reality filters are perceived differently from unmodified faces. We discuss possible explanations for these results, including a societal adjustment to traditional photo editing techniques or the inherent differences in the different types of filters. We conclude with a discussion of how to build online spaces more responsibly based on our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00612v3</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613905.3650989</arxiv:DOI>
      <dc:creator>Louisa Conwill, Sam English Anthony, Walter J. Scheirer</dc:creator>
    </item>
    <item>
      <title>Origami Single-end Capacitive Sensing for Continuous Shape Estimation of Morphing Structures</title>
      <link>https://arxiv.org/abs/2307.05370</link>
      <description>arXiv:2307.05370v2 Announce Type: replace 
Abstract: In this work, we propose a novel single-end morphing capacitive sensing method for shape tracking, FxC, by combining Folding origami structures and Capacitive sensing to detect the morphing structural motions using state-of-the-art sensing circuits and deep learning. It was observed through embedding areas of origami structures with conductive materials as single-end capacitive sensing patches, that the sensor signals change coherently with the motion of the structure. Different from other origami capacitors where the origami structures are used in adjusting the thickness of the dielectric layer of double-plate capacitors, FxC uses only a single conductive plate per channel, and the origami structure directly changes the geometry of the conductive plate. We examined the operation principle of morphing single-end capacitors through 3D geometry simulation combined with physics theoretical deduction, which deduced similar behaviour as observed in experimentation. Then a software pipeline was developed to use the sensor signals to reconstruct the dynamic structural geometry through data-driven deep neural network regression of geometric primitives extracted from vision tracking. We created multiple folding patterns to validate our approach, based on folding patterns including Accordion, Chevron, Sunray and V-Fold patterns with different layouts of capacitive sensors using paper-based and textile-based materials. Experimentation results show that the geometry primitives predicted from the capacitive signals have a strong correlation with the visual ground truth with R-squared value of up to 95% and tracking error of 6.5 mm for patches. The simulation and machine learning constitute two-way information exchange between the sensing signals and structural geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05370v2</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lala Shakti Swarup Ray, Daniel Gei{\ss}ler, Bo Zhou, Paul Lukowicz, Berit Greinke</dc:creator>
    </item>
    <item>
      <title>App Planner: Utilizing Generative AI in K-12 Mobile App Development Education</title>
      <link>https://arxiv.org/abs/2401.15182</link>
      <description>arXiv:2401.15182v3 Announce Type: replace 
Abstract: App Planner is an interactive support tool for K-12 students, designed to assist in creating mobile applications. By utilizing generative AI, App Planner helps students articulate the problem and solution through guided conversations via a chat-based interface. It assists them in brainstorming and formulating new ideas for applications, provides feedback on those ideas, and stimulates creative thinking. Here we report usability tests from our preliminary study with high-school students who appreciated App Planner for aiding the app design process and providing new viewpoints on human aspects especially the potential negative impact of their creation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15182v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3628516.3659392</arxiv:DOI>
      <dc:creator>David Kim, Prerna Ravi, Randi Williams, Daeun Yoo</dc:creator>
    </item>
    <item>
      <title>MetaStates: An Approach for Representing Human Workers' Psychophysiological States in the Industrial Metaverse</title>
      <link>https://arxiv.org/abs/2402.15340</link>
      <description>arXiv:2402.15340v2 Announce Type: replace 
Abstract: Photo-realistic avatar is a modern term referring to the digital asset that represents a human in computer graphic advanced systems such as video games and simulation tools. These avatars utilize the advances in graphic technologies in both software and hardware aspects. While photo-realistic avatars are increasingly used in industrial simulations, representing human factors such as human workers psychophysiological states, remains a challenge. This article addresses this issue by introducing the concept of MetaStates which are the digitization and representation of the psychophysiological states of a human worker in the digital world. The MetaStates influence the physical representation and performance of a digital human worker while performing a task. To demonstrate this concept, this study presents the development of a photo-realistic avatar enhanced with multi-level graphical representations of psychophysiological states relevant to Industry 5.0. This approach represents a major step forward in the use of digital humans for industrial simulations, allowing companies to better leverage the benefits of the Industrial Metaverse in their daily operations and simulations while keeping human workers at the center of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15340v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aitor Toichoa Eyam, Jose L. Martinez Lastra</dc:creator>
    </item>
    <item>
      <title>How Platform Exchange and Safeguards Matter: The Case of Sexual Risk in Airbnb and Couchsurfing</title>
      <link>https://arxiv.org/abs/2402.18705</link>
      <description>arXiv:2402.18705v2 Announce Type: replace 
Abstract: Recent work in CHI and CSCW has devoted increasing attention to how the design of network hospitality platforms shapes user experiences and relational outcomes. In this article, I interrogate how different risk factors emerge based on the type of exchanges these platforms facilitate. To do so, I juxtapose two prominent network hospitality platforms: one facilitating negotiated exchange (i.e., Airbnb) with another facilitating reciprocal exchange (i.e., Couchsurfing). Homing in on sexual risk, an underexplored form of platform danger, and drawing on interviews with 40 female dual-platform users, I argue that Airbnb's provision of binding negotiated exchange and institutional safeguards reduces risk through three mechanisms: casting initial guest-host relation into a buyer-seller arrangement, stabilizing interactional scripts, and formalizing sexual violence recourse. Conversely, Couchsurfing's focus on reciprocal exchange and lack of safeguards increase sexual precarity for users both on- and off-platform. This study demonstrates how platforms with strong prosocial motivations can jeopardize sociality and concludes with design implications for protecting vulnerable user populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18705v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3653695</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Hum.-Comput. Interact., 8, CSCW1, Article 204 (April 2024), 23 pages</arxiv:journal_reference>
      <dc:creator>Skyler Wang</dc:creator>
    </item>
    <item>
      <title>Enhanced DareFightingICE Competitions: Sound Design and AI Competitions</title>
      <link>https://arxiv.org/abs/2403.02687</link>
      <description>arXiv:2403.02687v2 Announce Type: replace 
Abstract: This paper presents a new and improved DareFightingICE platform, a fighting game platform with a focus on visually impaired players (VIPs), in the Unity game engine. It also introduces the separation of the DareFightingICE Competition into two standalone competitions called DareFightingICE Sound Design Competition and DareFightingICE AI Competition--at the 2024 IEEE Conference on Games (CoG)--in which a new platform will be used. This new platform is an enhanced version of the old DareFightingICE platform, having a better audio system to convey 3D sound and a better way to send audio data to AI agents. With this enhancement and by utilizing Unity, the new DareFightingICE platform is more accessible in terms of adding new features for VIPs and future audio research. This paper also improves the evaluation method for evaluating sound designs in the Sound Design Competition which will ensure a better sound design for VIPs as this competition continues to run at future CoG. To the best of our knowledge, both of our competitions are first of their kind, and the connection between the competitions to mutually improve the entries' quality with time makes these competitions an important part of representing an often overlooked segment within the broader gaming community, VIPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02687v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ibrahim Khan, Chollakorn Nimpattanavong, Thai Van Nguyen, Kantinan Plupattanakit, Ruck Thawonmas</dc:creator>
    </item>
    <item>
      <title>Exploring Algorithmic Explainability: Generating Explainable AI Insights for Personalized Clinical Decision Support Focused on Cannabis Intoxication in Young Adults</title>
      <link>https://arxiv.org/abs/2404.14563</link>
      <description>arXiv:2404.14563v2 Announce Type: replace 
Abstract: This study explores the possibility of facilitating algorithmic decision-making by combining interpretable artificial intelligence (XAI) techniques with sensor data, with the aim of providing researchers and clinicians with personalized analyses of cannabis intoxication behavior. SHAP analyzes the importance and quantifies the impact of specific factors such as environmental noise or heart rate, enabling clinicians to pinpoint influential behaviors and environmental conditions. SkopeRules simplify the understanding of cannabis use for a specific activity or environmental use. Decision trees provide a clear visualization of how factors interact to influence cannabis consumption. Counterfactual models help identify key changes in behaviors or conditions that may alter cannabis use outcomes, to guide effective individualized intervention strategies. This multidimensional analytical approach not only unveils changes in behavioral and physiological states after cannabis use, such as frequent fluctuations in activity states, nontraditional sleep patterns, and specific use habits at different times and places, but also highlights the significance of individual differences in responses to cannabis use. These insights carry profound implications for clinicians seeking to gain a deeper understanding of the diverse needs of their patients and for tailoring precisely targeted intervention strategies. Furthermore, our findings highlight the pivotal role that XAI technologies could play in enhancing the transparency and interpretability of Clinical Decision Support Systems (CDSS), with a particular focus on substance misuse treatment. This research significantly contributes to ongoing initiatives aimed at advancing clinical practices that aim to prevent and reduce cannabis-related harms to health, positioning XAI as a supportive tool for clinicians and researchers alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14563v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongze Zhang, Tammy Chung, Anind Dey, Sang Won Bae</dc:creator>
    </item>
    <item>
      <title>Quantitative Evaluation of driver's situation awareness in virtual driving through Eye tracking analysis</title>
      <link>https://arxiv.org/abs/2404.14817</link>
      <description>arXiv:2404.14817v2 Announce Type: replace 
Abstract: In driving tasks, the driver's situation awareness of the surrounding scenario is crucial for safety driving. However, current methods of measuring situation awareness mostly rely on subjective questionnaires, which interrupt tasks and lack non-intrusive quantification. To address this issue, our study utilizes objective gaze motion data to provide an interference-free quantification method for situation awareness. Three quantitative scores are proposed to represent three different levels of awareness: perception, comprehension, and projection, and an overall score of situation awareness is also proposed based on above three scores. To validate our findings, we conducted experiments where subjects performed driving tasks in a virtual reality simulated environment. All the four proposed situation awareness scores have clearly shown a significant correlation with driving performance. The proposed not only illuminates a new path for understanding and evaluating the situation awareness but also offers a satisfying proxy for driving performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14817v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunxiang Jiang, Qing Xu, Kai Zhen, Yu Chen</dc:creator>
    </item>
    <item>
      <title>Meta-Object: Interactive and Multisensory Virtual Object Learned from the Real World for the Post-Metaverse</title>
      <link>https://arxiv.org/abs/2404.17179</link>
      <description>arXiv:2404.17179v2 Announce Type: replace 
Abstract: With the proliferation of wearable Augmented Reality/Virtual Reality (AR/VR) devices, ubiquitous virtual experiences seamlessly integrate into daily life through metaverse platforms. To support immersive metaverse experiences akin to reality, we propose a next-generation virtual object, a meta-object, a property-embedded virtual object that contains interactive and multisensory characteristics learned from the real world. Current virtual objects differ significantly from real-world objects due to restricted sensory feedback based on limited physical properties. To leverage meta-objects in the metaverse, three key components are needed: meta-object modeling and property embedding, interaction-adaptive multisensory feedback, and an intelligence simulation-based post-metaverse platform. Utilizing meta-objects that enable both on-site and remote users to interact as if they were engaging with real objects could contribute to the advent of the post-metaverse era through wearable AR/VR devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17179v2</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dooyoung Kim, Taewook Ha, Jinseok Hong, Seonji Kim, Selin Choi, Heejeong Ko, Woontack Woo</dc:creator>
    </item>
    <item>
      <title>Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa</title>
      <link>https://arxiv.org/abs/2302.10786</link>
      <description>arXiv:2302.10786v3 Announce Type: replace-cross 
Abstract: Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109 questions) implying that Kwame for Science has a high chance of giving at least one useful answer among the 3 displayed. We categorized the reasons the model incorrectly answered questions to provide insights for future improvements. We also share challenges and lessons with the development, deployment, and human-computer interaction component of such a tool to enable other researchers to deploy similar tools. With a first-of-its-kind tool within the African context, Kwame for Science has the potential to enable the delivery of scalable, cost-effective, and quality remote education to millions of people across Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10786v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George Boateng, Samuel John, Samuel Boateng, Philemon Badu, Patrick Agyeman-Budu, Victor Kumbol</dc:creator>
    </item>
    <item>
      <title>Towards the New XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence</title>
      <link>https://arxiv.org/abs/2402.01292</link>
      <description>arXiv:2402.01292v2 Announce Type: replace-cross 
Abstract: Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper, we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy and reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01292v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thao Le, Tim Miller, Liz Sonenberg, Ronal Singh</dc:creator>
    </item>
    <item>
      <title>Leveraging AI to Advance Science and Computing Education across Africa: Challenges, Progress and Opportunities</title>
      <link>https://arxiv.org/abs/2402.07397</link>
      <description>arXiv:2402.07397v2 Announce Type: replace-cross 
Abstract: Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this chapter, we discuss challenges with using AI to advance education across Africa. Then, we describe our work developing and deploying AI in Education tools in Africa for science and computing education: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses, (5) Kwame for Science, a web-based AI teaching assistant that provides instant answers to students' science questions and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz competition. Finally, we discuss potential opportunities to leverage AI to advance education across Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07397v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George Boateng</dc:creator>
    </item>
    <item>
      <title>InstructEdit: Instruction-based Knowledge Editing for Large Language Models</title>
      <link>https://arxiv.org/abs/2402.16123</link>
      <description>arXiv:2402.16123v2 Announce Type: replace-cross 
Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approaches encounter issues with limited generalizability across tasks, necessitating one distinct editor for each task, significantly hindering the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdit consistently surpass previous strong baselines. To further investigate the underlying mechanisms of instruction-based knowledge editing, we analyze the principal components of the editing gradient directions, which unveils that instructions can help control optimization direction with stronger OOD generalization. Code and datasets are available in https://github.com/zjunlp/EasyEdit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16123v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ningyu Zhang, Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Yi Hu, Kouying Xue, Yanjie Gou, Xi Chen, Huajun Chen</dc:creator>
    </item>
    <item>
      <title>Prompting ChatGPT for Translation: A Comparative Analysis of Translation Brief and Persona Prompts</title>
      <link>https://arxiv.org/abs/2403.00127</link>
      <description>arXiv:2403.00127v2 Announce Type: replace-cross 
Abstract: Prompt engineering has shown potential for improving translation quality in LLMs. However, the possibility of using translation concepts in prompt design remains largely underexplored. Against this backdrop, the current paper discusses the effectiveness of incorporating the conceptual tool of translation brief and the personas of translator and author into prompt design for translation tasks in ChatGPT. Findings suggest that, although certain elements are constructive in facilitating human-to-human communication for translation tasks, their effectiveness is limited for improving translation quality in ChatGPT. This accentuates the need for explorative research on how translation theorists and practitioners can develop the current set of conceptual tools rooted in the human-to-human communication paradigm for translation purposes in this emerging workflow involving human-machine interaction, and how translation concepts developed in translation studies can inform the training of GPT models for translation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00127v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sui He</dc:creator>
    </item>
    <item>
      <title>MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition</title>
      <link>https://arxiv.org/abs/2404.17113</link>
      <description>arXiv:2404.17113v2 Announce Type: replace-cross 
Abstract: Multimodal emotion recognition is an important research topic in artificial intelligence. Over the past few decades, researchers have made remarkable progress by increasing dataset size and building more effective architectures. However, due to various reasons (such as complex environments and inaccurate labels), current systems still cannot meet the demands of practical applications. Therefore, we plan to organize a series of challenges around emotion recognition to further promote the development of this field. Last year, we launched MER2023, focusing on three topics: multi-label learning, noise robustness, and semi-supervised learning. This year, we continue to organize MER2024. In addition to expanding the dataset size, we introduce a new track around open-vocabulary emotion recognition. The main consideration for this track is that existing datasets often fix the label space and use majority voting to enhance annotator consistency, but this process may limit the model's ability to describe subtle emotions. In this track, we encourage participants to generate any number of labels in any category, aiming to describe the emotional state as accurately as possible. Our baseline is based on MERTools and the code is available at: https://github.com/zeroQiaoba/MERTools/tree/master/MER2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17113v2</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, Rui Liu, Kele Xu, Bin Liu, Erik Cambria, Guoying Zhao, Bj\"orn W. Schuller, Jianhua Tao</dc:creator>
    </item>
  </channel>
</rss>
