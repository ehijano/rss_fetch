<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jan 2026 02:32:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hamiltonian Monte Carlo for (Physics) Dummies</title>
      <link>https://arxiv.org/abs/2601.01422</link>
      <description>arXiv:2601.01422v1 Announce Type: new 
Abstract: Sampling-based inference has seen a surge of interest in recent years. Hamiltonian Monte Carlo (HMC) has emerged as a powerful algorithm that leverages concepts from Hamiltonian dynamics to efficiently explore complex target distributions. Variants of HMC are available in popular software packages, enabling off-the-shelf implementations that have greatly benefited the statistics and machine learning communities. At the same time, the availability of such black-box implementations has made it challenging for users to understand the inner workings of HMC, especially when they are unfamiliar with the underlying physical principles. We provide a pedagogical overview of HMC that aims to bridge the gap between its theoretical foundations and practical applicability. This review article seeks to make HMC more accessible to applied researchers by highlighting its advantages, limitations, and role in enabling scalable and exact Bayesian inference for complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01422v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arghya Mukherjee, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>grangersearch: An R Package for Exhaustive Granger Causality Testing with Tidyverse Integration</title>
      <link>https://arxiv.org/abs/2601.01604</link>
      <description>arXiv:2601.01604v1 Announce Type: new 
Abstract: This paper introduces grangersearch, an R package for performing exhaustive Granger causality searches on multiple time series. The package provides: (1) exhaustive pairwise search across multiple variables, (2) automatic lag order optimization with visualization, (3) tidyverse-compatible syntax with pipe operators and non-standard evaluation, and (4) integration with the broom ecosystem through tidy() and glance() methods. The package wraps the vars infrastructure while providing a simple interface for exploratory causal analysis. We describe the statistical methodology, demonstrate the package through worked examples, and discuss practical considerations for applied researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01604v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos Korfiatis</dc:creator>
    </item>
    <item>
      <title>Wasserstein Distributionally Robust Rare-Event Simulation</title>
      <link>https://arxiv.org/abs/2601.01642</link>
      <description>arXiv:2601.01642v1 Announce Type: cross 
Abstract: Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01642v1</guid>
      <category>stat.ME</category>
      <category>q-fin.CP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dohyun Ahn, Huiyi Chen, Lewen Zheng</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2601.02305</link>
      <description>arXiv:2601.02305v1 Announce Type: cross 
Abstract: Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02305v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Didong Li, Aritra Halder, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Squintability and Other Metrics for Assessing Projection Pursuit Indexes, and Guiding Optimization Choices</title>
      <link>https://arxiv.org/abs/2407.13663</link>
      <description>arXiv:2407.13663v4 Announce Type: replace 
Abstract: The projection pursuit (PP) guided tour optimizes a criterion function, known as the PP index, to gradually reveal projections of interest from high-dimensional data through animation. Optimization of some PP indexes can be non-trivial, if they are non-smooth functions, or when the optimum has a small "squint angle", detectable only from close proximity. Here, measures for calculating the smoothness and squintability properties of the PP index are defined. These are used to investigate the performance of a recently introduced swarm-based algorithm, Jellyfish Search Optimizer (JSO), for optimizing PP indexes. The performance of JSO in detecting the target pattern (pipe shape) is compared with existing optimizers in PP. Additionally, JSO's performance on detecting the sine-wave shape is evaluated using different PP indexes (hence different smoothness and squintability) across various data dimensions (d = 4, 6, 8, 10, 12) and JSO hyper-parameters. We observe empirically that higher squintability improves the success rate of the PP index optimization, while smoothness has no significant effect. The JSO algorithm has been implemented in the R package, `tourr`, and functions to calculate smoothness and squintability measures are implemented in the `ferrn` package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13663v4</guid>
      <category>stat.CO</category>
      <category>cs.NE</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 2026</arxiv:journal_reference>
      <dc:creator>H. Sherry Zhang, Dianne Cook, Nicolas Langren\'e, Jessica Wai Yin Leung</dc:creator>
    </item>
    <item>
      <title>Asymptotically exact variational flows via involutive MCMC kernels</title>
      <link>https://arxiv.org/abs/2506.02162</link>
      <description>arXiv:2506.02162v3 Announce Type: replace 
Abstract: Most expressive variational families -- such as normalizing flows -- lack practical convergence guarantees, as their theoretical assurances typically hold only at the intractable global optimum. In this work, we present a general recipe for constructing tuning-free, asymptotically exact variational flows on arbitrary state spaces from involutive MCMC kernels. The core methodological component is a novel representation of general involutive MCMC kernels as invertible, measurepreserving iterated random function systems, which act as the flow maps of our variational flows. This leads to three new variational families with provable total variation convergence. Our framework resolves key practical limitations of existing variational families with similar guarantees (e.g., MixFlows), while requiring substantially weaker theoretical assumptions. Finally, we demonstrate the competitive performance of our flows across tasks including posterior approximation, Monte Carlo estimates, and normalization constant estimation, outperforming or matching No-U-Turn sampler (NUTS) and black-box normalizing flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02162v3</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuheng Xu, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Automated Model Tuning for Multifidelity Uncertainty Propagation in Trajectory Simulation</title>
      <link>https://arxiv.org/abs/2509.16007</link>
      <description>arXiv:2509.16007v2 Announce Type: replace 
Abstract: Multifidelity uncertainty propagation combines the efficiency of low-fidelity models with the accuracy of a high-fidelity model to construct statistical estimators of quantities of interest. It is well known that the effectiveness of such methods depends crucially on the relative correlations and computational costs of the available computational models. However, the question of how to automatically tune low-fidelity models to maximize performance remains an open area of research. This work investigates automated model tuning, which optimizes model hyperparameters to minimize estimator variance within a target computational budget. Focusing on multifidelity trajectory simulation estimators, the cost-versus-precision tradeoff enabled by this approach is demonstrated in a practical, online setting where upfront tuning costs cannot be amortized. Using a real-world entry, descent, and landing example, it is shown that automated model tuning largely outperforms hand-tuned models even when the overall computational budget is relatively low. Furthermore, for scenarios where the computational budget is large, model tuning solutions can approach the best-case multifidelity estimator performance where optimal model hyperparameters are known a priori. Recommendations for applying model tuning in practice are provided and avenues for enabling adoption of such approaches for budget-constrained problems are highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16007v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James E. Warner, Geoffrey F. Bomarito, Gianluca Geraci, Michael S. Eldred</dc:creator>
    </item>
    <item>
      <title>MFAI: A Scalable Bayesian Matrix Factorization Approach to Leveraging Auxiliary Information</title>
      <link>https://arxiv.org/abs/2303.02566</link>
      <description>arXiv:2303.02566v3 Announce Type: replace-cross 
Abstract: In various practical situations, matrix factorization methods suffer from poor data quality, such as high data sparsity and low signal-to-noise ratio (SNR). Here, we consider a matrix factorization problem by utilizing auxiliary information, which is massively available in real-world applications, to overcome the challenges caused by poor data quality. Unlike existing methods that mainly rely on simple linear models to combine auxiliary information with the main data matrix, we propose to integrate gradient boosted trees in the probabilistic matrix factorization framework to effectively leverage auxiliary information (MFAI). Thus, MFAI naturally inherits several salient features of gradient boosted trees, such as the capability of flexibly modeling nonlinear relationships and robustness to irrelevant features and missing values in auxiliary information. The parameters in MFAI can be automatically determined under the empirical Bayes framework, making it adaptive to the utilization of auxiliary information and immune to overfitting. Moreover, MFAI is computationally efficient and scalable to large datasets by exploiting variational inference. We demonstrate the advantages of MFAI through comprehensive numerical results from simulation studies and real data analyses. Our approach is implemented in the R package mfair available at https://github.com/YangLabHKUST/mfair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02566v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2319160</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics 33 (2024) 1339-1349</arxiv:journal_reference>
      <dc:creator>Zhiwei Wang, Fa Zhang, Cong Zheng, Xianghong Hu, Mingxuan Cai, Can Yang</dc:creator>
    </item>
    <item>
      <title>The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise</title>
      <link>https://arxiv.org/abs/2508.02223</link>
      <description>arXiv:2508.02223v3 Announce Type: replace-cross 
Abstract: Factor analysis (FA) plays a critical role in psychometrics, econometrics, and statistics. Recently, maximum likelihood FA (MLFA) has been applied to direction of arrival (DOA) estimation in unknown nonuniform noise and a variety of iterative approaches have been developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN) method proposed by Stoica and Babu has excellent convergence properties. In this article, the Expectation/Conditional Maximization Either (ECME) algorithm, an extension of the expectation-maximization algorithm, is designed again for MLFA by introducing new complete data, which can thus use two explicit formulas to sequentially update the estimates of parameters at each iteration and have excellent convergence properties. Theoretical analysis shows that the ECME algorithm has almost the same computational complexity at each iteration as the FAAN method. However, numerical results show that the ECME algorithm yields faster stable convergence and the convergence to the global optimum is easier. Importantly, MLFA is not the best choice for the subspace based DOA estimation in unknown nonuniform noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02223v3</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Gong</dc:creator>
    </item>
    <item>
      <title>Modelling the emergence of open-ended cultural evolution</title>
      <link>https://arxiv.org/abs/2508.04828</link>
      <description>arXiv:2508.04828v2 Announce Type: replace-cross 
Abstract: Humans stand alone in terms of their potential to collectively and cumulatively change their culture in an open-ended manner. This open-endedness provides societies with the ability to continually expand their resources and to increase their capacity to store, transmit and process information at a collective-level. Here, we propose that the production of resources arises from the interaction between cultural systems (a society's repertoire of interdependent techniques, artifacts, norms and knowledge) and search spaces (an ensemble of needs, problems and goals facing a society). Starting from this premise we develop a macro-level model wherein both cultural systems and search spaces are subject to evolutionary dynamics. By manipulating the extent to which these dynamics are characterised by stochastic or selection-like processes, we demonstrate that open-ended growth is extremely rare, historically contingent and only possible when cultural systems and search spaces co-evolve. Here, stochastic factors must be strong enough to continually perturb the dynamics into a far-from-equilibrium state, whereas selection-like factors help maintain effectiveness and ensure the sustained production of resources. Only when this co-evolutionary dynamic maintains effective cultural systems, supports the ongoing expansion of the search space and leads to an increased provision of resources do we observe open-ended cultural evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04828v2</guid>
      <category>cs.NE</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1098/rstb.2025.0255</arxiv:DOI>
      <dc:creator>James Winters, Mathieu Charbonneau</dc:creator>
    </item>
    <item>
      <title>Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors</title>
      <link>https://arxiv.org/abs/2512.17632</link>
      <description>arXiv:2512.17632v2 Announce Type: replace-cross 
Abstract: High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\exp(-t^\alpha)$). We investigate a computationally efficient alternative: the \textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\tilde{O}(\sqrt{r(\Sigma)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17632v2</guid>
      <category>stat.ML</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Even He</dc:creator>
    </item>
  </channel>
</rss>
