<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 03:21:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatio-Temporal Differences in Bike Sharing Usage: A Tale of Six Cities</title>
      <link>https://arxiv.org/abs/2412.19294</link>
      <description>arXiv:2412.19294v1 Announce Type: new 
Abstract: This study investigates the spatio-temporal patterns of Bike Sharing System (BSS) usage in six major cities: New York, London, Tokyo, Boston, Chicago and Washington D.C. By analyzing data over a 30-day period with comparable climate and average temperatures, we explored differences in BSS usage between weekdays and weekends in those cities using Jensen-Shannon divergence (JSD) and rank distribution analysis. Our findings reveal significant temporal differences in BSS usage that were commonly observed in all cities, with weekday patterns dominated by commute peaks and weekend patterns reflecting recreational activities. Friday emerges as a transitional day, sharing the characteristics of both weekdays and weekends. Meanwhile, docking station usage rank distributions show remarkable consistency between weekdays and weekends for most cities, with London being a unique anomaly. This study highlights the potential of BSS data to uncover urban mobility patterns and the underlying structures of cities. The results suggest that BSS usage reflects both intrinsic user behavior and external influences such as urban planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19294v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu-ichi Kinoshita, Yuya Bando, Hiroki Sayama</dc:creator>
    </item>
    <item>
      <title>Identifying clusters in Czekanowski's diagram</title>
      <link>https://arxiv.org/abs/2412.19679</link>
      <description>arXiv:2412.19679v1 Announce Type: new 
Abstract: Visualizing data through Czekanowski's diagram has as its aim the illustration of the relationships between objects. Often, obvious clusters of observations are directly visible. However, it is not straightforward to precisely delineate these clusters. This paper presents the development of the package RMaCzek, which now includes features for cluster identification in Czekanowski diagrams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19679v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14708/ma.v51i2.7259</arxiv:DOI>
      <arxiv:journal_reference>Mathematica Applicanda (Matematyka Stosowana) 51(2): 183-198, 2023</arxiv:journal_reference>
      <dc:creator>Krzysztof Bartoszek, Ying Luo</dc:creator>
    </item>
    <item>
      <title>A General Framework of Brain Region Detection And Genetic Variants Selection in Imaging Genetics</title>
      <link>https://arxiv.org/abs/2412.19735</link>
      <description>arXiv:2412.19735v2 Announce Type: new 
Abstract: Imaging genetics is a growing field that employs structural or functional neuroimaging techniques to study individuals with genetic risk variants potentially linked to specific illnesses. This area presents considerable challenges to statisticians due to the heterogeneous information and different data forms it involves. In addition, both imaging and genetic data are typically high-dimensional, creating a "big data squared" problem. Moreover, brain imaging data contains extensive spatial information. Simply vectorizing tensor images and treating voxels as independent features can lead to computational issues and disregard spatial structure. This paper presents a novel statistical method for imaging genetics modeling while addressing all these challenges. We explore a Canonical Correlation Analysis based linear model for the joint modeling of brain imaging, genetic information, and clinical phenotype, enabling the simultaneous detection of significant brain regions and selection of important genetic variants associated with the phenotype outcome. Scalable algorithms are developed to tackle the "big data squared" issue. We apply the proposed method to explore the reaction speed, an indicator of cognitive functions, and its associations with brain MRI and genetic factors using the UK Biobank database. Our study reveals a notable connection between the caudate nucleus region of brain and specific significant SNPs, along with their respective regulated genes, and the reaction speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19735v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siqiang Su, Zhenghao Li, Long Feng, Ting Li</dc:creator>
    </item>
    <item>
      <title>The three most common needs for training on measurement uncertainty</title>
      <link>https://arxiv.org/abs/2412.18616</link>
      <description>arXiv:2412.18616v1 Announce Type: cross 
Abstract: Measurement uncertainty is key to assessing, stating and improving the reliability of measurements. An understanding of measurement uncertainty is the basis for confidence in measurements and is required by many communities; among others in national metrology institutes, accreditation bodies, calibration and testing laboratories, as well as in legal metrology, at universities and in different metrology fields. An important cornerstone to convey an understanding of measurement uncertainty is to provide training.
  This article identifies the status and the needs for training on measurement uncertainty in each of the above communities as well as among those teaching uncertainty. It is the first study to do so across many different disciplines, and it merges many different sources of information with a focus on Europe. As a result, awareness on the training needs of different communities is raised and teachers of uncertainty are supported in addressing their audiences' needs, in improving their uncertainty-specific pedagogical knowledge and by suggestions for training materials and tools.
  The three needs that are most commonly encountered in the communities requiring an understanding of measurement uncertainty, are 1) to address a general lack of training on measurement uncertainty, 2) to gain a better overview of existing training on measurement uncertainty in several communities, and 3) to deliver more training on specific technical topics including use of a Monte Carlo method for propagating probability distributions and treating multivariate measurands and measurement models.
  These needs will serve to guide future developments in uncertainty training and will, ultimately, contribute to increasing the understanding of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18616v1</guid>
      <category>physics.ed-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katy Klauenberg (Physikalisch-Technische Bundesanstalt), Peter Harris (National Physical Laboratory), Philipp M\"ohrke (Universit\"at Konstanz), Francesca Pennecchi (Istituto Nazionale di Ricerca Metrologica - INRiM)</dc:creator>
    </item>
    <item>
      <title>Functional structural equation modeling with latent variables</title>
      <link>https://arxiv.org/abs/2412.19242</link>
      <description>arXiv:2412.19242v1 Announce Type: cross 
Abstract: Handling latent variables in Structural Equation Models (SEMs) in a case where both the latent variables and their corresponding indicators in the measurement error part of the model are random curves presents significant challenges, especially with sparse data. In this paper, we develop a novel family of Functional Structural Equation Models (FSEMs) that incorporate latent variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built upon functional regression models having response variables modeled as underlying GPs. The model flexibly adapts to cases when the random curves' realizations are observed only over a sparse subset of the domain, and the inferential framework is based on a restricted maximum likelihood approach. The advantage of this framework lies in its ability and flexibility in handling various data scenarios, including regularly and irregularly spaced points and thus missing data. To extract smooth estimates for the functional parameters, we employ a penalized likelihood approach that selects the smoothing parameters using a cross-validation method. We evaluate the performance of the proposed model using simulation studies and a real data example, which suggests that our model performs well in practice. The uncertainty associated with the estimates of the functional coefficients is also assessed by constructing confidence regions for each estimate. The goodness of fit indices that are commonly used to evaluate the fit of SEMs are developed for the FSEMs introduced in this paper. Overall, the proposed method is a promising approach for modeling functional data in SEMs with functional latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19242v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Asgari, Valeria Vitelli, Uta Sailer</dc:creator>
    </item>
    <item>
      <title>Revisiting PCA for time series reduction in temporal dimension</title>
      <link>https://arxiv.org/abs/2412.19423</link>
      <description>arXiv:2412.19423v1 Announce Type: cross 
Abstract: Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao, Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series analysis (TSA), enabling the extraction of complex patterns for tasks like classification, forecasting, and regression. Although dimensionality reduction has traditionally focused on the variable space-achieving notable success in minimizing data redundancy and computational complexity-less attention has been paid to reducing the temporal dimension. In this study, we revisit Principal Component Analysis (PCA), a classical dimensionality reduction technique, to explore its utility in temporal dimension reduction for time series data. It is generally thought that applying PCA to the temporal dimension would disrupt temporal dependencies, leading to limited exploration in this area. However, our theoretical analysis and extensive experiments demonstrate that applying PCA to sliding series windows not only maintains model performance, but also enhances computational efficiency. In auto-regressive forecasting, the temporal structure is partially preserved through windowing, and PCA is applied within these windows to denoise the time series while retaining their statistical information. By preprocessing time-series data with PCA, we reduce the temporal dimensionality before feeding it into TSA models such as Linear, Transformer, CNN, and RNN architectures. This approach accelerates training and inference and reduces resource consumption. Notably, PCA improves Informer training and inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%, without sacrificing model accuracy. Comparative analysis against other reduction methods further highlights the effectiveness of PCA in improving the efficiency of TSA models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19423v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Gao, Wenbo Hu, Yuntian Chen</dc:creator>
    </item>
    <item>
      <title>Direct estimates of irreversibility from time series</title>
      <link>https://arxiv.org/abs/2412.19772</link>
      <description>arXiv:2412.19772v1 Announce Type: cross 
Abstract: The arrow of time can be quantified through the Kullback-Leibler divergence ($D_{KL}$) between the distributions of forward and reverse trajectories in a system. Many approaches to estimate this rely on specific models, but the use of incorrect models can introduce uncontrolled errors. Here, we describe a model-free method that uses trajectory data directly to estimate the evidence for irreversibility over finite windows of time. To do this we build on previous work to identify and correct for errors that arise from limited sample size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data generated by well understood models of nonequilibrium systems. We apply our method to trajectories of neural activity in the retina as it responds to naturalistic inputs, and find evidence of irreversibility in single neurons, emphasizing the non-Markovian character of these data. These results open new avenues for investigating how the brain represents the arrow of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19772v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Trevor GrandPre, Gianluca Teza, William Bialek</dc:creator>
    </item>
    <item>
      <title>The Impacts of the Gender Imbalance on the Marriage Market: Evidence from World War II in Japan</title>
      <link>https://arxiv.org/abs/2102.00687</link>
      <description>arXiv:2102.00687v4 Announce Type: replace-cross 
Abstract: This study uses the unprecedented changes in the sex ratio due to the losses of men during World War II to identify the impacts of the gender imbalance on marriage market outcomes in Japan. Using newly digitized census-based historical statistics, we find evidence that men have a stronger bargaining position in the marriage market than women do. Under the conditions of relative male scarcity, women are less likely to marry. Although the entry of younger cohorts with a natural gender balance into the marriage market attenuated its magnitude, this tendency persisted until the mid-1950s. Widowed women facing male scarcity are particularly unable to remarry. Our results suggest that reinstating military pensions in the early 1950s further reduced their incentive to remarry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.00687v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.labeco.2024.102653</arxiv:DOI>
      <arxiv:journal_reference>Labour Economics, Volume 92, January 2025, 102653</arxiv:journal_reference>
      <dc:creator>Kota Ogasawara, Erika Igarashi</dc:creator>
    </item>
    <item>
      <title>Loss-based prior for tree topologies in BART models</title>
      <link>https://arxiv.org/abs/2404.00359</link>
      <description>arXiv:2404.00359v2 Announce Type: replace-cross 
Abstract: We present a novel prior for tree topology within Bayesian Additive Regression Trees (BART) models. This approach quantifies the hypothetical loss in information and the loss due to complexity associated with choosing the wrong tree structure. The resulting prior distribution is compellingly geared toward sparsity, a critical feature considering BART models' tendency to overfit. Our method incorporates prior knowledge into the distribution via two parameters that govern the tree's depth and balance between its left and right branches. Additionally, we propose a default calibration for these parameters, offering an objective version of the prior. We demonstrate our method's efficacy on both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00359v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F. Serafini, F. Leisen, C. Villa, K. Wilson</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v4 Announce Type: replace-cross 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package, IJSE, that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Modeling Zero-Inflated Correlated Dental Data through Gaussian Copulas and Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2410.13949</link>
      <description>arXiv:2410.13949v2 Announce Type: replace-cross 
Abstract: We develop a new longitudinal count data regression model that accounts for zero-inflation and spatio-temporal correlation across responses. This project is motivated by an analysis of Iowa Fluoride Study (IFS) data, a longitudinal cohort study with data on caries (cavity) experience scores measured for each tooth across five time points. To that end, we use a hurdle model for zero-inflation with two parts: the presence model indicating whether a count is non-zero through logistic regression and the severity model that considers the non-zero counts through a shifted Negative Binomial distribution allowing overdispersion. To incorporate dependence across measurement occasion and teeth, these marginal models are embedded within a Gaussian copula that introduces spatio-temporal correlations. A distinct advantage of this formulation is that it allows us to determine covariate effects with population-level (marginal) interpretations in contrast to mixed model choices. Standard Bayesian sampling from such a model is infeasible, so we use approximate Bayesian computing for inference. This approach is applied to the IFS data to gain insight into the risk factors for dental caries and the correlation structure across teeth and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13949v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Mukherjee, Jeremy T. Gaskins, Shoumi Sarkar, Steven Levy, Somnath Datta</dc:creator>
    </item>
  </channel>
</rss>
