<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 May 2024 04:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Champuru 2: Improved scoring of alignments and a user-friendly graphical interface</title>
      <link>https://arxiv.org/abs/2405.06032</link>
      <description>arXiv:2405.06032v1 Announce Type: new 
Abstract: Champuru is a web software tool that helps determine the two sequences present in mixed Sanger chromatograms obtained by sequencing simultaneously two DNA templates of unequal lengths. A previous version (Champuru 1.0) was published as a simple Perl CGI (Common Gateway Interface) application, but the server hosting it was discontinued, which prompted us to update it and develop it further. The new Champuru 2, implemented in Haxe and hosted at GitHub Pages, offers an improved graphical user interface as well as more sophisticated algorithms to compute alignment scores, making it more efficient at detecting the most likely alignment positions between forward and reverse traces. Champuru 2 now make it possible to analyze other offset pairs than the one detected as most likely by the selected algorithm. Champuru 2 is freely accessible at https://eeg-ebe.github.io/Champuru/, including both a graphical user interface (running a JavaScript version transpiled from the Haxe source code) and a compiled command-line version (obtained by transpiling the Haxe source code into C++).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06032v1</guid>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yann Sp\"ori, Jean-Fran\c{c}ois Flot</dc:creator>
    </item>
    <item>
      <title>Automated Cell Structure Extraction for 3D Electron Microscopy by Deep Learning</title>
      <link>https://arxiv.org/abs/2405.06303</link>
      <description>arXiv:2405.06303v1 Announce Type: new 
Abstract: Modeling the 3D structures of cells and tissues is crucial in biology. Sequential cross-sectional images from electron microscopy provide high-resolution intracellular structure information. Segmentation of complex cell structures remains a laborious manual task for experts, demanding time and effort. This bottleneck in analyzing biological images requires efficient and automated solutions. This study explores deep learning-based automated segmentation of biological images, enabling accurate reconstruction of the 3D structures of cells and organelles. We constructed an analysis system for the cell images of Cyanidioschyzon merolae, a primitive unicellular red algae. This system utilizes sequential cross-sectional images captured by Focused Ion Beam Scanning Electron Microscopes (FIB-SEM). We adopted the U-Net and performed pre-training to identify and segment cell organelles from single-cell images. In addition, we employed the Segmentation Anything Model (SAM) and the 3D watershed algorithm to extract individual 3D images of each cell from large-scale microscope images containing numerous cells. Finally, we applied the pre-trained U-Net to segment each structure within these 3D images. Through this procedure, we could fully automate the creation of 3D cell models. Our approach would apply to other cell types, and we aim to build a versatile analysis system. We will also explore adopting other deep learning techniques and combinations of image processing methods to further enhance segmentation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06303v1</guid>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Kousaka, Atsuko H. Iwane, Yuichi Togashi</dc:creator>
    </item>
    <item>
      <title>Towards Less Biased Data-driven Scoring with Deep Learning-Based End-to-end Database Search in Tandem Mass Spectrometry</title>
      <link>https://arxiv.org/abs/2405.06511</link>
      <description>arXiv:2405.06511v1 Announce Type: new 
Abstract: Peptide identification in mass spectrometry-based proteomics is crucial for understanding protein function and dynamics. Traditional database search methods, though widely used, rely on heuristic scoring functions and statistical estimations have to be introduced for a higher identification rate. Here, we introduce DeepSearch, the first deep learning-based end-to-end database search method for tandem mass spectrometry. DeepSearch leverages a modified transformer-based encoder-decoder architecture under the contrastive learning framework. Unlike conventional methods that rely on ion-to-ion matching, DeepSearch adopts a data-driven approach to score peptide spectrum matches. DeepSearch is also the first deep learning-based method that can profile variable post-translational modifications in a zero-shot manner. We showed that DeepSearch's scoring scheme expressed less bias and did not require any statistical estimation. We validated DeepSearch's accuracy and robustness across various datasets, including those from species with diverse protein compositions and a modification-enriched dataset. DeepSearch sheds new light on database search methods in tandem mass spectrometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06511v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yonghan Yu, Ming Li</dc:creator>
    </item>
    <item>
      <title>Phylo2Vec: a vector representation for binary trees</title>
      <link>https://arxiv.org/abs/2304.12693</link>
      <description>arXiv:2304.12693v3 Announce Type: replace-cross 
Abstract: Binary phylogenetic trees inferred from biological data are central to understanding the shared history among evolutionary units. However, inferring the placement of latent nodes in a tree is NP-hard and thus computationally expensive. State-of-the-art methods rely on carefully designed heuristics for tree search. These methods use different data structures for easy manipulation (e.g., classes in object-oriented programming languages) and readable representation of trees (e.g., Newick-format strings). Here, we present Phylo2Vec, a parsimonious encoding for phylogenetic trees that serves as a unified approach for both manipulating and representing phylogenetic trees. Phylo2Vec maps any binary tree with $n$ leaves to a unique integer vector of length $n-1$. The advantages of Phylo2Vec are fourfold: i) fast tree sampling, (ii) compressed tree representation compared to a Newick string, iii) quick and unambiguous verification if two binary trees are identical topologically, and iv) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use Phylo2Vec for maximum likelihood inference on five real-world datasets and show that a simple hill-climbing-based optimisation scheme can efficiently traverse the vastness of tree space from a random to an optimal tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12693v3</guid>
      <category>q-bio.PE</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J Penn, Neil Scheidwasser, Mark P Khurana, David A Duch\^ene, Christl A Donnelly, Samir Bhatt</dc:creator>
    </item>
  </channel>
</rss>
