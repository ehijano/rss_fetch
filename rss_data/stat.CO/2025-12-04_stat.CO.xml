<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:33:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both</title>
      <link>https://arxiv.org/abs/2512.03225</link>
      <description>arXiv:2512.03225v1 Announce Type: new 
Abstract: We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03225v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christophe Andrieu, Nicolas Chopin, Ettore Fincato, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Numerical optimization for the compatibility constant of the lasso</title>
      <link>https://arxiv.org/abs/2512.03321</link>
      <description>arXiv:2512.03321v1 Announce Type: new 
Abstract: Compatibility condition and compatibility constant have been commonly used to evaluate the prediction error of the lasso when the number of variables exceeds the number of observations. However, the computation of the compatibility constant is generally difficult because it is a complicated nonlinear optimization problem. In this study, we present a numerical approach to compute the compatibility constant when the zero/nonzero pattern of true regression coefficients is given. We show that the optimization problem reduces to a quadratic program (QP) once the signs of the nonzero coefficients are specified. In this case, the compatibility constant can be obtained by solving QPs for all possible sign combinations. We also formulate a mixed-integer quadratic programming (MIQP) approach that can be applied when the number of true nonzero coefficients is moderately large. We investigate the finite-sample behavior of the compatibility constant for simulated data under a wide variety of parameter settings and compare the mean squared error with its theoretical error bound based on the compatibility constant. The behavior of the compatibility constant in finite samples is also investigated through a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03321v1</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kei Hirose</dc:creator>
    </item>
    <item>
      <title>SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models</title>
      <link>https://arxiv.org/abs/2512.03322</link>
      <description>arXiv:2512.03322v1 Announce Type: new 
Abstract: Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03322v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey J. McLachlan, Jinran Wu</dc:creator>
    </item>
    <item>
      <title>Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration</title>
      <link>https://arxiv.org/abs/2512.03102</link>
      <description>arXiv:2512.03102v1 Announce Type: cross 
Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03102v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiwei Shi, Hongnan Ma, Mengyue Yang, Cunjia Liu, Weiru Liu</dc:creator>
    </item>
    <item>
      <title>Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients</title>
      <link>https://arxiv.org/abs/2311.05025</link>
      <description>arXiv:2311.05025v4 Announce Type: replace 
Abstract: We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon&gt;0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method's computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that in large-scale applications, the unbiased algorithm we present can be 2-3 orders of magnitude more efficient than the ``gold-standard" randomized Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05025v4</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil K. Chada, Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</dc:creator>
    </item>
    <item>
      <title>Gradient-free optimization via integration</title>
      <link>https://arxiv.org/abs/2408.00888</link>
      <description>arXiv:2408.00888v2 Announce Type: replace 
Abstract: We develop and analyse an approach to optimize functions $l\colon \mathbb{R}^d
  \rightarrow \mathbb{R}$ not assumed to be convex, differentiable or even
  continuous. The algorithm belongs to the class of model-based search methods. The idea is to fit recursively $l$ to a parametric family of distributions, using a Bayesian update followed by a reprojection back
  onto the chosen family. Remarkably, reprojection in our scenario boils down to computing
  expectations, which can be simply approximated through Monte Carlo. We
  show that when the family of distributions is appropriately chosen this approach can be interpreted as an implicit time-inhomogeneous
  gradient descent algorithm on a sequence of smoothed approximations of $l$, providing a route to establishing convergence. We
  establish new results for generic inhomogeneous gradient descent algorithms, which we specialise to the model-based search algorithm in the Gaussian scenario. We illustrate the performance of the algorithm on a
  challenging classification task in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00888v2</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christophe Andrieu, Nicolas Chopin, Ettore Fincato, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Semiparametric Robust Estimation of Population Location</title>
      <link>https://arxiv.org/abs/2512.03021</link>
      <description>arXiv:2512.03021v2 Announce Type: replace 
Abstract: Real-world measurements often comprise a dominant signal contaminated by a noisy background. Robustly estimating the dominant signal in practice has been a fundamental statistical problem. Classically, mixture models have been used to cluster the heterogeneous population into homogeneous components. Modeling such data with fully parametric models risks bias under misspecification, while fully nonparametric approaches can dissipate power and computational resources. We propose a middle path: a semiparametric method that models only the dominant component parametrically and leaves the background completely nonparametric, yet remains computationally scalable and statistically robust. So instead of outlier downweighting, traditionally done in robust statistics literature, we maximize the observed likelihood such that the noisy background is absorbed by the nonparametric component. Computationally, we propose a new approximate FFT-accelerated likelihood maximization algorithm. Empirically, this FFT plug-in achieves order-of-magnitude speedups over vanilla weighted EM while preserving statistical accuracy and large sample properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03021v2</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananyabrata Barua, Ayanendranath Basu</dc:creator>
    </item>
  </channel>
</rss>
