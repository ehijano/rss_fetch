<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sharp Convergence Rates of Empirical Unbalanced Optimal Transport for Spatio-Temporal Point Processes</title>
      <link>https://arxiv.org/abs/2509.04225</link>
      <description>arXiv:2509.04225v1 Announce Type: new 
Abstract: We statistically analyze empirical plug-in estimators for unbalanced optimal transport (UOT) formalisms, focusing on the Kantorovich-Rubinstein distance, between general intensity measures based on observations from spatio-temporal point processes. Specifically, we model the observations by two weakly time-stationary point processes with spatial intensity measures $\mu$ and $\nu$ over the expanding window $(0,t]$ as $t$ increases to infinity, and establish sharp convergence rates of the empirical UOT in terms of the intrinsic dimensions of the measures. We assume a sub-quadratic temporal growth condition of the variance of the process, which allows for a wide range of temporal dependencies. As the growth approaches quadratic, the convergence rate becomes slower. This variance assumption is related to the time-reduced factorial covariance measure, and we exemplify its validity for various point processes, including the Poisson cluster, Hawkes, Neyman-Scott, and log-Gaussian Cox processes. Complementary to our upper bounds, we also derive matching lower bounds for various spatio-temporal point processes of interest and establish near minimax rate optimality of the empirical Kantorovich-Rubinstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04225v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Struleva, Shayan Hundrieser, Dominic Schuhmacher, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Comment on "Deep Regression Learning with Optimal Loss Function"</title>
      <link>https://arxiv.org/abs/2509.03702</link>
      <description>arXiv:2509.03702v1 Announce Type: cross 
Abstract: OpenReview benefits the peer-review system by promoting transparency, openness, and collaboration. By making reviews, comments, and author responses publicly accessible, the platform encourages constructive feedback, reduces bias, and allows the research community to engage directly in the review process. This level of openness fosters higher-quality reviews, greater accountability, and continuous improvement in scholarly communication. In the statistics community, such a transparent and open review system has not traditionally existed. This lack of transparency has contributed to significant variation in the quality of published papers, even in leading journals, with some containing substantial errors in both proofs and numerical analyses. To illustrate this issue, this note examines several results from Wang, Zhou and Lin (2025) [arXiv:2309.12872; https://doi.org/10.1080/01621459.2024.2412364] and highlights potential errors in their proofs, some of which are strikingly obvious. This raises a critical question: how important are mathematical proofs in statistical journals, and how should they be rigorously verified? Addressing this question is essential not only for maintaining academic rigor but also for fostering the right attitudes toward scholarship and quality assurance in the field. A plausible approach would be for arXiv to provide an anonymous discussion section, allowing readers-whether anonymous or not-to post comments, while also giving authors the opportunity to respond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03702v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Li</dc:creator>
    </item>
    <item>
      <title>Testing for correlation between network structure and high-dimensional node covariates</title>
      <link>https://arxiv.org/abs/2509.03772</link>
      <description>arXiv:2509.03772v1 Announce Type: cross 
Abstract: In many application domains, networks are observed with node-level features. In such settings, a common problem is to assess whether or not nodal covariates are correlated with the network structure itself. Here, we present four novel methods for addressing this problem. Two of these are based on a linear model relating node-level covariates to latent node-level variables that drive network structure. The other two are based on applying canonical correlation analysis to the node features and network structure, avoiding the linear modeling assumptions. We provide theoretical guarantees for all four methods when the observed network is generated according to a low-rank latent space model endowed with node-level covariates, which we allow to be high-dimensional. Our methods are computationally cheaper and require fewer modeling assumptions than previous approaches to network dependency testing. We demonstrate and compare the performance of our novel methods on both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03772v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Fuchs-Kreiss, Keith Levin</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Arm in One-Shot Multi-Arm RCTs: The Asymptotic Minimax-Regret Decision Framework for the Best-Population Selection Problem</title>
      <link>https://arxiv.org/abs/2509.03796</link>
      <description>arXiv:2509.03796v1 Announce Type: cross 
Abstract: We develop a frequentist decision-theoretic framework for selecting the best arm in one-shot, multi-arm randomized controlled trials (RCTs). Our approach characterizes the minimax-regret (MMR) optimal decision rule for any location-family reward distribution with full support. We show that the MMR rule is deterministic, unique, and computationally tractable, as it can be derived by solving the dual problem with nature's least-favorable prior. We then specialize to the case of multivariate normal (MVN) rewards with an arbitrary covariance matrix, and establish the local asymptotic minimaxity of a plug-in version of the rule when only estimated means and covariances are available. This asymptotic MMR (AMMR) procedure maps a covariance-matrix estimate directly into decision boundaries, allowing straightforward implementation in practice. Our analysis highlights a sharp contrast between two-arm and multi-arm designs. With two arms, the empirical success rule ("pick-the-winner") remains MMR-optimal, regardless of the arm-specific variances. By contrast, with three or more arms and heterogeneous variances, the empirical success rule is no longer optimal: the MMR decision boundaries become nonlinear and systematically penalize high-variance arms, requiring stronger evidence to select them. This result underscores that variance plays no role in optimal two-arm comparisons, but it matters critically when more than two options are on the table. Our multi-arm AMMR framework extends classical decision theory to multi-arm RCTs, offering a rigorous foundation and a practical tool for comparing multiple policies simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03796v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonhwi Joo</dc:creator>
    </item>
    <item>
      <title>The exact distribution of the conditional likelihood-ratio test in instrumental variables regression</title>
      <link>https://arxiv.org/abs/2509.04144</link>
      <description>arXiv:2509.04144v1 Announce Type: cross 
Abstract: We derive the exact asymptotic distribution of the conditional likelihood-ratio test in instrumental variables regression under weak instrument asymptotics and for multiple endogenous variables. The distribution is conditional on all eigenvalues of the concentration matrix, rather than only the smallest eigenvalue as in an existing asymptotic upper bound. This exact characterization leads to a substantially more powerful test if there are differently identified endogenous variables. We provide computational methods implementing the test and demonstrate the power gains through numerical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04144v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte Londschien</dc:creator>
    </item>
    <item>
      <title>Conformalized Multiple Testing under Unknown Null Distribution with Symmetric Errors</title>
      <link>https://arxiv.org/abs/2509.04231</link>
      <description>arXiv:2509.04231v1 Announce Type: cross 
Abstract: This article addresses a fundamental concern, first raised by Efron (2004), regarding the selection of null distributions in large-scale multiple testing. In modern data-intensive applications involving thousands or even millions of hypotheses, the theoretical null distribution of the test statistics often deviates from the true underlying null distribution, severely compromising the false discovery rate (FDR) analysis. We propose a conformalized empirical Bayes method using self-calibrated empirical null samples (SENS) for both one-sample and two-sample multiple testing problems. The new framework not only sidesteps the use of potentially erroneous theoretical null distributions, which is common in conventional practice, but also mitigates the impact of estimation errors in the unknown null distribution on the validity of FDR control, a challenge frequently encountered in the empirical Bayes FDR literature. In contrast to the empirical Bayes approaches (cf. Efron, 2004; Jin and Cai, 2007; Sun and Cai, 2007) that rely on Gaussian assumptions for the null models, SENS imposes only a weak condition on the symmetry of the error distribution, and leverages conformal tools to achieve FDR control in finite samples. Moreover, SENS incorporates structural insights from empirical Bayes into inference, exhibiting higher power compared to frequentist model-free methods. We conduct an in-depth analysis to establish a novel optimality theory for SENS under Efron's two-group model and demonstrate its superiority over existing empirical Bayes FDR methods and recent model-free FDR methods through numerical experiments on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04231v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Tian, Zinan Zhao, Wenguang Sun</dc:creator>
    </item>
    <item>
      <title>Connections between reinforcement learning with feedback,test-time scaling, and diffusion guidance: An anthology</title>
      <link>https://arxiv.org/abs/2509.04372</link>
      <description>arXiv:2509.04372v1 Announce Type: cross 
Abstract: In this note, we reflect on several fundamental connections among widely used post-training techniques. We clarify some intimate connections and equivalences between reinforcement learning with human feedback, reinforcement learning with internal feedback, and test-time scaling (particularly soft best-of-$N$ sampling), while also illuminating intrinsic links between diffusion guidance and test-time scaling. Additionally, we introduce a resampling approach for alignment and reward-directed diffusion models, sidestepping the need for explicit reinforcement learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04372v1</guid>
      <category>stat.ML</category>
      <category>cs.GL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Jiao, Yuxin Chen, Gen Li</dc:creator>
    </item>
    <item>
      <title>Minimum $\Phi$-distance estimators for finite mixing measures</title>
      <link>https://arxiv.org/abs/2304.10052</link>
      <description>arXiv:2304.10052v2 Announce Type: replace 
Abstract: Finite mixture models have long been used across a variety of fields in engineering and sciences. Recently there has been a great deal of interest in quantifying the convergence behavior of the \emph{mixing measure}, a fundamental object that encapsulates all unknown parameters in a mixture distribution. In this paper we propose a general framework for estimating the mixing measure arising in finite mixture models, which we term minimum $\Phi$-distance estimators. We establish a general theory for the minimum $\Phi$-distance estimator, where sharp probability bounds are obtained on the estimation error for the mixing measures in terms of the suprema of the associated empirical processes for a suitably chosen function class $\Phi$. Our framework includes several existing and seemingly distinct estimation methods as special cases using a weakened identifiability condition, but also motivates new estimators. For instance, it extends the minimum Kolmogorov-Smirnov distance estimator to the multivariate setting, and it extends the method of moments to cover a broader family of probability kernels beyond the Gaussian. Moreover, it also includes methods that are applicable to complex (e.g., non-Euclidean) observation domains, using tools from reproducing kernel Hilbert spaces. It will be shown that under general conditions the methods achieve optimal rates of estimation under Wasserstein metrics in either minimax or pointwise sense of convergence; the latter case can be achieved when no upper bound on the finite number of components is given. Also of interest is a sharp inequality that captures the local information geometry for general mixture models precisely in terms of moment differences between mixing measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10052v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yun Wei, Sayan Mukherjee, XuanLong Nguyen</dc:creator>
    </item>
    <item>
      <title>Reduce-Rank Matrix Integer-Valued Autoregressive Model</title>
      <link>https://arxiv.org/abs/2509.03338</link>
      <description>arXiv:2509.03338v2 Announce Type: replace 
Abstract: Integer-valued time series are widely present in many fields, such as finance, economics, disease transmission, and traffic flow. With data dimensions surging, the traditional multivariate generalized integer autoregressive (MGINAR) model faces parameter overload, poor interpretability, and structural information loss. Matrix integer-valued autoregression (MINAR) model captures row-column cross-correlations and reduces the number of parameters to be estimated. However, further growth in dimensionality causes data redundancy, which degrades the MINAR model's performance and increases the number of parameters. To solve the limitations of the MINAR model described above, this paper proposes the reduced-rank matrix integer-valued autoregression (RRMINAR) model. Reducing rank is achieved by adding low-rank constraints to the coefficient matrices in the MINAR model, leading to RRMINAR reducing parameter quantity while incorporating matrix structure information. We develop an iterative conditional least squares estimation and analyze its asymptotic properties. Simulation results demonstrate that the proposed RRMINAR model exhibits more robust parameter estimation and higher prediction accuracy than MGINAR and MINAR models when the data structure is low-rank. Empirical analysis using criminal data validates the proposed RRMINAR model's effectiveness and uncovers structural temporal-spatial information in criminal behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03338v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyan Cui, Tianyun Guo, Suping Wang</dc:creator>
    </item>
    <item>
      <title>Bootstrapping the Cross-Validation Estimate</title>
      <link>https://arxiv.org/abs/2307.00260</link>
      <description>arXiv:2307.00260v2 Announce Type: replace-cross 
Abstract: Cross-validation is a widely used technique for evaluating the performance of prediction models, ranging from simple binary classification to complex precision medicine strategies. It helps correct for optimism bias in error estimates, which can be significant for models built using complex statistical learning algorithms. However, since the cross-validation estimate is a random value dependent on observed data, it is essential to accurately quantify the uncertainty associated with the estimate. This is especially important when comparing the performance of two models using cross-validation, as one must determine whether differences in estimated error are due to chance. Although various methods have been developed to make inferences on cross-validation estimates, they often have many limitations, such as requiring stringent model assumptions. This paper proposes a fast bootstrap method that quickly estimates the standard error of the cross-validation estimate and produces valid confidence intervals for a population parameter measuring average model performance. Our method overcomes the computational challenges inherent in bootstrapping a cross-validation estimate by estimating the variance component within a random-effects model. It is also as flexible as the cross-validation procedure itself. To showcase the effectiveness of our approach, we conducted comprehensive simulations and real-data analysis across two applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00260v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryan Cai, Yuanhui Luo, Xinzhou Guo, Fabio Pellegrini, Menglan Pang, Carl de Moor, Changyu Shen, Vivek Charu, Lu Tian</dc:creator>
    </item>
    <item>
      <title>The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation</title>
      <link>https://arxiv.org/abs/2505.23445</link>
      <description>arXiv:2505.23445v2 Announce Type: replace-cross 
Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a measure becomes a target, it ceases to be a good measure''. As machine learning models and the optimisation capacity to train them grow, growing empirical evidence reinforced the belief in the validity of this law without however being formalised. Recently, a few attempts were made to formalise Goodhart's law, either by categorising variants of it, or by looking at how optimising a proxy metric affects the optimisation of an intended goal. In this work, we alleviate the simplifying independence assumption, made in previous works, and the assumption on the learning paradigm made in most of them, to study the effect of the coupling between the proxy metric and the intended goal on Goodhart's law. Our results show that in the case of light tailed goal and light tailed discrepancy, dependence does not change the nature of Goodhart's effect. However, in the light tailed goal and heavy tailed discrepancy case, we exhibit an example where over-optimisation occurs at a rate inversely proportional to the heavy tailedness of the discrepancy between the goal and the metric. %</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23445v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Majka, El-Mahdi El-Mhamdi</dc:creator>
    </item>
  </channel>
</rss>
