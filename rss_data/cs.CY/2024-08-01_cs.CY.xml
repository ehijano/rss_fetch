<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Measuring Falseness in News Articles based on Concealment and Overstatement</title>
      <link>https://arxiv.org/abs/2408.00156</link>
      <description>arXiv:2408.00156v1 Announce Type: new 
Abstract: This research investigates the extent of misinformation in certain journalistic articles by introducing a novel measurement tool to assess the degrees of falsity. It aims to measure misinformation using two metrics (concealment and overstatement) to explore how information is interpreted as false. This should help examine how articles containing partly true and partly false information can potentially harm readers, as they are more challenging to identify than completely fabricated information. In this study, the full story provided by the fact-checking website serves as a standardized source of information for comparing differences between fake and real news. The result suggests that false news has greater concealment and overstatement, due to longer and more complex new stories being shortened and ambiguously phrased. While there are no major distinctions among categories of politics science and civics, it demonstrates that misinformation lacks crucial details while simultaneously containing more redundant words. Hence, news articles containing partial falsity, categorized as misinformation, can deceive inattentive readers who lack background knowledge. Hopefully, this approach instigates future fact-checkers, journalists, and the readers to secure high quality articles for a resilient information environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00156v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiyoung Lee, Keeheon Lee</dc:creator>
    </item>
    <item>
      <title>A Taxonomy of Stereotype Content in Large Language Models</title>
      <link>https://arxiv.org/abs/2408.00162</link>
      <description>arXiv:2408.00162v1 Announce Type: new 
Abstract: This study introduces a taxonomy of stereotype content in contemporary large language models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three powerful and widely used LLMs, for the characteristics associated with 87 social categories (e.g., gender, race, occupations). We identify 14 stereotype dimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for ~90% of LLM stereotype associations. Warmth and Competence facets were the most frequent content, but all other dimensions were significantly prevalent. Stereotypes were more positive in LLMs (vs. humans), but there was significant variability across categories and dimensions. Finally, the taxonomy predicted the LLMs' internal evaluations of social categories (e.g., how positively/negatively the categories were represented), supporting the relevance of a multidimensional taxonomy for characterizing LLM stereotypes. Our findings suggest that high-dimensional human stereotypes are reflected in LLMs and must be considered in AI auditing and debiasing to minimize unidentified harms from reliance in low-dimensional views of bias in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00162v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gandalf Nicolas, Aylin Caliskan</dc:creator>
    </item>
    <item>
      <title>The Monetisation of Toxicity: Analysing YouTube Content Creators and Controversy-Driven Engagement</title>
      <link>https://arxiv.org/abs/2408.00534</link>
      <description>arXiv:2408.00534v1 Announce Type: new 
Abstract: YouTube is a major social media platform that plays a significant role in digital culture, with content creators at its core. These creators often engage in controversial behaviour to drive engagement, which can foster toxicity. This paper presents a quantitative analysis of controversial content on YouTube, focusing on the relationship between controversy, toxicity, and monetisation. We introduce a curated dataset comprising 20 controversial YouTube channels extracted from Reddit discussions, including 16,349 videos and more than 105 million comments. We identify and categorise monetisation cues from video descriptions into various models, including affiliate marketing and direct selling, using lists of URLs and keywords. Additionally, we train a machine learning model to measure the toxicity of comments in these videos. Our findings reveal that while toxic comments correlate with higher engagement, they negatively impact monetisation, indicating that controversy-driven interaction does not necessarily lead to financial gain. We also observed significant variation in monetisation strategies, with some creators showing extensive monetisation despite high toxicity levels. Our study introduces a curated dataset, lists of URLs and keywords to categorise monetisation, a machine learning model to measure toxicity, and is a significant step towards understanding the complex relationship between controversy, engagement, and monetisation on YouTube. The lists used for detecting and categorising monetisation cues are available on https://github.com/thalesbertaglia/toxmon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00534v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3677117.3685005</arxiv:DOI>
      <dc:creator>Thales Bertaglia, Catalina Goanta, Adriana Iamnitchi</dc:creator>
    </item>
    <item>
      <title>Speed Limit Reduction Enhances Urban Worker Safety: Evidence from a Decade of Traffic Incidents in Santiago, Chile</title>
      <link>https://arxiv.org/abs/2408.00687</link>
      <description>arXiv:2408.00687v1 Announce Type: new 
Abstract: Work-related traffic incidents significantly impact urban mobility and productivity. This study analyzes a decade of work-related traffic incident data (2012--2021) in Santiago, Chile, using records from a major social insurance company. We explore temporal, spatial, and demographic patterns in these incidents in urban and rural areas. We also evaluate the impact of a 2018 urban speed limit reduction law on incident injury severity. Using negative binomial regression, we assess how various factors, including the speed limit change, affect injury severity measured by prescribed medical leave days.
  Our analysis reveals distinct incident occurrence and severity patterns across different times, locations, and demographic groups. We find that motorcycles and cycles are associated with more severe injuries, with marginal effects of 26.94 and 13.06 additional days of medical leave, respectively, compared to motorized vehicles. Female workers tend to have less severe injuries, with an average of 7.57 fewer days of medical leave. Age is also a significant factor, with each year associated with 0.57 additional days of leave.
  Notably, the urban speed limit reduction is associated with a decrease of 4.26 days in prescribed medical leave for incidents in urban areas, suggesting that lower speed limits contribute to reduced injury severity in work-related traffic incidents. Our results provide insights for urban planning, transportation policy, and workplace safety initiatives, highlighting the potential benefits of speed management in urban areas for improving road safety and minimizing the economic impact of work-related incidents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00687v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo Graells-Garrido, Mat\'ias Toro, Gabriel Mansilla, Mat\'ias Nicolai, Santiago Mansilla, Jocelyn Dunstan</dc:creator>
    </item>
    <item>
      <title>Replication in Visual Diffusion Models: A Survey and Outlook</title>
      <link>https://arxiv.org/abs/2408.00001</link>
      <description>arXiv:2408.00001v1 Announce Type: cross 
Abstract: Visual diffusion models have revolutionized the field of creative AI, producing high-quality and diverse content. However, they inevitably memorize training images or videos, subsequently replicating their concepts, content, or styles during inference. This phenomenon raises significant concerns about privacy, security, and copyright within generated outputs. In this survey, we provide the first comprehensive review of replication in visual diffusion models, marking a novel contribution to the field by systematically categorizing the existing studies into unveiling, understanding, and mitigating this phenomenon. Specifically, unveiling mainly refers to the methods used to detect replication instances. Understanding involves analyzing the underlying mechanisms and factors that contribute to this phenomenon. Mitigation focuses on developing strategies to reduce or eliminate replication. Beyond these aspects, we also review papers focusing on its real-world influence. For instance, in the context of healthcare, replication is critically worrying due to privacy concerns related to patient data. Finally, the paper concludes with a discussion of the ongoing challenges, such as the difficulty in detecting and benchmarking replication, and outlines future directions including the development of more robust mitigation techniques. By synthesizing insights from diverse studies, this paper aims to equip researchers and practitioners with a deeper understanding at the intersection between AI technology and social good. We release this project at https://github.com/WangWenhao0716/Awesome-Diffusion-Replication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00001v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenhao Wang, Yifan Sun, Zongxin Yang, Zhengdong Hu, Zhentao Tan, Yi Yang</dc:creator>
    </item>
    <item>
      <title>Deceptive AI systems that give explanations are more convincing than honest AI systems and can amplify belief in misinformation</title>
      <link>https://arxiv.org/abs/2408.00024</link>
      <description>arXiv:2408.00024v1 Announce Type: cross 
Abstract: Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and erode trust in the truth. We examined the impact of deceptive AI generated explanations on individuals' beliefs in a pre-registered online experiment with 23,840 observations from 1,192 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from these effects caused by deceptive AI generated explanations. Instead, our results show that the logical validity of AI generated deceptive explanations, that is whether the explanation has a causal effect on the truthfulness of the AI's classification, plays a critical role in countering their persuasiveness - with logically invalid explanations being deemed less credible. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00024v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valdemar Danry, Pat Pataranutaporn, Matthew Groh, Ziv Epstein, Pattie Maes</dc:creator>
    </item>
    <item>
      <title>If It Looks Like a Rootkit and Deceives Like a Rootkit: A Critical Examination of Kernel-Level Anti-Cheat Systems</title>
      <link>https://arxiv.org/abs/2408.00500</link>
      <description>arXiv:2408.00500v1 Announce Type: cross 
Abstract: Addressing a critical aspect of cybersecurity in online gaming, this paper systematically evaluates the extent to which kernel-level anti-cheat systems mirror the properties of rootkits, highlighting the importance of distinguishing between protective and potentially invasive software. After establishing a definition for rootkits (making distinctions between rootkits and simple kernel-level applications) and defining metrics to evaluate such software, we introduce four widespread kernel-level anti-cheat solutions. We lay out the inner workings of these types of software, assess them according to our previously established definitions, and discuss ethical considerations and the possible privacy infringements introduced by such programs. Our analysis shows two of the four anti-cheat solutions exhibiting rootkit-like behaviour, threatening the privacy and the integrity of the system. This paper thus provides crucial insights for researchers and developers in the field of gaming security and software engineering, highlighting the need for informed development practices that carefully consider the intersection of effective anti-cheat mechanisms and user privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00500v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3664476.3670433</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 19th International Conference on Availability, Reliability and Security (ARES 2024), 2024, 62:1-62:11</arxiv:journal_reference>
      <dc:creator>Christoph Dorner, Lukas Daniel Klausner</dc:creator>
    </item>
    <item>
      <title>Unlocking Fair Use in the Generative AI Supply Chain: A Systematized Literature Review</title>
      <link>https://arxiv.org/abs/2408.00613</link>
      <description>arXiv:2408.00613v1 Announce Type: cross 
Abstract: Through a systematization of generative AI (GenAI) stakeholder goals and expectations, this work seeks to uncover what value different stakeholders see in their contributions to the GenAI supply line. This valuation enables us to understand whether fair use advocated by GenAI companies to train model progresses the copyright law objective of promoting science and arts. While assessing the validity and efficacy of the fair use argument, we uncover research gaps and potential avenues for future works for researchers and policymakers to address.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00613v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amruta Mahuli, Asia Biega</dc:creator>
    </item>
    <item>
      <title>Future Directions in Human Mobility Science</title>
      <link>https://arxiv.org/abs/2408.00702</link>
      <description>arXiv:2408.00702v1 Announce Type: cross 
Abstract: We provide a brief review of human mobility science and present three key areas where we expect to see substantial advancements. We start from the mind and discuss the need to better understand how spatial cognition shapes mobility patterns. We then move to societies and argue the importance of better understanding new forms of transportation. We conclude by discussing how algorithms shape mobility behaviour and provide useful tools for modellers. Finally, we discuss how progress in these research directions may help us address some of the challenges our society faces today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00702v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s43588-023-00469-4</arxiv:DOI>
      <arxiv:journal_reference>Nature Computational Science 3 (2023) 588-600</arxiv:journal_reference>
      <dc:creator>Luca Pappalardo, Ed Manley, Vedran Sekara, Laura Alessandretti</dc:creator>
    </item>
    <item>
      <title>Modelling Assessment Rubrics through Bayesian Networks: a Pragmatic Approach</title>
      <link>https://arxiv.org/abs/2209.05467</link>
      <description>arXiv:2209.05467v2 Announce Type: replace 
Abstract: Automatic assessment of learner competencies is a fundamental task in intelligent tutoring systems. An assessment rubric typically and effectively describes relevant competencies and competence levels. This paper presents an approach to deriving a learner model directly from an assessment rubric defining some (partial) ordering of competence levels. The model is based on Bayesian networks and exploits logical gates with uncertainty (often referred to as noisy gates) to reduce the number of parameters of the model, so to simplify their elicitation by experts and allow real-time inference in intelligent tutoring systems. We illustrate how the approach can be applied to automatize the human assessment of an activity developed for testing computational thinking skills. The simple elicitation of the model starting from the assessment rubric opens up the possibility of quickly automating the assessment of several tasks, making them more easily exploitable in the context of adaptive assessment tools and intelligent tutoring systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.05467v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.23919/softcom55329.2022.9911432</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of 2022 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)</arxiv:journal_reference>
      <dc:creator>Francesca Mangili, Giorgia Adorni, Alberto Piatti, Claudio Bonesana, Alessandro Antonucci</dc:creator>
    </item>
    <item>
      <title>Can ChatGPT Read Who You Are?</title>
      <link>https://arxiv.org/abs/2312.16070</link>
      <description>arXiv:2312.16070v2 Announce Type: replace 
Abstract: The interplay between artificial intelligence (AI) and psychology, particularly in personality assessment, represents an important emerging area of research. Accurate personality trait estimation is crucial not only for enhancing personalization in human-computer interaction but also for a wide variety of applications ranging from mental health to education. This paper analyzes the capability of a generic chatbot, ChatGPT, to effectively infer personality traits from short texts. We report the results of a comprehensive user study featuring texts written in Czech by a representative population sample of 155 participants. Their self-assessments based on the Big Five Inventory (BFI) questionnaire serve as the ground truth. We compare the personality trait estimations made by ChatGPT against those by human raters and report ChatGPT's competitive performance in inferring personality traits from text. We also uncover a 'positivity bias' in ChatGPT's assessments across all personality dimensions and explore the impact of prompt composition on accuracy. This work contributes to the understanding of AI capabilities in psychological assessment, highlighting both the potential and limitations of using large language models for personality inference. Our research underscores the importance of responsible AI development, considering ethical implications such as privacy, consent, autonomy, and bias in AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16070v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.chbah.2024.100088</arxiv:DOI>
      <dc:creator>Erik Derner, Dalibor Ku\v{c}era, Nuria Oliver, Jan Zah\'alka</dc:creator>
    </item>
    <item>
      <title>Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development</title>
      <link>https://arxiv.org/abs/2403.15481</link>
      <description>arXiv:2403.15481v2 Announce Type: replace 
Abstract: The rise in the use of AI/ML applications across industries has sparked more discussions about the fairness of AI/ML in recent times. While prior research on the fairness of AI/ML exists, there is a lack of empirical studies focused on understanding the perspectives and experiences of AI practitioners in developing a fair AI/ML system. Understanding AI practitioners' perspectives and experiences on the fairness of AI/ML systems are important because they are directly involved in its development and deployment and their insights can offer valuable real-world perspectives on the challenges associated with ensuring fairness in AI/ML systems. We conducted semi-structured interviews with 22 AI practitioners to investigate their understanding of what a 'fair AI/ML' is, the challenges they face in developing a fair AI/ML system, the consequences of developing an unfair AI/ML system, and the strategies they employ to ensure AI/ML system fairness. We developed a framework showcasing the relationship between AI practitioners' understanding of 'fair AI/ML' system and (i) their challenges in its development, (ii) the consequences of developing an unfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness. By exploring AI practitioners' perspectives and experiences, this study provides actionable insights to enhance AI/ML fairness, which may promote fairer systems, reduce bias, and foster public trust in AI technologies. Additionally, we also identify areas for further investigation and offer recommendations to aid AI practitioners and AI companies in navigating fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15481v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aastha Pant, Rashina Hoda, Chakkrit Tantithamthavorn, Burak Turhan</dc:creator>
    </item>
    <item>
      <title>An AI-Enabled Framework Within Reach for Enhancing Healthcare Sustainability and Fairness</title>
      <link>https://arxiv.org/abs/2406.07558</link>
      <description>arXiv:2406.07558v2 Announce Type: replace 
Abstract: Good health and well-being is among key issues in the United Nations 2030 Sustainable Development Goals. The rising prevalence of large-scale infectious diseases and the accelerated aging of the global population are driving the transformation of healthcare technologies. In this context, establishing large-scale public health datasets, developing medical models, and creating decision-making systems with a human-centric approach are of strategic significance. Recently, by leveraging the extraordinary number of accessible cameras, groundbreaking advancements have emerged in AI methods for physiological signal monitoring and disease diagnosis using camera sensors. These approaches, requiring no specialized medical equipment, offer convenient manners of collecting large-scale medical data in response to public health events. Therefore, we outline a prospective framework and heuristic vision for a camera-based public health (CBPH) framework utilizing visual physiological monitoring technology. The CBPH can be considered as a convenient and universal framework for public health, advancing the United Nations Sustainable Development Goals, particularly in promoting the universality, sustainability, and equity of healthcare in low- and middle-income countries or regions. Furthermore, CBPH provides a comprehensive solution for building a large-scale and human-centric medical database, and a multi-task large medical model for public health and medical scientific discoveries. It has a significant potential to revolutionize personal monitoring technologies, digital medicine, telemedicine, and primary health care in public health. Therefore, it can be deemed that the outcomes of this paper will contribute to the establishment of a sustainable and fair framework for public health, which serves as a crucial bridge for advancing scientific discoveries in the realm of AI for medicine (AI4Medicine).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07558v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Huang, Changchen Zhao, Zimeng Liu, Shenda Hong, Baochang Zhang, Hao Lu, Zhijun Liu, Wenjin Wang, Hui Liu</dc:creator>
    </item>
    <item>
      <title>From Counting Stations to City-Wide Estimates: Data-Driven Bicycle Volume Extrapolation</title>
      <link>https://arxiv.org/abs/2406.18454</link>
      <description>arXiv:2406.18454v2 Announce Type: replace 
Abstract: Shifting to cycling in urban areas reduces greenhouse gas emissions and improves public health. Street-level bicycle volume information would aid cities in planning targeted infrastructure improvements to encourage cycling and provide civil society with evidence to advocate for cyclists' needs. Yet, the data currently available to cities and citizens often only comes from sparsely located counting stations. This paper extrapolates bicycle volume beyond these few locations to estimate bicycle volume for the entire city of Berlin. We predict daily and average annual daily street-level bicycle volumes using machine-learning techniques and various public data sources. These include app-based crowdsourced data, infrastructure, bike-sharing, motorized traffic, socioeconomic indicators, weather, and holiday data. Our analysis reveals that the best-performing model is XGBoost, and crowdsourced cycling and infrastructure data are most important for the prediction. We further simulate how collecting short-term counts at predicted locations improves performance. By providing ten days of such sample counts for each predicted location to the model, we are able to halve the error and greatly reduce the variability in performance among predicted locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18454v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silke K. Kaiser, Nadja Klein, Lynn H. Kaack</dc:creator>
    </item>
    <item>
      <title>A Nested Model for AI Design and Validation</title>
      <link>https://arxiv.org/abs/2407.16888</link>
      <description>arXiv:2407.16888v2 Announce Type: replace 
Abstract: The growing AI field faces trust, transparency, fairness, and discrimination challenges. Despite the need for new regulations, there is a mismatch between regulatory science and AI, preventing a consistent framework. A five-layer nested model for AI design and validation aims to address these issues and streamline AI application design and validation, improving fairness, trust, and AI adoption. This model aligns with regulations, addresses AI practitioner's daily challenges, and offers prescriptive guidance for determining appropriate evaluation approaches by identifying unique validity threats. We have three recommendations motivated by this model: authors should distinguish between layers when claiming contributions to clarify the specific areas in which the contribution is made and to avoid confusion, authors should explicitly state upstream assumptions to ensure that the context and limitations of their AI system are clearly understood, AI venues should promote thorough testing and validation of AI systems and their compliance with regulatory requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16888v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.isci.2024.110603</arxiv:DOI>
      <dc:creator>Akshat Dubey, Zewen Yang, Georges Hattab</dc:creator>
    </item>
    <item>
      <title>Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</title>
      <link>https://arxiv.org/abs/2212.10717</link>
      <description>arXiv:2212.10717v2 Announce Type: replace-cross 
Abstract: We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10717v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari</dc:creator>
    </item>
    <item>
      <title>The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media</title>
      <link>https://arxiv.org/abs/2312.10269</link>
      <description>arXiv:2312.10269v3 Announce Type: replace-cross 
Abstract: Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and structure of the database, (ii) the structure of the database is partially inadequate for the platforms' reporting needs, (iii) the platforms exhibited substantial differences in their moderation actions, (iv) a remarkable fraction of the database data is inconsistent, (v) the platform X (formerly Twitter) presents the most inconsistencies. Our findings have far-reaching implications for policymakers and scholars across diverse disciplines. They offer guidance for future regulations that cater to the reporting needs of online platforms in general, but also highlight opportunities to improve and refine the database itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10269v3</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amaury Trujillo, Tiziano Fagni, Stefano Cresci</dc:creator>
    </item>
    <item>
      <title>Fairness Concerns in App Reviews: A Study on AI-based Mobile Apps</title>
      <link>https://arxiv.org/abs/2401.08097</link>
      <description>arXiv:2401.08097v4 Announce Type: replace-cross 
Abstract: Fairness is one of the socio-technical concerns that must be addressed in software systems. Considering the popularity of mobile software applications (apps) among a wide range of individuals worldwide, mobile apps with unfair behaviors and outcomes can affect a significant proportion of the global population, potentially more than any other type of software system. Users express a wide range of socio-technical concerns in mobile app reviews. This research aims to investigate fairness concerns raised in mobile app reviews. Our research focuses on AI-based mobile app reviews as the chance of unfair behaviors and outcomes in AI-based mobile apps may be higher than in non-AI-based apps. To this end, we first manually constructed a ground-truth dataset, including 1,132 fairness and 1,473 non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning models that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing model can detect fairness reviews with a precision of 94%. We then applied the best-performing model on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness reviews, followed by manual analysis, led to the identification of six distinct types of fairness concerns (e.g., 'receiving different quality of features and services in different platforms and devices' and 'lack of transparency and fairness in dealing with user-generated content'). Finally, the manual analysis of 2,248 app owners' responses to the fairness reviews identified six root causes (e.g., 'copyright issues') that app owners report to justify fairness concerns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08097v4</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Rezaei Nasab, Maedeh Dashti, Mojtaba Shahin, Mansooreh Zahedi, Hourieh Khalajzadeh, Chetan Arora, Peng Liang</dc:creator>
    </item>
    <item>
      <title>The opportunities and risks of large language models in mental health</title>
      <link>https://arxiv.org/abs/2403.14814</link>
      <description>arXiv:2403.14814v3 Announce Type: replace-cross 
Abstract: Global rates of mental health concerns are rising, and there is increasing realization that existing models of mental health care will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health related tasks. In this paper, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs' application to mental health and encourage the adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. It is especially critical to ensure that mental health LLMs are fine-tuned for mental health, enhance mental health equity, and adhere to ethical standards and that people, including those with lived experience with mental health concerns, are involved in all stages from development through deployment. Prioritizing these efforts will minimize potential harms to mental health and maximize the likelihood that LLMs will positively impact mental health globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14814v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2196/59479</arxiv:DOI>
      <arxiv:journal_reference>JMIR Ment Health 2024;11:e59479</arxiv:journal_reference>
      <dc:creator>Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell</dc:creator>
    </item>
  </channel>
</rss>
