<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Feb 2026 05:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Defining causal mechanism in dual process theory and two types of feedback control</title>
      <link>https://arxiv.org/abs/2602.11478</link>
      <description>arXiv:2602.11478v1 Announce Type: new 
Abstract: Mental events are considered to supervene on physical events. A supervenient event does not change without a corresponding change in the underlying subvenient physical events. Since wholes and their parts exhibit the same supervenience-subvenience relations, inter-level causation has been expected to serve as a model for mental causation. We proposed an inter-level causation mechanism to construct a model of consciousness and an agent's self-determination. However, a significant gap exists between this mechanism and cognitive functions. Here, we demonstrate how to integrate the inter-level causation mechanism with the widely known dual-process theories. We assume that the supervenience level is composed of multiple supervenient functions (i.e., neural networks), and we argue that inter-level causation can be achieved by controlling the feedback error defined through changing algebraic expressions combining these functions. Using inter-level causation allows for a dual laws model in which each level possesses its own distinct dynamics. In this framework, the feedback error is determined independently by two processes: (1) the selection of equations combining supervenient functions, and (2) the negative feedback error reduction to satisfy the equations through adjustments of neurons and synapses. We interpret these two independent feedback controls as Type 1 and Type 2 processes in the dual process theories. As a result, theories of consciousness, agency, and dual process theory are unified into a single framework, and the characteristic features of Type 1 and Type 2 processes are naturally derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11478v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiyuki Ohmura, Yasuo Kuniyoshi</dc:creator>
    </item>
    <item>
      <title>CL API: Real-Time Closed-Loop Interactions with Biological Neural Networks</title>
      <link>https://arxiv.org/abs/2602.11632</link>
      <description>arXiv:2602.11632v1 Announce Type: new 
Abstract: Biological neural networks (BNNs) are increasingly explored for their rich dynamics, parallelism, and adaptive behavior. Beyond understanding their function as a scientific endeavour, a key focus has been using these biological systems as a novel computing substrate. However, BNNs can only function as reliable information-processing systems if inputs are delivered in a temporally and structurally consistent manner. In practice, this requires stimulation with precisely controlled structure, microsecond-scale timing, multi-channel synchronization, and the ability to observe and respond to neural activity in real-time. Existing approaches to interacting with BNNs face a fundamental trade-off: they either depend on low-level hardware mechanisms, imposing prohibitive complexity for rapid iteration, or they sacrifice temporal and structural control, undermining consistency and reproducibility - particularly in closed-loop experiments. The Cortical Labs Application Programming Interface (CL API) enables real-time, sub-millisecond closed-loop interactions with BNNs. Taking a contract-based API design approach, the CL API provides users with precise stimulation semantics, transactional admission, deterministic ordering, and explicit synchronization guarantees. This contract is presented through a declarative Python interface, enabling non-expert programmers to express complex stimulation and closed-loop behavior without managing low-level scheduling or hardware details. Ultimately, the CL API provides an accessible and reproducible foundation for real-time experimentation with BNNs, supporting both fundamental biological research and emerging neurocomputing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11632v1</guid>
      <category>q-bio.NC</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Hogan, Andrew Doherty, Boon Kien Khoo, Johnson Zhou, Richard Salib, James Stewart, Kiaran Lawson, Alon Loeffler, Brett Kagan</dc:creator>
    </item>
    <item>
      <title>TAVAE: A VAE with Adaptable Priors Explains Contextual Modulation in the Visual Cortex</title>
      <link>https://arxiv.org/abs/2602.11956</link>
      <description>arXiv:2602.11956v1 Announce Type: new 
Abstract: The brain interprets visual information through learned regularities, a computation formalized as probabilistic inference under a prior. The visual cortex establishes priors for this inference, some delivered through established top-down connections that inform low-level cortices about statistics represented at higher levels in the cortical hierarchy. While evidence shows that adaptation leads to priors reflecting the structure of natural images, it remains unclear whether similar priors can be flexibly acquired when learning a specific task. To investigate this, we built a generative model of V1 optimized for a simple discrimination task and analyzed it together with large-scale recordings from mice performing an analogous task. In line with recent approaches, we assumed that neuronal activity in V1 corresponds to latent posteriors in the generative model, enabling investigation of task-related priors in neuronal responses. To obtain a flexible test bed, we extended the VAE formalism so that a task can be acquired efficiently by reusing previously learned representations. Task-specific priors learned by this Task-Amortized VAE were used to investigate biases in mice and model when presenting stimuli that violated trained task statistics. Mismatch between learned task statistics and incoming sensory evidence produced signatures of uncertainty in stimulus category in the TAVAE posterior, reflecting properties of bimodal response profiles in V1 recordings. The task-optimized generative model accounted for key characteristics of V1 population activity, including within-day updates to population responses. Our results confirm that flexible task-specific contextual priors can be learned on demand by the visual system and deployed as early as the entry level of visual cortex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11956v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bal\'azs Mesz\'ena, Keith T. Murray, Julien Corbo, O. Batuhan Erkat, M\'arton A. Hajnal, Pierre-Olivier Polack, Gerg\H{o} Orb\'an</dc:creator>
    </item>
    <item>
      <title>UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra</title>
      <link>https://arxiv.org/abs/2602.11206</link>
      <description>arXiv:2602.11206v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{=}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11206v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>math.RA</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Marie Antonio Mi\~noza</dc:creator>
    </item>
    <item>
      <title>Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks</title>
      <link>https://arxiv.org/abs/2602.11234</link>
      <description>arXiv:2602.11234v1 Announce Type: cross 
Abstract: Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11234v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankita Paul, Wenyi Wang</dc:creator>
    </item>
    <item>
      <title>When a Man Says He Is Pregnant: Event-related Potential Evidence for a Rational Account of Speaker-contextualized Language Comprehension</title>
      <link>https://arxiv.org/abs/2409.17525</link>
      <description>arXiv:2409.17525v3 Announce Type: replace 
Abstract: Spoken language is often, if not always, understood in a context formed by the identity of the speaker. For example, we can easily make sense of an utterance such as "I'm going to have a manicure this weekend" or "The first time I got pregnant I had a hard time" when spoken by a woman, but it would be harder to understand when it is spoken by a man. Previous ERP studies have shown mixed results regarding the neurophysiological responses to such speaker-content mismatches, with some reporting an N400 effect and others a P600 effect. In an EEG experiment involving 64 participants, we used social and biological mismatches as test cases to demonstrate how these distinct ERP patterns reflect different aspects of rational inference. We showed that when the mismatch involves social stereotypes (e.g., men getting a manicure), listeners can arrive at a "literal" interpretation by integrating the content with their social knowledge, though this integration requires additional effort due to stereotype violations-resulting in an N400 effect. In contrast, when the mismatch involves biological knowledge (e.g., men getting pregnant), a "literal" interpretation becomes highly implausible or impossible, leading listeners to treat the input as potentially containing errors and engage in correction processes-resulting in a P600 effect. Supporting this rational inference framework, we found that the social N400 effect decreased as a function of the listener's personality trait of openness (as more open-minded individuals maintain more flexible social expectations), while the biological P600 effect remained robust (as biological constraints are recognized regardless of individual personalities). Our findings help to reconcile empirical inconsistencies and reveal how rational inference shapes speaker-contextualized language comprehension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17525v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1162/JOCN.a.102</arxiv:DOI>
      <arxiv:journal_reference>J Cogn Neurosci 2026; 38 (3): 545-560</arxiv:journal_reference>
      <dc:creator>Hanlin Wu, Zhenguang G. Cai</dc:creator>
    </item>
    <item>
      <title>A sub-Riemannian model of neural states in the primary motor cortex</title>
      <link>https://arxiv.org/abs/2501.03247</link>
      <description>arXiv:2501.03247v3 Announce Type: replace 
Abstract: We develop a neurogeometric model for the arm area of motor cortex, which encodes complex motor primitives, ranging from simple movement features like movement direction, to short hand trajectories, termed fragments, and ultimately to more complex patterns known as neural states (Georgopoulos, Hatsopoulos, Kadmon-Harpaz et al). Based on the sub-riemannian framework introduced in 2023, we model the space of fragments as a set of short curves defined by kinematic parameters. We then introduce a geometric kernel that serves as a model for cortical connectivity and use it in a differential equation to describe cortical activity. By applying a grouping algorithm to this cortical activity model, we successfully recover the neural states observed in Kadmon-Harpaz et al, which were based on measured cortical activity. This confirms that the choice of kinematic variables and the distance metric used here are sufficient to explain the phenomena of neural state formation. The modularity of our model reflects the brain's hierarchical structure, where initial groupings in the kinematic space $\mathcal{M}$ lead to more abstract representations. This approach mimics how the brain processes stimuli at different scales, extracting both local and global properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03247v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Caterina Mazzetti, Jawad Ali, Alessandro Sarti, Giovanna Citti</dc:creator>
    </item>
    <item>
      <title>BrainSymphony: A parameter-efficient multimodal foundation model for brain dynamics with limited data</title>
      <link>https://arxiv.org/abs/2506.18314</link>
      <description>arXiv:2506.18314v2 Announce Type: replace-cross 
Abstract: Foundation models are transforming neuroscience but are often prohibitively large, data-hungry, and difficult to deploy. Here, we introduce BrainSymphony, a lightweight and parameter-efficient foundation model with plug-and-play integration of fMRI time series and diffusion-derived structural connectivity, allowing unimodal or multimodal training and deployment without architectural changes while requiring substantially less data compared to the state-of-the-art. The model processes fMRI time series through parallel spatial and temporal transformer streams, distilled into compact embeddings by a Perceiver module, while a novel signed graph transformer encodes anatomical connectivity from diffusion MRI. These complementary representations are then combined through an adaptive fusion mechanism. Despite its compact design, BrainSymphony consistently outperforms larger models on benchmarks spanning prediction, classification, and unsupervised network discovery. Highlighting the model's generalizability and interpretability, attention maps reveal drug-induced context-dependent reorganization of cortical hierarchies in an independent psilocybin neuroimaging dataset. BrainSymphony delivers accessible, interpretable, and clinically meaningful results and demonstrates that architecturally informed, multimodal models can surpass much larger counterparts and advance applications of AI in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18314v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moein Khajehnejad, Forough Habibollahi, Devon Stoliker, Adeel Razi</dc:creator>
    </item>
  </channel>
</rss>
