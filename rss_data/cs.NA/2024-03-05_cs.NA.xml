<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Mar 2024 05:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the Optimality of CVOD-based Column Selection</title>
      <link>https://arxiv.org/abs/2403.00121</link>
      <description>arXiv:2403.00121v1 Announce Type: new 
Abstract: While there exists a rich array of matrix column subset selection problem (CSSP) algorithms for use with interpolative and CUR-type decompositions, their use can often become prohibitive as the size of the input matrix increases. In an effort to address these issues, the authors in \cite{emelianenko2024adaptive} developed a general framework that pairs a column-partitioning routine with a column-selection algorithm. Two of the four algorithms presented in that work paired the Centroidal Voronoi Orthogonal Decomposition (\textsf{CVOD}) and an adaptive variant (\textsf{adaptCVOD}) with the Discrete Empirical Interpolation Method (\textsf{DEIM}) \cite{sorensen2016deim}. In this work, we extend this framework and pair the \textsf{CVOD}-type algorithms with any CSSP algorithm that returns linearly independent columns. Our results include detailed error bounds for the solutions provided by these paired algorithms, as well as expressions that explicitly characterize how the quality of the selected column partition affects the resulting CSSP solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00121v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Emelianenko, Guy B. Oldaker IV</dc:creator>
    </item>
    <item>
      <title>Penalty-free discontinuous Galerkin method</title>
      <link>https://arxiv.org/abs/2403.00125</link>
      <description>arXiv:2403.00125v1 Announce Type: new 
Abstract: In this paper, we present a new high-order discontinuous Galerkin (DG) method, in which neither a penalty parameter nor a stabilization parameter is needed. We refer to this method as penalty-free DG (\PFDG). In this method, the trial and test functions belong to the broken Sobolev space, in which the functions are in general discontinuous on the mesh skeleton and do not meet the Dirichlet boundary conditions. However, a subset can be distinguished in this space, where the functions are continuous and satisfy the Dirichlet boundary conditions, and this subset is called admissible. The trial solution is chosen to lie in an \emph{augmented} admissible subset, in which a small violation of the continuity condition is permitted. This subset is constructed by applying special augmented constraints to the linear combination of finite element basis functions. In this approach, all the advantages of the DG method are retained without the necessity of using stability parameters or numerical fluxes. Several benchmark problems in two dimensions (Poisson equation, linear elasticity, hyperelasticity, and biharmonic equation) on polygonal (triangles, quadrilateral and weakly convex polygons) meshes as well as a three-dimensional Poisson problem on hexahedral meshes are considered. Numerical results are presented that affirm the sound accuracy and optimal convergence of the method in the $L^2$ norm and the energy seminorm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00125v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Ja\'skowiec, N. Sukumar</dc:creator>
    </item>
    <item>
      <title>A Gradually Reinforced Sample-Average-Approximation Differentiable Homotopy Method for a System of Stochastic Equations</title>
      <link>https://arxiv.org/abs/2403.00294</link>
      <description>arXiv:2403.00294v1 Announce Type: new 
Abstract: This paper intends to apply the sample-average-approximation (SAA) scheme to solve a system of stochastic equations (SSE), which has many applications in a variety of fields. The SAA is an effective paradigm to address risks and uncertainty in stochastic models from the perspective of Monte Carlo principle. Nonetheless, a numerical conflict arises from the sample size of SAA when one has to make a tradeoff between the accuracy of solutions and the computational cost. To alleviate this issue, we incorporate a gradually reinforced SAA scheme into a differentiable homotopy method and develop a gradually reinforced sample-average-approximation (GRSAA) differentiable homotopy method in this paper. By introducing a series of continuously differentiable functions of the homotopy parameter $t$ ranging between zero and one, we establish a differentiable homotopy system, which is able to gradually increase the sample size of SAA as $t$ descends from one to zero. The set of solutions to the homotopy system contains an everywhere smooth path, which starts from an arbitrary point and ends at a solution to the SAA with any desired accuracy. The GRSAA differentiable homotopy method serves as a bridge to link the gradually reinforced SAA scheme and a differentiable homotopy method and retains the nice property of global convergence the homotopy method possesses while greatly reducing the computational cost for attaining a desired solution to the original SSE. Several numerical experiments further confirm the effectiveness and efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00294v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peixuan Li, Chuangyin Dang, Yang Zhan</dc:creator>
    </item>
    <item>
      <title>Enhancing Biomechanical Simulations Based on A Posteriori Error Estimates: The Potential of Dual Weighted Residual-Driven Adaptive Mesh Refinement</title>
      <link>https://arxiv.org/abs/2403.00401</link>
      <description>arXiv:2403.00401v1 Announce Type: new 
Abstract: The Finite Element Method (FEM) is a well-established procedure for computing approximate solutions to deterministic engineering problems described by partial differential equations. FEM produces discrete approximations of the solution with a discretisation error that can be an be quantified with \emph{a posteriori} error estimates. The practical relevance of error estimates for biomechanics problems, especially for soft tissue where the response is governed by large strains, is rarely addressed. In this contribution, we propose an implementation of \emph{a posteriori} error estimates targeting a user-defined quantity of interest, using the Dual Weighted Residual (DWR) technique tailored to biomechanics. The proposed method considers a general setting that encompasses three-dimensional geometries and model non-linearities, which appear in hyperelastic soft tissues. We take advantage of the automatic differentiation capabilities embedded in modern finite element software, which allows the error estimates to be computed generically for a large class of models and constitutive laws. First we validate our methodology using experimental measurements from silicone samples, and then illustrate its applicability for patient-specific computations of pressure ulcers on a human heel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00401v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huu Phuoc Bui, Michel Duprez, Pierre-Yves Rohan, Arnaud Lejeune, Stephane P. A. Bordas, Marek Bucki, Franz Chouly</dc:creator>
    </item>
    <item>
      <title>Implicit high-order gas-kinetic schemes for compressible flows on three-dimensional unstructured meshes II: unsteady flows</title>
      <link>https://arxiv.org/abs/2403.00482</link>
      <description>arXiv:2403.00482v1 Announce Type: new 
Abstract: For the simulations of unsteady flow, the global time step becomes really small with a large variation of local cell size. In this paper, an implicit high-order gas-kinetic scheme (HGKS) is developed to remove the restrictions on the time step for unsteady simulations. In order to improve the efficiency and keep the high-order accuracy, a two-stage third-order implicit time-accurate discretization is proposed. In each stage, an artificial steady solution is obtained for the implicit system with the pseudo-time iteration. In the iteration, the classical implicit methods are adopted to solve the nonlinear system, including the lower-upper symmetric Gauss-Seidel (LUSGS) and generalized minimum residual (GMRES) methods. To achieve the spatial accuracy, the HGKSs with both non-compact and compact reconstructions are constructed. For the non-compact scheme, the weighted essentially non-oscillatory (WENO) reconstruction is used. For the compact one, the Hermite WENO (HWENO) reconstruction is adopted due to the updates of both cell-averaged flow variables and their derivatives. The expected third-order temporal accuracy is achieved with the two-stage temporal discretization. For the smooth flow, only a single artificial iteration is needed. For uniform meshes, the efficiency of the current implicit method improves significantly in comparison with the explicit one. For the flow with discontinuities, compared with the well-known Crank-Nicholson method, the spurious oscillations in the current schemes are well suppressed. The increase of the artificial iteration steps introduces extra reconstructions associating with a reduction of the computational efficiency. Overall, the current implicit method leads to an improvement in efficiency over the explicit one in the cases with a large variation of mesh size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00482v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaqing Yang, Liang Pan, Kun Xu</dc:creator>
    </item>
    <item>
      <title>Computational homogenization for aerogel-like polydisperse open-porous materials using neural network--based surrogate models on the microscale</title>
      <link>https://arxiv.org/abs/2403.00571</link>
      <description>arXiv:2403.00571v1 Announce Type: new 
Abstract: The morphology of nanostructured materials exhibiting a polydisperse porous space, such as aerogels, is very open porous and fine grained. Therefore, a simulation of the deformation of a large aerogel structure resolving the nanostructure would be extremely expensive. Thus, multi-scale or homogenization approaches have to be considered. Here, a computational scale bridging approach based on the FE$^2$ method is suggested, where the macroscopic scale is discretized using finite elements while the microstructure of the open-porous material is resolved as a network of Euler-Bernoulli beams. Here, the beam frame based RVEs (representative volume elements) have pores whose size distribution follows the measured values for a specific material. This is a well-known approach to model aerogel structures. For the computational homogenization, an approach to average the first Piola-Kirchhoff stresses in a beam frame by neglecting rotational moments is suggested. To further overcome the computationally most expensive part in the homogenization method, that is, solving the RVEs and averaging their stress fields, a surrogate model is introduced based on neural networks. The networks input is the localized deformation gradient on the macroscopic scale and its output is the averaged stress for the specific material. It is trained on data generated by the beam frame based approach. The effiency and robustness of both homogenization approaches is shown numerically, the approximation properties of the surrogate model is verified for different macroscopic problems and discretizations. Different (Quasi-)Newton solvers are considered on the macroscopic scale and compared with respect to their convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00571v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel Klawonn, Martin Lanser, Lucas Mager, Ameya Rege</dc:creator>
    </item>
    <item>
      <title>Discrete minimizers of the interaction energy in collective behavior: a brief numerical and analytic review</title>
      <link>https://arxiv.org/abs/2403.00594</link>
      <description>arXiv:2403.00594v1 Announce Type: new 
Abstract: We consider minimizers of the N-particle interaction potential energy and briefly review numerical methods used to calculate them. We consider simple pair potentials which are attractive at short distances and repulsive at long distances, focusing on examples which are sums of two powers. The range of powers we look at includes the well-known case of the Lennard-Jones potential, but we are also interested in less singular potentials which are relevant in collective behavior models. We report on results using the software GMIN developed by Wales and collaborators for problems in chemistry. For all cases, this algorithm gives good candidates for the minimizers for relatively low values of the particle number N. This is well-known for potentials similar to Lennard-Jones, but not for the range which is of interest in collective behavior. Standard minimization procedures have been used in the literature in this range, but they are likely to yield stationary states which are not minimizers. We illustrate numerically some properties of the minimizers in 2D, such as lattice structure, Wulff shapes, and the continuous large-N limit for locally integrable (that is, less singular) potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00594v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. Ca\~nizo, Alejandro Ramos-Lora</dc:creator>
    </item>
    <item>
      <title>Analysis of the particle relaxation method for generating uniform particle distributions in smoothed particle hydrodynamics</title>
      <link>https://arxiv.org/abs/2403.00623</link>
      <description>arXiv:2403.00623v1 Announce Type: new 
Abstract: We establish a theoretical framework of the particle relaxation method for uniform particle generation of Smoothed Particle Hydrodynamics. We achieve this by reformulating the particle relaxation as an optimization problem. The objective function is an integral difference between discrete particle-based and smoothed-analytical volume fractions. The analysis demonstrates that the particle relaxation method in the domain interior is essentially equivalent to employing a gradient descent approach to solve this optimization problem, and we can extend such an equivalence to the bounded domain by introducing a proper boundary term. Additionally, each periodic particle distribution has a spatially uniform particle volume, denoted as characteristic volume. The relaxed particle distribution has the largest characteristic volume, and the kernel cut-off radius determines this volume. This insight enables us to control the relaxed particle distribution by selecting the target kernel cut-off radius for a given kernel function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00623v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Fan, Xiaoliang Li, Shuoguo Zhang, Xiangyu Hu, Nikolaus A. Adams</dc:creator>
    </item>
    <item>
      <title>An iterative method for the solution of Laplace-like equations in high and very high space dimensions</title>
      <link>https://arxiv.org/abs/2403.00682</link>
      <description>arXiv:2403.00682v1 Announce Type: new 
Abstract: This paper deals with the equation $-\Delta u+\mu u=f$ on high-dimensional spaces $\mathbb{R}^m$, where the right-hand side $f(x)=F(Tx)$ is composed of a separable function $F$ with an integrable Fourier transform on a space of a dimension $n&gt;m$ and a linear mapping given by a matrix $T$ of full rank and $\mu\geq 0$ is a constant. For example, the right-hand side can explicitly depend on differences $x_i-x_j$ of components of $x$. Following our publication [Numer. Math. (2020) 146:219--238], we show that the solution of this equation can be expanded into sums of functions of the same structure and develop in this framework an equally simple and fast iterative method for its computation. The method is based on the observation that in almost all cases and for large problem classes the expression $\|T^ty\|^2$ deviates on the unit sphere $\|y\|=1$ the less from its mean value the higher the dimension $m$ is, a concentration of measure effect. The higher the dimension $m$, the faster the iteration converges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00682v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Yserentant</dc:creator>
    </item>
    <item>
      <title>Mathematical models of drug delivery via a contact lens during wear</title>
      <link>https://arxiv.org/abs/2403.00008</link>
      <description>arXiv:2403.00008v1 Announce Type: cross 
Abstract: In this work we develop and investigate mathematical and computational models that describe drug delivery from a contact lens during wear. Our models are designed to predict the dynamics of drug release from the contact lens and subsequent transport into the adjacent pre-lens tear film and post-lens tear film as well as into the ocular tissue (e.g. cornea), into the eyelid, and out of these regions. These processes are modeled by one dimensional diffusion out of the lens coupled to compartment-type models for drug concentrations in the various accompanying regions. In addition to numerical solutions that are compared with experimental data on drug release in an in vitro eye model, we also identify a large diffusion limit model for which analytical solutions can be written down for all quantities of interest, such as cumulative release of the drug from the contact lens. We use our models to make assessments about possible mechanisms and drug transport pathways through the pre-lens and post-lens tear films and provide interpretation of experimental observations. We discuss successes and limitations of our models as well as their potential to guide further research to help understand the dynamics of ophthalmic drug delivery via drug-eluting contact lenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00008v1</guid>
      <category>physics.bio-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel M. Anderson, Rayanne A. Luke</dc:creator>
    </item>
    <item>
      <title>Inferring solar differential rotation and viscosity via passive imaging with inertial waves</title>
      <link>https://arxiv.org/abs/2403.00488</link>
      <description>arXiv:2403.00488v1 Announce Type: cross 
Abstract: The recent discovery of inertial waves on the surface of the Sun offers new possibilities to learn about the solar interior. These waves are long-lived with a period on the order of the Sun rotation period ($\sim$27 days) and are sensitive to parameters deep inside the Sun. They are excited by turbulent convection, leading to a passive imaging problem. In this work, we present the forward and inverse problem of reconstructing viscosity and differential rotation on the Sun from cross-covariance observations of these inertial waves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00488v1</guid>
      <category>astro-ph.SR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tram Thi Ngoc Nguyen, Thorsten Hohage, Damien Fournier, Laurent Gizon</dc:creator>
    </item>
    <item>
      <title>On the complexity of strong approximation of stochastic differential equations with a non-Lipschitz drift coefficient</title>
      <link>https://arxiv.org/abs/2403.00637</link>
      <description>arXiv:2403.00637v1 Announce Type: cross 
Abstract: We survey recent developments in the field of complexity of pathwise approximation in $p$-th mean of the solution of a stochastic differential equation at the final time based on finitely many evaluations of the driving Brownian motion. First, we briefly review the case of equations with globally Lipschitz continuous coefficients, for which an error rate of at least $1/2$ in terms of the number of evaluations of the driving Brownian motion is always guaranteed by using the equidistant Euler-Maruyama scheme. Then we illustrate that giving up the global Lipschitz continuity of the coefficients may lead to a non-polynomial decay of the error for the Euler-Maruyama scheme or even to an arbitrary slow decay of the smallest possible error that can be achieved on the basis of finitely many evaluations of the driving Brownian motion. Finally, we turn to recent positive results for equations with a drift coefficient that is not globally Lipschitz continuous. Here we focus on scalar equations with a Lipschitz continuous diffusion coefficient and a drift coefficient that satisfies piecewise smoothness assumptions or has fractional Sobolev regularity and we present corresponding complexity results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00637v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. M\"uller-Gronbach, L. Yaroslavtseva</dc:creator>
    </item>
    <item>
      <title>Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number</title>
      <link>https://arxiv.org/abs/2403.00643</link>
      <description>arXiv:2403.00643v1 Announce Type: cross 
Abstract: We study symmetric tensor decompositions, i.e., decompositions of the form $T = \sum_{i=1}^r u_i^{\otimes 3}$ where $T$ is a symmetric tensor of order 3 and $u_i \in \mathbb{C}^n$.In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from $u_i$. In this paper we assume that the $u_i$ are linearly independent. This implies $r \leq n$,that is, the decomposition of T is undercomplete.
  We give a randomized algorithm for the following problem in the exact arithmetic model of computation: Let $T$ be an order-3 symmetric tensor that has an undercomplete decomposition.Then given some $T'$ close to $T$, an accuracy parameter $\varepsilon$, and an upper bound B on the condition number of the tensor, output vectors $u'_i$ such that $||u_i - u'_i|| \leq \varepsilon$ (up to permutation and multiplication by cube roots of unity) with high probability. The main novel features of our algorithm are:
  1) We provide the first algorithm for this problem that runs in linear time in the size of the input tensor. More specifically, it requires $O(n^3)$ arithmetic operations for all accuracy parameters $\varepsilon =$ 1/poly(n) and B = poly(n).
  2) Our algorithm is robust, that is, it can handle inverse-quasi-polynomial noise (in $n$,B,$\frac{1}{\varepsilon}$) in the input tensor.
  3) We present a smoothed analysis of the condition number of the tensor decomposition problem. This guarantees that the condition number is low with high probability and further shows that our algorithm runs in linear time, except for some rare badly conditioned inputs.
  Our main algorithm is a reduction to the complete case ($r=n$) treated in our previous work [Koiran,Saha,CIAC 2023]. For efficiency reasons we cannot use this algorithm as a blackbox. Instead, we show that it can be run on an implicitly represented tensor obtained from the input tensor by a change of basis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00643v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal Koiran, Subhayan Saha</dc:creator>
    </item>
    <item>
      <title>Subhomogeneous Deep Equilibrium Models</title>
      <link>https://arxiv.org/abs/2403.00720</link>
      <description>arXiv:2403.00720v1 Announce Type: cross 
Abstract: Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feed-forward, convolutional, and graph neural network examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00720v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietro Sittoni, Francesco Tudisco</dc:creator>
    </item>
    <item>
      <title>Neural Acceleration of Incomplete Cholesky Preconditioners</title>
      <link>https://arxiv.org/abs/2403.00743</link>
      <description>arXiv:2403.00743v1 Announce Type: cross 
Abstract: The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the Preconditioned Conjugate Gradient method (PCG), are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends on the preconditioner utilized. The development of the preconditioner normally requires some insight into the sparse linear system and the desired trade-off of generating the preconditioner and the reduction in the number of iterations. Incomplete factorization methods tend to be black box methods to generate these preconditioners but may fail for a number of reasons. These reasons include numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult to parallelize algorithm. With a move towards heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete sparse Cholesky factorization that can be used as a preconditioner. This generated preconditioner is as good or better in terms of reduction of iterations than the one found using multiple preconditioning techniques such as scaling and shifting. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00743v1</guid>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Dennis Booth, Hongyang Sun, Trevor Garnett</dc:creator>
    </item>
    <item>
      <title>On inclusion of time-varying source in the acoustic wave equation</title>
      <link>https://arxiv.org/abs/2212.04466</link>
      <description>arXiv:2212.04466v5 Announce Type: replace 
Abstract: Acoustic wave equation is a partial differential equation (PDE) which describes propagation of acoustic waves through a material. In general, the solution to this PDE is nonunique. Therefore, it is necessary to impose initial conditions in the form of Cauchy conditions for obtaining a unique solution. Theoretically, solving the wave equation is equivalent to representing the wavefield in terms of a radiation source which possesses finite energy over space and time.The radiation source is represented by a forcing term in the right-hand-side of the wave equation. In practice, the source may be represented in terms of normal derivative of pressure or normal velocity over a surface. The pressure wavefield is then calculated by solving an associated boundary-value problem via imposing conditions on the boundary of a chosen solution space. From analytic point of view, this manuscript aims to review typical approaches for obtaining unique solution to the acoustic wave equation in terms of either a volumetric radiation source, or a surface source in terms of normal derivative of pressure or normal velocity. A numerical approximation of the derived formulae will then be explained. The key step for numerically approximating the derived analytic formulae is inclusion of source, and will be studied carefully in this manuscript.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04466v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashkan Javaherian</dc:creator>
    </item>
    <item>
      <title>Summation-by-parts operators for general function spaces: The second derivative</title>
      <link>https://arxiv.org/abs/2306.16314</link>
      <description>arXiv:2306.16314v2 Announce Type: replace 
Abstract: Many applications rely on solving time-dependent partial differential equations (PDEs) that include second derivatives. Summation-by-parts (SBP) operators are crucial for developing stable, high-order accurate numerical methodologies for such problems. Conventionally, SBP operators are tailored to the assumption that polynomials accurately approximate the solution, and SBP operators should thus be exact for them. However, this assumption falls short for a range of problems for which other approximation spaces are better suited. We recently addressed this issue and developed a theory for first-derivative SBP operators based on general function spaces, coined function-space SBP (FSBP) operators. In this paper, we extend the innovation of FSBP operators to accommodate second derivatives. The developed second-derivative FSBP operators maintain the desired mimetic properties of existing polynomial SBP operators while allowing for greater flexibility by being applicable to a broader range of function spaces. We establish the existence of these operators and detail a straightforward methodology for constructing them. By exploring various function spaces, including trigonometric, exponential, and radial basis functions, we illustrate the versatility of our approach. The work presented here opens up possibilities for using second-derivative SBP operators based on suitable function spaces, paving the way for a wide range of applications in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16314v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcp.2024.112889</arxiv:DOI>
      <dc:creator>Jan Glaubitz, Simon-Christian Klein, Jan Nordstr\"om, Philipp \"Offner</dc:creator>
    </item>
    <item>
      <title>Fine error bounds for approximate asymmetric saddle point problems</title>
      <link>https://arxiv.org/abs/2307.03742</link>
      <description>arXiv:2307.03742v2 Announce Type: replace 
Abstract: The theory of mixed finite element methods for solving different types of elliptic partial differential equations in saddle point formulation is well established since many decades. This topic was mostly studied for variational formulations defined upon the same product spaces of both shape- and test-pairs of primal variable-multiplier. Whenever either these spaces or the two bilinear forms involving the multiplier are distinct, the saddle point problem is asymmetric. The three inf-sup conditions to be satisfied by the product spaces stipulated in work on the subject, in order to guarantee well-posedness, are well known. However, the material encountered in the literature addressing the approximation of this class of problems left room for improvement and clarifications. After making a brief review of the existing contributions to the topic that justifies such an assertion, in this paper we set up finer global error bounds for the pair primal variable-multiplier solving an asymmetric saddle point problem. Besides well-posedness, the three constants in the aforementioned inf-sup conditions are identified as all that is needed for determining the stability constant appearing therein, whose expression is exhibited. As a complement, refined error bounds depending only on these three constants are given for both unknowns separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03742v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vitoriano Ruas</dc:creator>
    </item>
    <item>
      <title>Preconditioning techniques for generalized Sylvester matrix equations</title>
      <link>https://arxiv.org/abs/2307.07884</link>
      <description>arXiv:2307.07884v2 Announce Type: replace 
Abstract: Sylvester matrix equations are ubiquitous in scientific computing. However, few solution techniques exist for their generalized multiterm version, as they now arise in an increasingly large number of applications. In this work, we consider algebraic parameter-free preconditioning techniques for the iterative solution of generalized multiterm Sylvester equations. They consist in constructing low Kronecker rank approximations of either the operator itself or its inverse. While the former requires solving standard Sylvester equations in each iteration, the latter only requires matrix-matrix multiplications, which are highly optimized on modern computer architectures. Moreover, low Kronecker rank approximate inverses can be easily combined with sparse approximate inverse techniques, thereby enhancing their performance with little or no damage to their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07884v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannis Voet</dc:creator>
    </item>
    <item>
      <title>Numerical study of the Serre-Green-Naghdi equations in 2D</title>
      <link>https://arxiv.org/abs/2306.09731</link>
      <description>arXiv:2306.09731v2 Announce Type: replace-cross 
Abstract: A detailed numerical study of solutions to the Serre-Green-Naghdi (SGN) equations in 2D with vanishing curl of the velocity field is presented. The transverse stability of line solitary waves, 1D solitary waves being exact solutions of the 2D equations independent of the second variable, is established numerically. The study of localized initial data as well as crossing 1D solitary waves does not give an indication of existence of stable structures in SGN solutions localized in two spatial dimensions. For the numerical experiments, an approach based on a Fourier spectral method with a Krylov subspace technique is applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09731v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Gavrilyuk, C. Klein</dc:creator>
    </item>
  </channel>
</rss>
