<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 06:34:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Time-Spectral Efficiency</title>
      <link>https://arxiv.org/abs/2406.01740</link>
      <description>arXiv:2406.01740v1 Announce Type: new 
Abstract: This study concerns the efficiency of time-spectral methods for numerical solution of differential equations. It is found that the time-spectral method GWRM demonstrates insensitivity to stiffness and chaoticity due to the implicit nature of the solution algorithm. Accuracy is thus determined primarily by numerical resolution of the solution shape. Examples of efficient solution of stiff and chaotic problems, where explicit methods fail or are significantly slower, are given. Non-smooth and partially steep solutions, however, remain challenging for convergence and accuracy. Some, earlier suggested, smoothing algorithms are shown to be ineffective in addressing this issue. Our findings underscore the need for further exploration of time-spectral approaches to enhance convergence and accuracy for steep or non-smooth solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01740v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Scheffel</dc:creator>
    </item>
    <item>
      <title>Method for Verifying Solutions of Sparse Linear Systems with General Coefficients</title>
      <link>https://arxiv.org/abs/2406.02033</link>
      <description>arXiv:2406.02033v1 Announce Type: new 
Abstract: This paper proposes a verification method for sparse linear systems $Ax=b$ with general and nonsingular coefficients. A verification method produces the error bound for a given approximate solution. Conventional methods use one of two approaches. One approach is to verify the computed solution of the normal equation $A^TAx=A^Tb$ by exploiting symmetric and positive definiteness; however, the condition number of $A^TA$ is the square of that for $A$. The other approach uses an approximate inverse matrix of the coefficient; however, the approximate inverse may be dense even if $A$ is sparse. Here, we propose a method for the verification of solutions of sparse linear systems based on $LDL^T$ decomposition. The proposed method can reduce the fill-in and is applicable to many problems. Moreover, an efficient iterative refinement method is proposed for obtaining accurate solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02033v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takeshi Terao, Katsuhisa Ozaki</dc:creator>
    </item>
    <item>
      <title>A novel model reduction method to solve inverse problems of parabolic type</title>
      <link>https://arxiv.org/abs/2406.02119</link>
      <description>arXiv:2406.02119v1 Announce Type: new 
Abstract: In this paper, we propose novel proper orthogonal decomposition (POD)--based model reduction methods that effectively address the issue of inverse crime in solving parabolic inverse problems. Both the inverse initial value problems and inverse source problems are studied. By leveraging the inherent low-dimensional structures present in the data, our approach enables a reduction in the forward model complexity without compromising the accuracy of the inverse problem solution. Besides, we prove the convergence analysis of the proposed methods for solving parabolic inverse problems. Through extensive experimentation and comparative analysis, we demonstrate the effectiveness of our method in overcoming inverse crime and achieving improved inverse problem solutions. The proposed POD model reduction method offers a promising direction for improving the reliability and applicability of inverse problem-solving techniques in various domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02119v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Zhang, Zhiwen Zhang</dc:creator>
    </item>
    <item>
      <title>Learning the Hodgkin-Huxley Model with Operator Learning Techniques</title>
      <link>https://arxiv.org/abs/2406.02173</link>
      <description>arXiv:2406.02173v1 Announce Type: new 
Abstract: We construct and compare three operator learning architectures, DeepONet, Fourier Neural Operator, and Wavelet Neural Operator, in order to learn the operator mapping a time-dependent applied current to the transmembrane potential of the Hodgkin- Huxley ionic model. The underlying non-linearity of the Hodgkin-Huxley dynamical system, the stiffness of its solutions, and the threshold dynamics depending on the intensity of the applied current, are some of the challenges to address when exploiting artificial neural networks to learn this class of complex operators. By properly designing these operator learning techniques, we demonstrate their ability to effectively address these challenges, achieving a relative L2 error as low as 1.4% in learning the solutions of the Hodgkin-Huxley ionic model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02173v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Centofanti, Massimiliano Ghiotto, Luca F. Pavarino</dc:creator>
    </item>
    <item>
      <title>Numerical scheme for the solution of the "bad" Boussinesq equation</title>
      <link>https://arxiv.org/abs/2406.02183</link>
      <description>arXiv:2406.02183v1 Announce Type: new 
Abstract: We present a numerical scheme for the solution of the initial-value problem for the ``bad'' Boussinesq equation. The accuracy of the scheme is tested by comparison with exact soliton solutions as well as with recently obtained asymptotic formulas for the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02183v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe Charlier, Daniel Eriksson, Jonatan Lenells</dc:creator>
    </item>
    <item>
      <title>Minimum-norm solutions of the non-symmetric semidefinite Procrustes problem</title>
      <link>https://arxiv.org/abs/2406.02203</link>
      <description>arXiv:2406.02203v1 Announce Type: new 
Abstract: Given two matrices $X,B\in \mathbb{R}^{n\times m}$ and a set $\mathcal{A}\subseteq \mathbb{R}^{n\times n}$, a Procrustes problem consists in finding a matrix $A \in \mathcal{A}$ such that the Frobenius norm of $AX-B$ is minimized. When $\mathcal{A}$ is the set of the matrices whose symmetric part is positive semidefinite, we obtain the so-called non-symmetric positive semidefinite Procrustes (NSPDSP) problem. The NSPDSP problem arises in the estimation of compliance or stiffness matrix in solid and elastic structures. If $X$ has rank $r$, Baghel et al. (Lin. Alg. Appl., 2022) proposed a three-step semi-analytical approach: (1) construct a reduced NSPDSP problem in dimension $r\times r$, (2) solve the reduced problem by means of a fast gradient method with a linear rate of convergence, and (3) post-process the solution of the reduced problem to construct a solution of the larger original NSPDSP problem. In this paper, we revisit this approach of Baghel et al. and identify an unnecessary assumption used by the authors leading to cases where their algorithm cannot attain a minimum and produces solutions with unbounded norm. In fact, revising the post-processing phase of their semi-analytical approach, we show that the infimum of the NSPDSP problem is always attained, and we show how to compute a minimum-norm solution. We also prove that the symmetric part of the computed solution has minimum rank bounded by $r$, and that the skew-symmetric part has rank bounded by $2r$. Several numerical examples show the efficiency of this algorithm, both in terms of computational speed and of finding optimal minimum-norm solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02203v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Gillis, Stefano Sicilia</dc:creator>
    </item>
    <item>
      <title>Automatic nonstationary anisotropic Tikhonov regularization through bilevel optimization</title>
      <link>https://arxiv.org/abs/2406.02209</link>
      <description>arXiv:2406.02209v1 Announce Type: new 
Abstract: Regularization techniques are necessary to compute meaningful solutions to discrete ill-posed inverse problems. The well-known 2-norm Tikhonov regularization method equipped with a discretization of the gradient operator as regularization operator penalizes large gradient components of the solution to overcome instabilities. However, this method is homogeneous, i.e., it does not take into account the orientation of the regularized solution and therefore tends to smooth the desired structures, textures and discontinuities, which often contain important information. If the local orientation field of the solution is known, a possible way to overcome this issue is to implement local anisotropic regularization by penalizing weighted directional derivatives. In this paper, considering problems that are inherently two-dimensional, we propose to automatically and simultaneously recover the regularized solution and the local orientation parameters (used to define the anisotropic regularization term) by solving a bilevel optimization problem. Specifically, the lower level problem is Tikhonov regularization equipped with local anisotropic regularization, while the objective function of the upper level problem encodes some natural assumptions about the local orientation parameters and the Tikhonov regularization parameter. Application of the proposed algorithm to a variety of inverse problems in imaging (such as denoising, deblurring, tomography and Dix inversion), with both real and synthetic data, shows its effectiveness and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02209v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silvia Gazzola, Ali Gholami</dc:creator>
    </item>
    <item>
      <title>Deep Block Proximal Linearised Minimisation Algorithm for Non-convex Inverse Problems</title>
      <link>https://arxiv.org/abs/2406.02458</link>
      <description>arXiv:2406.02458v1 Announce Type: new 
Abstract: Image restoration is typically addressed through non-convex inverse problems, which are often solved using first-order block-wise splitting methods. In this paper, we consider a general type of non-convex optimisation model that captures many inverse image problems and present an inertial block proximal linearised minimisation (iBPLM) algorithm. Our new method unifies the Jacobi-type parallel and the Gauss-Seidel-type alternating update rules, and extends beyond these approaches. The inertial technique is also incorporated into each block-wise subproblem update, which can accelerate numerical convergence. Furthermore, we extend this framework with a plug-and-play variant (PnP-iBPLM) that integrates deep gradient denoisers, offering a flexible and robust solution for complex imaging tasks. We provide comprehensive theoretical analysis, demonstrating both subsequential and global convergence of the proposed algorithms. To validate our methods, we apply them to multi-block dictionary learning problems in image denoising and deblurring. Experimental results show that both iBPLM and PnP-iBPLM significantly enhance numerical performance and robustness in these applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02458v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyan Huang, Zhongming Wu, Yanqi Cheng, Tieyong Zeng, Carola-Bibiane Sch\"onlieb, Angelica I. Aviles-Rivero</dc:creator>
    </item>
    <item>
      <title>Tensor Network Space-Time Spectral Collocation Method for Solving the Nonlinear Convection Diffusion Equation</title>
      <link>https://arxiv.org/abs/2406.02505</link>
      <description>arXiv:2406.02505v1 Announce Type: new 
Abstract: Spectral methods provide highly accurate numerical solutions for partial differential equations, exhibiting exponential convergence with the number of spectral nodes. Traditionally, in addressing time-dependent nonlinear problems, attention has been on low-order finite difference schemes for time discretization and spectral element schemes for spatial variables. However, our recent developments have resulted in the application of spectral methods to both space and time variables, preserving spectral convergence in both domains. Leveraging Tensor Train techniques, our approach tackles the curse of dimensionality inherent in space-time methods. Here, we extend this methodology to the nonlinear time-dependent convection-diffusion equation. Our discretization scheme exhibits a low-rank structure, facilitating translation to tensor-train (TT) format. Nevertheless, controlling the TT-rank across Newton's iterations, needed to deal with the nonlinearity, poses a challenge, leading us to devise the "Step Truncation TT-Newton" method. We demonstrate the exponential convergence of our methods through various benchmark examples. Importantly, our scheme offers significantly reduced memory requirement compared to the full-grid scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02505v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dibyendu Adak, M. Engin Danis, Duc P. Truong, Kim {\O}. Rasmussen, Boian S. Alexandrov</dc:creator>
    </item>
    <item>
      <title>Input-output reduced order modeling for public health intervention evaluation</title>
      <link>https://arxiv.org/abs/2406.01657</link>
      <description>arXiv:2406.01657v1 Announce Type: cross 
Abstract: In recent years, mathematical models have become an indispensable tool in the planning, evaluation, and implementation of public health interventions. Models must often provide detailed information for many levels of population stratification. Such detail comes at a price: in addition to the computational costs, the number of considered input parameters can be large, making effective study design difficult. To address these difficulties, we propose a novel technique to reduce the dimension of the model input space to simplify model-informed intervention planning. The method works by first applying a dimension reduction technique on the model output space. We then develop a method which allows us to map each reduced output to a corresponding vector in the input space, thereby reducing its dimension. We apply the method to the HIV Optimization and Prevention Economics (HOPE) model, to validate the approach and establish proof of concept.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01657v1</guid>
      <category>math.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Viguerie, Chiara Piazzola, Md Hafizul Islam, Evin Uzun Jacobson</dc:creator>
    </item>
    <item>
      <title>Neural Green's Operators for Parametric Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2406.01857</link>
      <description>arXiv:2406.01857v1 Announce Type: cross 
Abstract: This work introduces neural Green's operators (NGOs), a novel neural operator network architecture that learns the solution operator for a parametric family of linear partial differential equations (PDEs). Our construction of NGOs is derived directly from the Green's formulation of such a solution operator. Similar to deep operator networks (DeepONets) and variationally mimetic operator networks (VarMiONs), NGOs constitutes an expansion of the solution to the PDE in terms of basis functions, that is returned from a sub-network, contracted with coefficients, that are returned from another sub-network. However, in accordance with the Green's formulation, NGOs accept weighted averages of the input functions, rather than sampled values thereof, as is the case in DeepONets and VarMiONs. Application of NGOs to canonical linear parametric PDEs shows that, while they remain competitive with DeepONets, VarMiONs and Fourier neural operators when testing on data that lie within the training distribution, they robustly generalize when testing on finer-scale data generated outside of the training distribution. Furthermore, we show that the explicit representation of the Green's function that is returned by NGOs enables the construction of effective preconditioners for numerical solvers for PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01857v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Melchers, Joost Prins, Michael Abdelmalik</dc:creator>
    </item>
    <item>
      <title>A Novel Paradigm Shift for Next-Generation: Symbiotic Backscatter Rate-Splitting Multiple Access Systems</title>
      <link>https://arxiv.org/abs/2406.01921</link>
      <description>arXiv:2406.01921v1 Announce Type: cross 
Abstract: Next-generation wireless networks are projected to empower a broad range of Internet-of-things (IoT) applications and services with extreme data rates, posing new challenges in delivering large-scale connectivity at a low cost to current communication paradigms. Rate-splitting multiple access (RSMA) is one of the most spotlight nominees, conceived to address spectrum scarcity while reaching massive connectivity. Meanwhile, symbiotic communication is said to be an inexpensive way to realize future IoT on a large scale. To reach the goal of spectrum efficiency improvement and low energy consumption, we merge these advances by means of introducing a novel paradigm shift, called symbiotic backscatter RSMA, for the next generation. Specifically, we first establish the way to operate the symbiotic system to assist the readers in apprehending the proposed paradigm, then guide detailed design in beamforming weights with four potential gain-control (GC) strategies for enhancing symbiotic communication, and finally provide an information-theoretic framework using a new metric, called symbiotic outage probability (SOP) to characterize the proposed system performance. Through numerical result experiments, we show that the developed framework can accurately predict the actual SOP and the efficacy of the proposed GC strategies in improving the SOP performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01921v1</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.NA</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thai-Hoc Vu, Daniel Benevides da Costa, Bao Vo Nguyen Quoc, Sunghwan Kim</dc:creator>
    </item>
    <item>
      <title>Multi-level quantum signal processing with applications to ground state preparation using fast-forwarded Hamiltonian evolution</title>
      <link>https://arxiv.org/abs/2406.02086</link>
      <description>arXiv:2406.02086v1 Announce Type: cross 
Abstract: The preparation of the ground state of a Hamiltonian $H$ with a large spectral radius has applications in many areas such as electronic structure theory and quantum field theory. Given an initial state with a constant overlap with the ground state, and assuming that the Hamiltonian $H$ can be efficiently simulated with an ideal fast-forwarding protocol, we first demonstrate that employing a linear combination of unitaries (LCU) approach can prepare the ground state at a cost of $\mathcal{O}(\log^2(\|H\| \Delta^{-1}))$ queries to controlled Hamiltonian evolution. Here $\|H\|$ is the spectral radius of $H$ and $\Delta$ the spectral gap. However, traditional Quantum Signal Processing (QSP)-based methods fail to capitalize on this efficient protocol, and its cost scales as $\mathcal{O}(\|H\| \Delta^{-1})$. To bridge this gap, we develop a multi-level QSP-based algorithm that exploits the fast-forwarding feature. This novel algorithm not only matches the efficiency of the LCU approach when an ideal fast-forwarding protocol is available, but also exceeds it with a reduced cost that scales as $\mathcal{O}(\log(\|H\| \Delta^{-1}))$. Additionally, our multi-level QSP method requires only $\mathcal{O}(\log(\|H\| \Delta^{-1}))$ coefficients for implementing single qubit rotations. This eliminates the need for constructing the PREPARE oracle in LCU, which prepares a state encoding $\mathcal{O}(\|H\| \Delta^{-1})$ coefficients regardless of whether the Hamiltonian can be fast-forwarded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02086v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulong Dong, Lin Lin</dc:creator>
    </item>
    <item>
      <title>Analysis and Simulation of a Coupled Fluid-Heat System in a Thin, Rough Layer</title>
      <link>https://arxiv.org/abs/2406.02150</link>
      <description>arXiv:2406.02150v1 Announce Type: cross 
Abstract: We investigate the effective coupling between heat and fluid dynamics within a thin fluid layer in contact with a solid structure via a rough surface. Moreover, the opposing vertical surfaces of the thin layer are in relative motion. This setup is particularly relevant to grinding processes, where cooling lubricants interact with the rough surface of a rotating grinding wheel. The resulting model is non-linearly coupled through(i) temperature-dependent viscosity and (ii) convective heat transport. The underlying geometry is highly heterogeneous due to the thin, rough surface characterized by a small parameter representing both the height of the layer and the periodicity of the roughness. We analyze this non-linear system for existence, uniqueness, and energy estimates and study the limit behavior within the framework of two-scale convergence in thin domains. In this limit, we derive an effective interface model in 3D (a line in 2D) for the heat and fluid interactions inside the fluid. We implement the system numerically and validate the limit problem through direct comparison with the micromodel. Additionally, we investigate the influence of the temperature-dependent viscosity and various geometrical configurations via simulation experiments. The corresponding numerical code is freely available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02150v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Freudenberg, Michael Eden</dc:creator>
    </item>
    <item>
      <title>Projection scheme for a perfect plasticity model with a time-dependent constraint set</title>
      <link>https://arxiv.org/abs/2406.02218</link>
      <description>arXiv:2406.02218v1 Announce Type: cross 
Abstract: This paper introduces a new numerical scheme for a system that includes evolution equations describing a perfect plasticity model with a time-dependent yield surface. We demonstrate that the solution to the proposed scheme is stable under suitable norms. Moreover, the stability leads to the existence of an exact solution, and we also prove that the solution to the proposed scheme converges strongly to the exact solution under suitable norms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02218v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiho Akagawa, Kazunori Matsui</dc:creator>
    </item>
    <item>
      <title>Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions</title>
      <link>https://arxiv.org/abs/2406.02502</link>
      <description>arXiv:2406.02502v1 Announce Type: cross 
Abstract: Given a matrix $A \in \mathbb{R}^{m\times d}$ with singular values $\sigma_1\geq \cdots \geq \sigma_d$, and a random matrix $G \in \mathbb{R}^{m\times d}$ with iid $N(0,T)$ entries for some $T&gt;0$, we derive new bounds on the Frobenius distance between subspaces spanned by the top-$k$ (right) singular vectors of $A$ and $A+G$. This problem arises in numerous applications in statistics where a data matrix may be corrupted by Gaussian noise, and in the analysis of the Gaussian mechanism in differential privacy, where Gaussian noise is added to data to preserve private information. We show that, for matrices $A$ where the gaps in the top-$k$ singular values are roughly $\Omega(\sigma_k-\sigma_{k+1})$ the expected Frobenius distance between the subspaces is $\tilde{O}(\frac{\sqrt{d}}{\sigma_k-\sigma_{k+1}} \times \sqrt{T})$, improving on previous bounds by a factor of $\frac{\sqrt{m}}{\sqrt{d}} \sqrt{k}$. To obtain our bounds we view the perturbation to the singular vectors as a diffusion process -- the Dyson-Bessel process -- and use tools from stochastic calculus to track the evolution of the subspace spanned by the top-$k$ singular vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02502v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyao Lai, Oren Mangoubi</dc:creator>
    </item>
    <item>
      <title>Fast randomized algorithms for computing the generalized tensor SVD based on the tubal product</title>
      <link>https://arxiv.org/abs/2305.05031</link>
      <description>arXiv:2305.05031v4 Announce Type: replace 
Abstract: This work deals with developing two fast randomized algorithms for computing the generalized tensor singular value decomposition (GTSVD) based on the tubal product (t-product). The random projection method is utilized to compute the important actions of the underlying data tensors and use them to get small sketches of the original data tensors, which are easier to be handled. Due to the small size of the sketch tensors, deterministic approaches are applied to them to compute their GTSVDs. Then, from the GTSVD of the small sketch tensors, the GTSVD of the original large-scale data tensors is recovered. Some experiments are conducted to show the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05031v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salman Ahmadi-Asl, Ugochukwu Ugwu</dc:creator>
    </item>
    <item>
      <title>Deep ReLU networks and high-order finite element methods II: Chebyshev emulation</title>
      <link>https://arxiv.org/abs/2310.07261</link>
      <description>arXiv:2310.07261v2 Announce Type: replace 
Abstract: We show expression rates and stability in Sobolev norms of deep feedforward ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$. Novel constructions of ReLU NN surrogates encoding function approximations in terms of Chebyshev polynomial expansion coefficients are developed which require fewer neurons than previous constructions. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability are obtained that are superior to those of constructions based on ReLU NN emulations of monomials as considered in [Opschoor, Petersen and Schwab, 2020] and [Montanelli, Yang and Du, 2021]. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for various classes of functions and norms, commonly encountered in numerical analysis. In particular, we show exponential ReLU emulation rate bounds for analytic functions with point singularities and develop an interface between Chebfun approximations and constructive ReLU NN emulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07261v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joost A. A. Opschoor, Christoph Schwab</dc:creator>
    </item>
    <item>
      <title>Structure-preserving semi-convex-splitting numerical scheme for a Cahn-Hilliard cross-diffusion system in lymphangiogenesis</title>
      <link>https://arxiv.org/abs/2311.11398</link>
      <description>arXiv:2311.11398v3 Announce Type: replace 
Abstract: A fully discrete semi-convex-splitting finite-element scheme with stabilization for a Cahn-Hilliard cross-diffusion system is analyzed. The system consists of parabolic fourth-order equations for the volume fraction of the fiber phase and solute concentration, modeling pre-patterning of lymphatic vessel morphology. The existence of discrete solutions is proved, and it is shown that the numerical scheme is energy stable up to stabilization, conserves the solute mass, and preserves the lower and upper bounds of the fiber phase fraction. Numerical experiments in two space dimensions using FreeFEM illustrate the phase segregation and pattern formation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11398v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ansgar J\"ungel, Boyi Wang</dc:creator>
    </item>
    <item>
      <title>Neural networks for the approximation of Euler's elastica</title>
      <link>https://arxiv.org/abs/2312.00644</link>
      <description>arXiv:2312.00644v2 Announce Type: replace 
Abstract: Euler's elastica is a classical model of flexible slender structures, relevant in many industrial applications. Static equilibrium equations can be derived via a variational principle. The accurate approximation of solutions of this problem can be challenging due to nonlinearity and constraints. We here present two neural network based approaches for the simulation of this Euler's elastica. Starting from a data set of solutions of the discretised static equilibria, we train the neural networks to produce solutions for unseen boundary conditions. We present a $\textit{discrete}$ approach learning discrete solutions from the discrete data. We then consider a $\textit{continuous}$ approach using the same training data set, but learning continuous solutions to the problem. We present numerical evidence that the proposed neural networks can effectively approximate configurations of the planar Euler's elastica for a range of different boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00644v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Celledoni, Ergys \c{C}okaj, Andrea Leone, Sigrid Leyendecker, Davide Murari, Brynjulf Owren, Rodrigo T. Sato Mart\'in de Almagro, Martina Stavole</dc:creator>
    </item>
    <item>
      <title>Learned Regularization for Inverse Problems: Insights from a Spectral Model</title>
      <link>https://arxiv.org/abs/2312.09845</link>
      <description>arXiv:2312.09845v2 Announce Type: replace 
Abstract: In this chapter we provide a theoretically founded investigation of state-of-the-art learning approaches for inverse problems from the point of view of spectral reconstruction operators. We give an extended definition of regularization methods and their convergence in terms of the underlying data distributions, which paves the way for future theoretical studies. Based on a simple spectral learning model previously introduced for supervised learning, we investigate some key properties of different learning paradigms for inverse problems, which can be formulated independently of specific architectures. In particular we investigate the regularization properties, bias, and critical dependence on training data distributions. Moreover, our framework allows to highlight and compare the specific behavior of the different paradigms in the infinite-dimensional limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09845v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Burger, Samira Kabri</dc:creator>
    </item>
    <item>
      <title>Reduced Order Model Enhanced Source Iteration with Synthetic Acceleration for Parametric Radiative Transfer Equation</title>
      <link>https://arxiv.org/abs/2402.10488</link>
      <description>arXiv:2402.10488v3 Announce Type: replace 
Abstract: Applications such as uncertainty quantification and optical tomography, require solving the radiative transfer equation (RTE) many times for various parameters. Efficient solvers for RTE are highly desired.
  Source Iteration with Synthetic Acceleration (SISA) is a popular and successful iterative solver for RTE. Synthetic Acceleration (SA) acts as a preconditioning step to accelerate the convergence of Source Iteration (SI). After each source iteration, classical SA strategies introduce a correction to the macroscopic particle density by solving a low order approximation to a kinetic correction equation. For example, Diffusion Synthetic Acceleration (DSA) uses the diffusion limit. However, these strategies may become less effective when the underlying low order approximations are not accurate enough. Furthermore, they do not exploit low rank structures concerning the parameters of parametric problems.
  To address these issues, we propose enhancing SISA with data-driven ROMs for the parametric problem and the corresponding kinetic correction equation. First, the ROM for the parametric problem can be utilized to obtain an improved initial guess. Second, the ROM for the kinetic correction equation can be utilized to design a novel SA strategy called ROMSAD. In the early stage, ROMSAD adopts a ROM based approximation, which builds on the kinetic description of the correction equation and leverages low rank structures concerning the parameters. This ROM-based approximation has greater efficiency than DSA in the early stage of SI. In the later stage, ROMSAD automatically switches to DSA to leverage its robustness. Additionally, we propose an approach to construct the ROM for the kinetic correction equation without directly solving it.
  In a series of of numerical tests, we compare the performance of the proposed methods with SI-DSA and DSA preconditioned GMRES solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10488v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichao Peng</dc:creator>
    </item>
    <item>
      <title>Extrapolated Plug-and-Play Three-Operator Splitting Methods for Nonconvex Optimization with Applications to Image Restoration</title>
      <link>https://arxiv.org/abs/2403.01144</link>
      <description>arXiv:2403.01144v2 Announce Type: replace 
Abstract: This paper investigates the convergence properties and applications of the three-operator splitting method, also known as Davis-Yin splitting (DYS) method, integrated with extrapolation and Plug-and-Play (PnP) denoiser within a nonconvex framework. We first propose an extrapolated DYS method to effectively solve a class of structural nonconvex optimization problems that involve minimizing the sum of three possible nonconvex functions. Our approach provides an algorithmic framework that encompasses both extrapolated forward-backward splitting and extrapolated Douglas-Rachford splitting methods. To establish the convergence of the proposed method, we rigorously analyze its behavior based on the Kurdyka-{\L}ojasiewicz property, subject to some tight parameter conditions. Moreover, we introduce two extrapolated PnP-DYS methods with convergence guarantee, where the traditional regularization prior is replaced by a gradient step-based denoiser. This denoiser is designed using a differentiable neural network and can be reformulated as the proximal operator of a specific nonconvex functional. We conduct extensive experiments on image deblurring and image super-resolution problems, where our results showcase the advantage of the extrapolation strategy and the superior performance of the learning-based model that incorporates the PnP denoiser in terms of achieving high-quality recovery images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01144v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongming Wu, Chaoyan Huang, Tieyong Zeng</dc:creator>
    </item>
    <item>
      <title>Existence and nonexistence of solutions for underdetermined generalized absolute value equations</title>
      <link>https://arxiv.org/abs/2405.16172</link>
      <description>arXiv:2405.16172v2 Announce Type: replace 
Abstract: Underdetermined generalized absolute value equations (GAVE) has real applications. The underdetermined GAVE may have no solution, one solution, finitely multiple solutions or infinitely many solutions. This paper aims to give some sufficient conditions which guarantee the existence or nonexistence of solutions for the underdetermined GAVE. Particularly, sufficient conditions under which certain or each sign pattern possesses infinitely many solutions of the underdetermined GAVE are given. In addition, iterative methods are developed to solve a solution of the underdetermined GAVE. Some existing results about the square GAVE are extended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16172v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cairong Chen, Xuehua Li, Ren-Cang Li</dc:creator>
    </item>
    <item>
      <title>A brief review of Reduced Order Models using intrusive and non-intrusive techniques</title>
      <link>https://arxiv.org/abs/2406.00559</link>
      <description>arXiv:2406.00559v2 Announce Type: replace 
Abstract: Reduced Order Models (ROMs) have gained a great attention by the scientific community in the last years thanks to their capabilities of significantly reducing the computational cost of the numerical simulations, which is a crucial objective in applications like real time control and shape optimization. This contribution aims to provide a brief overview about such a topic. We discuss both an intrusive framework based on a Galerkin projection technique and non-intrusive approaches, including Physics Informed Neural Networks (PINN), purely Data-Driven Neural Networks (DDNN), Radial Basis Functions (RBF), Dynamic Mode Decomposition (DMD) and Gaussian Process Regression (GPR). We also briefly mention geometrical parametrization and dimensionality reduction methods like Active Subspaces (AS). Then we present some results related to academic test cases as well as a preliminary investigation related to an industrial application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00559v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guglielmo Padula, Michele Girfoglio, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>Efficient Fourier representations of families of Gaussian processes</title>
      <link>https://arxiv.org/abs/2109.14081</link>
      <description>arXiv:2109.14081v3 Announce Type: replace-cross 
Abstract: We introduce a class of algorithms for constructing Fourier representations of Gaussian processes in $1$ dimension that are valid over ranges of hyperparameter values. The scaling and frequencies of the Fourier basis functions are evaluated numerically via generalized quadratures. The representations introduced allow for $O(m^3)$ inference, independent of $N$, for all hyperparameters in the user-specified range after $O(N + m^2\log{m})$ precomputation where $N$, the number of data points, is usually significantly larger than $m$, the number of basis functions. Inference independent of $N$ for various hyperparameters is facilitated by generalized quadratures, and the $O(N + m^2\log{m})$ precomputation is achieved with the non-uniform FFT. Numerical results are provided for Mat\'ern kernels with $\nu \in [3/2, 7/2]$ and lengthscale $\rho \in [0.1, 0.5]$ and squared-exponential kernels with lengthscale $\rho \in [0.1, 0.5]$. The algorithms of this paper generalize mathematically to higher dimensions, though they suffer from the standard curse of dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.14081v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Fractional Gradient Descent</title>
      <link>https://arxiv.org/abs/2311.18426</link>
      <description>arXiv:2311.18426v5 Announce Type: replace-cross 
Abstract: Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivatives. Finally, empirical results will be presented on the potential speed up of fractional gradient descent over standard gradient descent as well as some preliminary theoretical results explaining this speed up.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18426v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashwani Aggarwal</dc:creator>
    </item>
    <item>
      <title>An Image Segmentation Model with Transformed Total Variation</title>
      <link>https://arxiv.org/abs/2406.00571</link>
      <description>arXiv:2406.00571v2 Announce Type: replace-cross 
Abstract: Based on transformed $\ell_1$ regularization, transformed total variation (TTV) has robust image recovery that is competitive with other nonconvex total variation (TV) regularizers, such as TV$^p$, $0&lt;p&lt;1$. Inspired by its performance, we propose a TTV-regularized Mumford--Shah model with fuzzy membership function for image segmentation. To solve it, we design an alternating direction method of multipliers (ADMM) algorithm that utilizes the transformed $\ell_1$ proximal operator. Numerical experiments demonstrate that using TTV is more effective than classical TV and other nonconvex TV variants in image segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00571v2</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisha Dayag, Kevin Bui, Fredrick Park, Jack Xin</dc:creator>
    </item>
  </channel>
</rss>
