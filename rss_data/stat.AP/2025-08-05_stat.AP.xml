<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Aug 2025 04:02:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation</title>
      <link>https://arxiv.org/abs/2508.01861</link>
      <description>arXiv:2508.01861v1 Announce Type: new 
Abstract: Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with an asset-pricing pipeline tailored for tensor-structured financial data. Results show that ACT-Tensor not only reduces pricing errors but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01861v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyi Mo, Jiayu Li, Duo Zhang, Elynn Chen</dc:creator>
    </item>
    <item>
      <title>Entropy-Based Methods to Address Sampling Bias in Archaeological Predictive Modeling</title>
      <link>https://arxiv.org/abs/2508.02272</link>
      <description>arXiv:2508.02272v1 Announce Type: new 
Abstract: Predictive modeling in archaeology is essential for the understanding of people's behavior in the past and for guiding heritage conservation. However, spatial sampling bias caused by uneven research effort can severely limit model reliability. This research describes a novel new framework that integrates entropy-based corrections to measure and minimize such biases in archaeological modeling of foresight. Leveraging the open access data of the Grand Staircase-Escalante National Monument, we employ Shannon entropy to determine survey coverage and assign appropriate weights to pseudo-absence points. We combine these weights with predictive models such as Bayesian Spatial Logistic Regression (via R-INLA), Generalized Additive Models, Maximum Entropy and Random Forests. Our findings prove that entropy-aware models exhibit improved accuracy and robustness, especially for under-surveyed regions. This approach not only advances methodological transparency, but also improves the interpretation of archaeological prediction under conditions of data uncertainty. The proposed framework offers a scalable, theoretically grounded strategy for addressing spatial bias in archaeological datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02272v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehmet S{\i}dd{\i}k \c{C}ad{\i}rc{\i}, Golnaz Shahtahmassebi</dc:creator>
    </item>
    <item>
      <title>Understanding Heterogeneity in Adaptation to Intermittent Water Supply: Clustering Household Types in Amman, Jordan</title>
      <link>https://arxiv.org/abs/2508.02569</link>
      <description>arXiv:2508.02569v1 Announce Type: new 
Abstract: More than a billion people around the world experience intermittence in their water supply, posing challenges for urban households in Global South cities. An intermittent water supply (IWS) system prompts water users to adapt to service deficits which entails coping costs. Adaptation and its impacts can vary between households within the same city, leading to intra-urban inequality. Studies on household adaptation to IWS through survey data are limited to exploring income-based heterogeneity and do not account for the multidimensional and non-linear nature of the data. There is a need for a standardized methodology for understanding household responses to IWS that acknowledges the heterogeneity of households characterized by sets of multiple underlying factors and that is applicable across different settings. Here, we develop an analysis pipeline that applies hierarchical clustering analysis (HCA) in combination with the Welch-two-sample t-test on household survey data from Amman, Jordan. We identify three clusters of households distinguished by a set of characteristics including income, water social network, supply duration, relocation and water quality problems and identify their group-specific adaptive strategies such as contacting the utility or accessing an alternate water source. This study uncovers the unequal nature of IWS adaptation in Amman, giving insights into the link between household characteristics and adaptive behaviors, while proposing a standardized method to reveal relevant heterogeneity in households adapting to IWS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02569v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreyas Gadge, V\'itor V. Vasconcelos, Andr\'e de Roos, Elisabeth H. Krueger</dc:creator>
    </item>
    <item>
      <title>A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data</title>
      <link>https://arxiv.org/abs/2508.00888</link>
      <description>arXiv:2508.00888v1 Announce Type: cross 
Abstract: Naturalistic driving studies offer a powerful means for observing and quantifying real-world driving behaviour. One of their prominent applications in traffic safety is the continuous monitoring and classification of risky driving behaviour. However, many existing frameworks rely on fixed time windows and static thresholds for distinguishing between safe and risky behaviour - limiting their ability to respond to the stochastic nature of real-world driving. This study proposes a dynamic and individualised framework for identifying risky driving behaviour using Belgian naturalistic driving data. The approach leverages a rolling time window and bi-level optimisation to dynamically calibrate both risk thresholds and model hyperparameters, capturing subtle behavioural shifts. Two safety indicators, speed-weighted headway and harsh driving events, were evaluated using three data-driven models: Random Forest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong capability in capturing subtle changes in driving behaviour, particularly excelling in high-recall tasks, making it promising for early-stage risk detection. XGBoost provided the most balanced and stable performance across different thresholds and evaluation metrics. While random forest showed more variability, it responded sensitively to dynamic threshold adjustments, which may be advantageous during model adaptation or tuning. Speed-weighted headway emerged as a more stable and context-sensitive risk indicator than harsh driving events, likely due to its robustness to label sparsity and contextual variation. Overall, the findings support the value of adaptive, personalised risk detection approaches for enhancing real-time safety feedback and tailoring driver support in intelligent transport systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00888v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Hossein Kalantari, Eleonora Papadimitriou, Amir Pooyan Afghari</dc:creator>
    </item>
    <item>
      <title>BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation</title>
      <link>https://arxiv.org/abs/2508.01285</link>
      <description>arXiv:2508.01285v1 Announce Type: cross 
Abstract: Identifying novel hypotheses is essential to scientific research, yet this process risks being overwhelmed by the sheer volume and complexity of available information. Existing automated methods often struggle to generate novel and evidence-grounded hypotheses, lack robust iterative refinement and rarely undergo rigorous temporal evaluation for future discovery potential. To address this, we propose BioDisco, a multi-agent framework that draws upon language model-based reasoning and a dual-mode evidence system (biomedical knowledge graphs and automated literature retrieval) for grounded novelty, integrates an internal scoring and feedback loop for iterative refinement, and validates performance through pioneering temporal and human evaluations and a Bradley-Terry paired comparison model to provide statistically-grounded assessment. Our evaluations demonstrate superior novelty and significance over ablated configurations representative of existing agentic architectures. Designed for flexibility and modularity, BioDisco allows seamless integration of custom language models or knowledge graphs, and can be run with just a few lines of code. We anticipate researchers using this practical tool as a catalyst for the discovery of new hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01285v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujing Ke, Kevin George, Kathan Pandya, David Blumenthal, Maximilian Sprang, Gerrit Gro{\ss}mann, Sebastian Vollmer, David Antony Selby</dc:creator>
    </item>
    <item>
      <title>Fusion Sampling Validation in Data Partitioning for Machine Learning</title>
      <link>https://arxiv.org/abs/2508.01325</link>
      <description>arXiv:2508.01325v1 Announce Type: cross 
Abstract: Effective data partitioning is known to be crucial in machine learning. Traditional cross-validation methods like K-Fold Cross-Validation (KFCV) enhance model robustness but often compromise generalisation assessment due to high computational demands and extensive data shuffling. To address these issues, the integration of the Simple Random Sampling (SRS), which, despite providing representative samples, can result in non-representative sets with imbalanced data. The study introduces a hybrid model, Fusion Sampling Validation (FSV), combining SRS and KFCV to optimise data partitioning. FSV aims to minimise biases and merge the simplicity of SRS with the accuracy of KFCV. The study used three datasets of 10,000, 50,000, and 100,000 samples, generated with a normal distribution (mean 0, variance 1) and initialised with seed 42. KFCV was performed with five folds and ten repetitions, incorporating a scaling factor to ensure robust performance estimation and generalisation capability. FSV integrated a weighted factor to enhance performance and generalisation further. Evaluations focused on mean estimates (ME), variance estimates (VE), mean squared error (MSE), bias, the rate of convergence for mean estimates (ROC\_ME), and the rate of convergence for variance estimates (ROC\_VE). Results indicated that FSV consistently outperformed SRS and KFCV, with ME values of 0.000863, VE of 0.949644, MSE of 0.952127, bias of 0.016288, ROC\_ME of 0.005199, and ROC\_VE of 0.007137. FSV demonstrated superior accuracy and reliability in data partitioning, particularly in resource-constrained environments and extensive datasets, providing practical solutions for effective machine learning implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01325v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Godwin Udomboso, Caston Sigauke, Ini Adinya</dc:creator>
    </item>
    <item>
      <title>A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics</title>
      <link>https://arxiv.org/abs/2508.01490</link>
      <description>arXiv:2508.01490v1 Announce Type: cross 
Abstract: Spatial transcriptomics enables simultaneous measurement of gene expression and tissue morphology, offering unprecedented insights into cellular organization and disease mechanisms. However, the field lacks comprehensive benchmarks for evaluating multimodal learning methods that leverage both histology images and gene expression data. Here, we present HESCAPE, a large-scale benchmark for cross-modal contrastive pretraining in spatial transcriptomics, built on a curated pan-organ dataset spanning 6 different gene panels and 54 donors. We systematically evaluated state-of-the-art image and gene expression encoders across multiple pretraining strategies and assessed their effectiveness on two downstream tasks: gene mutation classification and gene expression prediction. Our benchmark demonstrates that gene expression encoders are the primary determinant of strong representational alignment, and that gene models pretrained on spatial transcriptomics data outperform both those trained without spatial data and simple baseline approaches. However, downstream task evaluation reveals a striking contradiction: while contrastive pretraining consistently improves gene mutation classification performance, it degrades direct gene expression prediction compared to baseline encoders trained without cross-modal objectives. We identify batch effects as a key factor that interferes with effective cross-modal alignment. Our findings highlight the critical need for batch-robust multimodal learning approaches in spatial transcriptomics. To accelerate progress in this direction, we release HESCAPE, providing standardized datasets, evaluation protocols, and benchmarking tools for the community</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01490v1</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rushin H. Gindra, Giovanni Palla, Mathias Nguyen, Sophia J. Wagner, Manuel Tran, Fabian J Theis, Dieter Saur, Lorin Crawford, Tingying Peng</dc:creator>
    </item>
    <item>
      <title>Why Heuristic Weighting Works: A Theoretical Analysis of Denoising Score Matching</title>
      <link>https://arxiv.org/abs/2508.01597</link>
      <description>arXiv:2508.01597v1 Announce Type: cross 
Abstract: Score matching enables the estimation of the gradient of a data distribution, a key component in denoising diffusion models used to recover clean data from corrupted inputs. In prior work, a heuristic weighting function has been used for the denoising score matching loss without formal justification. In this work, we demonstrate that heteroskedasticity is an inherent property of the denoising score matching objective. This insight leads to a principled derivation of optimal weighting functions for generalized, arbitrary-order denoising score matching losses, without requiring assumptions about the noise distribution. Among these, the first-order formulation is especially relevant to diffusion models. We show that the widely used heuristical weighting function arises as a first-order Taylor approximation to the trace of the expected optimal weighting. We further provide theoretical and empirical comparisons, revealing that the heuristical weighting, despite its simplicity, can achieve lower variance than the optimal weighting with respect to parameter gradients, which can facilitate more stable and efficient training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01597v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juyan Zhang, Rhys Newbury, Xinyang Zhang, Tin Tran, Dana Kulic, Michael Burke</dc:creator>
    </item>
    <item>
      <title>CITS: Nonparametric Statistical Causal Modeling for High-Resolution Neural Time Series</title>
      <link>https://arxiv.org/abs/2508.01920</link>
      <description>arXiv:2508.01920v1 Announce Type: cross 
Abstract: Understanding how signals propagate through neural circuits is central to deciphering brain computation. While functional connectivity captures statistical associations, it does not reveal directionality or causal mechanisms. We introduce CITS (Causal Inference in Time Series), a non-parametric method for inferring statistically causal neural circuitry from high-resolution time series data. CITS models neural dynamics using a structural causal model with arbitrary Markov order and tests for time-lagged conditional independence using either Gaussian or distribution-free statistics. Unlike classical Granger Causality, which assumes linear autoregressive models and Gaussian noise, or the Peter-Clark algorithm, which assumes i.i.d. data and no temporal structure, CITS handles temporally dependent, potentially non-Gaussian data with flexible testing procedures. We prove consistency under mild mixing assumptions and validate CITS on simulated linear, nonlinear, and continuous-time recurrent neural network data, where it outperforms state-of-the-art methods. We then apply CITS to Neuropixels recordings from mouse brain during visual tasks. CITS uncovers interpretable, stimulus-specific causal circuits linking cortical, thalamic, and hippocampal regions, consistent with experimental literature. It also reveals that neurons with similar orientation selectivity indices are more likely to be causally connected. Our results demonstrate the utility of CITS in uncovering biologically meaningful pathways and generating hypotheses for future experimental studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01920v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Biswas, SuryaNarayana Sripada, Somabha Mukherjee, Reza Abbasi-Asl</dc:creator>
    </item>
    <item>
      <title>The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise</title>
      <link>https://arxiv.org/abs/2508.02223</link>
      <description>arXiv:2508.02223v1 Announce Type: cross 
Abstract: Maximum likelihood factor analysis has been used for direction of arrival estimation in unknown nonuniform noise and some iterative approaches have been developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN) method proposed by Stoica and Babu has excellent convergence properties. In this letter, the Expectation/Conditional Maximization Either (ECME) algorithm, an extension of the expectation-maximization algorithm, is designed, which has almost the same computational complexity at each iteration as the FAAN method. However, numerical results show that the ECME algorithm yields faster stable convergence and is computationally more efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02223v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Gong</dc:creator>
    </item>
    <item>
      <title>Modelling Stochastic Inflow Patterns to a Reservoir with a Hidden Phase-Type Markov Model</title>
      <link>https://arxiv.org/abs/2508.02522</link>
      <description>arXiv:2508.02522v1 Announce Type: cross 
Abstract: This paper presents a novel methodology for modelling precipitation patterns in a specific geographical region using Hidden Markov Models (HMMs). Departing from conventional HMMs, where the hidden state process is assumed to be Markovian, we introduce non-Markovian behaviour by incorporating phase-type distributions to model state durations. The primary objective is to capture the alternating sequences of dry and wet periods that characterize the local climate, providing deeper insight into its temporal structure. Building on this foundation, we extend the model to represent reservoir inflow patterns, which are then used to explain the observed water storage levels via a Moran model. The dataset includes historical rainfall and inflow records, where the latter is influenced by latent conditions governed by the hidden states. Direct modelling based solely on observed rainfall is insufficient due to the complexity of the system, hence the use of HMMs to infer these unobserved dynamics. This approach facilitates more accurate characterization of the underlying climatic processes and enables forecasting of future inflows based on historical data, supporting improved water resource management in the region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02522v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. L. Gamiz, D. Montoro, M. C Segovia-Garcia</dc:creator>
    </item>
    <item>
      <title>Trustworthy scientific inference for inverse problems with generative models</title>
      <link>https://arxiv.org/abs/2508.02602</link>
      <description>arXiv:2508.02602v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) excels at producing complex data structures (text, images, videos) by learning patterns from training examples. Across scientific disciplines, researchers are now applying generative models to ``inverse problems'' to infer hidden parameters from observed data. While these methods can handle intractable models and large-scale studies, they can also produce biased or overconfident conclusions. We present a solution with Frequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes AI-generated probability distributions into confidence regions that consistently include true parameters with the expected probability, while achieving minimum size when training and target data align. We demonstrate FreB's effectiveness by tackling diverse case studies in the physical sciences: identifying unknown sources under dataset shift, reconciling competing theoretical models, and mitigating selection bias and systematics in observational studies. By providing validity guarantees with interpretable diagnostics, FreB enables trustworthy scientific inference across fields where direct likelihood evaluation remains impossible or prohibitively expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02602v1</guid>
      <category>stat.ML</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Luca Masserano, Joshua D. Ingram, Alex Shen, Antonio Carlos Herling Ribeiro Junior, Tommaso Dorigo, Michele Doro, Joshua S. Speagle, Rafael Izbicki, Ann B. Lee</dc:creator>
    </item>
    <item>
      <title>Multilevel Regression and Poststratification Interface: An Application to Track Community-level COVID-19 Viral Transmission</title>
      <link>https://arxiv.org/abs/2405.05909</link>
      <description>arXiv:2405.05909v2 Announce Type: replace 
Abstract: We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow. In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate community-level viral incidence, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system. The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP), a procedure that adjusts for nonrepresentativeness of the sample and yields stable small group estimates. We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05909v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajuan Si, Toan Tran, Jonah Gabry, Mitzi Morris, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Identifying Robust Mediators of Health Disparities: A Review and Simulation Studies With Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2506.19047</link>
      <description>arXiv:2506.19047v2 Announce Type: replace 
Abstract: Background. A central objective among health researchers across disciplines is to identify modifiable factors that can reduce health disparities. Three common methods--difference-in-coefficients (DIC), Kitagawa-Oaxaca-Blinder (KOB), and causal decomposition analysis (CDA)--share the same goal to identify such contributors but can produce divergent results depending on confounding and model assumptions. Despite these challenges, applied researchers lack clear guidance on selecting appropriate methods for different scenarios. Methods. We start with a brief review of each method, assuming no unmeasured confounders. We then move to two more realistic scenarios: 1) unmeasured confounders affect the relationship between intermediate confounders and the mediator, and 2) unmeasured confounders affect the relationship between the mediator and the outcome. For each scenario, we generate simulated data, apply all three methods, compare their estimates, and interpret the results using Directed Acyclic Graphs. Results. Under the assumption of no unmeasured confounders, the DIC approach is a simplistic method suitable only when no intermediate confounders are present. The KOB decomposition is appropriate unless adjustment for baseline covariates is necessary. When unmeasured confounding exists, the DIC method yields biased estimates in both scenarios, and all three methods produce biased results in the second scenario. However, CDA, when paired with sensitivity analysis can help assess the robustness of its estimates. Conclusions. We advise against using the DIC method, particularly, in observational studies, as the assumptions of no intermediate confounders is often unrealistic. When unmeasured confounders are anticipated, CDA combined with sensitivity analysis offers a more robust approach for identifying mediators over other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19047v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Su Yeon Kim, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Replicable Bandits for Digital Health Interventions</title>
      <link>https://arxiv.org/abs/2407.15377</link>
      <description>arXiv:2407.15377v3 Announce Type: replace-cross 
Abstract: Adaptive treatment assignment algorithms, such as bandit algorithms, are increasingly used in digital health intervention clinical trials. Frequently the data collected from these trials is used to conduct causal inference and related data analyses to decide how to refine the intervention, and whether to roll-out the intervention more broadly. This work studies inference for estimands that depend on the adaptive algorithm itself; a simple example is the mean reward under the adaptive algorithm. Specifically, we investigate the replicability of statistical analyses concerning such estimands when using data from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical estimators are guaranteed to be consistent and asymptotically normal. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15377v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kelly W. Zhang, Nowell Closser, Anna L. Trella, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>A class of modular and flexible covariate-based covariance functions for nonstationary spatial modeling</title>
      <link>https://arxiv.org/abs/2410.16716</link>
      <description>arXiv:2410.16716v2 Announce Type: replace-cross 
Abstract: The assumptions of stationarity and isotropy often stated over spatial processes have not aged well during the last two decades, partly explained by the combination of computational developments and the increasing availability of high-resolution spatial data. While a plethora of approaches have been developed to relax these assumptions, it is often a costly tradeoff between flexibility and a diversity of computational challenges. In this paper, we present a class of covariance functions that relies on fixed, observable spatial information that provides a convenient tradeoff while offering an extra layer of numerical and visual representation of the flexible spatial dependencies. This model allows for separate parametric structures for different sources of nonstationarity, such as marginal standard deviation, geometric anisotropy, and smoothness. It simplifies to a Mat\'ern covariance function in its basic form and is adaptable for large datasets, enhancing flexibility and computational efficiency. We analyze the capabilities of the presented model through simulation studies and an application to Swiss precipitation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16716v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Blasi, Reinhard Furrer</dc:creator>
    </item>
    <item>
      <title>The impact of strong activity disruption on building energetics</title>
      <link>https://arxiv.org/abs/2504.19921</link>
      <description>arXiv:2504.19921v2 Announce Type: replace-cross 
Abstract: Evidence shows that biological organisms tend to be more energetically efficient per unit size. These scaling patterns observed in biological organisms have also been observed in the energetic requirements of cities. However, at lower levels of organization where energetic interventions can be more manageable, such as buildings, this analysis has remained more elusive due to the difficulties in collecting fine-grained data. Here, we use the maintenance energy usage in buildings at the Massachusetts Institute of Technology (MIT) from 2009 to 2024 to analyze energetic trends at the scale of individual buildings and their sensitivity to strong external perturbations. We find that, similar to the baseline metabolism of biological organisms, large buildings are on average $24\%$ more energetically efficient per unit size than smaller buildings. Because it has become debatable how to better measure the efficiency of buildings, this scaling pattern naturally establishes a baseline efficiency for buildings, where deviations from the mean would imply a more or less efficient building than the baseline according to volume. This relative efficiency progressively increased to $34\%$ until 2020. However, the strong activity disruption caused by the COVID-19 pandemic acted as a major shock, removing this trend and leading to a reversal to the expected $24\%$ baseline level. This suggests that energetic adaptations are contingent on relatively stable conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19921v2</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu-Hsuan Hsu, Sara Beery, Christopher P. Kempes, Mingzhen Lu, Serguei Saavedra</dc:creator>
    </item>
    <item>
      <title>Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2505.10213</link>
      <description>arXiv:2505.10213v2 Announce Type: replace-cross 
Abstract: With the widespread adoption of Large Language Models (LLMs), there is a growing need to establish best practices for leveraging their capabilities beyond traditional natural language tasks. In this paper, a novel cross-domain knowledge transfer framework is proposed to enhance the performance of LLMs in time series forecasting -- a task of increasing relevance in fields such as energy systems, finance, and healthcare. The approach systematically infuses LLMs with structured temporal information to improve their forecasting accuracy. This study evaluates the proposed method on a real-world time series dataset and compares it to a naive baseline where the LLM receives no auxiliary information. Results show that knowledge-informed forecasting significantly outperforms the uninformed baseline in terms of predictive accuracy and generalization. These findings highlight the potential of knowledge transfer strategies to bridge the gap between LLMs and domain-specific forecasting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10213v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohammadmahdi Ghasemloo, Alireza Moradi</dc:creator>
    </item>
    <item>
      <title>Weighted Parameter Estimators of the Generalized Extreme Value Distribution in the Presence of Missing Observations</title>
      <link>https://arxiv.org/abs/2506.15964</link>
      <description>arXiv:2506.15964v2 Announce Type: replace-cross 
Abstract: Missing data occur in a variety of applications of extreme value analysis. In the block maxima approach to an extreme value analysis, missingness is often handled by either ignoring missing observations or dropping a block of observations from the analysis. However, in some cases, missingness may occur due to equipment failure during an extreme event, which can lead to bias in estimation. In this work, we propose weighted maximum likelihood and weighted moment-based estimators for the generalized extreme value distribution parameters to account for the presence of missing observations. We validate the procedures through an extensive simulation study and apply the estimation methods to data from multiple tidal gauges on the Eastern coast of Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15964v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James H. McVittie, Orla A. Murphy</dc:creator>
    </item>
    <item>
      <title>Constructing prediction intervals for the age distribution of deaths</title>
      <link>https://arxiv.org/abs/2506.17953</link>
      <description>arXiv:2506.17953v2 Announce Type: replace-cross 
Abstract: We introduce a model-agnostic procedure to construct prediction intervals for the age distribution of deaths. The age distribution of deaths is an example of constrained data, which are nonnegative and have a constrained integral. A centered log-ratio transformation and a cumulative distribution function transformation are used to remove the two constraints, where the latter transformation can also handle the presence of zero counts. Our general procedure divides data samples into training, validation, and testing sets. Within the validation set, we can select an optimal tuning parameter by calibrating the empirical coverage probabilities to be close to their nominal ones. With the selected optimal tuning parameter, we then construct the pointwise prediction intervals using the same models for the holdout data in the testing set. Using Japanese age- and sex-specific life-table death counts, we assess and evaluate the interval forecast accuracy with a suite of functional time-series models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17953v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Distributed Iterative ML and Message Passing for Grant-Free Cell-Free Massive MIMO Systems</title>
      <link>https://arxiv.org/abs/2507.21363</link>
      <description>arXiv:2507.21363v2 Announce Type: replace-cross 
Abstract: Cell-Free (CF) Massive Multiple-Input Multiple-Output (MaMIMO) is considered one of the leading candidates for enabling next-generation wireless communication. With the growing interest in the Internet of Things (IoT), the Grant-Free (GF) access scheme has emerged as a promising solution to support massive device connectivity. The integration of GF and CF-MaMIMO introduces significant challenges, particularly in designing distributed algorithms for activity detection and pilot contamination mitigation. In this paper, we propose a distributed algorithm that addresses these challenges. Our method first employs a component-wise iterative distributed Maximum Likelihood (ML) approach for activity detection, which considers both the pilot and data portions of the received signal. This is followed by a Pseudo-Prior Hybrid Variational Bayes and Expectation Propagation (PP-VB-EP) algorithm for joint data detection and channel estimation. Compared to conventional VB-EP, the proposed PP-VB-EP demonstrates improved convergence behavior and reduced sensitivity to initialization, especially when data symbols are drawn from a finite alphabet. The pseudo prior used in PP-VB-EP acts as an approximated posterior and serves as a regularization term that prevents the Message Passing (MP) algorithm from diverging. To compute the pseudo prior in a distributed fashion, we further develop a distributed version of the Variable-Level Expectation Propagation (VL-EP) algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21363v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zilu Zhao, Christian Forsch, Laura Cottatellucci, Dirk Slock</dc:creator>
    </item>
  </channel>
</rss>
