<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Nov 2025 05:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Kaplan-Meier Estimator as a Sum over Units</title>
      <link>https://arxiv.org/abs/2511.04721</link>
      <description>arXiv:2511.04721v1 Announce Type: new 
Abstract: A sum-wise formulation is proposed for the Kaplan-Meier product limit estimator of partially right-censored survival data. The derived representation permits to write the population's estimator as a sum over its individual units' semi-empirical estimators. This intuitive decomposition is applied to visualize the different contributions of failed and censored units to the overall population estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04721v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte C. Tichy</dc:creator>
    </item>
    <item>
      <title>An Integrative Approach for Subtyping Mental Disorders Using Multimodal Data</title>
      <link>https://arxiv.org/abs/2511.04816</link>
      <description>arXiv:2511.04816v1 Announce Type: new 
Abstract: Understanding the biological and behavioral heterogeneity underlying psychiatric disorders is critical for advancing precision diagnosis, treatment, and prevention. This paper addresses the scientific question of how multimodal data, spanning clinical, cognitive, and neuroimaging measures, can be integrated to identify biologically meaningful subtypes of mental disorders. We introduce Mixed INtegrative Data Subtyping (MINDS), a Bayesian hierarchical model designed to jointly analyze mixed-type data for simultaneous dimension reduction and clustering. Using data from the Adolescent Brain Cognitive Development (ABCD) Study, MINDS integrates clinical symptoms, cognitive performance, and brain structure measures to subtype Attention-Deficit/Hyperactivity Disorder (ADHD) and Obsessive-Compulsive Disorder (OCD). Our method leverages Polya-Gamma augmentation for computational efficiency and robust inference. Simulations demonstrate improved stability and accuracy compared to existing clustering approaches. Application to the ABCD data reveals clinically interpretable subtypes of ADHD and OCD with distinct cognitive and neurodevelopmental profiles. These findings show how integrative multimodal modeling can enhance the reproducibility and clinical relevance of psychiatric subtyping, supporting data-driven policies for early identification and targeted interventions in mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04816v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinjun Zhao, Yuanjia Wang, Ying LIu</dc:creator>
    </item>
    <item>
      <title>Inference for the Extended Functional Cox Model: A UK Biobank Case Study</title>
      <link>https://arxiv.org/abs/2511.04852</link>
      <description>arXiv:2511.04852v1 Announce Type: new 
Abstract: Multiple studies have shown that scalar summaries of objectively measured physical activity (PA) using accelerometers are the strongest predictors of mortality, outperforming all traditional risk factors, including age, sex, body mass index (BMI), and smoking. Here we show that diurnal patterns of PA and their day-to-day variability provide additional information about mortality. To do that, we introduce a class of extended functional Cox models and corresponding inferential tools designed to quantify the association between multiple functional and scalar predictors with time-to-event outcomes in large-scale (large $n$) high-dimensional (large $p$) datasets. Methods are applied to the UK Biobank study, which collected PA at every minute of the day for up to seven days, as well as time to mortality ($93{,}370$ participants with good quality accelerometry data and $931$ events). Simulation studies show that methods perform well in realistic scenarios and scale up to studies an order of magnitude larger than the UK Biobank accelerometry study. Establishing the feasibility and scalability of these methods for such complex and large data sets is a major milestone in applied Functional Data Analysis (FDA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04852v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erjia Cui, Angela Zhao, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Clustering in Networks with Time-varying Nodal Attributes</title>
      <link>https://arxiv.org/abs/2511.04859</link>
      <description>arXiv:2511.04859v1 Announce Type: new 
Abstract: This manuscript studies nodal clustering in graphs having a time series at each node. The framework includes priors for low-dimensional representations and a decoder that bridges the latent representations and time series. The structural and temporal patterns are fused into representations that facilitate clustering, addressing the limitation that the evolution of nodal attributes is often overlooked. Parameters are learned via maximum approximate likelihood, with a graph-fused LASSO regularization imposed on prior parameters. The optimization problem is solved via alternating direction method of multipliers; Langevin dynamics are employed for posterior inference. Simulation studies on block and grid graphs with autoregressive dynamics, and applications to California county temperatures and a book word co-occurrence network demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04859v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Oscar Hernan Madrid Padilla, Rebecca Killick, James Wilson, Xi Chen, Robert Lund</dc:creator>
    </item>
    <item>
      <title>On linkage bias-correction for estimators using iterated bootstraps</title>
      <link>https://arxiv.org/abs/2511.05004</link>
      <description>arXiv:2511.05004v1 Announce Type: new 
Abstract: By amalgamating data from disparate sources, the resulting integrated dataset becomes a valuable resource for statistical analysis. In probabilistic record linkage, the effectiveness of such integration relies on the availability of linkage variables free from errors. Where this is lacking, the linked data set would suffer from linkage errors and the resultant analyses, linkage bias. This paper proposes a methodology leveraging the bootstrap technique to devise linkage bias-corrected estimators. Additionally, it introduces a test to assess whether increasing the number of bootstrap iterations meaningfully reduces linkage bias or merely inflates variance without further improving accuracy. An application of these methodologies is demonstrated through the analysis of a simulated dataset featuring hormone information, along with a dataset obtained from linking two data sets from the Australian Bureau of Statistics' labour mobility surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05004v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siu-Ming Tam, Min Wang, Alicia Rambaldi, Dehua Tao</dc:creator>
    </item>
    <item>
      <title>Conditioning on posterior samples for flexible frequentist goodness-of-fit testing</title>
      <link>https://arxiv.org/abs/2511.05281</link>
      <description>arXiv:2511.05281v1 Announce Type: new 
Abstract: Tests of goodness of fit are used in nearly every domain where statistics is applied. One powerful and flexible approach is to sample artificial data sets that are exchangeable with the real data under the null hypothesis (but not under the alternative), as this allows the analyst to conduct a valid test using any test statistic they desire. Such sampling is typically done by conditioning on either an exact or approximate sufficient statistic, but existing methods for doing so have significant limitations, which either preclude their use or substantially reduce their power or computational tractability for many important models. In this paper, we propose to condition on samples from a Bayesian posterior distribution, which constitute a very different type of approximate sufficient statistic than those considered in prior work. Our approach, approximately co-sufficient sampling via Bayes (aCSS-B), considerably expands the scope of this flexible type of goodness-of-fit testing. We prove the approximate validity of the resulting test, and demonstrate its utility on three common null models where no existing methods apply, as well as its outperformance on models where existing methods do apply.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05281v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Bhaduri, Aabesh Bhattacharyya, Rina Foygel Barber, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Function on Scalar Regression with Complex Survey Designs</title>
      <link>https://arxiv.org/abs/2511.05487</link>
      <description>arXiv:2511.05487v1 Announce Type: new 
Abstract: Large health surveys increasingly collect high-dimensional functional data from wearable devices, and function on scalar regression (FoSR) is often used to quantify the relationship between these functional outcomes and scalar covariates such as age and sex. However, existing methods for FoSR fail to account for complex survey design. We introduce inferential methods for FoSR for studies with complex survey designs. The method combines fast univariate inference (FUI) developed for functional data outcomes and survey sampling inferential methods developed for scalar outcomes. Our approach consists of three steps: (1) fit survey weighted GLMs at each point along the functional domain, (2) smooth coefficients along the functional domain, and (3) use balanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap to obtain pointwise and joint confidence bands for the functional coefficients. The method is motivated by association studies between continuous physical activity data and covariates collected in the National Health and Nutrition Examination Survey (NHANES). A first-of-its-kind analytical simulation study and empirical simulation using the NHANES data demonstrates that our method performs better than existing methods that do not account for the survey structure. Finally, application of the method in NHANES shows the practical implications of accounting for survey structure. The method is implemented in the R package svyfosr.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05487v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lily Koffman, Sunan Gao, Xinkai Zhou, Andrew Leroux, Ciprian Crainiceanu, John Muschelli III</dc:creator>
    </item>
    <item>
      <title>Insights into Tail-Based and Order Statistics</title>
      <link>https://arxiv.org/abs/2511.04784</link>
      <description>arXiv:2511.04784v1 Announce Type: cross 
Abstract: Heavy-tailed phenomena appear across diverse domains --from wealth and firm sizes in economics to network traffic, biological systems, and physical processes-- characterized by the disproportionate influence of extreme values. These distributions challenge classical statistical models, as their tails decay too slowly for conventional approximations to hold. Among their key descriptive measures are quantile contributions, which quantify the proportion of a total quantity (such as income, energy, or risk) attributed to observations above a given quantile threshold. This paper presents a theoretical study of the quantile contribution statistic and its relationship with order statistics. We derive a closed-form expression for the joint cumulative distribution function (CDF) of order statistics and, based on it, obtain an explicit CDF for quantile contributions applicable to small samples. We then investigate the asymptotic behavior of these contributions as the sample size increases, establishing the asymptotic normality of the numerator and characterizing the limiting distribution of the quantile contribution. Finally, simulation studies illustrate the convergence properties and empirical accuracy of the theoretical results, providing a foundation for applying quantile contributions in the analysis of heavy-tailed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04784v1</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamidreza Maleki Almani</dc:creator>
    </item>
    <item>
      <title>Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning</title>
      <link>https://arxiv.org/abs/2511.05050</link>
      <description>arXiv:2511.05050v1 Announce Type: cross 
Abstract: In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relationships in real-world phenomena. Building on heteroskedasticity-based identification, the proposed method integrates a quasi-maximum likelihood estimator for simultaneous equation models with large scale online kernel learning. It employs random Fourier feature approximations to flexibly model nonlinear conditional means and variances, while an adaptive online gradient descent algorithm ensures computational efficiency for streaming and high-dimensional data. Results from extensive simulations demonstrate that the proposed method achieves superior accuracy and stability than single equation and polynomial approximation baselines, exhibiting lower bias and root mean squared error across various data-generating processes. These results confirm that the proposed approach effectively captures complex bidirectional causal effects with near-linear computational scaling. By combining econometric identification with modern machine learning techniques, the proposed framework offers a practical, scalable, and theoretically grounded solution for large scale causal inference in natural/social science, policy making, business, and industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05050v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference on Unlabeled Histograms</title>
      <link>https://arxiv.org/abs/2511.05077</link>
      <description>arXiv:2511.05077v1 Announce Type: cross 
Abstract: Statistical inference on histograms and frequency counts plays a central role in categorical data analysis. Moving beyond classical methods that directly analyze labeled frequencies, we introduce a framework that models the multiset of unlabeled histograms via a mixture distribution to better capture unseen domain elements in large-alphabet regime. We study the nonparametric maximum likelihood estimator (NPMLE) under this framework, and establish its optimal convergence rate under the Poisson setting. The NPMLE also immediately yields flexible and efficient plug-in estimators for functional estimation problems, where a localized variant further achieves the optimal sample complexity for a wide range of symmetric functionals. Extensive experiments on synthetic, real-world datasets, and large language models highlight the practical benefits of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05077v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yun Ma, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Doubly robust and computationally efficient high-dimensional variable selection</title>
      <link>https://arxiv.org/abs/2409.09512</link>
      <description>arXiv:2409.09512v2 Announce Type: replace 
Abstract: Variable selection can be performed by testing conditional independence (CI) between each predictor and the response, given the other predictors. A doubly robust and powerful option for these CI tests is the projected covariance measure (PCM) test. However, directly deploying PCM for variable selection brings computational challenges: testing a single variable involves a few machine learning fits, so testing $p$ variables requires $O(p)$ fits. Inspired by model-X ideas, we observe that an estimate of the joint predictor distribution and a single response-on-all-predictors fit can be used to reconstruct all PCM fits. This yields tower PCM (tPCM), a computationally efficient extension of PCM to variable selection. When the joint predictor distribution is sufficiently tractable, as in applications like genome-wide association studies, tPCM offers a substantial speedup over PCM -- up to 130$\times$ in our simulations -- while matching its power. tPCM also improves on model-X methods like knockoffs and holdout randomization test (HRT) by returning per-variable $p$-values and improving speed, respectively. We prove that tPCM is doubly robust and asymptotically equivalent to both PCM and HRT. We thus extend the bridge between model-X and doubly robust approaches, demonstrating their independent arrival at equivalent methods and showing that this intersection is a fruitful source of new methodologies like tPCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09512v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Jeffrey Zhang, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v3 Announce Type: replace 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, are a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent U.S. Food and Drug Administration and European Medicines Agency guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. However, when the number of baseline covariates $p$ is large, the recommendation is somewhat murky. In this paper, we show that HOIF-motivated estimators for the treatment-specific mean have significantly improved statistical properties compared to popular adjusted estimators in practice when $p$ is relatively large relative to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. More importantly, we demonstrate that several state-of-the-art adjusted estimators proposed recently can be interpreted as particular HOIF-motivated estimators, thereby placing these estimators in a more unified framework. Numerical and empirical studies are conducted to corroborate our theoretical findings. An accompanying R package can be found on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Statistical parametric simulation studies based on real data</title>
      <link>https://arxiv.org/abs/2504.04864</link>
      <description>arXiv:2504.04864v3 Announce Type: replace 
Abstract: Simulation studies are indispensable for evaluating statistical methods and ubiquitous in statistical research. The most common simulation approach is parametric simulation, where the data-generating mechanism (DGM) corresponds to a parametric model from which observations are drawn. While many simulation studies aim to give practical guidance on method suitability, parametric simulations in particular are often criticized for being unrealistic. To overcome this drawback, it is sensible to employ real data for constructing the parametric DGMs. However, although real-data-based parametric DGMs are widely used, the specific ways in which DGM components are inferred vary, and their implications may not be fully understood. Additionally, researchers often rely on a limited selection of real datasets, with the rationale for their selection being unclear. This paper reviews and formally discusses how components of parametric DGMs can be inferred from real data and how dataset selection can be performed more systematically. By doing so, we aim to support researchers in conducting simulation studies with a lower risk of overgeneralization and misinterpretation. We illustrate the construction of parametric DGMs based on a systematically selected set of real datasets using two examples: one on ordinal outcomes in randomized controlled trials and one on differential gene expression analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04864v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Sauer, F. Julian D. Lange, Maria Thurow, Ina Dormuth, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Predicting data value before collection: A coefficient for prioritizing sources under random distribution shift</title>
      <link>https://arxiv.org/abs/2504.06570</link>
      <description>arXiv:2504.06570v3 Announce Type: replace 
Abstract: Researchers often face choices between multiple data sources that differ in quality, cost, and representativeness. Which sources will most improve predictive performance? We study this data prioritization problem under a random distribution shift model, where candidate sources arise from random perturbations to a target population. We propose the Data Usefulness Coefficient (DUC), which predicts the reduction in prediction error from adding a dataset to training, using only covariate summary statistics and no outcome data. We prove that under random shifts, covariate differences between sources are informative about outcome prediction quality. Through theory and experiments on synthetic and real data, we demonstrate that DUC-based selection outperforms alternative strategies, allowing more efficient resource allocation across heterogeneous data sources. The method provides interpretable rankings between candidate datasets and works for any data modality, including ordinal, categorical, and continuous data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06570v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivy Zhang, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Inference with General Missingness Patterns and Machine Learning Imputation</title>
      <link>https://arxiv.org/abs/2508.15162</link>
      <description>arXiv:2508.15162v2 Announce Type: replace 
Abstract: Pre-trained machine learning (ML) predictions have been increasingly used to complement incomplete data to enable downstream scientific inquiries, but their naive integration risks biased inferences. Recently, multiple methods have been developed to provide valid inference with ML imputations regardless of prediction quality and to enhance efficiency relative to complete-case analyses. However, existing approaches are often limited to missing outcomes under a missing-completely-at-random (MCAR) assumption, failing to handle general missingness patterns (missing in both the outcome and exposures) under the more realistic missing-at-random (MAR) assumption. This paper develops a novel method that delivers a valid statistical inference framework for general Z-estimation problems using ML imputations under the MAR assumption and for general missingness patterns. The core technical idea is to stratify observations by distinct missingness patterns and construct an estimator by appropriately weighting and aggregating pattern-specific information through a masking-and-imputation procedure on the complete cases. We provide theoretical guarantees of asymptotic normality of the proposed estimator and efficiency dominance over weighted complete-case analyses. Practically, the method affords simple implementations by leveraging existing weighted complete-case analysis software. Extensive simulations are carried out to validate theoretical results. A real data example is provided to further illustrate the practical utility of the proposed method. The paper concludes with a brief discussion on practical implications, limitations, and potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15162v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingran Chen, Tyler McCormick, Bhramar Mukherjee, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>Correcting the Coverage Bias of Quantile Regression</title>
      <link>https://arxiv.org/abs/2511.00820</link>
      <description>arXiv:2511.00820v2 Announce Type: replace 
Abstract: We develop a collection of methods for adjusting the predictions of quantile regression to ensure coverage. Our methods are model agnostic and can be used to correct for high-dimensional overfitting bias with only minimal assumptions. Theoretical results show that the estimates we develop are consistent and facilitate accurate calibration in the proportional asymptotic regime where the ratio of the dimension of the data and the sample size converges to a constant. This is further confirmed by experiments on both simulated and real data. A key component of our work is a new connection between the leave-one-out coverage and the fitted values of variables appearing in a dual formulation of the quantile regression problem. This facilitates the use of cross-validation in a variety of settings at significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00820v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, John J. Cherian, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Classification of realisations of random sets</title>
      <link>https://arxiv.org/abs/2511.00937</link>
      <description>arXiv:2511.00937v2 Announce Type: replace 
Abstract: In this paper, the classification task for a family of sets representing the realisation of some random set models is solved. Both unsupervised and supervised classification methods are utilised using the similarity measure between two realisations derived as empirical estimates of $\mathcal N$-distances quantified based on geometric characteristics of the realisations, namely the boundary curvature and the perimeter over area ratios of obtained samples of connected components from the realisations. To justify the proposed methodology, a simulation study is performed using random set models. The methods are used further for classifying histological images of mastopathy and mammary cancer tissue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00937v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bogdan Radovi\'c, Vesna Gotovac {\DJ}oga\v{s}, Kate\v{r}ina Helisov\'a</dc:creator>
    </item>
    <item>
      <title>Asymptotically Unbiased Synthetic Control Methods by Moment Matching</title>
      <link>https://arxiv.org/abs/2307.11127</link>
      <description>arXiv:2307.11127v5 Announce Type: replace-cross 
Abstract: Synthetic Control Methods (SCMs) have become a fundamental tool for comparative case studies. The core idea behind SCMs is to estimate treatment effects by predicting counterfactual outcomes for a treated unit using a weighted combination of observed outcomes from untreated units. The accuracy of these predictions is crucial for evaluating the treatment effect of a policy intervention. Subsequent research has therefore focused on estimating SC weights. In this study, we highlight a key endogeneity issue in existing SCMs-namely, the correlation between the outcomes of untreated units and the error term of the synthetic control, which leads to bias in both counterfactual outcome prediction and treatment effect estimation. To address this issue, we propose a novel SCM based on moment matching, assuming that the outcome distribution of the treated unit can be approximated by a weighted mixture of the distributions of untreated units. Under this assumption, we estimate SC weights by matching the moments of the treated outcomes with the weighted sum of the moments of the untreated outcomes. Our method offers three advantages: first, under the mixture model assumption, our estimator is asymptotically unbiased; second, this asymptotic unbiasedness reduces the mean squared error in counterfactual predictions; and third, our method provides full distributions of the treatment effect rather than just expected values, thereby broadening the applicability of SCMs. Finally, we present experimental results that demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11127v5</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Akari Ohda</dc:creator>
    </item>
    <item>
      <title>Inference with Mondrian Random Forests</title>
      <link>https://arxiv.org/abs/2310.09702</link>
      <description>arXiv:2310.09702v4 Announce Type: replace-cross 
Abstract: Random forests are popular methods for regression and classification analysis, and many different variants have been proposed in recent years. One interesting example is the Mondrian random forest, in which the underlying constituent trees are constructed via a Mondrian process. We give precise bias and variance characterizations, along with a Berry-Esseen-type central limit theorem, for the Mondrian random forest regression estimator. By combining these results with a carefully crafted debiasing approach and an accurate variance estimator, we present valid statistical inference methods for the unknown regression function. These methods come with explicitly characterized error bounds in terms of the sample size, tree complexity parameter, and number of trees in the forest, and include coverage error rates for feasible confidence interval estimators. Our debiasing procedure for the Mondrian random forest also allows it to achieve the minimax-optimal point estimation convergence rate in mean squared error for multivariate $\beta$-H\"older regression functions, for all $\beta &gt; 0$, provided that the underlying tuning parameters are chosen appropriately. Efficient and implementable algorithms are devised for both batch and online learning settings, and we study the computational complexity of different Mondrian random forest implementations. Finally, simulations with synthetic data validate our theory and methodology, demonstrating their excellent finite-sample properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09702v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Jason M. Klusowski, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Adaptive Shrinkage Estimation</title>
      <link>https://arxiv.org/abs/2502.14166</link>
      <description>arXiv:2502.14166v3 Announce Type: replace-cross 
Abstract: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical problems, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14166v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sida Li, Nikolaos Ignatiadis</dc:creator>
    </item>
    <item>
      <title>On multipolar magnetic anomaly detection: multipolar signal subspaces, an analytical orthonormal basis, multipolar truncature and detection performance</title>
      <link>https://arxiv.org/abs/2504.05212</link>
      <description>arXiv:2504.05212v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the magnetic anomaly detection problem which aims to find hidden ferromagnetic masses by estimating the weak perturbation they induce on local Earth's magnetic field. We consider classical detection schemes that rely on signals recorded on a moving sensor, and modeling of the source as a function of unknown parameters. As the usual spherical harmonic decomposition of the anomaly has to be truncated in practice, we study the signal vector subspaces induced by each multipole of the decomposition, proving they are not in direct sum, and discussing the impact it has on the choice of the truncation order. Further, to ease the detection strategy based on generalized likelihood ratio test, we rely on orthogonal polynomials theory to derive an analytical set of orthonormal functions (multipolar orthonormal basis functions) that spans the space of the noise-free measured signal. Finally, based on the subspace structure of the multipole vector spaces, we study the impact of the truncation order on the detection performance, beyond the issue of potential surparametrization, and the behaviour of the information criteria used to choose this order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05212v2</guid>
      <category>eess.SP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cl\'ement Chenevas-Paule, Steeve Zozor, Laure-Line Rouve, Olivier J. J. Michel, Olivier Pinaud, Romain Kukla</dc:creator>
    </item>
    <item>
      <title>Less Greedy Equivalence Search</title>
      <link>https://arxiv.org/abs/2506.22331</link>
      <description>arXiv:2506.22331v2 Announce Type: replace-cross 
Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal discovery from observational data. In the sample limit, it recovers the Markov equivalence class of graphs that describe the data. Still, it faces two challenges in practice: computational cost and finite-sample accuracy. In this paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that retains its theoretical guarantees while partially addressing these limitations. LGES modifies the greedy step; rather than always applying the highest-scoring insertion, it avoids edge insertions between variables for which the score implies some conditional independence. This more targeted search yields up to a \(10\)-fold speed-up and a substantial reduction in structural error relative to GES. Moreover, LGES can guide the search using prior knowledge, and can correct this knowledge when contradicted by data. Finally, LGES can use interventional data to refine the learned observational equivalence class. We prove that LGES recovers the true equivalence class in the sample limit, even with misspecified knowledge. Experiments demonstrate that LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified knowledge. Our code is available at https://github.com/CausalAILab/lges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22331v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Forthcoming in Proceedings of the 39th Annual Conference on Neural Information Processing Systems (2025)</arxiv:journal_reference>
      <dc:creator>Adiba Ejaz, Elias Bareinboim</dc:creator>
    </item>
    <item>
      <title>Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles</title>
      <link>https://arxiv.org/abs/2507.01542</link>
      <description>arXiv:2507.01542v2 Announce Type: replace-cross 
Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning, particularly for unsupervised problems. While full GMMs suffer from the overparameterization of their covariance matrices in high-dimensional spaces, spherical GMMs (with isotropic covariance matrices) certainly lack flexibility to fit certain anisotropic distributions. Connecting these two extremes, we introduce a new family of parsimonious GMMs with piecewise-constant covariance eigenvalue profiles. These extend several low-rank models like the celebrated mixtures of probabilistic principal component analyzers (MPPCA), by enabling any possible sequence of eigenvalue multiplicities. If the latter are prespecified, then we can naturally derive an expectation-maximization (EM) algorithm to learn the mixture parameters. Otherwise, to address the notoriously-challenging issue of jointly learning the mixture parameters and hyperparameters, we propose a componentwise penalized EM algorithm, whose monotonicity is proven. We show the superior likelihood-parsimony tradeoffs achieved by our models on a variety of unsupervised experiments: density fitting, clustering and single-image denoising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01542v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Pierre-Alexandre Mattei, Charles Bouveyron, Xavier Pennec</dc:creator>
    </item>
    <item>
      <title>It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation</title>
      <link>https://arxiv.org/abs/2507.02275</link>
      <description>arXiv:2507.02275v3 Announce Type: replace-cross 
Abstract: Structure-agnostic causal inference studies how well one can estimate a treatment effect given black-box machine learning estimates of nuisance functions (like the impact of confounders on treatment and outcomes). Here, we find that the answer depends in a surprising way on the distribution of the treatment noise. Focusing on the partially linear model of \citet{robinson1988root}, we first show that the widely adopted double machine learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise, resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for independent non-Gaussian treatment noise, we show that DML is always suboptimal by constructing new practical procedures with higher-order robustness to nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant estimators to achieve $r$-th order insensitivity to nuisance errors whenever the $(r+1)$-st treatment cumulant is non-zero. We complement these core results with novel minimax guarantees for binary treatments in the partially linear model. Finally, using synthetic demand estimation experiments, we demonstrate the practical benefits of our higher-order robust estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02275v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Lester Mackey, Vasilis Syrgkanis</dc:creator>
    </item>
  </channel>
</rss>
