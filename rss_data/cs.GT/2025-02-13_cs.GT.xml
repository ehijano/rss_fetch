<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 02:47:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sink equilibria and the attractors of learning in games</title>
      <link>https://arxiv.org/abs/2502.07975</link>
      <description>arXiv:2502.07975v1 Announce Type: new 
Abstract: Characterizing the limit behavior -- that is, the attractors -- of learning dynamics is one of the most fundamental open questions in game theory. In recent work in this front, it was conjectured that the attractors of the replicator dynamic are in one-to-one correspondence with the sink equilibria of the game -- the sink strongly connected components of a game's preference graph -- , and it was established that they do stand in at least one-to-many correspondence with them. We make threefold progress on the problem of characterizing attractors. First, we show through a topological construction that the one-to-one conjecture is false. Second, we make progress on the attractor characterization problem for two-player games by establishing that the one-to-one conjecture is true in the absence of a local pattern called a weak local source -- a pattern that is absent from zero-sum games. Finally, we look -- for the first time in this context -- at fictitious play, the longest-studied learning dynamic, and examine to what extent the conjecture generalizes there. We establish that under fictitious play, sink equilibria always contain attractors (sometimes strictly), and every attractor corresponds to a strongly connected set of nodes in the preference graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07975v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Biggar, Christos Papadimitriou</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Performative Prediction Beyond the Insensitivity Assumption: A Case Study for Mortgage Competition</title>
      <link>https://arxiv.org/abs/2502.08063</link>
      <description>arXiv:2502.08063v1 Announce Type: new 
Abstract: Performative prediction models account for feedback loops in decision-making processes where predictions influence future data distributions. While existing work largely assumes insensitivity of data distributions to small strategy changes, this assumption usually fails in real-world competitive (i.e. multi-agent) settings. For example, in Bertrand-type competitions, a small reduction in one firm's price can lead that firm to capture the entire demand, while all others sharply lose all of their customers.
  We study a representative setting of multi-agent performative prediction in which insensitivity assumptions do not hold, and investigate the convergence of natural dynamics. To do so, we focus on a specific game that we call the ''Bank Game'', where two lenders compete over interest rates and credit score thresholds. Consumers act similarly as to in a Bertrand Competition, with each consumer selecting the firm with the lowest interest rate that they are eligible for based on the firms' credit thresholds. Our analysis characterizes the equilibria of this game and demonstrates that when both firms use a common and natural no-regret learning dynamic -- exponential weights -- with proper initialization, the dynamics always converge to stable outcomes despite the general-sum structure. Notably, our setting admits multiple stable equilibria, with convergence dependent on initial conditions. We also provide theoretical convergence results in the stochastic case when the utility matrix is not fully known, but each learner can observe sufficiently many samples of consumers at each time step to estimate it, showing robustness to slight mis-specifications. Finally, we provide experimental results that validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08063v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanghui Wang, Krishna Acharya, Lokranjan Lakshmikanthan, Vidya Muthukumar, Juba Ziani</dc:creator>
    </item>
    <item>
      <title>Mechanism Design in Max-Flows</title>
      <link>https://arxiv.org/abs/2502.08248</link>
      <description>arXiv:2502.08248v1 Announce Type: new 
Abstract: This paper studies allocation mechanisms in max-flow games with players' capacities as private information. We first show that no core-selection mechanism is truthful: there may exist a player whose payoff increases if she under-reports her capacity when a core-section mechanism is adopted. We then introduce five desirable properties for mechanisms in max-flow games: DSIC (truthful reporting is a dominant strategy), SIR (individual rationality and positive payoff for each player contributing positively to at least one coalition), SP (no edge has an incentive to split into parallel edges), MP (no parallel edges have incentives to merge), and CM (a player's payoff does not decrease as another player's capacity and max-flow increase). While the Shapley value mechanism satisfies DSIC and SIR, it fails to meet SP, MP and CM. We propose a new mechanism based on minimal cuts that satisfies all five properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08248v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyuan Huang, Wenjun Mei, Xiaoguang Yang, Zhigang Cao</dc:creator>
    </item>
    <item>
      <title>Data Pricing for Graph Neural Networks without Pre-purchased Inspection</title>
      <link>https://arxiv.org/abs/2502.08284</link>
      <description>arXiv:2502.08284v1 Announce Type: new 
Abstract: Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to $40\%$ in both MacroF1 and MicroF1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08284v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiping Liu, Mengxiao Zhang, Jiamou Liu, Song Yang</dc:creator>
    </item>
    <item>
      <title>Non-Monetary Mechanism Design without Distributional Information: Using Scarce Audits Wisely</title>
      <link>https://arxiv.org/abs/2502.08412</link>
      <description>arXiv:2502.08412v1 Announce Type: new 
Abstract: We study a repeated resource allocation problem with strategic agents where monetary transfers are disallowed and the central planner has no prior information on agents' utility distributions. In light of Arrow's impossibility theorem, acquiring information about agent preferences through some form of feedback is necessary. We assume that the central planner can request powerful but expensive audits on the winner in any round, revealing the true utility of the winner in that round. We design a mechanism achieving $T$-independent $O(K^2)$ regret in social welfare while requesting $O(K^3 \log T)$ audits in expectation, where $K$ is the number of agents and $T$ is the number of rounds. We also show an $\Omega(K)$ lower bound on the regret and an $\Omega(1)$ lower bound on the number of audits when having low regret. Algorithmically, we show that incentive-compatibility can be mostly enforced with an accurate estimation of the winning probability of each agent under truthful reporting. To do so, we impose future punishments and introduce a *flagging* component, allowing agents to flag any biased estimate (we show that doing so aligns with individual incentives). On the technical side, without monetary transfers and distributional information, the central planner cannot ensure that truthful reporting is exactly an equilibrium. Instead, we characterize the equilibrium via a reduction to a simpler *auxiliary game*, in which agents cannot strategize until late in the $T$ rounds of the allocation problem. The tools developed therein may be of independent interest for other mechanism design problems in which the revelation principle cannot be readily applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08412v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Dai, Moise Blanchard, Patrick Jaillet</dc:creator>
    </item>
    <item>
      <title>The Complexity of Symmetric Equilibria in Min-Max Optimization and Team Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2502.08519</link>
      <description>arXiv:2502.08519v1 Announce Type: new 
Abstract: We consider the problem of computing stationary points in min-max optimization, with a particular focus on the special case of computing Nash equilibria in (two-)team zero-sum games.
  We first show that computing $\epsilon$-Nash equilibria in $3$-player \emph{adversarial} team games -- wherein a team of $2$ players competes against a \emph{single} adversary -- is \textsf{CLS}-complete, resolving the complexity of Nash equilibria in such settings. Our proof proceeds by reducing from \emph{symmetric} $\epsilon$-Nash equilibria in \emph{symmetric}, identical-payoff, two-player games, by suitably leveraging the adversarial player so as to enforce symmetry -- without disturbing the structure of the game. In particular, the class of instances we construct comprises solely polymatrix games, thereby also settling a question left open by Hollender, Maystre, and Nagarajan (2024). We also provide some further results concerning equilibrium computation in adversarial team games.
  Moreover, we establish that computing \emph{symmetric} (first-order) equilibria in \emph{symmetric} min-max optimization is \textsf{PPAD}-complete, even for quadratic functions. Building on this reduction, we further show that computing symmetric $\epsilon$-Nash equilibria in symmetric, $6$-player ($3$ vs. $3$) team zero-sum games is also \textsf{PPAD}-complete, even for $\epsilon = \text{poly}(1/n)$. As an immediate corollary, this precludes the existence of symmetric dynamics -- which includes many of the algorithms considered in the literature -- converging to stationary points. Finally, we prove that computing a \emph{non-symmetric} $\text{poly}(1/n)$-equilibrium in symmetric min-max optimization is \textsf{FNP}-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08519v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioannis Anagnostides, Ioannis Panageas, Tuomas Sandholm, Jingming Yan</dc:creator>
    </item>
    <item>
      <title>Approximation guarantees of Median Mechanism in $\mathbb{R}^d$</title>
      <link>https://arxiv.org/abs/2502.08578</link>
      <description>arXiv:2502.08578v2 Announce Type: new 
Abstract: The coordinate-wise median is a classic and most well-studied strategy-proof mechanism in social choice and facility location scenarios. Surprisingly, there is no systematic study of its approximation ratio in $d$-dimensional spaces. The best known approximation guarantee in $d$-dimensional Euclidean space $\mathbb{L}_2(\mathbb{R}^d)$ is $\sqrt{d}$ via embedding $\mathbb{L}_1(\mathbb{R}^d)$ into $\mathbb{L}_2(\mathbb{R}^d)$ metric space, that only appeared in appendix of [Meir 2019].This upper bound is known to be tight in dimension $d=2$, but there are no known super constant lower bounds. Still, it seems that the community's belief about coordinate-wise median is on the side of $\Theta(\sqrt{d})$. E.g., a few recent papers on mechanism design with predictions [Agrawal, Balkanski, Gkatzelis, Ou, Tan 2022], [Christodoulou, Sgouritsa, Vlachos 2024], and [Barak, Gupta, Talgam-Cohen 2024] directly rely on the $\sqrt{d}$-approximation result.
  In this paper, we systematically study approximate efficiency of the coordinate-median in $\mathbb{L}_{q}(\mathbb{R}^d)$ spaces for any $\mathbb{L}_q$ norm with $q\in[1,\infty]$ and any dimension $d$. We derive a series of constant upper bounds $UB(q)$ independent of the dimension $d$. This series $UB(q)$ is growing with parameter $q$, but never exceeds the constant $UB(\infty)= 3$. Our bound $UB(2)=\sqrt{6\sqrt{3}-8}&lt;1.55$ for $\mathbb{L}_2$ norm is only slightly worse than the tight approximation guarantee of $\sqrt{2}&gt;1.41$ in dimension $d=2$. Furthermore, we show that our upper bounds are essentially tight by giving almost matching lower bounds $LB(q,d)=UB(q)\cdot(1-O(1/d))$ for any dimension $d$ with $LB(q,d)=UB(q)$ when $d\to\infty$. We also extend our analysis to the generalized median mechanism in [Agrawal, Balkanski, Gkatzelis, Ou, Tan 2022] for $\mathbb{L}_2(\mathbb{R}^2)$ space to arbitrary dimensions $d$ with similar results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08578v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Gravin, Jianhao Jia</dc:creator>
    </item>
    <item>
      <title>Learning in Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs. No-Regret Learners</title>
      <link>https://arxiv.org/abs/2502.08597</link>
      <description>arXiv:2502.08597v1 Announce Type: new 
Abstract: We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our agents aim to maximize the expected growth rate of their wealth but have different theories on how to learn this best. We focus on comparing Bayesian and no-regret learners in market dynamics. Bayesian learners with a prior over a finite set of models that assign positive prior probability to the correct model have posterior probabilities that converge exponentially to the correct model. Consequently, they survive even in the presence of agents who invest according to the correct model of the stochastic process. Bayesians with a continuum prior converge to the correct model at a rate of $O((\log T)/T)$. Online learning theory provides no-regret algorithms for maximizing the log of wealth in this setting, achieving a worst-case regret bound of $O(\log T)$ without assuming a steady underlying stochastic process but comparing to the best fixed investment rule. This regret, as we observe, is of the same order of magnitude as that of a Bayesian learner with a continuum prior. However, we show that even such low regret may not be sufficient for survival in asset markets: an agent can have regret as low as $O(\log T)$, but still vanish in market dynamics when competing against agents who invest according to the correct model or even against a perfect Bayesian with a finite prior. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Any no-regret learner will drive out of the market an imperfect Bayesian whose finite prior or update rule has even small errors. We formally establish the relationship between notions of survival, vanishing, and market domination studied in economics and the framework of regret minimization, thus bridging these theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08597v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Easley, Yoav Kolumbus, Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Centralization vs Decentralization in Hiring and Admissions</title>
      <link>https://arxiv.org/abs/2502.07792</link>
      <description>arXiv:2502.07792v1 Announce Type: cross 
Abstract: There is a range of ways to organize hiring and admissions in higher education, as in many domains, ranging from very centralized processes where a single person makes final decisions to very decentralized processes where many people make decisions about who to admit or hire. Decentralized processes can enable individual and collective empowerment, but this may come at the cost of efficiency. With the advent of automated decision making, this question of centralization has a big impact on hiring and admissions, given that automated systems often are easier to implement, or even require, more centralized decision making.
  In this paper, we develop a strategic model to explore the impact of the degree of centralization on both the candidates and the hirers, with a focus on university admissions. The model reflects a trade-off between a centralized committee where preferences may not capture individual hirers' preferences, and a decentralized process where individual hirers face extra costs to interview candidates themselves. We characterize when individual hirers prefer the decentralized process over the centralized process as a function of the degree to which the centralized process and hirers' preferences are aligned. We also show that decentralization can have devastating consequences for fairness, leading to major disparities in the likelihood of getting hired across candidates. Our results demonstrate the trade-offs that occur under the question of centralization vs decentralization, and point to how an answer to this question can impose significant harm to people in these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07792v1</guid>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Fish, Diptangshu Sen, Juba Ziani</dc:creator>
    </item>
    <item>
      <title>Optimal Pricing of Cloud Services: Committed Spend under Demand Uncertainty</title>
      <link>https://arxiv.org/abs/2502.08022</link>
      <description>arXiv:2502.08022v1 Announce Type: cross 
Abstract: We consider a seller who offers services to a buyer with multi-unit demand. Prior to the realization of demand, the buyer receives a noisy signal of their future demand, and the seller can design contracts based on the reported value of this signal. Thus, the buyer can contract with the service provider for an unknown level of future consumption, such as in the market for cloud computing resources or software services. We characterize the optimal dynamic contract, extending the classic sequential screening framework to a nonlinear and multi-unit setting. The optimal mechanism gives discounts to buyers who report higher signals, but in exchange they must provide larger fixed payments. We then describe how the optimal mechanism can be implemented by two common forms of contracts observed in practice, the two-part tariff and the committed spend contract. Finally, we use extensions of our base model to shed light on policy-focused questions, such as analyzing how the optimal contract changes when the buyer faces commitment costs, or when there are liquid spot markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08022v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dirk Bergemann, Michael C. Wang</dc:creator>
    </item>
    <item>
      <title>Existence of EFX for Two Additive Valuations</title>
      <link>https://arxiv.org/abs/2008.08798</link>
      <description>arXiv:2008.08798v4 Announce Type: replace 
Abstract: Fair division of indivisible items is a well-studied topic in Economics and Computer Science. The objective is to allocate items to agents in a fair manner, where each agent has a valuation for each subset of items. Envy-freeness is one of the most widely studied notions of fairness. Since complete envy-free allocations do not always exist when items are indivisible, several relaxations have been considered. Among them, possibly the most compelling one is envy-freeness up to any item (EFX), where no agent envies another agent after the removal of any single item from the other agent's bundle. However, despite significant efforts by many researchers for several years, it is known that a complete EFX allocation always exists only in limited cases. In this paper, we show that a complete EFX allocation always exists when each agent is of one of two given types, where agents of the same type have identical additive valuations. This is the first such existence result for non-identical valuations when there are any number of agents and items and no limit on the number of distinct values an agent can have for individual items. We give a constructive proof, in which we iteratively obtain a Pareto dominating (partial) EFX allocation from an existing partial EFX allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.08798v4</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoga Mahara</dc:creator>
    </item>
    <item>
      <title>Fast value iteration: A uniform approach to efficient algorithms for energy games</title>
      <link>https://arxiv.org/abs/2110.07346</link>
      <description>arXiv:2110.07346v3 Announce Type: replace 
Abstract: We study algorithms for solving parity, mean-payoff and energy games. We propose a systematic framework, which we call Fast value iteration, for describing, comparing, and proving correctness of such algorithms. The approach is based on potential reductions, as introduced by Gurvich, Karzanov and Khachiyan (1988). This framework allows us to provide simple presentations and correctness proofs of known algorithms, unifying the Optimal strategy improvement algorithm by Schewe (2008) and the quasi dominions approach by Benerecetti et al. (2020), amongst others. The new approach also leads to novel symmetric versions of these algorithms, highly efficient in practice, but for which we are unable to prove termination. We report on empirical evaluation, comparing the different fast value iteration algorithms, and showing that they are competitive even to top parity game solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.07346v3</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha\"el Cadilhac, Antonio Casares, Pierre Ohlmann</dc:creator>
    </item>
    <item>
      <title>Bayesian Conversations</title>
      <link>https://arxiv.org/abs/2307.08827</link>
      <description>arXiv:2307.08827v3 Announce Type: replace 
Abstract: We initiate the study of Bayesian conversations, which model interactive communication between two strategic agents without a mediator. We compare this to communication through a mediator and investigate the settings in which mediation can expand the range of implementable outcomes.
  We look into the eventual outcome of two-player games after interactive communication. We focus on games where only one agent has a non-trivial action and examine the performance of communication protocols that are individually rational (IR) for both parties. Our key findings reveal that for ex-ante IR the expected social welfare achievable through a mediator protocol are equivalent to that achievable through unmediated Bayesian conversations. For ex-post IR, we observe a gap in the achievable welfare of the two protocols. We also establish that the optimal welfare under ex-post IR Bayesian conversation may require infinitely many rounds of communication. Additionally, we provide characterizations of which distributions over posteriors are achievable via Bayesian conversations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08827v3</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renato Paes Leme, Jon Schneider, Heyang Shang, Shuran Zheng</dc:creator>
    </item>
    <item>
      <title>Algorithmic Persuasion Through Simulation</title>
      <link>https://arxiv.org/abs/2311.18138</link>
      <description>arXiv:2311.18138v5 Announce Type: replace 
Abstract: We study a Bayesian persuasion game where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (real-valued) state of the world, such as the quality of the product, but only has limited information about the receiver's beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver's behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender's optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender's expected utility in this game. We also consider approximate oracles, more general query structures, and costly queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18138v5</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>econ.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keegan Harris, Nicole Immorlica, Brendan Lucier, Aleksandrs Slivkins</dc:creator>
    </item>
    <item>
      <title>Single-token vs Two-token Blockchain Tokenomics</title>
      <link>https://arxiv.org/abs/2403.15429</link>
      <description>arXiv:2403.15429v3 Announce Type: replace 
Abstract: We study long-term equilibria that arise in the token monetary policy, or tokenomics, design of proof-of-stake (PoS) blockchain systems that engage utility maximizing users and validators. Validators are system maintainers who get rewarded with tokens for performing the work necessary for the system to function properly, while users compete and pay with such tokens for getting a desired portion of the system service.
  We study how the system service provision and suitable rewards schemes together can lead to equilibria with the following desirable characteristics (1) viability: the system keeps parties engaged,
  (2) decentralization and skin-in-the-game: multiple sufficiently invested validators are participating,
  (3) stability: the price path of the underlying token used to transact with the system does not change widely over time, and
  (4) feasibility: the mechanism is easy to implement as a smart contract, e.g., it does not require a fiat reserve on-chain to perform token {\em buybacks} or to perform bookkeeping of exponentially growing token holdings.
  Our analysis enables us to put forward a novel generic mechanism for blockchain monetary policy that we call quantitative rewarding (QR). We investigate how to implement QR in single-token and two-token proof of stake (PoS) blockchain systems. The latter are systems that utilize one token for the users to pay the transaction fees and a different token for the validators to participate in the PoS protocol and get rewarded. Our approach demonstrates a concrete advantage of the two-token setting in terms of the ability of the QR mechanism to be realized effectively and provide good equilibria. Our analysis also reveals an inherent limitation of the single token setting in terms of implementing an effective blockchain monetary policy - a distinction that is, to the best of our knowledge, highlighted for the first time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15429v3</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aggelos Kiayias, Philip Lazos, Paolo Penna</dc:creator>
    </item>
    <item>
      <title>The Metric Distortion of Randomized Social Choice Functions: C1 Maximal Lottery Rules and Simulations</title>
      <link>https://arxiv.org/abs/2403.18340</link>
      <description>arXiv:2403.18340v2 Announce Type: replace 
Abstract: The metric distortion of a randomized social choice function (RSCF) quantifies its worst-case approximation ratio to the optimal social cost when the voters' costs for alternatives are given by distances in a metric space. This notion has recently attracted significant attention as numerous RSCFs that aim to minimize the metric distortion have been suggested. Since such tailored voting rules have, however, little normative appeal other than their low metric distortion, we will study the metric distortion of well-established RSCFs. Specifically, we first show that C1 maximal lottery rules, a well-known class of RSCFs, have a metric distortion of $4$, which is optimal within the class of majoritarian RSCFs. Secondly, we conduct extensive computer experiments on the metric distortion of RSCFs to obtain insights into their average-case performance. These computer experiments are based on a new linear program for computing the metric distortion of a lottery and reveal that the average-case metric distortion of some classical RSCFs is often only slightly worse than that of RSCFs tailored to minimize the metric distortion. Finally, we also analytically study the expected metric distortion of RSCFs for the impartial culture distribution. Specifically, we show that, under this distribution, every reasonable RSCF has an expected metric distortion close to $2$ when the number of voters is large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18340v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Frank, Patrick Lederer</dc:creator>
    </item>
    <item>
      <title>Truthful Aggregation of LLMs with an Application to Online Advertising</title>
      <link>https://arxiv.org/abs/2405.05905</link>
      <description>arXiv:2405.05905v5 Announce Type: replace 
Abstract: The next frontier of online advertising is revenue generation from LLM-generated content. We consider a setting where advertisers aim to influence the responses of an LLM to align with their interests, while platforms seek to maximize advertiser value and ensure user satisfaction. The challenge is that advertisers' preferences generally conflict with those of the user, and advertisers may misreport their preferences. To address this, we introduce MOSAIC, an auction mechanism that ensures that truthful reporting is a dominant strategy for advertisers and that aligns the utility of each advertiser with their contribution to social welfare. Importantly, the mechanism operates without LLM fine-tuning or access to model weights and provably converges to the output of the optimally fine-tuned LLM as computational resources increase. Additionally, it can incorporate contextual information about advertisers, which significantly improves social welfare. Through experiments with a publicly available LLM, we show that MOSAIC leads to high advertiser value and platform revenue with low computational overhead. While our motivating application is online advertising, our mechanism can be applied in any setting with monetary transfers, making it a general-purpose solution for truthfully aggregating the preferences of self-interested agents over LLM-generated replies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05905v5</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ermis Soumalias, Michael J. Curry, Sven Seuken</dc:creator>
    </item>
    <item>
      <title>Quantifying the Value of Revert Protection</title>
      <link>https://arxiv.org/abs/2410.19106</link>
      <description>arXiv:2410.19106v2 Announce Type: replace 
Abstract: Revert protection is a feature provided by some blockchain platforms that prevents users from incurring fees for failed transactions. We study the economic implications and benefits of revert protection in the context of priority gas auctions and maximal extractable value. We develop a model in which searchers bid for a top-of-block arbitrage opportunity under varying degrees of revert protection. This model applies to a broad range of settings, including bundle auctions on L1s and priority ordering sequencing rules on L2s. We quantify, in closed form, how revert protection improves equilibrium auction revenue, market efficiency, and blockspace efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19106v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Z. Zhu, Xin Wan, Ciamac C. Moallemi, Dan Robinson, Brad Bachu</dc:creator>
    </item>
    <item>
      <title>am-AMM: An Auction-Managed Automated Market Maker</title>
      <link>https://arxiv.org/abs/2403.03367</link>
      <description>arXiv:2403.03367v4 Announce Type: replace-cross 
Abstract: Automated market makers (AMMs) have emerged as the dominant market mechanism for trading on decentralized exchanges implemented on blockchains. This paper presents a single mechanism that targets two important unsolved problems for AMMs: reducing losses to informed orderflow, and maximizing revenue from uninformed orderflow. The ``auction-managed AMM'' works by running a censorship-resistant onchain auction for the right to temporarily act as ``pool manager'' for a constant-product AMM. The pool manager sets the swap fee rate on the pool, and also receives the accrued fees from swaps. The pool manager can exclusively capture some arbitrage by trading against the pool in response to small price movements, and also can set swap fees incorporating price sensitivity of retail orderflow and adapting to changing market conditions, with the benefits from both ultimately accruing to liquidity providers. Liquidity providers can enter and exit the pool freely in response to changing rent, though they must pay a small fee on withdrawal. We prove that under certain assumptions, this AMM should have higher liquidity in equilibrium than any standard, fixed-fee AMM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03367v4</guid>
      <category>q-fin.TR</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Austin Adams, Ciamac C. Moallemi, Sara Reynolds, Dan Robinson</dc:creator>
    </item>
    <item>
      <title>Proper Dataset Valuation by Pointwise Mutual Information</title>
      <link>https://arxiv.org/abs/2405.18253</link>
      <description>arXiv:2405.18253v2 Announce Type: replace-cross 
Abstract: Data plays a central role in the development of modern artificial intelligence, with high-quality data emerging as a key driver of model performance. This has prompted the development of various data curation methods in recent years. However, measuring the effectiveness of these data curation techniques remains a major challenge. Traditional evaluation methods, which assess a trained model's performance on specific benchmarks, risk promoting practices that merely make the data more similar to the test data. This issue exemplifies Goodhart's law: when a measure becomes a target, it ceases to be a good measure. To address this, we propose an information-theoretic framework for evaluating data curation methods, where dataset quality is measured by its informativeness about the true model parameters using the Blackwell ordering. We compare informativeness by the Shannon mutual information of the evaluated data and the test data, and we propose a novel method for estimating the mutual information of datasets by training Bayesian models on embedded data and computing the mutual information from the model's parameter posteriors. Experiments on real-world data demonstrate that our mutual information-based evaluation assigns appropriately lower scores to data curation strategies that reduce dataset informativeness, while traditional test score-based evaluation methods may favor data curation strategies that overfit to the test set but compromise the training data's informativeness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18253v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuran Zheng, Xuan Qi, Rui Ray Chen, Yongchan Kwon, James Zou</dc:creator>
    </item>
    <item>
      <title>Explaining a probabilistic prediction on the simplex with Shapley compositions</title>
      <link>https://arxiv.org/abs/2408.01382</link>
      <description>arXiv:2408.01382v2 Announce Type: replace-cross 
Abstract: Originating in game theory, Shapley values are widely used for explaining a machine learning model's prediction by quantifying the contribution of each feature's value to the prediction. This requires a scalar prediction as in binary classification, whereas a multiclass probabilistic prediction is a discrete probability distribution, living on a multidimensional simplex. In such a multiclass setting the Shapley values are typically computed separately on each class in a one-vs-rest manner, ignoring the compositional nature of the output distribution. In this paper, we introduce Shapley compositions as a well-founded way to properly explain a multiclass probabilistic prediction, using the Aitchison geometry from compositional data analysis. We prove that the Shapley composition is the unique quantity satisfying linearity, symmetry and efficiency on the Aitchison simplex, extending the corresponding axiomatic properties of the standard Shapley value. We demonstrate this proper multiclass treatment in a range of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01382v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3233/FAIA240605</arxiv:DOI>
      <dc:creator>Paul-Gauthier No\'e, Miquel Perell\'o-Nieto, Jean-Fran\c{c}ois Bonastre, Peter Flach</dc:creator>
    </item>
    <item>
      <title>The effect of competition in contests: A unifying approach</title>
      <link>https://arxiv.org/abs/2410.04970</link>
      <description>arXiv:2410.04970v3 Announce Type: replace-cross 
Abstract: We study how increasing competition, by making prizes more unequal, affects effort in contests. In a finite type-space environment, we characterize the equilibrium, analyze the effect of competition under linear costs, and identify conditions under which these effects persist under general costs. Our findings reveal that competition may encourage or deter effort, depending on the relative likelihood of efficient versus inefficient types. We derive implications for the classical budget allocation problem and establish that the most competitive winner-takes-all contest is robustly optimal under linear and concave costs, thereby resolving an open question. Methodologically, our analysis of the finite type-space domain -- which includes complete information as a special case and can approximate any continuum type-space -- provides a unifying approach that sheds light on the contrasting results in these extensively studied environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04970v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrzej Baranski, Sumit Goel</dc:creator>
    </item>
  </channel>
</rss>
