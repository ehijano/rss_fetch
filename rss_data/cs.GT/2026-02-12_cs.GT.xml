<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 05:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Complexity of Strategic Behavior in Primary Elections</title>
      <link>https://arxiv.org/abs/2602.10290</link>
      <description>arXiv:2602.10290v1 Announce Type: new 
Abstract: We study the computational complexity of strategic behaviour in primary elections. Unlike direct voting systems, primaries introduce a multi-stage process in which voters first influence intra-party nominees before a general election determines the final winner. While previous work has evaluated primaries via welfare distortion, we instead examine their game-theoretic properties. We formalise a model of primaries under first-past-the-post with fixed tie-breaking and analyse voters' strategic behaviour. We show that determining whether a pure Nash equilibrium exists is $\Sigma_2^{\mathbf P}$-complete, computing a best response is NP-complete, and deciding the existence of subgame-perfect equilibria in sequential primaries is PSPACE-complete. These results reveal that primaries fundamentally increase the computational difficulty of strategic reasoning, situating them as a rich source of complexity-theoretic challenges within computational social choice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10290v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colin Cleveland, Bart de Keijzer, Maria Polukarov</dc:creator>
    </item>
    <item>
      <title>Informal and Privatized Transit: Incentives, Efficiency and Coordination</title>
      <link>https://arxiv.org/abs/2602.10456</link>
      <description>arXiv:2602.10456v1 Announce Type: new 
Abstract: Informal and privatized transit services, such as minibuses and shared auto-rickshaws, are integral to daily travel in large urban metropolises, providing affordable commutes where a formal public transport system is inadequate and other options are unaffordable. Despite the crucial role that these services play in meeting mobility needs, governments often do not account for these services or their underlying incentives when planning transit systems, which can significantly compromise system efficiency.
  Against this backdrop, we develop a framework to analyze the incentives underlying informal and privatized transit systems, while proposing mechanisms to guide public transit operation and incentive design when a substantial share of mobility is provided by such profit-driven private operators. We introduce a novel, analytically tractable game-theoretic model of a fully privatized informal transit system with a fixed menu of routes, in which profit-maximizing informal operators (drivers) decide where to provide service and cost-minimizing commuters (riders) decide whether to use these services. Within this framework, we establish tight price of anarchy bounds which demonstrate that decentralized, profit-maximizing driver behavior can lead to bounded yet substantial losses in cumulative driver profit and rider demand served. We further show that these performance losses can be mitigated through targeted interventions, including Stackelberg routing mechanisms in which a modest share of drivers are centrally controlled, reflecting environments where informal operators coexist with public transit, and cross-subsidization schemes that use route-specific tolls or subsidies to incentivize drivers to operate on particular routes. Finally, we reinforce these findings through numerical experiments based on a real-world informal transit system in Nalasopara, India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10456v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devansh Jalota, Matthew Tsao</dc:creator>
    </item>
    <item>
      <title>Online Generalized-mean Welfare Maximization: Achieving Near-Optimal Regret from Samples</title>
      <link>https://arxiv.org/abs/2602.10469</link>
      <description>arXiv:2602.10469v1 Announce Type: new 
Abstract: We study online fair allocation of $T$ sequentially arriving items among $n$ agents with heterogeneous preferences, with the objective of maximizing generalized-mean welfare, defined as the $p$-mean of agents' time-averaged utilities, with $p\in (-\infty, 1)$. We first consider the i.i.d. arrival model and show that the pure greedy algorithm -- which myopically chooses the welfare-maximizing integral allocation -- achieves $\widetilde{O}(1/T)$ average regret. Importantly, in contrast to prior work, our algorithm does not require distributional knowledge and achieves the optimal regret rate using only the online samples.
  We then go beyond i.i.d. arrivals and investigate a nonstationary model with time-varying independent distributions. In the absence of additional data about the distributions, it is known that every online algorithm must suffer $\Omega(1)$ average regret. We show that only a single historical sample from each distribution is sufficient to recover the optimal $\widetilde{O}(1/T)$ average regret rate, even in the face of arbitrary non-stationarity. Our algorithms are based on the re-solving paradigm: they assume that the remaining items will be the ones seen historically in those periods and solve the resulting welfare-maximization problem to determine the decision in every period. Finally, we also account for distribution shifts that may distort the fidelity of historical samples and show that the performance of our re-solving algorithms is robust to such shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10469v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongjun Yang, Rachitesh Kumar, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Pricing Query Complexity of Multiplicative Revenue Approximation</title>
      <link>https://arxiv.org/abs/2602.10483</link>
      <description>arXiv:2602.10483v1 Announce Type: new 
Abstract: We study the pricing query complexity of revenue maximization for a single buyer whose private valuation is drawn from an unknown distribution. In this setting, the seller must learn the optimal monopoly price by posting prices and observing only binary purchase decisions, rather than the realized valuations. Prior work has established tight query complexity bounds for learning a near-optimal price with additive error $\varepsilon$ when the valuation distribution is supported on $[0,1]$. However, our understanding of how to learn a near-optimal price that achieves at least a $(1-\varepsilon)$ fraction of the optimal revenue remains limited.
  In this paper, we study the pricing query complexity of the single-buyer revenue maximization problem under such multiplicative error guarantees in several settings. Observe that when pricing queries are the only source of information about the buyer's distribution, no algorithm can achieve a non-trivial approximation, since the scale of the distribution cannot be learned from pricing queries alone. Motivated by this fundamental impossibility, we consider two natural and well-motivated models that provide "scale hints": (i) a one-sample hint, in which the algorithm observes a single realized valuation before making pricing queries; and (ii) a value-range hint, in which the valuation support is known to lie within $[1, H]$. For each type of hint, we establish pricing query complexity guarantees that are tight up to polylogarithmic factors for several classes of distributions, including monotone hazard rate (MHR) distributions, regular distributions, and general distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10483v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Tang, Yifan Wang, Mengxiao Zhang</dc:creator>
    </item>
    <item>
      <title>Characterization and Computation of Normal-Form Proper Equilibria in Extensive-Form Games via the Sequence-Form Representation</title>
      <link>https://arxiv.org/abs/2602.10524</link>
      <description>arXiv:2602.10524v1 Announce Type: new 
Abstract: Normal-form proper equilibrium, introduced by Myerson as a refinement of normal-form perfect equilibrium, occupies a distinctive position in the equilibrium analysis of extensive-form games because its more stringent perturbation structure entails the sequential rationality. However, the size of the normal-form representation grows exponentially with the number of parallel information sets, making the direct determination of normal-form proper equilibria intractable. To address this challenge, we develop a compact sequence-form proper equilibrium by redefining the expected payoffs over sequences, and we prove that it coincides with the normal-form proper equilibrium via strategic equivalence. To facilitate computation, we further introduce an alternative representation by defining a class of perturbed games based on an $\varepsilon$-permutahedron over sequences. Building on this representation, we introduce two differentiable path-following methods for computing normal-form proper equilibria. These methods rely on artificial sequence-form games whose expected payoff functions incorporate logarithmic or entropy regularization through an auxiliary variable. We prove the existence of a smooth equilibrium path induced by each artificial game, starting from an arbitrary positive realization plan and converging to a normal-form proper equilibrium of the original game as the auxiliary variable approaches zero. Finally, our experimental results demonstrate the effectiveness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10524v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqing Hou, Yiyin Cao, Chuangyin Dang</dc:creator>
    </item>
    <item>
      <title>Necessary President in Elections with Parties</title>
      <link>https://arxiv.org/abs/2602.10601</link>
      <description>arXiv:2602.10601v1 Announce Type: new 
Abstract: Consider an election where the set of candidates is partitioned into parties, and each party must choose exactly one candidate to nominate for the election held over all nominees. The Necessary President problem asks whether a candidate, if nominated, becomes the winner of the election for all possible nominations from other parties.
  We study the computational complexity of Necessary President for several voting rules. We show that while this problem is solvable in polynomial time for Borda, Maximin, and Copeland$^\alpha$ for every $\alpha\in [0,1]$, it is $\mathsf{coNP}$-complete for general classes of positional scoring rules that include $\ell$-Approval and $\ell$-Veto, even when the maximum size of a party is two. For such positional scoring rules, we show that Necessary President is $\mathsf{W}[2]$-hard when parameterized by the number of parties, but fixed-parameter tractable with respect to the number of voter types. Additionally, we prove that Necessary President for Ranked Pairs is $\mathsf{coNP}$-complete even for maximum party size two, and $\mathsf{W}[1]$-hard with respect to the number of parties; remarkably, both of these results hold even for constant number of voters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10601v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katar\'ina Cechl\'arov\'a, Ildik\'o Schlotter</dc:creator>
    </item>
    <item>
      <title>Smart Lotteries in School Choice: Ex-ante Pareto-Improvement with Ex-post Stability</title>
      <link>https://arxiv.org/abs/2602.10679</link>
      <description>arXiv:2602.10679v1 Announce Type: new 
Abstract: In a typical school choice application, the students have strict preferences over the schools while the schools have coarse priorities over the students based on their distance and their enrolled siblings. The outcome of a centralized admission mechanism is then usually obtained by the Deferred Acceptance (DA) algorithm with random tie-breaking. Therefore, every possible outcome of this mechanism is a stable solution for the coarse priorities that will arise with certain probability. This implies a probabilistic assignment, where the admission probability for each student-school pair is specified. In this paper, we propose a new efficiency-improving stable `smart lottery' mechanism. We aim to improve the probabilistic assignment ex-ante in a stochastic dominance sense, while ensuring that the improved random matching is still ex-post stable, meaning that it can be decomposed into stable matchings regarding the original coarse priorities. Therefore, this smart lottery mechanism can provide a clear Pareto-improvement in expectation for any cardinal utilities compared to the standard DA with lottery solution, without sacrificing the stability of the final outcome. We show that although the underlying computational problem is NP-hard, we can solve the problem by using advanced optimization techniques such as integer programming with column generation. We conduct computational experiments on generated and real instances. Our results show that the welfare gains by our mechanism are substantially larger than the expected gains by standard methods that realize efficiency improvements after ties have already been broken.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10679v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haris Aziz, P\'eter Bir\'o, Gergely Cs\'aji, Tom Demeulemeester</dc:creator>
    </item>
    <item>
      <title>Core-Stable Kidney Exchange via Altruistic Donors</title>
      <link>https://arxiv.org/abs/2602.10725</link>
      <description>arXiv:2602.10725v1 Announce Type: new 
Abstract: Kidney exchange programs among hospitals in the United States and across European countries improve efficiency by pooling donors and patients on a centralized platform. Sustaining such cooperation requires stability. When the core is empty, hospitals or countries may withhold easily matched pairs for internal use, creating incentive problems that undermine participation and reduce the scope and efficiency of exchange.
  We propose a method to restore core stability by augmenting the platform with altruistic donors. Although the worst-case number of required altruists can be large, we show that in realistic settings only a small number is needed. We analyze two models of the compatibility graph, one based on random graphs and the other on compatibility types. When only pairwise exchanges are allowed, the number of required altruists is bounded by the maximum number of independent odd cycles, defined as disjoint odd cycles with no edges between them. This bound grows logarithmically with market size in the random graph model and is at most one third of the number of compatibility types in the type-based model. When small exchange cycles are allowed, it suffices for each participating organization to receive a number of altruists proportional to the number of compatibility types. Finally, simulations show that far fewer altruists are needed in practice than worst-case theory suggests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10725v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gergely Cs\'aji, Th\'anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Equity by Design: Fairness-Driven Recommendation in Heterogeneous Two-Sided Markets</title>
      <link>https://arxiv.org/abs/2602.10739</link>
      <description>arXiv:2602.10739v1 Announce Type: new 
Abstract: Two-sided marketplaces embody heterogeneity in incentives: producers seek exposure while consumers seek relevance, and balancing these competing objectives through constrained optimization is now a standard practice. Yet real platforms face finer-grained complexity: consumers differ in preferences and engagement patterns, producers vary in catalog value and capacity, and business objectives impose additional constraints beyond raw relevance. We formalize two-sided fairness under these realistic conditions, extending prior work from soft single-item allocations to discrete multi-item recommendations. We introduce Conditional Value-at-Risk (CVaR) as a consumer-side objective that compresses group-level utility disparities, and integrate business constraints directly into the optimization. Our experiments reveal that the "free fairness" regime, where producer constraints impose no consumer cost, disappears in multi item settings. Strikingly, moderate fairness constraints can improve business metrics by diversifying exposure away from saturated producers. Scalable solvers match exact solutions at a fraction of the runtime, making fairness-aware allocation practical at scale. These findings reframe fairness not as a tax on platform efficiency but as a lever for sustainable marketplace health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10739v1</guid>
      <category>cs.GT</category>
      <category>cs.IR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Dominykas Seputis, Rajeev Verma, Alexander Timans</dc:creator>
    </item>
    <item>
      <title>Near-Feasible Stable Matchings: Incentives and Optimality</title>
      <link>https://arxiv.org/abs/2602.10851</link>
      <description>arXiv:2602.10851v1 Announce Type: new 
Abstract: Stable matching is a fundamental area with many practical applications, such as centralised clearinghouses for school choice or job markets. Recent work has introduced the paradigm of near-feasibility in capacitated matching settings, where agent capacities are slightly modified to ensure the existence of desirable outcomes. While useful when no stable matching exists, or some agents are left unmatched, it has not previously been investigated whether near-feasible stable matchings satisfy desirable properties with regard to their stability in the original instance. Furthermore, prior works often leave open deviation incentive issues that arise when the centralised authority modifies agents' capacities.
  We consider these issues in the Stable Fixtures problem model, which generalises many classical models through non-bipartite preferences and capacitated agents. We develop a formal framework to analyse and quantify agent incentives to adhere to computed matchings. Then, we embed near-feasible stable matchings in this framework and study the trade-offs between instability, capacity modifications, and computational complexity. We prove that capacity modifications can be simultaneously optimal at individual and aggregate levels, and provide efficient algorithms to compute them. We show that different modification strategies significantly affect stability, and establish that minimal modifications and minimal deviation incentives are compatible and efficiently computable under general conditions. Finally, we provide exact algorithms and experimental results for tractable and intractable versions of these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10851v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Glitzner</dc:creator>
    </item>
    <item>
      <title>The Computational Intractability of Not Worst Responding</title>
      <link>https://arxiv.org/abs/2602.10966</link>
      <description>arXiv:2602.10966v1 Announce Type: new 
Abstract: Finding, counting, or determining the existence of Nash equilibria, where players must play optimally given each others' actions, are known to be computational intractable problems. We ask whether weakening optimality to the requirement that each player merely avoid worst responses -- arguably the weakest meaningful rationality criterion -- yields tractable solution concepts. We show that it does not: any solution concept with this minimal guarantee is ``as intractable'' as pure Nash equilibrium. In general games, determining the existence of no-worst-response action profiles is NP-complete, finding one is NP-hard, and counting them is #P-complete. In potential games, where existence is guaranteed, the search problem is PLS-complete. Computational intractability therefore stems not only from the requirement of optimality, but also from the requirement of a minimal rationality guarantee for each player. Moreover, relaxing the latter requirement gives rise to a tractability trade-off between the strength of individual rationality guarantees and the fraction of players satisfying them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10966v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mete \c{S}eref Ahunbay, Paul W. Goldberg, Edwin Lock, Panayotis Mertikopoulos, Bary S. R. Pradelski, Bassel Tarbush</dc:creator>
    </item>
    <item>
      <title>Let Leaders Play Games: Improving Timing in Leader-based Consensus</title>
      <link>https://arxiv.org/abs/2602.11147</link>
      <description>arXiv:2602.11147v1 Announce Type: new 
Abstract: Propagation latency is inherent to any distributed network, including blockchains. Typically, blockchain protocols provide a timing buffer for block propagation across the network. In leader-based blockchains, the leader -- block proposer -- is known in advance for each slot. A fast (or low-latency) proposer may delay the block proposal in anticipation of more rewards from the transactions that would otherwise be included in the subsequent block. Deploying such a strategy by manipulating the timing is known as timing games. It increases the risk of missed blocks due to reduced time for other nodes to vote on the block, affecting the overall efficiency of the blockchain. Moreover, proposers who play timing games essentially appropriate MEV (additional rewards over transaction fees and the block reward) that would otherwise accrue to the next block, making it unfair to subsequent block proposers. We propose a double-block proposal mechanism, 2-Prop, to curtail timing games. 2-Prop selects two proposers per slot to propose blocks and confirms one of them. We design a reward-sharing policy for proposers based on how quickly their blocks propagate to avoid strategic deviations. In the induced game, which we call the Latency Game, we show that it is a Nash Equilibrium for the proposers to propose the block without delay under homogeneous network settings. Under heterogeneous network settings, we study many configurations, and our analysis shows that a faster proposer would prefer not to delay unless the other proposer is extremely slow. Thus, we show the efficacy of 2-Prop in mitigating the effect of timing games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11147v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rasheed M, Parth Desai, Sujit Gujar</dc:creator>
    </item>
    <item>
      <title>Utilitarian Distortion Under Probabilistic Voting</title>
      <link>https://arxiv.org/abs/2602.11152</link>
      <description>arXiv:2602.11152v1 Announce Type: new 
Abstract: The utilitarian distortion framework evaluates voting rules by their worst-case efficiency loss when voters have cardinal utilities but express only ordinal rankings. Under the classical model, a longstanding tension exists: Plurality, which suffers from the spoiler effect, achieves optimal $\Theta(m^2)$ distortion among deterministic rules, while normatively superior rules like Copeland and Borda have unbounded distortion. We resolve this tension under probabilistic voting with the Plackett-Luce model, where rankings are noisy reflections of utilities governed by an inverse temperature parameter $\beta$. Copeland and Borda both achieve at most $\beta\frac{1+e^{-\beta}}{1-e^{-\beta}}$ distortion, independent of the number of candidates $m$, and within a factor of 2 of the lower bound for randomized rules satisfying the probabilistic Condorcet loser criterion known from prior work. This improves upon the prior $O(\beta^2)$ bound for Borda. These upper bounds are nearly tight: prior work establishes a $(1-o(1))\beta$ lower bound for Borda, and we prove a $(1-\epsilon)\beta$ lower bound for Copeland for any constant $\epsilon &gt;0$. In contrast, rules that rely only on top-choice information fare worse: Plurality has distortion $\Omega(\min(e^\beta, m))$ and Random Dictator has distortion $\Theta(m)$. Additional `veto' information is also insufficient to remove the dependence on $m$; Plurality Veto and Pruned Plurality Veto have distortion $\Omega(\beta \ln m)$. We also prove a lower bound of $(\frac{5}{8}-\epsilon)\beta$ (for any constant $\epsilon &gt;0$) for all deterministic finite-precision tournament-based rules, a class that includes Copeland and any rule based on pairwise comparison margins rounded to fixed precision. Our results show that the distortion framework aligns with normative intuitions once the probabilistic nature of real-world voting is taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11152v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamidreza Alipour, Mohak Goyal</dc:creator>
    </item>
    <item>
      <title>Games with Payments between Learning Agents</title>
      <link>https://arxiv.org/abs/2405.20880</link>
      <description>arXiv:2405.20880v3 Announce Type: replace 
Abstract: In repeated games, such as auctions, players rely on autonomous learning agents to choose their actions. We study settings in which players have their agents make monetary transfers to other agents during play at their own expense, in order to influence learning dynamics in their favor. Our goal is to understand when players have incentives to use such payments, how payments between agents affect learning outcomes, and what the resulting implications are for welfare and its distribution. We propose a simple game-theoretic model to capture the incentive structure of such scenarios. We find that, quite generally, abstaining from payments is not robust to strategic deviations by users of learning agents: self-interested players benefit from having their agents make payments to other learners. In a broad class of games, such endogenous payments between learning agents lead to higher welfare for all players. In first- and second-price auctions, equilibria of the induced "payment-policy game" lead to highly collusive learning outcomes, with low or vanishing revenue for the auctioneer. These results highlight a fundamental challenge for mechanism design, as well as for regulatory policies, in environments where learning agents may interact in the digital ecosystem beyond a mechanism's boundaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20880v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoav Kolumbus, Joe Halpern, \'Eva Tardos</dc:creator>
    </item>
    <item>
      <title>The Complexity of Tullock Contests</title>
      <link>https://arxiv.org/abs/2412.06444</link>
      <description>arXiv:2412.06444v3 Announce Type: replace 
Abstract: Despite the extensive literature on Tullock contests, computational results for the general model with heterogeneous contestants remain scarce. This paper studies the algorithmic complexity of computing a pure Nash Equilibrium (PNE) in such general Tullock contests. We find that the elasticity parameters {r_i}, which govern the returns to scale of contestants' production functions, play a decisive role in the problem's complexity. Our core conceptual insight is that the computational hardness is determined specifically by the number of contestants with medium elasticity (r_i \in (1, 2]). This is illustrated by a complete set of algorithmic results under two parameter regimes:
  -Efficient Regime: When the number of contestants with medium elasticity is logarithmically bounded by the total number of contestants (O(log n)), we provide an algorithm that determines the existence of a PNE and computes an epsilon-PNE in polynomial time in both n and log(1/epsilon) (i.e., Poly(n,log(1/epsilon))) whenever it exists. This result generalizes classical findings for concave (r_i &lt;= 1) and convex (r_i &gt; 2) cases, establishing computational tractability for a broader class of mixed-elasticity contests.
  -Hard Regime: In contrast, we show when the number of medium elasticity contestants exceed Omega(log n), determining the existence of PNEs is NP-complete and it is impossible for any algorithm to compute an epsilon-PNE within running time Poly(n,log(1/epsilon)). We then design a Fully Polynomial-Time Approximation Scheme (FPTAS) that computes an epsilon-PNE in Poly(n,1/epsilon), guaranteeing efficient approximations for hard instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06444v3</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu He, Fan Yao, Yang Yu, Xiaoyun Qiu, Minming Li, Haifeng Xu</dc:creator>
    </item>
    <item>
      <title>Explainable Information Design</title>
      <link>https://arxiv.org/abs/2508.14196</link>
      <description>arXiv:2508.14196v3 Announce Type: replace 
Abstract: Optimal signaling schemes in information design (Bayesian persuasion) often involve randomization or disconnected partitions of state space, which might be too intricate to be audited or communicated. We propose explainable information design in the context of linear information design with a continuous state space. In the case of single-dimensional state, we restrict the information designer to use $K$-partitional signaling schemes defined by deterministic and monotone partitions of the state space, where a unique signal is sent for all states in each part. We prove that the price of explainability (PoE) -- the ratio between the performances of the optimal explainable signaling scheme and unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning that partitional signaling schemes are never worse than arbitrary signaling schemes by a factor of $2$. For a uniform prior, this PoE can be improved to a tight $2/3$. We then extend the analysis to multi-dimensional state spaces by studying two natural explainability notions: convex-partitional policies and axis-aligned rectangular policies. For convex-partitional policies, we prove a tight PoE of $1/(m+1)$, while for rectangular policies we establish a PoE guarantee under uniform prior that is independent of $K$ but unavoidably exponential in $m$. On the computational side, we prove that the exact optimization of explainable policy is NP-hard in general, but provide efficient approximation methods, including an FPTAS for Lipschitz utility functions and a polynomial-time algorithm that achieves the worst-case $1/2$ benchmark for the broad class of discontinuous, piecewise Lipschitz, utility functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14196v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Chen, Tao Lin, Wei Tang, Jamie Tucker-Foltz</dc:creator>
    </item>
    <item>
      <title>A Policy Iteration Method for Inverse Mean Field Games</title>
      <link>https://arxiv.org/abs/2409.06184</link>
      <description>arXiv:2409.06184v4 Announce Type: replace-cross 
Abstract: We propose a policy iteration method to solve an inverse problem for a mean-field game (MFG) model, specifically to reconstruct the obstacle function in the game from the partial observation data of value functions, which represent the optimal costs for agents. The proposed approach decouples this complex inverse problem, which is an optimization problem constrained by a coupled nonlinear forward and backward PDE system in the MFG, into several iterations of solving linear PDEs and linear inverse problems. This method can also be viewed as a fixed-point iteration that simultaneously solves the MFG system and inversion. We prove its linear rate of convergence. In addition, numerical examples in 1D and 2D, along with performance comparisons to a direct least-squares method, demonstrate the superior efficiency and accuracy of the proposed method for solving inverse MFGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06184v4</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kui Ren, Nathan Soedjak, Shanyin Tong</dc:creator>
    </item>
    <item>
      <title>Statistical Equilibrium of Optimistic Beliefs</title>
      <link>https://arxiv.org/abs/2502.09569</link>
      <description>arXiv:2502.09569v3 Announce Type: replace-cross 
Abstract: We study finite normal-form games in which payoffs are subject to random perturbations and players face uncertainty about how these shocks co-move across actions, an ambiguity that naturally arises when only realized (not counterfactual) payoffs are observed. We introduce the Statistical Equilibrium of Optimistic Beliefs (SE-OB), inspired by discrete choice theory. We model players as \textit{optimistic better responders}: they face ambiguity about the dependence structure (copula) of payoff perturbations across actions and resolve this ambiguity by selecting, from a belief set, the joint distribution that maximizes the expected value of the best perturbed payoff. Given this optimistic belief, players choose actions according to the induced random-utility choice rule. We define SE-OB as a fixed point of this two-step response mapping.
  SE-OB generalizes the Nash equilibrium and the structural quantal response equilibrium. We establish existence under standard regularity conditions on belief sets. For the economically important class of marginal belief sets, that is, the set of all joint distributions with fixed action-wise marginals, optimistic belief selection reduces to an optimal coupling problem, and SE-OB admits a characterization via Nash equilibrium of a smooth regularized game, yielding tractability and enabling computation.
  We characterize the relationship between SE-OB and existing equilibrium notions and illustrate its empirical relevance in simulations, where it captures systematic violations of independence of irrelevant alternatives that standard logit-based models fail to explain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09569v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Gui, Bahar Ta\c{s}kesen</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations</title>
      <link>https://arxiv.org/abs/2502.10303</link>
      <description>arXiv:2502.10303v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10303v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelrhman Shaheen, Anas Badr, Ali Abohendy, Hatem Alsaadawy, Nadine Alsayad, Ehab H. El-Shazly</dc:creator>
    </item>
    <item>
      <title>Convergence and Connectivity: Dynamics of Multi-Agent Q-Learning in Random Networks</title>
      <link>https://arxiv.org/abs/2503.10186</link>
      <description>arXiv:2503.10186v2 Announce Type: replace-cross 
Abstract: Beyond specific settings, many multi-agent learning algorithms fail to converge to an equilibrium solution, instead displaying complex, non-stationary behaviours such as recurrent or chaotic orbits. In fact, recent literature suggests that such complex behaviours are likely to occur when the number of agents increases. In this paper, we study Q-learning dynamics in network polymatrix normal-form games where the network structure is drawn from classical random graph models. In particular, we focus on the Erd\H{o}s-R\'enyi model, which is used to analyze connectivity in distributed systems, and the Stochastic Block model, which generalizes the above by accounting for community structures that naturally arise in multi-agent systems. In each setting, we establish sufficient conditions under which the agents' joint strategies converge to a unique equilibrium. We investigate how this condition depends on the exploration rates, payoff matrices and, crucially, the probabilities of interaction between network agents. We validate our theoretical findings through numerical simulations and demonstrate that convergence can be reliably achieved in many-agent systems, provided interactions in the network are controlled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10186v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>math.DS</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Leonte, Aamal Hussain, Raphael Huser, Francesco Belardinelli, Dario Paccagnan</dc:creator>
    </item>
    <item>
      <title>Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms</title>
      <link>https://arxiv.org/abs/2601.07651</link>
      <description>arXiv:2601.07651v2 Announce Type: replace-cross 
Abstract: As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07651v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers</dc:creator>
    </item>
  </channel>
</rss>
