<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 02:22:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the importance of tail assumptions in climate extreme event attribution</title>
      <link>https://arxiv.org/abs/2507.14019</link>
      <description>arXiv:2507.14019v1 Announce Type: new 
Abstract: Extreme weather events are becoming more frequent and intense, posing serious threats to human life, biodiversity, and ecosystems. A key objective of extreme event attribution (EEA) is to assess whether and to what extent anthropogenic climate change influences such events. Central to EEA is the accurate statistical characterization of atmospheric extremes, which are inherently multivariate or spatial due to their measurement over high-dimensional grids. Within the counterfactual causal inference framework of Pearl, we evaluate how tail assumptions affect attribution conclusions by comparing three multivariate modeling approaches for estimating causation metrics. These include: (i) the multivariate generalized Pareto distribution, which imposes an invariant tail dependence structure; (ii) the factor copula model of Castro-Camilo and Huser (2020), which offers flexible subasymptotic behavior; and (iii) the model of Huser and Wadsworth (2019), which smoothly transitions between different forms of extremal dependence. We assess the implications of these modeling choices in both simulated scenarios (under varying forms of model misspecification) and real data applications, using weekly winter maxima over Europe from the M\'et\'eo-France CNRM model and daily precipitation from the ACCESS-CM2 model over the U.S. Our findings highlight that tail assumptions critically shape causality metrics in EEA. Misspecification of the extremal dependence structure can lead to substantially different and potentially misleading attribution conclusions, underscoring the need for careful model selection and evaluation when quantifying the influence of climate change on extreme events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14019v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengran Li, Daniela Castro-Camilo</dc:creator>
    </item>
    <item>
      <title>A mixture distribution approach for assessing genetic impact from twin study</title>
      <link>https://arxiv.org/abs/2507.13605</link>
      <description>arXiv:2507.13605v1 Announce Type: cross 
Abstract: This work was motivated by a twin study with the goal of assessing the genetic control of immune traits. We propose a mixture bivariate distribution to model twin data where the underlying order within a pair is unclear. Though estimation from mixture distribution is usually subject to low convergence rate, the combined likelihood, which is constructed over monozygotic and dizygotic twins combined, reaches root-n consistency and allows effective statistical inference on the genetic impact. The method is applicable to general unordered pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13605v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.9367</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine, 41, 2513-2522 (2022)</arxiv:journal_reference>
      <dc:creator>Zonghui Hu, Pengfei Li, Dean Follmann, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Intellectual Up-streams of Percentage Scale ($ps$) and Percentage Coefficient ($b_p$) -- Effect Size Analysis (Theory Paper 2)</title>
      <link>https://arxiv.org/abs/2507.13695</link>
      <description>arXiv:2507.13695v1 Announce Type: cross 
Abstract: Percentage thinking, i.e., assessing quantities as parts per hundred, has traveled widely from Roman tax ledgers to modern algorithms. Building on early decimalization by Simon Stevin in La Thiende (1585) and the 19th-century metrication movement that institutionalized base-10 measurement worldwide (Cajori, 1925), this article traces the intellectual trails through which base-10 normalization, especially 0~1 percentage scale. We discuss commonalities between those Wisconsin-Carolina experiments and classic indices, especially the plus minus 1 Pearson (1895) correlation (r) and 0~1 coefficient of determination, aka r squared (Wright, 1920). We pay tribute to the influential percent of maximum possible (POMP) coefficient by Cohen et al. (1999). The history of the 0~100 or 0~1 scales goes back far and wide. Roman fiscal records, early American grading experiments at Yale and Harvard, and contemporary analysis of percent scales (0~100) and percentage scales (0~1, or -1~1) show the tendency to rediscover the scales and the indices based on the scales (Durm, 1993; Schneider &amp; and Hutt, 2014). Data mining and machine learning since the last century adopted the same logic: min-max normalization, which maps any feature to [0, 1] (i.e., 0-100%), equalizing the scale ranges. Because 0~1 percentage scale assigns the entire scale to be the unit, equalizing the scales also equalizes the units of all percentized scales. Equitable units are necessary and sufficient for comparability of two indices, according to the percentage theory of measurement indices (Cohen et al., 1999; Zhao et al., 2024; Zhao &amp; Zhang, 2014). Thus, the success of modern AI serves as a large scale test confirming the comparability of percentage-based indices, foremost among them the percentage coefficient ($b_p$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13695v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshu Zhao, Qinru Ruby Ju, Piper Liping Liu, Dianshi Moses Li, Luxi Zhang, Jizhou Francis Ye, Song Harris Ao, Ming Milano Li</dc:creator>
    </item>
    <item>
      <title>Using off-treatment sequential multiple imputation for binary outcomes to address intercurrent events handled by a treatment policy strategy</title>
      <link>https://arxiv.org/abs/2507.14006</link>
      <description>arXiv:2507.14006v1 Announce Type: cross 
Abstract: The estimand framework proposes different strategies to address intercurrent events. The treatment policy strategy seems to be the most favoured as it is closely aligned with the pre-addendum intention-to-treat principle. All data for all patients should ideally be collected, however, in reality patients may withdraw from a study leading to missing data. This needs to be dealt with as part of the estimation. Several areas of research have been conducted exploring models to estimate the estimand when intercurrent events are handled using a treatment policy strategy, however the research is limited for binary endpoints. We explore different retrieved dropout models, where post-intercurrent event, the observed data can be used to multiply impute the missing post-intercurrent event data. We compare our proposed models to a simple imputation model that makes no distinction between the pre- and post-intercurrent event data, and assess varying statistical properties through a simulation study. We then provide an example how retrieved dropout models were used in practice for Phase 3 clinical trials in rheumatoid arthritis. From the models explored, we conclude that a simple retrieved dropout model including an indicator for whether or not the intercurrent event occurred is the most pragmatic choice. However, at least 50% of observed post-intercurrent event data is required for these models to work well. Therefore, the suitability of implementing this model in practice will depend on the amount of observed post-intercurrent event data available and missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14006v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunita Rehal, Nicky Best, Sarah Watts, Thomas Drury</dc:creator>
    </item>
    <item>
      <title>Bayesian modelling and computation utilising directed cycles in multiple network data</title>
      <link>https://arxiv.org/abs/2111.07840</link>
      <description>arXiv:2111.07840v2 Announce Type: replace 
Abstract: Modelling multiple network data is crucial for addressing a wide range of applied research questions. However, there are many challenges, both theoretical and computational, to address. Network cycles are often of particular interest in many applications; for example in ecology a largely unexplored area has been how to incorporate network cycles within the inferential framework in an explicit way. The recently developed Spherical Network Family of models (SNF) offers a flexible formulation for modelling multiple network data that permits any type of metric. This has opened up the possibility to formulate network models that focus on network properties hitherto not possible or practical to consider. In this article we propose a novel network distance metric that measures similarities between networks with respect to their cycles, and incorporates this within the SNF model to allow inferences that explicitly capture information on cycles. These network motifs are of particular interest in ecological studies aimed at understanding competitive and hierarchical interactions. We further propose a novel computational framework to allow posterior inferences from the intractable SNF model for moderate-sized networks. Lastly, we apply the resulting methodology to a set of ecological network data studying aggressive interactions between species of fish. We show our model is able to make cogent inferences concerning the cycle behaviour amongst the species, and beyond those possible from a model that does not consider this network motif.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.07840v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anastasia Mantziou, Sally Keith, David M. P. Jacoby, Simon Lunagomez, Robin Mitra</dc:creator>
    </item>
    <item>
      <title>The TruEnd-procedure: Treating trailing zero-valued balances in credit data</title>
      <link>https://arxiv.org/abs/2404.17008</link>
      <description>arXiv:2404.17008v4 Announce Type: replace-cross 
Abstract: A novel procedure is presented for finding the true but latent endpoints within the repayment histories of individual loans. The monthly observations beyond these true endpoints are false, largely due to operational failures that delay account closure, thereby corrupting some loans. Detecting these false observations is difficult at scale since each affected loan history might have a different sequence of trailing zero (or very small) month-end balances. Identifying these trailing balances requires an exact definition of a "small balance", which our method informs. We demonstrate this procedure and isolate the ideal small-balance definition using two different South African datasets. Evidently, corrupted loans are remarkably prevalent and have excess histories that are surprisingly long, which ruin the timing of risk events and compromise any subsequent time-to-event model, e.g., survival analysis. Having discarded these excess histories, we demonstrably improve the accuracy of both the predicted timing and severity of risk events, without materially impacting the portfolio. The resulting estimates of credit losses are lower and less biased, which augurs well for raising accurate credit impairments under IFRS 9. Our work therefore addresses a pernicious data error, which highlights the pivotal role of data preparation in producing credible forecasts of credit risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17008v4</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Roelinde Bester</dc:creator>
    </item>
  </channel>
</rss>
