<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Detecting Arbitrary Planted Subgraphs in Random Graphs</title>
      <link>https://arxiv.org/abs/2503.19069</link>
      <description>arXiv:2503.19069v1 Announce Type: new 
Abstract: The problems of detecting and recovering planted structures/subgraphs in Erd\H{o}s-R\'{e}nyi random graphs, have received significant attention over the past three decades, leading to many exciting results and mathematical techniques. However, prior work has largely focused on specific ad hoc planted structures and inferential settings, while a general theory has remained elusive. In this paper, we bridge this gap by investigating the detection of an \emph{arbitrary} planted subgraph $\Gamma = \Gamma_n$ in an Erd\H{o}s-R\'{e}nyi random graph $\mathcal{G}(n, q_n)$, where the edge probability within $\Gamma$ is $p_n$. We examine both the statistical and computational aspects of this problem and establish the following results. In the dense regime, where the edge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the information-theoretic and computational thresholds for detecting $\Gamma$, and provide conditions under which a computational-statistical gap arises. Most notably, these thresholds depend on $\Gamma$ only through its number of edges, maximum degree, and maximum subgraph density. Our lower and upper bounds are general and apply to any value of $p_n$ and $q_n$ as functions of $n$. Accordingly, we also analyze the sparse regime where $q_n = \Theta(n^{-\alpha})$ and $p_n-q_n =\Theta(q_n)$, with $\alpha\in[0,2]$, as well as the critical regime where $p_n=1-o(1)$ and $q_n = \Theta(n^{-\alpha})$, both of which have been widely studied, for specific choices of $\Gamma$. For these regimes, we show that our bounds are tight for all planted subgraphs investigated in the literature thus far\textemdash{}and many more. Finally, we identify conditions under which detection undergoes sharp phase transition, where the boundaries at which algorithms succeed or fail shift abruptly as a function of $q_n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19069v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dor Elimelech, Wasim Huleihel</dc:creator>
    </item>
    <item>
      <title>Estimation of accuracy and reliability of models of $\varphi$-sub-Gaussian stochastic processes in $C(T)$ spaces</title>
      <link>https://arxiv.org/abs/2503.19789</link>
      <description>arXiv:2503.19789v1 Announce Type: new 
Abstract: At present, in the theory of stochastic process modeling a problem of assessment of reliability and accuracy of stochastic process model in $C(T)$ space wasn't studied for the case of implicit decomposition of process in the form of a series with independent terms. The goal is to study reliability and accuracy in $C(T)$ of models of processes from $Sub_\varphi(\Omega)$ that cannot be decomposed in a series with independent elements explicitly. Using previous research in the field of modeling of stochastic processes, assumption is considered about possibility of decomposition of a stochastic process in the series with independent elements that can be found using approximations. Impact of approximation error of process decomposition in series with independent elements on reliability and accuracy of modeling of stochastic process in $C(T)$ is studied. Theorems are proved that allow estimation of reliability and accuracy of a model in $C(T)$ of a stochastic process from $Sub_\varphi(\Omega)$ in the case when decomposition of this process in a series with independent elements can be found only with some error, for example, using numerical approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19789v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.20535/1810-0546.2017.4.105428</arxiv:DOI>
      <arxiv:journal_reference>Research Bulletin of the National Technical University of Ukraine "Kyiv Polytechnics Institute", Iss. 4, 2017</arxiv:journal_reference>
      <dc:creator>Oleksandr Mokliachuk</dc:creator>
    </item>
    <item>
      <title>Lean Formalization of Generalization Error Bound by Rademacher Complexity</title>
      <link>https://arxiv.org/abs/2503.19605</link>
      <description>arXiv:2503.19605v1 Announce Type: cross 
Abstract: We formalize the generalization error bound using Rademacher complexity in the Lean 4 theorem prover. Generalization error quantifies the gap between a learning machine's performance on given training data versus unseen test data, and Rademacher complexity serves as an estimate of this error based on the complexity of learning machines, or hypothesis class. Unlike traditional methods such as PAC learning and VC dimension, Rademacher complexity is applicable across diverse machine learning scenarios including deep learning and kernel methods. We formalize key concepts and theorems, including the empirical and population Rademacher complexities, and establish generalization error bounds through formal proofs of McDiarmid's inequality, Hoeffding's lemma, and symmetrization arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19605v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sho Sonoda, Kazumi Kasaura, Yuma Mizuno, Kei Tsukamoto, Naoto Onda</dc:creator>
    </item>
    <item>
      <title>No-prior Bayesian inference reIMagined: probabilistic approximations of inferential models</title>
      <link>https://arxiv.org/abs/2503.19748</link>
      <description>arXiv:2503.19748v1 Announce Type: cross 
Abstract: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a "default prior" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM's desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19748v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Interpretable Deep Regression Models with Interval-Censored Failure Time Data</title>
      <link>https://arxiv.org/abs/2503.19763</link>
      <description>arXiv:2503.19763v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) have become powerful tools for modeling complex data structures through sequentially integrating simple functions in each hidden layer. In survival analysis, recent advances of DNNs primarily focus on enhancing model capabilities, especially in exploring nonlinear covariate effects under right censoring. However, deep learning methods for interval-censored data, where the unobservable failure time is only known to lie in an interval, remain underexplored and limited to specific data type or model. This work proposes a general regression framework for interval-censored data with a broad class of partially linear transformation models, where key covariate effects are modeled parametrically while nonlinear effects of nuisance multi-modal covariates are approximated via DNNs, balancing interpretability and flexibility. We employ sieve maximum likelihood estimation by leveraging monotone splines to approximate the cumulative baseline hazard function. To ensure reliable and tractable estimation, we develop an EM algorithm incorporating stochastic gradient descent. We establish the asymptotic properties of parameter estimators and show that the DNN estimator achieves minimax-optimal convergence. Extensive simulations demonstrate superior estimation and prediction accuracy over state-of-the-art methods. Applying our method to the Alzheimer's Disease Neuroimaging Initiative dataset yields novel insights and improved predictive performance compared to traditional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19763v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changhui Yuan, Shishun Zhao, Shuwei Li, Xinyuan Song, Zhao Chen</dc:creator>
    </item>
    <item>
      <title>A Martingale Approach to Large-$\theta$ Ewens-Pitman Model</title>
      <link>https://arxiv.org/abs/2503.19892</link>
      <description>arXiv:2503.19892v1 Announce Type: cross 
Abstract: We investigate the asymptotic behavior of the number of parts $K_n$ in the Ewens--Pitman partition model under the regime where the diversity parameter is scaled linearly with the sample size, that is, $\theta = \lambda n$ for some~$\lambda &gt; 0$. While recent work has established a law of large numbers (LLN) and a central limit theorem (CLT) for $K_n$ in this regime, we revisit these results through a martingale-based approach. Our method yields significantly shorter proofs, and leads to sharper convergence rates in the CLT, including improved Berry--Esseen bounds in the case $\alpha = 0$, and a new result for the regime $\alpha \in (0,1)$, filling a gap in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19892v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo Ribeiro</dc:creator>
    </item>
    <item>
      <title>Insight from the Kullback--Leibler divergence into adaptive importance sampling schemes for rare event analysis in high dimension</title>
      <link>https://arxiv.org/abs/2309.16828</link>
      <description>arXiv:2309.16828v2 Announce Type: replace 
Abstract: We study two adaptive importance sampling schemes for estimating the probability of a rare event in the high-dimensional regime $d \to \infty$ with $d$ the dimension. The first scheme is the prominent cross-entropy (CE) method, and the second scheme, motivated by recent results, uses as auxiliary distribution a projection of the optimal auxiliary distribution on a lower dimensional subspace. In these schemes, two samples are used: the first one to learn the auxiliary distribution and the second one, drawn according to the learned distribution, to perform the final probability estimation. Contrary to the common belief that the sample size needs to grow exponentially in the dimension to make the estimator consistent and avoid the weight degeneracy phenomenon, we find that a polynomial sample size in the first learning step is enough. We prove this result assuming that the sought probability is bounded away from 0. For CE, insight is provided on the polynomial growth rate which remains implicit. In contrast, we study the second scheme in a simple computational framework assuming that samples from the conditional distribution are available. This makes it possible to show that the sample size only needs to grow like $rd$ with $r$ the effective dimension of the projection, which highlights the potential benefits of these projection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16828v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jason Beh, Yonatan Shadmi, Florian Simatos</dc:creator>
    </item>
    <item>
      <title>Interpretation of local false discovery rates under the zero assumption</title>
      <link>https://arxiv.org/abs/2402.08792</link>
      <description>arXiv:2402.08792v2 Announce Type: replace 
Abstract: In large-scale studies with parallel signal-plus-noise observations, the local false discovery rate is a summary statistic that is often presumed to be equal to the posterior probability that the signal is null. We prefer to call the latter quantity the local null-signal rate to emphasize our view that a null signal and a false discovery are not identical events. The local null-signal rate is commonly estimated through empirical Bayes procedures that build on the `zero density assumption,' which attributes the density of observations near zero entirely to null signals. In this paper, we argue that this strategy does not furnish estimates of the local null-signal rate, but instead of a quantity we call the complementary local activity rate (clar). Although it is likely to be small, an inactive signal is not necessarily zero. The clar dominates both the local null-signal rate and the local false sign rate and is a weakly continuous functional of the signal distribution. As a consequence, it takes on sensible values when the signal is sparse but not exactly zero. Our findings clarify the interpretation of local false discovery rates estimated under the zero density assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08792v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Xiang, Nikolaos Ignatiadis, Peter McCullagh</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Estimators based on the Block Maxima Method</title>
      <link>https://arxiv.org/abs/2409.05529</link>
      <description>arXiv:2409.05529v2 Announce Type: replace 
Abstract: The block maxima method is a standard approach for analyzing the extremal behavior of a potentially multivariate time series. It has recently been found that the classical approach based on disjoint block maxima may be universally improved by considering sliding block maxima instead. However, the asymptotic variance formula for estimators based on sliding block maxima involves an integral over the covariance of a certain family of multivariate extreme value distributions, which makes its estimation, and inference in general, an intricate problem. As an alternative, one may rely on bootstrap approximations: we show that naive block-bootstrap approaches from time series analysis are inconsistent even in i.i.d.\ situations, and provide a consistent alternative based on resampling circular block maxima. As a by-product, we show consistency of the classical resampling bootstrap for disjoint block maxima, and that estimators based on circular block maxima have the same asymptotic variance as their sliding block maxima counterparts. The finite sample properties are illustrated by Monte Carlo experiments, and the methods are demonstrated by a case study of precipitation extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05529v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Torben Staud</dc:creator>
    </item>
    <item>
      <title>Cheap Permutation Testing</title>
      <link>https://arxiv.org/abs/2502.07672</link>
      <description>arXiv:2502.07672v2 Announce Type: replace 
Abstract: Permutation tests are a popular choice for distinguishing distributions and testing independence, due to their exact, finite-sample control of false positives and their minimax optimality when paired with U-statistics. However, standard permutation tests are also expensive, requiring a test statistic to be computed hundreds or thousands of times to detect a separation between distributions. In this work, we offer a simple approach to accelerate testing: group your datapoints into bins and permute only those bins. For U and V-statistics, we prove that these cheap permutation tests have two remarkable properties. First, by storing appropriate sufficient statistics, a cheap test can be run in time comparable to evaluating a single test statistic. Second, cheap permutation power closely approximates standard permutation power. As a result, cheap tests inherit the exact false positive control and minimax optimality of standard permutation tests while running in a fraction of the time. We complement these findings with improved power guarantees for standard permutation testing and experiments demonstrating the benefits of cheap permutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt independence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney, cross-MMD, and cross-HSIC tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07672v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Domingo-Enrich, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Targeted Separation and Convergence with Kernel Discrepancies</title>
      <link>https://arxiv.org/abs/2209.12835</link>
      <description>arXiv:2209.12835v5 Announce Type: replace-cross 
Abstract: Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our results for hypothesis testing, measuring and improving sample quality, and sampling with Stein variational gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.12835v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Classification of small-ball modes and maximum a posteriori estimators</title>
      <link>https://arxiv.org/abs/2306.16278</link>
      <description>arXiv:2306.16278v3 Announce Type: replace-cross 
Abstract: A mode, or `most likely point', for a probability measure $\mu$ can be defined in various ways via the asymptotic behaviour of the $\mu$-mass of balls as their radius tends to zero. Such points are of intrinsic interest in the local theory of measures on metric spaces and also arise naturally in the study of Bayesian inverse problems and diffusion processes. Building upon special cases already proposed in the literature, this paper develops a systematic framework for defining modes through small-ball probabilities. We propose `common-sense' axioms that such definitions should obey, including appropriate treatment of discrete and absolutely continuous measures, as well as symmetry and invariance properties. We show that there are exactly ten such definitions consistent with these axioms, and that they are partially but not totally ordered in strength, forming a complete, distributive lattice. We also show how this classification simplifies for well-behaved $\mu$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16278v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilja Klebanov, Hefin Lambley, T. J. Sullivan</dc:creator>
    </item>
    <item>
      <title>Conditional partial exchangeability: a probabilistic framework for multi-view clustering</title>
      <link>https://arxiv.org/abs/2307.01152</link>
      <description>arXiv:2307.01152v2 Announce Type: replace-cross 
Abstract: Standard clustering techniques assume a common configuration for all features in a dataset. However, when dealing with multi-view or longitudinal data, the clusters' number, frequencies, and shapes may need to vary across features to accurately capture dependence structures and heterogeneity. In this setting, classical model-based clustering fails to account for within-subject dependence across domains. We introduce conditional partial exchangeability, a novel probabilistic paradigm for dependent random partitions of the same objects across distinct domains. Additionally, we study a wide class of Bayesian clustering models based on conditional partial exchangeability, which allows for flexible dependent clustering of individuals across features, capturing the specific contribution of each feature and the within-subject dependence, while ensuring computational feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01152v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Maria De Iorio, Johan Eriksson</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference: Multi-Study ATE Estimation beyond Meta-Analysis</title>
      <link>https://arxiv.org/abs/2410.16870</link>
      <description>arXiv:2410.16870v2 Announce Type: replace-cross 
Abstract: We study Federated Causal Inference, an approach to estimate treatment effects from decentralized data across centers. We compare three classes of Average Treatment Effect (ATE) estimators derived from the Plug-in G-Formula, ranging from simple meta-analysis to one-shot and multi-shot federated learning, the latter leveraging the full data to learn the outcome model (albeit requiring more communication). Focusing on Randomized Controlled Trials (RCTs), we derive the asymptotic variance of these estimators for linear models. Our results provide practical guidance on selecting the appropriate estimator for various scenarios, including heterogeneity in sample sizes, covariate distributions, treatment assignment schemes, and center effects. We validate these findings with a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16870v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Khellaf, Aur\'elien Bellet, Julie Josse</dc:creator>
    </item>
    <item>
      <title>AutoBayes: A Compositional Framework for Generalized Variational Inference</title>
      <link>https://arxiv.org/abs/2503.18608</link>
      <description>arXiv:2503.18608v2 Announce Type: replace-cross 
Abstract: We introduce a new compositional framework for generalized variational inference, clarifying the different parts of a model, how they interact, and how they compose. We explain that both exact Bayesian inference and the loss functions typical of variational inference (such as variational free energy and its generalizations) satisfy chain rules akin to that of reverse-mode automatic differentiation, and we advocate for exploiting this to build and optimize models accordingly. To this end, we construct a series of compositional tools: for building models; for constructing their inversions; for attaching local loss functions; and for exposing parameters. Finally, we explain how the resulting parameterized statistical games may be optimized locally, too. We illustrate our framework with a number of classic examples, pointing to new areas of extensibility that are revealed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18608v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toby St Clere Smithe, Marco Perin</dc:creator>
    </item>
  </channel>
</rss>
