<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating complete basis set extrapolation error through random walk</title>
      <link>https://arxiv.org/abs/2503.09771</link>
      <description>arXiv:2503.09771v1 Announce Type: new 
Abstract: We propose a method of estimating the uncertainty of a result obtained through extrapolation to the complete basis set limit. The method is based on an ensemble of random walks which simulate all possible extrapolation outcomes that could have been obtained if results from larger basis sets had been available. The results assembled from a large collection of random walks can be then analyzed statistically, providing a route for uncertainty prediction at a confidence level required in a particular application. The method is free of empirical parameters and compatible with any extrapolation scheme. The proposed technique is tested in a series of numerical trials by comparing the determined confidence intervals with reliable reference data. We demonstrate that the predicted error bounds are reliable, tight, yet conservative at the same time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09771v1</guid>
      <category>physics.data-an</category>
      <category>physics.chem-ph</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub Lang, Micha{\l} Przybytek, Micha{\l} Lesiuk</dc:creator>
    </item>
    <item>
      <title>Data augmentation using diffusion models to enhance inverse Ising inference</title>
      <link>https://arxiv.org/abs/2503.10154</link>
      <description>arXiv:2503.10154v1 Announce Type: new 
Abstract: Identifying model parameters from observed configurations poses a fundamental challenge in data science, especially with limited data. Recently, diffusion models have emerged as a novel paradigm in generative machine learning, capable of producing new samples that closely mimic observed data. These models learn the gradient of model probabilities, bypassing the need for cumbersome calculations of partition functions across all possible configurations. We explore whether diffusion models can enhance parameter inference by augmenting small datasets. Our findings demonstrate this potential through a synthetic task involving inverse Ising inference and a real-world application of reconstructing missing values in neural activity data. This study serves as a proof-of-concept for using diffusion models for data augmentation in physics-related problems, thereby opening new avenues in data science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10154v1</guid>
      <category>physics.data-an</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yechan Lim, Sangwon Lee, Junghyo Jo</dc:creator>
    </item>
    <item>
      <title>Tools for Unbinned Unfolding</title>
      <link>https://arxiv.org/abs/2503.09720</link>
      <description>arXiv:2503.09720v1 Announce Type: cross 
Abstract: Machine learning has enabled differential cross section measurements that are not discretized. Going beyond the traditional histogram-based paradigm, these unbinned unfolding methods are rapidly being integrated into experimental workflows. In order to enable widespread adaptation and standardization, we develop methods, benchmarks, and software for unbinned unfolding. For methodology, we demonstrate the utility of boosted decision trees for unfolding with a relatively small number of high-level features. This complements state-of-the-art deep learning models capable of unfolding the full phase space. To benchmark unbinned unfolding methods, we develop an extension of existing dataset to include acceptance effects, a necessary challenge for real measurements. Additionally, we directly compare binned and unbinned methods using discretized inputs for the latter in order to control for the binning itself. Lastly, we have assembled two software packages for the OmniFold unbinned unfolding method that should serve as the starting point for any future analyses using this technique. One package is based on the widely-used RooUnfold framework and the other is a standalone package available through the Python Package Index (PyPI).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09720v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Milton, Vinicius Mikuni, Trevin Lee, Miguel Arratia, Tanvi Wamorkar, Benjamin Nachman</dc:creator>
    </item>
    <item>
      <title>Thermodynamic Bound on Energy and Negentropy Costs of Inference in Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2503.09980</link>
      <description>arXiv:2503.09980v1 Announce Type: cross 
Abstract: The fundamental thermodynamic bound is derived for the energy cost of inference in Deep Neural Networks (DNNs). By applying Landauer's principle, we demonstrate that the linear operations in DNNs can, in principle, be performed reversibly, whereas the non-linear activation functions impose an unavoidable energy cost. The resulting theoretical lower bound on the inference energy is determined by the average number of neurons undergoing state transition for each inference. We also restate the thermodynamic bound in terms of negentropy, a metric which is more universal than energy for assessing thermodynamic cost of information processing. Concept of negentropy is further elaborated in the context of information processing in biological and engineered system as well as human intelligence. Our analysis provides insight into the physical limits of DNN efficiency and suggests potential directions for developing energy-efficient AI architectures that leverage reversible analog computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09980v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexei V. Tkachenko</dc:creator>
    </item>
    <item>
      <title>Deep source separation of overlapping gravitational-wave signals and non-stationary noise artifacts</title>
      <link>https://arxiv.org/abs/2503.10398</link>
      <description>arXiv:2503.10398v1 Announce Type: cross 
Abstract: The Laser Interferometer Space Antenna (LISA) will observe gravitational waves in the millihertz frequency band, detecting signals from a vast number of astrophysical sources embedded in instrumental noise. Extracting individual signals from these overlapping contributions is a fundamental challenge in LISA data analysis and is traditionally addressed using computationally expensive stochastic Bayesian techniques. In this work, we present a deep learning-based framework for blind source separation in LISA data, employing an encoder-decoder architecture commonly used in digital audio processing to isolate individual signals within complex mixtures. Our approach enables signals from massive black-hole binaries, Galactic binaries, and instrumental glitches to be disentangled directly in a single step, circumventing the need for sequential source identification and subtraction. By learning clustered latent space representations, the framework provides a scalable alternative to conventional methods, with applications in both low-latency event detection and full-scale global-fit analyses. As a proof of concept, we assess the model's performance using simulated LISA data in a controlled setting with a limited number of overlapping sources. The results highlight deep source separation as a promising tool for LISA, paving the way for future extensions to more complex datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10398v1</guid>
      <category>astro-ph.IM</category>
      <category>gr-qc</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Houba</dc:creator>
    </item>
    <item>
      <title>Assessing high-order effects in feature importance via predictability decomposition</title>
      <link>https://arxiv.org/abs/2412.09964</link>
      <description>arXiv:2412.09964v2 Announce Type: replace 
Abstract: Leveraging the large body of work devoted in recent years to describe redundancy and synergy in multivariate interactions among random variables, we propose a novel approach to quantify cooperative effects in feature importance, one of the most used techniques for explainable artificial intelligence. In particular, we propose an adaptive version of a well-known metric of feature importance, named Leave One Covariate Out (LOCO), to disentangle high-order effects involving a given input feature in regression problems. LOCO is the reduction of the prediction error when the feature under consideration is added to the set of all the features used for regression. Instead of calculating the LOCO using all the features at hand, as in its standard version, our method searches for the multiplet of features that maximize LOCO and for the one that minimize it. This provides a decomposition of the LOCO as the sum of a two-body component and higher-order components (redundant and synergistic), also highlighting the features that contribute to building these high-order effects alongside the driving feature. We report the application to proton/pion discrimination from simulated detector measures by GEANT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09964v2</guid>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.111.L033301</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 111, L033301 (2025)</arxiv:journal_reference>
      <dc:creator>Marlis Ontivero-Ortega, Luca Faes, Jesus M Cortes, Daniele Marinazzo, Sebastiano Stramaglia</dc:creator>
    </item>
  </channel>
</rss>
