<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bias and Coverage Properties of the WENDy-IRLS Algorithm</title>
      <link>https://arxiv.org/abs/2510.03365</link>
      <description>arXiv:2510.03365v1 Announce Type: new 
Abstract: The Weak form Estimation of Nonlinear Dynamics (WENDy) method is a recently proposed class of parameter estimation algorithms that exhibits notable noise robustness and computational efficiency. This work examines the coverage and bias properties of the original WENDy-IRLS algorithm's parameter and state estimators in the context of the following differential equations: Logistic, Lotka-Volterra, FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction Benchmark. The estimators' performance was studied in simulated data examples, under four different noise distributions (normal, log-normal, additive censored normal, and additive truncated normal), and a wide range of noise, reaching levels much higher than previously tested for this algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03365v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abhi Chawla, David M. Bortz, Vanja Dukic</dc:creator>
    </item>
    <item>
      <title>Multivariate Zero-Inflated Causal Model for Regional Mobility Restriction Effects on Consumer Spending</title>
      <link>https://arxiv.org/abs/2510.03422</link>
      <description>arXiv:2510.03422v1 Announce Type: new 
Abstract: The COVID-19 pandemic presents challenges to both public health and the economy. Our objective is to examine how household expenditure, a significant component of private demand, reacts to changes in mobility. This investigation is crucial for developing policies that balance public health and the economic and social impacts. We utilize extensive scanner data from a major retail chain in India and Google mobility data to address this important question. However, there are a few challenges, including outcomes with excessive zeros and complicated correlations, time-varying confounding, and irregular observation times. We propose incorporating a multiplicative structural nested mean model with inverse intensity weighting techniques to tackle these challenges. Our framework allows semiparametric/nonparametric estimation for nuisance functions. The resulting rate doubly robust estimator enables the use of a conventional sandwich variance estimator without taking into account the variability introduced by these flexible estimation methods. We demonstrate the properties of our method theoretically and further validate it through simulation studies. Using the Indian consumer spending data and Google mobility data, our method reveals that the substantial reduction in mobility has a significant impact on consumers' fresh food expenditure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03422v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taekwon Hong, Wenbin Lu, Shu Yang, Pulak Ghosh</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for High-Dimensional Linear Regression via Adaptive Shrinkage</title>
      <link>https://arxiv.org/abs/2510.03449</link>
      <description>arXiv:2510.03449v1 Announce Type: new 
Abstract: We introduce BLAST, Bayesian Linear regression with Adaptive Shrinkage for Transfer, a Bayesian multi-source transfer learning framework for high-dimensional linear regression. The proposed analytical framework leverages global-local shrinkage priors together with Bayesian source selection to balance information sharing and regularization. We show how Bayesian source selection allows for the extraction of the most useful data sources, while discounting biasing information that may lead to negative transfer. In this framework, both source selection and sparse regression are jointly accounted for in prediction and inference via Bayesian model averaging. The structure of our model admits efficient posterior simulation via a Gibbs sampling algorithm allowing full posterior inference for the target regression coefficients, making BLAST both computationally practical and inferentially straightforward. Our method achieves more accurate posterior inference for the target than regularization approaches based on target data alone, while offering competitive predictive performance and superior uncertainty quantification compared to current state-of-the-art transfer learning methods. We validate its effectiveness through extensive simulation studies and illustrate its analytical properties when applied to a case study on the estimation of tumor mutational burden from gene expression, using data from The Cancer Genome Atlas (TCGA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03449v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parsa Jamshidian, Donatello Telesca</dc:creator>
    </item>
    <item>
      <title>Making high-order asymptotics practical: correcting goodness-of-fit test for astronomical count data</title>
      <link>https://arxiv.org/abs/2510.03466</link>
      <description>arXiv:2510.03466v1 Announce Type: new 
Abstract: The C statistic is a widely used likelihood-ratio statistic for model fitting and goodness-of-fit assessments with Poisson data in high-energy physics and astrophysics. Although it enjoys convenient asymptotic properties, the statistic is routinely applied in cases where its nominal null distribution relies on unwarranted assumptions. Because researchers do not typically carry out robustness checks, their scientific findings are left vulnerable to misleading significance calculations. With an emphasis on low-count scenarios, we present a comprehensive study of the theoretical properties of C statistics and related goodness-of-fit algorithms. We focus on common ``plug-in'' algorithms where moments of C are obtained by assuming the true parameter equals its estimate. To correct such methods, we provide a suite of new principled user-friendly algorithms and well-calibrated p-values that are ready for immediate deployment in the (astro)physics data-analysis pipeline. Using both theoretical and numerical results, we show (a) standard $\chi^2$-based goodness-of-fit assessments are invalid in low-count settings, (b) naive methods (e.g., vanilla bootstrap) result in biased null distributions, and (c) the corrected Z-test based on conditioning and high-order asymptotics gives the best precision with low computational cost. We illustrate our methods via a suite of simulations and applied astrophysical analyses. An open-source Python package is provided in a GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03466v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoli Li, Yang Chen, Xiao-Li Meng, David van Dyk, Massimiliano Bonamente, Vinay Kashyap</dc:creator>
    </item>
    <item>
      <title>Improving Sensitivity in A/B Tests: Integrating CUPED with Trimmed Mean Techniques</title>
      <link>https://arxiv.org/abs/2510.03468</link>
      <description>arXiv:2510.03468v1 Announce Type: new 
Abstract: Accurate estimation of treatment effects in online A/B testing is challenging with zero-inflated and skewed metrics. Traditional tests, like Welch's t-test, often lack sensitivity with heavy-tailed data due to their reliance on means, as opposed to e.g., percentiles. The Controlled Experiments Using Pre-experiment Data (CUPED) technique improves sensitivity by reducing variance, yet that variance reduction is insufficient for highly skewed metrics. Alternatively, Yuen's t-test uses trimmed means to robustly handle outliers and skewness. This paper introduces a method that combines the variance reduction of CUPED with the robustness of Yuen's t-test to enhance hypothesis testing sensitivity. Our novel approach integrates trimmed data in a principled manner, offering a framework that balances variance reduction with robust location measures. We demonstrate improved detection of significant effects with smaller sample sizes, enabling quicker experimental decisions without sacrificing statistical power. This work broadens the utility of controlled experiments in environments characterized by highly skewed or high-variance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03468v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kevin Charette, Tristan Boudreault</dc:creator>
    </item>
    <item>
      <title>Comparison of Parametric versus Machine-learning Multiple Imputation in Clinical Trials with Missing Continuous Outcomes</title>
      <link>https://arxiv.org/abs/2510.03512</link>
      <description>arXiv:2510.03512v1 Announce Type: new 
Abstract: The use of flexible machine-learning (ML) models to generate imputations of missing data within the framework of Multiple Imputation (MI) has recently gained traction, particularly in observational settings. For randomised controlled trials (RCTs), it is unclear whether ML approaches to MI provide valid inference, and whether they outperform parametric MI approaches under complex data generating mechanisms. We conducted two simulations in RCT settings that have incomplete continuous outcomes but fully observed covariates. We compared Complete Cases, standard MI (MI-norm), MI with predictive mean matching (MI-PMM) and ML-based approaches to MI, including classification and regression trees (MI-CART), Random Forests (MI-RF) and SuperLearner when outcomes are missing completely at random or missing at random conditional on treatment/covariate. The first simulation explored non-linear covariate-outcome relationships in the presence/absence of covariate-treatment interactions. The second simulation explored skewed repeated measures, motivated by a trial with digital outcomes. In the absence of interactions, we found that Complete Cases yields reliable inference; MI-norm performs similarly, except when missingness depends on the covariate. ML approaches can lead to smaller mean squared error than Complete Cases and MI-norm in specific non-linear settings, but provide unreliable inference for others. MI-PMM can lead to unreliable inference in several settings. In the presence of complex treatment-covariate interactions, performing MI separately by arm, either with MI-norm, MI-RF or MI-CART, provides inference that has comparable or with better properties compared to Complete Cases when the analysis model omits the interaction. For ML approaches, we observed unreliable inference in terms of bias in the estimated effect and/or its standard error when Rubin's Rules are implemented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03512v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mia S. Tackney, Jonathan W. Bartlett, Elizabeth Williamson, Kim May Lee</dc:creator>
    </item>
    <item>
      <title>Restrictions of PCBNs for integration-free computations</title>
      <link>https://arxiv.org/abs/2510.03518</link>
      <description>arXiv:2510.03518v1 Announce Type: new 
Abstract: The pair-copula Bayesian Networks (PCBN) are graphical models composed of a directed acyclic graph (DAG) that represents (conditional) independence in a joint distribution. The nodes of the DAG are associated with marginal densities, and arcs are assigned with bivariate (conditional) copulas following a prescribed collection of parental orders. The choice of marginal densities and copulas is unconstrained. However, the simulation and inference of a PCBN model may necessitate possibly high-dimensional integration.
  We present the full characterization of DAGs that do not require any integration for density evaluation or simulations. Furthermore, we propose an algorithm that can find all possible parental orders that do not lead to (expensive) integration. Finally, we show the asymptotic normality of estimators of PCBN models using stepwise estimating equations. Such estimators can be computed effectively if the PCBN does not require integration. A simulation study shows the good finite-sample properties of our estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03518v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny, Niels Horsman, Dorota Kurowicka</dc:creator>
    </item>
    <item>
      <title>Penalized mixed models to adjust for batch effects and unobserved confounding in high dimensional regression</title>
      <link>https://arxiv.org/abs/2510.03531</link>
      <description>arXiv:2510.03531v1 Announce Type: new 
Abstract: Confounding can lead to spurious associations. Typically, one must observe confounders in order to adjust for them, but in high-dimensional settings, recent research has shown that it becomes possible to adjust even for unobserved confounders. The methods for carrying out these adjustments, however, have not been thoroughly investigated. In this study, we derive a confounding framework in which the signal, bias, and variability can be cleanly partitioned and quantified, thereby enabling simulations in which one varies the bias-to-signal ratio while holding the signal-to-noise ratio fixed. Using this construction, we demonstrate the impact of the amount and complexity of unobserved confounding on the performance of competing methods, including the LASSO, principal components LASSO (PC-LASSO), and penalized linear mixed models (PLMMs). We identify scenarios in which each method outperforms the others and find that overall, PLMM is the most robust approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03531v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujing Lu, Patrick Breheny</dc:creator>
    </item>
    <item>
      <title>Handling Missing Data in Probabilistic Regression Trees: Methods and Implementation in R</title>
      <link>https://arxiv.org/abs/2510.03634</link>
      <description>arXiv:2510.03634v1 Announce Type: new 
Abstract: Probabilistic Regression Trees (PRTrees) generalize traditional decision trees by incorporating probability functions that associate each data point with different regions of the tree, providing smooth decisions and continuous responses. This paper introduces an adaptation of PRTrees capable of handling missing values in covariates through three distinct approaches: (i) a uniform probability method, (ii) a partial observation approach, and (iii) a dimension-reduced smoothing technique. The proposed methods preserve the interpretability properties of PRTrees while extending their applicability to incomplete datasets. Simulation studies under MCAR conditions demonstrate the relative performance of each approach, including comparisons with traditional regression trees on smooth function estimation tasks. The proposed methods, together with the original version, have been developed in R with highly optimized routines and are distributed in the PRTree package, publicly available on CRAN. In this paper we also present and discuss the main functionalities of the PRTree package, providing researchers and practitioners with new tools for incomplete data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03634v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Taiane Schaedler Prass, Alisson Silva Neimaier, Guilherme Pumi</dc:creator>
    </item>
    <item>
      <title>Efficient Log-Rank Updates for Random Survival Forests</title>
      <link>https://arxiv.org/abs/2510.03665</link>
      <description>arXiv:2510.03665v1 Announce Type: new 
Abstract: Random survival forests are widely used for estimating covariate-conditional survival functions under right-censoring. Their standard log-rank splitting criterion is typically recomputed at each candidate split. This O(M) cost per split, with M the number of distinct event times in a node, creates a bottleneck for large cohort datasets with long follow-up. We revisit approximations proposed by LeBlanc and Crowley (1995) and develop simple constant-time updates for the log-rank criterion. The method is implemented in grf and substantially reduces training time on large datasets while preserving predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03665v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Sverdrup, James Yang, Michael LeBlanc</dc:creator>
    </item>
    <item>
      <title>Beyond Regularization: Inherently Sparse Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2510.03729</link>
      <description>arXiv:2510.03729v1 Announce Type: new 
Abstract: Sparse principal component analysis (sparse PCA) is a widely used technique for dimensionality reduction in multivariate analysis, addressing two key limitations of standard PCA. First, sparse PCA can be implemented in high-dimensional low sample size settings, such as genetic microarrays. Second, it improves interpretability as components are regularized to zero. However, over-regularization of sparse singular vectors can cause them to deviate greatly from the population singular vectors, potentially misrepresenting the data structure. Additionally, sparse singular vectors are often not orthogonal, resulting in shared information between components, which complicates the calculation of variance explained. To address these challenges, we propose a methodology for sparse PCA that reflects the inherent structure of the data matrix. Specifically, we identify uncorrelated submatrices of the data matrix, meaning that the covariance matrix exhibits a sparse block diagonal structure. Such sparse matrices commonly occur in high-dimensional settings. The singular vectors of such a data matrix are inherently sparse, which improves interpretability while capturing the underlying data structure. Furthermore, these singular vectors are orthogonal by construction, ensuring that they do not share information. We demonstrate the effectiveness of our method through simulations and provide real data applications. Supplementary materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03729v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan O. Bauer</dc:creator>
    </item>
    <item>
      <title>A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling</title>
      <link>https://arxiv.org/abs/2510.04087</link>
      <description>arXiv:2510.04087v1 Announce Type: new 
Abstract: Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is \textit{better}, but what is \textit{good enough}. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04087v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.21203/rs.3.rs-7594024/v1</arxiv:DOI>
      <dc:creator>Hyung Gyu Rho</dc:creator>
    </item>
    <item>
      <title>Out-of-bag prediction balls for random forests in metric spaces</title>
      <link>https://arxiv.org/abs/2510.04299</link>
      <description>arXiv:2510.04299v1 Announce Type: new 
Abstract: Statistical methods for metric spaces provide a general and versatile framework for analyzing complex data types. We introduce a novel approach for constructing confidence regions around new predictions from any bagged regression algorithm with metric-space-valued responses. This includes the recent extensions of random forests for metric responses: Fr\'echet random forests (Capitaine et al., 2024), random forest weighted local constant Fr\'echet regression (Qiu et al., 2024), and metric random forests (Bult\'e and S{\o}rensen, 2024). Our prediction regions leverage out-of-bag observations generated during a single forest training, employing the entire data set for both prediction and uncertainty quantification. We establish asymptotic guarantees of out-of-bag prediction balls for four coverage types under certain regularity conditions. Moreover, we demonstrate the superior stability and smaller radius of out-of-bag balls compared to split-conformal methods through extensive numerical experiments where the response lies on the Euclidean space, sphere, hyperboloid, and space of positive definite matrices. A real data application illustrates the potential of the confidence regions for quantifying the uncertainty in the study of solar dynamics and the use of data-driven non-isotropic distances on the sphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04299v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Diego Serrano, Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>One-shot variable-ratio matching with fine balance</title>
      <link>https://arxiv.org/abs/2510.04383</link>
      <description>arXiv:2510.04383v1 Announce Type: new 
Abstract: Variable-ratio matching is a flexible alternative to conventional $1$-to-$k$ matching for designing observational studies that emulate a target randomized controlled trial (RCT). To achieve fine balance -- that is, matching treated and control groups to have the same marginal distribution on selected covariates -- conventional approaches typically partition the data into strata based on estimated entire numbers and then perform a series of $1$-to-$k$ matches within each stratum, with $k$ determined by the stratum-specific entire number. This ``divide-and-conquer" strategy has notable limitations: (1) fine balance typically does not hold in the final pooled sample, and (2) more controls may be discarded than necessary. To address these limitations, we propose a one-shot variable-ratio matching algorithm. Our method produces designs with exact fine balance on selected covariates in the matched sample, mimicking a hypothetical RCT where units are first grouped into sets of different sizes and one unit within each set is assigned to treatment while others to control. Moreover, our method achieves comparable or superior balance across many covariates and retains more controls in the final matched design, compared to the ``divide-and-conquer" approach. We demonstrate the advantages of the proposed design over the conventional approach via simulations and using a dataset studying the effect of right heart catheterization on mortality among critically ill patients. The algorithm is implemented in the R package match2C.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04383v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Meng, Chen Zhe, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Two new approaches to multiple canonical correlation analysis for repeated measures data</title>
      <link>https://arxiv.org/abs/2510.04457</link>
      <description>arXiv:2510.04457v1 Announce Type: new 
Abstract: In classical canonical correlation analysis (CCA), the goal is to determine the linear transformations of two random vectors into two new random variables that are most strongly correlated. Canonical variables are pairs of these new random variables, while canonical correlations are correlations between these pairs. In this paper, we propose and study two generalizations of this classical method:
  (1) Instead of two random vectors we study more complex data structures that appear in important applications. In these structures, there are $L$ features, each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objects over $T$ time points. We derive a suitable analog of the CCA for such data. Our approach relies on embeddings into Reproducing Kernel Hilbert Spaces, and covers several related data structures as well.
  (2) We develop an analogous approach for multidimensional random processes. In this case, the experimental units are multivariate continuous, square-integrable functions over a given interval. These functions are modeled as elements of a Hilbert space, so in this case, we define the multiple functional canonical correlation analysis, MFCCA.
  We justify our approaches by their application to two data sets and suitable large sample theory. We derive consistency rates for the related transformation and correlation estimators, and show that it is possible to relax two common assumptions on the compactness of the underlying cross-covariance operators and the independence of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04457v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz G\'orecki, Miros{\l}aw Krzy\'sko, Felix Gnettner, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>MUSE: Multi-Treatment Experiment Design for Winner Selection and Effect Estimation</title>
      <link>https://arxiv.org/abs/2510.04489</link>
      <description>arXiv:2510.04489v1 Announce Type: new 
Abstract: We study the design of experiments with multiple treatment levels, a setting common in clinical trials and online A/B/n testing. Unlike single-treatment studies, practical analyses of multi-treatment experiments typically first select a winning treatment, and then only estimate the effect therein. Motivated by this analysis paradigm, we propose a design for MUlti-treatment experiments that jointly maximizes the accuracy of winner Selection and effect Estimation (MUSE). Explicitly, we introduce a single objective that balances selection and estimation, and determine the unit allocation to treatments and control by optimizing this objective. Theoretically, we establish finite-sample guarantees and asymptotic equivalence between our proposal and the Neyman allocation for the true optimal treatment and control. Across simulations and a real data application, our method performs favorably in both selection and estimation compared to various standard alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04489v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiachen Xu, Jian Qian, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Selection Bias in Hybrid Randomized Controlled Trials using External Controls: A Simulation Study</title>
      <link>https://arxiv.org/abs/2510.04829</link>
      <description>arXiv:2510.04829v1 Announce Type: new 
Abstract: Hybrid randomized controlled trials (hybrid RCTs) integrate external control data, such as historical or concurrent data, with data from randomized trials. While numerous frequentist and Bayesian methods, such as the test-then-pool and Meta-Analytic-Predictive prior, have been developed to account for potential disagreement between the external control and randomized data, they cannot ensure strict type I error rate control. However, these methods can reduce biases stemming from systematic differences between external controls and trial data. A critical yet underexplored issue in hybrid RCTs is the prespecification of external data to be used in analysis.
  The validity of statistical conclusions in hybrid RCTs depends on the assumption that external control selection is independent of historical trials outcomes. In practice, historical data may be accessible during the planning stage, potentially influencing important decisions, such as which historical datasets to include or the sample size of the prospective part of the hybrid trial, thus introducing bias. Such data-driven design choices can be an additional source of bias, which can occur even when historical and prospective controls are exchangeable.
  Through a simulation study, we quantify the biases introduced by outcome-dependent selection of historical controls in hybrid RCTs using both Bayesian and frequentist approaches, and discuss potential strategies to mitigate this bias. Our scenarios consider variability and time trends in the historical studies, distributional shifts between historical and prospective control groups, sample sizes and allocation ratios, as well as the number of studies included. The impact of different rules for selecting external controls is demonstrated using a clinical trial example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04829v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Chang Chiam, Franz K\"onig, Martin Posch</dc:creator>
    </item>
    <item>
      <title>MCMC for State Space models</title>
      <link>https://arxiv.org/abs/2510.04932</link>
      <description>arXiv:2510.04932v1 Announce Type: new 
Abstract: A state-space model is a time-series model that has an unobserved latent process from which we take noisy measurements over time. The observations are conditionally independent given the latent process and the latent process itself is Markovian. These properties lead to simplifications for the conditional distribution of the latent process given the parameters and the observations. This chapter looks at how we can leverage the properties of state-space models to construct efficient MCMC samplers. We consider a range of Gibbs-sampler schemes, including those which use the forward-backward algorithm to simulate from the full conditional of the latent process given the parameters. For models where the forward-backward algorithm is not applicable we look at particle MCMC algorithms that, given the parameters, use particle filters to approximately simulate from the latent process or estimate the likelihood of the observations. Throughout, we provide intuition and informally discuss theory about the properties of the model that impact the efficiency of the different algorithms and how approaches such as reparameterization can improve mixing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04932v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Fearnhead, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>WOW: WAIC-Optimized Gating of Mixture Priors for External Data Borrowing</title>
      <link>https://arxiv.org/abs/2510.05085</link>
      <description>arXiv:2510.05085v1 Announce Type: new 
Abstract: The integration of external data using Bayesian mixture priors has become a powerful approach in clinical trials, offering significant potential to improve trial efficiency. Despite their strengths in analytical tractability and practical flexibility, existing methods such as the robust meta-analytic-predictive (rMAP) and self-adapting mixture (SAM) often presume borrowing without rigorously assessing whether, how, or when integration is appropriate. When external and concurrent data are discordant, excessive borrowing can bias estimates and lead to misleading conclusions. To address this, we introduce WOW, a Kullback-Leibler-based gating strategy guided by the widely applicable information criterion (WAIC). WOW conducts a preliminary compatibility assessment between external and concurrent trial data and gates the level of borrowing accordingly. The approach is prior-agnostic and can be seamlessly integrated with any mixture prior method, whether using fixed or adaptive weighting schemes, after the WOW step. Simulation studies demonstrate that incorporating the WOW strategy before Bayesian mixture prior borrowing methods effectively mitigates excessive borrowing and improves estimation accuracy. By providing robust and reliable inference, WOW strengthens the performance of mixture-prior methods and supports better decision-making in clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05085v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shouhao Zhou, Qiuxin Gao, Chenqi Fu, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model</title>
      <link>https://arxiv.org/abs/2510.03266</link>
      <description>arXiv:2510.03266v1 Announce Type: cross 
Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics, necessitating robust methods for detecting and analyzing anomalous behavior in plant productivity. This study presents a novel application of variational autoencoders (VAE) for identifying extreme events in gross primary productivity (GPP) from Community Earth System Model version 2 simulations across four AR6 regions in the Continental United States. We compare VAE-based anomaly detection with traditional singular spectral analysis (SSA) methods across three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario. The VAE architecture employs three dense layers and a latent space with an input sequence length of 12 months, trained on a normalized GPP time series to reconstruct the GPP and identifying anomalies based on reconstruction errors. Extreme events are defined using 5th percentile thresholds applied to both VAE and SSA anomalies. Results demonstrate strong regional agreement between VAE and SSA methods in spatial patterns of extreme event frequencies, despite VAE producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA across regions and periods). Both methods reveal increasing magnitudes and frequencies of negative carbon cycle extremes toward 2050-80, particularly in Western and Central North America. The VAE approach shows comparable performance to established SSA techniques, while offering computational advantages and enhanced capability for capturing non-linear temporal dependencies in carbon cycle variability. Unlike SSA, the VAE method does not require one to define the periodicity of the signals in the data; it discovers them from the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03266v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bharat Sharma, Jitendra Kumar</dc:creator>
    </item>
    <item>
      <title>Robust and efficient estimation for the Generalized Extreme-Value distribution with application to flood frequency analysis in the UK</title>
      <link>https://arxiv.org/abs/2510.03338</link>
      <description>arXiv:2510.03338v1 Announce Type: cross 
Abstract: A common approach for modeling extremes, such as peak flow or high temperatures, is the three-parameter Generalized Extreme-Value distribution. This is typically fit to extreme observations, here defined as maxima over disjoint blocks. This results in limited sample sizes and consequently, the use of classic estimators, such as the maximum likelihood estimator, may be inappropriate, as they are highly sensitive to outliers. To address these limitations, we propose a novel robust estimator based on the minimization of the density power divergence, controlled by a tuning parameter $\alpha$ that balances robustness and efficiency. When $\alpha = 0$, our estimator coincides with the maximum likelihood estimator; when $\alpha = 1$, it corresponds to the $L^2$ estimator, known for its robustness. We establish convenient theoretical properties of the proposed estimator, including its asymptotic normality and the boundedness of its influence function for $\alpha &gt; 0$. The practical efficiency of the method is demonstrated through empirical comparisons with the maximum likelihood estimator and other robust alternatives. Finally, we illustrate its relevance in a case study on flood frequency analysis in the UK and provide some general conclusions in Section 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03338v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huet, Ilaria Prosdocimi</dc:creator>
    </item>
    <item>
      <title>Exact and Approximate MCMC for Doubly-intractable Probabilistic Graphical Models Leveraging the Underlying Independence Model</title>
      <link>https://arxiv.org/abs/2510.03587</link>
      <description>arXiv:2510.03587v1 Announce Type: cross 
Abstract: Bayesian inference for doubly-intractable probabilistic graphical models typically involves variations of the exchange algorithm or approximate Markov chain Monte Carlo (MCMC) samplers. However, existing methods for both classes of algorithms require either perfect samplers or sequential samplers for complex models, which are often either not available, or suffer from poor mixing, especially in high dimensions. We develop a method that does not require perfect or sequential sampling, and can be applied to both classes of methods: exact and approximate MCMC. The key to our approach is to utilize the tractable independence model underlying an intractable probabilistic graphical model for the purpose of constructing a finite sample unbiased Monte Carlo (and not MCMC) estimate of the Metropolis--Hastings ratio. This innovation turns out to be crucial for scalability in high dimensions. The method is demonstrated on the Ising model. Gradient-based alternatives to construct a proposal, such as Langevin and Hamiltonian Monte Carlo approaches, also arise as a natural corollary to our general procedure, and are demonstrated as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03587v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Chen, Antik Chakraborty, Anindya Bhadra</dc:creator>
    </item>
    <item>
      <title>The analogy theorem in Hoare logic</title>
      <link>https://arxiv.org/abs/2510.03685</link>
      <description>arXiv:2510.03685v1 Announce Type: cross 
Abstract: The introduction of machine learning methods has led to significant advances in automation, optimization, and discoveries in various fields of science and technology. However, their widespread application faces a fundamental limitation: the transfer of models between data domains generally lacks a rigorous mathematical justification. The key problem is the lack of formal criteria to guarantee that a model trained on one type of data will retain its properties on another.This paper proposes a solution to this problem by formalizing the concept of analogy between data sets and models using first-order logic and Hoare logic.We formulate and rigorously prove a theorem that sets out the necessary and sufficient conditions for analogy in the task of knowledge transfer between machine learning models. Practical verification of the analogy theorem on model data obtained using the Monte Carlo method, as well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively.The proposed approach not only allows us to justify the correctness of transfer between domains but also provides tools for comparing the applicability of models to different types of data.The main contribution of the work is a rigorous formalization of analogy at the level of program logic, providing verifiable guarantees of the correctness of knowledge transfer, which opens new opportunities for both theoretical research and the practical use of machine learning models in previously inaccessible areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03685v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.LO</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikitin Nikita</dc:creator>
    </item>
    <item>
      <title>On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records</title>
      <link>https://arxiv.org/abs/2510.03844</link>
      <description>arXiv:2510.03844v1 Announce Type: cross 
Abstract: Objective: Electronic health records (EHR) data are prone to missingness and errors. Previously, we devised an "enriched" chart review protocol where a "roadmap" of auxiliary diagnoses (anchors) was used to recover missing values in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a missing hemoglobin A1c value would be considered unhealthy). Still, chart reviews are expensive and time-intensive, which limits the number of patients whose data can be reviewed. Now, we investigate the accuracy and scalability of a roadmap-driven algorithm, based on ICD-10 codes (International Classification of Diseases, 10th revision), to mimic expert chart reviews and recover missing values. Materials and Methods: In addition to the clinicians' original roadmap from our previous work, we consider new versions that were iteratively refined using large language models (LLM) in conjunction with clinical expertise to expand the list of auxiliary diagnoses. Using chart reviews for 100 patients from the EHR at an extensive learning health system, we examine algorithm performance with different roadmaps. Using the larger study of $1000$ patients, we applied the final algorithm, which used a roadmap with clinician-approved additions from the LLM. Results: The algorithm recovered as much, if not more, missing data as the expert chart reviewers, depending on the roadmap. Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing EHR data with similar accuracy to chart reviews and can feasibly be applied to large samples. Extending them to monitor other dimensions of data quality (e.g., plausability) is a promising future direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03844v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Abbey Collins, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon, Lucy D'Agostino McGowan</dc:creator>
    </item>
    <item>
      <title>Simulation-based inference via telescoping ratio estimation for trawl processes</title>
      <link>https://arxiv.org/abs/2510.04042</link>
      <description>arXiv:2510.04042v1 Announce Type: cross 
Abstract: The growing availability of large and complex datasets has increased interest in temporal stochastic processes that can capture stylized facts such as marginal skewness, non-Gaussian tails, long memory, and even non-Markovian dynamics. While such models are often easy to simulate from, parameter estimation remains challenging. Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. To address these challenges, we propose a fast and accurate, sample-efficient SBI framework for amortized posterior inference applicable to intractable stochastic processes. The proposed approach relies on two main steps: first, we learn the posterior density by decomposing it sequentially across parameter dimensions. Then, we use Chebyshev polynomial approximations to efficiently generate independent posterior samples, enabling accurate inference even when Markov chain Monte Carlo methods mix poorly. We further develop novel diagnostic tools for SBI in this context, as well as post-hoc calibration techniques; the latter not only lead to performance improvements of the learned inferential tool, but also to the ability to reuse it directly with new time series of varying lengths, thus amortizing the training cost. We demonstrate the method's effectiveness on trawl processes, a class of flexible infinitely divisible models that generalize univariate Gaussian processes, applied to energy demand data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04042v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Leonte, Rapha\"el Huser, Almut E. D. Veraart</dc:creator>
    </item>
    <item>
      <title>ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</title>
      <link>https://arxiv.org/abs/2510.04514</link>
      <description>arXiv:2510.04514v1 Announce Type: cross 
Abstract: Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04514v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)</title>
      <link>https://arxiv.org/abs/2510.04950</link>
      <description>arXiv:2510.04950v1 Announce Type: cross 
Abstract: The wording of natural language prompts has been shown to influence the performance of large language models (LLMs), yet the role of politeness and tone remains underexplored. In this study, we investigate how varying levels of prompt politeness affect model accuracy on multiple-choice questions. We created a dataset of 50 base questions spanning mathematics, science, and history, each rewritten into five tone variants: Very Polite, Polite, Neutral, Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we evaluated responses across these conditions and applied paired sample t-tests to assess statistical significance. Contrary to expectations, impolite prompts consistently outperformed polite ones, with accuracy ranging from 80.8% for Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from earlier studies that associated rudeness with poorer outcomes, suggesting that newer LLMs may respond differently to tonal variation. Our results highlight the importance of studying pragmatic aspects of prompting and raise broader questions about the social dimensions of human-AI interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04950v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Om Dobariya, Akhil Kumar</dc:creator>
    </item>
    <item>
      <title>Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning</title>
      <link>https://arxiv.org/abs/2510.04970</link>
      <description>arXiv:2510.04970v1 Announce Type: cross 
Abstract: We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. It pairs fast parent selection with iterative Cholesky-based score updates, cutting run-times over prior algorithms. This makes it feasible to fully embrace discrete search, enabling iterated local search with principled order initialization to find graphs with scores at or close to the global optimum. The resulting structures are highly accurate across benchmarks, with near-perfect recovery in standard settings. This performance calls for revisiting discrete search over graphs as a reasonable approach to causal discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04970v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcel Wien\"obst, Leonard Henckel, Sebastian Weichwald</dc:creator>
    </item>
    <item>
      <title>Pearson Chi-squared Conditional Randomization Test</title>
      <link>https://arxiv.org/abs/2111.00027</link>
      <description>arXiv:2111.00027v2 Announce Type: replace 
Abstract: Conditional independence (CI) testing arises naturally in many scientific problems and applications domains. The goal of this problem is to investigate the conditional independence between a response variable $Y$ and another variable $X$, while controlling for the effect of a high-dimensional confounding variable $Z$. In this paper, we introduce a novel test, called `Pearson Chi-squared Conditional Randomization' (PCR) test, which uses the distributional information on covariates $X,Z$ and constructs randomizations to test conditional independence. PCR leverages the i.i.d-ness property of the observations to obtain high-resolution p-values with a very small number of conditional randomizations. We also provide a power analysis of the PCR test, which captures the effect of various parameters of the test, the sample size and the distance of the alternative from the set of null distributions, measured in terms of a notion called `conditional relative density'. In addition, we propose two extensions of the PCR test, with important practical implications: $(i)$ parameter-free PCR, which uses Bonferroni's correction to decide on a tuning parameter in the test; $(ii)$ robust PCR, which avoids inflations in the size of the test when there is slight error in estimating the conditional law $P_{X|Z}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.00027v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adel Javanmard, Mohammad Mehrabi</dc:creator>
    </item>
    <item>
      <title>Quick Adaptive Ternary Segmentation: An Efficient Decoding Procedure For Hidden Markov Models</title>
      <link>https://arxiv.org/abs/2305.18578</link>
      <description>arXiv:2305.18578v2 Announce Type: replace 
Abstract: Hidden Markov models (HMMs) are characterized by an unobservable Markov chain and an observable process -- a noisy version of the hidden chain. Decoding the original signal from the noisy observations is one of the main goals in nearly all HMM based data analyses. Existing decoding algorithms such as Viterbi and the pointwise maximum a posteriori (PMAP) algorithm have computational complexity at best linear in the length of the observed sequence, and sub-quadratic in the size of the state space of the hidden chain.
  We present Quick Adaptive Ternary Segmentation (QATS), a divide-and-conquer procedure with computational complexity polylogarithmic in the length of the sequence, and cubic in the size of the state space, hence particularly suited for large scale HMMs with relatively few states. It also suggests an effective way of data storage as specific cumulative sums. In essence, the estimated sequence of states sequentially maximizes local likelihood scores among all local paths with at most three segments, and is meanwhile admissible. The maximization is performed only approximately using an adaptive search procedure. Our simulations demonstrate the speedups offered by QATS in comparison to Viterbi and PMAP, along with a precision analysis. An implementation of QATS is in the R-package QATS on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18578v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre M\"osching, Housen Li, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Divisive Hierarchical Clustering of Variables Identified by Singular Vectors</title>
      <link>https://arxiv.org/abs/2308.06820</link>
      <description>arXiv:2308.06820v4 Announce Type: replace 
Abstract: In this work, we introduce a novel methodology for divisive hierarchical clustering. Our divisive (``top-down'') approach is motivated by the fact that agglomerative hierarchical clustering (``bottom-up''), which is commonly used for hierarchical clustering, is not the best choice for all settings. The proposed methodology approximates the similarity matrix by a block diagonal matrix to identify clusters. While divisively clustering $p$ elements involves evaluating $2^{p-1}-1$ possible splits, which makes the task computationally costly, this approximation effectively reduces this number to at most $p(p-1)$ candidates, ensuring computational feasibility. We elaborate on the methodology and describe the incorporation of linkage functions to assess distances between clusters. We further show that these distances are ultrametric, ensuring that the resulting hierarchical cluster structure can be uniquely represented by a dendrogram, with interpretable heights. Additionally, the proposed methodology exhibits the flexibility to also optimize objectives of other clustering methods, and it can outperform these. The methodology is also applicable for constructing balanced clusters. To validate the efficiency of our approach, we conduct simulation studies and analyze real-world data. Supplementary materials for this article can be accessed online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06820v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan O. Bauer</dc:creator>
    </item>
    <item>
      <title>Addressing Duplicated Data in Spatial Point Patterns</title>
      <link>https://arxiv.org/abs/2405.15192</link>
      <description>arXiv:2405.15192v4 Announce Type: replace 
Abstract: Spatial point process models are widely applied to point pattern data from various applications in the social and environmental sciences. However, a serious hurdle in fitting point process models is the presence of duplicated points, wherein multiple observations share identical spatial coordinates. This often occurs because of decisions made in the geo-coding process, such as assigning representative locations (e.g., aggregate-level centroids) to observations when data producers lack exact location information. Because spatial point process models like the Log-Gaussian Cox Process (LGCP) assume unique locations, researchers often employ ad hoc solutions (e.g., removing duplicates or jittering) to address duplicated data before analysis. As an alternative, this study proposes a Modified Minimum Contrast (MMC) method that adapts the inference procedure to account for the effect of duplicates in estimation, without needing to alter the data. The proposed MMC method is applied to LGCP models, focusing on the inference of second-order intensity parameters, which govern the clustering structure of point patterns. Under a variety of simulated conditions, our results demonstrate the advantages of the proposed MMC method compared to existing ad hoc solutions. We then apply the MMC methods to a real-data application of conflict events in Afghanistan (2008-2009).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15192v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingling Chen, Mikyoung Jun, Scott J. Cook</dc:creator>
    </item>
    <item>
      <title>Fast Emulation, Modular Calibration, and Active Learning for Simulators with Functional Response</title>
      <link>https://arxiv.org/abs/2405.16298</link>
      <description>arXiv:2405.16298v2 Announce Type: replace 
Abstract: Scalable surrogate models enable efficient emulation of computer models (or simulators), particularly when dealing with large ensembles of runs. While Gaussian process (GP) models are commonly employed for emulation, they face limitations in scaling to large datasets. Furthermore, when dealing with dense functional output, such as spatial or time-series data, additional complexities arise, requiring careful handling to ensure fast emulation. This work presents a highly scalable emulator for functional data incorporating local Gaussian process regression. The emulator utilizes global GP lengthscale parameter estimates to scale the input space, leading to a substantial improvement in prediction speed. We demonstrate that our fast approximation-based emulator can serve as a viable alternative to a fully Bayesian approach for functional response, while drastically reducing computational costs. The proposed emulator is applied to quickly calibrate a multiphysics continuum hydrodynamics simulator with a large ensemble of 20000 runs. The methods presented are implemented in the R package FlaGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16298v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grant Hutchings, Derek Bingham, Kellin Rumsey, Earl Lawrence</dc:creator>
    </item>
    <item>
      <title>A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data</title>
      <link>https://arxiv.org/abs/2407.03389</link>
      <description>arXiv:2407.03389v4 Announce Type: replace 
Abstract: In this paper, we present an information-theoretic method for clustering mixed-type data, that is, data consisting of both continuous and categorical variables. The proposed approach extends the Information Bottleneck principle to heterogeneous data through generalised product kernels, integrating continuous, nominal, and ordinal variables within a unified optimization framework. We address the following challenges: developing a systematic bandwidth selection strategy that equalises contributions across variable types, and proposing an adaptive hyperparameter updating scheme that ensures a valid solution into a predetermined number of potentially imbalanced clusters. Through simulations on 28,800 synthetic data sets and ten publicly available benchmarks, we demonstrate that the proposed method, named DIBmix, achieves superior performance compared to four established methods (KAMILA, K-Prototypes, FAMD with K-Means, and PAM with Gower's dissimilarity). Results show DIBmix particularly excels when clusters exhibit size imbalances, data contain low or moderate cluster overlap, and categorical and continuous variables are equally represented. The method presents a significant advantage over traditional centroid-based algorithms, establishing DIBmix as a competitive and theoretically grounded alternative for mixed-type data clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03389v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma, Angelos Markos</dc:creator>
    </item>
    <item>
      <title>Rethinking the handling of method failure in comparison studies</title>
      <link>https://arxiv.org/abs/2408.11594</link>
      <description>arXiv:2408.11594v3 Announce Type: replace 
Abstract: Comparison studies in methodological research are intended to compare methods in an evidence-based manner to help data analysts select a suitable method for their application. To provide trustworthy evidence, they must be carefully designed, implemented, and reported, especially given the many decisions made in planning and running. A common challenge in comparison studies is to handle the "failure" of one or more methods to produce a result for some (real or simulated) data sets, such that their performances cannot be measured in those instances. Despite an increasing emphasis on this topic in recent literature (focusing on non-convergence as a common manifestation), there is little guidance on proper handling and interpretation, and reporting of the chosen approach is often neglected. This paper aims to fill this gap and offers practical guidance on handling method failure in comparison studies. After exploring common handlings across various published comparison studies from classical statistics and predictive modeling, we show that the popular approaches of discarding data sets yielding failure (either for all or the failing methods only) and imputing are inappropriate in most cases. We then recommend a different perspective on method failure - viewing it as the result of a complex interplay of several factors rather than just its manifestation. Building on this, we provide recommendations on more adequate handlings of method failure derived from realistic considerations. In particular, we propose considering fallback strategies that directly reflect the behavior of real-world users. Finally, we illustrate our recommendations and the dangers of inadequate handling of method failure through two exemplary comparison studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11594v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Milena W\"unsch, Moritz Herrmann, Elisa Noltenius, Mattia Mohr, Tim P. Morris, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Co-factor analysis of citation networks</title>
      <link>https://arxiv.org/abs/2408.14604</link>
      <description>arXiv:2408.14604v2 Announce Type: replace 
Abstract: One compelling use of citation networks is to characterize papers by their relationships to the surrounding literature. We propose a method to characterize papers by embedding them into two distinct "co-factor" spaces: one describing how papers send citations, and the other describing how papers receive citations. This approach presents several challenges. First, older documents cannot cite newer documents, and thus it is not clear that co-factors are even identifiable. We resolve this challenge by developing a co-factor model for asymmetric adjacency matrices with missing lower triangles and showing that identification is possible. We then frame estimation as a matrix completion problem and develop a specialized implementation of matrix completion because prior implementations are memory bound in our setting. Simulations show that our estimator has promising finite sample properties, and that naive approaches fail to recover latent co-factor structure. We leverage our estimator to investigate 255,780 papers published in statistics journals from 1898 to 2024, resulting in the most comprehensive topic model of the statistics literature to date. We find interpretable co-factors corresponding to many statistical subfields, including time series, variable selection, spatial methods, graphical models, GLM(M)s, causal inference, multiple testing, quantile regression, semiparametrics, dimension reduction, and several more.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14604v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2394464</arxiv:DOI>
      <dc:creator>Alex Hayes, Karl Rohe</dc:creator>
    </item>
    <item>
      <title>Comparing causal parameters with many treatments and positivity violations</title>
      <link>https://arxiv.org/abs/2410.13522</link>
      <description>arXiv:2410.13522v3 Announce Type: replace 
Abstract: Comparing outcomes across treatments is essential in medicine and public policy. To do so, researchers typically estimate a set of parameters, possibly counterfactual, with each targeting a different treatment. Treatment-specific means are commonly used, but their identification requires a positivity assumption, that every subject has a non-zero probability of receiving each treatment. This is often implausible, especially when treatment can take many values. Causal parameters based on dynamic stochastic interventions offer robustness to positivity violations. However, comparing these parameters may fail to reflect the effects of the underlying target treatments because the parameters can depend on outcomes under non-target treatments. To clarify when two parameters targeting different treatments yield a useful comparison of treatment efficacy, we propose a comparability criterion: if the conditional treatment-specific mean for one treatment is greater than that for another, then the corresponding causal parameter should also be greater. Many standard parameters fail to satisfy this criterion, but we show that only a mild positivity assumption is needed to identify parameters that yield useful comparisons. We then provide two simple examples that satisfy this criterion and are identifiable under the milder positivity assumption: trimmed and smooth trimmed treatment-specific means with multi-valued treatments. For smooth trimmed treatment-specific means, we develop doubly robust-style estimators that attain parametric convergence rates under nonparametric conditions. We illustrate our methods with an analysis of dialysis providers in New York State.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13522v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Yiting Li, Sunjae Bae, Mara A. McAdams-DeMarco, Iv\'an D\'iaz, Wenbo Wu</dc:creator>
    </item>
    <item>
      <title>Fast nonparametric spectral density estimation from irregularly sampled data</title>
      <link>https://arxiv.org/abs/2503.00492</link>
      <description>arXiv:2503.00492v3 Announce Type: replace 
Abstract: We introduce a nonparametric spectral density estimator for continuous-time and continuous-space processes measured at fully irregular locations. Our estimator is constructed using a weighted nonuniform Fourier sum whose weights yield a high-accuracy quadrature rule with respect to a user-specified window function. The resulting estimator significantly reduces the aliasing seen in periodogram approaches and least squares spectral analysis, sidesteps the dangers of ill-conditioning of the nonuniform Fourier inverse problem, and can be adapted to a wide variety of irregular sampling settings. We describe methods for rapidly computing the necessary weights in various settings, making the estimator scalable to large datasets. We then provide a theoretical analysis of sources of bias, and close with demonstrations of the method's efficacy, including for processes that exhibit very slow spectral decay and are observed at up to a million locations in multiple dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00492v3</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Geoga, Paul G. Beckman</dc:creator>
    </item>
    <item>
      <title>Seeded Poisson Factorization: leveraging domain knowledge to fit topic models</title>
      <link>https://arxiv.org/abs/2503.02741</link>
      <description>arXiv:2503.02741v2 Announce Type: replace 
Abstract: Topic models are widely used for discovering latent thematic structures in large text corpora, yet traditional unsupervised methods often struggle to align with pre-defined conceptual domains. This paper introduces seeded Poisson Factorization (SPF), a novel approach that extends the Poisson Factorization (PF) framework by incorporating domain knowledge through seed words. SPF enables a structured topic discovery by modifying the prior distribution of topic-specific term intensities, assigning higher initial rates to pre-defined seed words. The model is estimated using variational inference with stochastic gradient optimization, ensuring scalability to large datasets.
  We present in detail the results of applying SPF to an Amazon customer feedback dataset, leveraging pre-defined product categories as guiding structures. SPF achieves superior performance compared to alternative guided probabilistic topic models in terms of computational efficiency and classification performance. Robustness checks highlight SPF's ability to adaptively balance domain knowledge and data-driven topic discovery, even in case of imperfect seed word selection. Further applications of SPF to four additional benchmark datasets, where the corpus varies in size and the number of topics differs, demonstrate its general superior classification performance compared to the unseeded PF model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02741v2</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.knosys.2025.114116</arxiv:DOI>
      <arxiv:journal_reference>Knowledge-Based Systems 327 (2025) 114116</arxiv:journal_reference>
      <dc:creator>Bernd Prostmaier, Jan V\'avra, Bettina Gr\"un, Paul Hofmarcher</dc:creator>
    </item>
    <item>
      <title>Clustered Flexible Calibration Plots For Binary Outcomes Using Random Effects Modeling</title>
      <link>https://arxiv.org/abs/2503.08389</link>
      <description>arXiv:2503.08389v2 Announce Type: replace 
Abstract: Evaluation of clinical prediction models across multiple clusters, whether centers or datasets, is becoming increasingly common. A comprehensive evaluation includes an assessment of the agreement between the estimated risks and the observed outcomes, also known as calibration. Calibration is of utmost importance for clinical decision making with prediction models and it may vary between clusters. We present three approaches to take clustering into account when evaluating calibration. (1) Clustered group calibration (CG-C), (2) two-stage meta-analysis calibration (2MA-C) and (3) mixed model calibration (MIX-C) can obtain flexible calibration plots with random effects modelling and providing confidence and prediction intervals. As a case example, we externally validate a model to estimate the risk that an ovarian tumor is malignant in multiple centers (N = 2489). We also conduct a simulation study and synthetic data study generated from a true clustered dataset to evaluate the methods. In the simulation study MIX-C and 2MA-C (splines) gave estimated curves closest to the true overall curve. In the synthetic data study MIX-C produced cluster specific curves closest to the truth. Coverage of the prediction interval across the plot was best for 2MA-C with splines. We recommend using 2MA-C with splines to estimate the overall curve and the 95% PI and MIX-C for the cluster specific curves, especially when sample size per cluster is limited. We provide ready-to-use code to construct summary flexible calibration curves with confidence and prediction intervals to assess heterogeneity in calibration across datasets or centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08389v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lasai Barre\~nada, Bavo D. C. Campo, Laure Wynants, Ben Van Calster</dc:creator>
    </item>
    <item>
      <title>False Discovery estimation in Record Linkage</title>
      <link>https://arxiv.org/abs/2503.20627</link>
      <description>arXiv:2503.20627v3 Announce Type: replace 
Abstract: Integrating data from multiple sources expands research opportunities at low cost. However, due to different data collection processes and privacy constraints, unique identifiers are unavailable. Record Linkage (RL) algorithms address this by probabilistically linking records based on partially identifying variables. Since these variables lack the strength to perfectly combine information, RL procedures yield an imperfect set of linked records. Therefore, assessing the false discovery proportion (FDP) in RL is crucial for ensuring the reliability of subsequent analyses. In this paper, we introduce a novel method for estimating the FDP in RL for two overlapping data sets. We synthesise data from their estimated empirical distribution and use it along with real data in the linkage process. Since synthetic records cannot form links with real entities, they provide a means to estimate the amount of falsely linked pairs. Notably, this method applies to all RL techniques and across diverse settings where links and non-links have similar distributions -- typical in complex tasks with poorly discriminative linking variables and multiple records sharing similar information while representing different entities. By identifying the FDP in RL and selecting suitable model parameters, our approach enables to assess and improve the reliability of linked data. We evaluate its performance using established RL algorithms and benchmark data applications before deploying it to link siblings from the Netherlands Perinatal Registry, where the reliability of previous RL applications has never been confirmed. Through this application, we highlight the importance of accounting for linkage errors when studying mother-child dynamics in healthcare records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20627v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.70292</arxiv:DOI>
      <dc:creator>Kayan\'e Robach, Michel H. Hof, Mark A. van de Wiel</dc:creator>
    </item>
    <item>
      <title>Scalable and robust regression models for continuous proportional data</title>
      <link>https://arxiv.org/abs/2504.15269</link>
      <description>arXiv:2504.15269v2 Announce Type: replace 
Abstract: Beta regression is used routinely for continuous proportional data, but it often encounters practical issues such as a lack of robustness to misspecification of the beta distribution and sensitivity to outliers. We develop an improved class of generalized linear models starting with the continuous binomial (cobin) distribution and further extending to dispersion mixtures of cobin distributions (micobin). The proposed cobin regression and micobin regression models have attractive robustness, computation, and flexibility properties. A key innovation is the Kolmogorov-Gamma data augmentation scheme, which facilitates Gibbs sampling for Bayesian computation, including in hierarchical cases involving nested, longitudinal, or spatial data. We demonstrate robustness, ability to handle responses exactly at the boundary (0 or 1), and computational efficiency relative to beta regression in simulation experiments and through analysis of the benthic macroinvertebrate multimetric index of US lakes using lake watershed covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15269v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, Benjamin K. Dahl, Otso Ovaskainen, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Conditional inference for high-dimensional multi-omics survival data</title>
      <link>https://arxiv.org/abs/2504.21324</link>
      <description>arXiv:2504.21324v2 Announce Type: replace 
Abstract: Multi-omics data present significant challenges for statistical inference due to the complex interdependencies among biological layers. In this paper, we introduce a novel Multi-Omics Factor-Adjusted Cox (MOFA-Cox) model for analyzing multi-omics survival data, effectively addressing the intricate correlations across various omics layers. We provide a factor-adjusted decorrelated score test for the MOFA-Cox model in high-dimensional survival analysis. Our method accommodates situations where the dimension of the parameters being tested exceeds the sample size, while not imposing a sparsity assumption on them. We establish the limiting null distribution of the proposed test and analyze its power under local alternatives. Numerical studies and an application to the TCGA breast cancer dataset demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21324v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyuan Zhang, Meiling Hao, Lianqiang Qu, Liuquan Sun</dc:creator>
    </item>
    <item>
      <title>Association and Independence Test for Random Objects</title>
      <link>https://arxiv.org/abs/2505.01983</link>
      <description>arXiv:2505.01983v3 Announce Type: replace 
Abstract: We develop a unified framework for testing independence and quantifying association between random objects that are located in general metric spaces. Special cases include functional and high-dimensional data as well as networks, covariance matrices and data on Riemannian manifolds, among other metric space-valued data. A key concept is the profile association, a measure based on distance profiles that intrinsically characterize the distributions of random objects in metric spaces. We rigorously establish a connection between the Hoeffding D statistic and the profile association and derive a permutation test with theoretical guarantees for consistency and power under alternatives to the null hypothesis of independence/no association. We extend this framework to the conditional setting, where the independence between random objects given a Euclidean predictor is of interest. In simulations across various metric spaces, the proposed profile independence test is found to outperform existing approaches. The practical utility of this framework is demonstrated with applications to brain connectivity networks derived from magnetic resonance imaging and age-at-death distributions for males and females obtained from human mortality data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01983v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Sparse Clustering via Iterative Semidefinite Programming Relaxed K-Means</title>
      <link>https://arxiv.org/abs/2505.20478</link>
      <description>arXiv:2505.20478v2 Announce Type: replace 
Abstract: We study high-dimensional clustering where the true signal is sparse among features. To help design algorithms that do not rely on precise estimation of sparse nuisance parameters or signal feature set, we establish minimax separation for exact cluster recovery of semidefinite programming (SDP) relaxation of $K$-means using varying subsets of features. It highlights the critical role of feature selection and, at the same time, shows that the signal feature set can be slightly overestimated without compromising clustering performance. Guided by this theory, our method alternates between rough feature selection and clustering: feature selection is performed by thresholding a rough estimate of the discriminative direction, while clustering is carried out via an SDP-relaxed $K$-means. We further extend the method to settings with unknown sparse precision matrices, avoiding full model parameter estimation by computing only the minimally required quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20478v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongmin Mun, Paromita Dubey, Yingying Fan</dc:creator>
    </item>
    <item>
      <title>Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models</title>
      <link>https://arxiv.org/abs/2506.09850</link>
      <description>arXiv:2506.09850v3 Announce Type: replace 
Abstract: The usefulness of Bayesian models for density and cluster estimation is well established across multiple literatures. However, there is still a known tension between the use of simpler, more interpretable models and more flexible, complex ones. In this paper, we propose a novel method that integrates these two approaches by projecting the fit of a flexible, overparameterized model onto a lower-dimensional parametric surrogate, which serves as a summary. This process increases interpretability while preserving most of the fit of the original model. Our approach involves three main steps. First, we fit the data using nonparametric or overparameterized models. Second, we project the posterior predictive distribution of the original model onto a sequence of parametric summary point estimates with varying dimensions using a decision-theoretic approach. Finally, given the parametric summary estimate, obtained in the second step, that best approximates the original model, we construct uncertainty quantification for this summary by projecting the original posterior distribution. We demonstrate the effectiveness of our method for generating summaries for both nonparametric and overparameterized models, delivering both point estimates and uncertainty quantification for density and cluster summaries across synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09850v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrique Bolfarine, Hedibert F. Lopes, Carlos M. Carvalho</dc:creator>
    </item>
    <item>
      <title>Unveiling Complex Territorial Socio-Economic Dynamics: A Statistical Mechanics Approach</title>
      <link>https://arxiv.org/abs/2506.16872</link>
      <description>arXiv:2506.16872v3 Announce Type: replace 
Abstract: This study proposes a novel approach based on the Ising model for analyzing socio-economic emerging patterns between municipalities by investigating the observed configuration of a network of selected territorial units which are classified as being central hubs or peripheral areas. This is interpreted as being a reference of a system of interacting territorial binary units. The socio-economic structure of the municipalities is synthesized into interpretable composite indices, which are further aggregated by means of Principal Components Analysis in order to reduce dimensionality and construct a univariate external field compatible with the Ising framework. Monte Carlo simulations via parallel computing are conducted adopting a Simulated Annealing variant of the classic Metropolis-Hastings algorithm. This ensures an efficient local exploration of the configuration space in the neighbourhood of the reference of the system. Model consistency is assessed both in terms of energy stability and the likelihood of these configurations. The comparison between observed configuration and simulated ones is crucial in the analysis of multivariate phenomena, concomitantly accounting for territorial interactions. Model uncertainty in estimating the probability of each municipality being a central hub or peripheral area is quantified by adopting the model-agnostic Conformal Prediction framework which yields adaptive intervals with guaranteed coverage. The innovative use of geographical maps of the prediction intervals renders this approach an effective tool. It combines statistical mechanics, multivariate analysis and uncertainty quantification, providing a robust and interpretable framework for modeling socio-economic territorial dynamics, with potential applications in Official Statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16872v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierpaolo Massoli</dc:creator>
    </item>
    <item>
      <title>Learning growth mechanisms of tail realistic preferential attachment models from network degree distributions</title>
      <link>https://arxiv.org/abs/2506.18726</link>
      <description>arXiv:2506.18726v2 Announce Type: replace 
Abstract: Identifying the generating mechanism of a network is challenging as, more often than not, only snapshots are available, but not the full evolution. One candidate for the generating mechanism is preferential attachment which, in its simplest form, results in a degree distribution that follows the power law. Consequently, the growth of real-life networks that display such power-law behaviour is commonly modelled by preferential attachment. The ubiquity of the power law has been challenged by the presence of alternatives with comparable performance, as well as the recent findings that the tail of the degree distribution is often lighter than implied by the body, whilst still being regularly varying. In this paper, we propose a preferential attachment model with a flexible preference function. Using methods for discrete extremes, we characterise the tail behaviour of the limiting degree distribution directly by the form of the preference function. Directly relating the tail index to the model parameters enables them to be inferred when fitting to degree distributions alone, which is supported by simulation studies. Results of applications to real data are promising and comparable to alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18726v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Boughen, Clement Lee, Vianey Palacios Ramirez</dc:creator>
    </item>
    <item>
      <title>Measuring frailty in the elderly: an indicator based on a super-classifier</title>
      <link>https://arxiv.org/abs/2506.22349</link>
      <description>arXiv:2506.22349v2 Announce Type: replace 
Abstract: Identifying frail older adults in an ageing population is essential for improving healthcare services. This study proposes a composite indicator to assess individual frailty levels using administrative healthcare data. Given the complex and multidimensional nature of frailty, a multi-outcome approach is adopted. Following an extensive literature review, a set of adverse health events is selected as proxies for frailty. These events were modelled using logistic classifiers, with frailty determinants (associated to adverse health events, selected using a gradient tree boosting) serving as covariates. The sensitivity and specificity of each classifier is used to compose their combined likelihood. From this, we derive an indicator capable of quantifying frailty across the population. The indicator shows robust performance across multiple outcomes and over time. Its primary innovation lies in allowing the use of diverse and outcome-specific sets of frailty determinants without any structural constraint. Overall, we offer an effective tool for quantifying frailty among older adults, potentially supporting health authorities in the prevention of frailty-related adverse events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22349v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sara Rebottini, Pietro Belloni</dc:creator>
    </item>
    <item>
      <title>Doubly robust outlier resistant inference on causal treatment effect</title>
      <link>https://arxiv.org/abs/2507.17439</link>
      <description>arXiv:2507.17439v3 Announce Type: replace 
Abstract: Outliers can severely distort causal effect estimation in observational studies, especially in small samples. We develop a doubly robust estimator of the ATE under a contaminated-data model that explicitly accommodates outliers. Robustness to outliers is delivered via a bounded-influence estimating equation for the outcome model and covariate balancing propensity scores (CBPS) for treatment assignment. To mitigate overfitting in high dimensions, we incorporate variable selection and unify all components within a penalized empirical likelihood framework. For further inference, we derive an optimal finite-sample confidence interval (CI) whose endpoints are invariant to outliers under the contaminated model. Across extensive simulations and two gene-expression applications (Golub; Khan pediatric tumor), the proposed ATE estimator and finite-sample CI outperform state-of-the-art competitors in bias, mean squared error, empirical coverage, and interval length over a wide range of contamination levels and sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17439v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Juhyun Park, Saebom Jeon, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Estimating effects of longitudinal modified treatment policies (LMTPs) on rates of change in health outcomes</title>
      <link>https://arxiv.org/abs/2508.11131</link>
      <description>arXiv:2508.11131v2 Announce Type: replace 
Abstract: Longitudinal data often contains outcomes measured at multiple visits and scientific interest may lie in quantifying the effect of an intervention on an outcome's rate of change. For example, one may wish to study the progression (or trajectory) of a disease over time under different hypothetical interventions. We extend the longitudinal modified treatment policy (LMTP) methodology introduced in D\'iaz et al. (2023) to estimate effects of complex interventions on rates of change in an outcome over time. We exploit the theoretical properties of a nonparametric efficient influence function (EIF)-based estimator to introduce a novel inference framework that can be used to construct simultaneous confidence intervals for a variety of causal effects of interest and to formally test relevant global and local hypotheses about rates of change. We demonstrate the utility of our framework in investigating whether a longitudinal shift intervention affects an outcome's counterfactual trajectory, as compared with no intervention. We present results from a simulation study to illustrate the performance of our inference framework in a longitudinal setting with time-varying confounding and a continuous exposure. We also apply our inference framework to the Columbia Brain Health DataBank (CBDB) to examine the effect of shifting blood pressure on the progression of dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11131v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja Shahu, Weijie Xia, Ying Wei, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Generalized promotion time cure model: A new modeling framework to identify cell-type-specific genes and improve survival prognosis</title>
      <link>https://arxiv.org/abs/2509.01001</link>
      <description>arXiv:2509.01001v2 Announce Type: replace 
Abstract: Single-cell technologies provide an unprecedented opportunity for dissecting the interplay between the cancer cells and the associated tumor microenvironment, and the produced high-dimensional omics data should also augment existing survival modeling approaches for identifying tumor cell type-specific genes predictive of cancer patient survival. However, there is no statistical model to integrate multiscale data including individual-level survival data, multicellular-level cell composition data and cellular-level single-cell omics covariates. We propose a class of Bayesian generalized promotion time cure models (GPTCMs) for the multiscale data integration to identify cell-type-specific genes and improve cancer prognosis. We demonstrate with simulations in both low- and high-dimensional settings that the proposed Bayesian GPTCMs are able to identify cell-type-associated covariates and improve survival prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01001v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhao, Fatih K{\i}z{\i}laslan, Shixiong Wang, Manuela Zucknick</dc:creator>
    </item>
    <item>
      <title>An information metric for comparing and assessing informative interim decisions in sequential clinical trials</title>
      <link>https://arxiv.org/abs/2509.04904</link>
      <description>arXiv:2509.04904v3 Announce Type: replace 
Abstract: Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04904v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</dc:creator>
    </item>
    <item>
      <title>A queuing theory-based operating capacity model for multi-modal port operations</title>
      <link>https://arxiv.org/abs/2509.22961</link>
      <description>arXiv:2509.22961v2 Announce Type: replace 
Abstract: This paper investigates how "operating capacity" can be meaningfully defined in multi-modal maritime freight systems with limited data. Shipping channels and ports are complex systems that interact deeply, and the capacities of individual components may differ from the overall capacity of these systems. Traditional methods for port capacity assessment often rely on data-driven simulations, which can be difficult to calibrate due to complex interactions among port systems and the large volume of data required.
  We present a data-efficient alternative for estimating port capacities by developing a novel queuing theory-based formulation. We define the operating capacity of a port system as the maximum vessel arrival rate that can be sustained over an extended period of stable operations. Furthermore, we define terminal-level operating capacities for import and export processes in terms of the maximum achievable throughput for each process per unit time. Our approach requires only minimal data, which can be readily obtained from archival Automatic Identification System (AIS) vessel trajectories and historical port terminal logs, thereby making implementation both robust and efficient.
  We demonstrate the utility of the proposed method using data from the Port of Houston. The results suggest that the model is a viable approach for estimating port operating capacity. Specifically, the inbound operating capacity of the Port of Houston was estimated to be approximately 0.8 vessels per hour. At the Barbour's Cut Container Terminal, the operating capacities were found to be 57.8 containers per hour for import processes and 74.6 containers per hour for export processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22961v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debojjal Bagchi, Kyle Bathgate, Stephen D. Boyles, Kenneth N. Mitchell, Magdalena I. Asborno, Marin M. Kress</dc:creator>
    </item>
    <item>
      <title>Change-Point Testing for Risk Measures in Time Series</title>
      <link>https://arxiv.org/abs/1809.02303</link>
      <description>arXiv:1809.02303v3 Announce Type: replace-cross 
Abstract: We propose novel methods for change-point testing for nonparametric estimators of expected shortfall and related risk measures in weakly dependent time series. We can detect general multiple structural changes in the tails of marginal distributions of time series under general assumptions. Self-normalization allows us to avoid the issues of standard error estimation. The theoretical foundations for our methods are functional central limit theorems, which we develop under weak assumptions. An empirical study of S&amp;P 500 and US Treasury bond returns illustrates the practical use of our methods in detecting and quantifying instability in the tails of financial time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.02303v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Fan, Junting Duan, Peter W. Glynn, Markus Pelger</dc:creator>
    </item>
    <item>
      <title>KL-BSS: Rethinking optimality for neighbourhood selection in structural equation models</title>
      <link>https://arxiv.org/abs/2306.02244</link>
      <description>arXiv:2306.02244v3 Announce Type: replace-cross 
Abstract: We introduce a new method for neighbourhood selection in linear structural equation models that improves over classical methods such as best subset selection (BSS) and the Lasso. Our method, called KL-BSS, takes advantage of the existence of underlying structure in SEM -- even when this structure is unknown -- and is easily implemented using existing solvers. Under weaker eigenvalue conditions compared to BSS and the Lasso, KL-BSS can provably recover the support of linear models with fewer samples. We establish both the pointwise and minimax sample complexity for recovery, which KL-BSS obtains. Extensive experiments on both real and simulated data confirm the improvements offered by KL-BSS. While it is well-known that the Lasso encounters difficulties under structured dependencies, it is less well-known that even BSS runs into trouble as well, and can be substantially improved. These results have implications for structure learning in graphical models, which often relies on neighbourhood selection as a subroutine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02244v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Gao, Wai Ming Tai, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Optimal testing in a class of nonregular models</title>
      <link>https://arxiv.org/abs/2403.16413</link>
      <description>arXiv:2403.16413v2 Announce Type: replace-cross 
Abstract: This paper studies optimal hypothesis testing for nonregular econometric models with parameter-dependent support. We consider both one-sided and two-sided hypothesis testing and develop asymptotically uniformly most powerful tests based on a limit experiment. Our two-sided test becomes asymptotically uniformly most powerful without imposing further restrictions such as unbiasedness, and can be inverted to construct a confidence set for the nonregular parameter. Simulation results illustrate desirable finite sample properties of the proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16413v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Shimizu, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>Multiscale modelling of animal movement with persistent dynamics</title>
      <link>https://arxiv.org/abs/2406.15195</link>
      <description>arXiv:2406.15195v2 Announce Type: replace-cross 
Abstract: Wild animals are commonly fitted with trackers that record their position through time, and statistical models for tracking data broadly fall into two categories: models focused on small-scale movement decisions, and models for large-scale spatial distributions. Due to this dichotomy, it is challenging to describe mathematically how animals' distributions arise from their short-term movement patterns, and to combine data sets collected at different scales. We propose a multiscale model of animal movement and space use based on the underdamped Langevin process, widely used in statistical physics. The model is convenient to describe animal movement for three reasons: it is specified in continuous time (such that its parameters are not dependent on an arbitrary time scale), its speed and direction are autocorrelated (similarly to real animal trajectories), and it has a closed form stationary distribution that we can view as a model of long-term space use. We use the common form of a resource selection function for the stationary distribution, to model the environmental drivers behind the animal's movement decisions. We further increase flexibility by allowing movement parameters to be time-varying, and find conditions under which the stationary distribution is preserved. We derive an explicit mathematical link to step selection functions, commonly used in wildlife studies, providing new theoretical results about their scale-dependence. We formulate the underdamped Langevin model as a state-space model and present a computationally efficient method of inference based on the Kalman filter and a marginal likelihood approach for mixed effect extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15195v2</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Michelot, Ephraim M. Hanks</dc:creator>
    </item>
    <item>
      <title>A Martingale-Free Introduction to Conditional Gaussian Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2410.24056</link>
      <description>arXiv:2410.24056v2 Announce Type: replace-cross 
Abstract: The conditional Gaussian nonlinear system (CGNS) is a broad class of nonlinear stochastic dynamical systems. Given the trajectories for a subset of state variables, the remaining follow a Gaussian distribution. Despite the conditionally linear structure, the CGNS exhibits strong nonlinearity, thus capturing many non-Gaussian characteristics observed in nature through its joint and marginal distributions. Desirably, it enjoys closed analytic formulae for the time evolution of its conditional Gaussian statistics, which facilitate the study of data assimilation and other related topics. In this paper, we develop a martingale-free approach to improve the understanding of CGNSs. This methodology provides a tractable approach to proving the time evolution of the conditional statistics by deriving results through time discretization schemes, with the continuous-time regime obtained via a formal limiting process as the discretization time-step vanishes. This discretized approach further allows for developing analytic formulae for optimal posterior sampling of unobserved state variables with correlated noise. These tools are particularly valuable for studying extreme events and intermittency and apply to high-dimensional systems. Moreover, the approach improves the understanding of different sampling methods in characterizing uncertainty. The effectiveness of the framework is demonstrated through a physics-constrained, triad-interaction climate model with cubic nonlinearity and state-dependent cross-interacting noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24056v2</guid>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>nlin.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/e27010002</arxiv:DOI>
      <arxiv:journal_reference>Entropy 27, 1 (2025): 2</arxiv:journal_reference>
      <dc:creator>Marios Andreou, Nan Chen</dc:creator>
    </item>
    <item>
      <title>Vector Copula Variational Inference and Dependent Block Posterior Approximations</title>
      <link>https://arxiv.org/abs/2503.01072</link>
      <description>arXiv:2503.01072v2 Announce Type: replace-cross 
Abstract: The key to VI is the selection of a tractable density to approximate the Bayesian posterior. For large and complex models a common choice is to assume independence between multivariate blocks in a partition of the parameter space. While this simplifies the problem it can reduce accuracy. This paper proposes using vector copulas to capture dependence between the blocks parsimoniously. Tailored multivariate marginals are constructed using learnable transport maps. We call the resulting joint distribution a ``dependent block posterior'' approximation. Vector copula models are suggested that make tractable and flexible variational approximations. They allow for differing marginals, numbers of blocks, block sizes and forms of between block dependence. They also allow for solution of the variational optimization using efficient stochastic gradient methods. The approach is demonstrated using four different statistical models and 16 datasets which have posteriors that are challenging to approximate. This includes models that use global-local shrinkage priors for regularization, and hierarchical models for smoothing and heteroscedastic time series. In all cases, our method produces more accurate posterior approximations than benchmark VI methods that either assume block independence or factor-based dependence, at limited additional computational cost. A python package implementing the method is available on GitHub at https://github.com/YuFuOliver/VCVI_Rep_PyPackage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01072v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Fu, Michael Stanley Smith, Anastasios Panagiotelis</dc:creator>
    </item>
    <item>
      <title>Testing Random Effects for Binomial Data</title>
      <link>https://arxiv.org/abs/2504.13977</link>
      <description>arXiv:2504.13977v2 Announce Type: replace-cross 
Abstract: In modern scientific research, small-scale studies with limited participants are increasingly common. However, interpreting individual outcomes can be challenging, making it standard practice to combine data across studies using random effects to draw broader scientific conclusions. In this work, we introduce an optimal methodology for assessing the goodness of fit of a reference distribution for the random effects arising from binomial counts. For meta-analyses, we also derive optimal tests to evaluate whether multiple studies are in agreement before pooling the data. In all cases, we prove that the proposed tests optimally distinguish null and alternative hypotheses separated in the 1-Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13977v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Larry Wasserman, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
      <link>https://arxiv.org/abs/2504.19963</link>
      <description>arXiv:2504.19963v3 Announce Type: replace-cross 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We demonstrate the performance of the proposed method via several numerical examples in computational mechanics and structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19963v3</guid>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00466-025-02701-6</arxiv:DOI>
      <dc:creator>Akash Yadav, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Long-Tailed Classification</title>
      <link>https://arxiv.org/abs/2507.06867</link>
      <description>arXiv:2507.06867v2 Announce Type: replace-cross 
Abstract: Many real-world classification problems, such as plant identification, have extremely long-tailed class distributions. In order for prediction sets to be useful in such settings, they should (i) provide good class-conditional coverage, ensuring that rare classes are not systematically omitted from the prediction sets, and (ii) be a reasonable size, allowing users to easily verify candidate labels. Unfortunately, existing conformal prediction methods, when applied to the long-tailed setting, force practitioners to make a binary choice between small sets with poor class-conditional coverage or sets with very good class-conditional coverage but that are extremely large. We propose methods with guaranteed marginal coverage that smoothly trade off between set size and class-conditional coverage. First, we introduce a new conformal score function called prevalence-adjusted softmax that targets macro-coverage, a relaxed notion of class-conditional coverage. Second, we propose a new procedure that interpolates between marginal and class-conditional conformal prediction by linearly interpolating their conformal score thresholds. We demonstrate our methods on Pl@ntNet-300K and iNaturalist-2018, two long-tailed image datasets with 1,081 and 8,142 classes, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06867v2</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiffany Ding, Jean-Baptiste Fermanian, Joseph Salmon</dc:creator>
    </item>
    <item>
      <title>FARS: Factor Augmented Regression Scenarios in R</title>
      <link>https://arxiv.org/abs/2507.10679</link>
      <description>arXiv:2507.10679v2 Announce Type: replace-cross 
Abstract: In the context of macroeconomic/financial time series, the FARS package provides a comprehensive framework in R for the construction of conditional densities of the variable of interest based on the factor-augmented quantile regressions (FA-QRs) methodology, with the factors extracted from multi-level dynamic factor models (ML-DFMs) with potential overlapping group-specific factors. Furthermore, the package also allows the construction of measures of risk as well as modeling and designing economic scenarios based on the conditional densities. In particular, the package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the FA-QRs together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; and (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10679v2</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>On the Voigt profile and its dual</title>
      <link>https://arxiv.org/abs/2508.13252</link>
      <description>arXiv:2508.13252v2 Announce Type: replace-cross 
Abstract: The Voigt profile is the density obtained from the convolution of a Gaussian and a Cauchy and it is widely used in atomic and molecular spectroscopy. We show that the Voigt profile is a scale mixture of Gaussian distributions, with mixing Levy distribution. A consequence of this result is that there exists a dual of the Voigt distribution, which is itself a normal scale mixture. Both the Dual Voigt and its mixing are transformations, via truncation and reflection, of the Normal and Levy random variables. We discuss the dual Voigt characteristics, propose algorithms for parameter estimation and outline further developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13252v2</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimo Cannas</dc:creator>
    </item>
    <item>
      <title>The Limits of Inference in Complex Systems: When Stochastic Models Become Indistinguishable</title>
      <link>https://arxiv.org/abs/2509.24977</link>
      <description>arXiv:2509.24977v2 Announce Type: replace-cross 
Abstract: Robust inference for stochastic dynamical systems is often hampered by sparse sampling and the absence of closed-form likelihoods. We introduce a Monte Carlo path-inference framework that leverages full-path statistics and bridge processes to deliver reliable parameter estimation and model selection from coarsely sampled time series, without requiring analytical solutions. Crucially, we couple mechanistic stochastic models with their inference procedures to quantify how experimental design -specifically, sampling frequency and dataset size- governs estimator precision and model distinguishability. This analysis reveals optimal sampling regimes and sharp, resolution-dependent limits beyond which competing models become empirically indistinguishable. We validate the approach across four disparate systems -trajectories of optically trapped particles, human microbiome dynamics, social-media topic mentions, and forest population time series- recovering parameters and identifying when inference is fundamentally constrained by measurement resolution, thereby clarifying ongoing debates about dominant noise sources in these systems. Together, these results establish path-based Monte Carlo as a practical, general tool for inference and model discrimination in complex systems and provide principled guidelines for designing measurements that maximize information under real-world constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24977v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Aguilar, Miguel A. Mu\~noz, Sandro Azaele</dc:creator>
    </item>
  </channel>
</rss>
