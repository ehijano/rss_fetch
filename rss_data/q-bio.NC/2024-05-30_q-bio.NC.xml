<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 May 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Emergence and long-term maintenance of modularity in spiking neural networks with plasticity</title>
      <link>https://arxiv.org/abs/2405.18587</link>
      <description>arXiv:2405.18587v1 Announce Type: new 
Abstract: In the last three decades the field of brain connectivity has uncovered that cortical regions, interconnected via white-matter fibers, form a modular and hierarchical network. This type of organization, which has also been recognised at the microscopic level in the form of interconnected neural assemblies, is typically believed to support the coexistence of segregation (specialization) and integration (binding) of information. A prominent remaining question is to understand how the brain could possibly become such a complex network. Here, we give a first step into answering this question and propose that adaptation to various inputs could be the key driving mechanism for the formation of structural assemblies at different scales. To illustrate that, we develop a model of (QIF) spiking neurons, subjected to stimuli targetting distributed populations. The model follows several biologically plausible constraints: (i) it contains both excitatory and inhibitory neurons with two classes of plasticity: Hebbian and anti-Hebbian STDP, (ii) dynamics are not frozen after the entrainment is finished but the network is allowed to continue firing spontaneously, and (iii) plasticity remains always active, also after the learning phase. We find that only the combination of Hebbian and anti-Hebbian inhibitory plasticity allows the formation of stable modular organization in the network. Besides, given that the model continues ``alive'' after the learning, the network settles into an asynchronous irregular firing state displaying spontaneous memory recalls which, as we show, turn crucial for the long-term consolidation of the learned memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18587v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raph\"ael Bergoin, Alessandro Torcini, Gustavo Deco, Mathias Quoy, Gorka Zamora-L\'opez</dc:creator>
    </item>
    <item>
      <title>Improving Speech Decoding from ECoG with Self-Supervised Pretraining</title>
      <link>https://arxiv.org/abs/2405.18639</link>
      <description>arXiv:2405.18639v1 Announce Type: new 
Abstract: Recent work on intracranial brain-machine interfaces has demonstrated that spoken speech can be decoded with high accuracy, essentially by treating the problem as an instance of supervised learning and training deep neural networks to map from neural activity to text. However, such networks pay for their expressiveness with very large numbers of labeled data, a requirement that is particularly burdensome for invasive neural recordings acquired from human patients. On the other hand, these patients typically produce speech outside of the experimental blocks used for training decoders. Making use of such data, and data from other patients, to improve decoding would ease the burden of data collection -- especially onerous for dys- and anarthric patients. Here we demonstrate that this is possible, by reengineering wav2vec -- a simple, self-supervised, fully convolutional model that learns latent representations of audio using a noise-contrastive loss -- for electrocorticographic (ECoG) data. We train this model on unlabelled ECoG recordings, and subsequently use it to transform ECoG from labeled speech sessions into wav2vec's representation space, before finally training a supervised encoder-decoder to map these representations to text. We experiment with various numbers of labeled blocks; for almost all choices, the new representations yield superior decoding performance to the original ECoG data, and in no cases do they yield worse. Performance can also be improved in some cases by pretraining wav2vec on another patient's data. In the best cases, wav2vec's representations decrease word error rates over the original data by upwards of 50%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18639v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian A. Yuan, Joseph G. Makin</dc:creator>
    </item>
    <item>
      <title>D-CoRP: Differentiable Connectivity Refinement for Functional Brain Networks</title>
      <link>https://arxiv.org/abs/2405.18658</link>
      <description>arXiv:2405.18658v1 Announce Type: new 
Abstract: Brain network is an important tool for understanding the brain, offering insights for scientific research and clinical diagnosis. Existing models for brain networks typically primarily focus on brain regions or overlook the complexity of brain connectivities. MRI-derived brain network data is commonly susceptible to connectivity noise, underscoring the necessity of incorporating connectivities into the modeling of brain networks. To address this gap, we introduce a differentiable module for refining brain connectivity. We develop the multivariate optimization based on information bottleneck theory to address the complexity of the brain network and filter noisy or redundant connections. Also, our method functions as a flexible plugin that is adaptable to most graph neural networks. Our extensive experimental results show that the proposed method can significantly improve the performance of various baseline models and outperform other state-of-the-art methods, indicating the effectiveness and generalizability of the proposed method in refining brain network connectivity. The code will be released for public availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18658v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Hu, Hongrun Zhang, Chao Li</dc:creator>
    </item>
    <item>
      <title>TVB C++: A Fast and Flexible Back-End for The Virtual Brain</title>
      <link>https://arxiv.org/abs/2405.18788</link>
      <description>arXiv:2405.18788v1 Announce Type: new 
Abstract: This paper introduces TVB C++, a streamlined and fast C++ Back-End for The Virtual Brain (TVB), a renowned platform and a benchmark tool for full-brain simulation. TVB C++ is engineered with speed as a primary focus while retaining the flexibility and ease of use characteristic of the original TVB platform. Positioned as a complementary tool, TVB serves as a prototyping platform, whereas TVB C++ becomes indispensable when performance is paramount, particularly for large-scale simulations and leveraging advanced computation facilities like supercomputers. Developed as a TVB-compatible Back-End, TVB C++ seamlessly integrates with the original TVB implementation, facilitating effortless usage. Users can easily configure TVB C++ to execute the same code as in TVB but with enhanced performance and parallelism capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18788v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Mart\'in, Gorka Zamora, Jan Fousek, Michael Schirner, Petra Ritter, Viktor Jirsa, Gustavo Deco, Gustavo Patow</dc:creator>
    </item>
    <item>
      <title>Precision microfluidic control of neuronal ensembles in cultured cortical networks</title>
      <link>https://arxiv.org/abs/2405.19159</link>
      <description>arXiv:2405.19159v1 Announce Type: new 
Abstract: In vitro neuronal culture is an important research platform in cellular and network neuroscience. However, neurons cultured on a homogeneous scaffold form dense, randomly connected networks and display excessively synchronized activity; this phenomenon has limited their applications in network-level studies, such as studies of neuronal ensembles, or coordinated activity by a group of neurons. Herein, we develop polydimethylsiloxane-based microfluidic devices to create small neuronal networks exhibiting a hierarchically modular structure resembling the connectivity observed in the mammalian cortex. The strength of intermodular coupling was manipulated by varying the width and height of the microchannels that connect the modules. Using fluorescent calcium imaging, we observe that the spontaneous activity in networks with smaller microchannels (2.2$-$5.5 $\mu$m$^2$) had lower synchrony and exhibit a threefold variety of neuronal ensembles. Optogenetic stimulation demonstrates that a reduction in intermodular coupling enriches evoked neuronal activity patterns and that repeated stimulation induces plasticity in neuronal ensembles in these networks. These findings suggest that cell engineering technologies based on microfluidic devices enable in vitro reconstruction of the intricate dynamics of neuronal ensembles, thus providing a robust platform for studying neuronal ensembles in a well-defined physicochemical environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19159v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hakuba Murota, Hideaki Yamamoto, Nobuaki Monma, Shigeo Sato, Ayumi Hirano-Iwata</dc:creator>
    </item>
    <item>
      <title>Motor Imagery Task Alters Dynamics of Human Body Posture</title>
      <link>https://arxiv.org/abs/2405.19228</link>
      <description>arXiv:2405.19228v1 Announce Type: new 
Abstract: Motor Imagery (MI) is gaining traction in both rehabilitation and sports settings, but its immediate influence on human postural control is not yet clearly understood. The focus of this study is to examine the effects of MI on the dynamics of the Center of Pressure (COP), a crucial metric for evaluating postural stability. In the experiment, thirty healthy young adults participated in four different scenarios: normal standing with both open and closed eyes, and kinesthetic motor imagery focused on mediolateral (ML) and anteroposterior (AP) sway movements. A mathematical model was developed to characterize the nonlinear dynamics of the COP and to assess the impact of MI on these dynamics. Our results show a statistically significant increase (p-value&lt;0.05) in variables such as COP path length and Long-Range Correlation (LRC) during MI compared to the closed-eye and normal standing conditions. These observations align well with psycho-neuromuscular theory, which suggests that imagining a specific movement activates neural pathways, consequently affecting postural control. This study presents compelling evidence that motor imagery not only has a quantifiable impact on COP dynamics but also that changes in the Center of Pressure (COP) are directionally consistent with the imagined movements. This finding holds significant implications for the field of rehabilitation science, suggesting that motor imagery could be strategically utilized to induce targeted postural adjustments. Nonetheless, additional research is required to fully understand the complex mechanisms that underlie this relationship and to corroborate these results across a more diverse set of populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19228v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Delavari, Seyyed Mohammad Reza Hashemi Golpayegani, Mohammad Ali Ahmadi-Pajouh</dc:creator>
    </item>
    <item>
      <title>A theoretical framework for learning through structural plasticity</title>
      <link>https://arxiv.org/abs/2307.11735</link>
      <description>arXiv:2307.11735v5 Announce Type: replace 
Abstract: A growing body of research indicates that structural plasticity mechanisms are crucial for learning and memory consolidation. Starting from a simple phenomenological model, we exploit a mean-field approach to develop a theoretical framework of learning through this kind of plasticity, capable of taking into account several features of the connectivity and pattern of activity of biological neural networks, including probability distributions of neuron firing rates, selectivity of the responses of single neurons to multiple stimuli, probabilistic connection rules and noisy stimuli. More importantly, it describes the effects of stabilization, pruning and reorganization of synaptic connections. This framework is used to compute the values of some relevant quantities used to characterize the learning and memory capabilities of the neuronal network in training and testing procedures as the number of training patterns and other model parameters vary. The results are then compared with those obtained through simulations with firing-rate-based neuronal network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11735v5</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianmarco Tiddia, Luca Sergi, Bruno Golosio</dc:creator>
    </item>
  </channel>
</rss>
