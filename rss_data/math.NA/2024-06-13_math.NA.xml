<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 01:42:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multi-objective optimization for multi-agent injection strategies in subsurface CO$_2$ storage</title>
      <link>https://arxiv.org/abs/2406.07711</link>
      <description>arXiv:2406.07711v1 Announce Type: new 
Abstract: We propose a novel framework for optimizing injection strategies in large-scale CO$_2$ storage combining multi-agent models with multi-objective optimization, and reservoir simulation. We investigate whether agents should form coalitions for collaboration to maximize the outcome of their storage activities. In multi-agent systems, it is typically assumed that the optimal strategy for any given coalition structure is already known, and it remains to identify which coalition structure is optimal according to some predefined criterion. For any coalition structure in this work, the optimal CO$_2$ injection strategy is not a priori known, and needs to be found by a combination of reservoir simulation and a multi-objective optimization problem. The multi-objective optimization problems all come with the numerical challenges of repeated evaluations of complex-physics models. We use versatile evolutionary algorithms to solve the multi-objective optimization problems, where the solution is a set of values, e.g., a Pareto front. The Pareto fronts are first computed using the so-called weighted sum method that transforms the multi-objective optimization problem into a set of single-objective optimization problems. Results based on two different Pareto front selection criteria are presented. Then a truly multi-objective optimization method is used to obtain the Pareto fronts, and compared to the previous weighted sum method. We demonstrate the proposed framework on the Bjarmeland formation, a pressure-limited prospective storage site in the Barents Sea. The problem is constrained by the maximum sustainable pressure buildup and a supply of CO$_2$ that can vary over time. In addition to identifying the optimal coalitions, the methodology shows how distinct suboptimal coalitions perform in comparison to the optimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07711v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Per Pettersson, Sebastian Krumscheid, Sarah Gasda</dc:creator>
    </item>
    <item>
      <title>Nitsche stabilized Virtual element approximations for a Brinkman problem with mixed boundary conditions</title>
      <link>https://arxiv.org/abs/2406.07724</link>
      <description>arXiv:2406.07724v1 Announce Type: new 
Abstract: In this paper, we formulate, analyse and implement the discrete formulation of the Brinkman problem with mixed boundary conditions, including slip boundary condition, using the Nitsche's technique for virtual element methods. The divergence conforming virtual element spaces for the velocity function and piecewise polynomials for pressure are approached for the discrete scheme. We derive a robust stability analysis of the Nitsche stabilized discrete scheme for this model problem. We establish an optimal and vigorous a priori error estimates of the discrete scheme with constants independent of the viscosity. Moreover, a set of numerical tests demonstrates the robustness with respect to the physical parameters and verifies the derived convergence results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07724v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Mora, Jesus Vellojin, Nitesh Verma</dc:creator>
    </item>
    <item>
      <title>A posteriori error estimates for the exponential midpoint method for linear and semilinear parabolic equations</title>
      <link>https://arxiv.org/abs/2406.07789</link>
      <description>arXiv:2406.07789v1 Announce Type: new 
Abstract: In this paper, the a posteriori error estimates of the exponential midpoint method for time discretization are studied for linear and semilinear parabolic equations. Using the exponential midpoint approximation defined by a continuous and piecewise linear interpolation of nodal values yields the suboptimal order estimates. Based on the property of the entire function, we introduce a continuous and piecewise quadratic time reconstruction of the exponential midpoint method to derive the optimal order estimates, and the error bounds are solely dependent on the discretization parameters, the data of the problem and the approximation of the entire function. Several numerical examples are implemented to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07789v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianfa Hu, Wansheng Wang, Mengli Mao, Jiliang Cao</dc:creator>
    </item>
    <item>
      <title>The Galerkin method for a regularised combined field integral equation without a dual basis function</title>
      <link>https://arxiv.org/abs/2406.07924</link>
      <description>arXiv:2406.07924v1 Announce Type: new 
Abstract: We propose discretisation of a regularised combined field integral equation (regularised CFIE) only with the Rao-Wilton-Glisson (RWG) basis function. The CFIE is a formulation of integral equations, which avoids the so-called ficticious frequencies of integral equations. The most typical CFIE, which is a linear combination of the electric field integral equation (EFIE) and magnetic field integral equation (MFIE), is known to be ill-conditioned and requires many iterations when solved with iteration methods such as the generalised minimum residual (GMRES) method. The regularised CFIE is another formulation of the CFIE to solve this problem by applying a regularising operator to the part of the EFIE. In several previous studies the regularising operator is determined based on the Calderon preconditioning. This regularising operator however takes much more computatonal time than the standard CFIE since discretising the EFIE with the Calderon preconditioner requires the dual basis function. In this article we propose a formulation of the regularised CFIE, which can be discretised with the Galerkin method without the dual basis function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07924v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuki Niino, Shunpei Yamamoto</dc:creator>
    </item>
    <item>
      <title>Global-in-time energy stability: a powerful analysis tool for the gradient flow problem without maximum principle or Lipschitz assumption</title>
      <link>https://arxiv.org/abs/2406.07941</link>
      <description>arXiv:2406.07941v1 Announce Type: new 
Abstract: Before proving (unconditional) energy stability for gradient flows, most existing studies either require a strong Lipschitz condition regarding the non-linearity or certain $L^{\infty}$ bounds on the numerical solutions (the maximum principle). However, proving energy stability without such premises is a very challenging task. In this paper, we aim to develop a novel analytical tool, namely global-in-time energy stability, to demonstrate energy dissipation without assuming any strong Lipschitz condition or $L^{\infty}$ boundedness. The fourth-order-in-space Swift-Hohenberg equation is used to elucidate the theoretical results in detail. We also propose a temporal second-order accurate scheme for efficiently solving such a strongly stiff equation. Furthermore, we present the corresponding optimal $L^2$ error estimate and provide several numerical simulations to demonstrate the dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07941v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Sun, H. Wang, H. Zhang, X. Qian, S. Song</dc:creator>
    </item>
    <item>
      <title>Reduced Basis method for finite volume simulations of parabolic PDEs applied to porous media flows</title>
      <link>https://arxiv.org/abs/2406.07950</link>
      <description>arXiv:2406.07950v1 Announce Type: new 
Abstract: Numerical simulations are a highly valuable tool to evaluate the impact of the uncertainties of various modelparameters, and to optimize e.g. injection-production scenarios in the context of underground storage (of CO2typically). Finite volume approximations of Darcy's parabolic model for flows in porous media are typically runmany times, for many values of parameters like permeability and porosity, at costly computational efforts.We study the relevance of reduced basis methods as a way to lower the overall simulation cost of finite volumeapproximations to Darcy's parabolic model for flows in porous media for different values of the parameters suchas permeability. In the context of underground gas storage (of CO2 typically) in saline aquifers, our aim isto evaluate quickly, for many parameter values, the flux along some interior boundaries near the well injectionarea-regarded as a quantity of interest-. To this end, we construct reduced bases by a standard POD-Greedyalgorithm. Our POD-Greedy algorithm uses a new goal-oriented error estimator designed from a discrete space-time energy norm independent of the parameter. We provide some numerical experiments that validate theefficiency of the proposed estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07950v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jana Tarhini (IFPEN), S\'ebastien Boyaval (MATHERIALS, LHSV), Guillaume Ench\'ery (IFPEN), Quang Huy Tran (IFPEN)</dc:creator>
    </item>
    <item>
      <title>Non-stationary Gaussian random fields on hypersurfaces: Sampling and strong error analysis</title>
      <link>https://arxiv.org/abs/2406.08185</link>
      <description>arXiv:2406.08185v1 Announce Type: new 
Abstract: A flexible model for non-stationary Gaussian random fields on hypersurfaces is introduced. The class of random fields on curves and surfaces is characterized by a power spectral density of a second order elliptic differential operator. Sampling is done by a Galerkin--Chebyshev approximation based on the surface finite element method and Chebyshev polynomials. Strong error bounds are shown with convergence rates depending on the smoothness of the approximated random field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08185v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Jansson, Annika Lang, Mike Pereira</dc:creator>
    </item>
    <item>
      <title>The need for accuracy and smoothness in numerical simulations</title>
      <link>https://arxiv.org/abs/2406.08257</link>
      <description>arXiv:2406.08257v1 Announce Type: new 
Abstract: We consider the problem of estimating the error when solving a system of differential algebraic equations. Richardson extrapolation is a classical technique that can be used to judge when computational errors are irrelevant and estimate the discretization error. We have simulated molecular dynamics with constraints using the GROMACS library and found that the output is not always amenable to Richardson extrapolation. We derive and illustrate Richardson extrapolation using a variety of numerical experiments. We identify two necessary conditions that are not always satisfied by the GROMACS library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08257v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carl Christian Kjelgaard Mikkelsen, Lori\'en L\'opez-Villellas</dc:creator>
    </item>
    <item>
      <title>Genetic Column Generation for Computing Lower Bounds for Adversarial Classification</title>
      <link>https://arxiv.org/abs/2406.08331</link>
      <description>arXiv:2406.08331v1 Announce Type: new 
Abstract: Recent theoretical results on adversarial multi-class classification showed a similarity to the multi-marginal formulation of Wasserstein-barycenter in optimal transport. Unfortunately, both problems suffer from the curse of dimension, making it hard to exploit the nice linear program structure of the problems for numerical calculations. We investigate how ideas from Genetic Column Generation for multi-marginal optimal transport can be used to overcome the curse of dimension in computing the minimal adversarial risk in multi-class classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08331v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Penka</dc:creator>
    </item>
    <item>
      <title>Scalable Optimal Motion Planning for Multi-Agent Systems by Cosserat Theory of Rods</title>
      <link>https://arxiv.org/abs/2406.07684</link>
      <description>arXiv:2406.07684v1 Announce Type: cross 
Abstract: We address the motion planning problem for large multi-agent systems, utilizing Cosserat rod theory to model the dynamic behavior of vehicle formations. The problem is formulated as an optimal control problem over partial differential equations (PDEs) that describe the system as a continuum. This approach ensures scalability with respect to the number of vehicles, as the problem's complexity remains unaffected by the size of the formation. The numerical discretization of the governing equations and problem's constraints is achieved through Bernstein surface polynomials, facilitating the conversion of the optimal control problem into a nonlinear programming (NLP) problem. This NLP problem is subsequently solved using off-the-shelf optimization software. We present several properties and algorithms related to Bernstein surface polynomials to support the selection of this methodology. Numerical demonstrations underscore the efficacy of this mathematical framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07684v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirreza Fahim Golestaneh, Maxwell Hammond, Venanzio Cichella</dc:creator>
    </item>
    <item>
      <title>A square root algorithm faster than Newton's method for multiprecision numbers, using floating-point arithmetic</title>
      <link>https://arxiv.org/abs/2406.07751</link>
      <description>arXiv:2406.07751v1 Announce Type: cross 
Abstract: In this paper, an optimized version of classical Bombelli's algorithm for computing integer square roots is presented. In particular, floating-point arithmetic is used to compute the initial guess of each digit of the root, following similar ideas to those used in "The Art of Computer Programming" Vol. 2, p. 4.3.1 for division. A program with an implementation of the algorithm in Java is also presented, and its running time is compared with that of the algorithm provided by the Java standard library, which uses the Newton's method. From tests, the algorithm presented here turns out to be much faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07751v1</guid>
      <category>cs.MS</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Romano</dc:creator>
    </item>
    <item>
      <title>A New Linear Programming Approach and a New Backtracking Strategy for Multiple-Gradient Descent in Multi-Objective Optimization</title>
      <link>https://arxiv.org/abs/2406.08147</link>
      <description>arXiv:2406.08147v1 Announce Type: cross 
Abstract: In this work, the author presents a novel method for finding descent directions shared by two or more differentiable functions defined on the same unconstrained domain space. Then, the author illustrates an alternative Multiple-Gradient Descent procedure for Multi-Objective Optimization problems that is based on this new method. In particular, the proposed method consists in finding the shared descent direction solving a relatively cheap Linear Programming (LP) problem, where the LP's objective function and the constraints are defined by the gradients of the objective functions of the Multi-Objective Optimization problem. More precisely, the formulation of the LP problem is such that, if a shared descent direction does not exist for the objective functions, but a non-ascent direction for all the objectives does, the LP problem returns the latter. Moreover, the author defines a new backtracking strategy for Multiple-Gradient Descent methods such that, if the proposed LP is used for computing the direction, the ability to reach and/or explore the Pareto set and the Pareto front is improved. A theoretical analysis of the properties of the new methods is performed, and tests on classic Multi-Objective Optimization problems are proposed to assess their goodness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08147v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Della Santa</dc:creator>
    </item>
    <item>
      <title>A simple GPU implementation of spectral-element methods for solving 3D Poisson type equations on rectangular domains and its applications</title>
      <link>https://arxiv.org/abs/2310.00226</link>
      <description>arXiv:2310.00226v3 Announce Type: replace 
Abstract: It is well known since 1960s that by exploring the tensor product structure of the discrete Laplacian on Cartesian meshes, one can develop a simple direct Poisson solver with an $\mathcal O(N^{\frac{d+1}d})$ complexity in d-dimension, where N is the number of the total unknowns. The GPU acceleration of numerically solving PDEs has been explored successfully around fifteen years ago and become more and more popular in the past decade, driven by significant advancement in both hardware and software technologies, especially in the recent few years. We present in this paper a simple but extremely fast MATLAB implementation on a modern GPU, which can be easily reproduced, for solving 3D Poisson type equations using a spectral-element method. In particular, it costs less than one second on a Nvidia A100 for solving a Poisson equation with one billion degree of freedoms. We also present applications of this fast solver to solve a linear (time-independent) Schr\"odinger equation and a nonlinear (time-dependent) Cahn-Hilliard equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00226v3</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Liu, Jie Shen, Xiangxiong Zhang</dc:creator>
    </item>
    <item>
      <title>The Generalized Matrix Norm Problem</title>
      <link>https://arxiv.org/abs/2310.00605</link>
      <description>arXiv:2310.00605v2 Announce Type: replace 
Abstract: We study the computability of the operator norm of a matrix with respect to norms induced by linear operators. Our findings reveal that this problem can be solved exactly in polynomial time in certain situations, and we discuss how it can be approximated for other cases. Along the way, we investigate the concept of push-forward and pull-back of seminorms, which leads us to uncover novel duality principles that come into play when optimizing over the unit ball of norms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00605v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Kulmburg</dc:creator>
    </item>
    <item>
      <title>Determining initial conditions for nonlinear hyperbolic equations with time dimensional reduction and the Carleman contraction</title>
      <link>https://arxiv.org/abs/2312.01179</link>
      <description>arXiv:2312.01179v3 Announce Type: replace 
Abstract: This paper aims to determine the initial conditions for quasi-linear hyperbolic equations that include nonlocal elements. We suggest a method where we approximate the solution of the hyperbolic equation by truncating its Fourier series in the time domain with a polynomial-exponential basis. This truncation effectively removes the time variable, transforming the problem into a system of quasi-linear elliptic equations. We refer to this technique as the "time dimensional reduction method." To numerically solve this system comprehensively without the need for an accurate initial estimate, we used the newly developed Carleman contraction principle. We show the efficiency of our method through various numerical examples. The time dimensional reduction method stands out not only for its precise solutions but also for its remarkable speed in computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01179v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Trong D. Dang, Loc H. Nguyen, Huong T. T. Vu</dc:creator>
    </item>
    <item>
      <title>Rectified deep neural networks overcome the curse of dimensionality when approximating solutions of McKean--Vlasov stochastic differential equations</title>
      <link>https://arxiv.org/abs/2312.07042</link>
      <description>arXiv:2312.07042v2 Announce Type: replace 
Abstract: In this paper we prove that rectified deep neural networks do not suffer from the curse of dimensionality when approximating McKean--Vlasov SDEs in the sense that the number of parameters in the deep neural networks only grows polynomially in the space dimension $d$ of the SDE and the reciprocal of the accuracy $\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07042v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Tuan Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Efficient Spectral Element Method for the Euler Equations on Unbounded Domains in Multiple Dimensions</title>
      <link>https://arxiv.org/abs/2401.05624</link>
      <description>arXiv:2401.05624v2 Announce Type: replace 
Abstract: Mitigating the impact of waves leaving a numerical domain has been a persistent challenge in numerical modeling. Reducing wave reflection at the domain boundary is crucial for accurate simulations. Absorbing layers, while common, often incur significant computational costs. This paper introduces an efficient application of a Legendre-Laguerre basis for absorbing layers for two-dimensional non-linear compressible Euler equations. The method couples a spectral-element bounded domain with a semi-infinite region, employing a tensor product of Lagrange and scaled Laguerre basis functions. Semi-infinite elements are used in the absorbing layer with Rayleigh damping. In comparison to existing methods with similar absorbing layer extensions, this approach, a pioneering application to the Euler equations of compressible and stratified flows, demonstrates substantial computational savings. The study marks the first application of semi-infinite elements to mitigate wave reflection in the solution of the Euler equations, particularly in nonhydrostatic atmospheric modeling. A comprehensive set of tests demonstrates the method's versatility for general systems of conservation laws, with a focus on its effectiveness in damping vertically propagating mountain gravity waves, a benchmark for atmospheric models. Across all tests, the model presented in this paper consistently exhibits notable performance improvements compared to a traditional Rayleigh damping approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05624v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.ao-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yassine Tissaoui, James F. Kelly, Simone Marras</dc:creator>
    </item>
    <item>
      <title>Fast Kernel Summation in High Dimensions via Slicing and Fourier Transforms</title>
      <link>https://arxiv.org/abs/2401.08260</link>
      <description>arXiv:2401.08260v2 Announce Type: replace 
Abstract: Kernel-based methods are heavily used in machine learning. However, they suffer from $O(N^2)$ complexity in the number $N$ of considered data points. In this paper, we propose an approximation procedure, which reduces this complexity to $O(N)$. Our approach is based on two ideas. First, we prove that any radial kernel with analytic basis function can be represented as sliced version of some one-dimensional kernel and derive an analytic formula for the one-dimensional counterpart. It turns out that the relation between one- and $d$-dimensional kernels is given by a generalized Riemann-Liouville fractional integral. Hence, we can reduce the $d$-dimensional kernel summation to a one-dimensional setting. Second, for solving these one-dimensional problems efficiently, we apply fast Fourier summations on non-equispaced data, a sorting algorithm or a combination of both. Due to its practical importance we pay special attention to the Gaussian kernel, where we show a dimension-independent error bound and represent its one-dimensional counterpart via a closed-form Fourier transform. We provide a run time comparison and error estimate of our fast kernel summations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08260v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Hertrich</dc:creator>
    </item>
    <item>
      <title>Validated numerics for algebraic path tracking</title>
      <link>https://arxiv.org/abs/2401.17973</link>
      <description>arXiv:2401.17973v2 Announce Type: replace 
Abstract: Using validated numerical methods, interval arithmetic and Taylor models, we propose a certified predictor-corrector loop for tracking zeros of polynomial systems with a parameter. We provide a Rust implementation which shows tremendous improvement over existing software for certified path tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17973v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3666000.3669673</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of ISSAC 2024</arxiv:journal_reference>
      <dc:creator>Alexandre Guillemot, Pierre Lairez</dc:creator>
    </item>
    <item>
      <title>Interpolatory model order reduction of large-scale dynamical systems with root mean squared error measures</title>
      <link>https://arxiv.org/abs/2403.08894</link>
      <description>arXiv:2403.08894v2 Announce Type: replace 
Abstract: The root mean squared error is an important measure used in a variety of applications such as structural dynamics and acoustics to model averaged deviations from standard behavior. For large-scale systems, simulations of this quantity quickly become computationally prohibitive. Classical model order reduction techniques attempt to resolve this issue via the construction of surrogate models that emulate the root mean squared error measure using an intermediate linear system. However, this approach requires a potentially large number of linear outputs, which can be disadvantageous in the design of reduced-order models. In this work, we consider directly the root mean squared error as the quantity of interest using the concept of quadratic-output models and propose several new model reduction techniques for the construction of appropriate surrogates. We test the proposed methods on a model for the vibrational response of a plate with tuned vibration absorbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08894v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean Reiter, Steffen W. R. Werner</dc:creator>
    </item>
    <item>
      <title>A fast low-rank inversion algorithm of dielectric matrix in GW approximation</title>
      <link>https://arxiv.org/abs/2403.12340</link>
      <description>arXiv:2403.12340v2 Announce Type: replace 
Abstract: The dielectric response function and its inverse are crucial physical quantities in materials science. We propose an accurate and efficient strategy to invert the dielectric function matrix. The GW approximation, a powerful approach to accurately describe many-body excited states, is taken as an application to demonstrate accuracy and efficiency. We incorporate the interpolative separable density fitting (ISDF) algorithm with Sherman--Morrison--Woodbury (SMW) formula to accelerate the inversion process by exploiting low-rank properties of dielectric function in plane-wave GW calculations. Our ISDF--SMW strategy produces accurate quasiparticle energies with $O(N_{\mathrm{r}}N_{\mathrm{e}}^2)$ computational cost $(N_{\mathrm{e}}$ is the number of electrons and $N_{\mathrm{r}}=100$--$1000N_{\mathrm{e}}$ is the number of grid points) with negligible small error of $0.03$ eV for both complex molecules and solids. This new strategy for inverting the dielectric matrix can be \(50\times\) faster than the current state-of-the-art implementation in BerkeleyGW, resulting in two orders of magnitude speedup for total GW calculations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12340v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengbang Zhou, Huanhuan Ma, Wentiao Wu, Weiguo Gao, Jinlong Yang, Meiyue Shao, Wei Hu</dc:creator>
    </item>
    <item>
      <title>Hybrid weakly over-penalised symmetric interior penalty method on anisotropic meshes</title>
      <link>https://arxiv.org/abs/2404.15288</link>
      <description>arXiv:2404.15288v2 Announce Type: replace 
Abstract: In this study, we investigate a hybrid-type anisotropic weakly over-penalised symmetric interior penalty method for the Poisson equation on convex domains. Compared with the well-known hybrid discontinuous Galerkin methods, our approach is simple and easy to implement. Our primary contributions are the proposal of a new scheme and the demonstration of a proof for the consistency term, which allows us to estimate the anisotropic consistency error. The key idea of the proof is to apply the relation between the Raviart--Thomas finite element space and a discontinuous space. In numerical experiments, we compare the calculation results for standard and anisotropic mesh partitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15288v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Ishizaka</dc:creator>
    </item>
    <item>
      <title>Beating Posits at Their Own Game: Takum Arithmetic</title>
      <link>https://arxiv.org/abs/2404.18603</link>
      <description>arXiv:2404.18603v2 Announce Type: replace 
Abstract: Recent evaluations have highlighted the tapered posit number format as a promising alternative to the uniform precision IEEE 754 floating-point numbers, which suffer from various deficiencies. Although the posit encoding scheme offers superior coding efficiency at values close to unity, its efficiency markedly diminishes with deviation from unity. This reduction in efficiency leads to suboptimal encodings and a consequent diminution in dynamic range, thereby rendering posits suboptimal for general-purpose computer arithmetic.
  This paper introduces and formally proves 'takum' as a novel general-purpose logarithmic tapered-precision number format, synthesising the advantages of posits in low-bit applications with high encoding efficiency for numbers distant from unity. Takums exhibit an asymptotically constant dynamic range in terms of bit string length, which is delineated in the paper to be suitable for a general-purpose number format. It is demonstrated that takums either match or surpass existing alternatives. Moreover, takums address several issues previously identified in posits while unveiling novel and beneficial arithmetic properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18603v2</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laslo Hunhold</dc:creator>
    </item>
    <item>
      <title>Approximation properties relative to continuous scale space for hybrid discretizations of Gaussian derivative operators</title>
      <link>https://arxiv.org/abs/2405.05095</link>
      <description>arXiv:2405.05095v3 Announce Type: replace 
Abstract: This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences. The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.
  While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.
  In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05095v3</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Piecewise rational rotation-minimizing motions via data stream interpolation</title>
      <link>https://arxiv.org/abs/2405.14229</link>
      <description>arXiv:2405.14229v2 Announce Type: replace 
Abstract: When a moving frame defined along a space curve is required to keep an axis aligned with the tangent direction of motion, the use of rotation-minimizing frames (RMF) avoids unnecessary rotations in the normal plane. The construction of rigid body motions using a specific subset of quintic curves with rational RMFs (RRMFs) is here considered. In particular, a novel geometric characterization of such subset enables the design of a local algorithm to interpolate an assigned stream of positions, together with an initial frame orientation. To achieve this, the translational part of the motion is described by a parametric $G^1$ spline curve whose segments are quintic RRMFs, with a globally continuous piecewise rational rotation-minimizing frame. A selection of numerical experiments illustrates the performances of the proposed method on synthetic and arbitrary data streams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14229v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carlotta Giannelli, Lorenzo Sacco, Alessandra Sestini, Zbyn\v{e}k \v{S}\'ir</dc:creator>
    </item>
    <item>
      <title>Lower eigenvalue bounds with hybrid high-order methods</title>
      <link>https://arxiv.org/abs/2406.06244</link>
      <description>arXiv:2406.06244v2 Announce Type: replace 
Abstract: This paper proposes hybrid high-order eigensolvers for the computation of guaranteed lower eigenvalue bounds. These bounds display higher order convergence rates and are accessible to adaptive mesh-refining algorithms. The involved constants arise from local embeddings and are available for all polynomial degrees. Applications include the linear elasticity and Steklov eigenvalue problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06244v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ngoc Tien Tran</dc:creator>
    </item>
    <item>
      <title>A PDE-based Explanation of Extreme Numerical Sensitivities and Edge of Stability in Training Neural Networks</title>
      <link>https://arxiv.org/abs/2206.02001</link>
      <description>arXiv:2206.02001v4 Announce Type: replace-cross 
Abstract: We discover restrained numerical instabilities in current training practices of deep networks with stochastic gradient descent (SGD), and its variants. We show numerical error (on the order of the smallest floating point bit and thus the most extreme or limiting numerical perturbations induced from floating point arithmetic in training deep nets can be amplified significantly and result in significant test accuracy variance (sensitivities), comparable to the test accuracy variance due to stochasticity in SGD. We show how this is likely traced to instabilities of the optimization dynamics that are restrained, i.e., localized over iterations and regions of the weight tensor space. We do this by presenting a theoretical framework using numerical analysis of partial differential equations (PDE), and analyzing the gradient descent PDE of convolutional neural networks (CNNs). We show that it is stable only under certain conditions on the learning rate and weight decay. We show that rather than blowing up when the conditions are violated, the instability can be restrained. We show this is a consequence of the non-linear PDE associated with the gradient descent of the CNN, whose local linearization changes when over-driving the step size of the discretization, resulting in a stabilizing effect. We link restrained instabilities to the recently discovered Edge of Stability (EoS) phenomena, in which the stable step size predicted by classical theory is exceeded while continuing to optimize the loss and still converging. Because restrained instabilities occur at the EoS, our theory provides new insights and predictions about the EoS, in particular, the role of regularization and the dependence on the network complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02001v4</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Sun, Dong Lao, Ganesh Sundaramoorthi, Anthony Yezzi</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic convergence bounds for modified tamed unadjusted Langevin algorithm in non-convex setting</title>
      <link>https://arxiv.org/abs/2207.02600</link>
      <description>arXiv:2207.02600v2 Announce Type: replace-cross 
Abstract: We consider the problem of sampling from a high-dimensional target distribution $\pi_\beta$ on $\mathbb{R}^d$ with density proportional to $\theta\mapsto e^{-\beta U(\theta)}$ using explicit numerical schemes based on discretising the Langevin stochastic differential equation (SDE). In recent literature, taming has been proposed and studied as a method for ensuring stability of Langevin-based numerical schemes in the case of super-linearly growing drift coefficients for the Langevin SDE. In particular, the Tamed Unadjusted Langevin Algorithm (TULA) was proposed in [Bro+19] to sample from such target distributions with the gradient of the potential $U$ being super-linearly growing. However, theoretical guarantees in Wasserstein distances for Langevin-based algorithms have traditionally been derived assuming strong convexity of the potential $U$. In this paper, we propose a novel taming factor and derive, under a setting with possibly non-convex potential $U$ and super-linearly growing gradient of $U$, non-asymptotic theoretical bounds in Wasserstein-1 and Wasserstein-2 distances between the law of our algorithm, which we name the modified Tamed Unadjusted Langevin Algorithm (mTULA), and the target distribution $\pi_\beta$. We obtain respective rates of convergence $\mathcal{O}(\lambda)$ and $\mathcal{O}(\lambda^{1/2})$ in Wasserstein-1 and Wasserstein-2 distances for the discretisation error of mTULA in step size $\lambda$. High-dimensional numerical simulations which support our theoretical findings are presented to showcase the applicability of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02600v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Matthew Ng Cheng En, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>A face-centred finite volume method for laminar and turbulent incompressible flows</title>
      <link>https://arxiv.org/abs/2403.01496</link>
      <description>arXiv:2403.01496v2 Announce Type: replace-cross 
Abstract: This work develops, for the first time, a face-centred finite volume (FCFV) solver for the simulation of laminar and turbulent viscous incompressible flows. The formulation relies on the Reynolds-averaged Navier-Stokes (RANS) equations coupled with the negative Spalart-Allmaras (SA) model and three novel convective stabilisations, inspired by Riemann solvers, are derived and compared numerically. The resulting method achieves first-order convergence of the velocity, the velocity-gradient tensor and the pressure. FCFV accurately predicts engineering quantities of interest, such as drag and lift, on unstructured meshes and, by avoiding gradient reconstruction, the method is less sensitive to mesh quality than other FV methods, even in the presence of highly distorted and stretched cells. A monolithic and a staggered solution strategies for the RANS-SA system are derived and compared numerically. Numerical benchmarks, involving laminar and turbulent, steady and transient cases are used to assess the performance, accuracy and robustness of the proposed FCFV method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01496v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luan M. Vieira, Matteo Giacomini, Ruben Sevilla, Antonio Huerta</dc:creator>
    </item>
    <item>
      <title>Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs</title>
      <link>https://arxiv.org/abs/2403.10642</link>
      <description>arXiv:2403.10642v2 Announce Type: replace-cross 
Abstract: Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately out-of-domain (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses these well-calibrated UQ estimates within the ProbConserv framework to update the model. Our empirical results show that Operator-ProbConserv enhances OOD model performance for a variety of challenging PDE problems and satisfies physical constraints such as conservation laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10642v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Chandra Mouli, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta, Andrew Stuart, Michael W. Mahoney, Yuyang Wang</dc:creator>
    </item>
  </channel>
</rss>
