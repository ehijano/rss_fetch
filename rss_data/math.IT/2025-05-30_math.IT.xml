<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Noncoherent MIMO Communications: Theoretical Foundation, Design Approaches, and Future Challenges</title>
      <link>https://arxiv.org/abs/2505.23172</link>
      <description>arXiv:2505.23172v1 Announce Type: new 
Abstract: Noncoherent communication is a promising paradigm for future wireless systems where acquiring accurate channel state information (CSI) is challenging or infeasible. It provides methods to bypass the need for explicit channel estimation in practical scenarios such as high-mobility networks, massive distributed antenna arrays, energy-constrained Internet-of-Things devices, and unstructured propagation environments. This survey provides a comprehensive overview of noncoherent communication strategies in multiple-input multiple-output (MIMO) systems, focusing on recent advances since the early 2000s. We classify noncoherent communication schemes into three main approaches where CSI-free signal recovery is based on subspace detection (i.e., Grassmannian signaling), differential detection, and energy detection, respectively. For each approach, we review the theoretical foundation and design methodologies. We also provide comparative insights into their suitability across different channel models and system constraints, highlighting application scenarios where noncoherent methods offer performance and scalability advantages over traditional coherent communication. Furthermore, we discuss practical considerations of noncoherent communication, including compatibility with orthogonal frequency division multiplexing (OFDM), resilience to hardware impairments, and scalability with the number of users. Finally, we provide an outlook on future challenges and research directions in designing robust and efficient noncoherent systems for next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23172v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khac-Hoang Ngo, Diego Cuevas, Ruben de Miguel Gil, Victor Monzon Baeza, Ana Garcia Armada, Ignacio Santamaria</dc:creator>
    </item>
    <item>
      <title>Pure Gaps at Many Places and Multi-point AG Codes from Arbitrary Kummer Extensions</title>
      <link>https://arxiv.org/abs/2505.23274</link>
      <description>arXiv:2505.23274v1 Announce Type: new 
Abstract: For a Kummer extension defined by the affine equation $y^{m}=\prod_{i=1}^{r} (x-\a_i)^{\lambda_i}$ over
  an algebraic extension $K$ of a finite field $\fq$, where $\la_i\in \Z\backslash\{0\}$ for $1\leq i\leq r$, $\gcd(m,q) = 1$, and $\a_1,\cdots,\a_r\in K$ are pairwise distinct elements,
  we propose a simple and efficient method to find all pure gaps at many totally ramified places.
  We introduce a bottom set of pure gaps and indicate that the set of pure gaps is completely determined by the bottom set.
  Furthermore, we demonstrate that a pure gap can be deduced from a known pure gap by easily verifying only one inequality.
  Then, in the case where $\lambda_1 = \lambda_2 = \cdots = \lambda_r$, we fully determine an explicit description of the set of pure gaps at many totally ramified places,
  This includes the scenario in which the set of these places contains the infinite place.
  Finally, we apply these results to construct multi-point algebraic geometry codes with good parameters.
  As one of the examples, a presented code with parameters $[74, 60, \geq 10]$ over $\mathbb{F}_{25}$ yields a new record.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23274v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huachao Zhang, Chang-An Zhao</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Wireless Communication Systems Assisted by Fluid Reconfigurable Intelligent Surfaces</title>
      <link>https://arxiv.org/abs/2505.23680</link>
      <description>arXiv:2505.23680v1 Announce Type: new 
Abstract: This letter investigates the performance of emerging wireless communication systems assisted by a fluid reconfigurable intelligent surface (FRIS). Unlike conventional reconfigurable intelligent surfaces (RISs), an FRIS consists of fluid-inspired metamaterials arranged in a densely packed matrix of sub-elements over a surface. It dynamically activates specific elements for signal reflection and modulation based on real-time channel conditions. Considering a downlink scenario where a base station communicates with a user terminal via a FRIS, we first characterize the statistical behavior of the equivalent end-to-end channel by deriving closed-form approximations for its cumulative distribution and probability density functions. Using these expressions, an analytical approximation for the outage probability and a tight upper bound on the ergodic capacity, including their asymptotic behaviors for high signal-to-noise ratio values, are derived. Our findings reveal key performance trends demonstrating that FRIS can substantially improve link reliability and spectral efficiency compared to conventional RISs, owing to its capability to dynamically select optimal elements from a dense preconfigured grid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23680v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farshad Rostami Ghadi, Kai-Kit Wong, F. Javier Lopez-Martinez, George C. Alexandropoulos, Chan-Byoung Chae</dc:creator>
    </item>
    <item>
      <title>Distribution free M-estimation</title>
      <link>https://arxiv.org/abs/2505.22807</link>
      <description>arXiv:2505.22807v1 Announce Type: cross 
Abstract: The basic question of delineating those statistical problems that are solvable without making any assumptions on the underlying data distribution has long animated statistics and learning theory. This paper characterizes when a (univariate) convex M-estimation or stochastic optimization problem is solvable in such an assumption-free setting, providing a precise dividing line between solvable and unsolvable problems. The conditions we identify show, perhaps surprisingly, that Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization, and they are also distinct from classical characterizations of learnability in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22807v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John C. Duchi</dc:creator>
    </item>
    <item>
      <title>Best Arm Identification with Possibly Biased Offline Data</title>
      <link>https://arxiv.org/abs/2505.23165</link>
      <description>arXiv:2505.23165v1 Announce Type: cross 
Abstract: We study the best arm identification (BAI) problem with potentially biased offline data in the fixed confidence setting, which commonly arises in real-world scenarios such as clinical trials. We prove an impossibility result for adaptive algorithms without prior knowledge of the bias bound between online and offline distributions. To address this, we propose the LUCB-H algorithm, which introduces adaptive confidence bounds by incorporating an auxiliary bias correction to balance offline and online data within the LUCB framework. Theoretical analysis shows that LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and significantly outperforms it when offline data is helpful. We also derive an instance-dependent lower bound that matches the upper bound of LUCB-H in certain scenarios. Numerical experiments further demonstrate the robustness and adaptability of LUCB-H in effectively incorporating offline data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23165v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Yang, Vincent Y. F. Tan, Wang Chi Cheung</dc:creator>
    </item>
    <item>
      <title>Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference</title>
      <link>https://arxiv.org/abs/2505.23389</link>
      <description>arXiv:2505.23389v1 Announce Type: cross 
Abstract: Quantum sensing exploits non-classical effects to overcome limitations of classical sensors, with applications ranging from gravitational-wave detection to nanoscale imaging. However, practical quantum sensors built on noisy intermediate-scale quantum (NISQ) devices face significant noise and sampling constraints, and current variational quantum sensing (VQS) methods lack rigorous performance guarantees. This paper proposes an online control framework for VQS that dynamically updates the variational parameters while providing deterministic error bars on the estimates. By leveraging online conformal inference techniques, the approach produces sequential estimation sets with a guaranteed long-term risk level. Experiments on a quantum magnetometry task confirm that the proposed dynamic VQS approach maintains the required reliability over time, while still yielding precise estimates. The results demonstrate the practical benefits of combining variational quantum algorithms with online conformal inference to achieve reliable quantum sensing on NISQ devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23389v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivana Nikoloska, Hamdi Joudeh, Ruud van Sloun, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Refining Labeling Functions with Limited Labeled Data</title>
      <link>https://arxiv.org/abs/2505.23470</link>
      <description>arXiv:2505.23470v1 Announce Type: cross 
Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for labeling data by combining the outputs of user-provided labeling functions (LFs) on unlabeled datapoints. However, the quality of the generated labels depends directly on the accuracy of the LFs. In this work, we study the problem of fixing LFs based on a small set of labeled examples. Towards this goal, we develop novel techniques for repairing a set of LFs by minimally changing their results on the labeled examples such that the fixed LFs ensure that (i) there is sufficient evidence for the correct label of each labeled datapoint and (ii) the accuracy of each repaired LF is sufficiently high. We model LFs as conditional rules which enables us to refine them, i.e., to selectively change their output for some inputs. We demonstrate experimentally that our system improves the quality of LFs based on surprisingly small sets of labeled datapoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23470v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenjie Li, Amir Gilad, Boris Glavic, Zhengjie Miao, Sudeepa Roy</dc:creator>
    </item>
    <item>
      <title>Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats</title>
      <link>https://arxiv.org/abs/2505.23706</link>
      <description>arXiv:2505.23706v1 Announce Type: cross 
Abstract: In connected and autonomous vehicles, machine learning for safety message classification has become critical for detecting malicious or anomalous behavior. However, conventional approaches that rely on centralized data collection or purely local training face limitations due to the large scale, high mobility, and heterogeneous data distributions inherent in inter-vehicle networks. To overcome these challenges, this paper explores Distributed Federated Learning (DFL), whereby vehicles collaboratively train deep learning models by exchanging model updates among one-hop neighbors and propagating models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi) Extension Dataset, we show that DFL can significantly improve classification accuracy across all vehicles compared to learning strictly with local data. Notably, vehicles with low individual accuracy see substantial accuracy gains through DFL, illustrating the benefit of knowledge sharing across the network. We further show that local training data size and time-varying network connectivity correlate strongly with the model's overall accuracy. We investigate DFL's resilience and vulnerabilities under attacks in multiple domains, namely wireless jamming and training data poisoning attacks. Our results reveal important insights into the vulnerabilities of DFL when confronted with multi-domain attacks, underlining the need for more robust strategies to secure DFL in vehicular networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23706v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, Mengran Xue</dc:creator>
    </item>
    <item>
      <title>On the Convergence Analysis of Muon</title>
      <link>https://arxiv.org/abs/2505.23737</link>
      <description>arXiv:2505.23737v1 Announce Type: cross 
Abstract: The majority of parameters in neural networks are naturally represented as matrices. However, most commonly used optimizers treat these matrix parameters as flattened vectors during optimization, potentially overlooking their inherent structural properties. Recently, an optimizer called Muon has been proposed, specifically designed to optimize matrix-structured parameters. Extensive empirical evidence shows that Muon can significantly outperform traditional optimizers when training neural networks. Nonetheless, the theoretical understanding of Muon's convergence behavior and the reasons behind its superior performance remain limited. In this work, we present a comprehensive convergence rate analysis of Muon and its comparison with Gradient Descent (GD). We further characterize the conditions under which Muon can outperform GD. Our theoretical results reveal that Muon can benefit from the low-rank and approximate blockwise diagonal structure of Hessian matrices -- phenomena widely observed in practical neural network training. Our experimental results support and corroborate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23737v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Shen, Ruichuan Huang, Minhui Huang, Cong Shen, Jiawei Zhang</dc:creator>
    </item>
    <item>
      <title>A Superposition Code-Based Semantic Communication Approach with Quantifiable and Controllable Security</title>
      <link>https://arxiv.org/abs/2401.13980</link>
      <description>arXiv:2401.13980v3 Announce Type: replace 
Abstract: This paper addresses the challenge of achieving security in semantic communication (SemCom) over a wiretap channel, where a legitimate receiver coexists with an eavesdropper experiencing a poorer channel condition. Despite previous efforts to secure SemCom against eavesdroppers, guarantee of approximately zero information leakage remains an open issue. In this work, we propose a secure SemCom approach based on superposition codes, aiming to provide quantifiable and controllable security for digital SemCom systems. The proposed method employs a double-layered constellation map, where semantic information is associated with satellite constellation points and cloud center constellation points are randomly selected. By carefully allocating power between these two layers of constellation, we ensure that the symbol error probability (SEP) of the eavesdropper decoding satellite constellation points is nearly equivalent to random guessing, while maintaining a low SEP for the legitimate receiver to successfully decode the semantic information. Simulation results demonstrate that the peak signal-to-noise ratio (PSNR) and mean squared error (MSE) of the eavesdropper' s reconstructed data, under the proposed method, can range from decoding Gaussian-distributed random noise to approaching the variance of the data. This validates the effectiveness of our method in nearly achieving the experimental upper bound of security for digital SemCom systems when both eavesdroppers and legitimate users utilize identical decoding schemes. Furthermore, the proposed method consistently outperforms benchmark techniques, showcasing superior data security and robustness against eavesdropping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13980v3</guid>
      <category>cs.IT</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixuan Chen, Shuo Shao, Qianqian Yang, Zhaoyang Zhang, Ping Zhang</dc:creator>
    </item>
    <item>
      <title>Surveying the space of descriptions of a composite system with machine learning</title>
      <link>https://arxiv.org/abs/2411.18579</link>
      <description>arXiv:2411.18579v2 Announce Type: replace 
Abstract: Multivariate information theory provides a general and principled framework for understanding how the components of a complex system are connected. Existing analyses are coarse in nature -- built up from characterizations of discrete subsystems -- and can be computationally prohibitive. In this work, we propose to study the continuous space of possible descriptions of a composite system as a window into its organizational structure. A description consists of specific information conveyed about each of the components, and the space of possible descriptions is equivalent to the space of lossy compression schemes of the components. We introduce a machine learning framework to optimize descriptions that extremize key information theoretic quantities used to characterize organization, such as total correlation and O-information. Through case studies on spin systems, sudoku boards, and letter sequences from natural language, we identify extremal descriptions that reveal how system-wide variation emerges from individual components. By integrating machine learning into a fine-grained information theoretic analysis of composite random variables, our framework opens a new avenues for probing the structure of real-world complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18579v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/gxrh-2xsv</arxiv:DOI>
      <dc:creator>Kieran A. Murphy, Yujing Zhang, Dani S. Bassett</dc:creator>
    </item>
    <item>
      <title>Rate-reliability tradeoff for deterministic identification</title>
      <link>https://arxiv.org/abs/2502.02389</link>
      <description>arXiv:2502.02389v3 Announce Type: replace 
Abstract: We investigate deterministic identification over arbitrary memoryless channels under the constraint that the error probabilities of first and second kind are exponentially small in the block length $n$, controlled by reliability exponents $E_1,E_2 \geq 0$. In contrast to the regime of slowly vanishing errors, where the identifiable message length scales linearithmically as $\Theta(n\log n)$, here we find that for positive exponents linear scaling is restored, now with a rate that is a function of the reliability exponents. We give upper and lower bounds on the ensuing rate-reliability function in terms of (the logarithm of) the packing and covering numbers of the channel output set, which for small error exponents $E_1,E_2&gt;0$ can be expanded in leading order as the product of the Minkowski dimension of a certain parametrisation the channel output set and $\log\min\{E_1,E_2\}$. These allow us to recover the previously observed slightly superlinear identification rates, and offer a different perspective for understanding them in more traditional information theory terms. We also show that even if only one of the two errors is required to be exponentially small, the linearithmic scaling is lost. We further illustrate our results with a discussion of the case of dimension zero, and extend them to classical-quantum channels and quantum channels with tensor product input restriction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02389v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>quant-ph</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pau Colomer, Christian Deppe, Holger Boche, Andreas Winter</dc:creator>
    </item>
    <item>
      <title>Concurrent Composition for Interactive Differential Privacy with Adaptive Privacy-Loss Parameters</title>
      <link>https://arxiv.org/abs/2309.05901</link>
      <description>arXiv:2309.05901v2 Announce Type: replace-cross 
Abstract: In this paper, we study the concurrent composition of interactive mechanisms with adaptively chosen privacy-loss parameters. In this setting, the adversary can interleave queries to existing interactive mechanisms, as well as create new ones. We prove that every valid privacy filter and odometer for noninteractive mechanisms extends to the concurrent composition of interactive mechanisms if privacy loss is measured using $(\epsilon, \delta)$-DP, $f$-DP, or R\'enyi DP of fixed order. Our results offer strong theoretical foundations for enabling full adaptivity in composing differentially private interactive mechanisms, showing that concurrency does not affect the privacy guarantees. We also provide an implementation for users to deploy in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05901v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Haney, Michael Shoemate, Grace Tian, Salil Vadhan, Andrew Vyrros, Vicki Xu, Wanrong Zhang</dc:creator>
    </item>
    <item>
      <title>Towards Quantum-Native Communication Systems: State-of-the-Art, Trends, and Challenges</title>
      <link>https://arxiv.org/abs/2311.05239</link>
      <description>arXiv:2311.05239v4 Announce Type: replace-cross 
Abstract: The potential synergy between quantum communications and future wireless communication systems is explored. By proposing a quantum-native or quantum-by-design philosophy, the survey examines technologies such as quantumdomain (QD) multi-input multi-output, QD non-orthogonal multiple access, quantum secure direct communication, QD resource allocation, QD routing, and QD artificial intelligence. The recent research advances in these areas are summarized. Given the behavior of photonic and particle-like Terahertz (THz) systems, a comprehensive system-oriented perspective is adopted to assess the feasibility of using quantum communications in future systems. This survey also reviews quantum optimization algorithms and quantum neural networks to explore the potential integration of quantum communication and quantum computing in future systems. Additionally, the current status of quantum sensing, quantum radar, and quantum timing is briefly reviewed in support of future applications. The associated research gaps and future directions are identified, including extending the entanglement coherence time, developing THz quantum communications devices, addressing challenges in channel estimation and tracking, and establishing the theoretical bounds and performance trade-offs of quantum communication, computing, and sensing. This survey offers a unique perspective on the potential for quantum communications to revolutionize future systems and pave the way for even more advanced technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05239v4</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolin Zhou, Anqi Shen, Shuyan Hu, Wei Ni, Xin Wang, Ekram Hossain</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees on the best-of-n alignment policy</title>
      <link>https://arxiv.org/abs/2401.01879</link>
      <description>arXiv:2401.01879v3 Announce Type: replace-cross 
Abstract: A simple and effective method for the inference-time alignment and scaling test-time compute of generative models is best-of-$n$ sampling, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n &lt; 1000$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01879v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh</dc:creator>
    </item>
    <item>
      <title>Entrywise dynamics and universality of general first order methods</title>
      <link>https://arxiv.org/abs/2406.19061</link>
      <description>arXiv:2406.19061v2 Announce Type: replace-cross 
Abstract: General first order methods (GFOMs), including various gradient descent and AMP algorithms, constitute a broad class of iterative algorithms in modern statistical learning problems. Some GFOMs also serve as constructive proof devices, iteratively characterizing the empirical distributions of statistical estimators in the large system limits for any fixed number of iterations.
  This paper develops a non-asymptotic, entrywise characterization for a general class of GFOMs. Our characterizations capture the precise entrywise behavior of the GFOMs, and hold universally across a broad class of heterogeneous random matrix models. As a corollary, we provide the first non-asymptotic description of the empirical distributions of the GFOMs beyond Gaussian ensembles.
  We demonstrate the utility of these general results in two applications. In the first application, we prove entrywise universality for regularized least squares estimators in the linear model, by controlling the entrywise error relative to a suitably constructed GFOM. This algorithmic proof method also leads to systematically improved averaged universality results for regularized regression estimators in the linear model, and resolves the universality conjecture for (regularized) MLEs in logistic regression. In the second application, we obtain entrywise Gaussian approximations for a class of gradient descent algorithms. Our approach provides non-asymptotic state evolution for the bias and variance of the algorithm along the iteration path, applicable for non-convex loss functions.
  The proof relies on a new recursive leave-k-out method that provides almost delocalization for the GFOMs and their derivatives. Crucially, our method ensures entrywise universality for up to poly-logarithmic many iterations, which facilitates effective $\ell_2/\ell_\infty$ control between certain GFOMs and statistical estimators in applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19061v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han</dc:creator>
    </item>
    <item>
      <title>Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2410.03972</link>
      <description>arXiv:2410.03972v2 Announce Type: replace-cross 
Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions-a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks-flip-flop memory, sine wave generation, delayed discrimination, and path integration-while systematically varying task complexity, learning regime, network size, and regularization. We find that higher task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers aiming to tailor RNN solutions-whether to uncover shared neural mechanisms or to model individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03972v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ann Huang, Satpreet H. Singh, Flavio Martinelli, Kanaka Rajan</dc:creator>
    </item>
    <item>
      <title>On the Training Convergence of Transformers for In-Context Classification of Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2410.11778</link>
      <description>arXiv:2410.11778v3 Announce Type: replace-cross 
Abstract: Although transformers have demonstrated impressive capabilities for in-context learning (ICL) in practice, theoretical understanding of the underlying mechanism that allows transformers to perform ICL is still in its infancy. This work aims to theoretically study the training dynamics of transformers for in-context classification tasks. We demonstrate that, for in-context classification of Gaussian mixtures under certain assumptions, a single-layer transformer trained via gradient descent converges to a globally optimal model at a linear rate. We further quantify the impact of the training and testing prompt lengths on the ICL inference error of the trained transformer. We show that when the lengths of training and testing prompts are sufficiently large, the prediction of the trained transformer approaches the ground truth distribution of the labels. Experimental results corroborate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11778v3</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Shen, Ruida Zhou, Jing Yang, Cong Shen</dc:creator>
    </item>
    <item>
      <title>On the Efficacy of the Peeling Decoder for the Quantum Expander Code</title>
      <link>https://arxiv.org/abs/2504.21845</link>
      <description>arXiv:2504.21845v2 Announce Type: replace-cross 
Abstract: The problem of recovering from qubit erasures has recently gained attention as erasures occur in many physical systems such as photonic systems, trapped ions, superconducting qubits and circuit quantum electrodynamics. While several linear-time decoders for error correction are known, their error-correcting capability is limited to half the minimum distance of the code, whereas erasure correction allows one to go beyond this limit. As in the classical case, stopping sets pose a major challenge in designing efficient erasure decoders for quantum LDPC codes. In this paper, we show through simulation, that an attractive alternative here, is the use of quantum expander codes in conjunction with the peeling decoder that has linear complexity. We also discuss additional techniques including small-set-flip decoding, that can be applied following the peeling operation, to improve decoding performance and their associated complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21845v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jefrin Sharmitha Prabhu, Abhinav Vaishya, Shobhit Bhatnagar, Aryaman Manish Kolhe, V. Lalitha, P. Vijay Kumar</dc:creator>
    </item>
  </channel>
</rss>
