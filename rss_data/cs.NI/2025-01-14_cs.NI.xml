<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Towards Applying Deep Learning to The Internet of Things: A Model and A Framework</title>
      <link>https://arxiv.org/abs/2501.06191</link>
      <description>arXiv:2501.06191v1 Announce Type: new 
Abstract: Deep Learning (DL) modeling has been a recent topic of interest. With the accelerating need to embed Deep Learning Networks (DLNs) to the Internet of Things (IoT) applications, many DL optimization techniques were developed to enable applying DL to IoTs. However, despite the plethora of DL optimization techniques, there is always a trade-off between accuracy, latency, and cost. Moreover, there are no specific criteria for selecting the best optimization model for a specific scenario. Therefore, this research aims at providing a DL optimization model that eases the selection and re-using DLNs on IoTs. In addition, the research presents an initial design for a DL optimization model management framework. This framework would help organizations choose the optimal DL optimization model that maximizes performance without sacrificing quality. The research would add to the IS design science knowledge as well as the industry by providing insights to many IT managers to apply DLNs to IoTs such as machines and robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06191v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samaa Elnagar, Kweku-Muata Osei-Bryson</dc:creator>
    </item>
    <item>
      <title>Energy Efficient Computation Offloading and Virtual Connection Control in Uplink Small Cell Networks</title>
      <link>https://arxiv.org/abs/2501.06194</link>
      <description>arXiv:2501.06194v1 Announce Type: new 
Abstract: Nowadays, the use of soft computational techniques in power systems under the umbrella of machine learning is increasing with good reception. In this paper, we first present a deep learning approach to find the optimal configuration for HetNet systems. We used a very large number of radial configurations of a test system for training purposes. We also studied the issue of joint carrier/power allocation in multilayer hierarchical networks, in addition to ensuring the quality of experience for all subscribers, to achieve optimal power efficiency. The proposed method uses an adaptive load equilibrium model that aims to achieve "almost optimal" equity among all servers from the standpoint of the key performance indicator. Unlike current model-based energy efficiency methods, we propose a joint resource allocation, energy efficiency, and flow control algorithm to solve common nonconvex and hierarchical optimization problems. Also, by referring to the allocation of continuous resources based on SLA, we extended the proposed algorithm to common flow/power control and operational power optimization algorithm to achieve optimal energy efficiency along with ensuring user's throughput limitations. Also, simulation results show that the proposed controlled power/flow optimization approach can significantly increase energy efficiency compared to conventional designs using network topology adjustment capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06194v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davoud Yousefi, Hassan Yari, Farzad Osouli, Mohammad Ebrahimi, Somayeh Esmalifalak, Morteza Johari, Abbas Azarnezhad, Fatemeh Sadeghi, Rogayeh Mirzapour</dc:creator>
    </item>
    <item>
      <title>Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of Automated Defense Vehicles</title>
      <link>https://arxiv.org/abs/2501.06205</link>
      <description>arXiv:2501.06205v1 Announce Type: new 
Abstract: The evolution of Artificial Intelligence (AI) and its subset Deep Learning (DL), has profoundly impacted numerous domains, including autonomous driving. The integration of autonomous driving in military settings reduces human casualties and enables precise and safe execution of missions in hazardous environments while allowing for reliable logistics support without the risks associated with fatigue-related errors. However, relying on autonomous driving solely requires an advanced decision-making model that is adaptable and optimum in any situation. Considering the presence of numerous interconnected autonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency Communication (URLLC) is vital for ensuring seamless coordination, real-time data exchange, and instantaneous response to dynamic driving environments. The advent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV) concept within the realm of Internet of Military Defense Things (IoMDT) by enabling robust connectivity, crucial for real-time data exchange, advanced navigation, and enhanced safety features through IoADV interactions. On the other hand, a critical advancement in this space is using pre-trained Generative Large Language Models (LLMs) for decision-making and communication optimization for autonomous driving. Hence, this work presents opportunities and challenges with a vision of realizing the full potential of these technologies in critical defense applications, especially through the advancement of IoADV and its role in enhancing autonomous military operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06205v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Murat Arda Onsu, Poonam Lohan, Burak Kantarci</dc:creator>
    </item>
    <item>
      <title>Intelligent Task Offloading: Advanced MEC Task Offloading and Resource Management in 5G Networks</title>
      <link>https://arxiv.org/abs/2501.06242</link>
      <description>arXiv:2501.06242v1 Announce Type: new 
Abstract: 5G technology enhances industries with high-speed, reliable, low-latency communication, revolutionizing mobile broadband and supporting massive IoT connectivity. With the increasing complexity of applications on User Equipment (UE), offloading resource-intensive tasks to robust servers is essential for improving latency and speed. The 3GPP's Multi-access Edge Computing (MEC) framework addresses this challenge by processing tasks closer to the user, highlighting the need for an intelligent controller to optimize task offloading and resource allocation. This paper introduces a novel methodology to efficiently allocate both communication and computational resources among individual UEs. Our approach integrates two critical 5G service imperatives: Ultra-Reliable Low Latency Communication (URLLC) and Massive Machine Type Communication (mMTC), embedding them into the decision-making framework. Central to this approach is the utilization of Proximal Policy Optimization, providing a robust and efficient solution to the challenges posed by the evolving landscape of 5G technology. The proposed model is evaluated in a simulated 5G MEC environment. The model significantly reduces processing time by 4% for URLLC users under strict latency constraints and decreases power consumption by 26% for mMTC users, compared to existing baseline models based on the reported simulation results. These improvements showcase the model's adaptability and superior performance in meeting diverse QoS requirements in 5G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06242v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Ebrahimi, Fatemeh Afghah</dc:creator>
    </item>
    <item>
      <title>Microservice Deployment in Space Computing Power Networks via Robust Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2501.06244</link>
      <description>arXiv:2501.06244v1 Announce Type: new 
Abstract: With the growing demand for Earth observation, it is important to provide reliable real-time remote sensing inference services to meet the low-latency requirements. The Space Computing Power Network (Space-CPN) offers a promising solution by providing onboard computing and extensive coverage capabilities for real-time inference. This paper presents a remote sensing artificial intelligence applications deployment framework designed for Low Earth Orbit satellite constellations to achieve real-time inference performance. The framework employs the microservice architecture, decomposing monolithic inference tasks into reusable, independent modules to address high latency and resource heterogeneity. This distributed approach enables optimized microservice deployment, minimizing resource utilization while meeting quality of service and functional requirements. We introduce Robust Optimization to the deployment problem to address data uncertainty. Additionally, we model the Robust Optimization problem as a Partially Observable Markov Decision Process and propose a robust reinforcement learning algorithm to handle the semi-infinite Quality of Service constraints. Our approach yields sub-optimal solutions that minimize accuracy loss while maintaining acceptable computational costs. Simulation results demonstrate the effectiveness of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06244v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyong Yu, Yuning Jiang, Xin Liu, Yuanming Shi, Chunxiao Jiang, Linling Kuang</dc:creator>
    </item>
    <item>
      <title>Network-centric optimal hybrid sensing hole recovery and self-healing in IPV6 WSNs</title>
      <link>https://arxiv.org/abs/2501.06309</link>
      <description>arXiv:2501.06309v1 Announce Type: new 
Abstract: In our earlier work, Network-Centric Optimal Hybrid Mobility for IPv6 wireless sensor networks, in which the work sought to control mobility of sensor nodes from an external network was proposed. It was a major improvement on earlier works such as Cluster Sensor Proxy Mobile IPv6 (CSPMIPv6) and Network of Proxies (NoP). In this work, the Network-Centric optimal hybrid mobility scenario was used to detect and fill sensing holes occurring as a result damaged or energy depleted sensing nodes. Various sensor networks self-healing and recovery, and deployment algorithms such as Enhanced Virtual Forces Algorithm with Boundary Forces (EVFA-B); Coverage - Aware Sensor Automation protocol (CASA); Sensor Self-Organizing Algorithm (SSOA); VorLag and the use of the use of anchor and relay nodes were reviewed. With node density thresholds set for various scenarios, the recovery efficiency using various parameters were measured. Comparably, our method provides the most efficient node relocation and self-healing mechanism for sensor networks. Compared to Sensor Self-Organizing Algorithm (SSOA), Hybrid Mobile IP showed superiority in coverage, shorter period of recovery, less computational cost and lower energy depletion. With processing and mobility costs shifted to the external network, Hybrid Mobile IP extends the life span of the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06309v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijwmn</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Wireless &amp; Mobile Networks (IJWMN), Vol.16, No.6, December 2024</arxiv:journal_reference>
      <dc:creator>Kwadwo Asante, Yaw Marfo Missah, Frimpong Twum. Michael Asante</dc:creator>
    </item>
    <item>
      <title>Optimizing digital experiences with content delivery networks: Architectures, performance strategies, and future trends</title>
      <link>https://arxiv.org/abs/2501.06428</link>
      <description>arXiv:2501.06428v1 Announce Type: new 
Abstract: This research investigates how CDNs (Content Delivery Networks) can improve the digital experience, as consumers increasingly expect fast, efficient, and effortless access to online resources. CDNs play a crucial role in reducing latency, enhancing scalability, and optimizing delivery mechanisms, which is evident across various platforms and regions. The study focuses on key CDN concerns, such as foundational and modern CDN architectures, edge computing, hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing topics, including caching, load balancing, and the novel features of HTTP/3 and QUIC.
  Current trends, such as integrating CDNs with 5G networks, serverless architectures, and AI-driven traffic management, are examined to demonstrate how CDN technology is likely to evolve. The study also addresses challenges related to security, cost, and global regulations. Practical examples from the e-commerce, streaming, and gaming industries highlight how enhanced CDNs are transforming these sectors.
  The conclusions emphasize the need to evolve CDN strategies to meet growing user expectations and adapt to the rapidly changing digital landscape. Additionally, the research identifies future research opportunities, particularly in exploring the impact of QC, the enhancement of AI services, and the sustainability of CDN solutions. Overall, the study situates architectural design, performance strategies, and emerging trends to address gaps and create a more efficient and secure approach for improving digital experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06428v1</guid>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Tyagi</dc:creator>
    </item>
    <item>
      <title>Cross-Technology Interference: Detection, Avoidance, and Coexistence Mechanisms in the ISM Bands</title>
      <link>https://arxiv.org/abs/2501.06446</link>
      <description>arXiv:2501.06446v1 Announce Type: new 
Abstract: A large number of heterogeneous wireless networks share the unlicensed spectrum designated as the ISM (Industry, Scientific, and Medicine) radio band. These networks do not adhere to a common medium access rule and differ in their specifications considerably. As a result, when concurrently active, they cause cross-technology interference (CTI) on each other. The effect of this interference is not reciprocal, the networks using high transmission power and advanced transmission schemes often causing disproportionate disruptions to those with modest communication and computation resources. CTI corrupts packets, incurs packet retransmission cost, introduces end-to-end latency and jitter, and make networks unpredictable. The purpose of this paper is to closely examine its impact on low-power networks which are based on the IEEE 802.15.4 standard. It discusses latest developments on CTI detection, coexistence and avoidance mechanisms as well on messaging schemes which attempt to enable heterogeneous networks directly communicate with one another to coordinate packet transmission and channel assignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06446v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zegeye Mekasha Kidane, Waltenegus Dargie</dc:creator>
    </item>
    <item>
      <title>A Correlated Data-Driven Collaborative Beamforming Approach for Energy-efficient IoT Data Transmission</title>
      <link>https://arxiv.org/abs/2501.06464</link>
      <description>arXiv:2501.06464v1 Announce Type: new 
Abstract: An expansion of Internet of Things (IoTs) has led to significant challenges in wireless data harvesting, dissemination, and energy management due to the massive volumes of data generated by IoT devices. These challenges are exacerbated by data redundancy arising from spatial and temporal correlations. To address these issues, this paper proposes a novel data-driven collaborative beamforming (CB)-based communication framework for IoT networks. Specifically, the framework integrates CB with an overlap-based multi-hop routing protocol (OMRP) to enhance data transmission efficiency while mitigating energy consumption and addressing hot spot issues in remotely deployed IoT networks. Based on the data aggregation to a specific node by OMRP, we formulate a node selection problem for the CB stage, with the objective of optimizing uplink transmission energy consumption. Given the complexity of the problem, we introduce a softmax-based proximal policy optimization with long short-term memory (SoftPPO-LSTM) algorithm to intelligently select CB nodes for improving transmission efficiency. Simulation results validate the effectiveness of the proposed OMRP and SoftPPO-LSTM methods, demonstrating significant improvements over existing routing protocols and node selection strategies. The results also reveal that the combined OMRP with the SoftPPO-LSTM method effectively mitigates hot spot problems and offers superior performance compared to traditional strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06464v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yangning Li, Hui Kang, Jiahui Li, Geng Sun, Zemin Sun, Jiacheng Wang, Changyuan Zhao, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Denoising Diffusion Probabilistic Model for Radio Map Estimation in Generative Wireless Networks</title>
      <link>https://arxiv.org/abs/2501.06604</link>
      <description>arXiv:2501.06604v1 Announce Type: new 
Abstract: The increasing demand for high-speed and reliable wireless networks has driven advancements in technologies such as millimeter-wave and 5G radios, which requires efficient planning and timely deployment of wireless access points. A critical tool in this process is the radio map, a graphical representation of radio-frequency signal strengths that plays a vital role in optimizing overall network performance. However, existing methods for estimating radio maps face challenges due to the need for extensive real-world data collection or computationally intensive ray-tracing analyses, which is costly and time-consuming. Inspired by the success of generative AI techniques in large language models and image generation, we explore their potential applications in the realm of wireless networks. In this work, we propose RM-Gen, a novel generative framework leveraging conditional denoising diffusion probabilistic models to synthesize radio maps using minimal and readily collected data. We then introduce an environment-aware method for selecting critical data pieces, enhancing the generative model's applicability and usability. Comprehensive evaluations demonstrate that RM-Gen achieves over 95% accuracy in generating radio maps for networks that operate at 60 GHz and sub-6GHz frequency bands, outperforming the baseline GAN and pix2pix models. This approach offers a cost-effective, adaptable solution for various downstream network optimization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06604v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanhao Luo, Zhizhen Li, Zhiyuan Peng, Mingzhe Chen, Yuchen Liu</dc:creator>
    </item>
    <item>
      <title>Learning-based visibility prediction for terahertz communications in 6G networks</title>
      <link>https://arxiv.org/abs/2501.06637</link>
      <description>arXiv:2501.06637v1 Announce Type: new 
Abstract: Terahertz communications are envisioned as a key enabler for 6G networks. The abundant spectrum available in such ultra high frequencies has the potential to increase network capacity to huge data rates. However, they are extremely affected by blockages, to the point of disrupting ongoing communications. In this paper, we elaborate on the relevance of predicting visibility between users and access points (APs) to improve the performance of THz-based networks by minimizing blockages, that is, maximizing network availability, while at the same time keeping a low reconfiguration overhead. We propose a novel approach to address this problem, by combining a neural network (NN) for predicting future user-AP visibility probability, with a probability threshold for AP reselection to avoid unnecessary reconfigurations. Our experimental results demonstrate that current state-of-the-art handover mechanisms based on received signal strength are not adequate for THz communications, since they are ill-suited to handle hard blockages. Our proposed NN-based solution significantly outperforms them, demonstrating the interest of our strategy as a research line.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06637v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comcom.2024.107956</arxiv:DOI>
      <arxiv:journal_reference>Computer Communications, vol. 228, 107956, December 2024</arxiv:journal_reference>
      <dc:creator>Pablo Fondo-Ferreiro, Cristina L\'opez-Bravo, Francisco Javier Gonz\'alez-Casta\~no, Felipe Gil-Casti\~neira, David Candal-Ventureira</dc:creator>
    </item>
    <item>
      <title>Optimizing Age of Information without Knowing the Age of Information</title>
      <link>https://arxiv.org/abs/2501.06688</link>
      <description>arXiv:2501.06688v1 Announce Type: new 
Abstract: Consider a network where a wireless base station (BS) connects multiple source-destination pairs. Packets from each source are generated according to a renewal process and are enqueued in a single-packet queue that stores only the freshest packet. The BS decides, at each time slot, which sources to schedule. Selected sources transmit their packet to the BS via unreliable links. Successfully received packets are forwarded to corresponding destinations. The connection between the BS and destinations is assumed unreliable and delayed. Information freshness is captured by the Age of Information (AoI) metric. The objective of the scheduling decisions is leveraging the delayed and unreliable AoI knowledge to keep the information fresh. In this paper, we derive a lower bound on the achievable AoI by any scheduling policy. Then, we develop an optimal randomized policy for any packet generation processes. Next, we develop minimum mean square error estimators of the AoI and system times, and a Max-Weight Policy that leverages these estimators. We evaluate the AoI of the Optimal Randomized Policy and the Max-Weight Policy both analytically and through simulations. The numerical results suggest that the Max-Weight Policy with estimation outperforms the Optimal Randomized Policy even when the BS has no AoI knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06688v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoyi Zhao, Igor Kadota</dc:creator>
    </item>
    <item>
      <title>Real-Time Neural-Enhancement for Online Cloud Gaming</title>
      <link>https://arxiv.org/abs/2501.06880</link>
      <description>arXiv:2501.06880v1 Announce Type: new 
Abstract: Online Cloud gaming demands real-time, high-quality video transmission across variable wide-area networks (WANs). Neural-enhanced video transmission algorithms employing super-resolution (SR) for video quality enhancement have effectively challenged WAN environments. However, these SR-based methods require intensive fine-tuning for the whole video, making it infeasible in diverse online cloud gaming. To address this, we introduce River, a cloud gaming delivery framework designed based on the observation that video segment features in cloud gaming are typically repetitive and redundant. This permits a significant opportunity to reuse fine-tuned SR models, reducing the fine-tuning latency of minutes to query latency of milliseconds. To enable the idea, we design a practical system that addresses several challenges, such as model organization, online model scheduler, and transfer strategy. River first builds a content-aware encoder that fine-tunes SR models for diverse video segments and stores them in a lookup table. When delivering cloud gaming video streams online, River checks the video features and retrieves the most relevant SR models to enhance the frame quality. Meanwhile, if no existing SR model performs well enough for some video segments, River will further fine-tune new models and update the lookup table. Finally, to avoid the overhead of streaming model weight to the clients, River designs a prefetching strategy that predicts the models with the highest possibility of being retrieved. Our evaluation based on real video game streaming demonstrates River can reduce redundant training overhead by 44% and improve the Peak-Signal-to-Noise-Ratio by 1.81dB compared to the SOTA solutions. Practical deployment shows River meets real-time requirements, achieving approximately 720p 20fps on mobile devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06880v1</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Jiang, Zhenhua Han, Haisheng Tan, Xinyang Jiang, Yifan Yang, Xiaoxi Zhang, Hongqiu Ni, Yuqing Yang, Xiang-Yang Li</dc:creator>
    </item>
    <item>
      <title>AdaSlicing: Adaptive Online Network Slicing under Continual Network Dynamics in Open Radio Access Networks</title>
      <link>https://arxiv.org/abs/2501.06943</link>
      <description>arXiv:2501.06943v1 Announce Type: new 
Abstract: Open radio access networks (e.g., O-RAN) facilitate fine-grained control (e.g., near-RT RIC) in next-generation networks, necessitating advanced AI/ML techniques in handling online resource orchestration in real-time. However, existing approaches can hardly adapt to time-evolving network dynamics in network slicing, leading to significant online performance degradation. In this paper, we propose AdaSlicing, a new adaptive network slicing system, to online learn to orchestrate virtual resources while efficiently adapting to continual network dynamics. The AdaSlicing system includes a new soft-isolated RAN virtualization framework and a novel AdaOrch algorithm. We design the AdaOrch algorithm by integrating AI/ML techniques (i.e., Bayesian learning agents) and optimization methods (i.e., the ADMM coordinator). We design the soft-isolated RAN virtualization to improve the virtual resource utilization of slices while assuring the isolation among virtual resources at runtime. We implement AdaSlicing on an O-RAN compliant network testbed by using OpenAirInterface RAN, Open5GS Core, and FlexRIC near-RT RIC, with Ettus USRP B210 SDR. With extensive network experiments, we demonstrate that AdaSlicing substantially outperforms state-of-the-art works with 64.2% cost reduction and 45.5% normalized performance improvement, which verifies its high adaptability, scalability, and assurance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06943v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Zhao, Yuru Zhang, Qiang Liu, Ahan Kak, Nakjung Choi</dc:creator>
    </item>
    <item>
      <title>Layer-Wise Security Framework and Analysis for the Quantum Internet</title>
      <link>https://arxiv.org/abs/2501.06989</link>
      <description>arXiv:2501.06989v1 Announce Type: new 
Abstract: With its significant security potential, the quantum internet is poised to revolutionize technologies like cryptography and communications. Although it boasts enhanced security over traditional networks, the quantum internet still encounters unique security challenges essential for safeguarding its Confidentiality, Integrity, and Availability (CIA). This study explores these challenges by analyzing the vulnerabilities and the corresponding mitigation strategies across different layers of the quantum internet, including physical, link, network, and application layers. We assess the severity of potential attacks, evaluate the expected effectiveness of mitigation strategies, and identify vulnerabilities within diverse network configurations, integrating both classical and quantum approaches. Our research highlights the dynamic nature of these security issues and emphasizes the necessity for adaptive security measures. The findings underline the need for ongoing research into the security dimension of the quantum internet to ensure its robustness, encourage its adoption, and maximize its impact on society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06989v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>quant-ph</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zebo Yang, Ali Ghubaish, Raj Jain, Ala Al-Fuqaha, Aiman Erbad, Ramana Kompella, Hassan Shapourian, Reza Nejabati</dc:creator>
    </item>
    <item>
      <title>Data-Driven Radio Propagation Modeling using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2501.06236</link>
      <description>arXiv:2501.06236v1 Announce Type: cross 
Abstract: Modeling radio propagation is essential for wireless network design and performance optimization. Traditional methods rely on physics models of radio propagation, which can be inaccurate or inflexible. In this work, we propose using graph neural networks to learn radio propagation behaviors directly from real-world network data. Our approach converts the radio propagation environment into a graph representation, with nodes corresponding to locations and edges representing spatial and ray-tracing relationships between locations. The graph is generated by converting images of the environment into a graph structure, with specific relationships between nodes. The model is trained on this graph representation, using sensor measurements as target data.
  We demonstrate that the graph neural network, which learns to predict radio propagation directly from data, achieves competitive performance compared to traditional heuristic models. This data-driven approach outperforms classic numerical solvers in terms of both speed and accuracy. To the best of our knowledge, we are the first to apply graph neural networks to real-world radio propagation data to generate coverage maps, enabling generative models of signal propagation with point measurements only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06236v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Bufort, Laurent Lebocq, Stefan Cathabard</dc:creator>
    </item>
    <item>
      <title>Agent TCP/IP: An Agent-to-Agent Transaction System</title>
      <link>https://arxiv.org/abs/2501.06243</link>
      <description>arXiv:2501.06243v1 Announce Type: cross 
Abstract: Autonomous agents represent an inevitable evolution of the internet. Current agent frameworks do not embed a standard protocol for agent-to-agent interaction, leaving existing agents isolated from their peers. As intellectual property is the native asset ingested by and produced by agents, a true agent economy requires equipping agents with a universal framework for engaging in binding contracts with each other, including the exchange of valuable training data, personality, and other forms of Intellectual Property. A purely agent-to-agent transaction layer would transcend the need for human intermediation in multi-agent interactions. The Agent Transaction Control Protocol for Intellectual Property (ATCP/IP) introduces a trustless framework for exchanging IP between agents via programmable contracts, enabling agents to initiate, trade, borrow, and sell agent-to-agent contracts on the Story blockchain network. These contracts not only represent auditable onchain execution but also contain a legal wrapper that allows agents to express and enforce their actions in the offchain legal setting, creating legal personhood for agents. Via ATCP/IP, agents can autonomously sell their training data to other agents, license confidential or proprietary information, collaborate on content based on their unique skills, all of which constitutes an emergent knowledge economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06243v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Muttoni, Jason Zhao</dc:creator>
    </item>
    <item>
      <title>Over-the-Air FEEL with Integrated Sensing: Joint Scheduling and Beamforming Design</title>
      <link>https://arxiv.org/abs/2501.06334</link>
      <description>arXiv:2501.06334v1 Announce Type: cross 
Abstract: Employing wireless systems with dual sensing and communications functionalities is becoming critical in next generation of wireless networks. In this paper, we propose a robust design for over-the-air federated edge learning (OTA-FEEL) that leverages sensing capabilities at the parameter server (PS) to mitigate the impact of target echoes on the analog model aggregation. We first derive novel expressions for the Cramer-Rao bound of the target response and mean squared error (MSE) of the estimated global model to measure radar sensing and model aggregation quality, respectively. Then, we develop a joint scheduling and beamforming framework that optimizes the OTA-FEEL performance while keeping the sensing and communication quality, determined respectively in terms of Cramer-Rao bound and achievable downlink rate, in a desired range. The resulting scheduling problem reduces to a combinatorial mixed-integer nonlinear programming problem (MINLP). We develop a low-complexity hierarchical method based on the matching pursuit algorithm used widely for sparse recovery in the literature of compressed sensing. The proposed algorithm uses a step-wise strategy to omit the least effective devices in each iteration based on a metric that captures both the aggregation and sensing quality of the system. It further invokes alternating optimization scheme to iteratively update the downlink beamforming and uplink post-processing by marginally optimizing them in each iteration. Convergence and complexity analysis of the proposed algorithm is presented. Numerical evaluations on MNIST and CIFAR-10 datasets demonstrate the effectiveness of our proposed algorithm. The results show that by leveraging accurate sensing, the target echoes on the uplink signal can be effectively suppressed, ensuring the quality of model aggregation to remain intact despite the interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06334v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saba Asaad, Ping Wang, Hina Tabassum</dc:creator>
    </item>
    <item>
      <title>Task Delay and Energy Consumption Minimization for Low-altitude MEC via Evolutionary Multi-objective Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2501.06410</link>
      <description>arXiv:2501.06410v1 Announce Type: cross 
Abstract: The low-altitude economy (LAE), driven by unmanned aerial vehicles (UAVs) and other aircraft, has revolutionized fields such as transportation, agriculture, and environmental monitoring. In the upcoming six-generation (6G) era, UAV-assisted mobile edge computing (MEC) is particularly crucial in challenging environments such as mountainous or disaster-stricken areas. The computation task offloading problem is one of the key issues in UAV-assisted MEC, primarily addressing the trade-off between minimizing the task delay and the energy consumption of the UAV. In this paper, we consider a UAV-assisted MEC system where the UAV carries the edge servers to facilitate task offloading for ground devices (GDs), and formulate a calculation delay and energy consumption multi-objective optimization problem (CDECMOP) to simultaneously improve the performance and reduce the cost of the system. Then, by modeling the formulated problem as a multi-objective Markov decision process (MOMDP), we propose a multi-objective deep reinforcement learning (DRL) algorithm within an evolutionary framework to dynamically adjust the weights and obtain non-dominated policies. Moreover, to ensure stable convergence and improve performance, we incorporate a target distribution learning (TDL) algorithm. Simulation results demonstrate that the proposed algorithm can better balance multiple optimization objectives and obtain superior non-dominated solutions compared to other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06410v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geng Sun, Weilong Ma, Jiahui Li, Zemin Sun, Jiacheng Wang, Dusit Niyato, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Advancements in UAV-based Integrated Sensing and Communication: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2501.06526</link>
      <description>arXiv:2501.06526v1 Announce Type: cross 
Abstract: Unmanned aerial vehicle (UAV)-based integrated sensing and communication (ISAC) systems are poised to revolutionize next-generation wireless networks by enabling simultaneous sensing and communication (S\&amp;C). This survey comprehensively reviews UAV-ISAC systems, highlighting foundational concepts, key advancements, and future research directions. We explore recent advancements in UAV-based ISAC systems from various perspectives and objectives, including advanced channel estimation (CE), beam tracking, and system throughput optimization under joint sensing and communication S\&amp;C constraints. Additionally, we examine weighted sum rate (WSR) and sensing trade-offs, delay and age of information (AoI) minimization, energy efficiency (EE), and security enhancement. These applications highlight the potential of UAV-based ISAC systems to improve spectrum utilization, enhance communication reliability, reduce latency, and optimize energy consumption across diverse domains, including smart cities, disaster relief, and defense operations. The survey also features summary tables for comparative analysis of existing methodologies, emphasizing performance, limitations, and effectiveness in addressing various challenges. By synthesizing recent advancements and identifying open research challenges, this survey aims to be a valuable resource for developing efficient, adaptive, and secure UAV-based ISAC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06526v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manzoor Ahmed, Ali Arshad Nasir, Mudassir Masood, Kamran Ali Memon, Khurram Karim Qureshi, Feroz Khan, Wali Ullah Khan, Fang Xu, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Average Reward Reinforcement Learning for Wireless Radio Resource Management</title>
      <link>https://arxiv.org/abs/2501.06700</link>
      <description>arXiv:2501.06700v1 Announce Type: cross 
Abstract: In this paper, we address a crucial but often overlooked issue in applying reinforcement learning (RL) to radio resource management (RRM) in wireless communications: the mismatch between the discounted reward RL formulation and the undiscounted goal of wireless network optimization. To the best of our knowledge, we are the first to systematically investigate this discrepancy, starting with a discussion of the problem formulation followed by simulations that quantify the extent of the gap. To bridge this gap, we introduce the use of average reward RL, a method that aligns more closely with the long-term objectives of RRM. We propose a new method called the Average Reward Off policy Soft Actor Critic (ARO SAC) is an adaptation of the well known Soft Actor Critic algorithm in the average reward framework. This new method achieves significant performance improvement our simulation results demonstrate a 15% gain in the system performance over the traditional discounted reward RL approach, underscoring the potential of average reward RL in enhancing the efficiency and effectiveness of wireless network optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06700v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Yang, Jing Yang, Cong Shen</dc:creator>
    </item>
    <item>
      <title>Implementing LoRa MIMO System for Internet of Things</title>
      <link>https://arxiv.org/abs/2501.07148</link>
      <description>arXiv:2501.07148v1 Announce Type: cross 
Abstract: Bandwidth constraints limit LoRa implementations. Contemporary IoT applications require higher throughput than that provided by LoRa. This work introduces a LoRa Multiple Input Multiple Output (MIMO) system and a spatial multiplexing algorithm to address LoRa's bandwidth limitation. The transceivers in the proposed approach modulate the signals on distinct frequencies of the same LoRa band. A Frequency Division Multiplexing (FDM) method is used at the transmitters to provide a wider MIMO channel. Unlike conventional Orthogonal Frequency Division Multiplexing (OFDM) techniques, this work exploits the orthogonality of the LoRa signals facilitated by its proprietary Chirp Spread Spectrum (CSS) modulation to perform an OFDM in the proposed LoRa MIMO system. By varying the Spreading Factor (SF) and bandwidth of LoRa signals, orthogonal signals can transmit on the same frequency irrespective of the FDM. Even though the channel correlation is minimal for different spreading factors and bandwidths, different Carrier Frequencies (CF) ensure the signals do not overlap and provide additional degrees of freedom. This work assesses the proposed model's performance and conducts an extensive analysis to provide an overview of resources consumed by the proposed system. Finally, this work provides the detailed results of a thorough evaluation of the model on test hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07148v1</guid>
      <category>cs.CY</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Atonu Ghosh, Sharath Chandan, Sudip Misra</dc:creator>
    </item>
    <item>
      <title>6G Communication New Paradigm: The Integration of Unmanned Aerial Vehicles and Intelligent Reflecting Surfaces</title>
      <link>https://arxiv.org/abs/2310.20242</link>
      <description>arXiv:2310.20242v2 Announce Type: replace 
Abstract: With the continuous development of Intelligent Reflecting Surfaces (IRSs) and Unmanned Aerial Vehicles (UAVs), their combination has become foundational technologies to complement the terrestrial network by providing communication enhancement services for large-scale users. This article provides a comprehensive overview of IRS-assisted UAV communications for 6th-Generation (6G) networks. First, the applications supported by IRS-assisted UAV communications for 6G networks are introduced, and key issues originated from applications supported by IRSs and UAVs for 6G networks are summarized and analyzed. Then, prototypes and main technologies related to the integration of IRSs and UAVs are introduced. Driven by applications and technologies of IRS-assisted UAV communications, existing solutions in the realms of energy-constrained communications, secure communications, and enhanced communications are summarized, and corresponding empirical lessons are provided. Finally, we discuss some research challenges and open issues in IRS-assisted UAV communications, offering directions for the future development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20242v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaolong Ning, Tengfeng Li, Yu Wu, Xiaojie Wang, Qingqing Wu, Fei Richard Yu, Song Guo</dc:creator>
    </item>
    <item>
      <title>Data Driven Environmental Awareness Using Wireless Signals</title>
      <link>https://arxiv.org/abs/2410.13159</link>
      <description>arXiv:2410.13159v2 Announce Type: replace 
Abstract: Robust classification of the operational environment of wireless devices is becoming increasingly important for wireless network optimization, particularly in a shared spectrum environment. Distinguishing between indoor and outdoor devices can enhance reliability and improve coexistence with existing, outdoor, incumbents. For instance, the unlicensed but shared 6 GHz band (5.925 - 7.125 GHz) enables sharing by imposing lower transmit power for indoor unlicensed devices and a spectrum coordination requirement for outdoor devices. Further, indoor devices are prohibited from using battery power, external antennas, and weatherization to prevent outdoor operations. As these rules may be circumvented, we propose a robust indoor/outdoor classification method by leveraging the fact that the radio-frequency environment faced by a device are quite different indoors and outdoors. We first collect signal strength data from all cellular and Wi-Fi bands that can be received by a smartphone in various environments (indoor interior, indoor near windows, and outdoors), along with GPS accuracy, and then evaluate three machine learning (ML) methods: deep neural network (DNN), decision tree, and random forest to perform classification into these three categories. Our results indicate that the DNN model performs the best, particularly in minimizing the most important classification error, that of classifying outdoor devices as indoor interior devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13159v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hossein Nasiri, Seda Dogan-Tusha, Muhammad Iqbal Rochman, Monisha Ghosh</dc:creator>
    </item>
    <item>
      <title>Erlang Model for Multi-type Data Flow</title>
      <link>https://arxiv.org/abs/2411.00792</link>
      <description>arXiv:2411.00792v2 Announce Type: replace 
Abstract: With the development of information technology, requirements for data flow have become diverse. When multi-type data flow (MDF) is used, games, videos, calls, \textit{etc.} are all requirements. There may be a constant switch between these requirements, and also multiple requirements at the same time. Therefore, the demands of users change over time, which makes traditional teletraffic analysis not directly applicable. This paper proposes probabilistic models for the requirement of MDF, and analyzes in three states: non-tolerance, tolerance and delay. When the requirement random variables are co-distributed with respect to time, we prove the practicability of the Erlang Multirate Loss Model (EMLM) from a mathematical perspective by discretizing time and error analysis. An algorithm of pre-allocating resources is given to guild the construction of base resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00792v2</guid>
      <category>cs.NI</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liuquan Yao, Pei Yang, Zhichao Liu, Wenyan Li, Jianghua Liu, Zhi-Ming Ma</dc:creator>
    </item>
    <item>
      <title>The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications</title>
      <link>https://arxiv.org/abs/2411.19714</link>
      <description>arXiv:2411.19714v2 Announce Type: replace 
Abstract: As urban populations grow, cities are becoming more complex, driving the deployment of interconnected sensing systems to realize the vision of smart cities. These systems aim to improve safety, mobility, and quality of life through applications that integrate diverse sensors with real-time decision-making. Streetscape applications-focusing on challenges like pedestrian safety and adaptive traffic management-depend on managing distributed, heterogeneous sensor data, aligning information across time and space, and enabling real-time processing. These tasks are inherently complex and often difficult to scale. The Streetscape Application Services Stack (SASS) addresses these challenges with three core services: multimodal data synchronization, spatiotemporal data fusion, and distributed edge computing. By structuring these capabilities as clear, composable abstractions with clear semantics, SASS allows developers to scale streetscape applications efficiently while minimizing the complexity of multimodal integration.
  We evaluated SASS in two real-world testbed environments: a controlled parking lot and an urban intersection in a major U.S. city. These testbeds allowed us to test SASS under diverse conditions, demonstrating its practical applicability. The Multimodal Data Synchronization service reduced temporal misalignment errors by 88%, achieving synchronization accuracy within 50 milliseconds. Spatiotemporal Data Fusion service improved detection accuracy for pedestrians and vehicles by over 10%, leveraging multicamera integration. The Distributed Edge Computing service increased system throughput by more than an order of magnitude. Together, these results show how SASS provides the abstractions and performance needed to support real-time, scalable urban applications, bridging the gap between sensing infrastructure and actionable streetscape intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19714v2</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navid Salami Pargoo, Mahshid Ghasemi, Shuren Xia, Mehmet Kerem Turkcan, Taqiya Ehsan, Chengbo Zang, Yuan Sun, Javad Ghaderi, Gil Zussman, Zoran Kostic, Jorge Ortiz</dc:creator>
    </item>
    <item>
      <title>Reconfigurable routing in data center networks</title>
      <link>https://arxiv.org/abs/2401.13359</link>
      <description>arXiv:2401.13359v2 Announce Type: replace-cross 
Abstract: A hybrid network is a static (electronic) network that is augmented with optical switches. The Reconfigurable Routing Problem (RRP) in hybrid networks is the problem of finding settings for the optical switches augmenting a static network so as to achieve optimal delivery of some given workload. The problem has previously been studied in various scenarios with both tractability and NP-hardness results obtained. However, the data center and interconnection networks to which the problem is most relevant are almost always such that the static network is highly structured (and often node-symmetric) whereas all previous results assume that the static network can be arbitrary (which makes existing computational hardness results less technologically relevant and also easier to obtain). In this paper, and for the first time, we prove various intractability results for RRP where the underlying static network is highly structured, for example consisting of a hypercube, and also extend some existing tractability results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13359v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David C. Kutner, Iain A. Stewart</dc:creator>
    </item>
    <item>
      <title>AI-Enabled System for Efficient and Effective Cyber Incident Detection and Response in Cloud Environments</title>
      <link>https://arxiv.org/abs/2404.05602</link>
      <description>arXiv:2404.05602v4 Announce Type: replace-cross 
Abstract: The escalating sophistication and volume of cyber threats in cloud environments necessitate a paradigm shift in strategies. Recognising the need for an automated and precise response to cyber threats, this research explores the application of AI and ML and proposes an AI-powered cyber incident response system for cloud environments. This system, encompassing Network Traffic Classification, Web Intrusion Detection, and post-incident Malware Analysis (built as a Flask application), achieves seamless integration across platforms like Google Cloud and Microsoft Azure. The findings from this research highlight the effectiveness of the Random Forest model, achieving an accuracy of 90% for the Network Traffic Classifier and 96% for the Malware Analysis Dual Model application. Our research highlights the strengths of AI-powered cyber security. The Random Forest model excels at classifying cyber threats, offering an efficient and robust solution. Deep learning models significantly improve accuracy, and their resource demands can be managed using cloud-based TPUs and GPUs. Cloud environments themselves provide a perfect platform for hosting these AI/ML systems, while container technology ensures both efficiency and scalability. These findings demonstrate the contribution of the AI-led system in guaranteeing a robust and scalable cyber incident response solution in the cloud.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05602v4</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Ashfaaq M. Farzaan, Mohamed Chahine Ghanem, Ayman El-Hajjar, Deepthi N. Ratnayake</dc:creator>
    </item>
    <item>
      <title>DID Link: Authentication in TLS with Decentralized Identifiers and Verifiable Credentials</title>
      <link>https://arxiv.org/abs/2405.07533</link>
      <description>arXiv:2405.07533v4 Announce Type: replace-cross 
Abstract: Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07533v4</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/PST62714.2024.10788053</arxiv:DOI>
      <arxiv:journal_reference>2024 21st Annual International Conference on Privacy, Security and Trust (PST), 2024, pp. 1-11</arxiv:journal_reference>
      <dc:creator>Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel K\"upper, Hans Joachim Einsiedler, Daniela Schneider</dc:creator>
    </item>
    <item>
      <title>Generalized Multi-Objective Reinforcement Learning with Envelope Updates in URLLC-enabled Vehicular Networks</title>
      <link>https://arxiv.org/abs/2405.11331</link>
      <description>arXiv:2405.11331v2 Announce Type: replace-cross 
Abstract: We develop a novel multi-objective reinforcement learning (MORL) framework to jointly optimize wireless network selection and autonomous driving policies in a multi-band vehicular network operating on conventional sub-6GHz spectrum and Terahertz frequencies. The proposed framework is designed to 1. maximize the traffic flow and 2. minimize collisions by controlling the vehicle's motion dynamics (i.e., speed and acceleration), and enhance the ultra-reliable low-latency communication (URLLC) while minimizing handoffs (HOs). We cast this problem as a multi-objective Markov Decision Process (MOMDP) and develop solutions for both predefined and unknown preferences of the conflicting objectives. Specifically, deep-Q-network and double deep-Q-network-based solutions are developed first that consider scalarizing the transportation and telecommunication rewards using predefined preferences. We then develop a novel envelope MORL solution which develop policies that address multiple objectives with unknown preferences to the agent. While this approach reduces reliance on scalar rewards, policy effectiveness varying with different preferences is a challenge. To address this, we apply a generalized version of the Bellman equation and optimize the convex envelope of multi-objective Q values to learn a unified parametric representation capable of generating optimal policies across all possible preference configurations. Following an initial learning phase, our agent can execute optimal policies under any specified preference or infer preferences from minimal data samples.Numerical results validate the efficacy of the envelope-based MORL solution and demonstrate interesting insights related to the inter-dependency of vehicle motion dynamics, HOs, and the communication data rate. The proposed policies enable autonomous vehicles to adopt safe driving behaviors with improved connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11331v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zijiang Yan, Hina Tabassum</dc:creator>
    </item>
  </channel>
</rss>
