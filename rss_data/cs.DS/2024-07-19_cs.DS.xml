<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>R\'enyi-infinity constrained sampling with $d^3$ membership queries</title>
      <link>https://arxiv.org/abs/2407.12967</link>
      <description>arXiv:2407.12967v1 Announce Type: new 
Abstract: Uniform sampling over a convex body is a fundamental algorithmic problem, yet the convergence in KL or R\'enyi divergence of most samplers remains poorly understood. In this work, we propose a constrained proximal sampler, a principled and simple algorithm that possesses elegant convergence guarantees. Leveraging the uniform ergodicity of this sampler, we show that it converges in the R\'enyi-infinity divergence ($\mathcal R_\infty$) with no query complexity overhead when starting from a warm start. This is the strongest of commonly considered performance metrics, implying rates in $\{\mathcal R_q, \mathsf{KL}\}$ convergence as special cases.
  By applying this sampler within an annealing scheme, we propose an algorithm which can approximately sample $\varepsilon$-close to the uniform distribution on convex bodies in $\mathcal R_\infty$-divergence with $\widetilde{\mathcal{O}}(d^3\, \text{polylog} \frac{1}{\varepsilon})$ query complexity. This improves on all prior results in $\{\mathcal R_q, \mathsf{KL}\}$-divergences, without resorting to any algorithmic modifications or post-processing of the sample. It also matches the prior best known complexity in total variation distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12967v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunbum Kook, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>The Madness of Multiple Entries in March Madness</title>
      <link>https://arxiv.org/abs/2407.13438</link>
      <description>arXiv:2407.13438v1 Announce Type: new 
Abstract: This paper explores multi-entry strategies for betting pools related to single-elimination tournaments. In such betting pools, participants select winners of games, and their respective score is a weighted sum of the number of correct selections. Most betting pools have a top-heavy payoff structure, so the paper focuses on strategies that maximize the expected score of the best-performing entry. There is no known closed-formula expression for the estimation of this metric, so the paper investigates the challenges associated with the estimation and the optimization of multi-entry solutions. We present an exact dynamic programming approach for calculating the maximum expected score of any given fixed solution, which is exponential in the number of entries. We explore the structural properties of the problem to develop several solution techniques. In particular, by extracting insights from the solutions produced by one of our algorithms, we design a simple yet effective problem-specific heuristic that was the best-performing technique in our experiments, which were based on real-world data extracted from recent March Madness tournaments. In particular, our results show that the best 100-entry solution identified by our heuristic had a 2.2% likelihood of winning a $1 million prize in a real-world betting pool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13438v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Decary, David Bergman, Carlos Cardonha, Jason Imbrogno, Andrea Lodi</dc:creator>
    </item>
    <item>
      <title>Matchings, Predictions and Counterfactual Harm in Refugee Resettlement Processes</title>
      <link>https://arxiv.org/abs/2407.13052</link>
      <description>arXiv:2407.13052v1 Announce Type: cross 
Abstract: Resettlement agencies have started to adopt data-driven algorithmic matching to match refugees to locations using employment rate as a measure of utility. Given a pool of refugees, data-driven algorithmic matching utilizes a classifier to predict the probability that each refugee would find employment at any given location. Then, it uses the predicted probabilities to estimate the expected utility of all possible placement decisions. Finally, it finds the placement decisions that maximize the predicted utility by solving a maximum weight bipartite matching problem. In this work, we argue that, using existing solutions, there may be pools of refugees for which data-driven algorithmic matching is (counterfactually) harmful -- it would have achieved lower utility than a given default policy used in the past, had it been used. Then, we develop a post-processing algorithm that, given placement decisions made by a default policy on a pool of refugees and their employment outcomes, solves an inverse~matching problem to minimally modify the predictions made by a given classifier. Under these modified predictions, the optimal matching policy that maximizes predicted utility on the pool is guaranteed to be not harmful. Further, we introduce a Transformer model that, given placement decisions made by a default policy on multiple pools of refugees and their employment outcomes, learns to modify the predictions made by a classifier so that the optimal matching policy that maximizes predicted utility under the modified predictions on an unseen pool of refugees is less likely to be harmful than under the original predictions. Experiments on simulated resettlement processes using synthetic refugee data created from a variety of publicly available data suggest that our methodology may be effective in making algorithmic placement decisions that are less likely to be harmful than existing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13052v1</guid>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungeon Lee, Nina Corvelo Benz, Suhas Thejaswi, Manuel Gomez-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Parameterized Complexity of Streaming Diameter and Connectivity Problems</title>
      <link>https://arxiv.org/abs/2207.04872</link>
      <description>arXiv:2207.04872v2 Announce Type: replace 
Abstract: We initiate the investigation of the parameterized complexity of Diameter and Connectivity in the streaming paradigm. On the positive end, we show that knowing a vertex cover of size $k$ allows for algorithms in the Adjacency List (AL) streaming model whose number of passes is constant and memory is $O(\log n)$ for any fixed $k$. Underlying these algorithms is a method to execute a breadth-first search in $O(k)$ passes and $O(k \log n)$ bits of memory. On the negative end, we show that many other parameters lead to lower bounds in the AL model, where $\Omega(n/p)$ bits of memory is needed for any $p$-pass algorithm even for constant parameter values. In particular, this holds for graphs with a known modulator (deletion set) of constant size to a graph that has no induced subgraph isomorphic to a fixed graph $H$, for most $H$. For some cases, we can also show one-pass, $\Omega(n \log n)$ bits of memory lower bounds. We also prove a much stronger $\Omega(n^2/p)$ lower bound for Diameter on bipartite graphs.
  Finally, using the insights we developed into streaming parameterized graph exploration algorithms, we show a new streaming kernelization algorithm for computing a vertex cover of size $k$. This yields a kernel of $2k$ vertices (with $O(k^2)$ edges) produced as a stream in $\text{poly}(k)$ passes and only $O(k \log n)$ bits of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.04872v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jelle J. Oostveen, Erik Jan van Leeuwen</dc:creator>
    </item>
    <item>
      <title>Exploiting Automorphisms of Temporal Graphs for Fast Exploration and Rendezvous</title>
      <link>https://arxiv.org/abs/2312.07140</link>
      <description>arXiv:2312.07140v2 Announce Type: replace 
Abstract: Temporal graphs are graphs where the edge set can change in each time step, and the vertex set stays the same. Exploration of temporal graphs whose snapshot in each time step is a connected graph, called connected temporal graphs, has been widely studied. We extend the concept of graph automorphisms from static graphs to temporal graphs and show that symmetries enable faster exploration: We prove that a connected temporal graph with $n$ vertices and orbit number $r$ (i.e., $r$ is the number of automorphism orbits) can be explored in $O(r n^{1+\epsilon})$ time steps, for any fixed $\epsilon&gt;0$. For $r=O(n^c)$ for constant $c&lt;1$, this is a significant improvement over the known tight worst-case bound of $\Theta(n^2)$ time steps for arbitrary connected temporal graphs. We also give two lower bounds for exploration, showing that $\Omega(n \log n)$ time steps are required for some inputs with $r=O(1)$ and that $\Omega(rn)$ time steps are required for some inputs for any $r$ with $1\le r\le n$.
  The techniques we develop for fast exploration are used to derive the following result for rendezvous in connected temporal graphs: Two agents are placed by an adversary at arbitrary vertices and given full information about the temporal graph, except that they do not have consistent vertex labels. The agents can meet at a common vertex after $O(n^{1+\epsilon})$ time steps, for any $\epsilon&gt;0$. For some connected temporal graphs with constant orbit number we present a complementary lower bound of $\Omega(n\log n)$ time steps. Finally, we give a randomized algorithm to construct a temporal walk $W$ that visits all vertices of a given orbit with probability at least $1-\epsilon$ for any $0&lt;\epsilon&lt;1$ such that $W$ spans $O((n^{5/3}+rn)\log n)$ time steps. The runtime of this algorithm consists of $O(n^{1/3} \log (n/\epsilon))$ linear-time scans of the snapshots that exist in this time span.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07140v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ICALP.2024.55</arxiv:DOI>
      <arxiv:journal_reference>Leibniz International Proceedings in Informatics (LIPIcs), 51st International Colloquium on Automata, Languages, and Programming (ICALP 2024), Volume 297, pp. 55:1-55:18</arxiv:journal_reference>
      <dc:creator>Konstantinos Dogeas, Thomas Erlebach, Frank Kammer, Johannes Meintrup, William K. Moses Jr</dc:creator>
    </item>
    <item>
      <title>Testing Intersectingness of Uniform Families</title>
      <link>https://arxiv.org/abs/2404.11504</link>
      <description>arXiv:2404.11504v2 Announce Type: replace 
Abstract: A set family ${\cal F}$ is called intersecting if every two members of ${\cal F}$ intersect, and it is called uniform if all members of ${\cal F}$ share a common size. A uniform family ${\cal F} \subseteq \binom{[n]}{k}$ of $k$-subsets of $[n]$ is $\varepsilon$-far from intersecting if one has to remove more than $\varepsilon \cdot \binom{n}{k}$ of the sets of ${\cal F}$ to make it intersecting. We study the property testing problem that given query access to a uniform family ${\cal F} \subseteq \binom{[n]}{k}$, asks to distinguish between the case that ${\cal F}$ is intersecting and the case that it is $\varepsilon$-far from intersecting. We prove that for every fixed integer $r$, the problem admits a non-adaptive two-sided error tester with query complexity $O(\frac{\ln n}{\varepsilon})$ for $\varepsilon \geq \Omega( (\frac{k}{n})^r)$ and a non-adaptive one-sided error tester with query complexity $O(\frac{\ln k}{\varepsilon})$ for $\varepsilon \geq \Omega( (\frac{k^2}{n})^r)$. The query complexities are optimal up to the logarithmic terms. For $\varepsilon \geq \Omega( (\frac{k^2}{n})^2)$, we further provide a non-adaptive one-sided error tester with optimal query complexity of $O(\frac{1}{\varepsilon})$. Our findings show that the query complexity of the problem behaves differently from that of testing intersectingness of non-uniform families, studied recently by Chen, De, Li, Nadimpalli, and Servedio (ITCS, 2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11504v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishay Haviv, Michal Parnas</dc:creator>
    </item>
    <item>
      <title>Private Mean Estimation with Person-Level Differential Privacy</title>
      <link>https://arxiv.org/abs/2405.20405</link>
      <description>arXiv:2405.20405v2 Announce Type: replace 
Abstract: We study person-level differentially private (DP) mean estimation in the case where each person holds multiple samples. DP here requires the usual notion of distributional stability when $\textit{all}$ of a person's datapoints can be modified. Informally, if $n$ people each have $m$ samples from an unknown $d$-dimensional distribution with bounded $k$-th moments, we show that people are necessary and sufficient to estimate the mean up to distance $\alpha$ in $\ell_2$-norm under $\varepsilon$-differential privacy (and its common relaxations). In the multivariate setting, we give computationally efficient algorithms under approximate-DP and computationally inefficient algorithms under pure DP, and our nearly matching lower bounds hold for the most permissive case of approximate DP. Our computationally efficient estimators are based on the standard clip-and-noise framework, but the analysis for our setting requires both new algorithmic techniques and new analyses. In particular, our new bounds on the tails of sums of independent, vector-valued, bounded-moments random variables may be of interest.
  \[n = \tilde \Theta\left(\frac{d}{\alpha^2 m} + \frac{d}{\alpha m^{1/2} \varepsilon} + \frac{d}{\alpha^{k/(k-1)} m \varepsilon} + \frac{d}{\varepsilon}\right)\]</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20405v2</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sushant Agarwal, Gautam Kamath, Mahbod Majid, Argyris Mouzakis, Rose Silver, Jonathan Ullman</dc:creator>
    </item>
    <item>
      <title>Probabilistic estimates of the diameters of the Rubik's Cube groups</title>
      <link>https://arxiv.org/abs/2404.07337</link>
      <description>arXiv:2404.07337v5 Announce Type: replace-cross 
Abstract: The diameter of the Cayley graph of the Rubik's Cube group is the fewest number of turns needed to solve the Cube from any initial configurations. For the 2$\times$2$\times$2 Cube, the diameter is 11 in the half-turn metric, 14 in the quarter-turn metric, 19 in the semi-quarter-turn metric, and 10 in the bi-quarter-turn metric. For the 3$\times$3$\times$3 Cube, the diameter was determined by Rokicki et al. to be 20 in the half-turn metric and 26 in the quarter-turn metric. This study shows that a modified version of the coupon collector's problem in probabilistic theory can predict the diameters correctly for both 2$\times$2$\times$2 and 3$\times$3$\times$3 Cubes insofar as the quarter-turn metric is adopted. In the half-turn metric, the diameters are overestimated by one and two, respectively, for the 2$\times$2$\times$2 and 3$\times$3$\times$3 Cubes, whereas for the 2$\times$2$\times$2 Cube in the semi-quarter-turn and bi-quarter-turn metrics, they are overestimated by two and underestimated by one, respectively. Invoking the same probabilistic logic, the diameters of the 4$\times$4$\times$4 and 5$\times$5$\times$5 Cubes are predicted to be 48 (41) and 68 (58) in the quarter-turn (half-turn) metric, whose precise determinations are far beyond reach of classical supercomputing. It is shown that the probabilistically estimated diameter is approximated by $\ln N / \ln r + \ln N / r$, where $N$ is the number of configurations and $r$ is the branching ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07337v5</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>So Hirata</dc:creator>
    </item>
  </channel>
</rss>
