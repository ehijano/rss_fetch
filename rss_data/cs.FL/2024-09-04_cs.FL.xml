<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Sep 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal L-Systems for Stochastic L-system Inference Problems</title>
      <link>https://arxiv.org/abs/2409.02259</link>
      <description>arXiv:2409.02259v1 Announce Type: cross 
Abstract: This paper presents two novel theorems that address two open problems in stochastic Lindenmayer-system (L-system) inference, specifically focusing on the construction of an optimal stochastic L-system capable of generating a given sequence of strings. The first theorem delineates a method for crafting a stochastic L-system that maximizes the likelihood of producing a given sequence of words through a singular derivation. Furthermore, the second theorem determines the stochastic L-systems with the highest probability of producing a given sequence of words with multiple possible derivations. From these, we introduce an algorithm to infer an optimal stochastic L-system from a given sequence. This algorithm incorporates sophisticated optimization techniques, such as interior point methods, ensuring production of a stochastically optimal stochastic L-system suitable for generating the given sequence. This allows for the use of using stochastic L-systems as model for machine learning using only positive data for training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02259v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Lotfi, Ian McQuillan</dc:creator>
    </item>
    <item>
      <title>Tractable Offline Learning of Regular Decision Processes</title>
      <link>https://arxiv.org/abs/2409.02747</link>
      <description>arXiv:2409.02747v1 Announce Type: cross 
Abstract: This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this reason, many RDP algorithms first reconstruct this unknown dependency using automata learning techniques. In this paper, we show that it is possible to overcome two strong limitations of previous offline RL algorithms for RDPs, notably RegORL. This can be accomplished via the introduction of two original techniques: the development of a new pseudometric based on formal languages, which removes a problematic dependency on $L_\infty^\mathsf{p}$-distinguishability parameters, and the adoption of Count-Min-Sketch (CMS), instead of naive counting. The former reduces the number of samples required in environments that are characterized by a low complexity in language-theoretic terms. The latter alleviates the memory requirements for long planning horizons. We derive the PAC sample complexity bounds associated to each of these techniques, and we validate the approach experimentally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02747v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahana Deb, Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi</dc:creator>
    </item>
    <item>
      <title>Introducing Divergence for Infinite Probabilistic Models</title>
      <link>https://arxiv.org/abs/2308.08842</link>
      <description>arXiv:2308.08842v4 Announce Type: replace 
Abstract: Computing the reachability probability in infinite state probabilistic models has been the topic of numerous works. Here we introduce a new property called divergence that when satisfied allows to compute reachability probabilities up to an arbitrary precision. One of the main interest of divergence is that this computation does not require the reachability problem to be decidable. Then we study the decidability of divergence for random walks and the probabilistic versions of Petri nets where the weights associated with transitions may also depend on the current state. This should be contrasted with most of the existing works that assume weights independent of the state. Such an extended framework is motivated by the modeling of real case studies. Moreover, we exhibit some subclasses of channel systems and pushdown automata that are divergent by construction, particularly suited for specifying open distributed systems and networks prone to performance collapsing where probabilities related to service requirements are needed. where probabilities related to service requirements are needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08842v4</guid>
      <category>cs.FL</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Finkel, Serge Haddad, Lina Ye</dc:creator>
    </item>
    <item>
      <title>A Direct Translation from LTL with Past to Deterministic Rabin Automata</title>
      <link>https://arxiv.org/abs/2405.01178</link>
      <description>arXiv:2405.01178v3 Announce Type: replace 
Abstract: We present a translation from linear temporal logic with past to deterministic Rabin automata. The translation is direct in the sense that it does not rely on intermediate non-deterministic automata, and asymptotically optimal, resulting in Rabin automata of doubly exponential size. It is based on two main notions. One is that it is possible to encode the history contained in the prefix of a word, as relevant for the formula under consideration, by performing simple rewrites of the formula itself. As a consequence, a formula involving past operators can (through such rewrites, which involve alternating between weak and strong versions of past operators in the formula's syntax tree) be correctly evaluated at an arbitrary point in the future without requiring backtracking through the word. The other is that this allows us to generalize to linear temporal logic with past the result that the language of a pure-future formula can be decomposed into a Boolean combination of simpler languages, for which deterministic automata with simple acceptance conditions are easily constructed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01178v3</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.MFCS.2024.13</arxiv:DOI>
      <dc:creator>Shaun Azzopardi, David Lidell, Nir Piterman</dc:creator>
    </item>
    <item>
      <title>Blow-up in Non-Deterministic Automata</title>
      <link>https://arxiv.org/abs/2407.09891</link>
      <description>arXiv:2407.09891v2 Announce Type: replace 
Abstract: In this paper we examine the difficulty of finding an equivalent deterministic automaton when confronted with a non-deterministic one. While for some automata the exponential blow-up in their number of states is unavoidable, we show that in general, any approximation of state complexity with polynomial precision remains PSPACE-hard. The same is true when using the subset construction to determinize the NFA, meaning that it is PSPACE-hard to predict whether subset construction will produce an exponential ''blow-up'' in the number of states or not. To give an explanation for its behaviour, we propose the notion of subset complexity, which serves as an upper bound on the size of subset construction. Due to it simple and intuitive nature it allows to identify large classes of automata which can have limited non-determinism and completely avoid the ''blow-up''. Subset complexity also remains invariant under NFA reversal and allows to predict how the introduction or removal of transitions from the NFA will affect its size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09891v2</guid>
      <category>cs.FL</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ivan Baburin, Ryan Cotterell</dc:creator>
    </item>
    <item>
      <title>What Formal Languages Can Transformers Express? A Survey</title>
      <link>https://arxiv.org/abs/2311.00208</link>
      <description>arXiv:2311.00208v3 Announce Type: replace-cross 
Abstract: As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00208v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/tacl_a_00663</arxiv:DOI>
      <arxiv:journal_reference>Transactions of the Association for Computational Linguistics, 12:543-561, 2024</arxiv:journal_reference>
      <dc:creator>Lena Strobl, William Merrill, Gail Weiss, David Chiang, Dana Angluin</dc:creator>
    </item>
  </channel>
</rss>
