<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:05:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reinforcement-Learning Portfolio Allocation with Dynamic Embedding of Market Information</title>
      <link>https://arxiv.org/abs/2501.17992</link>
      <description>arXiv:2501.17992v1 Announce Type: new 
Abstract: We develop a portfolio allocation framework that leverages deep learning techniques to address challenges arising from high-dimensional, non-stationary, and low-signal-to-noise market information. Our approach includes a dynamic embedding method that reduces the non-stationary, high-dimensional state space into a lower-dimensional representation. We design a reinforcement learning (RL) framework that integrates generative autoencoders and online meta-learning to dynamically embed market information, enabling the RL agent to focus on the most impactful parts of the state space for portfolio allocation decisions. Empirical analysis based on the top 500 U.S. stocks demonstrates that our framework outperforms common portfolio benchmarks and the predict-then-optimize (PTO) approach using machine learning, particularly during periods of market stress. Traditional factor models do not fully explain this superior performance. The framework's ability to time volatility reduces its market exposure during turbulent times. Ablation studies confirm the robustness of this performance across various reinforcement learning algorithms. Additionally, the embedding and meta-learning techniques effectively manage the complexities of high-dimensional, noisy, and non-stationary financial data, enhancing both portfolio performance and risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17992v1</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinghai He, Cheng Hua, Chunyang Zhou, Zeyu Zheng</dc:creator>
    </item>
    <item>
      <title>Pontryagin-Guided Deep Learning for Large-Scale Constrained Dynamic Portfolio Choice</title>
      <link>https://arxiv.org/abs/2501.12600</link>
      <description>arXiv:2501.12600v2 Announce Type: replace 
Abstract: We present a Pontryagin-Guided Direct Policy Optimization (\textbf{PG-DPO}) method for \emph{constrained} dynamic portfolio choice -- incorporating consumption and multi-asset investment -- that scales to \emph{thousands} of risky assets. By combining neural-network controls with Pontryagin's Maximum Principle (PMP), it circumvents the curse of dimensionality that renders dynamic programming (DP) grids intractable beyond a handful of assets. Unlike value-based PDE or BSDE approaches, PG-DPO enforces PMP conditions at each gradient step, naturally accommodating no-short-selling or borrowing constraints and optional consumption bounds. A ``one-shot" variant rapidly computes Pontryagin-optimal controls after a brief warm-up, leading to substantially higher accuracy than naive baselines. On modern GPUs, near-optimal solutions typically arise in around ten minutes, and refining to higher precision usually takes under an hour. Numerical experiments show that in the unconstrained case, PG-DPO recovers the known closed-form solution even at 10{,}000 assets, while under constraints it remains tractable up to 1{,}000 assets -- both well beyond the long-standing DP-based limit of fewer than seven.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12600v2</guid>
      <category>q-fin.PM</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeonggyu Huh, Hyeng Keun Koo, Jaegi Jeon</dc:creator>
    </item>
  </channel>
</rss>
