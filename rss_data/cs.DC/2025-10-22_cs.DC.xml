<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Oct 2025 01:44:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis</title>
      <link>https://arxiv.org/abs/2510.17852</link>
      <description>arXiv:2510.17852v1 Announce Type: new 
Abstract: With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To address this issue, we present a framework for migrating large-scale atmospheric and oceanic models from PyTorch to MindSpore and optimizing for Chinese chips, and evaluating their performance against GPUs. The framework focuses on software-hardware adaptation, memory optimization, and parallelism. Furthermore, the model's performance is evaluated across multiple metrics, including training speed, inference speed, model accuracy, and energy efficiency, with comparisons against GPU-based implementations. Experimental results demonstrate that the migration and optimization process preserves the models' original accuracy while significantly reducing system dependencies and improving operational efficiency by leveraging Chinese chips as a viable alternative for scientific computing. This work provides valuable insights and practical guidance for leveraging Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17852v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang</dc:creator>
    </item>
    <item>
      <title>Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation</title>
      <link>https://arxiv.org/abs/2510.18152</link>
      <description>arXiv:2510.18152v1 Announce Type: new 
Abstract: Recent advances in distributed learning systems have introduced effective solutions for implementing collaborative artificial intelligence techniques in wireless communication networks. Federated learning approaches provide a model-aggregation mechanism among edge devices to achieve collaborative training, while ensuring data security, communication efficiency, and sharing computational overheads. On the other hand, limited transmission resources and complex communication environments remain significant bottlenecks to the efficient collaborations among edge devices, particularly within large-scale networks. To address such issues, this paper proposes an over-the-air (OTA) analog aggregation method designed for the distributed swarm learning (DSL), termed DSL-OTA, aiming to enhance communication efficiency, enable effective cooperation, and ensure privacy preserving. Incorporating multi-worker selection strategy with over-the-air aggregation not only makes the standard DSL based on single best worker contributing to global model update to become more federated, but also secures the aggregation from potential risks of data leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA algorithm in terms of fast convergence rate and low communication costs. Simulation results reveal that our DSL-OTA outperforms the other existing methods by achieving better learning performance under both homogeneous and heterogeneous dataset settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18152v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoyu Yao, Yue Wang, Songyang Zhang, Yingshu Li, Zhipeng Cai, Zhi Tian</dc:creator>
    </item>
    <item>
      <title>A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces</title>
      <link>https://arxiv.org/abs/2510.18300</link>
      <description>arXiv:2510.18300v1 Announce Type: new 
Abstract: Large-scale GPU traces play a critical role in identifying performance bottlenecks within heterogeneous High-Performance Computing (HPC) architectures. However, the sheer volume and complexity of a single trace of data make performance analysis both computationally expensive and time-consuming. To address this challenge, we present an end-to-end parallel performance analysis framework designed to handle multiple large-scale GPU traces efficiently. Our proposed framework partitions and processes trace data concurrently and employs causal graph methods and parallel coordinating chart to expose performance variability and dependencies across execution flows. Experimental results demonstrate a 67% improvement in terms of scalability, highlighting the effectiveness of our pipeline for analyzing multiple traces independently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18300v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard</dc:creator>
    </item>
    <item>
      <title>SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices</title>
      <link>https://arxiv.org/abs/2510.18544</link>
      <description>arXiv:2510.18544v1 Announce Type: new 
Abstract: Large Language Models (LLMs), as the foundational architecture for next-generation interactive AI applications, not only power intelligent dialogue systems but also drive the evolution of embodied intelligence on edge devices, including humanoid robots, smart vehicles, and other scenarios. The applications running on these edge devices impose differentiated Service Level Objectives (SLO) requirements on LLM services, specifically manifested as distinct constraints on Time to First Token (TTFT) and Time Per Output Token (TPOT) as well as end-to-end latency. Notably, edge devices typically handle real-time tasks that are extremely sensitive to latency, such as machine control and navigation planning. However, existing scheduling service systems still prioritize maximizing output token throughput as the sole optimization objective, failing to adequately address the diversity of SLO requirements. This ultimately results in persistently high violation rates for end-to-end latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for edge computing scenarios with differentiated SLO requirements. By combining a utility-maximizing request scheduling algorithm with a dynamic iterative control mechanism for generation rates, SLICE significantly improves LLM inference service SLO attainment. Experimental results demonstrate that compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to 35x higher SLO attainment and 3.4x advantage in task completion time than the other two solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18544v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pan Zhou, Yiming Lei, Ling Liu, Xiaoqiong Xu, Ying Cai, Daji Ergu, Hongfang Yu, Yueyue Dai</dc:creator>
    </item>
    <item>
      <title>Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications</title>
      <link>https://arxiv.org/abs/2510.18586</link>
      <description>arXiv:2510.18586v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18586v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo</dc:creator>
    </item>
    <item>
      <title>Distributed Interactive Proofs for Planarity with Log-Star Communication</title>
      <link>https://arxiv.org/abs/2510.18592</link>
      <description>arXiv:2510.18592v1 Announce Type: new 
Abstract: We provide new communication-efficient distributed interactive proofs for planarity. The notion of a \emph{distributed interactive proof (DIP)} was introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover} is a single centralized entity whose goal is to prove a certain claim regarding an input graph $G$. To do so, the prover communicates with a distributed \emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is measured by the amount of prover-verifier communication it requires. Namely, the goal is to design a DIP with a small number of interaction rounds and a small \emph{proof size}, i.e., a small amount of communication per round. Our main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log ^{*}n\rceil)$, respectively. In fact, this result can be generalized as follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and $O(\log ^{(r)}n+\log \Delta /r)$, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18592v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuval Gil, Merav Parter</dc:creator>
    </item>
    <item>
      <title>Towards an Optimized Benchmarking Platform for CI/CD Pipelines</title>
      <link>https://arxiv.org/abs/2510.18640</link>
      <description>arXiv:2510.18640v1 Announce Type: new 
Abstract: Performance regressions in large-scale software systems can lead to substantial resource inefficiencies, making their early detection critical. Frequent benchmarking is essential for identifying these regressions and maintaining service-level agreements (SLAs). Performance benchmarks, however, are resource-intensive and time-consuming, which is a major challenge for integration into Continuous Integration / Continuous Deployment (CI/CD) pipelines. Although numerous benchmark optimization techniques have been proposed to accelerate benchmark execution, there is currently no practical system that integrates these optimizations seamlessly into real-world CI/CD pipelines. In this vision paper, we argue that the field of benchmark optimization remains under-explored in key areas that hinder its broader adoption. We identify three central challenges to enabling frequent and efficient benchmarking: (a) the composability of benchmark optimization strategies, (b) automated evaluation of benchmarking results, and (c) the usability and complexity of applying these strategies as part of CI/CD systems in practice. We also introduce a conceptual cloud-based benchmarking framework handling these challenges transparently. By presenting these open problems, we aim to stimulate research toward making performance regression detection in CI/CD systems more practical and effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18640v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IC2E65552.2025.00010</arxiv:DOI>
      <dc:creator>Nils Japke, Sebastian Koch, Helmut Lukasczyk, David Bermbach</dc:creator>
    </item>
    <item>
      <title>PCMS: Parallel Coupler For Multimodel Simulations</title>
      <link>https://arxiv.org/abs/2510.18838</link>
      <description>arXiv:2510.18838v1 Announce Type: new 
Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a new GPU accelerated generalized coupling framework for coupling simulation codes on leadership class supercomputers. PCMS includes distributed control and field mapping methods for up to five dimensions. For field mapping PCMS can utilize discretization and field information to accommodate physics constraints. PCMS is demonstrated with a coupling of the gyrokinetic microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and with a 5D distribution function coupling of an energetic particle transport code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of 85%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18838v1</guid>
      <category>cs.DC</category>
      <category>physics.comp-ph</category>
      <category>physics.plasm-ph</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob S. Merson, Cameron W. Smith, Mark S. Shephard, Fuad Hasan, Abhiyan Paudel, Angel Castillo-Crooke, Joyal Mathew, Mohammad Elahi</dc:creator>
    </item>
    <item>
      <title>The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications</title>
      <link>https://arxiv.org/abs/2510.17901</link>
      <description>arXiv:2510.17901v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications.
  In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17901v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alex Acero, Daniel M. Jimenez-Gutierrez, Dario Pighin, Enrique Zuazua, Joaquin Del Rio, Xabi Uribe-Etxebarria</dc:creator>
    </item>
    <item>
      <title>A New Broadcast Model for Several Network Topologies</title>
      <link>https://arxiv.org/abs/2510.18058</link>
      <description>arXiv:2510.18058v1 Announce Type: cross 
Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast algorithm designed to optimize communication efficiency across diverse network topologies. BBS maximizes node utilization, addressing challenges in broadcast operations such as topology constraints, bandwidth limitations, and synchronization overhead, particularly in large-scale systems like supercomputers. The algorithm ensures sustained activity with nodes throughout the broadcast, thereby enhancing data propagation and significantly reducing latency. Through a precise communication cycle, BBS provides a repeatable, streamlined, stepwise broadcasting framework. Simulation results across various topologies demonstrate that the BBS algorithm consistently outperforms common general broadcast algorithms, often by a substantial margin. These findings suggest that BBS is a versatile and robust framework with the potential to redefine broadcast strategies across network topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18058v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongbo Lu, Junsung Hwang, Bernard Tenreiro, Nabila Jaman Tripti, Darren Hamilton, Yuefan Deng</dc:creator>
    </item>
    <item>
      <title>Efficient Long-context Language Model Training by Core Attention Disaggregation</title>
      <link>https://arxiv.org/abs/2510.18121</link>
      <description>arXiv:2510.18121v1 Announce Type: cross 
Abstract: We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18121v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonghao Zhuang, Junda Chen, Bo Pang, Yi Gu, Yibo Zhu, Yimin Jiang, Ion Stoica, Eric Xing, Hao Zhang</dc:creator>
    </item>
    <item>
      <title>Distributed Allocation and Resource Scheduling Algorithms Resilient to Link Failure</title>
      <link>https://arxiv.org/abs/2510.18273</link>
      <description>arXiv:2510.18273v1 Announce Type: cross 
Abstract: Distributed resource allocation (DRA) is fundamental to modern networked systems, spanning applications from economic dispatch in smart grids to CPU scheduling in data centers. Conventional DRA approaches require reliable communication, yet real-world networks frequently suffer from link failures, packet drops, and communication delays due to environmental conditions, network congestion, and security threats.
  We introduce a novel resilient DRA algorithm that addresses these critical challenges, and our main contributions are as follows: (1) guaranteed constraint feasibility at all times, ensuring resource-demand balance even during algorithm termination or network disruption; (2) robust convergence despite sector-bound nonlinearities at nodes/links, accommodating practical constraints like quantization and saturation; and (3) optimal performance under merely uniformly-connected networks, eliminating the need for continuous connectivity.
  Unlike existing approaches that require persistent network connectivity and provide only asymptotic feasibility, our graph-theoretic solution leverages network percolation theory to maintain performance during intermittent disconnections. This makes it particularly valuable for mobile multi-agent systems where nodes frequently move out of communication range. Theoretical analysis and simulations demonstrate that our algorithm converges to optimal solutions despite heterogeneous time delays and substantial link failures, significantly advancing the reliability of distributed resource allocation in practical network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18273v1</guid>
      <category>eess.SY</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Doostmohammadian, Sergio Pequito</dc:creator>
    </item>
    <item>
      <title>sNVMe-oF: Secure and Efficient Disaggregated Storage</title>
      <link>https://arxiv.org/abs/2510.18756</link>
      <description>arXiv:2510.18756v1 Announce Type: cross 
Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the standard solution in modern data centers, achieving superior performance, resource utilization, and power efficiency. Simultaneously, confidential computing (CC) is becoming the de facto security paradigm, enforcing stronger isolation and protection for sensitive workloads. However, securing state-of-the-art storage with traditional CC methods struggles to scale and compromises performance or security. To address these issues, we introduce sNVMe-oF, a storage management system extending the NVMe-oF protocol and adhering to the CC threat model by providing confidentiality, integrity, and freshness guarantees. sNVMe-oF offers an appropriate control path and novel concepts such as counter-leasing. sNVMe-oF also optimizes data path performance by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree (HMT), and avoiding redundant IPSec protections. We achieve this without modifying the NVMe-oF protocol. To prevent excessive resource usage while delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs. We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can achieve as little as 2% performance degradation for synthetic patterns and AI training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18756v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcin Chrapek, Meni Orenbach, Ahmad Atamli, Marcin Copik, Fritz Alder, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training</title>
      <link>https://arxiv.org/abs/2510.18830</link>
      <description>arXiv:2510.18830v1 Announce Type: cross 
Abstract: The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at https://github.com/microsoft/MInference/tree/main/MTraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18830v1</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</dc:creator>
    </item>
    <item>
      <title>The Art of the Fugue: Minimizing Interleaving in Collaborative Text Editing</title>
      <link>https://arxiv.org/abs/2305.00583</link>
      <description>arXiv:2305.00583v3 Announce Type: replace 
Abstract: Most existing algorithms for replicated lists, which are widely used in collaborative text editors, suffer from a problem: when two users concurrently insert text at the same position in the document, the merged outcome may interleave the inserted text passages, resulting in corrupted and potentially unreadable text. The problem has gone unnoticed for decades, and it affects both CRDTs and Operational Transformation. This paper defines maximal non-interleaving, our new correctness property for replicated lists. We introduce two related CRDT algorithms, Fugue and FugueMax, and prove that FugueMax satisfies maximal non-interleaving. We also implement our algorithms and demonstrate that Fugue offers performance comparable to state-of-the-art CRDT libraries for text editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00583v3</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TPDS.2025.3611880</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Parallel and Distributed Systems, vol. 36, no. 11, pp. 2425-2437, Nov. 2025</arxiv:journal_reference>
      <dc:creator>Matthew Weidner, Martin Kleppmann</dc:creator>
    </item>
    <item>
      <title>ParaLog: Consistent Host-side Logging for Parallel Checkpoints</title>
      <link>https://arxiv.org/abs/2401.14576</link>
      <description>arXiv:2401.14576v3 Announce Type: replace 
Abstract: Output-intensive scientific applications are highly sensitive to low storage throughput. While existing scientific application stacks are optimized for traditional High-Performance Computing (HPC) environments with high remote storage and network bandwidth, these assumptions often fail in modern settings like cloud deployment. This is because the existing scientific application I/O stack fails to leverage the available resources. At the same time, scientific applications exhibit special synchronization and data output requirements that are difficult to satisfy using traditional approaches such as block-level or filesystem-level caching. We introduce ParaLog, a distributed host-side logging approach designed to accelerate scientific applications transparently. ParaLog emphasizes deployability, enabling support for unmodified message passing interface (MPI) applications and implementations while preserving crash consistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC, local clusters, and hybrid environments, demonstrating its capability to reduce end-to-end execution time by 13-26% for popular scientific applications in cloud settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14576v3</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3772052.3772212</arxiv:DOI>
      <dc:creator>Steven W. D. Chien, Kento Sato, Artur Podobas, Niclas Jansson, Stefano Markidis, Michio Honda</dc:creator>
    </item>
    <item>
      <title>A New Execution Model and Executor for Adaptively Optimizing the Performance of Parallel Algorithms Using HPX Runtime System</title>
      <link>https://arxiv.org/abs/2504.07206</link>
      <description>arXiv:2504.07206v2 Announce Type: replace 
Abstract: Developing parallel algorithms efficiently requires careful management of concurrency across diverse hardware architectures. C++ executors provide a standardized interface that simplifies the development process, allowing developers to write portable and uniform code. However, in some cases, they may not fully leverage hardware capabilities or optimally allocate resources for specific workloads, leading to potential performance inefficiencies. Building on our earlier conference paper [ Adaptively Optimizing the Performance of HPX's Parallel algorithms], which introduces a preliminary strategy based on cores and chunking (workload), and integrated it into HPX's executor API, that dynamically optimizes for workload distribution and resource allocation, based on runtime metrics and overheads, this paper, introduces a more detailed model of that strategy. It evaluates the efficiency of this implementation (as an HPX executor) across a wide range of compute-bound and memory-bound workloads on different architectures and with different algorithms. The results show consistent speedups across all tests, configurations, and workloads studied, offering improved performance through a familiar and user-friendly c++ executors API. Additionally, the paper highlights how runtime-driven executor adaptation can simplify performance optimization without increasing the complexity of algorithm development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07206v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s42979-025-04442-y</arxiv:DOI>
      <arxiv:journal_reference>Volume 6 , article number 911 , Springer Nature journal , Published 16 October 2025</arxiv:journal_reference>
      <dc:creator>Karame Mohammadiporshokooh, Steven R. Brandt, Hartmut Kaiser</dc:creator>
    </item>
    <item>
      <title>A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated in a coupled reactive transport HPC simulation</title>
      <link>https://arxiv.org/abs/2504.14374</link>
      <description>arXiv:2504.14374v3 Announce Type: replace 
Abstract: Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14374v3</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-97635-3_28</arxiv:DOI>
      <dc:creator>Max L\"ubke, Marco De Lucia, Steffen Christgau, Stefan Petri, Bettina Schnor</dc:creator>
    </item>
    <item>
      <title>Exploring Influence Factors on LLM Suitability for No-Code Development of End User IoT Applications</title>
      <link>https://arxiv.org/abs/2505.04710</link>
      <description>arXiv:2505.04710v2 Announce Type: replace 
Abstract: No-Code Development Platforms (NCDPs) empower non-technical end users to build applications tailored to their specific demands without writing code. While NCDPs lower technical barriers, users still require some technical knowledge, e.g., to structure process steps or define event-action rules. Large Language Models (LLMs) offer a promising solution to further reduce technical requirements by supporting natural language interaction and dynamic code generation. By integrating LLM, NCDPs can be more accessible to non-technical users, enabling application development truly without requiring any technical expertise.
  Despite growing interest in LLM-powered NCDPs, a systematic investigation into the factors influencing LLM suitability and performance remains absent. Understanding these factors is critical to effectively leveraging LLMs capabilities and maximizing their impact. In this paper, we investigate key factors influencing the effectiveness of LLMs in supporting end-user application development within NCDPs. By conducting comprehensive experiments, we evaluate the impact of four key factors, i.e., model selection, prompt language, training data background, and an error-informed few-shot setup, on the quality of generated applications. Specifically, we selected a range of LLMs based on their architecture, scale, design focus, and training data, and evaluated them across four real-world smart home automation scenarios implemented on a representative open-source LLM-powered NCDP. Our findings offer practical insights into how LLMs can be effectively integrated into NCDPs, informing both platform design and the selection of suitable LLMs for end-user application development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04710v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/spe.70027</arxiv:DOI>
      <dc:creator>Minghe Wang, Alexandra Kapp, Trever Schirmer, Tobias Pfandzelter, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Wait-free Replicated Data Types and Fair Reconciliation</title>
      <link>https://arxiv.org/abs/2508.18193</link>
      <description>arXiv:2508.18193v2 Announce Type: replace 
Abstract: Replication ensures data availability in fault-prone distributed systems. The celebrated CAP theorem stipulates that replicas cannot guarantee both strong consistency and availability under network partitions. A popular alternative, adopted by CRDTs, is to relax consistency to be eventual. It enables progress to be wait-free, as replicas can serve requests immediately. Yet, wait-free replication faces a key challenge: due to asynchrony and concurrency, operations may be constantly reordered, leading to results inconsistent with their original contexts and preventing them from stabilizing over time. Moreover, a particular client may experience starvation if, from some point on, each of its operations is reordered at least once.
  We make two contributions. First, we formalize the problem addressed by wait-free replicated data types (e.g., CRDTs) as eventual state-machine replication. We then augment it with stability and fairness ensuring, respectively, that (1)~all replicas share a growing stable prefix of operations, and (2)~no client starves. Second, we present a generic DAG-based framework to achieve eventual state-machine replication for any replicated data type, where replicas exchange their local views and merge them using a reconciliation function. We then propose reconciliation functions ensuring stability and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18193v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Kuznetsov, Maxence Perion, Sara Tucci-Piergiovanni</dc:creator>
    </item>
    <item>
      <title>Transaction Capacity, Security and Latency in Blockchains</title>
      <link>https://arxiv.org/abs/2402.10138</link>
      <description>arXiv:2402.10138v2 Announce Type: replace-cross 
Abstract: We analyze how secure a block is after the block becomes $k$-deep, i.e., security-latency, for Nakamoto consensus under an exponential network delay model. We provide the fault tolerance and extensive bounds on safety violation probabilities given mining rate, delay rate and confirmation rules. Next, modeling the blockchain system as a batch service queue with exponential network delay, we connect the security-latency analysis to sustainable transaction rate of the queue system. As our model assumes exponential network delay, batch service queue models give a meaningful trade-off between transaction capacity, security and latency. Our results indicate that, by simply picking $k=7$-block confirmation rule in Bitcoin instead of the convention of $k=6$, mining rate, latency and throughput can be increased sixfold with the same safety guarantees. We further consider adversarial attacks on the queue service to hamper the service process. In an extreme scenario, we consider the selfish-mining attack for this purpose and provide the maximum adversarial block ratio in the longest chain under the exponential delay model. The ratio in turn reflects the maximum rate of decrease in the sustainable transaction rate of the queue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10138v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Doger, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>$\mu$OpTime: Statically Reducing the Execution Time of Microbenchmark Suites Using Stability Metrics</title>
      <link>https://arxiv.org/abs/2501.12878</link>
      <description>arXiv:2501.12878v2 Announce Type: replace-cross 
Abstract: Performance regressions have a tremendous impact on the quality of software. One way to catch regressions before they reach production is executing performance tests before deployment, e.g., using microbenchmarks, which measure performance at subroutine level. In projects with many microbenchmarks, this may take several hours due to repeated execution to get accurate results, disqualifying them from frequent use in CI/CD pipelines. We propose $\mu$OpTime, a static approach to reduce the execution time of microbenchmark suites by configuring the number of repetitions for each microbenchmark. Based on the results of a full, previous microbenchmark suite run, $\mu$OpTime determines the minimal number of (measurement) repetitions with statistical stability metrics that still lead to accurate results. We evaluate $\mu$OpTime with an experimental study on 14 open-source projects written in two programming languages and five stability metrics. Our results show that (i) $\mu$OpTime reduces the total suite execution time (measurement phase) by up to 95.83% (Go) and 94.17% (Java), (ii) the choice of stability metric depends on the project and programming language, (iii) microbenchmark warmup phases have to be considered for Java projects (potentially leading to higher reductions), and (iv) $\mu$OpTime can be used to reliably detect performance regressions in CI/CD pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12878v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715322</arxiv:DOI>
      <dc:creator>Nils Japke, Martin Grambow, Christoph Laaber, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Evaluating the Impact Of Spatial Features Of Mobility Data and Index Choice On Database Performance</title>
      <link>https://arxiv.org/abs/2505.14466</link>
      <description>arXiv:2505.14466v3 Announce Type: replace-cross 
Abstract: The growing number of moving Internet-of-Things (IoT) devices has led to a surge in moving object data, powering applications such as traffic routing, hotspot detection, or weather forecasting. When managing such data, spatial database systems offer various index options and data formats, e.g., point-based or trajectory-based. Likewise, dataset characteristics such as geographic overlap and skew can vary significantly. All three significantly affect database performance. While this has been studied in existing papers, none of them explore the effects and trade-offs resulting from a combination of all three aspects. In this paper, we evaluate the performance impact of index choice, data format, and dataset characteristics on a popular spatial database system, PostGIS. We focus on two aspects of dataset characteristics, the degree of overlap and the degree of skew, and propose novel approximation methods to determine these features. We design a benchmark that compares a variety of spatial indexing strategies and data formats, while also considering the impact of dataset characteristics on database performance. We include a variety of real-world and synthetic datasets, write operations, and read queries to cover a broad range of scenarios that might occur during application runtime. Our results offer practical guidance for developers looking to optimize spatial storage and querying, while also providing insights into dataset characteristics and their impact on database performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14466v3</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IC2E65552.2025.00007</arxiv:DOI>
      <dc:creator>Tim C. Rese, Alexandra Kapp, David Bermbach</dc:creator>
    </item>
  </channel>
</rss>
