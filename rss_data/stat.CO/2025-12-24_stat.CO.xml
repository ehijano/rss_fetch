<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Dec 2025 05:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Numerical Analysis of Test Optimality</title>
      <link>https://arxiv.org/abs/2512.19843</link>
      <description>arXiv:2512.19843v1 Announce Type: cross 
Abstract: In nonstandard testing environments, researchers often derive ad hoc tests with correct (asymptotic) size, but their optimality properties are typically unknown a priori and difficult to assess. This paper develops a numerical framework for determining whether an ad hoc test is effectively optimal - approximately maximizing a weighted average power criterion for some weights over the alternative and attaining a power envelope generated by a single weighted average power-maximizing test. Our approach uses nested optimization algorithms to approximate the weight function that makes an ad hoc test's weighted average power as close as possible to that of a true weighted average power-maximizing test, and we show the surprising result that the rejection probabilities corresponding to the latter form an approximate power envelope for the former. We provide convergence guarantees, discuss practical implementation and apply the method to the weak instrument-robust conditional likelihood ratio test and a recently-proposed test for when a nuisance parameter may be on or near its boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19843v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Ketz, Adam McCloskey, Jan Scherer</dc:creator>
    </item>
    <item>
      <title>Generative Bayesian Hyperparameter Tuning</title>
      <link>https://arxiv.org/abs/2512.20051</link>
      <description>arXiv:2512.20051v1 Announce Type: cross 
Abstract: \noindent Hyper-parameter selection is a central practical problem in modern machine learning, governing regularization strength, model capacity, and robustness choices. Cross-validation is often computationally prohibitive at scale, while fully Bayesian hyper-parameter learning can be difficult due to the cost of posterior sampling. We develop a generative perspective on hyper-parameter tuning that combines two ideas: (i) optimization-based approximations to Bayesian posteriors via randomized, weighted objectives (weighted Bayesian bootstrap), and (ii) amortization of repeated optimization across many hyper-parameter settings by learning a transport map from hyper-parameters (including random weights) to the corresponding optimizer. This yields a ``generator look-up table'' for estimators, enabling rapid evaluation over grids or continuous ranges of hyper-parameters and supporting both predictive tuning objectives and approximate Bayesian uncertainty quantification. We connect this viewpoint to weighted $M$-estimation, envelope/auxiliary-variable representations that reduce non-quadratic losses to weighted least squares, and recent generative samplers for weighted $M$-estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20051v1</guid>
      <category>stat.ML</category>
      <category>stat.CO</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Hedibert Lopes, Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Generalized method of L-moment estimation for stationary and nonstationary extreme value models</title>
      <link>https://arxiv.org/abs/2512.20385</link>
      <description>arXiv:2512.20385v1 Announce Type: cross 
Abstract: Precisely estimating out-of-sample upper quantiles is very important in risk assessment and in engineering practice for structural design to prevent a greater disaster. For this purpose, the generalized extreme value (GEV) distribution has been broadly used. To estimate the parameters of GEV distribution, the maximum likelihood estimation (MLE) and L-moment estimation (LME) methods have been primarily employed. For a better estimation using the MLE, several studies considered the generalized MLE (penalized likelihood or Bayesian) methods to cooperate with a penalty function or prior information for parameters. However, a generalized LME method for the same purpose has not been developed yet in the literature. We thus propose the generalized method of L-moment estimation (GLME) to cooperate with a penalty function or prior information. The proposed estimation is based on the generalized L-moment distance and a multivariate normal likelihood approximation. Because the L-moment estimator is more efficient and robust for small samples than the MLE, we reasonably expect the advantages of LME to continue to hold for GLME. The proposed method is applied to the stationary and nonstationary GEV models with two novel (data-adaptive) penalty functions to correct the bias of LME. A simulation study indicates that the biases of LME are considerably corrected by the GLME with slight increases in the standard error. Applications to US flood damage data and maximum rainfall at Phliu Agromet in Thailand illustrate the usefulness of the proposed method. This study may promote further work on penalized or Bayesian inferences based on L-moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20385v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yonggwan Shin, Yire Shin, Jihong Park, Jeong-Soo Park</dc:creator>
    </item>
    <item>
      <title>Iterated sampling importance resampling with adaptive number of proposals</title>
      <link>https://arxiv.org/abs/2512.00220</link>
      <description>arXiv:2512.00220v2 Announce Type: replace 
Abstract: Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00220v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietari Laitinen, Matti Vihola</dc:creator>
    </item>
    <item>
      <title>Algorithmic Aspects of the Log-Laplace Transform and a Non-Euclidean Proximal Sampler</title>
      <link>https://arxiv.org/abs/2302.06085</link>
      <description>arXiv:2302.06085v3 Announce Type: replace-cross 
Abstract: The development of efficient sampling algorithms catering to non-Euclidean geometries has been a challenging endeavor, as discretization techniques which succeed in the Euclidean setting do not readily carry over to more general settings. We develop a non-Euclidean analog of the recent proximal sampler of [LST21], which naturally induces regularization by an object known as the log-Laplace transform (LLT) of a density. We prove new mathematical properties (with an algorithmic flavor) of the LLT, such as strong convexity-smoothness duality and an isoperimetric inequality, which are used to prove a mixing time on our proximal sampler matching [LST21] under a warm start. As our main application, we show our warm-started sampler improves the value oracle complexity of differentially private convex optimization in $\ell_p$ and Schatten-$p$ norms for $p \in [1, 2]$ to match the Euclidean setting [GLL22], while retaining state-of-the-art excess risk bounds [GLLST23]. We find our investigation of the LLT to be a promising proof-of-concept of its utility as a tool for designing samplers, and outline directions for future exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06085v3</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivakanth Gopi, Yin Tat Lee, Daogao Liu, Ruoqi Shen, Kevin Tian</dc:creator>
    </item>
    <item>
      <title>Optimization-centric cutting feedback for semiparametric models</title>
      <link>https://arxiv.org/abs/2509.18708</link>
      <description>arXiv:2509.18708v2 Announce Type: replace-cross 
Abstract: Complex statistical models are often built by combining multiple submodels, called modules. Here, we consider modular inference where the modules contain both parametric and nonparametric components. In such cases, standard Bayesian inference can be highly sensitive to misspecification in any module, and common priors for the nonparametric components may compromise inference for the parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. Proposed cut posteriors are defined via a variational optimization problem like other generalized posteriors, but regularization is based on R\'{e}nyi divergence, instead of Kullback-Leibler divergence (KLD). We show empirically that defining the cut posterior using R\'{e}nyi divergence delivers more robust inference than KLD, and R\'{e}nyi divergence reduces the tendency of uncertainty underestimation when the variational approximations impose strong parametric or independence assumptions. Novel posterior concentration results that accommodate the R\'{e}nyi divergence and allow for semiparametric components are derived, extending existing results for cut posteriors that only apply to KLD and parametric models. These new methods are demonstrated in a benchmark example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18708v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linda S. L. Tan, David J. Nott, David T. Frazier</dc:creator>
    </item>
    <item>
      <title>Scalable approximation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares</title>
      <link>https://arxiv.org/abs/2511.13296</link>
      <description>arXiv:2511.13296v4 Announce Type: replace-cross 
Abstract: Simplicia-simplicial regression concerns statistical modeling scenarios in which both the predictors and the responses are vectors constrained to lie on the simplex. \cite{fiksel2022} introduced a transformation-free linear regression framework for this setting, wherein the regression coefficients are estimated by minimizing the Kullback-Leibler divergence between the observed and fitted compositions, using an expectation-maximization (EM) algorithm for optimization. In this work, we reformulate the problem as a constrained logistic regression model, in line with the methodological perspective of \cite{tsagris2025}, and we obtain parameter estimates via constrained iteratively reweighted least squares. Simulation results indicate that the proposed procedure substantially improves computational efficiency-yielding speed gains ranging from $6\times--326\times$-while providing estimates that closely approximate those obtained from the EM-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13296v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris, Omar Alzeley</dc:creator>
    </item>
  </channel>
</rss>
