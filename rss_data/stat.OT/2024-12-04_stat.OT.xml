<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 12:54:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Internalist Reliabilism in Statistics and Machine Learning: Thoughts on Jun Otsuka's Thinking about Statistics</title>
      <link>https://arxiv.org/abs/2412.02367</link>
      <description>arXiv:2412.02367v1 Announce Type: new 
Abstract: Otsuka (2023) argues for a correspondence between data science and traditional epistemology: Bayesian statistics is internalist; classical (frequentist) statistics is externalist, owing to its reliabilist nature; model selection is pragmatist; and machine learning is a version of virtue epistemology. Where he sees diversity, I see an opportunity for unity. In this article, I argue that classical statistics, model selection, and machine learning share a foundation that is reliabilist in an unconventional sense that aligns with internalism. Hence a unification under internalist reliabilism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02367v1</guid>
      <category>stat.OT</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s44204-024-00210-6</arxiv:DOI>
      <arxiv:journal_reference>The Asian Journal of Philosophy 3, 81 (2024)</arxiv:journal_reference>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Frequentist Statistics as Internalist Reliabilism</title>
      <link>https://arxiv.org/abs/2411.08547</link>
      <description>arXiv:2411.08547v2 Announce Type: replace 
Abstract: There has long been an impression that reliabilism implies externalism and that frequentist statistics is considered externalist due to its reliabilist nature. I argue, however, that frequentist statistics can be plausibly understood as a form of internalist reliabilism -- internalist in the conventional sense but reliabilist in certain unconventional yet intriguing ways. Crucially, I develop the thesis that reliabilism does not imply externalism, not by stretching the meaning of `reliabilism' merely to break the implication, but in order to gain a deeper understanding of frequentist statistics, which represents one of the most sustained attempts by scientists to develop an epistemology for their own use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08547v2</guid>
      <category>stat.OT</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>The Logic of Counterfactuals and the Epistemology of Causal Inference</title>
      <link>https://arxiv.org/abs/2405.11284</link>
      <description>arXiv:2405.11284v2 Announce Type: replace-cross 
Abstract: The 2021 Nobel Prize in Economics recognizes a type of causal model known as the Rubin causal model, or potential outcome framework, which deserves far more attention from philosophers than it currently receives. To spark philosophers' interest, I develop a dialectic connecting the Rubin causal model to the Lewis-Stalnaker debate on a logical principle of counterfactuals: Conditional Excluded Middle (CEM). I begin by playing good cop for CEM, developing a new argument in its favor -- a Quine-Putnam-style indispensability argument. This argument is based on the observation that CEM seems to be indispensable to the Rubin causal model, which underpins our best scientific theory of causal inference in health and social sciences -- a Nobel Prize-winning theory. Indeed, CEM has long remained a core assumption of the Rubin causal model, despite challenges from within the statistics and economics communities over twenty years ago. I then switch sides to play bad cop for CEM, undermining the indispensability argument by developing a new theory of causal inference that dispenses with CEM while preserving the successes of the original theory (thanks to a new theorem proved here). The key, somewhat surprisingly, is to integrate two approaches to causal modeling: the Rubin causal model, more familiar in health and social sciences, and the causal Bayes net, more familiar in philosophy. The good cop/bad cop dialectic is concluded with a connection to broader philosophical issues, including intertheory relations, the revisability of logic, and the role of background assumptions in justifying scientific inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11284v2</guid>
      <category>cs.AI</category>
      <category>stat.OT</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
  </channel>
</rss>
