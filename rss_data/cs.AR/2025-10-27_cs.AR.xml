<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Oct 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>FIFOAdvisor: A DSE Framework for Automated FIFO Sizing of High-Level Synthesis Designs</title>
      <link>https://arxiv.org/abs/2510.20981</link>
      <description>arXiv:2510.20981v1 Announce Type: new 
Abstract: Dataflow hardware designs enable efficient FPGA implementations via high-level synthesis (HLS), but correctly sizing first-in-first-out (FIFO) channel buffers remains challenging. FIFO sizes are user-defined and balance latency and area-undersized FIFOs cause stalls and potential deadlocks, while oversized ones waste memory. Determining optimal sizes is non-trivial: existing methods rely on restrictive assumptions, conservative over-allocation, or slow RTL simulations. We emphasize that runtime-based analyses (i.e., simulation) are the only reliable way to ensure deadlock-free FIFO optimization for data-dependent designs.
  We present FIFOAdvisor, a framework that automatically determines FIFO sizes in HLS designs. It leverages LightningSim, a 99.9\% cycle-accurate simulator supporting millisecond-scale incremental runs with new FIFO configurations. FIFO sizing is formulated as a dual-objective black-box optimization problem, and we explore heuristic and search-based methods to characterize the latency-resource trade-off. FIFOAdvisor also integrates with Stream-HLS, a framework for optimizing affine dataflow designs lowered from C++, MLIR, or PyTorch, enabling deeper optimization of FIFOs in these workloads.
  We evaluate FIFOAdvisor on Stream-HLS design benchmarks spanning linear algebra and deep learning workloads. Our results reveal Pareto-optimal latency-memory frontiers across optimization strategies. Compared to baseline designs, FIFOAdvisor achieves much lower memory usage with minimal delay overhead. Additionally, it delivers significant runtime speedups over traditional HLS/RTL co-simulation, making it practical for rapid design space exploration. We further demonstrate its capability on a complex accelerator with data-dependent control flow.
  Code and results: https://github.com/sharc-lab/fifo-advisor</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20981v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Abi-Karam, Rishov Sarkar, Suhail Basalama, Jason Cong, Callie Hao</dc:creator>
    </item>
    <item>
      <title>Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs</title>
      <link>https://arxiv.org/abs/2510.21533</link>
      <description>arXiv:2510.21533v1 Announce Type: new 
Abstract: As IoT and edge inference proliferate,there is a growing need to simultaneously optimize area and delay in lookup-table (LUT)-based multipliers that implement large numbers of low-bitwidth operations in parallel. This paper proposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx 7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the logic functions mapped to the LUTs, the proposed method reduces the LUT count by one compared with the prior 12-LUT design while also shortening the critical path. Evaluation confirms that the circuit attains minimal resource usage and a critical-path delay of 2.750 ns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21533v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Misaki Kida, Shimpei Sato</dc:creator>
    </item>
    <item>
      <title>Accelerating Electrostatics-based Global Placement with Enhanced FFT Computation</title>
      <link>https://arxiv.org/abs/2510.21547</link>
      <description>arXiv:2510.21547v1 Announce Type: new 
Abstract: Global placement is essential for high-quality and efficient circuit placement for complex modern VLSI designs. Recent advancements, such as electrostatics-based analytic placement, have improved scalability and solution quality. This work demonstrates that using an accelerated FFT technique, AccFFT, for electric field computation significantly reduces runtime. Experimental results on standard benchmarks show significant improvements when incorporated into the ePlace-MS and Pplace-MS algorithms, e.g., a 5.78x speedup in FFT computation and a 32% total runtime improvement against ePlace-MS, with 1.0% reduction of scaled half-perimeter wirelength after detailed placement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21547v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hangyu Zhang, Sachin S. Sapatnekar</dc:creator>
    </item>
    <item>
      <title>Lincoln AI Computing Survey (LAICS) and Trends</title>
      <link>https://arxiv.org/abs/2510.20931</link>
      <description>arXiv:2510.20931v1 Announce Type: cross 
Abstract: In the past year, generative AI (GenAI) models have received a tremendous amount of attention, which in turn has increased attention to computing systems for training and inference for GenAI. Hence, an update to this survey is due. This paper is an update of the survey of AI accelerators and processors from past seven years, which is called the Lincoln AI Computing Survey -- LAICS (pronounced "lace"). This multi-year survey collects and summarizes the current commercial accelerators that have been publicly announced with peak performance and peak power consumption numbers. In the same tradition of past papers of this survey, the performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. Market segments are highlighted on the scatter plot, and zoomed plots of each segment are also included. A brief description of each of the new accelerators that have been added in the survey this year is included, and this update features a new categorization of computing architectures that implement each of the accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20931v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner</dc:creator>
    </item>
    <item>
      <title>GreenMalloc: Allocator Optimisation for Industrial Workloads</title>
      <link>https://arxiv.org/abs/2510.21405</link>
      <description>arXiv:2510.21405v1 Announce Type: cross 
Abstract: We present GreenMalloc, a multi objective search-based framework for automatically configuring memory allocators. Our approach uses NSGA II and rand_malloc as a lightweight proxy benchmarking tool. We efficiently explore allocator parameters from execution traces and transfer the best configurations to gem5, a large system simulator, in a case study on two allocators: the GNU C/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads, our empirical results show up to 4.1 percantage reduction in average heap usage without loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21405v1</guid>
      <category>cs.SE</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Dakhama, W. B. Langdon, Hector D. Menendez, Karine Even-Mendoza</dc:creator>
    </item>
    <item>
      <title>Selective Parallel Loading of Large-Scale Compressed Graphs with ParaGrapher</title>
      <link>https://arxiv.org/abs/2404.19735</link>
      <description>arXiv:2404.19735v3 Announce Type: replace 
Abstract: Comprehensive evaluation is one of the basis of experimental science. In High-Performance Graph Processing, a thorough evaluation of contributions becomes more achievable by supporting common input formats over different frameworks. However, each framework creates its specific format, which may not support reading large-scale real-world graph datasets. This shows a demand for high-performance libraries capable of loading graphs to (i) accelerate designing new graph algorithms, (ii) to evaluate the contributions on a wide range of graph algorithms, and (iii) to facilitate easy and fast comparison over different graph frameworks.
  To that end, we present ParaGrapher, a high-performance API and library for loading large-scale and compressed graphs. ParaGrapher supports different types of requests for accessing graphs in shared- and distributed-memory and out-of-core graph processing. We explain the design of ParaGrapher and present a performance model of graph decompression, which is used for evaluation of ParaGrapher over three storage types. Our evaluation shows that by decompressing compressed graphs in WebGraph format, ParaGrapher delivers up to 3.2 times speedup in loading and up to 5.2 times speedup in end-to-end execution in comparison to the binary and textual formats.
  ParaGrapher is available online on https://blogs.qub.ac.uk/DIPSA/ParaGrapher/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19735v3</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Koohi Esfahani, Marco D'Antonio, Syed Ibtisam Tauhidi, Thai Son Mai, Hans Vandierendonck</dc:creator>
    </item>
  </channel>
</rss>
