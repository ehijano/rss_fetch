<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Mar 2024 14:39:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Resolution of Simpson's paradox via the common cause principle</title>
      <link>https://arxiv.org/abs/2403.00957</link>
      <description>arXiv:2403.00957v1 Announce Type: cross 
Abstract: Simpson's paradox is an obstacle to establishing a probabilistic association between two events $a_1$ and $a_2$, given the third (lurking) random variable $B$. We focus on scenarios when the random variables $A$ (which combines $a_1$, $a_2$, and their complements) and $B$ have a common cause $C$ that need not be observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For such cases, the correct association between $a_1$ and $a_2$ is to be defined via conditioning over $C$. This set-up generalizes the original Simpson's paradox. Now its two contradicting options simply refer to two particular and different causes $C$. We show that if $B$ and $C$ are binary and $A$ is quaternary (the minimal and the most widespread situation for valid Simpson's paradox), the conditioning over any binary common cause $C$ establishes the same direction of the association between $a_1$ and $a_2$ as the conditioning over $B$ in the original formulation of the paradox. Thus, for the minimal common cause, one should choose the option of Simpson's paradox that assumes conditioning over $B$ and not its marginalization. For tertiary (unobserved) common causes $C$ all three options of Simpson's paradox become possible (i.e. marginalized, conditional, and none of them), and one needs prior information on $C$ to choose the right option.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00957v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Hovhannisyan, A. E. Allahverdyan</dc:creator>
    </item>
    <item>
      <title>Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice</title>
      <link>https://arxiv.org/abs/2403.00961</link>
      <description>arXiv:2403.00961v1 Announce Type: cross 
Abstract: With the increasing availability of diverse datasets, ranging from small-scale experimental data points to large and complex data repositories and powerful data analysis tools, it is increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned in integrating data science into undergraduate physics education. In this article, we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and inspiration to educators who seek to integrate data science into their teaching, helping to prepare the next generation of physicists for a data-driven world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00961v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karan Shah, Julie Butler, Alexis Knaub, An{\i}l Zengino\u{g}lu, William Ratcliff, Mohammad Soltanieh-ha</dc:creator>
    </item>
    <item>
      <title>Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings</title>
      <link>https://arxiv.org/abs/2403.01234</link>
      <description>arXiv:2403.01234v1 Announce Type: cross 
Abstract: Exploring molecular spaces is crucial for advancing our understanding of chemical properties and reactions, leading to groundbreaking innovations in materials science, medicine, and energy. This paper explores an approach for active learning in molecular discovery using Deep Kernel Learning (DKL), a novel approach surpassing the limits of classical Variational Autoencoders (VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, which analyze molecular structures based on similarity, revealing limitations due to sparse regularities in latent spaces. DKL, however, offers a more holistic perspective by correlating structure with properties, creating latent spaces that prioritize molecular functionality. This is achieved by recalculating embedding vectors iteratively, aligning with the experimental availability of target properties. The resulting latent spaces are not only better organized but also exhibit unique characteristics such as concentrated maxima representing molecular functionalities and a correlation between predictive uncertainty and error. Additionally, the formation of exclusion regions around certain compounds indicates unexplored areas with potential for groundbreaking functionalities. This study underscores DKL's potential in molecular research, offering new avenues for understanding and discovering molecular functionalities beyond classical VAE limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01234v1</guid>
      <category>cs.LG</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayana Ghosh, Maxim Ziatdinov and, Sergei V. Kalinin</dc:creator>
    </item>
    <item>
      <title>Preserving correlations: A statistical method for generating synthetic data</title>
      <link>https://arxiv.org/abs/2403.01471</link>
      <description>arXiv:2403.01471v1 Announce Type: cross 
Abstract: We propose a method to generate statistically representative synthetic data. The main goal is to be able to maintain in the synthetic dataset the correlations of the features present in the original one, while offering a comfortable privacy level that can be eventually tailored on specific customer demands.
  We describe in detail our algorithm used both for the analysis of the original dataset and for the generation of the synthetic data points. The approach is tested using a large energy-related dataset. We obtain good results both qualitatively (e.g. via vizualizing correlation maps) and quantitatively (in terms of suitable $\ell^1$-type error norms used as evaluation metrics).
  The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01471v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicklas J\"averg{\aa}rd, Rainey Lyons, Adrian Muntean, Jonas Forsman</dc:creator>
    </item>
    <item>
      <title>Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa</title>
      <link>https://arxiv.org/abs/2403.01571</link>
      <description>arXiv:2403.01571v1 Announce Type: cross 
Abstract: The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification algorithm gives the confusion matrix and Kappa. Theory and methods are discussed in detail and then applied to Monte Carlo data and real datasets. Four very different real datasets - Breast Cancer, Coronary Heart Disease, Bankruptcy, and Particle Identification - are analysed, with both continuous and discrete values, and their classification performance compared to the expected theoretical limit. In all cases this analysis shows that the algorithms could not have performed any better due to the underlying probability density functions for the two classes. Important lessons are learnt on how to predict the performance of algorithms for imbalanced data using training datasets that are approximately balanced. Machine learning is very powerful but classification performance ultimately depends on the quality of the data and the relevance of the variables to the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01571v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. Crow, S. J. Watts</dc:creator>
    </item>
    <item>
      <title>nimCSO: A Nim package for Compositional Space Optimization</title>
      <link>https://arxiv.org/abs/2403.02340</link>
      <description>arXiv:2403.02340v1 Announce Type: cross 
Abstract: nimCSO is a high-performance tool implementing several methods for selecting components (data dimensions) in compositional datasets, which optimize the data availability and density for applications such as machine learning. Making said choice is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present. Such spaces are encountered, for instance, in materials science, where datasets on Compositionally Complex Materials (CCMs) often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions.
  At its core, nimCSO leverages the metaprogramming ability of the Nim language (nim-lang.org) to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file. As demonstrated in this paper, nimCSO reaches the physical limits of the hardware (L1 cache latency) and can outperform an efficient native Python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming NumPy implementation 35 and 17 times, respectively, when checking a candidate solution.
  It is designed to be both (1) a user-ready tool, implementing two efficient brute-force approaches (for handling up to 25 dimensions), a custom search algorithm (for up to 40 dimensions), and a genetic algorithm (for any dimensionality), and (2) a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability. All configuration is done with a simple human-readable YAML config file and plain text data files, making it easy to modify the search method and its parameters with no knowledge of programming and only basic command line skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02340v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam M. Krajewski, Arindam Debnath, Wesley F. Reinhart, Allison M. Beese, Zi-Kui Liu</dc:creator>
    </item>
    <item>
      <title>Quantum Anomaly Detection with a Spin Processor in Diamond</title>
      <link>https://arxiv.org/abs/2201.10263</link>
      <description>arXiv:2201.10263v2 Announce Type: replace-cross 
Abstract: In the processing of quantum computation, analyzing and learning the pattern of the quantum data are essential for many tasks. Quantum machine learning algorithms can not only deal with the quantum states generated in the preceding quantum procedures, but also the quantum registers encoding classical problems. In this work, we experimentally demonstrate the anomaly detection of quantum states encoding audio samples with a three-qubit quantum processor consisting of solid-state spins in diamond. By training the quantum machine with a few normal samples, the quantum machine can detect the anomaly samples with a minimum error rate of 15.4%. These results show the power of quantum anomaly detection in dealing with machine learning tasks and the potential to detect abnormal output of quantum devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.10263v2</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihua Chai, Ying Liu, Mengqi Wang, Yuhang Guo, Fazhan Shi, Zhaokai Li, Ya Wang, Jiangfeng Du</dc:creator>
    </item>
    <item>
      <title>Topological Kolmogorov complexity and the Berezinskii-Kosterlitz-Thouless mechanism</title>
      <link>https://arxiv.org/abs/2305.05396</link>
      <description>arXiv:2305.05396v2 Announce Type: replace-cross 
Abstract: Topology plays a fundamental role in our understanding of many-body physics, from vortices and solitons in classical field theory, to phases and excitations in quantum matter. Topological phenomena are intimately connected to the distribution of information content - that, differently from ordinary matter, is now governed by non-local degrees of freedom. However, a precise characterization of how topological effects govern the complexity of a many-body state - i.e., its partition function - is presently unclear. In this work, we show how topology and complexity are directly intertwined concepts in the context of classical statistical mechanics. In concrete, we present a theory that shows how the Kolmogorov complexity of a classical partition function sampling carries unique, distinctive features depending on the presence of topological excitations in the system. We confront two-dimensional Ising, Heisenberg, and XY models on several topologies and study the corresponding samplings as high-dimensional manifolds in configuration space, quantifying their complexity via the intrinsic dimension. While for the Ising and Heisenberg models the intrinsic dimension is independent of the real-space topology, for the XY model it depends crucially on temperature: across the Berezkinskii-Kosterlitz-Thouless (BKT) transition, complexity becomes topology dependent. In the BKT phase, it displays a characteristic dependence on the homology of the real-space manifold, and, for $g$-torii, it follows a scaling that is solely genus dependent. We argue that this behavior is intimately connected to the emergence of an order parameter in data space, the conditional connectivity, that displays scaling behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05396v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vittorio Vitale, Tiago Mendes-Santos, Alex Rodriguez, Marcello Dalmonte</dc:creator>
    </item>
    <item>
      <title>Comparison of modularity-based approaches for nodes clustering in hypergraphs</title>
      <link>https://arxiv.org/abs/2401.14028</link>
      <description>arXiv:2401.14028v2 Announce Type: replace-cross 
Abstract: Statistical analysis and node clustering in hypergraphs constitute an emerging topic suffering from a lack of standardization. In contrast to the case of graphs, the concept of nodes' community in hypergraphs is not unique and encompasses various distinct situations. In this work, we conducted a comparative analysis of the performance of modularity-based methods for clustering nodes in binary hypergraphs. To address this, we begin by presenting, within a unified framework, the various hypergraph modularity criteria proposed in the literature, emphasizing their differences and respective focuses. Subsequently, we provide an overview of the state-of-the-art codes available to maximize hypergraph modularities for detecting node communities in binary hypergraphs. Through exploration of various simulation settings with controlled ground truth clustering, we offer a comparison of these methods using different quality measures, including true clustering recovery, running time, (local) maximization of the objective, and the number of clusters detected. Our contribution marks the first attempt to clarify the advantages and drawbacks of these newly available methods. This effort lays the foundation for a better understanding of the primary objectives of modularity-based node clustering methods for binary hypergraphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14028v2</guid>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Veronica PodaLPSM, Catherine MatiasLPSM</dc:creator>
    </item>
  </channel>
</rss>
