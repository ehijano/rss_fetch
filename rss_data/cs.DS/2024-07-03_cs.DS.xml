<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jul 2024 01:48:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Validation and Implementation of ILBFS</title>
      <link>https://arxiv.org/abs/2407.01637</link>
      <description>arXiv:2407.01637v1 Announce Type: new 
Abstract: Recursive Best-First Search (RBFS) is a heuristic search algorithm known for its efficient memory usage compared to traditional best-first search methods like A*. Despite its theoretical advantages, RBFS is complex and difficult to teach and to implement, limiting its widespread adoption. To address these challenges, Iterative Linear Best-First Search (ILBFS) was introduced as a simpler, more intuitive alternative while maintaining the linear space complexity of RBFS. In this paper, we present the first implementation of ILBFS, validate its memory usage and node expansion order claims, and explore critical aspects of its implementation, such as tie-breaking and node deletion mechanisms. Our findings demonstrate that ILBFS can serve as an effective stepping stone for researchers and practitioners looking to use memory efficient best-first search methods, facilitating the adoption of RBFS-like algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01637v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fred Matanel Grabovski, Lior Yasur</dc:creator>
    </item>
    <item>
      <title>From Directed Steiner Tree to Directed Polymatroid Steiner Tree in Planar Graphs</title>
      <link>https://arxiv.org/abs/2407.01904</link>
      <description>arXiv:2407.01904v1 Announce Type: new 
Abstract: In the Directed Steiner Tree (DST) problem the input is a directed edge-weighted graph $G=(V,E)$, a root vertex $r$ and a set $S \subseteq V$ of $k$ terminals. The goal is to find a min-cost subgraph that connects $r$ to each of the terminals. DST admits an $O(\log^2 k/\log \log k)$-approximation in quasi-polynomial time, and an $O(k^{\epsilon})$-approximation for any fixed $\epsilon &gt; 0$ in polynomial-time. Resolving the existence of a polynomial-time poly-logarithmic approximation is a major open problem in approximation algorithms. In a recent work, Friggstad and Mousavi [ICALP 2023] obtained a simple and elegant polynomial-time $O(\log k)$-approximation for DST in planar digraphs via Thorup's shortest path separator theorem. We build on their work and obtain several new results on DST and related problems.
  - We develop a tree embedding technique for rooted problems in planar digraphs via an interpretation of the recursion in Friggstad and Mousavi [ICALP 2023]. Using this we obtain polynomial-time poly-logarithmic approximations for Group Steiner Tree, Covering Steiner Tree, and the Polymatroid Steiner Tree problems in planar digraphs. All these problems are hard to approximate to within a factor of $\Omega(\log^2 n/\log \log n)$ even in trees.
  - We prove that the natural cut-based LP relaxation for DST has an integrality gap of $O(\log^2 k)$ in planar graphs. This is in contrast to general graphs where the integrality gap of this LP is known to be $\Omega(k)$ and $\Omega(n^{\delta})$ for some fixed $\delta &gt; 0$.
  - We combine the preceding results with density based arguments to obtain poly-logarithmic approximations for the multi-rooted versions of the problems in planar digraphs. For DST our result improves the $O(R + \log k)$ approximation of Friggstad and Mousavi [ICALP 2023] when $R= \omega(\log^2 k)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01904v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandra Chekuri, Rhea Jain, Shubhang Kulkarni, Da Wei Zheng, Weihao Zhu</dc:creator>
    </item>
    <item>
      <title>Online Unbounded Knapsack</title>
      <link>https://arxiv.org/abs/2407.02045</link>
      <description>arXiv:2407.02045v1 Announce Type: new 
Abstract: We analyze the competitive ratio and the advice complexity of the online unbounded knapsack problem. An instance is given as a sequence of n items with a size and a value each, and an algorithm has to decide how often to pack each item into a knapsack of bounded capacity. The items are given online and the total size of the packed items must not exceed the knapsack's capacity, while the objective is to maximize the total value of the packed items. While each item can only be packed once in the classical 0-1 knapsack problem, the unbounded version allows for items to be packed multiple times. We show that the simple unbounded knapsack problem, where the size of each item is equal to its value, allows for a competitive ratio of 2. We also analyze randomized algorithms and show that, in contrast to the 0-1 knapsack problem, one uniformly random bit cannot improve an algorithm's performance. More randomness lowers the competitive ratio to less than 1.736, but it can never be below 1.693. In the advice complexity setting, we measure how many bits of information the algorithm has to know to achieve some desired solution quality. For the simple unbounded knapsack problem, one advice bit lowers the competitive ratio to 3/2. While this cannot be improved with fewer than log(n) advice bits for instances of length n, a competitive ratio of 1+epsilon can be achieved with O(log(n/epsilon)/epsilon) advice bits for any epsilon&gt;0. We further show that no amount of advice bounded by a function f(n) allows an algorithm to be optimal. We also study the online general unbounded knapsack problem and show that it does not allow for any bounded competitive ratio for deterministic and randomized algorithms, as well as for algorithms using fewer than log(n) advice bits. We also provide an algorithm that uses O(log(n/epsilon)/epsilon) advice bits to achieve a competitive ratio of 1+epsilon for any epsilon&gt;0.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02045v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans-Joachim B\"ockenhauer, Matthias Gehnen, Juraj Hromkovi\v{c}, Ralf Klasing, Dennis Komm, Henri Lotze, Daniel Mock, Peter Rossmanith, Moritz Stocker</dc:creator>
    </item>
    <item>
      <title>On polynomial kernelization for Stable Cutset</title>
      <link>https://arxiv.org/abs/2407.02086</link>
      <description>arXiv:2407.02086v1 Announce Type: new 
Abstract: A stable cutset in a graph $G$ is a set $S\subseteq V(G)$ such that vertices of $S$ are pairwise non-adjacent and such that $G-S$ is disconnected, i.e., it is both stable (or independent) set and a cutset (or separator). Unlike general cutsets, it is $NP$-complete to determine whether a given graph $G$ has any stable cutset. Recently, Rauch et al.\ [FCT 2023] gave a number of fixed-parameter tractable (FPT) algorithms, time $f(k)\cdot |V(G)|^c$, for Stable Cutset under a variety of parameters $k$ such as the size of a (given) dominating set, the size of an odd cycle transversal, or the deletion distance to $P_5$-free graphs. Earlier works imply FPT algorithms relative to clique-width and relative to solution size.
  We complement these findings by giving the first results on the existence of polynomial kernelizations for \stablecutset, i.e., efficient preprocessing algorithms that return an equivalent instance of size polynomial in the parameter value. Under the standard assumption that $NP\nsubseteq coNP/poly$, we show that no polynomial kernelization is possible relative to the deletion distance to a single path, generalizing deletion distance to various graph classes, nor by the size of a (given) dominating set. We also show that under the same assumption no polynomial kernelization is possible relative to solution size, i.e., given $(G,k)$ answering whether there is a stable cutset of size at most $k$. On the positive side, we show polynomial kernelizations for parameterization by modulators to a single clique, to a cluster or a co-cluster graph, and by twin cover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02086v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Kratsch, Van Bang Le</dc:creator>
    </item>
    <item>
      <title>Minsum Problem for Discrete and Weighted Set Flow on Dynamic Path Network</title>
      <link>https://arxiv.org/abs/2407.02177</link>
      <description>arXiv:2407.02177v1 Announce Type: new 
Abstract: In this research, we examine the minsum flow problem in dynamic path networks where flows are represented as discrete and weighted sets. The minsum flow problem has been widely studied for its relevance in finding evacuation routes during emergencies such as earthquakes. However, previous approaches often assume that individuals are separable and identical, which does not adequately account for the fact that some groups of people, such as families, need to move together and that some groups may be more important than others. To address these limitations, we modify the minsum flow problem to support flows represented as discrete and weighted sets. We also propose a 2-approximation pseudo-polynomial time algorithm to solve this modified problem for path networks with uniform capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02177v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bubai Manna, Bodhayan Roy, Vorapong Suppakitpaisarn</dc:creator>
    </item>
    <item>
      <title>Finer-Grained Hardness of Kernel Density Estimation</title>
      <link>https://arxiv.org/abs/2407.02372</link>
      <description>arXiv:2407.02372v1 Announce Type: new 
Abstract: In batch Kernel Density Estimation (KDE) for a kernel function $f$, we are given as input $2n$ points $x^{(1)}, \cdots, x^{(n)}, y^{(1)}, \cdots, y^{(n)}$ in dimension $m$, as well as a vector $v \in \mathbb{R}^n$. These inputs implicitly define the $n \times n$ kernel matrix $K$ given by $K[i,j] = f(x^{(i)}, y^{(j)})$. The goal is to compute a vector $v$ which approximates $K w$ with $|| Kw - v||_\infty &lt; \varepsilon ||w||_1$. A recent line of work has proved fine-grained lower bounds conditioned on SETH. Backurs et al. first showed the hardness of KDE for Gaussian-like kernels with high dimension $m = \Omega(\log n)$ and large scale $B = \Omega(\log n)$. Alman et al. later developed new reductions in roughly this same parameter regime, leading to lower bounds for more general kernels, but only for very small error $\varepsilon &lt; 2^{- \log^{\Omega(1)} (n)}$.
  In this paper, we refine the approach of Alman et al. to show new lower bounds in all parameter regimes, closing gaps between the known algorithms and lower bounds. In the setting where $m = C\log n$ and $B = o(\log n)$, we prove Gaussian KDE requires $n^{2-o(1)}$ time to achieve additive error $\varepsilon &lt; \Omega(m/B)^{-m}$, matching the performance of the polynomial method up to low-order terms. In the low dimensional setting $m = o(\log n)$, we show that Gaussian KDE requires $n^{2-o(1)}$ time to achieve $\varepsilon$ such that $\log \log (\varepsilon^{-1}) &gt; \tilde \Omega ((\log n)/m)$, matching the error bound achievable by FMM up to low-order terms. To our knowledge, no nontrivial lower bound was previously known in this regime.
  Our new lower bounds make use of an intricate analysis of a special case of the kernel matrix -- the `counting matrix'. As a key technical lemma, we give a novel approach to bounding the entries of its inverse by using Schur polynomials from algebraic combinatorics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02372v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Josh Alman, Yunfeng Guan</dc:creator>
    </item>
    <item>
      <title>Improved Space-Efficient Approximate Nearest Neighbor Search Using Function Inversion</title>
      <link>https://arxiv.org/abs/2407.02468</link>
      <description>arXiv:2407.02468v1 Announce Type: new 
Abstract: Approximate nearest neighbor search (ANN) data structures have widespread applications in machine learning, computational biology, and text processing. The goal of ANN is to preprocess a set S so that, given a query q, we can find a point y whose distance from q approximates the smallest distance from q to any point in S. For most distance functions, the best-known ANN bounds for high-dimensional point sets are obtained using techniques based on locality-sensitive hashing (LSH).
  Unfortunately, space efficiency is a major challenge for LSH-based data structures. Classic LSH techniques require a very large amount of space, oftentimes polynomial in |S|. A long line of work has developed intricate techniques to reduce this space usage, but these techniques suffer from downsides: they must be hand tailored to each specific LSH, are often complicated, and their space reduction comes at the cost of significantly increased query times.
  In this paper we explore a new way to improve the space efficiency of LSH using function inversion techniques, originally developed in (Fiat and Naor 2000).
  We begin by describing how function inversion can be used to improve LSH data structures. This gives a fairly simple, black box method to reduce LSH space usage.
  Then, we give a data structure that leverages function inversion to improve the query time of the best known near-linear space data structure for approximate nearest neighbor search under Euclidean distance: the ALRW data structure of (Andoni, Laarhoven, Razenshteyn, and Waingarten 2017). ALRW was previously shown to be optimal among "list-of-points" data structures for both Euclidean and Manhattan ANN; thus, in addition to giving improved bounds, our results imply that list-of-points data structures are not optimal for Euclidean or Manhattan ANN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02468v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel McCauley</dc:creator>
    </item>
    <item>
      <title>Spanner for the $0/1/\infty$ weighted region problem</title>
      <link>https://arxiv.org/abs/2407.01951</link>
      <description>arXiv:2407.01951v1 Announce Type: cross 
Abstract: We consider the problem of computing an approximate weighted shortest path in a weighted subdivision, with weights assigned from the set $\{0, 1, \infty\}$. We present a data structure $B$, which stores a set of convex, non-overlapping regions. These include zero-cost regions (0-regions) with a weight of $0$ and obstacles with a weight of $\infty$, all embedded in a plane with a weight of $1$. The data structure $B$ can be constructed in expected time $O(N + (n/\varepsilon^3)(\log(n/\varepsilon) + \log N))$, where $n$ is the total number of regions, $N$ represents the total complexity of the regions, and $1 + \varepsilon$ is the approximation factor, for any $0 &lt; \varepsilon &lt; 1$. Using $B$, one can compute an approximate weighted shortest path from any point $s$ to any point $t$ in $O(N + n/\varepsilon^3 + (n/\varepsilon^2) \log(n/\varepsilon) + (\log N)/\varepsilon)$ time. In the special case where the 0-regions and obstacles are polygons (not necessarily convex), $B$ contains a $(1 + \varepsilon)$-spanner of the input vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01951v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joachim Gudmundsson, Zijin Huang, Andr\'e van Renssen, Sampson Wong</dc:creator>
    </item>
    <item>
      <title>Laminar Matroid Secretary: Greedy Strikes Back</title>
      <link>https://arxiv.org/abs/2308.09880</link>
      <description>arXiv:2308.09880v2 Announce Type: replace 
Abstract: We show that a simple greedy algorithm is $4.75$ probability-competitive for the Laminar Matroid Secretary Problem, improving the $3\sqrt{3} \approx 5.196$-competitive algorithm based on the forbidden sets technique (Soto, Turkieltaub, and Verdugo, 2018).</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09880v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyi Huang, Zahra Parsaeian, Zixuan Zhu</dc:creator>
    </item>
    <item>
      <title>A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton</title>
      <link>https://arxiv.org/abs/2310.04218</link>
      <description>arXiv:2310.04218v5 Announce Type: replace 
Abstract: Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same "skeleton" (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).
  These combinatorial characterizations also suggest several natural algorithmic questions. One of these is: given an undirected graph $G$ as input, how many distinct Markov equivalence classes have the skeleton $G$? Much work has been devoted in the last few years to this and other closely related problems. However, to the best of our knowledge, a polynomial time algorithm for the problem remains unknown.
  In this paper, we make progress towards this goal by giving a fixed parameter tractable algorithm for the above problem, with the parameters being the treewidth and the maximum degree of the input graph $G$. The main technical ingredient in our work is a construction we refer to as shadow, which lets us create a "local description" of long-range constraints imposed by the combinatorial characterizations of MECs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04218v5</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)</arxiv:journal_reference>
      <dc:creator>Vidya Sagar Sharma</dc:creator>
    </item>
    <item>
      <title>Dynamically Maintaining the Persistent Homology of Time Series</title>
      <link>https://arxiv.org/abs/2311.01115</link>
      <description>arXiv:2311.01115v2 Announce Type: replace 
Abstract: We present a dynamic data structure for maintaining the persistent homology of a time series of real numbers. The data structure supports local operations, including the insertion and deletion of an item and the cutting and concatenating of lists, each in time $O(\log n + k)$, in which $n$ counts the critical items and $k$ the changes in the augmented persistence diagram. To achieve this, we design a tailor-made tree structure with an unconventional representation, referred to as banana tree, which may be useful in its own right.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01115v2</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Cultrera di Montesano, Herbert Edelsbrunner, Monika Henzinger, Lara Ost</dc:creator>
    </item>
    <item>
      <title>Deterministic Minimum Steiner Cut in Maximum Flow Time</title>
      <link>https://arxiv.org/abs/2312.16415</link>
      <description>arXiv:2312.16415v2 Announce Type: replace 
Abstract: We devise a deterministic algorithm for minimum Steiner cut, which uses $(\log n)^{O(1)}$ maximum flow calls and additional near-linear time. This algorithm improves on Li and Panigrahi's (FOCS 2020) algorithm, which uses $(\log n)^{O(1/\epsilon^4)}$ maximum flow calls and additional $O(m^{1+\epsilon})$ time, for $\epsilon &gt; 0$. Our algorithm thus shows that deterministic minimum Steiner cut can be solved in maximum flow time up to polylogarithmic factors, given any black-box deterministic maximum flow algorithm. Our main technical contribution is a novel deterministic graph decomposition method for terminal vertices that generalizes all existing $s$-strong partitioning methods, which we believe may have future applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16415v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Ding, Jason Li</dc:creator>
    </item>
    <item>
      <title>A Nearly Linear Time Construction of Approximate Single-Source Distance Sensitivity Oracles</title>
      <link>https://arxiv.org/abs/2401.01103</link>
      <description>arXiv:2401.01103v2 Announce Type: replace 
Abstract: An \emph{$\alpha$-approximate vertex fault-tolerant distance sensitivity oracle} (\emph{$\alpha$-VSDO}) for a weighted input graph $G=(V, E, w)$ and a source vertex $s \in V$ is the data structure answering an $\alpha$-approximate distance from $s$ to $t$ in $G-x$ for any given query $(x, t) \in V \times V$. It is a data structure version of the so-called single-source replacement path problem (SSRP). In this paper, we present a new \emph{nearly linear-time} algorithm of constructing a $(1 + \epsilon)$-VSDO for any directed input graph with polynomially bounded integer edge weights. More precisely, the presented oracle attains $\tilde{O}(m \log (nW)/ \epsilon + n \log^2 (nW)/\epsilon^2)$ construction time, $\tilde{O}(n \log (nW) / \epsilon)$ size, and $\tilde{O}(1/\epsilon)$ query time, where $n$ is the number of vertices, $m$ is the number of edges, and $W$ is the maximum edge weight. These bounds are all optimal up to polylogarithmic factors. To the best of our knowledge, this is the first non-trivial algorithm for SSRP/VSDO beating $\tilde{O}(mn)$ computation time for directed graphs with general edge weight functions, and also the first nearly linear-time construction breaking approximation factor 3. Such a construction has been unknown even for undirected and unweighted graphs. In addition, our result implies that the known conditional lower bounds for the exact SSRP computation does not apply to the case of approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01103v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaito Harada, Naoki Kitamura, Taisuke Izumi, Toshimitsu Masuzawa</dc:creator>
    </item>
    <item>
      <title>Noisy Computing of the Threshold Function</title>
      <link>https://arxiv.org/abs/2403.07227</link>
      <description>arXiv:2403.07227v2 Announce Type: replace 
Abstract: Let $\mathsf{TH}_k$ denote the $k$-out-of-$n$ threshold function: given $n$ input Boolean variables, the output is $1$ if and only if at least $k$ of the inputs are $1$. We consider the problem of computing the $\mathsf{TH}_k$ function using noisy readings of the Boolean variables, where each reading is incorrect with some fixed and known probability $p \in (0,1/2)$. As our main result, we show that it is sufficient to use $(1+o(1)) \frac{n\log \frac{m}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)}$ queries in expectation to compute the $\mathsf{TH}_k$ function with a vanishing error probability $\delta = o(1)$, where $m\triangleq \min\{k,n-k\}$ and $D_{\mathsf{KL}}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Conversely, we show that any algorithm achieving an error probability of $\delta = o(1)$ necessitates at least $(1-o(1))\frac{(n-m)\log\frac{m}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)}$ queries in expectation. The upper and lower bounds are tight when $m=o(n)$, and are within a multiplicative factor of $\frac{n}{n-m}$ when $m=\Theta(n)$. In particular, when $k=n/2$, the $\mathsf{TH}_k$ function corresponds to the $\mathsf{MAJORITY}$ function, in which case the upper and lower bounds are tight up to a multiplicative factor of two. Compared to previous work, our result tightens the dependence on $p$ in both the upper and lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07227v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziao Wang, Nadim Ghaddar, Banghua Zhu, Lele Wang</dc:creator>
    </item>
    <item>
      <title>A Nearly Optimal Deterministic Algorithm for Online Transportation Problem</title>
      <link>https://arxiv.org/abs/2406.03778</link>
      <description>arXiv:2406.03778v2 Announce Type: replace 
Abstract: For the online transportation problem with $m$ server sites, it has long been known that the competitive ratio of any deterministic algorithm is at least $2m-1$. Kalyanasundaram and Pruhs conjectured in 1998 that a deterministic $(2m-1)$-competitive algorithm exists for this problem, a conjecture that has remained open for over two decades.
  In this paper, we propose a new deterministic algorithm named Subtree-Decomposition for the online transportation problem and show that it achieves a competitive ratio of at most $8m-5$. This is the first $O(m)$-competitive deterministic algorithm, coming close to the lower bound of $2m-1$ within a constant factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03778v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tsubasa Harada, Toshiya Itoh</dc:creator>
    </item>
    <item>
      <title>Optimized Deletion From an AVL Tree</title>
      <link>https://arxiv.org/abs/2406.05162</link>
      <description>arXiv:2406.05162v5 Announce Type: replace 
Abstract: An AVL tree is a binary search tree that guarantees $ O\left( \log n \right ) $ search. The guarantee is obtained at the cost of rebalancing the AVL tree, potentially after every insertion or deletion. This article proposes a deletion algorithm that reduces the rebalancing required after deletion compared to the rebalancing required after deletion by a previously reported algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05162v5</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell A. Brown</dc:creator>
    </item>
    <item>
      <title>Near Optimal Dual Fault Tolerant Distance Oracle</title>
      <link>https://arxiv.org/abs/2406.19709</link>
      <description>arXiv:2406.19709v2 Announce Type: replace 
Abstract: We present a dual fault-tolerant distance oracle for undirected and unweighted graphs. Given a set $F$ of two edges, as well as a source node $s$ and a destination node $t$, our oracle returns the length of the shortest path from $s$ to $t$ that avoids $F$ in $O(1)$ time with a high probability. The space complexity of our oracle is $\Tilde{O}(n^2)$ \footnote{$\Tilde{O}$ hides poly$\log n$ factor }, making it nearly optimal in terms of both space and query time. Prior to our work, Pettie and Duan [SODA 2009] designed a dual fault-tolerant distance oracle that required $\Tilde{O}(n^2)$ space and $O(\log n)$ query time. In addition to improving the query time, our oracle is much simpler than the previous approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19709v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipan Dey, Manoj Gupta</dc:creator>
    </item>
    <item>
      <title>Balanced Learned Sort: a new learned model for fast and balanced item bucketing</title>
      <link>https://arxiv.org/abs/2407.00734</link>
      <description>arXiv:2407.00734v2 Announce Type: replace 
Abstract: This paper aims to better understand the strengths and limitations of adopting learned-based approaches in sequential sorting numerical data, via two main research steps.
  First, we study different learned models for distribution-based sorting, starting from some known ones (i.e., two-layer RMI or simple linear models) and then introducing some novel models that either improve the two-layer RMI or are fully new in their algorithmic structure thus resulting space efficient, monotonic, and very fast in building balanced buckets. We test those models over 11 synthetic datasets drawn from different distributions of 200M 64-bit floating-point items, so deriving hints about their ultimate performance and usefulness in designing a sorting algorithm.
  Based on these findings, we select and plug the best models from above in a new learned-based algorithmic scheme and devise three new sorters that we will test against other 6 sequential sorters (5 classic and 1 learned, known and new ones) over 33 datasets (11 synthetic and 22 real), whose size will be up to 800M items. Our experimental figures will show that our learned sorters achieve superior performance on 31 out of all 33 datasets (synthetic and real). In conclusion, these experimental results provide, on the one hand, a comprehensive answer to the main question: Which algorithmic structure for distribution-based sorting is suited to leverage a learned model in order to achieve efficient performance? and, on the other hand, they leave open several other research and engineering questions about the design of a highly performing sequential sorter that is robust over different input distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00734v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Ferragina, Mattia Odorisio</dc:creator>
    </item>
    <item>
      <title>Complexity of Digital Quantum Simulation in the Low-Energy Subspace: Applications and a Lower Bound</title>
      <link>https://arxiv.org/abs/2312.08867</link>
      <description>arXiv:2312.08867v2 Announce Type: replace-cross 
Abstract: Digital quantum simulation has broad applications in approximating unitary evolution of Hamiltonians. In practice, many simulation tasks for quantum systems focus on quantum states in the low-energy subspace instead of the entire Hilbert space. In this paper, we systematically investigate the complexity of digital quantum simulation based on product formulas in the low-energy subspace. We show that the simulation error depends on the effective low-energy norm of the Hamiltonian for a variety of digital quantum simulation algorithms and quantum systems, allowing improvements over the previous complexities for full unitary simulations even for imperfect state preparations {due to thermalization}. In particular, for simulating spin models in the low-energy subspace, we prove that randomized product formulas such as qDRIFT and random permutation require smaller Trotter numbers. Such improvement also persists in symmetry-protected digital quantum simulations. We prove a similar improvement in simulating the dynamics of power-law quantum interactions. We also provide a query lower bound for general digital quantum simulations in the low-energy subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08867v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiyuan Gong, Shuo Zhou, Tongyang Li</dc:creator>
    </item>
    <item>
      <title>On the on-line coloring of unit interval graphs with proper interval representation</title>
      <link>https://arxiv.org/abs/2401.05648</link>
      <description>arXiv:2401.05648v2 Announce Type: replace-cross 
Abstract: We define the problem as a two-player game between Algorithm and Builder. The game is played in rounds. Each round, Builder presents an interval that is neither contained in nor contains any previously presented interval. Algorithm immediately and irrevocably assigns the interval a color that has not been assigned to any interval intersecting it. The set of intervals form an interval representation for a unit interval graph and the colors form a proper coloring of that graph. For every positive integer $\omega$, we define the value $R(\omega)$ as the maximum number of colors for which Builder has a strategy that forces Algorithm to use $R(\omega)$ colors with the restriction that the unit interval graph constructed cannot contain a clique of size $\omega$. In 1981, Chrobak and \'{S}lusarek showed that $R(\omega)\leq2\omega -1$. In 2005, Epstein and Levy showed that $R(\omega)\geq\lfloor{3\omega/2\rfloor}$. This problem remained unsolved for $\omega\geq 3$. In 2022, Bir\'o and Curbelo showed that $R(3)=5$. In this paper, we show that $R(4)=7$</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05648v2</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Israel R. Curbelo, Hannah R. Malko</dc:creator>
    </item>
  </channel>
</rss>
