<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Are there stars in Bluesky? A comparative exploratory analysis of altmetric mentions between X and Bluesky</title>
      <link>https://arxiv.org/abs/2412.05624</link>
      <description>arXiv:2412.05624v1 Announce Type: new 
Abstract: This study examines the shift in the scientific community from X (formerly Twitter) to Bluesky, its impact on scientific communication, and consequently on social metrics (altmetrics). Analyzing 10,174 publications from multidisciplinary and library and information science (LIS) journals in 2024, the results reveal a notable increase in Bluesky activity for multidisciplinary journals in November 2024, likely influenced by political and platform changes, with mentions doubling or quadrupling for journals like Nature and Science. In LIS, the adoption of Bluesky is more limited and shows significant variations across journals, suggesting discipline-specific adoption patterns. However, overall engagement on Bluesky remains significantly lower than on X. While X currently dominates altmetric mentions, the observed growth on Bluesky suggests a potential shift in the future, underscoring its emerging role in academic dissemination and the challenges of adapting scholarly communication metrics across evolving platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05624v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14290752</arxiv:DOI>
      <dc:creator>Wenceslao Arroyo-Machado, Nicolas Robinson-Garcia, Daniel Torres-Salinas</dc:creator>
    </item>
    <item>
      <title>Enhancing Research Methodology and Academic Publishing: A Structured Framework for Quality and Integrity</title>
      <link>https://arxiv.org/abs/2412.05683</link>
      <description>arXiv:2412.05683v1 Announce Type: new 
Abstract: Following a brief introduction to research, research processes, research types, papers, reviews, and evaluations, this paper presents a structured framework for addressing inconsistencies in research methodology, technical writing, quality assessment, and publication standards across academic disciplines. Using a four-dimensional evaluation model that focuses on 1) technical content, 2) structural coherence, 3) writing precision, and 4) ethical integrity, this framework not only standardizes review and publication processes but also serves as a practical guide for authors in preparing high-quality manuscripts. Each of these four dimensions cannot be compromised for the sake of another. Following that, we discuss the components of a research paper adhering to the four-dimensional evaluation model in detail by providing guidelines and principles. By aligning manuscripts with journal standards, reducing review bias, and enhancing transparency, the framework contributes to more reliable and reproducible research results. Moreover, by strengthening cross-disciplinary credibility, improving publication consistency, and fostering public trust in academic literature, this initiative is expected to positively influence both research quality and scholarly publishing's reputation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05683v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Jalil Piran, Nguyen H. Tran</dc:creator>
    </item>
    <item>
      <title>Canadian Publications in Library and Information Science: A Database of research by LIS academics and practitioners in Canada</title>
      <link>https://arxiv.org/abs/2412.05815</link>
      <description>arXiv:2412.05815v1 Announce Type: new 
Abstract: The aim of the Canadian publications in Library and Information Science (LIS) database is to help break down the silos in which the two main target audiences - LIS faculty members and academic librarians - conduct their research. As part of a larger project entitled "Breaking down research silos", we created a database of research contributions by Canadian LIS researchers (academics and practitioners). This was motivated by a desire to make research by Canadian LIS scholars and practitioners more visible and foster collaboration between these two groups. The aim of this paper is to introduce the database, describe the process through which it was created, provide descriptive statistics of the database content, and highlight areas for future development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05815v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-S\'ebastien Sauv\'e, Madelaine Hare, Geoff Krause, Constance Poitras, Poppy Riddle, Philippe Mongeon</dc:creator>
    </item>
    <item>
      <title>Institutional Shifts in Contribution to Indian Research Output during the last two decades</title>
      <link>https://arxiv.org/abs/2412.06652</link>
      <description>arXiv:2412.06652v1 Announce Type: new 
Abstract: In the past few decades, India has emerged as a major knowledge producer, with research output being contributed by a diverse set of institutions ranging from centrally funded to state funded, and from public funded to private funded institutions. A significant change has been witnessed in Indian institutional actors during the last two decades, with various new private universities being set up and several new IITs, NITs, IISERs being established. Therefore, it is important to identify whether the composition of the list of the top 100 research output producing institutions of India has changed significantly during the recent two decades. This study attempted to analyse the changes during the two 10-year periods (2004-13 and 2014-23). The institutions which retain their position within top 100 during both periods are identified, along with the change in their positions. Similarly, institutions that were there in top 100 list during first time period (2004-13) and go out of top 100 list during second time period (2014-23) are also identified. In the same line, the new entrant institutions in the top 100 list during second time period (2014-23) are identified too. The results obtained indicate towards an institutional shift in the contribution to Indian research output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06652v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Kumar Singh, Mousumi Karmakar, Anurag Kanaujia</dc:creator>
    </item>
    <item>
      <title>Leveraging virtual technologies to enhance museums and art collections: insights from project CHANGES</title>
      <link>https://arxiv.org/abs/2412.05880</link>
      <description>arXiv:2412.05880v1 Announce Type: cross 
Abstract: We investigated the use of virtual technologies to digitise and enhance cultural heritage (CH), aligning with Open Science and FAIR principles. Through case studies in museums, we developed reproducible workflows, 3D models, and tools fostering accessibility, inclusivity, and sustainability of CH. Applications include interdisciplinary research, educational innovation, and CH preservation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05880v1</guid>
      <category>cs.GR</category>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Genovese, Ivan Heibi, Silvio Peroni, Sofia Pescarin</dc:creator>
    </item>
    <item>
      <title>Vulnerability of Text-Matching in ML/AI Conference Reviewer Assignments to Collusions</title>
      <link>https://arxiv.org/abs/2412.06606</link>
      <description>arXiv:2412.06606v1 Announce Type: cross 
Abstract: In the peer review process of top-tier machine learning (ML) and artificial intelligence (AI) conferences, reviewers are assigned to papers through automated methods. These assignment algorithms consider two main factors: (1) reviewers' expressed interests indicated by their bids for papers, and (2) reviewers' domain expertise inferred from the similarity between the text of their previously published papers and the submitted manuscripts. A significant challenge these conferences face is the existence of collusion rings, where groups of researchers manipulate the assignment process to review each other's papers, providing positive evaluations regardless of their actual quality. Most efforts to combat collusion rings have focused on preventing bid manipulation, under the assumption that the text similarity component is secure. In this paper, we demonstrate that even in the absence of bidding, colluding reviewers and authors can exploit the machine learning based text-matching component of reviewer assignment used at top ML/AI venues to get assigned their target paper. We also highlight specific vulnerabilities within this system and offer suggestions to enhance its robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06606v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Jhih-Yi (Janet),  Hsieh, Aditi Raghunathan, Nihar B. Shah</dc:creator>
    </item>
    <item>
      <title>Can tweets predict article retractions? A comparison between human and LLM labelling</title>
      <link>https://arxiv.org/abs/2403.16851</link>
      <description>arXiv:2403.16851v2 Announce Type: replace 
Abstract: Quickly detecting problematic research articles is crucial to safeguarding the integrity of scientific research. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to their retraction, potentially serving as an early warning system for scholars. To investigate this, we analysed a dataset of 4,354 Twitter mentions associated with 504 retracted articles. The effectiveness of Twitter mentions in predicting article retractions was evaluated by both manual and Large Language Model (LLM) labelling. Manual labelling results indicated that 25.7% of tweets signalled problems before retraction. Using the manual labelling results as the baseline, we found that LLMs (GPT-4o-mini, Gemini 1.5 Flash, and Claude-3.5-Haiku) outperformed lexicon-based sentiment analysis tools (e.g., TextBlob) in detecting potential problems, suggesting that automatic detection of problematic articles from social media using LLMs is technically feasible. Nevertheless, since only a small proportion of retracted articles (11.1%) were criticised on Twitter prior to retraction, such automatic systems would detect only a minority of problematic articles. Overall, this study offers insights into how social media data, coupled with emerging generative AI techniques, can support research integrity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16851v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Er-Te Zheng, Hui-Zhen Fu, Mike Thelwall, Zhichao Fang</dc:creator>
    </item>
    <item>
      <title>An Analysis of the Impact of Gold Open Access Publications in Computer Science</title>
      <link>https://arxiv.org/abs/2408.10262</link>
      <description>arXiv:2408.10262v2 Announce Type: replace 
Abstract: There has been some concern about the impact of predatory publishers on scientific research for some time. Recently, publishers that might previously have been considered `predatory' have established their bona fides, at least to the extent that they are included in citation impact scores such as the field-weighted citation impact (FWCI). These are sometimes called `grey' publishers (MDPI, Frontiers, Hindawi). In this paper, we show that the citation landscape for these grey publications is significantly different from the mainstream landscape and that affording publications in these venues the same status as publications in mainstream journals may significantly distort metrics such as the FWCI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10262v2</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Padraig Cunningham, Barry Smyth</dc:creator>
    </item>
    <item>
      <title>Past, Present, and Future of Citation Practices in HCI</title>
      <link>https://arxiv.org/abs/2405.16526</link>
      <description>arXiv:2405.16526v5 Announce Type: replace-cross 
Abstract: Science is a complex system comprised of many scientists who individually make decisions that, due to the size and nature of the academic system, largely do not affect the system as a whole. However, certain decisions at the meso-level of research communities, such as the Human-Computer Interaction (HCI) community, may result in deep and long-lasting behavioral changes in scientists. In this article, we provide empirical evidence on how a change in editorial policies introduced at the ACM CHI Conference in 2016 destabilized the CHI research community and launched it on an expansive path, denoted by a year-by-year increase in the mean number of references included in CHI articles. If this near-linear trend continues undisrupted, an article at CHI 2030 will include on average almost 130 references. The trend towards more citations reflects a citation culture where quantity is prioritized over quality, contributing to both author and peer reviewer fatigue. Our exploratory analysis underscores the profound impact of meso-level policy adjustments on the evolution of scientific fields and disciplines, urging all stakeholders to carefully consider the broader implications of such changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16526v5</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender</dc:creator>
    </item>
  </channel>
</rss>
