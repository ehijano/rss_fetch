<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Mar 2024 05:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Demolition and Reinforcement of Memories in Spin-Glass-like Neural Networks</title>
      <link>https://arxiv.org/abs/2403.02537</link>
      <description>arXiv:2403.02537v1 Announce Type: new 
Abstract: Statistical mechanics has made significant contributions to the study of biological neural systems by modeling them as recurrent networks of interconnected units with adjustable interactions. Several algorithms have been proposed to optimize the neural connections to enable network tasks such as information storage (i.e. associative memory) and learning probability distributions from data (i.e. generative modeling). Among these methods, the Unlearning algorithm, aligned with emerging theories of synaptic plasticity, was introduced by John Hopfield and collaborators. The primary objective of this thesis is to understand the effectiveness of Unlearning in both associative memory models and generative models. Initially, we demonstrate that the Unlearning algorithm can be simplified to a linear perceptron model which learns from noisy examples featuring specific internal correlations. The selection of structured training data enables an associative memory model to retrieve concepts as attractors of a neural dynamics with considerable basins of attraction. Subsequently, a novel regularization technique for Boltzmann Machines is presented, proving to outperform previously developed methods in learning hidden probability distributions from data-sets. The Unlearning rule is derived from this new regularized algorithm and is showed to be comparable, in terms of inferential performance, to traditional Boltzmann-Machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02537v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Enrico Ventura</dc:creator>
    </item>
    <item>
      <title>Geometric Dynamics of Signal Propagation Predict Trainability of Transformers</title>
      <link>https://arxiv.org/abs/2403.02579</link>
      <description>arXiv:2403.02579v1 Announce Type: new 
Abstract: We investigate forward signal propagation and gradient back propagation in deep, randomly initialized transformers, yielding simple necessary and sufficient conditions on initialization hyperparameters that ensure trainability of deep transformers. Our approach treats the evolution of the representations of $n$ tokens as they propagate through the transformer layers in terms of a discrete time dynamical system of $n$ interacting particles. We derive simple update equations for the evolving geometry of this particle system, starting from a permutation symmetric simplex. Our update equations show that without MLP layers, this system will collapse to a line, consistent with prior work on rank collapse in transformers. However, unlike prior work, our evolution equations can quantitatively track particle geometry in the additional presence of nonlinear MLP layers, and it reveals an order-chaos phase transition as a function of initialization hyperparameters, like the strength of attentional and MLP residual connections and weight variances. In the ordered phase the particles are attractive and collapse to a line, while in the chaotic phase the particles are repulsive and converge to a regular $n$-simplex. We analytically derive two Lyapunov exponents: an angle exponent that governs departures from the edge of chaos in this particle system, and a gradient exponent that governs the rate of exponential growth or decay of backpropagated gradients. We show through experiments that, remarkably, the final test loss at the end of training is well predicted just by these two exponents at the beginning of training, and that the simultaneous vanishing of these two exponents yields a simple necessary and sufficient condition to achieve minimal test loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02579v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aditya Cowsik, Tamra Nebabu, Xiao-Liang Qi, Surya Ganguli</dc:creator>
    </item>
    <item>
      <title>From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima</title>
      <link>https://arxiv.org/abs/2403.02418</link>
      <description>arXiv:2403.02418v1 Announce Type: cross 
Abstract: We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes. We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end. Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze. The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian. Through both theoretical analysis and numerical experiments, we show that in practical cases, i.e. for finite but even very large $N$, successful optimization via gradient descent in phase retrieval is achieved by falling towards the good minima before reaching the bad ones. This mechanism explains why successful recovery is obtained well before the algorithmic transition corresponding to the high-dimensional limit. Technically, this is associated to strong logarithmic corrections of the algorithmic transition at large $N$ with respect to the one expected in the $N\to\infty$ limit. Our analysis sheds light on such a new mechanism that facilitate gradient descent dynamics in finite large dimensions, also highlighting the importance of good initialization of spectral properties for optimization in complex high-dimensional landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02418v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Bonnaire, Giulio Biroli, Chiara Cammarota</dc:creator>
    </item>
    <item>
      <title>Existence of de Almeida-Thouless-type instability in the transverse field Sherrington-Kirkpatrick model</title>
      <link>https://arxiv.org/abs/2403.02699</link>
      <description>arXiv:2403.02699v1 Announce Type: cross 
Abstract: The interpolation method for mean field spin glass models developed by Guerra and Talagrand is extended to a quantum mean field spin glass model. This extension enables us to obtain both replica-symmetric (RS) and one step replica-symmetry breaking (1RSB) solutions of the free energy density in the transverse field Sherrington-Kirkpatrick model. It is shown that the RS solution is exact in the paramagnetic phase. We provide a sufficient condition on coupling constants where the 1RSB solution gives better bound than the RS one. This condition reduced to physical quantities in disordered single spin systems allows a simple computer-assisted proof for the existence of the de Almeida-Thouless-type instability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02699v1</guid>
      <category>math-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>math.MP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. Itoi, K. Fujiwara, Y. Sakamoto</dc:creator>
    </item>
    <item>
      <title>Operator Learning Renormalization Group</title>
      <link>https://arxiv.org/abs/2403.03199</link>
      <description>arXiv:2403.03199v1 Announce Type: cross 
Abstract: In this paper, we present a general framework for quantum many-body simulations called the operator learning renormalization group (OLRG). Inspired by machine learning perspectives, OLRG is a generalization of Wilson's numerical renormalization group and White's density matrix renormalization group, which recursively builds a simulatable system to approximate a target system of the same number of sites via operator maps. OLRG uses a loss function to minimize the error of a target property directly by learning the operator map in lieu of a state ansatz. This loss function is designed by a scaling consistency condition that also provides a provable bound for real-time evolution. We implement two versions of the operator maps for classical and quantum simulations. The former, which we call the Operator Matrix Map, can be implemented via neural networks on classical computers. The latter, which we call the Hamiltonian Expression Map, generates device pulse sequences to leverage the capabilities of quantum computing hardware. We illustrate the performance of both maps for calculating time-dependent quantities in the quantum Ising model Hamiltonian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03199v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiu-Zhe Luo, Di Luo, Roger G. Melko</dc:creator>
    </item>
    <item>
      <title>Out of Time Order Correlation of the Hubbard Model with Random Local Disorder</title>
      <link>https://arxiv.org/abs/2403.03214</link>
      <description>arXiv:2403.03214v1 Announce Type: cross 
Abstract: The out of time order correlator (OTOC) serves as a powerful tool for investigating quantum information spreading and chaos in complex systems. We present a method employing non-equilibrium dynamical mean-field theory (DMFT) and coherent potential approximation (CPA) combined with diagrammatic perturbation on the Schwinger-Keldysh contour to calculate the OTOC for correlated fermionic systems subjected to both random disorder and electrons interaction. Our key finding is that random disorder enhances the OTOC decay in the Hubbard model for the metallic phase in the weak coupling limit. However, the current limitation of our perturbative solver restricts the applicability to weak interaction regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03214v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chakradhar Rangi, Juana Moreno, Ka-Ming Tam</dc:creator>
    </item>
    <item>
      <title>Finite-size subthermal regime in disordered SU(N)-symmetric Heisenberg chains</title>
      <link>https://arxiv.org/abs/2304.03099</link>
      <description>arXiv:2304.03099v2 Announce Type: replace-cross 
Abstract: SU(N) symmetry is incompatible with the many-body localized (MBL) phase, even when strong disorder is present. However, recent studies have shown that finite-size SU(2) systems exhibit non-ergodic, subthermal behavior, characterized by the breakdown of the eigenstate thermalization hypothesis, and by the excited eigenstates entanglement entropy that is intermediate between area and volume law. In this work, we extend previous studies of the SU(2)-symmetric disordered Heisenberg model to larger systems, using the time-dependent density matrix renormalization group (tDMRG) method. We simulate quench dynamics from weakly entangled initial states up to long times, finding robust subthermal behavior at stronger disorder. Although we find an increased tendency towards thermalization at larger system sizes, the subthermal regime persists at intermediate time scales, nevertheless, and therefore should be accessible experimentally. At weaker disorder, we observe signatures of thermalization, however, entanglement entropy exhibits slow sublinear growth, in contrast to conventional thermalizing systems. Furthermore, we study dynamics of the SU(3)-symmetric disordered Heisenberg model. Similarly, strong disorder drives the system into subthermal regime, albeit thermalizing phase is broader compared to the SU(2) case. Our findings demonstrate the robustness of the subthermal regime in spin chains with non-Abelian continuous symmetry, and are consistent with eventual thermalization at large system sizes and long time scales, suggested by previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03099v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.109.094201</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 109, 094201 (2024)</arxiv:journal_reference>
      <dc:creator>Dimitris Saraidaris, Jheng-Wei Li, Andreas Weichselbaum, Jan von Delft, Dmitry A. Abanin</dc:creator>
    </item>
    <item>
      <title>To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets</title>
      <link>https://arxiv.org/abs/2310.13061</link>
      <description>arXiv:2310.13061v2 Announce Type: replace-cross 
Abstract: Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider multi-layer perceptron (MLP) and Transformer architectures trained on modular arithmetic tasks, where ($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve $100\%$ accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (``mechanistically'') interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes \emph{grokking} dynamics reaching high train \emph{and} test accuracy; second, it unlearns the memorizing representations, where the train accuracy suddenly jumps from $100\%$ to $100 (1-\xi)\%$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13061v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darshil Doshi, Aritra Das, Tianyu He, Andrey Gromov</dc:creator>
    </item>
    <item>
      <title>Inferring Structure of Cortical Neuronal Networks from Firing Data: A Statistical Physics Approach</title>
      <link>https://arxiv.org/abs/2402.18788</link>
      <description>arXiv:2402.18788v2 Announce Type: replace-cross 
Abstract: Understanding the relation between cortical neuronal network structure and neuronal activity is a fundamental unresolved question in neuroscience, with implications to our understanding of the mechanism by which neuronal networks evolve over time, spontaneously or under stimulation. It requires a method for inferring the structure and composition of a network from neuronal activities. Tracking the evolution of networks and their changing functionality will provide invaluable insight into the occurrence of plasticity and the underlying learning process. We devise a probabilistic method for inferring the effective network structure by integrating techniques from Bayesian statistics, statistical physics and principled machine learning. The method and resulting algorithm allow one to infer the effective network structure, identify the excitatory and inhibitory nature of its constituents, and predict neuronal spiking activities by employing the inferred structure. We validate the method and algorithm's performance using synthetic data, spontaneous activity of an in silico emulator and realistic in vitro neuronal networks of modular and homogeneous connectivity, demonstrating excellent structure inference and activity prediction. We also show that our method outperforms commonly used existing methods for inferring neuronal network structure. Inferring the evolving effective structure of neuronal networks will provide new insight into the learning process due to stimulation in general and will facilitate the development of neuron-based circuits with computing capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18788v2</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ho Fai Po, Akke Mats Houben, Anna-Christina Haeb, David Rhys Jenkins, Eric J. Hill, H. Rheinallt Parri, Jordi Soriano, David Saad</dc:creator>
    </item>
  </channel>
</rss>
