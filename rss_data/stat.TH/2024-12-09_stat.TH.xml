<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Dec 2024 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sequential anomaly identification with observation control under generalized error metrics</title>
      <link>https://arxiv.org/abs/2412.04693</link>
      <description>arXiv:2412.04693v1 Announce Type: new 
Abstract: The problem of sequential anomaly detection and identification is considered, where multiple data sources are simultaneously monitored and the goal is to identify in real time those, if any, that exhibit ``anomalous" statistical behavior. An upper bound is postulated on the number of data sources that can be sampled at each sampling instant, but the decision maker selects which ones to sample based on the already collected data. Thus, in this context, a policy consists not only of a stopping rule and a decision rule that determine when sampling should be terminated and which sources to identify as anomalous upon stopping, but also of a sampling rule that determines which sources to sample at each time instant subject to the sampling constraint. Two distinct formulations are considered, which require control of different, ``generalized" error metrics. The first one tolerates a certain user-specified number of errors, of any kind, whereas the second tolerates distinct, user-specified numbers of false positives and false negatives. For each of them, a universal asymptotic lower bound on the expected time for stopping is established as the error probabilities go to 0, and it is shown to be attained by a policy that combines the stopping and decision rules proposed in the full-sampling case with a probabilistic sampling rule that achieves a specific long-run sampling frequency for each source. Moreover, the optimal to a first order asymptotic approximation expected time for stopping is compared in simulation studies with the corresponding factor in a finite regime, and the impact of the sampling constraint and tolerance to errors is assessed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04693v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aristomenis Tsopelakos, Georgios Fellouris</dc:creator>
    </item>
    <item>
      <title>Constructing optimal treatment length strategies to maximize quality-adjusted lifetimes</title>
      <link>https://arxiv.org/abs/2412.05108</link>
      <description>arXiv:2412.05108v1 Announce Type: new 
Abstract: Real-world clinical decision making is a complex process that involves balancing the risks and benefits of treatments. Quality-adjusted lifetime is a composite outcome that combines patient quantity and quality of life, making it an attractive outcome in clinical research. We propose methods for constructing optimal treatment length strategies to maximize this outcome. Existing methods for estimating optimal treatment strategies for survival outcomes cannot be applied to a quality-adjusted lifetime due to induced informative censoring. We propose a weighted estimating equation that adjusts for both confounding and informative censoring. We also propose a nonparametric estimator of the mean counterfactual quality-adjusted lifetime survival curve under a given treatment length strategy, where the weights are estimated using an undersmoothed sieve-based estimator. We show that the estimator is asymptotically linear and provide a data-dependent undersmoothing criterion. We apply our method to obtain the optimal time for percutaneous endoscopic gastrostomy insertion in patients with amyotrophic lateral sclerosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05108v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Sun, Ashkan Ertefaie, Luke Duttweiler, Brent A. Johnson</dc:creator>
    </item>
    <item>
      <title>On one dimensional weighted Poincare inequalities for Global Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2412.04918</link>
      <description>arXiv:2412.04918v1 Announce Type: cross 
Abstract: One-dimensional Poincare inequalities are used in Global Sensitivity Analysis (GSA) to provide derivative-based upper bounds and approximations of Sobol indices. We add new perspectives by investigating weighted Poincare inequalities. Our contributions are twofold. In a first part, we provide new theoretical results for weighted Poincare inequalities, guided by GSA needs. We revisit the construction of weights from monotonic functions, providing a new proof from a spectral point of view. In this approach, given a monotonic function g, the weight is built such that g is the first non-trivial eigenfunction of a convenient diffusion operator. This allows us to reconsider the linear standard, i.e. the weight associated to a linear g. In particular, we construct weights that guarantee the existence of an orthonormal basis of eigenfunctions, leading to approximation of Sobol indices with Parseval formulas. In a second part, we develop specific methods for GSA. We study the equality case of the upper bound of a total Sobol index, and link the sharpness of the inequality to the proximity of the main effect to the eigenfunction. This leads us to theoretically investigate the construction of data-driven weights from estimators of the main effects when they are monotonic, another extension of the linear standard. Finally, we illustrate the benefits of using weights on a GSA study of two toy models and a real flooding application, involving the Poincare constant and/or the whole eigenbasis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04918v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Heredia (IMT, CIMI), Ald\'eric Joulin (IMT, CIMI), Olivier Roustant (IMT, INSA Toulouse, RT-UQ, CIMI)</dc:creator>
    </item>
    <item>
      <title>Causal discovery with endogenous context variables</title>
      <link>https://arxiv.org/abs/2412.04981</link>
      <description>arXiv:2412.04981v1 Announce Type: cross 
Abstract: Causal systems often exhibit variations of the underlying causal mechanisms between the variables of the system. Often, these changes are driven by different environments or internal states in which the system operates, and we refer to context variables as those variables that indicate this change in causal mechanisms. An example are the causal relations in soil moisture-temperature interactions and their dependence on soil moisture regimes: Dry soil triggers a dependence of soil moisture on latent heat, while environments with wet soil do not feature such a feedback, making it a context-specific property. Crucially, a regime or context variable such as soil moisture need not be exogenous and can be influenced by the dynamical system variables - precipitation can make a dry soil wet - leading to joint systems with endogenous context variables. In this work we investigate the assumptions for constraint-based causal discovery of context-specific information in systems with endogenous context variables. We show that naive approaches such as learning different regime graphs on masked data, or pooling all data, can lead to uninformative results. We propose an adaptive constraint-based discovery algorithm and give a detailed discussion on the connection to structural causal models, including sufficiency assumptions, which allow to prove the soundness of our algorithm and to interpret the results causally. Numerical experiments demonstrate the performance of the proposed method over alternative baselines, but they also unveil current limitations of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04981v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wiebke G\"unther, Oana-Iuliana Popescu, Martin Rabel, Urmi Ninad, Andreas Gerhardus, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>Generating Rectifiable Measures through Neural Networks</title>
      <link>https://arxiv.org/abs/2412.05109</link>
      <description>arXiv:2412.05109v1 Announce Type: cross 
Abstract: We derive universal approximation results for the class of (countably) $m$-rectifiable measures. Specifically, we prove that $m$-rectifiable measures can be approximated as push-forwards of the one-dimensional Lebesgue measure on $[0,1]$ using ReLU neural networks with arbitrarily small approximation error in terms of Wasserstein distance. What is more, the weights in the networks under consideration are quantized and bounded and the number of ReLU neural networks required to achieve an approximation error of $\varepsilon$ is no larger than $2^{b(\varepsilon)}$ with $b(\varepsilon)=\mathcal{O}(\varepsilon^{-m}\log^2(\varepsilon))$. This result improves Lemma IX.4 in Perekrestenko et al. as it shows that the rate at which $b(\varepsilon)$ tends to infinity as $\varepsilon$ tends to zero equals the rectifiability parameter $m$, which can be much smaller than the ambient dimension. We extend this result to countably $m$-rectifiable measures and show that this rate still equals the rectifiability parameter $m$ provided that, among other technical assumptions, the measure decays exponentially on the individual components of the countably $m$-rectifiable support set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05109v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erwin Riegler, Alex B\"uhler, Yang Pan, Helmut B\"olcskei</dc:creator>
    </item>
    <item>
      <title>Algebraic Sparse Factor Analysis</title>
      <link>https://arxiv.org/abs/2312.14762</link>
      <description>arXiv:2312.14762v3 Announce Type: replace 
Abstract: Factor analysis is a statistical technique that explains correlations among observed random variables with the help of a smaller number of unobserved factors. In traditional full factor analysis, each observed variable is influenced by every factor. However, many applications exhibit interesting sparsity patterns, that is, each observed variable only depends on a subset of the factors. In this paper, we study such sparse factor analysis models from an algebro-geometric perspective. Under mild conditions on the sparsity pattern, we examine the dimension of the set of covariance matrices that corresponds to a given model. Moreover, we study algebraic relations among the covariances in sparse two-factor models. In particular, we identify cases in which a Gr\"obner basis for these relations can be derived via a 2-delightful term order and join of toric ideals of graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14762v3</guid>
      <category>math.ST</category>
      <category>math.AC</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathias Drton, Alexandros Grosdos, Irem Portakal, Nils Sturma</dc:creator>
    </item>
    <item>
      <title>On the Mathematical foundations of Diffusion Monte Carlo</title>
      <link>https://arxiv.org/abs/2402.04642</link>
      <description>arXiv:2402.04642v3 Announce Type: replace 
Abstract: The Diffusion Monte Carlo method with constant number of walkers, also called Stochastic Reconfiguration as well as Sequential Monte Carlo, is a widely used Monte Carlo methodology for computing the ground-state energy and wave function of quantum systems. In this study, we present the first mathematically rigorous analysis of this class of stochastic methods on non necessarily compact state spaces, including linear diffusions evolving in quadratic absorbing potentials, yielding what seems to be the first result of this type for this class of models. We present a novel and general mathematical framework with easily checked Lyapunov stability conditions that ensure the uniform-in-time convergence of Diffusion Monte Carlo estimates towards the top of the spectrum of Schr\"odinger operators. For transient free evolutions, we also present a divergence blow up of the estimates w.r.t. the time horizon even when the asymptotic fluctuation variances are uniformly bounded. We also illustrate the impact of these results in the context of generalized coupled quantum harmonic oscillators with non necessarily reversible nor stable diffusive particle and a quadratic energy absorbing well associated with a semi-definite positive matrix force.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04642v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Caffarel (GMO, LCPQ), Pierre del Moral (ASTRAL), Luc de Montella (ASTRAL)</dc:creator>
    </item>
    <item>
      <title>On the improved estimation of ordered parameters based on doubly type-II censored sample</title>
      <link>https://arxiv.org/abs/2411.06888</link>
      <description>arXiv:2411.06888v2 Announce Type: replace 
Abstract: A doubly type-II censored scheme is an important sampling scheme in the life testing experiment and reliability engineering. In the present commutation, we have considered estimating ordered scale parameters of two exponential distributions based on doubly type-II censored samples with respect to a general scale invariant loss function. We have obtained several estimators that improve upon the BAEE. We also propose a class of improved estimators. It is shown that the boundary estimator of this class is generalized Bayes. As an application, we have derived improved estimators with respect to three special loss functions, namely quadratic loss, entropy loss, and symmetric loss function. We have applied these results to special life-testing sampling schemes. Finally, we conducted a simulation study to compare the performance of the improved estimators. A real-life data analysis has been considered for implementation purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06888v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shrajal Bajpai, Lakshmi Kanta Patra</dc:creator>
    </item>
    <item>
      <title>Testing LRD in the spectral domain for functional time series in manifolds</title>
      <link>https://arxiv.org/abs/2411.07731</link>
      <description>arXiv:2411.07731v2 Announce Type: replace 
Abstract: A statistical hypothesis test for long range dependence (LRD) in manifold-supported functional time series is formulated in the spectral domain. The proposed test statistic is based on the weighted periodogram operator, assuming that the elements of the spectral density operator family are invariant with respect to the group of isometries of the manifold. A Central Limit Theorem is derived to obtain the asymptotic Gaussian distribution of the proposed test statistics operator under the null hypothesis. The rate of convergence to zero, in the Hilbert--Schmidt operator norm, of the bias of the integrated empirical second and fourth order cumulant spectral density operators is established under the alternative hypothesis. The consistency of the test is then derived, from the obtained consistency of the integrated weighted periodogram operator under LRD. Practical implementation of our testing approach is based on the random projection methodology. The frequency-varying Karhunen-Lo\'eve expansion of invariant Gaussian random spectral Hilbert-Schmidt kernels on manifolds is considered for generation of random directions in the implementation of this methodology. A simulation study illustrates the main results regarding asymptotic normality and consistency, and the empirical size and power properties of the proposed testing approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07731v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. D. Ruiz-Medina, R. M. Crujeiras</dc:creator>
    </item>
    <item>
      <title>Robust Sparse Recovery with Sparse Bernoulli matrices via Expanders</title>
      <link>https://arxiv.org/abs/2112.14148</link>
      <description>arXiv:2112.14148v3 Announce Type: replace-cross 
Abstract: Sparse binary matrices are of great interest in the field of sparse recovery, nonnegative compressed sensing, statistics in networks, and theoretical computer science. This class of matrices makes it possible to perform signal recovery with lower storage costs and faster decoding algorithms. In particular, Bernoulli$(p)$ matrices formed by independent identically distributed (i.i.d.) Bernoulli$(p)$ random variables are of practical relevance in the context of noise-blind recovery in nonnegative compressed sensing.
  In this work, we investigate the robust nullspace property of Bernoulli$(p)$ matrices. Previous results in the literature establish that such matrices can accurately recover $n$-dimensional $s$-sparse vectors with $m=O\left(\frac{s}{c(p)}\log\frac{en}{s}\right)$ measurements, where $c(p) \le p$ is a constant dependent only on the parameter $p$. These results suggest that in the sparse regime, as $p$ approaches zero, the (sparse) Bernoulli$(p)$ matrix requires significantly more measurements than the minimal necessary, as achieved by standard isotropic subgaussian designs. However, we show that this is not the case.
  Our main result characterizes, for a wide range of sparsity levels $s$, the smallest $p$ for which sparse recovery can be achieved with the minimal number of measurements. We also provide matching lower bounds to establish the optimality of our results and explore connections with the theory of invertibility of discrete random matrices and integer compressed sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.14148v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pedro Abdalla</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v5 Announce Type: replace-cross 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Memorization With Neural Nets: Going Beyond the Worst Case</title>
      <link>https://arxiv.org/abs/2310.00327</link>
      <description>arXiv:2310.00327v3 Announce Type: replace-cross 
Abstract: In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite data set with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of the number of samples and hence move beyond worst-case memorization capacity bounds. We verify our theoretical result with numerical experiments and additionally investigate the effectiveness of the algorithm on MNIST and CIFAR-10.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00327v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>J. Mach. Learn. Res. 25:347 (2024) 1-38</arxiv:journal_reference>
      <dc:creator>Sjoerd Dirksen, Patrick Finke, Martin Genzel</dc:creator>
    </item>
    <item>
      <title>Investigating Self-Supervised Image Denoising with Denaturation</title>
      <link>https://arxiv.org/abs/2405.01124</link>
      <description>arXiv:2405.01124v4 Announce Type: replace-cross 
Abstract: Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning. However, theoretical understanding of the performance of the approach that uses denatured data is lacking. To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments. Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels. We also conduct several experiments to investigate the performance of an extended algorithm in practice. The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results. These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01124v4</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hiroki Waida, Kimihiro Yamazaki, Atsushi Tokuhisa, Mutsuyo Wada, Yuichiro Wada</dc:creator>
    </item>
  </channel>
</rss>
