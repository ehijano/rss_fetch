<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2024 05:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Targeted Maximum Likelihood Estimation for Integral Projection Models in Population Ecology</title>
      <link>https://arxiv.org/abs/2411.08150</link>
      <description>arXiv:2411.08150v1 Announce Type: new 
Abstract: Integral projection models (IPMs) are widely used to study population growth and the dynamics of demographic structure (e.g. age and size distributions) within a population.These models use data on individuals' growth, survival, and reproduction to predict changes in the population from one time point to the next and use these in turn to ask about long-term growth rates, the sensitivity of that growth rate to environmental factors, and aspects of the long term population such as how much reproduction concentrates in a few individuals; these quantities are not directly measurable from data and must be inferred from the model. Building IPMs requires us to develop models for individual fates over the next time step -- Did they survive? How much did they grow or shrink? Did they Reproduce? -- conditional on their initial state as well as on environmental covariates in a manner that accounts for the unobservable quantities that are are ultimately interested in estimating.Targeted maximum likelihood estimation (TMLE) methods are particularly well-suited to a framework in which we are largely interested in the consequences of models. These build machine learning-based models that estimate the probability distribution of the data we observe and define a target of inference as a function of these. The initial estimate for the distribution is then modified by tilting in the direction of the efficient influence function to both de-bias the parameter estimate and provide more accurate inference. In this paper, we employ TMLE to develop robust and efficient estimators for properties derived from a fitted IPM. Mathematically, we derive the efficient influence function and formulate the paths for the least favorable sub-models. Empirically, we conduct extensive simulations using real data from both long term studies of Idaho steppe plant communities and experimental Rotifer populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08150v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunzhe Zhou, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>MSTest: An R-Package for Testing Markov Switching Models</title>
      <link>https://arxiv.org/abs/2411.08188</link>
      <description>arXiv:2411.08188v1 Announce Type: new 
Abstract: We present the R package MSTest, which implements hypothesis testing procedures to identify the number of regimes in Markov switching models. These models have wide-ranging applications in economics, finance, and numerous other fields. The MSTest package includes the Monte Carlo likelihood ratio test procedures proposed by Rodriguez-Rondon and Dufour (2024), the moment-based tests of Dufour and Luger (2017), the parameter stability tests of Carrasco, Hu, and Ploberger (2014), and the likelihood ratio test of Hansen (1992). Additionally, the package enables users to simulate and estimate univariate and multivariate Markov switching and hidden Markov processes, using the expectation-maximization (EM) algorithm or maximum likelihood estimation (MLE). We demonstrate the functionality of the MSTest package through both simulation experiments and an application to U.S. GNP growth data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08188v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Rodriguez-Rondon, Jean-Marie Dufour</dc:creator>
    </item>
    <item>
      <title>$K$-means clustering for sparsely observed longitudinal data</title>
      <link>https://arxiv.org/abs/2411.08256</link>
      <description>arXiv:2411.08256v1 Announce Type: new 
Abstract: In longitudinal data analysis, observation points of repeated measurements over time often vary among subjects except in well-designed experimental studies. Additionally, measurements for each subject are typically obtained at only a few time points. From such sparsely observed data, identifying underlying cluster structures can be challenging. This paper proposes a fast and simple clustering method that generalizes the classical $k$-means method to identify cluster centers in sparsely observed data. The proposed method employs the basis function expansion to model the cluster centers, providing an effective way to estimate cluster centers from fragmented data. We establish the statistical consistency of the proposed method, as with the classical $k$-means method. Through numerical experiments, we demonstrate that the proposed method performs competitively with, or even outperforms, existing clustering methods. Moreover, the proposed method offers significant gains in computational efficiency due to its simplicity. Applying the proposed method to real-world data illustrates its effectiveness in identifying cluster structures in sparsely observed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08256v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michio Yamamoto, Yoshikazu Terada</dc:creator>
    </item>
    <item>
      <title>Adaptive Shrinkage with a Nonparametric Bayesian Lasso</title>
      <link>https://arxiv.org/abs/2411.08262</link>
      <description>arXiv:2411.08262v1 Announce Type: new 
Abstract: Modern approaches to perform Bayesian variable selection rely mostly on the use of shrinkage priors. That said, an ideal shrinkage prior should be adaptive to different signal levels, ensuring that small effects are ruled out, while keeping relatively intact the important ones. With this task in mind, we develop the nonparametric Bayesian Lasso, an adaptive and flexible shrinkage prior for Bayesian regression and variable selection, particularly useful when the number of predictors is comparable or larger than the number of available data points. We build on spike-and-slab Lasso ideas and extend them by placing a Dirichlet Process prior on the shrinkage parameters. The result is a prior on the regression coefficients that can be seen as an infinite mixture of Double Exponential densities, all offering different amounts of regularization, ensuring a more adaptive and flexible shrinkage. We also develop an efficient Markov chain Monte Carlo algorithm for posterior inference. Through various simulation exercises and real-world data analyses, we demonstrate that our proposed method leads to a better recovery of the true regression coefficients, a better variable selection, and better out-of-sample predictions, highlighting the benefits of the nonparametric Bayesian Lasso over existing shrinkage priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08262v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago Marin, Bronwyn Loong, Anton H. Westveld</dc:creator>
    </item>
    <item>
      <title>Optimal individualized treatment regimes for survival data with competing risks</title>
      <link>https://arxiv.org/abs/2411.08315</link>
      <description>arXiv:2411.08315v1 Announce Type: new 
Abstract: Precision medicine leverages patient heterogeneity to estimate individualized treatment regimens, formalized, data-driven approaches designed to match patients with optimal treatments. In the presence of competing events, where multiple causes of failure can occur and one cause precludes others, it is crucial to assess the risk of the specific outcome of interest, such as one type of failure over another. This helps clinicians tailor interventions based on the factors driving that particular cause, leading to more precise treatment strategies. Currently, no precision medicine methods simultaneously account for both survival and competing risk endpoints. To address this gap, we develop a nonparametric individualized treatment regime estimator. Our two-phase method accounts for both overall survival from all events as well as the cumulative incidence of a main event of interest. Additionally, we introduce a multi-utility value function that incorporates both outcomes. We develop random survival and random cumulative incidence forests to construct individual survival and cumulative incidence curves. Simulation studies demonstrated that our proposed method performs well, which we applied to a cohort of peripheral artery disease patients at high risk for limb loss and mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08315v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina W. Zhou, Nikki L. B. Freeman, Katharine L. McGinigle, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Imputation-based randomization tests for randomized experiments with interference</title>
      <link>https://arxiv.org/abs/2411.08352</link>
      <description>arXiv:2411.08352v1 Announce Type: new 
Abstract: The presence of interference renders classic Fisher randomization tests infeasible due to nuisance unknowns. To address this issue, we propose imputing the nuisance unknowns and computing Fisher randomization p-values multiple times, then averaging them. We term this approach the imputation-based randomization test and provide theoretical results on its asymptotic validity. Our method leverages the merits of randomization and the flexibility of the Bayesian framework: for multiple imputations, we can either employ the empirical distribution of observed outcomes to achieve robustness against model mis-specification or utilize a parametric model to incorporate prior information. Simulation results demonstrate that our method effectively controls the type I error rate and significantly enhances the testing power compared to existing randomization tests for randomized experiments with interference. We apply our method to a two-round randomized experiment with multiple treatments and one-way interference, where existing randomization tests exhibit limited power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08352v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingxuan Han, Ke Zhu, Hanzhong Liu, Ke Deng</dc:creator>
    </item>
    <item>
      <title>Expected Information Gain Estimation via Density Approximations: Sample Allocation and Dimension Reduction</title>
      <link>https://arxiv.org/abs/2411.08390</link>
      <description>arXiv:2411.08390v1 Announce Type: new 
Abstract: Computing expected information gain (EIG) from prior to posterior (equivalently, mutual information between candidate observations and model parameters or other quantities of interest) is a fundamental challenge in Bayesian optimal experimental design. We formulate flexible transport-based schemes for EIG estimation in general nonlinear/non-Gaussian settings, compatible with both standard and implicit Bayesian models. These schemes are representative of two-stage methods for estimating or bounding EIG using marginal and conditional density estimates. In this setting, we analyze the optimal allocation of samples between training (density estimation) and approximation of the outer prior expectation. We show that with this optimal sample allocation, the MSE of the resulting EIG estimator converges more quickly than that of a standard nested Monte Carlo scheme. We then address the estimation of EIG in high dimensions, by deriving gradient-based upper bounds on the mutual information lost by projecting the parameters and/or observations to lower-dimensional subspaces. Minimizing these upper bounds yields projectors and hence low-dimensional EIG approximations that outperform approximations obtained via other linear dimension reduction schemes. Numerical experiments on a PDE-constrained Bayesian inverse problem also illustrate a favorable trade-off between dimension truncation and the modeling of non-Gaussianity, when estimating EIG from finite samples in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08390v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengyi Li, Ricardo Baptista, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v1 Announce Type: new 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, is a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent US Food \&amp; Drug Administration (FDA) and European Medical Agency (EMA) guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. In this paper, we show that an HOIF-motivated estimator for the treatment-specific mean has significantly improved statistical properties compared to popular adjusted estimators in practice when the number of baseline covariates $p$ is relatively large compared to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted estimator. Furthermore, we demonstrate that a novel debiased adjusted estimator proposed recently by Lu et al. is, in fact, another HOIF-motivated estimator under disguise. Finally, simulation studies are conducted to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for adaptive trial designs I: A methodological review</title>
      <link>https://arxiv.org/abs/2411.08495</link>
      <description>arXiv:2411.08495v1 Announce Type: new 
Abstract: Regulatory guidance notes the need for caution in the interpretation of confidence intervals (CIs) constructed during and after an adaptive clinical trial. Conventional CIs of the treatment effects are prone to undercoverage (as well as other undesirable properties) in many adaptive designs, because they do not take into account the potential and realised trial adaptations. This paper is the first in a two-part series that explores CIs for adaptive trials. It provides a comprehensive review of the methods to construct CIs for adaptive designs, while the second paper illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. We describe several classes of techniques for constructing CIs for adaptive clinical trials, before providing a systematic literature review of available methods, classified by the type of adaptive design. As part of this, we assess, through a proposed traffic light system, which of several desirable features of CIs (such as achieving nominal coverage and consistency with the hypothesis test decision) each of these methods holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08495v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Robertson, Thomas Burnett, Babak Choodari-Oskooei, Munya Dimairo, Michael Grayling, Philip Pallmann, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Evaluating Parameter Uncertainty in the Poisson Lognormal Model with Corrected Variational Estimators</title>
      <link>https://arxiv.org/abs/2411.08524</link>
      <description>arXiv:2411.08524v1 Announce Type: new 
Abstract: Count data analysis is essential across diverse fields, from ecology and accident analysis to single-cell RNA sequencing (scRNA-seq) and metagenomics. While log transformations are computationally efficient, model-based approaches such as the Poisson-Log-Normal (PLN) model provide robust statistical foundations and are more amenable to extensions. The PLN model, with its latent Gaussian structure, not only captures overdispersion but also enables correlation between variables and inclusion of covariates, making it suitable for multivariate count data analysis. Variational approximations are a golden standard to estimate parameters of complex latent variable models such as PLN, maximizing a surrogate likelihood. However, variational estimators lack theoretical statistical properties such as consistency and asymptotic normality. In this paper, we investigate the consistency and variance estimation of PLN parameters using M-estimation theory. We derive the Sandwich estimator, previously studied in Westling and McCormick (2019), specifically for the PLN model. We compare this approach to the variational Fisher Information method, demonstrating the Sandwich estimator's effectiveness in terms of coverage through simulation studies. Finally, we validate our method on a scRNA-seq dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08524v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bastien Batardi\`ere, Julien Chiquet, Mahendra Mariadassou</dc:creator>
    </item>
    <item>
      <title>A Conjecture on Group Decision Accuracy in Voter Networks through the Regularized Incomplete Beta Function</title>
      <link>https://arxiv.org/abs/2411.08625</link>
      <description>arXiv:2411.08625v1 Announce Type: new 
Abstract: This paper presents a conjecture on the regularized incomplete beta function in the context of majority decision systems modeled through a voter framework. We examine a network where voters interact, with some voters fixed in their decisions while others are free to change their states based on the influence of their neighbors. We demonstrate that as the number of free voters increases, the probability of selecting the correct majority outcome converges to $1-I_{0.5}(\alpha,\beta)$, where $I_{0.5}(\alpha,\beta)$ is the regularized incomplete beta function. The conjecture posits that when $\alpha &gt; \beta$, $1-I_{0.5}(\alpha,\beta) &gt; \alpha/(\alpha+\beta)$, meaning the group's decision accuracy exceeds that of an individual voter. We provide partial results, including a proof for integer values of $\alpha$ and $\beta$, and support the general case using a probability bound. This work extends Condorcet's Jury Theorem by incorporating voter dependence driven by network dynamics, showing that group decision accuracy can exceed individual accuracy under certain conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08625v1</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Braha, Marcus A. M. de Aguiar</dc:creator>
    </item>
    <item>
      <title>Statistical Operating Characteristics of Current Early Phase Dose Finding Designs with Toxicity and Efficacy in Oncology</title>
      <link>https://arxiv.org/abs/2411.08698</link>
      <description>arXiv:2411.08698v1 Announce Type: new 
Abstract: Traditional phase I dose finding cancer clinical trial designs aim to determine the maximum tolerated dose (MTD) of the investigational cytotoxic agent based on a single toxicity outcome, assuming a monotone dose-response relationship. However, this assumption might not always hold for newly emerging therapies such as immuno-oncology therapies and molecularly targeted therapies, making conventional dose finding trial designs based on toxicity no longer appropriate. To tackle this issue, numerous early phase dose finding clinical trial designs have been developed to identify the optimal biological dose (OBD), which takes both toxicity and efficacy outcomes into account. In this article, we review the current model-assisted dose finding designs, BOIN-ET, BOIN12, UBI, TEPI-2, PRINTE, STEIN, and uTPI to identify the OBD and compare their operating characteristics. Extensive simulation studies and a case study using a CAR T-cell therapy phase I trial have been conducted to compare the performance of the aforementioned designs under different possible dose-response relationship scenarios. The simulation results demonstrate that the performance of different designs varies depending on the particular dose-response relationship and the specific metric considered. Based on our simulation results and practical considerations, STEIN, PRINTE, and BOIN12 outperform the other designs from different perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08698v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10543406.2024.2424845</arxiv:DOI>
      <dc:creator>Hao Sun, Hsin-Yu Lin, Jieqi Tu, Revathi Ananthakrishnan, Eunhee Kim</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for adaptive trial designs II: Case study and practical guidance</title>
      <link>https://arxiv.org/abs/2411.08771</link>
      <description>arXiv:2411.08771v1 Announce Type: new 
Abstract: In adaptive clinical trials, the conventional confidence interval (CI) for a treatment effect is prone to undesirable properties such as undercoverage and potential inconsistency with the final hypothesis testing decision. Accordingly, as is stated in recent regulatory guidance on adaptive designs, there is the need for caution in the interpretation of CIs constructed during and after an adaptive clinical trial. However, it may be unclear which of the available CIs in the literature are preferable. This paper is the second in a two-part series that explores CIs for adaptive trials. Part I provided a methodological review of approaches to construct CIs for adaptive designs. In this paper (part II), we present an extended case study based around a two-stage group sequential trial, including a comprehensive simulation study of the proposed CIs for this setting. This facilitates an expanded description of considerations around what makes for an effective CI procedure following an adaptive trial. We show that the CIs can have notably different properties. Finally, we propose a set of guidelines for researchers around the choice of CIs and the reporting of CIs following an adaptive design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08771v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Robertson, Thomas Burnett, Babak Choodari-Oskooei, Munya Dimairo, Michael Grayling, Philip Pallmann, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Causal-DRF: Conditional Kernel Treatment Effect Estimation using Distributional Random Forest</title>
      <link>https://arxiv.org/abs/2411.08778</link>
      <description>arXiv:2411.08778v1 Announce Type: new 
Abstract: The conditional average treatment effect (CATE) is a commonly targeted statistical parameter for measuring the mean effect of a treatment conditional on covariates. However, the CATE will fail to capture effects of treatments beyond differences in conditional expectations. Inspired by causal forests for CATE estimation, we develop a forest-based method to estimate the conditional kernel treatment effect (CKTE), based on the recently introduced Distributional Random Forest (DRF) algorithm. Adapting the splitting criterion of DRF, we show how one forest fit can be used to obtain a consistent and asymptotically normal estimator of the CKTE, as well as an approximation of its sampling distribution. This allows to study the difference in distribution between control and treatment group and thus yields a more comprehensive understanding of the treatment effect. In particular, this enables the construction of a conditional kernel-based test for distributional effects with provably valid type-I error. We show the effectiveness of the proposed estimator in simulations and apply it to the 1991 Survey of Income and Program Participation (SIPP) pension data to study the effect of 401(k) eligibility on wealth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08778v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N\"af, Herbert Susmann</dc:creator>
    </item>
    <item>
      <title>Interaction Testing in Variation Analysis</title>
      <link>https://arxiv.org/abs/2411.08861</link>
      <description>arXiv:2411.08861v1 Announce Type: new 
Abstract: Relationships of cause and effect are of prime importance for explaining scientific phenomena. Often, rather than just understanding the effects of causes, researchers also wish to understand how a cause $X$ affects an outcome $Y$ mechanistically -- i.e., what are the causal pathways that are activated between $X$ and $Y$. For analyzing such questions, a range of methods has been developed over decades under the rubric of causal mediation analysis. Traditional mediation analysis focuses on decomposing the average treatment effect (ATE) into direct and indirect effects, and therefore focuses on the ATE as the central quantity. This corresponds to providing explanations for associations in the interventional regime, such as when the treatment $X$ is randomized. Commonly, however, it is of interest to explain associations in the observational regime, and not just in the interventional regime. In this paper, we introduce \text{variation analysis}, an extension of mediation analysis that focuses on the total variation (TV) measure between $X$ and $Y$, written as $\mathrm{E}[Y \mid X=x_1] - \mathrm{E}[Y \mid X=x_0]$. The TV measure encompasses both causal and confounded effects, as opposed to the ATE which only encompasses causal (direct and mediated) variations. In this way, the TV measure is suitable for providing explanations in the natural regime and answering questions such as ``why is $X$ associated with $Y$?''. Our focus is on decomposing the TV measure, in a way that explicitly includes direct, indirect, and confounded variations. Furthermore, we also decompose the TV measure to include interaction terms between these different pathways. Subsequently, interaction testing is introduced, involving hypothesis tests to determine if interaction terms are significantly different from zero. If interactions are not significant, more parsimonious decompositions of the TV measure can be used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08861v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drago Plecko</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty in the numerical integration of evolution equations based on Bayesian isotonic regression</title>
      <link>https://arxiv.org/abs/2411.08338</link>
      <description>arXiv:2411.08338v1 Announce Type: cross 
Abstract: This paper presents a new Bayesian framework for quantifying discretization errors in numerical solutions of ordinary differential equations. By modelling the errors as random variables, we impose a monotonicity constraint on the variances, referred to as discretization error variances. The key to our approach is the use of a shrinkage prior for the variances coupled with variable transformations. This methodology extends existing Bayesian isotonic regression techniques to tackle the challenge of estimating the variances of a normal distribution. An additional key feature is the use of a Gaussian mixture model for the $\log$-$\chi^2_1$ distribution, enabling the development of an efficient Gibbs sampling algorithm for the corresponding posterior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08338v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuto Miyatake, Kaoru Irie, Takeru Matsuda</dc:creator>
    </item>
    <item>
      <title>Data fusion methods for the heterogeneity of treatment effect and confounding function</title>
      <link>https://arxiv.org/abs/2007.12922</link>
      <description>arXiv:2007.12922v3 Announce Type: replace 
Abstract: The heterogeneity of treatment effect (HTE) lies at the heart of precision medicine. Randomized controlled trials are gold-standard for treatment effect estimation but are typically underpowered for heterogeneous effects. In contrast, large observational studies have high predictive power but are often confounded due to the lack of randomization of treatment. We show that an observational study, even subject to hidden confounding, may be used to empower trials in estimating the HTE using the notion of confounding function. The confounding function summarizes the impact of unmeasured confounders on the difference between the observed treatment effect and the causal treatment effect, given the observed covariates, which is unidentifiable based only on the observational study. Coupling the trial and observational study, we show that the HTE and confounding function are identifiable. We then derive the semiparametric efficient scores and the integrative estimators of the HTE and confounding function. We clarify the conditions under which the integrative estimator of the HTE is strictly more efficient than the trial estimator. Finally, we illustrate the integrative estimators via simulation and an application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.12922v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Yang, Siyi Liu, Donglin Zeng, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>Exact PPS Sampling with Bounded Sample Size</title>
      <link>https://arxiv.org/abs/2105.10809</link>
      <description>arXiv:2105.10809v2 Announce Type: replace 
Abstract: Probability proportional to size (PPS) sampling schemes with a target sample size aim to produce a sample comprising a specified number $n$ of items while ensuring that each item in the population appears in the sample with a probability proportional to its specified "weight" (also called its "size"). These two objectives, however, cannot always be achieved simultaneously. Existing PPS schemes prioritize control of the sample size, violating the PPS property if necessary. We provide a new PPS scheme that allows a different trade-off: our method enforces the PPS property at all times while ensuring that the sample size never exceeds the target value $n$. The sample size is exactly equal to $n$ if possible, and otherwise has maximal expected value and minimal variance. Thus we bound the sample size, thereby avoiding storage overflows and helping to control the time required for analytics over the sample, while allowing the user complete control over the sample contents. The method is both simple to implement and efficient, being a one-pass streaming algorithm with an amortized processing time of $O(1)$ per item.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.10809v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brian Hentschel, Peter J. Haas, Yuanyuan Tian</dc:creator>
    </item>
    <item>
      <title>Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk</title>
      <link>https://arxiv.org/abs/2208.07590</link>
      <description>arXiv:2208.07590v4 Announce Type: replace 
Abstract: Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07590v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1907</arxiv:DOI>
      <arxiv:journal_reference>Annals of Applied Statistics 18(4), 2818-2839 (2024)</arxiv:journal_reference>
      <dc:creator>Olivier C. Pasche, Sebastian Engelke</dc:creator>
    </item>
    <item>
      <title>Discovering the Network Granger Causality in Large Vector Autoregressive Models</title>
      <link>https://arxiv.org/abs/2303.15158</link>
      <description>arXiv:2303.15158v3 Announce Type: replace 
Abstract: This paper proposes novel inferential procedures for discovering the network Granger causality in high-dimensional vector autoregressive models. In particular, we mainly offer two multiple testing procedures designed to control the false discovery rate (FDR). The first procedure is based on the limiting normal distribution of the $t$-statistics with the debiased lasso estimator. The second procedure is its bootstrap version. We also provide a robustification of the first procedure against any cross-sectional dependence using asymptotic e-variables. Their theoretical properties, including FDR control and power guarantee, are investigated. The finite sample evidence suggests that both procedures can successfully control the FDR while maintaining high power. Finally, the proposed methods are applied to discovering the network Granger causality in a large number of macroeconomic variables and regional house prices in the UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15158v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoshimasa Uematsu, Takashi Yamagata</dc:creator>
    </item>
    <item>
      <title>Improving randomized controlled trial analysis via data-adaptive borrowing</title>
      <link>https://arxiv.org/abs/2306.16642</link>
      <description>arXiv:2306.16642v2 Announce Type: replace 
Abstract: In recent years, real-world external controls have grown in popularity as a tool to empower randomized placebo-controlled trials, particularly in rare diseases or cases where balanced randomization is unethical or impractical. However, as external controls are not always comparable to the trials, direct borrowing without scrutiny may heavily bias the treatment effect estimator. Our paper proposes a data-adaptive integrative framework capable of preventing unknown biases of the external controls. The adaptive nature is achieved by dynamically sorting out a comparable subset of the external controls via bias penalization. Our proposed method can simultaneously achieve (a) the semiparametric efficiency bound when the external controls are comparable and (b) selective borrowing that mitigates the impact of the existence of incomparable external controls. Furthermore, we establish statistical guarantees, including consistency, asymptotic distribution, and inference, providing type-I error control and good power. Extensive simulations and two real-data applications show that the proposed method leads to improved performance over the trial-only estimator across various bias-generating scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16642v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyin Gao, Shu Yang, Mingyang Shan, Wenyu Ye, Ilya Lipkovich, Douglas Faries</dc:creator>
    </item>
    <item>
      <title>Robust Quickest Change Detection in Non-Stationary Processes</title>
      <link>https://arxiv.org/abs/2310.09673</link>
      <description>arXiv:2310.09673v2 Announce Type: replace 
Abstract: Optimal algorithms are developed for robust detection of changes in non-stationary processes. These are processes in which the distribution of the data after change varies with time. The decision-maker does not have access to precise information on the post-change distribution. It is shown that if the post-change non-stationary family has a distribution that is least favorable in a well-defined sense, then the algorithms designed using the least favorable distributions are robust and optimal. Non-stationary processes are encountered in public health monitoring and space and military applications. The robust algorithms are applied to real and simulated data to show their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09673v2</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingze Hou, Yousef Oleyaeimotlagh, Rahul Mishra, Hoda Bidkhori, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Weighted Q-learning for optimal dynamic treatment regimes with nonignorable missing covariates</title>
      <link>https://arxiv.org/abs/2312.01735</link>
      <description>arXiv:2312.01735v4 Announce Type: replace 
Abstract: Dynamic treatment regimes (DTRs) formalize medical decision-making as a sequence of rules for different stages, mapping patient-level information to recommended treatments. In practice, estimating an optimal DTR using observational data from electronic medical record (EMR) databases can be complicated by nonignorable missing covariates resulting from informative monitoring of patients. Since complete case analysis can provide consistent estimation of outcome model parameters under the assumption of outcome-independent missingness, Q-learning is a natural approach to accommodating nonignorable missing covariates. However, the backward induction algorithm used in Q-learning can introduce challenges, as nonignorable missing covariates at later stages can result in nonignorable missing pseudo-outcomes at earlier stages, leading to suboptimal DTRs, even if the longitudinal outcome variables are fully observed. To address this unique missing data problem in DTR settings, we propose two weighted Q-learning approaches where inverse probability weights for missingness of the pseudo-outcomes are obtained through estimating equations with valid nonresponse instrumental variables or sensitivity analysis. The asymptotic properties of the weighted Q-learning estimators are derived, and the finite-sample performance of the proposed methods is evaluated and compared with alternative methods through extensive simulation studies. Using EMR data from the Medical Information Mart for Intensive Care database, we apply the proposed methods to investigate the optimal fluid strategy for sepsis patients in intensive care units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01735v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Sun, Li Su, Bo Fu</dc:creator>
    </item>
    <item>
      <title>Functional Gaussian Graphical Regression Models For Air Quality Data</title>
      <link>https://arxiv.org/abs/2401.10196</link>
      <description>arXiv:2401.10196v3 Announce Type: replace 
Abstract: Functional data describe a wide range of processes, such as growth curves and spectral absorption. In this study, we analyze air pollution data from the In-service Aircraft for a Global Observing System, focusing on the spatial interactions among chemicals in the atmosphere and their dependence on meteorological conditions. This requires functional regression, where both response and covariates are functional objects evolving over the troposphere. Evaluating both the functional relatedness between the response and covariates and the relatedness of a multivariate response function can be challenging.
  We propose a solution to these challenges by introducing a functional Gaussian graphical regression model, extending conditional Gaussian graphical models to partially separable functions. To estimate the model, we propose a doubly-penalized estimator. Additionally, we present a novel adaptation of Kullback-Leibler cross-validation tailored for graph estimators which accounts for precision and regression matrices when the population presents one or more sub-groups, named joint Kullback-Leibler cross-validation. Evaluation of model performance is done in terms of Kullback-Leibler divergence and graph recovery power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10196v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rita Fici, Gianluca Sottile, Luigi Augugliaro, Ernst-Jan Camiel Wit</dc:creator>
    </item>
    <item>
      <title>Causal machine learning methods and use of sample splitting in settings with high-dimensional confounding</title>
      <link>https://arxiv.org/abs/2405.15242</link>
      <description>arXiv:2405.15242v2 Announce Type: replace 
Abstract: Observational epidemiological studies commonly seek to estimate the causal effect of an exposure on an outcome. Adjustment for potential confounding bias in modern studies is challenging due to the presence of high-dimensional confounding, which occurs when there are many confounders relative to sample size or complex relationships between continuous confounders and exposure and outcome. Despite recent advances, limited evaluation, and guidance are available on the implementation of doubly robust methods, Augmented Inverse Probability Weighting (AIPW) and Targeted Maximum Likelihood Estimation (TMLE), with data-adaptive approaches and cross-fitting in realistic settings where high-dimensional confounding is present. Motivated by an early-life cohort study, we conducted an extensive simulation study to compare the relative performance of AIPW and TMLE using data-adaptive approaches in estimating the average causal effect (ACE). We evaluated the benefits of using cross-fitting with a varying number of folds, as well as the impact of using a reduced versus full (larger, more diverse) library in the Super Learner ensemble learning approach used for implementation. We found that AIPW and TMLE performed similarly in most cases for estimating the ACE, but TMLE was more stable. Cross-fitting improved the performance of both methods, but was more important for estimation of standard error and coverage than for point estimates, with the number of folds a less important consideration. Using a full Super Learner library was important to reduce bias and variance in complex scenarios typical of modern health research studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15242v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Ellul, John B. Carlin, Stijn Vansteelandt, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Closed-Form Power and Sample Size Calculations for Bayes Factors</title>
      <link>https://arxiv.org/abs/2406.19940</link>
      <description>arXiv:2406.19940v2 Announce Type: replace 
Abstract: Determining an appropriate sample size is a critical element of study design, and the method used to determine it should be consistent with the planned analysis. When the planned analysis involves Bayes factor hypothesis testing, the sample size is usually desired to ensure a sufficiently high probability of obtaining a Bayes factor indicating compelling evidence for a hypothesis, given that the hypothesis is true. In practice, Bayes factor sample size determination is typically performed using computationally intensive Monte Carlo simulation. Here, we summarize alternative approaches that enable sample size determination without simulation. We show how, under approximate normality assumptions, sample sizes can be determined numerically, and provide the R package bfpwr for this purpose. Additionally, we identify conditions under which sample sizes can even be determined in closed-form, resulting in novel, easy-to-use formulas that also help foster intuition, enable asymptotic analysis, and can also be used for hybrid Bayesian/likelihoodist design. Furthermore, we show how power and sample size can be computed without simulation for more complex analysis priors, such as Jeffreys-Zellner-Siow priors or non-local normal moment priors. Case studies from medicine and psychology illustrate how researchers can use our methods to design informative yet cost-efficient studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19940v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>A Causal Framework for Evaluating Drivers of Policy Effect Heterogeneity Using Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2408.16670</link>
      <description>arXiv:2408.16670v2 Announce Type: replace 
Abstract: Policymakers and researchers often seek to understand how a policy differentially affects a population and the pathways driving this heterogeneity. For example, when studying an excise tax on sweetened beverages, researchers might assess the roles of cross-border shopping, economic competition, and store-level price changes on beverage sales trends. However, traditional policy evaluation tools, like the difference-in-differences (DiD) approach, primarily target average effects of the observed intervention rather than the underlying drivers of effect heterogeneity. Traditional approaches to evaluate sources of heterogeneity traditionally lack a causal framework, making it difficult to determine whether observed outcome differences are truly driven by the proposed source of heterogeneity or by other confounding factors. In this paper, we present a framework for evaluating such policy drivers by representing questions of effect heterogeneity under hypothetical interventions and apply it to evaluate drivers of the Philadelphia sweetened beverage tax policy effects. Building on recent advancements in estimating causal effect curves under DiD designs, we provide tools to assess policy effect heterogeneity while addressing practical challenges including confounding and neighborhood dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16670v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gary Hettinger, Youjin Lee, Nandita Mitra</dc:creator>
    </item>
    <item>
      <title>Variable selection in convex nonparametric least squares via structured Lasso: An application to the Swedish electricity distribution networks</title>
      <link>https://arxiv.org/abs/2409.01911</link>
      <description>arXiv:2409.01911v2 Announce Type: replace 
Abstract: We study the problem of variable selection in convex nonparametric least squares (CNLS). Whereas the least absolute shrinkage and selection operator (Lasso) is a popular technique for least squares, its variable selection performance is unknown in CNLS problems. In this work, we investigate the performance of the Lasso estimator and find out it is usually unable to select variables efficiently. Exploiting the unique structure of the subgradients in CNLS, we develop a structured Lasso method by combining $\ell_1$-norm and $\ell_{\infty}$-norm. The relaxed version of the structured Lasso is proposed for achieving model sparsity and predictive performance simultaneously, where we can control the two effects--variable selection and model shrinkage--using separate tuning parameters. A Monte Carlo study is implemented to verify the finite sample performance of the proposed approaches. We also use real data from Swedish electricity distribution networks to illustrate the effects of the proposed variable selection techniques. The results from the simulation and application confirm that the proposed structured Lasso performs favorably, generally leading to sparser and more accurate predictive models, relative to the conventional Lasso methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01911v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao</dc:creator>
    </item>
    <item>
      <title>Clustered Factor Analysis for Multivariate Spatial Data</title>
      <link>https://arxiv.org/abs/2409.07018</link>
      <description>arXiv:2409.07018v2 Announce Type: replace 
Abstract: Factor analysis has been extensively used to reveal the dependence structures among multivariate variables, offering valuable insight in various fields. However, it cannot incorporate the spatial heterogeneity that is typically present in spatial data. To address this issue, we introduce an effective method specifically designed to discover the potential dependence structures in multivariate spatial data. Our approach assumes that spatial locations can be approximately divided into a finite number of clusters, with locations within the same cluster sharing similar dependence structures. By leveraging an iterative algorithm that combines spatial clustering with factor analysis, we simultaneously detect spatial clusters and estimate a unique factor model for each cluster. The proposed method is evaluated through comprehensive simulation studies, demonstrating its flexibility. In addition, we apply the proposed method to a dataset of railway station attributes in the Tokyo metropolitan area, highlighting its practical applicability and effectiveness in uncovering complex spatial dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07018v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanxiu Jin, Tomoya Wakayama, Renhe Jiang, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Agnostic Characterization of Interference in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2410.13142</link>
      <description>arXiv:2410.13142v3 Announce Type: replace 
Abstract: We give an approach for characterizing interference by lower bounding the number of units whose outcome depends on certain groups of treated individuals, such as depending on the treatment of others, or others who are at least a certain distance away. The approach is applicable to randomized experiments with binary-valued outcomes. Asymptotically conservative point estimates and one-sided confidence intervals may be constructed with no assumptions beyond the known randomization design, allowing the approach to be used when interference is poorly understood, or when an observed network might only be a crude proxy for the underlying social mechanisms. Point estimates are equal to Hajek-weighted comparisons of units with differing levels of treatment exposure. Empirically, we find that the size of our interval estimates is competitive with (and often smaller than) those of the EATE, an assumption-lean treatment effect, suggesting that the proposed estimands may be intrinsically easier to estimate than treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13142v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Choi</dc:creator>
    </item>
    <item>
      <title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!</title>
      <link>https://arxiv.org/abs/2206.04902</link>
      <description>arXiv:2206.04902v4 Announce Type: replace-cross 
Abstract: Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04902v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Gruber, Gregor Kastner</dc:creator>
    </item>
    <item>
      <title>Predictive Inference in Multi-environment Scenarios</title>
      <link>https://arxiv.org/abs/2403.16336</link>
      <description>arXiv:2403.16336v2 Announce Type: replace-cross 
Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, potentially hierarchical data-generating scenarios. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets. Our contributions also include extensions for settings with non-real-valued responses, a theory of consistency for predictive inference in these general problems, and insights on the limits of conditional coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16336v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John C. Duchi, Suyash Gupta, Kuanhao Jiang, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Strong Approximations for Empirical Processes Indexed by Lipschitz Functions</title>
      <link>https://arxiv.org/abs/2406.04191</link>
      <description>arXiv:2406.04191v2 Announce Type: replace-cross 
Abstract: This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by possibly Lipschitz functions, improving on previous results in the literature. In the setting considered by Rio (1994), and if the function class is Lipschitzian, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\max\{d,2\}}$, up to a $\operatorname{polylog}(n)$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the rate $n^{-1/2}\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml\'os et al., 1975). Second, a uniform Gaussian strong approximation is established for multiplicative separable empirical processes indexed by possibly Lipschitz functions, which addresses some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). Finally, two other uniform Gaussian strong approximation results are presented when the function class is a sequence of Haar basis based on quasi-uniform partitions. Applications to nonparametric density and regression estimation are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04191v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ruiqi Rae Yu</dc:creator>
    </item>
  </channel>
</rss>
