<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Nov 2024 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>RPCAcc: A High-Performance and Reconfigurable PCIe-attached RPC Accelerator</title>
      <link>https://arxiv.org/abs/2411.07632</link>
      <description>arXiv:2411.07632v1 Announce Type: new 
Abstract: The emerging microservice/serverless-based cloud programming paradigm and the rising networking speeds leave the RPC stack as the predominant data center tax. Domain-specific hardware acceleration holds the potential to disentangle the overhead and save host CPU cycles. However, state-of-the-art RPC accelerators integrate RPC logic into the CPU or use specialized low-latency interconnects, hardly adopted in commodity servers.
  To this end, we design and implement RPCAcc, a software-hardware co-designed RPC on-NIC accelerator that enables reconfigurable RPC kernel offloading. RPCAcc connects to the server through the most widely used PCIe interconnect.
  To grapple with the ramifications of PCIe-induced challenges, RPCAcc introduces three techniques:(a) a target-aware deserializer that effectively batches cross-PCIe writes on the accelerator's on-chip memory using compacted hardware data structures; (b) a memory-affinity CPU-accelerator collaborative serializer, which trades additional host memory copies for slow cross-PCIe transfers; (c) an automatic field update technique that transparently codifies the schema based on dynamic reconfigure RPC kernels to minimize superfluous PCIe traversals. We prototype RPCAcc using the Xilinx U280 FPGA card. On HyperProtoBench, RPCAcc achieves 3.2X lower serialization time than a comparable RPC accelerator baseline and demonstrates up to 2.6X throughput improvement in the end-to-end cloud workload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07632v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Zhang, Hongjing Huang, Xuzheng Xu, Xiang Li, Ming Liu, Zeke Wang</dc:creator>
    </item>
    <item>
      <title>Web-Based Simulator of Superscalar RISC-V Processors</title>
      <link>https://arxiv.org/abs/2411.07721</link>
      <description>arXiv:2411.07721v1 Announce Type: new 
Abstract: Mastering computational architectures is essential for developing fast and power-efficient programs. Our advanced simulator empowers both IT students and professionals to grasp the fundamentals of superscalar RISC-V processors, HW/SW co-design and HPC optimization techniques. With customizable processor and memory architecture, full C compiler support, and detailed runtime statistics, this tool offers a comprehensive learning experience. Enjoy the convenience of a modern, web-based GUI to enhance your understanding and skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07721v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiri Jaros, Michal Majer, Jakub Horky, Jan Vavra</dc:creator>
    </item>
    <item>
      <title>MANTIS: A Mixed-Signal Near-Sensor Convolutional Imager SoC Using Charge-Domain 4b-Weighted 5-to-84-TOPS/W MAC Operations for Feature Extraction and Region-of-Interest Detection</title>
      <link>https://arxiv.org/abs/2411.07946</link>
      <description>arXiv:2411.07946v1 Announce Type: new 
Abstract: Recent advances in artificial intelligence have prompted the search for enhanced algorithms and hardware to support the deployment of machine learning at the edge. More specifically, in the context of the Internet of Things (IoT), vision chips must be able to fulfill tasks of low to medium complexity, such as feature extraction or region-of-interest (RoI) detection, with a sub-mW power budget imposed by the use of small batteries or energy harvesting. Mixed-signal vision chips relying on in- or near-sensor processing have emerged as an interesting candidate, thanks to their favorable tradeoff between energy efficiency (EE) and computational accuracy compared to digital systems for these specific tasks. In this paper, we introduce a mixed-signal convolutional imager system-on-chip (SoC) codenamed MANTIS, featuring a unique combination of large 16$\times$16 4b-weighted filters, operation at multiple scales, and double sampling, well suited to the requirements of medium-complexity tasks. The main contributions are (i) circuits called DS3 units combining delta-reset sampling, image downsampling, and voltage downshifting, and (ii) charge-domain multiply-and-accumulate (MAC) operations based on switched-capacitor amplifiers and charge sharing in the capacitive DAC of the successive-approximation ADCs. MANTIS achieves peak EEs normalized to 1b operations of 4.6 and 84.1 TOPS/W at the accelerator and SoC levels, while computing feature maps with a root mean square error ranging from 3 to 11.3$\%$. It also demonstrates a face RoI detection with a false negative rate of 11.5$\%$, while discarding 81.3$\%$ of image patches and reducing the data transmitted off chip by 13$\times$ compared to the raw image.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07946v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSSC.2024.3484766</arxiv:DOI>
      <dc:creator>Martin Lefebvre, David Bol</dc:creator>
    </item>
    <item>
      <title>Spiking Transformer Hardware Accelerators in 3D Integration</title>
      <link>https://arxiv.org/abs/2411.07397</link>
      <description>arXiv:2411.07397v1 Announce Type: cross 
Abstract: Spiking neural networks (SNNs) are powerful models of spatiotemporal computation and are well suited for deployment on resource-constrained edge devices and neuromorphic hardware due to their low power consumption. Leveraging attention mechanisms similar to those found in their artificial neural network counterparts, recently emerged spiking transformers have showcased promising performance and efficiency by capitalizing on the binary nature of spiking operations. Recognizing the current lack of dedicated hardware support for spiking transformers, this paper presents the first work on 3D spiking transformer hardware architecture and design methodology. We present an architecture and physical design co-optimization approach tailored specifically for spiking transformers. Through memory-on-logic and logic-on-logic stacking enabled by 3D integration, we demonstrate significant energy and delay improvements compared to conventional 2D CMOS integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07397v1</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boxun Xu, Junyoung Hwang, Pruek Vanna-iampikul, Sung Kyu Lim, Peng Li</dc:creator>
    </item>
    <item>
      <title>Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks</title>
      <link>https://arxiv.org/abs/2411.07902</link>
      <description>arXiv:2411.07902v1 Announce Type: cross 
Abstract: Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by generating an ensemble of predictive distributions. However, inference via ensembling is resource-intensive, requiring additional entropy sources to generate stochasticity which increases resource consumption. We introduce Bayes2IMC, an in-memory computing (IMC) architecture designed for binary Bayesian neural networks that leverage nanoscale device stochasticity to generate desired distributions. Our novel approach utilizes Phase-Change Memory (PCM) to harness inherent noise characteristics, enabling the creation of a binary neural network. This design eliminates the necessity for a pre-neuron Analog-to-Digital Converter (ADC), significantly improving power and area efficiency. We also develop a hardware-software co-optimized correction method applied solely on the logits in the final layer to reduce device-induced accuracy variations across deployments on hardware. Additionally, we devise a simple compensation technique that ensures no drop in classification accuracy despite conductance drift of PCM. We validate the effectiveness of our approach on the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy metrics comparable to ideal software implementations as well as results reported in the literature using other technologies. Finally, we present a complete core architecture and compare its projected power, performance, and area efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6 \times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6 \times$ improvement in power efficiency (in GOPS/W). In addition, the projected hardware performance of Bayes2IMC surpasses that of most of the BNN architectures based on memristive devices reported in the literature, and achieves up to $20\%$ higher power efficiency compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07902v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prabodh Katti, Clement Ruah, Osvaldo Simeone, Bashir M. Al-Hashimi, Bipin Rajendran</dc:creator>
    </item>
    <item>
      <title>A 1.1- / 0.9-nA Temperature-Independent 213- / 565-ppm/$^\circ$C Self-Biased CMOS-Only Current Reference in 65-nm Bulk and 22-nm FDSOI</title>
      <link>https://arxiv.org/abs/2302.04504</link>
      <description>arXiv:2302.04504v2 Announce Type: replace 
Abstract: In many applications, the ability of current references to cope with process, voltage, and temperature (PVT) variations is critical to maintaining system-level performance. However, temperature-independent current references operating in the nA range are rarely area-efficient due to the use of large resistors which occupy a significant silicon area at this current level. In this article, we introduce a nA-range constant-with-temperature (CWT) current reference relying on a self-cascode MOSFET (SCM), biased by a proportional-to-absolute-temperature (PTAT) voltage with a CWT offset. On the one hand, the proposed reference has been simulated post-layout in 65-nm bulk. This design consumes 5.4 nW at 0.7 V and achieves a 1.1-nA current with a line sensitivity (LS) of 0.69 %/V and a temperature coefficient (TC) of 213 ppm/$^\circ$C. On the other hand, the proposed reference has been simulated and fabricated in 22-nm fully depleted silicon-on-insulator (FDSOI). This second design requires additional features to mitigate the impact of parasitic diode leakage at high temperature. In measurement, it consumes 5.8 nW at 0.9 V and achieves a 0.9-nA current with a 0.39-%/V LS and a 565-ppm/$^\circ$C TC. As a result of using an SCM, the proposed references occupy a silicon area of 0.0021 mm$^2$ in 65 nm (respectively, 0.0132 mm$^2$ in 22 nm) at least 25$\times$ (respectively, 4$\times$) smaller than state-of-the-art CWT references operating in the same current range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04504v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSSC.2023.3240209</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal of Solid-State Circuits, vol. 58, no. 8, pp. 2239-2251, Aug. 2023</arxiv:journal_reference>
      <dc:creator>Martin Lefebvre, Denis Flandre, David Bol</dc:creator>
    </item>
    <item>
      <title>A 2.5-nA Area-Efficient Temperature-Independent 176-/82-ppm/{\deg}C CMOS-Only Current Reference in 0.11-$\mu$m Bulk and 22-nm FD-SOI</title>
      <link>https://arxiv.org/abs/2406.04741</link>
      <description>arXiv:2406.04741v2 Announce Type: replace 
Abstract: Internet-of-Things (IoT) applications require nW-power current references that are robust to process, voltage and temperature (PVT) variations, to maintain the performance of IoT sensor nodes in a wide range of operating conditions. However, nA-range current references are rarely area-efficient due to the use of large gate-leakage transistors or resistors, which occupy a significant silicon area at this current level. In this paper, we introduce a nA-range constant-with-temperature (CWT) current reference, relying on a self-cascode MOSFET (SCM) biased by a four-transistor ultra-low-power voltage reference through a single-transistor buffer. The proposed reference includes a temperature coefficient (TC) calibration mechanism to maintain performance across process corners. In addition, as the proposed design relies on the body effect, it has been fabricated and measured in 0.11-$\mu$m bulk and 22-nm fully-depleted silicon-on-insulator (FD-SOI) to demonstrate feasibility in both technology types. On the one hand, the 0.11-$\mu$m design consumes a power of 16.8 nW at 1.2 V and achieves a 2.3-nA current with a line sensitivity (LS) of 2.23 %/V at 25{\deg}C and a TC of 176 ppm/{\deg}C at 1.2 V from -40 to 85{\deg}C. On the other hand, the 22-nm design consumes a power of 16.3 nW at 1.5 V and achieves a 2.5-nA current with a 1.53-%/V LS at 25{\deg}C and an 82-ppm/{\deg}C TC at 1.5 V from -40 to 85{\deg}C. Thanks to their simple architecture, the proposed references achieve a silicon area of 0.0106 mm$^2$ in 0.11 $\mu$m and 0.0026 mm$^2$ in 22 nm without compromising other figures of merit, and are thus competitive with state-of-the-art CWT references operating in the same current range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04741v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSSC.2024.3402960</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal of Solid-State Circuits, vol. 59, no. 11, pp. 3752-3766, Nov. 2024</arxiv:journal_reference>
      <dc:creator>Martin Lefebvre, David Bol</dc:creator>
    </item>
    <item>
      <title>MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization</title>
      <link>https://arxiv.org/abs/2411.05282</link>
      <description>arXiv:2411.05282v2 Announce Type: replace 
Abstract: Quantization of foundational models (FMs) is significantly more challenging than traditional DNNs due to the emergence of large magnitude features called outliers. Existing outlier-aware algorithm/architecture co-design techniques either use mixed-precision, retaining outliers at high precision but compromise hardware efficiency, or quantize inliers and outliers at the same precision, improving hardware efficiency at the cost of accuracy. To address this mutual exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique that leverages pruning to complement outlier-aware quantization. MicroScopiQ retains outliers at higher precision while pruning a certain fraction of least important weights to distribute the additional outlier bits; ensuring high accuracy, aligned memory and hardware efficiency. We design a high-throughput, low overhead accelerator architecture composed of simple multi-precision INT processing elements and a novel network-on-chip called ReCoN that efficiently abstracts the complexity of supporting high-precision outliers. Additionally, unlike existing alternatives, MicroScopiQ does not assume any locality of outlier weights, enabling applicability to a broad range of FMs. Extensive experiments across various quantization settings show that MicroScopiQ achieves SoTA quantization performance while simultaneously improving inference performance by 3x and reducing energy by 2x over existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05282v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshat Ramachandran, Souvik Kundu, Tushar Krishna</dc:creator>
    </item>
    <item>
      <title>A Review of SRAM-based Compute-in-Memory Circuits</title>
      <link>https://arxiv.org/abs/2411.06079</link>
      <description>arXiv:2411.06079v2 Announce Type: replace 
Abstract: This paper presents a tutorial and review of SRAM-based Compute-in-Memory (CIM) circuits, with a focus on both Digital CIM (DCIM) and Analog CIM (ACIM) implementations. We explore the fundamental concepts, architectures, and operational principles of CIM technology. The review compares DCIM and ACIM approaches, examining their respective advantages and challenges. DCIM offers high computational precision and process scaling benefits, while ACIM provides superior power and area efficiency, particularly for medium-precision applications. We analyze various ACIM implementations, including current-based, time-based, and charge-based approaches, with a detailed look at charge-based ACIMs. The paper also discusses emerging hybrid CIM architectures that combine DCIM and ACIM to leverage the strengths of both approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06079v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 13 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Yoshioka, Shimpei Ando, Satomi Miyagi, Yung-Chin Chen, Wenlun Zhang</dc:creator>
    </item>
  </channel>
</rss>
