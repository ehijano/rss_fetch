<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Power Consumption Patterns Using Telemetry Data</title>
      <link>https://arxiv.org/abs/2602.22339</link>
      <description>arXiv:2602.22339v1 Announce Type: new 
Abstract: This paper examines the analysis of package power consumption using Intel's telemetry data. It challenges the prevailing belief that hardware choice is the primary determinant of a device's power consumption and instead emphasizes the significant role of user behavior. The paper includes two sections: Exploratory Data Analysis (EDA) and a linear model for power consumption. The EDA section provides valuable insights from Intel's telemetry data, comparing power consumption across countries, with a specific focus on power consumption patterns in the US and China. Our simple linear model affirms those patterns and highlight the possible importance of user behavior and its influence on power consumption. Ultimately, the paper underscores the need to understand power consumption patterns and identifies areas where stakeholders like Intel can make improvements to reduce environmental impact effectively and efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22339v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Cheon, Yuyang Pang, Zhiting Hu, Benjamin Smarr, Julien Sebot, Bijan Arbab, Ahmed Shams</dc:creator>
    </item>
    <item>
      <title>The Inference Bottleneck: Antitrust and Neutrality Duties in the Age of Cognitive Infrastructure</title>
      <link>https://arxiv.org/abs/2602.22750</link>
      <description>arXiv:2602.22750v1 Announce Type: new 
Abstract: As generative AI commercializes, competitive advantage is shifting from one-time model training toward continuous inference, distribution, and routing. At the frontier, large-scale inference can function as cognitive infrastructure: a bottleneck input that downstream applications rely on to compete, controlled by firms that often compete downstream through integrated assistants, productivity suites, and developer tooling. Foreclosure risk is not limited to price. It can be executed through non-price discrimination (latency, throughput, error rates, context limits, feature gating) and, where models select tools and services, through steering and default routing that is difficult to observe and harder to litigate. This essay makes three moves. First, it defines cognitive infrastructure as a falsifiable concept built around measurable reliance, vertical incentives, and discrimination capacity, without assuming a clean market definition. Second, it frames theories of harm using raising-rivals'-costs logic for vertically related and platform markets, where foreclosure can be profitable without anticompetitive pricing. Third, it proposes Neutral Inference: a targeted, auditable conduct approach built around (i) quality-of-service parity, (ii) routing transparency, and (iii) FRAND-style non-discrimination for similarly situated buyers, applied only when observable evidence indicates functional gatekeeper status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22750v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaston Besanson, Marcelo Celani</dc:creator>
    </item>
    <item>
      <title>Fairness in Limited Resources Settings</title>
      <link>https://arxiv.org/abs/2602.23026</link>
      <description>arXiv:2602.23026v1 Announce Type: new 
Abstract: In recent years many important societal decisions are made by machine-learning algorithms, and many such important decisions have strict capacity limits, allowing resources to be allocated only to the highest utility individuals. For example, allocating physician appointments to the patients most likely to have some medical condition, or choosing which children will attend a special program. When performing such decisions, we consider both the prediction aspect of the decision and the resource allocation aspect. In this work we focus on the fairness of the decisions in such settings. The fairness aspect here is critical as the resources are limited, and allocating the resources to one individual leaves less resources for others. When the decision involves prediction together with the resource allocation, there is a risk that information gaps between different populations will lead to a very unbalanced allocation of resources.
  We address settings by adapting definitions from resource allocation schemes, identifying connections between the algorithmic fairness definitions and resource allocation ones, and examining the trade-offs between fairness and utility. We analyze the price of enforcing the different fairness definitions compared to a strictly utility-based optimization of the predictor, and show that it can be unbounded. We introduce an adaptation of proportional fairness and show that it has a bounded price of fairness, indicating greater robustness, and propose a variant of equal opportunity that also has a bounded price of fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23026v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eitan Bachmat, Inbal Livni Navon</dc:creator>
    </item>
    <item>
      <title>Sorting Methods for Online Deliberation: Towards a Principled Approach</title>
      <link>https://arxiv.org/abs/2602.23168</link>
      <description>arXiv:2602.23168v1 Announce Type: new 
Abstract: Recent years have seen an increase in the use of online deliberation platforms (DPs). One of the main objectives of DPs is to enhance democratic participation, by allowing citizens to post, comment, and vote on policy proposals. But in what order should these proposals be listed? This paper makes a start with the principled evaluation of sorting methods on DPs. First, we introduce a conceptual framework that allows us to classify and compare sorting methods in terms of their purpose and the parameters they take into account. Second, we observe that the choice for a sorting method is often ad hoc and rarely justified. Third and last, we criticise sorting by number of approvals ('likes'), a method that is very common in practice. On the one hand, we show that if approvals are used for sorting, this should be done in an integrated way, also taking into account other parameters. On the other hand, we argue that even if proposals are on a par in terms of those other parameters, there are other, more appropriate ways to sort proposals in light of the approvals they have received.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23168v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolien Janssens, Frederik van de Putte</dc:creator>
    </item>
    <item>
      <title>Work Design and Multidimensional AI Threat as Predictors of Workplace AI Adoption and Depth of Use</title>
      <link>https://arxiv.org/abs/2602.23278</link>
      <description>arXiv:2602.23278v1 Announce Type: new 
Abstract: Artificial intelligence tools are increasingly embedded in everyday work, yet employees' uptake varies widely even within the same organization. Drawing on sociotechnical and work design perspectives, this research examines whether motivational job characteristics and multidimensional AI threat perceptions jointly predict workplace AI adoption and depth of use. Using cross-sectional survey data from 2,257 employees, we tested group differences across role level, years of experience, and region, along with multivariable predictors of AI adoption and use depth, specifically frequency and duration. Across models, job design, especially skill variety and autonomy, showed the most consistent positive associations with AI adoption, whereas threat dimensions exhibited differentiated patterns for depth of use. Perceived changes in work were positively associated with frequency and duration, while status threat showed a negative but not consistently significant relationship with deeper use. Findings are correlational given the cross-sectional and self-report design. Practical implications emphasize aligning AI enablement efforts with work design and monitoring potential workload expansion alongside adoption initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23278v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Reich, Diana Wolfe, Matt Price, Alice Choe, Fergus Kidd, Hannah Wagner</dc:creator>
    </item>
    <item>
      <title>Safety First: Psychological Safety as the Key to AI Transformation</title>
      <link>https://arxiv.org/abs/2602.23279</link>
      <description>arXiv:2602.23279v1 Announce Type: new 
Abstract: Organizations continue to invest in artificial intelligence, yet many struggle to ensure that employees adopt and engage with these tools. Drawing on research highlighting the interpersonal and learning demands of technology use, this study examines whether psychological safety is associated with AI adoption and usage in the workplace. Using survey data from 2,257 employees in a global consulting firm, we test whether psychological safety is associated with adoption, usage frequency, and usage duration; and whether these relationships vary by organizational level, professional experience, or geographic region. Logistic and linear regression analyses show that psychological safety reliably predicts whether employees adopt AI tools but does not predict how often or how long they use AI once adoption has occurred. Moreover, the relationship between psychological safety and AI adoption is consistent across experience levels, role levels, and regions, and no moderation effects emerge. These findings suggest that psychological safety functions as a key antecedent of initial AI engagement but not of subsequent usage intensity. The study underscores the need to distinguish between adoption and sustained use and highlights opportunities for targeted organizational interventions in early-stage AI implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23279v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Reich, Diana Wolfe, Matt Price, Alice Choe, Fergus Kidd, Hannah Wagner</dc:creator>
    </item>
    <item>
      <title>Misinformation Exposure in the Chinese Web: A Cross-System Evaluation of Search Engines, LLMs, and AI Overviews</title>
      <link>https://arxiv.org/abs/2602.22221</link>
      <description>arXiv:2602.22221v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into search services, providing direct answers that can reduce users' reliance on traditional result pages. Yet their factual reliability in non-English web ecosystems remains poorly understood, particularly when answering real user queries. We introduce a fact-checking dataset of 12~161 Chinese Yes/No questions derived from real-world online search logs and develop a unified evaluation pipeline to compare three information-access paradigms: traditional search engines, standalone LLMs, and AI-generated overview modules. Our analysis reveals substantial differences in factual accuracy and topic-level variability across systems. By combining this performance with real-world Baidu Index statistics, we further estimate potential exposure to incorrect factual information of Chinese users across regions. These findings highlight structural risks in AI-mediated search and underscore the need for more reliable and transparent information-access tools for the digital world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22221v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geng Liu, Junjie Mu, Li Feng, Mengxiao Zhu, Francesco Pierri</dc:creator>
    </item>
    <item>
      <title>Moral Preferences of LLMs Under Directed Contextual Influence</title>
      <link>https://arxiv.org/abs/2602.22831</link>
      <description>arXiv:2602.22831v1 Announce Type: cross 
Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22831v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phil Blandfort, Tushar Karayil, Urja Pawar, Robert Graham, Alex McKenzie, Dmitrii Krasheninnikov</dc:creator>
    </item>
    <item>
      <title>They Think AI Can Do More Than It Actually Can: Practices, Challenges, &amp; Opportunities of AI-Supported Reporting In Local Journalism</title>
      <link>https://arxiv.org/abs/2602.22887</link>
      <description>arXiv:2602.22887v1 Announce Type: cross 
Abstract: Declining newspaper revenues prompt local newsrooms to adopt automation to maintain efficiency and keep the community informed. However, current research provides a limited understanding of how local journalists work with digital data and which newsroom processes would benefit most from AI-supported (data) reporting. To bridge this gap, we conducted 21 semi-structured interviews with local journalists in Germany. Our study investigates how local journalists use data and AI (RQ1); the challenges they encounter when interacting with data and AI (RQ2); and the self-perceived opportunities of AI-supported reporting systems through the lens of discursive design (RQ3). Our findings reveal that local journalists do not fully leverage AI's potential to support data-related work. Despite local journalists' limited awareness of AI's capabilities, they are willing to use it to process data and discover stories. Finally, we provide recommendations for improving AI-supported reporting in the context of local news, grounded in the journalists' socio-technical perspective and their imagined AI future capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22887v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791130</arxiv:DOI>
      <dc:creator>Besjon Cifliku, Hendrik Heuer</dc:creator>
    </item>
    <item>
      <title>Certified Circuits: Stability Guarantees for Mechanistic Circuits</title>
      <link>https://arxiv.org/abs/2602.22968</link>
      <description>arXiv:2602.22968v1 Announce Type: cross 
Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22968v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alaa Anani, Tobias Lorenz, Bernt Schiele, Mario Fritz, Jonas Fischer</dc:creator>
    </item>
    <item>
      <title>Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive</title>
      <link>https://arxiv.org/abs/2602.23239</link>
      <description>arXiv:2602.23239v1 Announce Type: cross 
Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23239v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Radha Sarma</dc:creator>
    </item>
    <item>
      <title>Impacts of Aggregation on Model Diversity and Consumer Utility</title>
      <link>https://arxiv.org/abs/2602.23293</link>
      <description>arXiv:2602.23293v1 Announce Type: cross 
Abstract: Consider a marketplace of AI tools, each with slightly different strengths and weaknesses. By selecting the right model for the task at hand, a user can do better than simply committing to a single model for everything. Routers operate under a similar principle, where sophisticated model selection can increase overall performance. However, aggregation is often noisy, reflecting in imperfect user choices or routing decisions. This leads to two main questions: first, what does a "healthy marketplace" of models look like for maximizing consumer utility? Secondly, how can we incentivize producers to create such models? Here, we study two types of model changes: market entry (where an entirely new model is created and added to the set of available models), and model replacement (where an existing model has its strengths and weaknesses changed). We show that winrate, a standard benchmark in LLM evaluation, can incentivize model creators to homogenize for both types of model changes, reducing consumer welfare. We propose a new mechanism, weighted winrate, which rewards models for answers that are higher quality, and show that it provably improves incentives for producers to specialize and increases consumer welfare. We conclude by demonstrating that our theoretical results generalize to empirical benchmark datasets and discussing implications for evaluation design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23293v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kate Donahue, Manish Raghavan</dc:creator>
    </item>
    <item>
      <title>LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</title>
      <link>https://arxiv.org/abs/2602.23329</link>
      <description>arXiv:2602.23329v1 Announce Type: cross 
Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23329v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus, Jason Hausenloy, Pedro Medeiros, Nathaniel Li, Aiden Kim, Yury Orlovskiy, Coleman Breen, Bryce Cai, Jasper G\"otting, Andrew Bo Liu, Samira Nedungadi, Paula Rodriguez, Yannis Yiming He, Mohamed Shaaban, Zifan Wang, Seth Donoughe, Julian Michael</dc:creator>
    </item>
    <item>
      <title>Chameleon Channels: Measuring YouTube Accounts Repurposed for Deception and Profit</title>
      <link>https://arxiv.org/abs/2507.16045</link>
      <description>arXiv:2507.16045v2 Announce Type: replace 
Abstract: Online content creators spend significant time and effort building their user base through a long, often arduous process that requires finding the right "niche" to cater to. So, what incentive is there for an established content creator known for cat memes to completely reinvent their channel and start promoting cryptocurrency services or covering electoral news events?
  We explore this problem of repurposed channels, whereby a channel changes its identity and contents. We first characterize a market for "second-hand" social media accounts, which recorded sales exceeding USD 1M during our 6-month observation period. Observing YouTube channels (re)sold over these 6 months, we find that a substantial number (53%) are used to disseminate policy-sensitive content, often without facing any penalty. Surprisingly, these channels seem to gain rather than lose subscribers.
  We estimate the prevalence of repurposing using two snapshots of ~1.4M YouTube accounts sampled from an ecologically valid proxy. In a 3-month period, we estimate that ~0.25% channels were repurposed. We experimentally confirm that these repurposed channels share several characteristics with sold channels -- mainly, they have a significantly high presence of policy-sensitive content. Across repurposed channels, we find channels similar to those used in influence operations, as well as channels used for financial scams. Repurposed channels have large audiences; across two observed samples, repurposed channels held ~193M and ~44M subscribers. We reason that purchasing an existing audience and the credibility associated with an established account is advantageous to financially- and ideologically-motivated adversaries. This phenomenon is not exclusive to YouTube and we posit that the market for cultivating organic audiences is set to grow, particularly if it remains unchallenged by mitigations, technical or otherwise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16045v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>USENIX Security 2026</arxiv:journal_reference>
      <dc:creator>Alejandro Cuevas, Manoel Horta Ribeiro, Nicolas Christin</dc:creator>
    </item>
    <item>
      <title>BioBlue: Systematic runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format</title>
      <link>https://arxiv.org/abs/2509.02655</link>
      <description>arXiv:2509.02655v2 Announce Type: replace 
Abstract: Many AI alignment discussions of "runaway optimisation" focus on RL agents: unbounded utility maximisers that over-optimise a proxy objective (e.g., "paperclip maximiser", specification gaming) at the expense of everything else. LLM-based systems are often assumed to be safer because they function as next-token predictors rather than persistent optimisers. In this work, we empirically test this assumption by placing LLMs in simple, long-horizon control-style environments that require maintaining state of or balancing objectives over time: sustainability of a renewable resource, single- and multi-objective homeostasis, and balancing unbounded objectives with diminishing returns. We find that, although models frequently behave appropriately for many steps and clearly understand the stated objectives, they often lose context in structured ways and drift into runaway behaviours: ignoring homeostatic targets, collapsing from multi-objective trade-offs into single-objective maximisation - thus failing to respect concave utility structures. These failures emerge reliably after initial periods of competent behaviour and exhibit characteristic patterns (including self-imitative oscillations, unbounded maximisation, and reverting to single-objective optimisation). The problem is not that the LLMs just lose context or become incoherent - the failures systematically resemble runaway optimisers. Our results suggest that long-horizon, multi-objective misalignment is a genuine and under-evaluated failure mode in LLM agents, even in extremely simple settings with transparent and explicitly multi-objective feedback. Although LLMs appear multi-objective and bounded on the surface, their behaviour under sustained interaction, particularly involving multiple objectives, resembles brittle, poorly aligned optimisers whose effective objective gradually shifts toward unbounded and single-metric maximisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02655v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roland Pihlakas, Sruthi Susan Kuriakose</dc:creator>
    </item>
    <item>
      <title>The Market Consequences of Perceived Strategic Generosity: An Empirical Examination of NFT Charity Fundraisers</title>
      <link>https://arxiv.org/abs/2401.12064</link>
      <description>arXiv:2401.12064v3 Announce Type: replace-cross 
Abstract: Crypto donations now represent a significant fraction of charitable giving worldwide. Nonfungible token (NFT) charity fundraisers, which involve the sale of NFTs of artistic works with the proceeds donated to philanthropic causes, have emerged as a novel development in this space. A unique aspect of NFT charity fundraisers is the significant potential for donors to reap financial gains from the rising value of purchased NFTs. Questions may arise about donors' motivations in these charity fundraisers, potentially resulting in a negative social image. NFT charity fundraisers thus offer a unique opportunity to understand the economic consequences of a donor's social image. We investigate these effects in the context of a large NFT charity fundraiser. We identify the causal effect of purchasing an NFT within the charity fundraiser on a donor's later market outcomes by leveraging random variation in transaction processing times on the blockchain. Further, we demonstrate a clear pattern of heterogeneity based on an individual's decision to relist (versus hold) the purchased charity NFTs (a sign of perceived strategic generosity) and based on an individual's social exposure within the NFT marketplace. We show that charity-NFT 're-listers' experience significant penalties in the market regarding the prices they can command for their other NFTs, particularly among those who are more socially exposed. Finally, we report the results of a scenario-based online experiment, which again support our findings, highlighting that the re-listing a charity NFT for sale at a profit leads others to perceive their initial donation as strategic generosity and reduces those others' willingness to purchase NFTs from the donor. Our study underscores the growing importance of digital visibility and traceability, features that characterize crypto-philanthropy, and online philanthropy more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12064v3</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Liang, Murat Tunc, Gordon Burtch</dc:creator>
    </item>
    <item>
      <title>Simple contagion drives population-scale platform migration</title>
      <link>https://arxiv.org/abs/2505.24801</link>
      <description>arXiv:2505.24801v2 Announce Type: replace-cross 
Abstract: Social media platforms mediate professional communication, political expression, and community formation, making the rare instances when users collectively abandon an incumbent platform particularly consequential. Strong network effects raise switching costs and strengthen incumbents' positions, making coordinated exit difficult. Here we link 276,431 scholars on Twitter/X to their respective new profiles among the universe of all 16.7 million Bluesky accounts, tracked from January 2023 to December 2024, using a scalable, high-precision cross-platform matching pipeline. Exploiting exogenous variation from Brazil's court-ordered suspension of Twitter/X and a dynamic matching design, we show that adoption is peer-driven, treatment effects are short-lived and dose-dependent, and contagion is simple, not complex. Three patterns characterize adoption and retention. Adoption concentrates among users deeply embedded in Twitter's social graph. Public political expression predicts migration, consistent with homophilous inflows into a largely left-of-center Bluesky information space. Early reconnection with prior contacts predicts longer tenure and engagement. Our findings provide the first population-scale causal evidence of peer influence in a social media platform migration by exploiting exogenous exposure variation in a natural experiment and using daily dynamic matching. Rather than the complex contagion mechanism often emphasized in the literature, contagion is predominantly simple. Our findings recast migration as a multi-homing strategy that insures against governance uncertainty and show that users who quickly reconnect with prior contacts remain active longer on Bluesky.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24801v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dorian Quelle, Frederic Denker, Prashant Garg, Alexandre Bovet</dc:creator>
    </item>
    <item>
      <title>PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives</title>
      <link>https://arxiv.org/abs/2602.19463</link>
      <description>arXiv:2602.19463v2 Announce Type: replace-cross 
Abstract: As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19463v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790685</arxiv:DOI>
      <dc:creator>Emma Jiren Wang, Siying Hu, Zhicong Lu</dc:creator>
    </item>
  </channel>
</rss>
