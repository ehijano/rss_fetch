<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Data-driven Model Reduction for Parameter-Dependent Matrix Equations via Operator Inference</title>
      <link>https://arxiv.org/abs/2511.16033</link>
      <description>arXiv:2511.16033v1 Announce Type: new 
Abstract: This work develops a non-intrusive, data-driven surrogate modeling framework based on Operator Inference (OpInf) for rapidly solving parameter-dependent matrix equations in many-query settings. Motivated by the requirements of the OpInf methodology, we reformulate the matrix equations into a structured representation that explicitly shows the parameter dependence in polynomial form. This reformulation is crucial for efficient model reduction. This approach constructs reduced-order models via regression on solution snapshots, bypassing the need for expensive full-order operators and thus overcoming the primary bottlenecks of intrusive methods in high-dimensional contexts. Numerical experiments confirm their accuracy and computational efficiency, demonstrating that our work is a scalable and practical solution for parameter-dependent matrix equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16033v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuelian Wen, Qiuqi Li, Juan Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal error analysis of an interior penalty virtual element method for fourth-order singular perturbation problems</title>
      <link>https://arxiv.org/abs/2511.16070</link>
      <description>arXiv:2511.16070v1 Announce Type: new 
Abstract: In recent studies \cite{ZZ24, FY24}, the Interior Penalty Virtual Element Method (IPVEM) has been developed for solving a fourth-order singular perturbation problem, with uniform convergence established in the lowest-order case concerning the perturbation parameter. However, the resulting uniform convergence rate is only of half-order, which is suboptimal. In this work, we demonstrate that the proposed IPVEM in fact achieves optimal and uniform error estimates, even in the presence of boundary layers. The theoretical results are substantiated through extensive numerical experiments, which confirm the validity of the error estimates and highlight the method's effectiveness for singularly perturbed problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16070v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Feng, Yuanyi Sun, Yue Yu</dc:creator>
    </item>
    <item>
      <title>Shallow neural network yields regularization for ill-posed inverse problems</title>
      <link>https://arxiv.org/abs/2511.16171</link>
      <description>arXiv:2511.16171v1 Announce Type: new 
Abstract: In this paper, we establish universal approximation theorems for neural networks applied to general nonlinear ill-posed operator equations. In addition to the approximation error, the measurement error is also taken into account in our error estimation. We introduce the expanding neural network method as a novel iterative regularization scheme and prove its regularization properties under different a priori assumptions about the exact solutions. Within this framework, the number of neurons serves as both the regularization parameter and iteration number. We demonstrate that for data with high noise levels, a small network architecture is sufficient to obtain a stable solution, whereas a larger architecture may compromise stability due to overfitting. Furthermore, under standard assumptions in regularization theory, we derive convergence rate results for neural networks in the context of variational regularization. Several numerical examples are presented to illustrate the robustness of the proposed neural network-based algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16171v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lan Wang, Qiao Zhu, Bangti Jin, Ye Zhang</dc:creator>
    </item>
    <item>
      <title>Robust PAMPA Scheme in the DG Formulation on Unstructured Triangular Meshes: bound preservation, oscillation elimination, and boundary conditions</title>
      <link>https://arxiv.org/abs/2511.16180</link>
      <description>arXiv:2511.16180v1 Announce Type: new 
Abstract: We propose an improved version of the PAMPA algorithm where the solution is sought as globally continuous. The scheme is locally conservative, and there is no mass matrix to invert. This method had been developed in a series of papers, see e.g \cite{Abgrall2024a} and the references therein. In \cite{Abgrall2025d}, we had shown the connection between PAMPA and the discontinuous Galerkin method, for the linear hyperbolic problem. Taking advantage of this reinterpretation, we use it to define a family of methods, show how to implement the boundary conditions in a rigorous manner. In addition, we propose a method that complements the bound preserving method developed in \cite{Abgrall2025d} in the sense that it is non oscillatory. A truncation error analysis is provided, it shows that the scheme should be third order accurate for smooth solutions. This is confirmed by numerical experiments. Several numerical examples are presented to show that the scheme is indeed bound preserving and non oscillatory on a wide range on numerical benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16180v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Abgrall, Yongle Liu</dc:creator>
    </item>
    <item>
      <title>Numerical identification of the time-dependent coefficient in the heat equation with fractional Laplacian</title>
      <link>https://arxiv.org/abs/2511.16238</link>
      <description>arXiv:2511.16238v1 Announce Type: new 
Abstract: We address the inverse problem of identifying a time-dependent source coefficient in a one-dimensional heat equation with a fractional Laplacian subject to Dirichlet boundary conditions and an integral nonlocal data. An a priori estimate is established to ensure the uniqueness and stability of the solution. A fully implicit Crank-Nicolson (CN) finite-difference scheme is proposed and rigorously analysed for stability and convergence. An efficient noise-stable computation algorithm is developed and verified through numerical experiments, demonstrating accuracy and robustness under noisy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16238v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arshyn Altybay, Niyaz Tokmagambetov, Gulzat Nalzhupbayeva</dc:creator>
    </item>
    <item>
      <title>Human-aligned Quantification of Numerical Data</title>
      <link>https://arxiv.org/abs/2511.15723</link>
      <description>arXiv:2511.15723v1 Announce Type: cross 
Abstract: Quantifying numerical data involves addressing two key challenges: first, determining whether the data can be naturally quantified, and second, identifying the numerical intervals or ranges of values that correspond to specific value classes, referred to as "quantums," which represent statistically meaningful states. If such quantification is feasible, continuous streams of numerical data can be transformed into sequences of "symbols" that reflect the states of the system described by the measured parameter. People often perform this task intuitively, relying on common sense or practical experience, while information theory and computer science offer computable metrics for this purpose. In this study, we assess the applicability of metrics based on information compression and the Silhouette coefficient for quantifying numerical data. We also investigate the extent to which these metrics correlate with one another and with what is commonly referred to as "human intuition." Our findings suggest that the ability to classify numeric data values into distinct categories is associated with a Silhouette coefficient above 0.65 and a Dip Test below 0.5; otherwise, the data can be treated as following a unimodal normal distribution. Furthermore, when quantification is possible, the Silhouette coefficient appears to align more closely with human intuition than the "normalized centroid distance" method derived from information compression perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15723v1</guid>
      <category>physics.data-an</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Kolonin</dc:creator>
    </item>
    <item>
      <title>Micro-Macro Simulation of Shallow Water Moment Equations</title>
      <link>https://arxiv.org/abs/2511.15737</link>
      <description>arXiv:2511.15737v1 Announce Type: cross 
Abstract: Shallow flows are governed by the Navier-Stokes equations. They are commonly modelled using the shallow water equations, a great simplification of the Navier-Stokes equations, which often yields inaccurate results. For that reason, a model called shallow water moment equations has been developed. It uses more equations and variables than the shallow water equations. While this model is significantly more accurate, it is also computationally more expensive. To speed up computations, the micro-macro method may be used. The micro-macro method switches between two models of varying levels of detail allowing for larger stable time steps. In this paper we formulate the micro-macro method for shallow water moment equations. We perform a theoretical runtime analysis of the method and present a series of results for a dam break test and a wave transport test. The micro-macro method achieves a significant speed-up while retaining a sufficient level of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15737v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vil\'em Ro\v{z}ek</dc:creator>
    </item>
    <item>
      <title>Approximation rates of quantum neural networks for periodic functions via Jackson's inequality</title>
      <link>https://arxiv.org/abs/2511.16149</link>
      <description>arXiv:2511.16149v1 Announce Type: cross 
Abstract: Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16149v1</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Philipp Schmocker, Viet Khoa Tran</dc:creator>
    </item>
    <item>
      <title>Algorithms and optimizations for global non-linear hybrid fluid-kinetic finite element stellarator simulations</title>
      <link>https://arxiv.org/abs/2511.16412</link>
      <description>arXiv:2511.16412v1 Announce Type: cross 
Abstract: Predictive modeling of stellarator plasmas is crucial for advancing nuclear fusion energy, yet it faces unique computational difficulties. One of the main challenges is accurately simulating the dynamics of specific particle species that are not well captured by fluid models, which necessitates the use of hybrid fluid-kinetic models. The non-axisymmetric geometry of stellarators fundamentally couples the toroidal Fourier modes, in contrast to what happens in tokamaks, requiring different numerical and computational treatment.
  This work presents a novel, globally coupled projection scheme inside the JOREK finite element framework. The approach ensures a self-consistent and physically accurate transfer of kinetic markers to the fluid grid, effectively handling the complex 3D mesh by constructing and solving a unified linear system that encompasses all toroidal harmonics simultaneously. To manage the computational complexity of this coupling, the construction of the system's matrix is significantly accelerated using the Fast Fourier Transform (FFT). The efficient localization of millions of particles is made possible by implementing a 3D R-Tree spatial index, which supports this projection and ensures computational tractability at scale.
  On realistic Wendelstein 7-X stellarator geometries, the fidelity of the framework is rigorously shown. In sharp contrast to the uncoupled approaches' poor performance, quantitative convergence tests verify that the coupled scheme attains the theoretically anticipated spectral convergence.
  This study offers a crucial capability for the predictive analysis and optimization of next-generation stellarator designs by developing a validated, high-fidelity computational tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16412v1</guid>
      <category>physics.plasm-ph</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luca Venerando Greco</dc:creator>
    </item>
    <item>
      <title>A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation</title>
      <link>https://arxiv.org/abs/2511.16420</link>
      <description>arXiv:2511.16420v1 Announce Type: cross 
Abstract: The rapid growth of data centers increasingly requires data center operators to "bring own generation" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with minor accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16420v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaked Regev, Eve Tsybina, Slaven Peles</dc:creator>
    </item>
    <item>
      <title>Entrywise Approximate Solutions for SDDM Systems in Almost-Linear Time</title>
      <link>https://arxiv.org/abs/2511.16570</link>
      <description>arXiv:2511.16570v1 Announce Type: cross 
Abstract: We present an algorithm that given any invertible symmetric diagonally dominant M-matrix (SDDM), i.e., a principal submatrix of a graph Laplacian, $\boldsymbol{\mathit{L}}$ and a nonnegative vector $\boldsymbol{\mathit{b}}$, computes an entrywise approximation to the solution of $\boldsymbol{\mathit{L}} \boldsymbol{\mathit{x}} = \boldsymbol{\mathit{b}}$ in $\tilde{O}(m n^{o(1)})$ time with high probability, where $m$ is the number of nonzero entries and $n$ is the dimension of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16570v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angelo Farfan, Mehrdad Ghadiri, Junzhao Yang</dc:creator>
    </item>
    <item>
      <title>Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems</title>
      <link>https://arxiv.org/abs/2511.16657</link>
      <description>arXiv:2511.16657v1 Announce Type: cross 
Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16657v1</guid>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan C. King, Jose M. Amigo</dc:creator>
    </item>
    <item>
      <title>Artificial neural network evaluation of geometric constants for polygonal domains</title>
      <link>https://arxiv.org/abs/2206.10292</link>
      <description>arXiv:2206.10292v3 Announce Type: replace 
Abstract: We propose an approach based on Artificial Neural Networks (ANNs) to evaluate geometric constants relevant to the analysis and design of numerical schemes for partial differential equations. These constants play a central role, significantly influencing, for instance, a posteriori error estimates and the overall design of the computational strategy. Our technique leverages ANNs to learn the dependencies between these constants and a set of descriptive geometric features associated to polytopal mesh elements. The main computational costs are confined to data processing and training phases, which can be performed offline once and for all. This yields an effective tool for computing the constants, which we verify and show to be applicable across different scenarios, without substantial modifications - demonstrating its broader usability beyond the specific example considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.10292v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beatrice Crippa, Sofia Imperatore, Silvia Bertoluzza, Micol Pennacchio</dc:creator>
    </item>
    <item>
      <title>Fredholm Neural Networks</title>
      <link>https://arxiv.org/abs/2408.09484</link>
      <description>arXiv:2408.09484v3 Announce Type: replace 
Abstract: Within the family of explainable machine-learning, we present Fredholm neural networks (Fredholm NNs): deep neural networks (DNNs) architectures motivated by fixed- point iteration schemes for the solution of linear and nonlinear Fredholm integral equations (FIEs) of the second kind. We also show how the proposed framework can be used for the solution of inverse problems. Applications of FIEs include the solution of ordinary, as well as partial differential equations (ODEs, PDEs) and many more. We first prove that Fredholm NNs provide accurate solutions. We then provide insight into the values of the hyperparameters and trainable/explainable weights and biases of the DNN, by directly connecting their values to the underlying mathematical theory. For our illustrations, we use Fredholm NNs to solve both linear and nonlinear problems, including elliptic PDEs and boundary value problems. We show that the proposed scheme achieves significant numerical approximation accuracy across both the domain and boundary. The proposed methodology provides insight into the connection between neural networks and classical numerical methods, and we posit that it can have applications in fields such as Uncertainty Quantification (UQ) and explainable artificial intelligence (XAI). Thus, we believe that it will trigger further advances in the intersection between scientific machine learning and numerical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09484v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/24M1686991</arxiv:DOI>
      <arxiv:journal_reference>SIAM Journal on Scientific Computing, Vol. 47, Iss. 4, 2025</arxiv:journal_reference>
      <dc:creator>Kyriakos Georgiou, Constantinos Siettos, Athanasios N. Yannacopoulos</dc:creator>
    </item>
    <item>
      <title>Quaternion tensor low rank Quaternion tensor low-rank approximation using a family of non-convex norms</title>
      <link>https://arxiv.org/abs/2409.10724</link>
      <description>arXiv:2409.10724v4 Announce Type: replace 
Abstract: In this paper, we propose a new approaches for low rank approximation of quaternion tensors \cite{chen2019low,zhang1997quaternions,hamilton1866elements}. The first method uses quasi-norms to approximate the tensor by a low-rank tensor using the QT-product \cite{miao2023quaternion}, which generalizes the known L-product to N-mode quaternions. The second method involves Non-Convex norms to approximate the Tucker and TT-rank for the completion problem. We demonstrate that the proposed methods can effectively approximate the tensor compared to the convexifying of the rank, such as the nuclear norm. We provide theoretical results and numerical experiments to show the efficiency of the proposed methods in the Inpainting and Denoising applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10724v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11075-025-02226-2</arxiv:DOI>
      <dc:creator>Alaeddine Zahir, Ahmed Ratnani, Khalide Jbilou</dc:creator>
    </item>
    <item>
      <title>Multiscale Neural Networks for Approximating Green's Functions</title>
      <link>https://arxiv.org/abs/2410.18439</link>
      <description>arXiv:2410.18439v3 Announce Type: replace 
Abstract: Neural networks (NNs) have been widely used to solve partial differential equations (PDEs) in the applications of physics, biology, and engineering. One effective approach for solving PDEs with a fixed differential operator is learning Green's functions. However, Green's functions are notoriously difficult to learn due to their poor regularity, which typically requires larger NNs and longer training times. In this paper, we address these challenges by leveraging multiscale NNs to learn Green's functions. Through theoretical analysis using multiscale Barron space methods and experimental validation, we show that the multiscale approach significantly reduces the necessary NN size and accelerates training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18439v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenrui Hao, Rui Peng Li, Yuanzhe Xi, Tianshi Xu, Yahong Yang</dc:creator>
    </item>
    <item>
      <title>Convergence of the non-staggered Nessyahu-Tadmor scheme for coupled systems of one-dimensional nonlocal balance laws</title>
      <link>https://arxiv.org/abs/2501.14425</link>
      <description>arXiv:2501.14425v3 Announce Type: replace 
Abstract: We derive a second-order accurate, non-staggered central scheme based on the well-known Nessyahu-Tadmor scheme to approximate solutions of coupled systems of nonlocal balance laws. We show that the approximate solutions stay bounded by an exponential $L^\infty$ bound in time. Under linearity assumptions on the flux and source terms the approximate solutions converge weakly-$*$ to weak solutions of the nonlocal balance laws. Assuming stronger regularity, in particular on the convolution kernel, we show strong convergence towards entropy weak solutions in the nonlinear case. Numerical examples validate our results and demonstrate its applicability to various systems of nonlocal problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14425v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjibanee Sudha, Jan Friedrich, Samala Rathan</dc:creator>
    </item>
    <item>
      <title>Robust iterative methods for linear systems with saddle point structure</title>
      <link>https://arxiv.org/abs/2502.21174</link>
      <description>arXiv:2502.21174v2 Announce Type: replace 
Abstract: We propose a new class of multi-layer iterative schemes for solving sparse linear systems in saddle point structure. The new scheme consist of an iterative preconditioner that is based on the (approximate) nullspace method, combined with an iterative least squares approach and an iterative projection method. We present a theoretical analysis and demonstrate the effectiveness and robustness of the new scheme on sparse matrices from various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21174v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murat Manguo\u{g}lu, Volker Mehrmann</dc:creator>
    </item>
    <item>
      <title>Comparison of $\mathcal{H}$-matrix- and FMM-based 3D-ACA for a time-domain boundary element method</title>
      <link>https://arxiv.org/abs/2505.00715</link>
      <description>arXiv:2505.00715v3 Announce Type: replace 
Abstract: The homogeneous wave equation is solved by a time-domain boundary element method (BEM) using low-order shape functions for spatial, and the generalised convolution quadrature method (gCQ) by Lopez-Fernandez and Sauter for temporal discretisation. The three-dimensional array of BEM matrices according to a set of complex frequencies in Laplace domain is approximated by generalised Adaptive Cross Approximation (3D-ACA). Its rank is increased adaptively until a prescribed accuracy is reached, relying on a pure algebraic error criterion. The data slices for the selected frequency points are further processed by either the standard $\mathcal{H}$-matrices approach with ACA or by a fast multipole method (FMM). This paper compares both approaches with respect to their demands in storage and computing time. Both techniques are illustrated for calculating the sound scattered by an electric machine, for which the proposed algebraic compression techniques make time-domain BEM feasible for the first time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00715v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martin Schanz, Vibudha Lakshmi Keshava, Herbert de Gersem</dc:creator>
    </item>
    <item>
      <title>A mixed finite element method for a class of fourth-order stochastic evolution equations with multiplicative noise</title>
      <link>https://arxiv.org/abs/2505.04866</link>
      <description>arXiv:2505.04866v2 Announce Type: replace 
Abstract: We develop a fully discrete, semi-implicit mixed finite element method for approximating solutions to a class of fourth-order stochastic partial differential equations (SPDEs) with non-globally Lipschitz and non-monotone nonlinearities, perturbed by spatially smooth multiplicative Gaussian noise. The proposed scheme is applicable to a range of physically relevant nonlinear models, including the stochastic Landau--Lifshitz--Baryakhtar (sLLBar) equation, the stochastic convective Cahn--Hilliard equation with mass source, and the stochastic regularised Landau--Lifshitz--Bloch (sLLB) equation, among others. To overcome the difficulties posed by the interplay between the nonlinearities and the stochastic forcing, we adopt a `truncate-then-discretise' strategy: the nonlinear term is first truncated before discretising the resulting modified problem. We show that the strong solution to the truncated problem converges in probability to that of the original problem. A fully discrete numerical scheme is then proposed for the truncated system, and we establish both convergence in probability and strong convergence (with quantitative rates) for the two fields used in the mixed formulation. Numerical simulations are provided to support the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04866v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beniamin Goldys, Agus L. Soenjaya, Thanh Tran</dc:creator>
    </item>
    <item>
      <title>$S^{\top\!}S$-SVD via Sketching and the Nearest $S^{\top\!}S$-orthogonal Matrix</title>
      <link>https://arxiv.org/abs/2505.23582</link>
      <description>arXiv:2505.23582v2 Announce Type: replace 
Abstract: Sketching techniques have gained popularity in numerical linear algebra to accelerate the solution of least squares problems. The so-called $\varepsilon$-subspace embedding property of a sketching matrix $S$ has been largely used to characterize the problem residual norm, since the procedure is no longer optimal in terms of the (classical) Frobenius or Euclidean norm.
  By building on available results on the SVD of the sketched matrix $SA$ derived by Gilbert, Park, and Wakin (Proc. of SPARS-2013), a novel decomposition of $A$, the $S^{\top\!}S$-SVD, is proposed, which \emph{holds} with high probability, and in which the left singular vectors are orthonormal with respect to a (semi-)norm defined by the sketching matrix $S$. The new decomposition is less expensive to compute than the standard SVD, while preserving the singular values with probabilistic confidence.The $S^{\top\!}S$-SVD appears to be the right tool to analyze the quality of several sketching-based techniques in the literature, for which examples are reported. For instance, it is possible to simply bound the distance from (standard) orthogonality of sketching-based orthogonal matrices in state-of-the-art randomized algorithms for QR factorizations. As an application, the classical problem of the nearest orthogonal matrix is generalized to the new $S^{\top\!}S$-orthogonality, and the $S^{\top\!}S$-SVD is used to solve it. Probabilistic bounds on the quality of the solution are also derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23582v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Palitta, Valeria Simoncini</dc:creator>
    </item>
    <item>
      <title>An energy-stable parametric finite element method for Willmore flow with normal-tangential velocity splitting</title>
      <link>https://arxiv.org/abs/2507.00193</link>
      <description>arXiv:2507.00193v2 Announce Type: replace 
Abstract: We propose and analyze an energy-stable fully discrete parametric approximation for Willmore flow of hypersurfaces in two and three space dimensions. We allow for the presence of spontaneous curvature effects and for open surfaces with boundary. The presented scheme is based on a new geometric partial differential equation (PDE) that combines an evolution equation for the mean curvature with a separate equation that prescribes the tangential velocity. The mean curvature is used to determine the normal velocity within the gradient flow structure, thus guaranteeing an unconditional energy stability for the discrete solution upon suitable discretization. We introduce a novel weak formulation for this geometric PDE, in which different types of boundary conditions can be naturally enforced. We further discretize the weak formulation to obtain a fully discrete parametric finite element method, for which well-posedness can be rigorously shown. Moreover, the constructed scheme admits an unconditional stability estimate in terms of the discrete energy. Extensive numerical experiments are reported to showcase the accuracy and robustness of the proposed method for computing Willmore flow of both curves in $\mathbb{R}^2$ and surfaces in $\mathbb{R}^3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00193v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harald Garcke, Robert N\"urnberg, Quan Zhao</dc:creator>
    </item>
    <item>
      <title>Error Estimates for Sparse Tensor Products of B-spline Approximation Spaces</title>
      <link>https://arxiv.org/abs/2510.21517</link>
      <description>arXiv:2510.21517v2 Announce Type: replace 
Abstract: This work introduces and analyzes B-spline approximation spaces defined on general geometric domains obtained through a mapping from a parameter domain. These spaces are constructed as sparse-grid tensor products of univariate spaces in the parameter domain and are mapped to the physical domain via a geometric parametrization. Both the univariate approximation spaces and the geometric mapping are built using maximally smooth B-splines. We construct two such spaces, employing either the sparse-grid combination technique or the hierarchical subspace decomposition of sparse-grid tensor products, and we prove their mathematical equivalence. Furthermore, we derive approximation error estimates and inverse inequalities that highlight the advantages of sparse-grid tensor products. Specifically, under suitable regularity assumptions on the solution, these spaces achieve the same approximation order as standard tensor product spaces while using significantly fewer degrees of freedom. Additionally, our estimates indicate that, in the case of non-tensor-product domains, stronger regularity assumptions on the solution -- particularly concerning isotropic (non-mixed) derivatives -- are required to achieve optimal convergence rates compared to sparse-grid methods defined on tensor-product domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21517v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement Guillet</dc:creator>
    </item>
    <item>
      <title>FG-PINNs: frequency-guided physics-informed neural networks for solving PDEs with high frequency components</title>
      <link>https://arxiv.org/abs/2511.12055</link>
      <description>arXiv:2511.12055v2 Announce Type: replace 
Abstract: In this work, we propose the frequency-guided physics-informed neural networks (FG-PINNs), specifically designed for solving partial differential equations (PDEs) with high-frequency components. The core of the proposed algorithm lies in utilizing high-frequency information obtained from PDEs to guide the neural network in rapidly approximating the high-frequency components of the solution. The FG-PINNs consist of two subnetworks, including a high-frequency subnetwork for capturing high-frequency components and a low-frequency subnetwork for capturing low-frequency components. The key innovation in the high-frequency subnetworks is to embed prior knowledge for high-frequency components into the network structure. For nonhomogeneous PDEs ($f(x)\neq c, c\in R$), we embed the source term with high-frequency components into the neural network. For homogeneous PDEs, we embed the initial/boundary conditions with high-frequency components into the neural network. Based on spectral bias, we use a fully connected neural network as the low-frequency subnetwork to capture low-frequency components of the solution. A series of numerical examples demonstrate the effectiveness of the FG-PINNs, including the one-dimensional heat equation (relative $L^{2}$ error: $O(10^{-4})$), the nonlinear wave equations (relative $L^{2}$ error: $O(10^{-4})$) and the two-dimensional heat equation (relative $L^{2}$ error: $O(10^{-3})$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12055v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiachun Zheng, Yunqing Huang, Nianyu Yi, Yunlei Yang</dc:creator>
    </item>
    <item>
      <title>A low-rank non-convex norm method for multiview graph clustering</title>
      <link>https://arxiv.org/abs/2312.11157</link>
      <description>arXiv:2312.11157v2 Announce Type: replace-cross 
Abstract: This study introduces a novel technique for multi-view clustering known as the "Consensus Graph-Based Multi-View Clustering Method Using Low-Rank Non-Convex Norm" (CGMVC-NC). Multi-view clustering is a challenging task in machine learning as it requires the integration of information from multiple data sources or views to cluster data points accurately. The suggested approach makes use of the structural characteristics of multi-view data tensors, introducing a non-convex tensor norm to identify correlations between these views. In contrast to conventional methods, this approach demonstrates superior clustering accuracy across several benchmark datasets. Despite the non-convex nature of the tensor norm used, the proposed method remains amenable to efficient optimization using existing algorithms. The approach provides a valuable tool for multi-view data analysis and has the potential to enhance our understanding of complex systems in various fields. Further research can explore the application of this method to other types of data and extend it to other machine-learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11157v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alaeddine Zahir, Khalide Jbilou, Ahmed Ratnani</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of the transformed gradient projection algorithms on compact matrix manifolds</title>
      <link>https://arxiv.org/abs/2404.19392</link>
      <description>arXiv:2404.19392v3 Announce Type: replace-cross 
Abstract: In this paper, we study the optimization problem on a compact matrix manifold. While existing feasible algorithms can be broadly categorized into retraction-based and projection-based methods, compared to the more comprehensive and in-depth algorithmic and convergence research framework for retraction-based line-search (RetrLS) algorithms using only tangent vectors, the theoretical understanding and algorithmic design of projection-based line-search (ProjLS) algorithms remain limited, especially when general search directions and stepsizes are involved. To bridge this gap, we propose a unified algorithmic framework called the Transformed Gradient Projection (TGP) algorithm. The key idea is to construct the search direction as a transformed Riemannian (or Euclidean) gradient augmented by an additional normal component, allowing the framework to encompass and generalize numerous existing algorithms. Then, we conduct a thorough exploration of the convergence properties of the TGP algorithms under various stepsizes, including the Armijo, Zhang-Hager type nonmonotone Armijo, and fixed stepsizes. To achieve this, we extensively analyze the geometric properties of the projection onto compact matrix manifolds, which may be of independent interest. Building upon these insights, we establish the weak convergence, iteration complexity, and global convergence of TGP algorithms under three distinct stepsizes. In cases where the compact matrix manifold is the Stiefel or Grassmann manifold, our convergence results either encompass or surpass those found in the literature. Finally, through a series of numerical experiments and theoretical analysis, we observe that different choices of scaling matrices and normal components in the search direction of TGP algorithms can lead to significantly different performance in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19392v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wentao Ding, Jianze Li, Shuzhong Zhang</dc:creator>
    </item>
    <item>
      <title>Stochastic gradient with least-squares control variates</title>
      <link>https://arxiv.org/abs/2507.20981</link>
      <description>arXiv:2507.20981v2 Announce Type: replace-cross 
Abstract: The stochastic gradient descent (SGD) method is a widely used approach for solving stochastic optimization problems, but its convergence is typically slow. Existing variance reduction techniques, such as SAGA, improve convergence by leveraging stored gradient information; however, they are restricted to settings where the objective functional is a finite sum, and their performance degrades when the number of terms in the sum is large. In this work, we propose a novel approach which is well suited when the objective is given by an expectation over random variables with a continuous probability distribution. Our method constructs a control variate by fitting a linear model to past gradient evaluations using weighted discrete least-squares, effectively reducing variance while preserving computational efficiency. We establish theoretical sublinear convergence guarantees for strongly convex objectives and demonstrate the method's effectiveness through numerical experiments on random PDE-constrained optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20981v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Nobile, Matteo Raviola, Nathan Schaeffer</dc:creator>
    </item>
    <item>
      <title>Best $m$-term trigonometric approximation in weighted Wiener spaces and applications</title>
      <link>https://arxiv.org/abs/2508.07336</link>
      <description>arXiv:2508.07336v2 Announce Type: replace-cross 
Abstract: In this paper we study best \(m\)-term trigonometric approximation in weighted Wiener spaces and its consequences for Besov and Sobolev spaces with bounded mixed derivative/difference. We obtain several sharp asymptotic bounds for weighted Wiener spaces including the quasi-Banach case. It has recently been observed that best \(m\)-term trigonometric widths in the uniform norm together with recovery algorithms stemming from compressed sensing serve to control the optimal sampling recovery error in various relevant spaces of multivariate functions. We use a collection of old and new tools as well as novel findings to extend the recovery bounds to classical multivariate smoothness spaces. It turns out that embeddings into Wiener spaces serve as a powerful tool to improve certain recent bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07336v2</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Moeller, Serhii Stasyuk, Tino Ullrich</dc:creator>
    </item>
  </channel>
</rss>
