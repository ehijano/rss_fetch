<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 05:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Clustering country-level all-cause mortality data: a review</title>
      <link>https://arxiv.org/abs/2512.04831</link>
      <description>arXiv:2512.04831v1 Announce Type: new 
Abstract: Mortality data are relevant to demography, public health, and actuarial science. Whilst clustering is increasingly used to explore patterns in such data, no study has reviewed its application to country-level all-cause mortality. This review therefore summarises recent work and addresses key questions: why clustering is used, which mortality data are analysed, which methods are most common, and what main findings emerge. To address these questions, we examine studies applying clustering to country-level all-cause mortality, focusing on mortality indices, data sources, and methodological choices, and we replicate some approaches using Human Mortality Database (HMD) data. Our analysis reveals that clustering is mainly motivated by forecasting and by studying convergence and inequality. Most studies use HMD data from developed countries and rely on k-means, hierarchical, or functional clustering. Main findings include a persistent East-West European division across applications, with clustering generally improving forecast accuracy over single-country models. Overall, this review highlights the methodological range in the literature, summarises clustering results, and identifies gaps, such as the limited evaluation of clustering quality and the underuse of data from countries outside the high-income world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04831v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Menezes de Araujo, Isobel Claire Gormley, Thomas Brendan Murphy</dc:creator>
    </item>
    <item>
      <title>Sequential Randomization Tests Using E-values: A Betting Approach for Clinical Trials</title>
      <link>https://arxiv.org/abs/2512.04366</link>
      <description>arXiv:2512.04366v1 Announce Type: cross 
Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We present a nonparametric sequential test, the randomization e-process (e-RT), that derives validity solely from the randomization mechanism. Using a betting framework, e-RT constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. The e-RT provides a conservative, assumption-free complement to model-based sequential analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04366v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando G Zampieri</dc:creator>
    </item>
    <item>
      <title>Bayesian Graphical High-Dimensional Time Series Models for Detecting Structural Changes</title>
      <link>https://arxiv.org/abs/2512.04444</link>
      <description>arXiv:2512.04444v1 Announce Type: cross 
Abstract: We study the structural changes in multivariate time-series by estimating and comparing stationary graphs for macroeconomic time series before and after an economic crisis such as the Great Recession. Building on a latent time series framework called Orthogonally-rotated Univariate Time-series (OUT), we propose a shared-parameter framework-the spOUT autoregressive model (spOUTAR)-that jointly models two related multivariate time series and enables coherent Bayesian estimation of their corresponding stationary precision matrices. This framework provides a principled mechanism to detect and quantify which conditional relationships among the variables changed, or formed following the crisis. Specifically, we study the impact of the Great Recession (December 2007-June 2009) that substantially disrupted global and national economies, prompting long-lasting shifts in macroeconomic indicators and their interrelationships. While many studies document its economic consequences, far less is known about how the underlying conditional dependency structure among economic variables changed as economies moved from pre-crisis stability through the shock and back to normalcy. Using the proposed approach to analyze U.S. and OECD macroeconomic data, we demonstrate that spOUTAR effectively captures recession-induced changes in stationary graphical structure, offering a flexible and interpretable tool for studying structural shifts in economic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04444v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuvrarghya Ghosh, Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Thompson, Ulam, or Gauss? Multi-criteria recommendations for posterior probability computation methods in Bayesian response-adaptive trials</title>
      <link>https://arxiv.org/abs/2411.19871</link>
      <description>arXiv:2411.19871v3 Announce Type: replace-cross 
Abstract: Bayesian adaptive designs enable flexible clinical trials by adapting features based on accumulating data. Among these, Bayesian Response-Adaptive Randomization (BRAR) skews patient allocation towards more promising treatments based on interim data. Implementing BRAR requires the relatively quick evaluation of posterior probabilities. However, the limitations of existing closed-form solutions mean trials often rely on computationally intensive approximations which can impact accuracy and the scope of scenarios explored. While faster Gaussian approximations exist, their reliability is not guaranteed. Critically, the approximation method used is often poorly reported, and the literature lacks practical guidance for selecting and comparing these methods, particularly regarding the trade-offs between computational speed, inferential accuracy, and their implications for patient benefit.
  In this paper, we focus on BRAR trials with binary endpoints, developing a novel algorithm that efficiently and exactly computes these posterior probabilities, enabling a robust assessment of existing approximation methods in use. Leveraging these exact computations, we establish a comprehensive benchmark for evaluating approximation methods based on their computational speed, patient benefit, and inferential accuracy. Our comprehensive analysis, conducted through a range of simulations in the two-armed case and a re-analysis of the three-armed Established Status Epilepticus Treatment Trial, reveals that the exact calculation algorithm is often the fastest, even for up to 12 treatment arms. Furthermore, we demonstrate that commonly used approximation methods can lead to significant power loss and Type I error rate inflation. We conclude by providing practical guidance to aid practitioners in selecting the most appropriate computation method for various clinical trial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19871v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kaddaj, Stef Baas, Edwin Y. N. Tang, David S. Robertson, Lukas Pin, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Introducing multiverse analysis to bibliometrics: The case of team size effects on disruptive research</title>
      <link>https://arxiv.org/abs/2506.03726</link>
      <description>arXiv:2506.03726v2 Announce Type: replace-cross 
Abstract: Although bibliometrics has become an essential tool in the evaluation of research performance, bibliometric analyses are sensitive to a range of methodological choices. Subtle choices in data selection, indicator construction, and modeling decisions can substantially alter results. Ensuring robustness (meaning that findings hold up under different reasonable scenarios) is therefore critical for credible research and research evaluation. To address this issue, this study introduces multiverse analysis to bibliometrics. Multiverse analysis is a statistical tool that enables analysts to transparently discuss modeling assumptions and thoroughly assess model robustness. Whereas standard robustness checks usually cover only a small subset of all plausible models, multiverse analysis includes all plausible models. The benefits of multiverse analysis are illustrated by assessing the robustness of the findings reported by Wu et al. (2019), who observed that small teams tend to produce more disruptive research than large teams. While we found robust evidence of a negative effect of team size on disruption scores, the effect size depends substantially on the model specification. Our findings underscore the importance of assessing the multiverse robustness of bibliometric results to clarify their practical implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03726v2</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Leibel, Lutz Bornmann</dc:creator>
    </item>
    <item>
      <title>Implicit score-driven filters for time-varying parameter models</title>
      <link>https://arxiv.org/abs/2512.02744</link>
      <description>arXiv:2512.02744v2 Announce Type: replace-cross 
Abstract: We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02744v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
  </channel>
</rss>
