<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 02:08:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tiny-QMoE</title>
      <link>https://arxiv.org/abs/2509.22951</link>
      <description>arXiv:2509.22951v1 Announce Type: new 
Abstract: The QMoE model provides a practical approach for compression of massive Mixture-of-Experts (MoE) models. QMoE offers a solution geared towards memory limitations that often reach terabyte scales, and it has the advantage of working with high sparsity models which implicitly lend themselves to compression techniques. QMoE also has the advantage of only taking MoE models into account and does not evaluate its use with non mixture of expert systems. Although this prior attempt focuses on the limitations of large servers with the latest NVIDIA hardware which in the case of the H100 and V100 which have 80 GB of HBM (High Bandwidth Memory), what is not being considered is a significantly more constrained environment, such as in the case of mobile devices which may have in the case of the iPhone anywhere from 4 to 8 GB of unified memory which also needs to be shared with the operating system and additional processes. Although edge devices such as phones and laptops are becoming increasingly more computationally powerful, they are still not close to the level of advanced server machines such as NVIDIA. An additional constraint that we must consider is that of latency. The communication time of sending a request to an LLM server and then getting it back is an additional waiting time that can be removed. We may also want to use LLM technology in environments where there is no reliable network connection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22951v1</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Cashman, Jiaqi Nie</dc:creator>
    </item>
    <item>
      <title>DarwinGame: Playing Tournaments for Tuning Applications in Noisy Cloud Environments</title>
      <link>https://arxiv.org/abs/2509.25090</link>
      <description>arXiv:2509.25090v1 Announce Type: new 
Abstract: This work introduces a new subarea of performance tuning -- performance tuning in a shared interference-prone computing environment. We demonstrate that existing tuners are significantly suboptimal by design because of their inability to account for interference during tuning. Our solution, DarwinGame, employs a tournament-based design to systematically compare application executions with different tunable parameter configurations, enabling it to identify the relative performance of different tunable parameter configurations in a noisy environment. Compared to existing solutions, DarwinGame achieves more than 27% reduction in execution time, with less than 0.5% performance variability. DarwinGame is the first performance tuner that will help developers tune their applications in shared, interference-prone, cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25090v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Basu Roy, Vijay Gadepally, Devesh Tiwari</dc:creator>
    </item>
    <item>
      <title>ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs</title>
      <link>https://arxiv.org/abs/2509.22684</link>
      <description>arXiv:2509.22684v1 Announce Type: cross 
Abstract: Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic proofs to demonstrate knowledge of a secret input in a computation without revealing any information about the secret. ZKPs enable novel applications in private and verifiable computing such as anonymized cryptocurrencies and blockchain scaling and have seen adoption in several real-world systems. Prior work has accelerated ZKPs on GPUs by leveraging the inherent parallelism in core computation kernels like Multi-Scalar Multiplication (MSM). However, we find that a systematic characterization of execution bottlenecks in ZKPs, as well as their scalability on modern GPU architectures, is missing in the literature. This paper presents ZKProphet, a comprehensive performance study of Zero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that ZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they account for up to 90% of the proof generation latency on GPUs when paired with optimized MSM implementations. Available NTT implementations under-utilize GPU compute resources and often do not employ architectural features like asynchronous compute and memory operations. We observe that the arithmetic operations underlying ZKPs execute exclusively on the GPU's 32-bit integer pipeline and exhibit limited instruction-level parallelism due to data dependencies. Their performance is thus limited by the available integer compute units. While one way to scale the performance of ZKPs is adding more compute units, we discuss how runtime parameter tuning for optimizations like precomputed inputs and alternative data representations can extract additional speedup. With this work, we provide the ZKP community a roadmap to scale performance on GPUs and construct definitive GPU-accelerated ZKPs for their application requirements and available hardware resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22684v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarunesh Verma (Computer Science and Engineering, University of Michigan, USA), Yichao Yuan (Computer Science and Engineering, University of Michigan, USA), Nishil Talati (Computer Science and Engineering, University of Michigan, USA), Todd Austin (Computer Science and Engineering, University of Michigan, USA)</dc:creator>
    </item>
    <item>
      <title>Unlicensed Band Allocation for Heterogeneous Networks</title>
      <link>https://arxiv.org/abs/2509.23216</link>
      <description>arXiv:2509.23216v1 Announce Type: cross 
Abstract: Based on the License-Assisted Access (LAA) small cell architecture, the LAA coexisting with Wi-Fi heterogeneous networks provides LTE mobile users with high bandwidth efficiency as the unlicensed channels are shared among LAA and Wi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use the same unlicensed channel in heterogeneous networks. In such a network, unlicensed band allocation for LAA and Wi-Fi is an important issue that may affect the quality of service (QoS) of both systems significantly. In this paper, we propose an analytical model and conduct simulation experiments to study four allocations for the unlicensed band: unlicensed full allocation (UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering mechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance of these unlicensed band allocation schemes in terms of the acceptance rate of both LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides guidelines for designing the channel occupation phase and the buffer size of the LAA small cell.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23216v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1587/transcom.2019EBP3094</arxiv:DOI>
      <arxiv:journal_reference>IEICE Transactions on Communications, vol. E103-B, no. 2, pp. 103-117, Feb. 2020</arxiv:journal_reference>
      <dc:creator>Po-Heng Chou</dc:creator>
    </item>
    <item>
      <title>Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism</title>
      <link>https://arxiv.org/abs/2509.23217</link>
      <description>arXiv:2509.23217v1 Announce Type: cross 
Abstract: In this letter, we propose an analytical model and conduct simulation experiments to study listen-before-talk-based unlicensed band allocation with the buffering mechanism for the License-Assisted Access (LAA) packets in the heterogeneous networks. In such a network, unlicensed band allocation for LAA and Wi-Fi is an important issue, which may affect the quality of service for both systems significantly. We evaluate the performance of these unlicensed band allocations in terms of the acceptance rate of both LAA and Wi-Fi packets. This letter provides the guidelines for designing the channel occupation phase and buffer threshold of the LAA systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23217v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCOMM.2019.2892988</arxiv:DOI>
      <arxiv:journal_reference>IEEE Communications Letters, vol. 23, no. 3, Mar. 2019</arxiv:journal_reference>
      <dc:creator>Po-Heng Chou</dc:creator>
    </item>
    <item>
      <title>Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D</title>
      <link>https://arxiv.org/abs/2509.23218</link>
      <description>arXiv:2509.23218v1 Announce Type: cross 
Abstract: In this paper, a novel analytical model for resource allocation is proposed for a device-to-device (D2D) assisted cellular network. The proposed model can be applied to underlay and overlay D2D systems for sharing licensed bands and offloading cellular traffic. The developed model also takes into account the problem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a global system state reflects the interaction among D2D, conventional cellular, and Wi-Fi packets. Under the standard traffic model assumptions, a threshold-based flow control is proposed for guaranteeing the quality-of-service (QoS) of Wi-Fi. The packet blockage probability is then derived. Simulation results show the proposed scheme sacrifices conventional cellular performance slightly to improve overlay D2D performance significantly while maintaining the performance for Wi-Fi users. Meanwhile, the proposed scheme has more flexible adjustments between D2D and Wi-Fi than the underlay scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23218v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICC51166.2024.10622941</arxiv:DOI>
      <arxiv:journal_reference>Proc. IEEE International Conference on Communications (ICC), pp. 1-6, Denver, CO, USA, Jun. 2024</arxiv:journal_reference>
      <dc:creator>Po-Heng Chou, Yen-Ting Liu, Wei-Chang Chen, Walid Saad</dc:creator>
    </item>
    <item>
      <title>PATCH: Learnable Tile-level Hybrid Sparsity for LLMs</title>
      <link>https://arxiv.org/abs/2509.23410</link>
      <description>arXiv:2509.23410v1 Announce Type: cross 
Abstract: Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23410v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younes Hourri, Mohammad Mozaffari, Maryam Mehri Dehnavi</dc:creator>
    </item>
    <item>
      <title>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</title>
      <link>https://arxiv.org/abs/2509.24063</link>
      <description>arXiv:2509.24063v1 Announce Type: cross 
Abstract: Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24063v1</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <category>cs.PF</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Breitwieser, Ahmad Hesam, Abdullah Giray Ya\u{g}l{\i}k\c{c}{\i}, Mohammad Sadrosadati, Fons Rademakers, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>PerfBench: Can Agents Resolve Real-World Performance Bugs?</title>
      <link>https://arxiv.org/abs/2509.24091</link>
      <description>arXiv:2509.24091v1 Announce Type: cross 
Abstract: Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24091v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Spandan Garg, Roshanak Zilouchian Moghaddam</dc:creator>
    </item>
    <item>
      <title>Demystifying Serverless Costs on Public Platforms: Bridging Billing, Architecture, and OS Scheduling</title>
      <link>https://arxiv.org/abs/2506.01283</link>
      <description>arXiv:2506.01283v2 Announce Type: replace-cross 
Abstract: Public cloud serverless platforms have attracted a large user base due to their high scalability, plug-and-play deployment model, and pay-per-use billing. However, compared to virtual machines and container hosting services, modern serverless offerings typically impose higher per-unit time and resource charges. Additionally, billing practices such as wall-clock time allocation-based billing, invocation fees, and usage rounding up can further increase costs.
  This work, for the first time, holistically demystifies these costs by conducting an in-depth, top-down characterization and analysis from user-facing billing models, through request serving architectures, and down to operating system scheduling on major public serverless platforms. We quantify, for the first time, how current billing practices inflate billable resources up to 4.35x beyond actual consumption. Also, our analysis reveals previously unreported cost drivers, such as operational patterns of serving architectures that create overheads, details of resource allocation during keep-alive periods, and OS scheduling granularity effects that directly impact both performance and billing. By tracing the sources of costs from billing models down to OS scheduling, we uncover the rationale behind today's expensive serverless billing model and practices and provide insights for designing performant and cost-effective serverless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01283v2</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3767295.3769374</arxiv:DOI>
      <dc:creator>Changyuan Lin, Yuanzhi Ma, Mohammad Shahrad</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem</title>
      <link>https://arxiv.org/abs/2507.01076</link>
      <description>arXiv:2507.01076v4 Announce Type: replace-cross 
Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical analysis on its practical behaviour despite theoretical studies. This paper addresses this gap by implementing and evaluating three distinct algorithms -- a direct random heuristic, a hypergraph-based approximation, and a genetic algorithm -- on diverse synthetic graph datasets, including those with analytically known $\mu(G)$ values and general graph models. Our results demonstrate that for smaller graphs, the algorithms consistently achieve MV set sizes aligning with theoretical bounds. However, for larger instances, achieved solution sizes notably diverge from theoretical limits; this, combined with the absence of tight bounds, complicates absolute quality assessment. Nevertheless, validation on known optimal graphs showed the Genetic Algorithm and other heuristics empirically performing best among tested methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01076v4</guid>
      <category>cs.CG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vanja Stojanovi\'c, Bor Panger\v{s}i\v{c}</dc:creator>
    </item>
    <item>
      <title>BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search</title>
      <link>https://arxiv.org/abs/2507.12189</link>
      <description>arXiv:2507.12189v2 Announce Type: replace-cross 
Abstract: We present BenchRL-QAS, a unified benchmarking framework for reinforcement learning (RL) in quantum architecture search (QAS) across a spectrum of variational quantum algorithm tasks on 2- to 8-qubit systems. Our study systematically evaluates 9 different RL agents, including both value-based and policy-gradient methods, on quantum problems such as variational eigensolver, quantum state diagonalization, variational quantum classification (VQC), and state preparation, under both noiseless and noisy execution settings. To ensure fair comparison, we propose a weighted ranking metric that integrates accuracy, circuit depth, gate count, and training time. Results demonstrate that no single RL method dominates universally, the performance dependents on task type, qubit count, and noise conditions providing strong evidence of no free lunch principle in RL-QAS. As a byproduct we observe that a carefully chosen RL algorithm in RL-based VQC outperforms baseline VQCs. BenchRL-QAS establishes the most extensive benchmark for RL-based QAS to date, codes and experimental made publicly available for reproducibility and future advances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12189v2</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of AAAI Symposium series (2025)</arxiv:journal_reference>
      <dc:creator>Azhar Ikhtiarudin, Aditi Das, Param Thakkar, Akash Kundu</dc:creator>
    </item>
    <item>
      <title>Impact of RHIs and ipSIC on Active RIS-NOMA Systems with Low-Precision ADCs</title>
      <link>https://arxiv.org/abs/2509.19383</link>
      <description>arXiv:2509.19383v2 Announce Type: replace-cross 
Abstract: This study evaluates the performance of an active reconfigurable intelligent surface (ARIS)-assisted non-orthogonal multiple access (NOMA) system employing low-precision analog-to-digital converters (ADCs). Analytical approximations for the outage probability (OP) are derived, considering residual hardware impairments (RHIs) and imperfect successive interference cancellation (ipSIC). Additionally, we analyze the asymptotic OP, system throughput, and diversity order at high signal-to-noise ratios (SNRs). Simulation results demonstrate that the proposed quantized ARIS-NOMA system outperforms its passive counterpart (PRIS-NOMA), achieving lower OP and higher throughput with reduced transmit power requirements and fewer reflecting elements. Moreover, the outage performance of both quantized ARIS-NOMA and PRIS-NOMA systems demonstrates significant improvement as the number of reflecting elements increases. The negative impacts of low-precision ADCs can be effectively mitigated by optimizing transmit power and scaling the number of reflecting elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19383v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianqian Li, Hua Li, Shiya Hao, Lintao Li, Xiaoming Dai</dc:creator>
    </item>
  </channel>
</rss>
