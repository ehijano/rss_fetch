<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery</title>
      <link>https://arxiv.org/abs/2509.08207</link>
      <description>arXiv:2509.08207v1 Announce Type: new 
Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer, designed to accelerate scientific discovery with cutting-edge architectural innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named Ponte Vecchio) on each compute node. Aurora also integrates the Distributed Asynchronous Object Storage (DAOS), a novel exascale storage solution, and leverages Intel's oneAPI programming environment. This paper presents an in-depth exploration of Aurora's node architecture, the HPE Slingshot interconnect, the supporting software ecosystem, and DAOS. We provide insights into standard benchmark performance and applications readiness efforts via Aurora's Early Science Program and the Exascale Computing Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08207v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <category>cs.PF</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin S. Allen, James Anchell, Victor Anisimov, Thomas Applencourt, Abhishek Bagusetty, Ramesh Balakrishnan, Riccardo Balin, Solomon Bekele, Colleen Bertoni, Cyrus Blackworth, Renzo Bustamante, Kevin Canada, John Carrier, Christopher Chan-nui, Lance C. Cheney, Taylor Childers, Paul Coffman, Susan Coghlan, Michael D'Mello, Murali Emani, Kyle G. Felker, Sam Foreman, Olivier Franza, Longfei Gao, Marta Garc\'ia, Mar\'ia Garzar\'an, Balazs Gerofi, Yasaman Ghadar, Neha Gupta, Kevin Harms, V\"ain\"o Hatanp\"a\"a, Brian Holland, Carissa Holohan, Brian Homerding, Khalid Hossain, Louise Huot, Huda Ibeid, Joseph A. Insley, Sai Jayanthi, Hong Jiang, Wei Jiang, Xiao-Yong Jin, Jeongnim Kim, Christopher Knight, Kalyan Kumaran, JaeHyuk Kwack, Ti Leggett, Ben Lenard, Chris Lewis, Nevin Liber, Johann Lombardi, Raymond M. Loy, Ye Luo, Bethany Lusch, Nilakantan Mahadevan, Victor A. Mateevitsi, Gordon McPheeters, Ryan Milner, Vitali A. Morozov, Servesh Muralidharan, Tom Musta, Mrigendra Nagar, Vikram Narayana, Marieme Ngom, Anthony-Trung Nguyen, Nathan Nichols, Aditya Nishtala, James C. Osborn, Michael E. Papka, Scott Parker, Saumil S. Patel, Adrian C. Pope, Sucheta Raghunanda, Esteban Rangel, Paul M. Rich, Silvio Rizzi, Kris Rowe, Varuni Sastry, Adam Scovel, Filippo Simini, Haritha Siddabathuni Som, Patrick Steinbrecher, Rick Stevens, Xinmin Tian, Peter Upton, Thomas Uram, Archit K. Vasan, \'Alvaro V\'azquez-Mayagoitia, Kaushik Velusamy, Brice Videau, Venkatram Vishwanath, Brian Whitney, Timothy J. Williams, Michael Woodacre, Sam Zeltner, Gengbin Zheng, Huihuo Zheng</dc:creator>
    </item>
    <item>
      <title>Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem</title>
      <link>https://arxiv.org/abs/2509.08215</link>
      <description>arXiv:2509.08215v1 Announce Type: new 
Abstract: In the rapidly evolving industry of software development, coding efficiency and accuracy play significant roles in delivering high-quality software. Various code suggestion and completion tools, such as CodeBERT from Microsoft and GPT-3.5 from OpenAI, have been developed using deep learning techniques and integrated into IDEs to assist software engineers' development. Research has shown that CodeBERT has outstanding performance in code summarization and capturing code semantics, while GPT-3.5 demonstrated its adept capability at code generation. This study focuses on implementing a hybrid model that integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in three main metrics: accuracy, quality of generated code and performance efficiency with various software and hardware, the hybrid model outperforms benchmarks, demonstrating its feasibility and effectiveness. Robustness testing further confirms the reliability and stability of the hybrid model. This study not only emphasizes the importance of deep learning in the software development industry, but also reveals the potential of synthesizing complementary deep learning models to fully exploit strengths of each model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08215v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingbing Zhang, Ziyu Lin, Yingxin Su</dc:creator>
    </item>
    <item>
      <title>Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism</title>
      <link>https://arxiv.org/abs/2509.08309</link>
      <description>arXiv:2509.08309v1 Announce Type: new 
Abstract: The significant resource demands in LLM serving prompts production clusters to fully utilize heterogeneous hardware by partitioning LLM models across a mix of high-end and low-end GPUs. However, existing parallelization approaches often struggle to scale efficiently in heterogeneous environments due to their coarse-grained and static parallelization strategies.
  In this paper, we introduce Hetis, a new LLM system tailored for heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory inefficiency caused by the mismatch between memory capacity and computational power in heterogeneous devices, and (2) computational inefficiency arising from performance gaps across different LLM modules. To tackle these issues, Hetis employs a fine-grained and dynamic parallelism design. Specifically, it selectively parallelizes compute-intensive operations to reduce latency and dynamically distributes Attention computations to low-end GPUs at a head granularity, leveraging the distinct characteristics of each module. Additionally, Hetis features an online load dispatching policy that continuously optimizes serving performance by carefully balancing network latency, computational load, and memory intensity. Evaluation results demonstrate that Hetis can improve serving throughput by up to $2.25\times$ and reduce latency by $1.49\times$ compared to existing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08309v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712285.3759784</arxiv:DOI>
      <dc:creator>Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu</dc:creator>
    </item>
    <item>
      <title>An HPC Benchmark Survey and Taxonomy for Characterization</title>
      <link>https://arxiv.org/abs/2509.08347</link>
      <description>arXiv:2509.08347v1 Announce Type: new 
Abstract: The field of High-Performance Computing (HPC) is defined by providing computing devices with highest performance for a variety of demanding scientific users. The tight co-design relationship between HPC providers and users propels the field forward, paired with technological improvements, achieving continuously higher performance and resource utilization. A key device for system architects, architecture researchers, and scientific users are benchmarks, allowing for well-defined assessment of hardware, software, and algorithms. Many benchmarks exist in the community, from individual niche benchmarks testing specific features, to large-scale benchmark suites for whole procurements. We survey the available HPC benchmarks, summarizing them in table form with key details and concise categorization, also through an interactive website. For categorization, we present a benchmark taxonomy for well-defined characterization of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08347v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Herten, Olga Pearce, Filipe S. M. Guimar\~aes</dc:creator>
    </item>
    <item>
      <title>Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data</title>
      <link>https://arxiv.org/abs/2509.08409</link>
      <description>arXiv:2509.08409v1 Announce Type: new 
Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks of the parameter server in FGL by establishing a peer-to-peer (P2P) communication network among workers. However, while extensive cross-worker communication of graph node embeddings is crucial for DFGL training, it introduces substantial communication costs. Most existing works typically construct sparse network topologies or utilize graph neighbor sampling methods to alleviate the communication overhead in DFGL. Intuitively, integrating these methods may offer promise for doubly improving communication efficiency in DFGL. However, our preliminary experiments indicate that directly combining these methods leads to significant training performance degradation if they are jointly optimized. To address this issue, we propose Duplex, a unified framework that jointly optimizes network topology and graph sampling by accounting for their coupled relationship, thereby significantly reducing communication cost while enhancing training performance in DFGL. To overcome practical DFGL challenges, eg, statistical heterogeneity and dynamic network environments, Duplex introduces a learning-driven algorithm to adaptively determine optimal network topologies and graph sampling ratios for workers. Experimental results demonstrate that Duplex reduces completion time by 20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target accuracy, while improving accuracy by 3.3%--7.9% under identical resource budgets compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08409v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shilong Wang, Jianchun Liu, Hongli Xu, Chenxia Tang, Qianpiao Ma, Liusheng Huang</dc:creator>
    </item>
    <item>
      <title>A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN</title>
      <link>https://arxiv.org/abs/2509.08608</link>
      <description>arXiv:2509.08608v1 Announce Type: new 
Abstract: We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced O-RAN. The cores and cluster architecture are customized for baseband processing, supporting complex (16-bit real&amp;imaginary) instructions: multiply&amp;accumulate, division&amp;square-root, SIMD instructions, and hardware-managed systolic queues, improving up to 1.89x the energy efficiency of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s on complex-valued wireless workloads. Furthermore, the cores also support efficient AI processing on received data at up to 72 GOP/s. HeartStream is fully compatible with base station power and processing latency limits: it achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for B5G/6G uplink.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08608v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yichao Zhang, Marco Bertuletti, Sergio Mazzola, Samuel Riedel, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges</title>
      <link>https://arxiv.org/abs/2509.08770</link>
      <description>arXiv:2509.08770v1 Announce Type: new 
Abstract: To overcome the challenges of ultra-low latency, ubiquitous coverage, and soaring data rates, this article presents a combined use of Near Field Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for Non-Terrestrial Networks (NTN). A system architecture has been presented, which shows that the integration of RHS with NTN platforms such as satellites, High Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can achieve precise beamforming and intelligent wavefront control in near-field regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial resolution. Moreover, key applications, challenges, and future directions have been identified to fully adopt this integration. In addition, a use case analysis has been presented to improve the EE of the system in a public safety use case scenario, further strengthening the UAV-RHS fusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08770v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Ali Jamshed, Muhammad Ahmed Mohsin, Hongliang Zhang, Bushra Haq, Aryan Kaushik, Boya Di, Weiwei Jiang</dc:creator>
    </item>
    <item>
      <title>Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor</title>
      <link>https://arxiv.org/abs/2509.08020</link>
      <description>arXiv:2509.08020v1 Announce Type: cross 
Abstract: Quantitative proteomics plays a central role in uncovering regulatory mechanisms, identifying disease biomarkers, and guiding the development of precision therapies. These insights are often obtained through complex Bayesian models, whose inference procedures are computationally intensive, especially when applied at scale to biological datasets. This limits the accessibility of advanced modelling techniques needed to fully exploit proteomics data. Although Sequential Monte Carlo (SMC) methods offer a parallelisable alternative to traditional Markov Chain Monte Carlo, their high-performance implementations often rely on specialised hardware, increasing both financial and energy costs. We address these challenges by introducing an opportunistic computing framework for SMC samplers, tailored to the demands of large-scale proteomics inference. Our approach leverages idle compute resources at the University of Liverpool via HTCondor, enabling scalable Bayesian inference without dedicated high-performance computing infrastructure. Central to this framework is a novel Coordinator-Manager-Follower architecture that reduces synchronisation overhead and supports robust operation in heterogeneous, unreliable environments. We evaluate the framework on a realistic proteomics model and show that opportunistic SMC delivers accurate inference with weak scaling, increasing samples generated under a fixed time budget as more resources join. To support adoption, we release CondorSMC, an open-source package for deploying SMC samplers in opportunistic computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08020v1</guid>
      <category>q-bio.QM</category>
      <category>cs.DC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell</dc:creator>
    </item>
    <item>
      <title>Sketched Gaussian Mechanism for Private Federated Learning</title>
      <link>https://arxiv.org/abs/2509.08195</link>
      <description>arXiv:2509.08195v1 Announce Type: cross 
Abstract: Communication cost and privacy are two major considerations in federated learning (FL). For communication cost, gradient compression by sketching the clients' transmitted model updates is often used for reducing per-round communication. For privacy, the Gaussian mechanism (GM), which consists of clipping updates and adding Gaussian noise, is commonly used to guarantee client-level differential privacy. Existing literature on private FL analyzes privacy of sketching and GM in an isolated manner, illustrating that sketching provides privacy determined by the sketching dimension and that GM has to supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which directly combines sketching and the Gaussian mechanism for privacy. Using R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy guarantee, which is significantly more flexible and sharper compared to isolated analysis of sketching and GM privacy. In particular, we prove that the privacy level of SGM for a fixed noise magnitude is proportional to $1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for moderate $b$) SGM can provide much stronger privacy guarantees than the original GM under the same noise budget. We demonstrate the application of SGM to FL with either gradient descent or adaptive server optimizers, and establish theoretical results on optimization convergence, which exhibits only a logarithmic dependence on the number of parameters $d$. Experimental results confirm that at the same privacy level, SGM based FL is at least competitive with non-sketching private FL variants and outperforms them in some settings. Moreover, using adaptive optimization at the server improves empirical performance while maintaining the privacy guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08195v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiaobo Li, Zhijie Chen, Arindam Banerjee</dc:creator>
    </item>
    <item>
      <title>DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation</title>
      <link>https://arxiv.org/abs/2509.08449</link>
      <description>arXiv:2509.08449v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, offering strong privacy guarantees. However, existing FL protocols struggle to defend against Byzantine participants, maintain model utility under non-independent and identically distributed (non-IID) data, and remain lightweight for edge devices. Prior work either assumes trusted hardware, uses expensive cryptographic tools, or fails to address privacy and robustness simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated Learning framework that addresses these limitations using a group-based secure aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest servers, DSFL removes this dependency by revealing a key vulnerability: privacy leakage through client-server collusion. DSFL introduces three key innovations: (1) a dual-server secure aggregation protocol that protects updates without encryption or key exchange, (2) a group-wise credit-based filtering mechanism to isolate Byzantine clients based on deviation scores, and (3) a dynamic reward-penalty system for enforcing fair participation. DSFL is evaluated on MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in both IID and non-IID settings. It consistently outperforms existing baselines, including LSFL, homomorphic encryption methods, and differential privacy approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and 68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB communication per round.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08449v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charuka Herath, Yogachandran Rahulamathavan, Varuna De Silva, Sangarapillai Lambotharan</dc:creator>
    </item>
    <item>
      <title>PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints</title>
      <link>https://arxiv.org/abs/2509.08750</link>
      <description>arXiv:2509.08750v1 Announce Type: cross 
Abstract: Federating heterogeneous models on edge devices with diverse resource constraints has been a notable trend in recent years. Compared to traditional federated learning (FL) that assumes an identical model architecture to cooperate, model-heterogeneous FL is more practical and flexible since the model can be customized to satisfy the deployment requirement. Unfortunately, no prior work ever dives into the existing model-heterogeneous FL algorithms under the practical edge device constraints and provides quantitative analysis on various data scenarios and metrics, which motivates us to rethink and re-evaluate this paradigm. In our work, we construct the first system platform \textbf{PracMHBench} to evaluate model-heterogeneous FL on practical constraints of edge devices, where diverse model heterogeneity algorithms are classified and tested on multiple data tasks and metrics. Based on the platform, we perform extensive experiments on these algorithms under the different edge constraints to observe their applicability and the corresponding heterogeneity pattern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08750v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanchun Guo, Bingyan Liu, Yulong Sha, Zhensheng Xian</dc:creator>
    </item>
    <item>
      <title>FRESCO: Fast and Reliable Edge Offloading with Reputation-based Hybrid Smart Contracts</title>
      <link>https://arxiv.org/abs/2410.06715</link>
      <description>arXiv:2410.06715v3 Announce Type: replace 
Abstract: Mobile devices offload latency-sensitive application tasks to edge servers to satisfy applications' Quality of Service (QoS) deadlines. Consequently, ensuring reliable offloading without QoS violations is challenging in distributed and unreliable edge environments. However, current edge offloading solutions are either centralized or do not adequately address challenges in distributed environments. We propose FRESCO, a fast and reliable edge offloading framework that utilizes a blockchain-based reputation system, which enhances the reliability of offloading in the distributed edge. The distributed reputation system tracks the historical performance of edge servers, while blockchain through a consensus mechanism ensures that sensitive reputation information is secured against tampering. However, blockchain consensus typically has high latency, and therefore we employ a Hybrid Smart Contract (HSC) that automatically computes and stores reputation securely on-chain (i.e., on the blockchain) while allowing fast offloading decisions off-chain (i.e., outside of blockchain). The offloading decision engine uses a reputation score to derive fast offloading decisions, which are based on Satisfiability Modulo Theory (SMT). The SMT models edge resource constraints, and QoS deadlines, and can formally guarantee a feasible solution that is valuable for latency-sensitive applications that require high reliability. With a combination of on-chain HSC reputation state management and an off-chain SMT decision engine, FRESCO offloads tasks to reliable servers without being hindered by blockchain consensus. We evaluate FRESCO against real availability traces and simulated applications. FRESCO reduces response time by up to 7.86 times and saves energy by up to 5.4% compared to all baselines while minimizing QoS violations to 0.4% and achieving an average decision time of 5.05 milliseconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06715v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josip Zilic, Vincenzo de Maio, Shashikant Ilager, Ivona Brandic</dc:creator>
    </item>
    <item>
      <title>Validity in Network-Agnostic Byzantine Agreement</title>
      <link>https://arxiv.org/abs/2410.19721</link>
      <description>arXiv:2410.19721v2 Announce Type: replace 
Abstract: Byzantine Agreement (BA) considers a setting of $n$ parties, out of which up to $t$ can exhibit byzantine (malicious) behavior. Honest parties must decide on a common value (agreement), which must belong to a set determined by the honest inputs (validity). Depending on the use case, this set can grow or shrink, leading to various possible desiderata collectively known as validity conditions. Varying the validity property requirement can affect the regime under which BA is solvable.
  Our work investigates how the selected validity property impacts BA solvability in the network-agnostic model, where the network can either be synchronous with up to $t_s$ byzantine parties or asynchronous with up to $t_a \leq t_s$ byzantine parties. We give necessary and sufficient conditions for a validity property to render BA solvable, both for the case with cryptographic setup and for the one without. This traces the precise boundary of solvability in the network-agnostic model for every validity property. Our proof of sufficiency provides a universal protocol, that achieves BA for a given validity property whenever the provided conditions are satisfied.
  We note that, for any non-trivial validity property, the condition $2 \cdot t_s + t_a &lt; n$ is necessary for BA to be solvable, even with cryptographic setup. Specializing this claim to $t_a = 0$ gives that $t &lt; n / 2$ is required whenever one expects a purely synchronous protocol to also work in an asynchronous network when there are no corruptions. This is especially surprising given that, for some validity properties, $t &lt; n$ is a sufficient condition without the last stipulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19721v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrei Constantinescu, Marc Dufay, Diana Ghinea, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Energy-Aware Scheduling Strategies for Partially-Replicable Task Chains on Heterogeneous Processors</title>
      <link>https://arxiv.org/abs/2502.10000</link>
      <description>arXiv:2502.10000v2 Announce Type: replace 
Abstract: The arrival of heterogeneous (or hybrid) multicore architectures has brought new performance trade-offs for applications, and efficiency opportunities to systems. They have also increased the challenges related to thread scheduling, as tasks' execution times will vary depending if they are placed on big (performance) cores or little (efficient) ones. In this paper, we focus on the challenges heterogeneous multicore processors bring to partially-replicable task chains, such as the ones that implement digital communication standards in Software-Defined Radio (SDR). Our objective is to maximize the throughput of these task chains while also minimizing their power consumption. We model this problem as a pipelined workflow scheduling problem using pipelined and replicated parallelism on two types of resources whose objectives are to minimize the period and to use as many little cores as necessary. We propose two greedy heuristics (FERTAC and 2CATAC) and one optimal dynamic programming (HeRAD) solution to the problem. We study an open source implementation of the DVB-S2 communication standard based on the StreamPU runtime. Leading processor vendors are covered with ARM, Apple, AMD, and Intel platforms. Both the achieved throughput and the energy consumption are evaluated. Our results demonstrate the benefits and drawbacks of the different proposed solutions. On average, FERTAC and 2CATAC achieve near-optimal solutions, with periods that are less than 10% worse than the optimal (HeRAD). These three scheduling strategies now enable programmers and users of StreamPU to transparently make use of heterogeneous multicore processors and achieve a throughput that differs from its theoretical maximum by less than 6% on average. On the DVB-S2 receiver, it is also shown that the heterogeneous solutions outperform the best homogeneous ones in terms of energy efficiency by 8% on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10000v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yacine Idouar (ALSOC), Adrien Cassagne (ALSOC), La\'ercio Lima Pilla (TOPAL), Julien Sopena (DELYS), Manuel Bouyer (ALSOC), Diane Orhan (STORM), Lionel Lacassagne (ALSOC), Dimitri Galayko (CYAN), Denis Barthou (Bordeaux INP), Christophe Jego (IMS)</dc:creator>
    </item>
    <item>
      <title>Supervised Distributed Computing</title>
      <link>https://arxiv.org/abs/2503.11600</link>
      <description>arXiv:2503.11600v2 Announce Type: replace 
Abstract: We introduce a new framework for distributed computing that extends and refines the standard master-worker approach of scheduling multi-threaded computations. In this framework, there are different roles: a supervisor, a source, a target, and a collection of workers. Initially, the source stores some instance $I$ of a computational problem, and at the end, the target is supposed to store a correct solution $S(I)$ for that instance. We assume that the computation required for $S(I)$ can be modeled as a directed acyclic graph $G=(V,E)$, where $V$ is a set of tasks and $(v,w) \in E$ if and only if task $w$ needs information from task $v$ in order to be executed. Given $G$, the role of the supervisor is to schedule the execution of the tasks in $G$ by assigning them to the workers. If all workers are honest, information can be exchanged between the workers, and the workers have access to the source and target, the supervisor only needs to know $G$ to successfully schedule the computations. I.e., the supervisor does not have to handle any data itself like in standard master-worker approaches, which has the tremendous benefit that tasks can be run massively in parallel in large distributed environments without the supervisor becoming a bottleneck. But what if a constant fraction of the workers is adversarial? Interestingly, we show that under certain assumptions a data-agnostic scheduling approach would even work in an adversarial setting without (asymptotically) increasing the work required for communication and computations. We demonstrate the validity of these assumptions by presenting concrete solutions for supervised matrix multiplication and sorting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11600v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Augustine, Christian Scheideler, Julian Werthmann</dc:creator>
    </item>
    <item>
      <title>D\'ej\`a Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse</title>
      <link>https://arxiv.org/abs/2506.14107</link>
      <description>arXiv:2506.14107v2 Announce Type: replace 
Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces D\'ej\`a Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, D\'ej\`a Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that D\'ej\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14107v2</guid>
      <category>cs.DC</category>
      <category>cs.CV</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwoo Hwang, Daeun Kim, Sangyeop Lee, Yoonsung Kim, Guseul Heo, Hojoon Kim, Yunseok Jeong, Tadiwos Meaza, Eunhyeok Park, Jeongseob Ahn, Jongse Park</dc:creator>
    </item>
    <item>
      <title>Distributed Algorithms for Potential Problems</title>
      <link>https://arxiv.org/abs/2507.12038</link>
      <description>arXiv:2507.12038v3 Announce Type: replace 
Abstract: In this work we present a fast distributed algorithm for local potential problems: these are graph problems where the task is to find a locally optimal solution where no node can unilaterally improve the utility in its local neighborhood by changing its own label. A simple example of such a problem is the task of finding a locally optimal cut, i.e., a cut where for each node at least half of its incident edges are cut edges. The distributed round complexity of locally optimal cut has been wide open; the problem is known to require $\Omega(\log n)$ rounds in the deterministic LOCAL model and $\Omega(\log \log n)$ rounds in the randomized LOCAL model, but the only known upper bound is the trivial brute-force solution of $O(n)$ rounds. Locally optimal cut in bounded-degree graphs is perhaps the simplest example of a locally checkable labeling problem for which there is still such a large gap between current upper and lower bounds. We show that in bounded-degree graphs, all local potential problems, including locally optimal cut, can be solved in $\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models. In particular, the deterministic round complexity of the locally optimal cut problem is now settled to $\log^{\Theta(1)} n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12038v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkida Balliu, Thomas Boudier, Francesco d'Amore, Dennis Olivetti, Gustav Schmid, Jukka Suomela</dc:creator>
    </item>
    <item>
      <title>Formal Modeling and Verification of the Algorand Consensus Protocol in CADP</title>
      <link>https://arxiv.org/abs/2508.19452</link>
      <description>arXiv:2508.19452v4 Announce Type: replace 
Abstract: Algorand is a scalable and secure permissionless blockchain that achieves proof-of-stake consensus via cryptographic self-sortition and binary Byzantine agreement. In this paper, we present a process algebraic model of the Algorand consensus protocol with the aim of enabling rigorous formal verification. Our model captures the behavior of participants with respect to the structured alternation of consensus steps toward a committee-based agreement by means of a probabilistic process calculus. We validate the correctness of the protocol in the absence of adversaries and then extend our model to capture the influence of coordinated malicious nodes that can force the commit of an empty block instead of the proposed one. The adversarial scenario is analyzed through an equivalence-checking-based noninterference framework that we have implemented in the CADP verification toolkit. In addition to highlighting both the robustness and the limitations of the Algorand protocol under adversarial assumptions, this work illustrates the added value of using formal methods for the analysis of blockchain consensus algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19452v4</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Esposito, Francesco P. Rossi, Marco Bernardo, Francesco Fabris, Hubert Garavel</dc:creator>
    </item>
    <item>
      <title>Ocior: Ultra-Fast Asynchronous Leaderless Consensus with Two-Round Finality, Linear Overhead, and Adaptive Security</title>
      <link>https://arxiv.org/abs/2509.01118</link>
      <description>arXiv:2509.01118v2 Announce Type: replace 
Abstract: In this work, we propose Ocior, a practical asynchronous Byzantine fault-tolerant (BFT) consensus protocol that achieves the optimal performance in resilience, communication, computation, and round complexity. Unlike traditional BFT consensus protocols, Ocior processes incoming transactions individually and concurrently using parallel instances of consensus. While leader-based consensus protocols rely on a designated leader to propose transactions, Ocior is a leaderless consensus protocol that guarantees stable liveness. Ocior achieves: 1) Optimal resilience: Ocior tolerates up to $t$ faulty nodes controlled by an adaptive adversary, for $n\geq 3t+1$. 2) Optimal communication complexity: The total expected communication per transaction is $O(n)$. 3) Optimal (or near-optimal) computation complexity: The total computation per transaction is $O(n)$ in the best case, or $O(n \log^2 n)$ in the worst case. 4) Optimal round complexity: A legitimate two-party transaction can be finalized with a good-case latency of two asynchronous rounds, for any $n\geq 3t+1$. The good case in terms of latency refers to the scenario where the transaction is proposed by any (not necessarily designated) honest node. A two-party transaction involves the transfer of digital assets from one user (or group of users) to one or more recipients. To support efficient consensus, we introduce a novel non-interactive threshold signature (TS) scheme called OciorBLSts. It offers fast signature aggregation, and is adaptively secure. OciorBLSts achieves a computation complexity of signature aggregation of only $O(n)$ in the good cases. Moreover, OciorBLSts supports the property of Instantaneous TS Aggregation. This enables real-time aggregation of partial signatures as they arrive, reducing waiting time and improving responsiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01118v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chen</dc:creator>
    </item>
    <item>
      <title>FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models</title>
      <link>https://arxiv.org/abs/2403.09904</link>
      <description>arXiv:2403.09904v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09904v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Yi, Georg Meinhardt, Laurent Condat, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Traversal Learning: A Lossless And Efficient Distributed Learning Framework</title>
      <link>https://arxiv.org/abs/2504.07471</link>
      <description>arXiv:2504.07471v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies. The implementation of TL is available at https://github.com/neouly-inc/Traversal-Learning</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07471v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erdenebileg Batbaatar, Jeonggeol Kim, Yongcheol Kim, Young Yoon</dc:creator>
    </item>
  </channel>
</rss>
